2020.acl-main.455,W13-2322,0,0.0294654,"S (1) j=1...M i.e. the average embedding cosine distance to all arguments in the summary. Argument embeddings EiD and EjS are average embeddings of contentword tokens belonging to the arguments:6 Ei∗ = avg e∗k k∈A∗ i ,k6∈stops (2) ∗ ∈ {D, S}, “stops” denotes a list of stopwords. Fact-based weighting: We can represent the summary as of  and the document  two sequences S , facts F1D , F2D , · · · FND0 and F1S , F2S , · · · FM 0 and weight the i-th fact in the document by its average distance to facts in the summary: wif = 3 avg dfij (3) j∈1...M 0 We avoid using sentence-level MRs such as AMR (Banarescu et al., 2013), since current state-of-the-art performance of parsers is far behind compared to the simpler SRL task. 4 By concatenating, the information in each text can be embedded in each other through self-attention. This is useful since the summary sometimes contains additional and/or common-sense knowledge not captured in the document. 5 For example, in Fig. 1, ARG0, V, ARG1 in FACT1, and all the arguments in FACT2 are leaf arguments in the sentence, whereas ARG2 in FACT1 is not. 6 For example, in Fig. 1, “her” and “thanks” are two tokens directly attached to the argument ARG1 of FACT1. Thus, the embe"
2020.acl-main.455,P19-1264,0,0.0182834,"ERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 507"
2020.acl-main.455,J10-3005,0,0.133206,"ssages on social media”. (Teufel and van Halteren, 2004), or via crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors"
2020.acl-main.455,N19-1423,0,0.021835,"and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary, i.e. be more sensitive to hallucinations and omissions.1 In particular, we weight the facts present in the source document according to the facts selected by a human-written summary. This alignment is conducted using contextual, rather than token-level, embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Good"
2020.acl-main.455,P19-1330,0,0.0360994,"Missing"
2020.acl-main.455,P18-2058,0,0.0125368,"ng SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple observation that arguments can function as separate predicates themselves, we construct a hierarchical tree structure for the whole sentence. We create the tree meaning representa2 We use the SRL implementation of He et al. (2018) found in https://allennlp.org with 86.49 test F1 on the Ontonotes 5.0 dataset. Automatic Content Weighting We compute argument and fact weights by measuring the similarity of facts/arguments in the original document and the target summary based on their BERT word embeddings (for content words only) and their distance in the tree MR. We denote tokens its S as  D ofD a document D and  summary D D S S S t = t1 , t2 , · · · tn and t = t1 , t2 , · · · tSm . To get their corresponding contextual embeddings S 4 eD k and ek , we concatenate the two texts, feed them into a pre-trained BERT model (De"
2020.acl-main.455,P17-1044,0,0.0358549,"sed “abdicate” with 13 We computed HROUGE for B ERT S UM A BS using https://github.com/sheffieldnlp/highres. 5074 Model TC ONV S2S P T G EN B ERT S UM A BS CorrF/A Corr-F Corr-A 0.616 0.636 0.596 0.623 0.655 0.683 CorrF/A(L) Corr-F Corr-A 0.700 0.650 0.664 0.620 0.715 0.670 ROUGE R1 R2 RL 31.89 11.54 25.75 29.70 9.21 23.24 38.53 16.09 30.80 BERTScore P R F1 0.613 0.573 0.591 0.577 0.566 0.570 0.628 0.616 0.621 Table 4: Summarisation models evaluated using Corr-F/A on full test set, with ROUGE and BERTScore scores. Note that Corr-F/A(L) is Corr-F/A calculated using a lower-performing SRL tool (He et al., 2017, see Section 6.2). # Source Ground truth 1 B ERT S UM A BS TC ONV S2S Ground truth 2 B ERT S UM A BS P T G EN Ground truth 3 B ERT S UM A BS TC ONV S2S Ground truth 4 B ERT S UM A BS TC ONV S2S Summary Corr-F Corr-A BS-F1 Japan’s emperor Akihito has expressed his desire to abdicate in the next few years, public broadcaster NHK reports. Japan’s emperor Akihito is considering whether to become the next president of the country, reports say. 0.68 0.68 0.67 Japan’s emperor Akihito has announced that he will step down in the Japanese capital, Tokyo. 0.81 0.71 0.67 Dick Advocaat has resigned as Sun"
2020.acl-main.455,D19-1051,0,0.0534601,"Missing"
2020.acl-main.455,W04-1013,0,0.409006,"9), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary, i.e. be more sensitive to hallucinations and omissions.1 In particular, we weight the facts present in the source document according to the facts selected by a human-written summary. This alignment is conducted using contextual, rather than token-level, embeddings, e.g., BERT"
2020.acl-main.455,D19-1387,0,0.102108,"summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Li"
2020.acl-main.455,D18-1206,0,0.449005,"). 1 Introduction Text summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using e"
2020.acl-main.455,N04-1019,0,0.198524,"dressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5071–5081 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tion (MR) from the list of facts by choosing the fact with the largest coverage as the root and recursively build sub-trees by replacing arguments with their corresponding sub-facts (ARG2 in FACT1 is replaced by FACT2 in Fig. 1).3 FACT1-tweet: [ARG0: the queen] has [V: tweeted] [ARG1: her tha"
2020.acl-main.455,J05-1004,0,0.01223,"the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple observation that arguments can function as separate predicates themselves, we construct a hierarchical tree structure for the whole sentence. We create the tree meaning representa2 We use the SRL implementation of He et al. (2018) found in ht"
2020.acl-main.455,D18-1437,0,0.0205634,"ng and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary,"
2020.acl-main.455,E17-2007,0,0.0468259,"ant information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on"
2020.acl-main.455,D19-1320,0,0.0733442,"crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple"
2020.acl-main.455,P17-1099,0,0.349363,"Hardy et al. (2019). 1 Introduction Text summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate agai"
2020.acl-main.455,D19-1116,0,0.103063,"using contextual, rather than token-level, embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting"
2020.acl-main.455,W04-3254,0,0.241663,"Missing"
2020.acl-main.455,W17-4508,0,0.0528854,"Missing"
2020.acl-main.455,D19-1618,0,0.0129665,"M-LOC on social media] SRL Propositions Tree MR FACT1-tweet 3.2 ARG0 V ARG1 ARG2 the queen had tweeted her thanks FACT2-send ARG0 R-ARG0 V ARG1 ARGM-LOC people who sent her 90th birthday messages on social media Figure 1: List of SRL propositions and corresponding tree MR with two facts for the sentence “The queen has tweeted her thanks to people who sent her 90th birthday messages on social media”. (Teufel and van Halteren, 2004), or via crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact R"
2020.acl-main.455,D19-1053,0,0.0162519,"embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational L"
2020.inlg-1.19,D19-1052,0,0.0537992,"Missing"
2020.inlg-1.19,W19-8652,1,0.85801,"= Medium, 3 = Correct) and answers are averaged over multiple annotators. In our experiments discussed in Section 5.1, we consider a sentence correct if it achieved human rating 2.5 or higher (we also tried a threshold of 2.0, with slightly worse results). For the E2E dataset, the challenge results were checked for semantic accuracy using a handcrafted automatic script (Dušek et al., 2020),5 we therefore use this automatic script as the ground truth for evaluating our metric in Section 5.2. We further use small sets of system outputs and humanwritten texts with expert annotation (provided by Dušek et al., 2019) to evaluate our approach against gold-standard annotation and to compare to existing semantic accuracy classifiers for E2E data in Section 5.3. We evaluate the Default and Backoff approaches to acquiring templates as described in Section 3.2. The Default setup works with one custom template per predicate type. For WebNLG, we obtained templates by delexicalizing human references for single-triple examples from WebNLG training data.6 For E2E, we handcrafted 8 templates. The templates are filled with values from individual input triples and concatenated for multitriple inputs as described in Sec"
2020.inlg-1.19,P16-2008,1,0.902942,"Missing"
2020.inlg-1.19,W17-3518,0,0.0360895,"s ROUGH) where the latter three results are collapsed into not_OK. The F INE 4-way output is more useful for system evaluation (we can distinguish whether the system tends to hallucinate or omit information). The ROUGH 2-way output corresponds more to a usage inside an NLG system for output reranking or filtering: any output that is not_OK should be penalized/filtered out. Additionally, we compute a confidence score of the model as the minimum of all the entailment probabilities. 4 Experimental Setup We experiment with two recent English data-totext datasets with a triple-like format: WebNLG (Gardent et al., 2017) and E2E (Novikova et al., 2017).4 Since both of them were used in shared tasks, sets of system outputs and measures of semantic accuracy are available (see Supplementary for details). For WebNLG, we compare our metric with crowdsourced human ratings of semantic adequacy (Shimorina et al., 2019). Human annotators used a three-point Likert scale (1 = Incorrect, 2 = Medium, 3 = Correct) and answers are averaged over multiple annotators. In our experiments discussed in Section 5.1, we consider a sentence correct if it achieved human rating 2.5 or higher (we also tried a threshold of 2.0, with sli"
2020.inlg-1.19,W18-6505,0,0.0397197,"l entailment between the input data and the output text in both directions, allowing us to reveal omissions or hallucinations. Input data are converted to text for NLI using trivial templates. Our experiments on two recent D2T datasets show that our metric can achieve high accuracy in identifying erroneous system outputs. 1 Introduction Neural models may reduce the effort for building natural language generation (NLG) systems and produce very natural outputs, at the cost of limited control over the model outputs. State-of-the-art neural D2T models are prone to omitting or hallucinating facts (Gehrmann et al., 2018; Castro Ferreira et al., 2019; Dušek et al., 2020), which restricts their real-world deployment. Recognizing these errors is thus essential for proper system evaluation and further research in D2T generation. In general, evaluating the semantic accuracy of D2T generation outputs requires full natural language understanding. Minor changes in wording may cause major differences in the meaning of the text, making it difficult for handcrafted heuristics to cover all edge cases. Human evaluation, on the other hand, is expensive and difficult to scale. We note that the task of checking if a generat"
2020.inlg-1.19,2020.coling-main.218,0,0.122877,"Missing"
2020.inlg-1.19,N18-1014,0,0.0238347,"main: 134 Human-written (E2E training set) Af Ar R P F1 Af System outputs (TGen) Ar R P F1 Slug2Slug aligner E2E slot error script TGen reranker 0.685 0.820 0.110 0.765 0.885 0.435 0.550 1.000 0.975 0.800 0.777 0.413 0.652 0.874 0.579 0.995 0.995 0.220 1.000 0.995 0.278 1.000 1.000 1.000 1.000 0.950 0.116 1.000 0.974 0.208 Default Backoff 0.600 0.530 0.700 0.640 0.625 0.675 0.625 0.540 0.625 0.600 0.978 0.833 0.978 0.833 0.947 0.974 0.837 0.359 0.888 0.525 Table 3: Semantic classifiers evaluated on expert human annotation on E2E data (see Table 2 for metrics legend). • Slug2Slug slot aligner (Juraska et al., 2018) is based on keyword matches. It is carefully tuned but not designed to detect hallucination; it only checks for presence of facts from the input MR. • E2E slot error script (used in Section 5.2) is based on regular expressions; it is also able to detect irrelevant facts. • TGen reranker is an LSTM-based model trained on the E2E training data to rerank outputs of the TGen system (Dušek and Jurˇcíˇcek, 2016) based on their semantic accuracy. The results for all classifiers (in Table 3) are much weaker on human-written data, which exhibit much more variability than system outputs. The TGen reran"
2020.inlg-1.19,K18-1031,0,0.0247462,"al error analysis shows that some instances marked as errors were in fact assessed correctly by our metric; we also identified a few major sources of errors that can be mitigated by in-domain tuning. The experimental code for our metric is now available on GitHub.2 2 Related Work Automatic Evaluation of NLG NLG outputs were traditionally evaluated by reference-based metrics measuring n-gram overlap with a reference, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007). Alternative, referenceless quality estimation metrics based on language model scores (Kann et al., 2018) or linguistic features (Tian et al., 2018) focus on fluency and do not consider semantic accuracy. Recent works try to estimate NLG output quality with finetuned pretrained models (Zhou and Xu, 2020; Zhang et al., 2020a; Sellam et al., 2020). The score from these models can capture some aspects of semantic accuracy, but only implicitly. Semantic Accuracy To our knowledge, there is no generally accepted automatic metric for explicitly measuring semantic accuracy of NLG outputs. The closest commonly used metric is the slot error rate, which is typically based on pattern matching tailored for a"
2020.inlg-1.19,W19-8672,0,0.0526029,"plicitly. Semantic Accuracy To our knowledge, there is no generally accepted automatic metric for explicitly measuring semantic accuracy of NLG outputs. The closest commonly used metric is the slot error rate, which is typically based on pattern matching tailored for a given dataset (Reed et al., 2018; Mi et al., 2019; Dušek et al., 2020). Recently, Goodrich et al. (2019) introduced a metric based on training a neural model on named-entity recognition and fact extraction. Faithful NLG Some recent neural NLG systems train specifically for semantic accuracy (Nie et al., 2019; Tian et al., 2019; Kedzie and McKeown, 2019). Similarly to us, Harkous et al. (2020) use a pretrained neural model as a classifier to detect inaccurate output, finetuning the classifier on manually augmented domain-specific data. Unlike previous works, we use a pretrained neural model finetuned for NLI which we do not fur2 ther train on any domain-specific data. 3 Method 3.1 NLI Model We use pretrained RoBERTa (Liu et al., 2019b) as implemented in the Transformers library (Wolf et al., 2020) for our NLI model. Specifically, we use the roberta-large-mnli3 checkpoint, which was finetuned on the MultiNLI dataset (Williams et al., 2018). We"
2020.inlg-1.19,W07-0734,0,0.23551,"Missing"
2020.inlg-1.19,W04-1013,0,0.0523956,"Missing"
2020.inlg-1.19,P19-1441,0,0.0329027,"making it difficult for handcrafted heuristics to cover all edge cases. Human evaluation, on the other hand, is expensive and difficult to scale. We note that the task of checking if a generated sentence includes/entails a particular fact is very close to the task of natural language inference (NLI). NLI is a sequence classification task which takes two inputs—a hypothesis and a premise— and produces one of the possible outputs: the hypothesis is entailed by (follows from) the premise, contradicts the premise, or their relation is neutral. Recently, neural models for NLI (Zhang et al., 2020b; Liu et al., 2019a,b) reached near-human levels of performance and NLI was used for evaluating the output of abstractive summarization systems (Maynez et al., 2020). This brings a question: Can we use an NLI model for evaluating the semantic accuracy of D2T outputs? The main idea of our method is to check with a general pretrained NLI model if the semantic information implied by the input data and the generated text is equal. We achieve this by using the NLI model to check for entailment in two directions: By inferring input facts from the generated text we can check for omissions, while the other direction al"
2020.inlg-1.19,2021.ccl-1.108,0,0.0517514,"Missing"
2020.inlg-1.19,2020.acl-main.173,0,0.0255967,"e. We note that the task of checking if a generated sentence includes/entails a particular fact is very close to the task of natural language inference (NLI). NLI is a sequence classification task which takes two inputs—a hypothesis and a premise— and produces one of the possible outputs: the hypothesis is entailed by (follows from) the premise, contradicts the premise, or their relation is neutral. Recently, neural models for NLI (Zhang et al., 2020b; Liu et al., 2019a,b) reached near-human levels of performance and NLI was used for evaluating the output of abstractive summarization systems (Maynez et al., 2020). This brings a question: Can we use an NLI model for evaluating the semantic accuracy of D2T outputs? The main idea of our method is to check with a general pretrained NLI model if the semantic information implied by the input data and the generated text is equal. We achieve this by using the NLI model to check for entailment in two directions: By inferring input facts from the generated text we can check for omissions, while the other direction allows us to check for hallucinations.1 For instance, consider the two input facts from Figure 1: (Blue Spice |eat_type |pub), (Blue Spice |area |riv"
2020.inlg-1.19,P19-1256,0,0.0277448,"cts of semantic accuracy, but only implicitly. Semantic Accuracy To our knowledge, there is no generally accepted automatic metric for explicitly measuring semantic accuracy of NLG outputs. The closest commonly used metric is the slot error rate, which is typically based on pattern matching tailored for a given dataset (Reed et al., 2018; Mi et al., 2019; Dušek et al., 2020). Recently, Goodrich et al. (2019) introduced a metric based on training a neural model on named-entity recognition and fact extraction. Faithful NLG Some recent neural NLG systems train specifically for semantic accuracy (Nie et al., 2019; Tian et al., 2019; Kedzie and McKeown, 2019). Similarly to us, Harkous et al. (2020) use a pretrained neural model as a classifier to detect inaccurate output, finetuning the classifier on manually augmented domain-specific data. Unlike previous works, we use a pretrained neural model finetuned for NLI which we do not fur2 ther train on any domain-specific data. 3 Method 3.1 NLI Model We use pretrained RoBERTa (Liu et al., 2019b) as implemented in the Transformers library (Wolf et al., 2020) for our NLI model. Specifically, we use the roberta-large-mnli3 checkpoint, which was finetuned on th"
2020.inlg-1.19,W17-5525,1,0.869845,"results are collapsed into not_OK. The F INE 4-way output is more useful for system evaluation (we can distinguish whether the system tends to hallucinate or omit information). The ROUGH 2-way output corresponds more to a usage inside an NLG system for output reranking or filtering: any output that is not_OK should be penalized/filtered out. Additionally, we compute a confidence score of the model as the minimum of all the entailment probabilities. 4 Experimental Setup We experiment with two recent English data-totext datasets with a triple-like format: WebNLG (Gardent et al., 2017) and E2E (Novikova et al., 2017).4 Since both of them were used in shared tasks, sets of system outputs and measures of semantic accuracy are available (see Supplementary for details). For WebNLG, we compare our metric with crowdsourced human ratings of semantic adequacy (Shimorina et al., 2019). Human annotators used a three-point Likert scale (1 = Incorrect, 2 = Medium, 3 = Correct) and answers are averaged over multiple annotators. In our experiments discussed in Section 5.1, we consider a sentence correct if it achieved human rating 2.5 or higher (we also tried a threshold of 2.0, with slightly worse results). For the E2"
2020.inlg-1.19,P02-1040,0,0.106343,"Missing"
2020.inlg-1.19,W18-6535,0,0.0366637,"features (Tian et al., 2018) focus on fluency and do not consider semantic accuracy. Recent works try to estimate NLG output quality with finetuned pretrained models (Zhou and Xu, 2020; Zhang et al., 2020a; Sellam et al., 2020). The score from these models can capture some aspects of semantic accuracy, but only implicitly. Semantic Accuracy To our knowledge, there is no generally accepted automatic metric for explicitly measuring semantic accuracy of NLG outputs. The closest commonly used metric is the slot error rate, which is typically based on pattern matching tailored for a given dataset (Reed et al., 2018; Mi et al., 2019; Dušek et al., 2020). Recently, Goodrich et al. (2019) introduced a metric based on training a neural model on named-entity recognition and fact extraction. Faithful NLG Some recent neural NLG systems train specifically for semantic accuracy (Nie et al., 2019; Tian et al., 2019; Kedzie and McKeown, 2019). Similarly to us, Harkous et al. (2020) use a pretrained neural model as a classifier to detect inaccurate output, finetuning the classifier on manually augmented domain-specific data. Unlike previous works, we use a pretrained neural model finetuned for NLI which we do not f"
2020.inlg-1.19,2020.acl-main.704,0,0.0438957,"now available on GitHub.2 2 Related Work Automatic Evaluation of NLG NLG outputs were traditionally evaluated by reference-based metrics measuring n-gram overlap with a reference, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007). Alternative, referenceless quality estimation metrics based on language model scores (Kann et al., 2018) or linguistic features (Tian et al., 2018) focus on fluency and do not consider semantic accuracy. Recent works try to estimate NLG output quality with finetuned pretrained models (Zhou and Xu, 2020; Zhang et al., 2020a; Sellam et al., 2020). The score from these models can capture some aspects of semantic accuracy, but only implicitly. Semantic Accuracy To our knowledge, there is no generally accepted automatic metric for explicitly measuring semantic accuracy of NLG outputs. The closest commonly used metric is the slot error rate, which is typically based on pattern matching tailored for a given dataset (Reed et al., 2018; Mi et al., 2019; Dušek et al., 2020). Recently, Goodrich et al. (2019) introduced a metric based on training a neural model on named-entity recognition and fact extraction. Faithful NLG Some recent neural NLG"
2020.inlg-1.19,W18-6512,0,0.0182883,"marked as errors were in fact assessed correctly by our metric; we also identified a few major sources of errors that can be mitigated by in-domain tuning. The experimental code for our metric is now available on GitHub.2 2 Related Work Automatic Evaluation of NLG NLG outputs were traditionally evaluated by reference-based metrics measuring n-gram overlap with a reference, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007). Alternative, referenceless quality estimation metrics based on language model scores (Kann et al., 2018) or linguistic features (Tian et al., 2018) focus on fluency and do not consider semantic accuracy. Recent works try to estimate NLG output quality with finetuned pretrained models (Zhou and Xu, 2020; Zhang et al., 2020a; Sellam et al., 2020). The score from these models can capture some aspects of semantic accuracy, but only implicitly. Semantic Accuracy To our knowledge, there is no generally accepted automatic metric for explicitly measuring semantic accuracy of NLG outputs. The closest commonly used metric is the slot error rate, which is typically based on pattern matching tailored for a given dataset (Reed et al., 2018; Mi et al."
2020.inlg-1.19,N18-1101,0,0.0381551,"Kedzie and McKeown, 2019). Similarly to us, Harkous et al. (2020) use a pretrained neural model as a classifier to detect inaccurate output, finetuning the classifier on manually augmented domain-specific data. Unlike previous works, we use a pretrained neural model finetuned for NLI which we do not fur2 ther train on any domain-specific data. 3 Method 3.1 NLI Model We use pretrained RoBERTa (Liu et al., 2019b) as implemented in the Transformers library (Wolf et al., 2020) for our NLI model. Specifically, we use the roberta-large-mnli3 checkpoint, which was finetuned on the MultiNLI dataset (Williams et al., 2018). We use the model as is, without any further training. Given a premise text and a hypothesis text, the NLI model produces a probability distribution over three results: contradiction, neutral and entailment (cf. Section 1). We consider a NLI check as passed if the probability for entailment is the highest of the three. 3.2 Data Preparation The input to our metric is a set of facts (the input for a D2T system) and the corresponding verbalization of these facts (the output from a D2T system). In our setup, the facts are RDF-like triples in the subject-predicate-object form. We convert each trip"
2020.inlg-1.19,D17-1239,0,0.0539105,"Missing"
2020.inlg-1.9,P19-1331,0,0.0196055,"019) or employ a natural language understanding model (Nie et al., 2019) to improve the faithfulness of the output. Recently, Chen et al. (2020) finetuned GPT-2 (Radford et al., 2019) for a few-shot domain adaptation. Several models were recently applied to generic text editing tasks. L ASERTAGGER (Malmi et al., 2019), which we use in our approach, is a sequence tagging model based on the Transformer (Vaswani et al., 2017) architecture with the BERT (Devlin et al., 2019) pre-trained language model as the encoder. Other recent text-editing models without a pre-trained backbone include EditNTS (Dong et al., 2019) and Levenshtein Transformer (Gu et al., 2019). Concurrently with our work, Kale and Rastogi (2020) explored using templates for dialogue response generation. They use the sequence-tosequence T5 model (Raffel et al., 2019) to generate the output text from scratch instead of iteratively editing the intermediate outputs, which leaves less control over the model. 3 3.1 Template Extraction We collect a set of templates for each predicate. The templates can be either handcrafted, or automatically extracted from the lexicalizations of the single-triple examples in the training data. For unseen predi"
2020.inlg-1.9,W05-0909,0,0.0437877,"ore diverse, e.g. if the template for the predicate country has to be selected from {<subject> is situated within <object>, <subject> is a dish found in <object>}, LMS CORER helps to select the semantically accurate template for the specific entities. The literal copying of entities can be too ˙ rigid in some cases, e.g. Atatürk Monument (Izmir) is made of “Bronze”, but these disfluencies can be improved in the fusion step. Analysis of Results We compute the metrics used in the evaluation of the E2E Challenge (Dušek et al., 2020): BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015). The results are shown in Table 2. The scores from the automatic metrics lag behind the state-of-theart, although both the fusion and the zero-shot approaches show improvements over the baseline. We examine the details in the following paragraphs, discussing the behavior of our approach, and we outline plans for improving the results in Section 6. Reordering L ASERTAGGER does not allow arbitrary reordering of words in the sentence, which can limit the expressiveness of the sentence fusion model. Consider the example in Figure 1: in order t"
2020.inlg-1.9,J05-3002,0,0.0991197,"he neural model can be used for combining the lexicalizations to improve the output fluency. In traditional pipeline-based NLG systems (Reiter and Dale, 2000), combining the lexicalizations is a non-trivial multi-stage process. Text structuring and sentence aggregation are first used to determine the order of facts and their assignment to sentences, followed by referring expression generation and linguistic realization. We argue that with a neural model, combining the lexicalizations can be simplified as several iterations of sentence fusion—a task of combining sentences into a coherent text (Barzilay and McKeown, 2005). Our contributions are the following: We present a novel approach to data-to-text generation based on iterative text editing. Our approach maximizes the completeness and semantic accuracy of the output text while leveraging the abilities of recent pre-trained models for text editing (L ASERTAGGER) and language modeling (GPT-2) to improve the text fluency. To this end, we first transform data items to text using trivial templates, and then we iteratively improve the resulting text by a neural model trained for the sentence fusion task. The output of the model is filtered by a simple heuristic"
2020.inlg-1.9,W19-8652,1,0.919775,"using them in the subsequent steps of the algorithm (this is possible due to the low variability of the predicates in the dataset).4 See Table 1 for examples of templates we used in our experiments. ¸1 P pxi |x1 . . . xi´1 q Datasets The WebNLG dataset (Gardent et al., 2017) consists of sets of DBPedia RDF triples and their lexicalizations. Following previous work, we use version 1.4 from Castro Ferreira et al. (2018). The E2E dataset (Novikova et al., 2017) contains restaurant descriptions based on sets of attributes (slots). In this work, we refer to the cleaned version of the E2E dataset (Dušek et al., 2019). For the domain adaptation experiments, we use D ISCO F USE (Geva et al., 2019), which is a large-scale dataset for sentence fusion. makes the model less prone to common neural model errors such as hallucination, which allows us to control the semantic accuracy to a great extent using only simple heuristics and language model rescoring. 3.3 Experiments Decoding Algorithm The input of the algorithm (Figure 1) is a set of n ordered triples. First, we lexicalize the triple t0 to get the base text X0 . We choose the lexicalization for the triple as the filled template with the best score from LMS"
2020.inlg-1.9,D18-1547,0,0.038926,"Missing"
2020.inlg-1.9,P16-2008,1,0.920564,"Missing"
2020.inlg-1.9,D19-1052,0,0.0347665,"Missing"
2020.inlg-1.9,2020.inlg-1.19,1,0.824441,"w York City, Staten Island. Reference Albert Jennings Fountain was born in Staten Island, New York City and died in the New Mexico Territory. Table 3: An example of correct behavior of the algorithm on the WebNLG dataset. Newly added entities are underlined, the output from Step #2 is the output text. sistency of results with a higher number of data items. Our string matching heuristic is quite crude and may lead to a high number of fallbacks. Introducing a more precise heuristic, such as a semantic fidelity classifier (Harkous et al., 2020), or a model trained for natural language inference (Dušek and Kasner, 2020) could help to promote lexical variability of the text. Finally, we note that the text-editing paradigm allows to visualize the changes made by the model, introducing the option to accept or reject the changes at each step, and even build a set of custom rules on top of the individual edit operations based on the affected tokens. This flexibility could be useful for tweaking the model manually for a production system. which has to be present in the vocabulary. Domain Independence The zero-shot model trained on D ISCO F USE is able to correctly pronominalize or delete repeated entities and join"
2020.inlg-1.9,W17-3518,0,0.0469555,"such examples; therefore our solution is twofold: we extract the templates for pairs of predicates, using them as a starting point for the algorithm in order to leverage the lexical variability in the data (manually filtering out the templates with semantic noise), and we also create a small set of templates for each single predicate manually, using them in the subsequent steps of the algorithm (this is possible due to the low variability of the predicates in the dataset).4 See Table 1 for examples of templates we used in our experiments. ¸1 P pxi |x1 . . . xi´1 q Datasets The WebNLG dataset (Gardent et al., 2017) consists of sets of DBPedia RDF triples and their lexicalizations. Following previous work, we use version 1.4 from Castro Ferreira et al. (2018). The E2E dataset (Novikova et al., 2017) contains restaurant descriptions based on sets of attributes (slots). In this work, we refer to the cleaned version of the E2E dataset (Dušek et al., 2019). For the domain adaptation experiments, we use D ISCO F USE (Geva et al., 2019), which is a large-scale dataset for sentence fusion. makes the model less prone to common neural model errors such as hallucination, which allows us to control the semantic acc"
2020.inlg-1.9,W18-6521,0,0.0188977,"thm in order to leverage the lexical variability in the data (manually filtering out the templates with semantic noise), and we also create a small set of templates for each single predicate manually, using them in the subsequent steps of the algorithm (this is possible due to the low variability of the predicates in the dataset).4 See Table 1 for examples of templates we used in our experiments. ¸1 P pxi |x1 . . . xi´1 q Datasets The WebNLG dataset (Gardent et al., 2017) consists of sets of DBPedia RDF triples and their lexicalizations. Following previous work, we use version 1.4 from Castro Ferreira et al. (2018). The E2E dataset (Novikova et al., 2017) contains restaurant descriptions based on sets of attributes (slots). In this work, we refer to the cleaned version of the E2E dataset (Dušek et al., 2019). For the domain adaptation experiments, we use D ISCO F USE (Geva et al., 2019), which is a large-scale dataset for sentence fusion. makes the model less prone to common neural model errors such as hallucination, which allows us to control the semantic accuracy to a great extent using only simple heuristics and language model rescoring. 3.3 Experiments Decoding Algorithm The input of the algorithm ("
2020.inlg-1.9,2020.acl-main.18,0,0.0848025,"acy of neural D2T approaches has attracted a lot of research interest lately. Similarly to us, other systems use a generatethen-rerank approach (Dušek and Jurˇcíˇcek, 2016; Juraska et al., 2018) or a classifier to filter incorrect output (Harkous et al., 2020). Moryossef et al. (2019a,b) split the D2T process into a symbolic text-planning stage and a neural generation stage. Other works improve the robustness of the neural model (Tian et al., 2019; Kedzie and McKeown, 2019) or employ a natural language understanding model (Nie et al., 2019) to improve the faithfulness of the output. Recently, Chen et al. (2020) finetuned GPT-2 (Radford et al., 2019) for a few-shot domain adaptation. Several models were recently applied to generic text editing tasks. L ASERTAGGER (Malmi et al., 2019), which we use in our approach, is a sequence tagging model based on the Transformer (Vaswani et al., 2017) architecture with the BERT (Devlin et al., 2019) pre-trained language model as the encoder. Other recent text-editing models without a pre-trained backbone include EditNTS (Dong et al., 2019) and Levenshtein Transformer (Gu et al., 2019). Concurrently with our work, Kale and Rastogi (2020) explored using templates f"
2020.inlg-1.9,N19-1348,0,0.028346,"low variability of the predicates in the dataset).4 See Table 1 for examples of templates we used in our experiments. ¸1 P pxi |x1 . . . xi´1 q Datasets The WebNLG dataset (Gardent et al., 2017) consists of sets of DBPedia RDF triples and their lexicalizations. Following previous work, we use version 1.4 from Castro Ferreira et al. (2018). The E2E dataset (Novikova et al., 2017) contains restaurant descriptions based on sets of attributes (slots). In this work, we refer to the cleaned version of the E2E dataset (Dušek et al., 2019). For the domain adaptation experiments, we use D ISCO F USE (Geva et al., 2019), which is a large-scale dataset for sentence fusion. makes the model less prone to common neural model errors such as hallucination, which allows us to control the semantic accuracy to a great extent using only simple heuristics and language model rescoring. 3.3 Experiments Decoding Algorithm The input of the algorithm (Figure 1) is a set of n ordered triples. First, we lexicalize the triple t0 to get the base text X0 . We choose the lexicalization for the triple as the filled template with the best score from LMS CORER. This promotes templates which sound more natural for particular values."
2020.inlg-1.9,N19-1423,0,0.0260552,"xt-planning stage and a neural generation stage. Other works improve the robustness of the neural model (Tian et al., 2019; Kedzie and McKeown, 2019) or employ a natural language understanding model (Nie et al., 2019) to improve the faithfulness of the output. Recently, Chen et al. (2020) finetuned GPT-2 (Radford et al., 2019) for a few-shot domain adaptation. Several models were recently applied to generic text editing tasks. L ASERTAGGER (Malmi et al., 2019), which we use in our approach, is a sequence tagging model based on the Transformer (Vaswani et al., 2017) architecture with the BERT (Devlin et al., 2019) pre-trained language model as the encoder. Other recent text-editing models without a pre-trained backbone include EditNTS (Dong et al., 2019) and Levenshtein Transformer (Gu et al., 2019). Concurrently with our work, Kale and Rastogi (2020) explored using templates for dialogue response generation. They use the sequence-tosequence T5 model (Raffel et al., 2019) to generate the output text from scratch instead of iteratively editing the intermediate outputs, which leaves less control over the model. 3 3.1 Template Extraction We collect a set of templates for each predicate. The templates can"
2020.inlg-1.9,2020.coling-main.218,0,0.134517,"Missing"
2020.inlg-1.9,N19-1236,0,0.193566,"ataset for sentence fusion. 1 Introduction Data-to-text (D2T) generation is the task of transforming structured data into a natural language text which represents it (Reiter and Dale, 2000; Gatt and Krahmer, 2018). The output text can be generated in several steps following a pipeline, or in an end-to-end (E2E) fashion. Neural-based E2E architectures recently gained attention due to their potential to reduce the human input needed for building D2T systems. A disadvantage of E2E architectures is the lack of intermediate steps, which makes it hard to control the semantic fidelity of the output (Moryossef et al., 2019b; Castro Ferreira et al., 2019). We focus on a D2T setup where the input data is a set of RDF triples in the form of (subject, predicate, object) and the output text represents all and only facts in the data. This setup can be used by all 1) We show how to reframe D2T generation as iterative text editing, which makes it independent of dataset-specific input data format and allows to control the output over a series of intermediate steps. 2) We perform initial experiments using our approach on two major D2T datasets (WebNLG and Cleaned E2E) and include a quantitative and qualitative analysis o"
2020.inlg-1.9,N18-1014,0,0.0172094,"? 0.9 0.4 ... ... ? ? Beam Filtering + LMScorer Figure 1: An example of a single iteration of our algorithm for D2T generation. In Step 1, the template for the triple is selected and filled. In Step 2, the sentence is fused with the template. In Step 3, the result for the next iteration is selected from the beam by filtering and language model scoring. 2 Background description of the decoding algorithm. Improving the accuracy of neural D2T approaches has attracted a lot of research interest lately. Similarly to us, other systems use a generatethen-rerank approach (Dušek and Jurˇcíˇcek, 2016; Juraska et al., 2018) or a classifier to filter incorrect output (Harkous et al., 2020). Moryossef et al. (2019a,b) split the D2T process into a symbolic text-planning stage and a neural generation stage. Other works improve the robustness of the neural model (Tian et al., 2019; Kedzie and McKeown, 2019) or employ a natural language understanding model (Nie et al., 2019) to improve the faithfulness of the output. Recently, Chen et al. (2020) finetuned GPT-2 (Radford et al., 2019) for a few-shot domain adaptation. Several models were recently applied to generic text editing tasks. L ASERTAGGER (Malmi et al., 2019),"
2020.inlg-1.9,P19-1256,0,0.0330235,"2 Background description of the decoding algorithm. Improving the accuracy of neural D2T approaches has attracted a lot of research interest lately. Similarly to us, other systems use a generatethen-rerank approach (Dušek and Jurˇcíˇcek, 2016; Juraska et al., 2018) or a classifier to filter incorrect output (Harkous et al., 2020). Moryossef et al. (2019a,b) split the D2T process into a symbolic text-planning stage and a neural generation stage. Other works improve the robustness of the neural model (Tian et al., 2019; Kedzie and McKeown, 2019) or employ a natural language understanding model (Nie et al., 2019) to improve the faithfulness of the output. Recently, Chen et al. (2020) finetuned GPT-2 (Radford et al., 2019) for a few-shot domain adaptation. Several models were recently applied to generic text editing tasks. L ASERTAGGER (Malmi et al., 2019), which we use in our approach, is a sequence tagging model based on the Transformer (Vaswani et al., 2017) architecture with the BERT (Devlin et al., 2019) pre-trained language model as the encoder. Other recent text-editing models without a pre-trained backbone include EditNTS (Dong et al., 2019) and Levenshtein Transformer (Gu et al., 2019). Concur"
2020.inlg-1.9,2020.inlg-1.14,0,0.0149761,"r 62 WebNLG baseline zero-shot w/fusion SFC T5 BLEU 0.277 0.288 0.353 0.524 0.571 NIST 6.328 6.677 7.923 - Cleaned E2E METEOR ROUGEL CIDEr 0.379 0.524 1.614 0.385 0.530 1.751 0.386 0.555 2.515 0.424 0.660 3.700 0.440 - BLEU 0.207 0.220 0.252 0.436 - NIST 3.679 3.941 4.460 - METEOR ROUGEL CIDEr 0.334 0.401 0.365 0.340 0.408 0.473 0.338 0.436 0.944 0.390 0.575 2.000 - Table 2: Results of automatic metrics on the WebNLG and Cleaned E2E test sets. The comparison is made with the results from the papers on the Semantic Fidelity Classifier (SFC; Harkous et al., 2020) and the finetuned T5 model (T5; Kale, 2020). with a beam of size 10. We use all reference lexicalizations and the vocabulary size V “ 100, following our preliminary experiments, which showed that filtering the references only by limiting the vocabulary size brings the best results (see Supplementary for details). We finetune the model for 10,000 updates with batch size 32 and learning rate 2ˆ10´5 . For the beam filtering heuristic, we check for the presence of entities by simple string matching in WebNLG; for the E2E dataset, we use a set of regular expressions from TGen5 (Dušek et al., 2019). We do not use any pre-ordering steps for t"
2020.inlg-1.9,W17-5525,1,0.858545,"ability in the data (manually filtering out the templates with semantic noise), and we also create a small set of templates for each single predicate manually, using them in the subsequent steps of the algorithm (this is possible due to the low variability of the predicates in the dataset).4 See Table 1 for examples of templates we used in our experiments. ¸1 P pxi |x1 . . . xi´1 q Datasets The WebNLG dataset (Gardent et al., 2017) consists of sets of DBPedia RDF triples and their lexicalizations. Following previous work, we use version 1.4 from Castro Ferreira et al. (2018). The E2E dataset (Novikova et al., 2017) contains restaurant descriptions based on sets of attributes (slots). In this work, we refer to the cleaned version of the E2E dataset (Dušek et al., 2019). For the domain adaptation experiments, we use D ISCO F USE (Geva et al., 2019), which is a large-scale dataset for sentence fusion. makes the model less prone to common neural model errors such as hallucination, which allows us to control the semantic accuracy to a great extent using only simple heuristics and language model rescoring. 3.3 Experiments Decoding Algorithm The input of the algorithm (Figure 1) is a set of n ordered triples."
2020.inlg-1.9,W19-8672,0,0.0918279,"teration is selected from the beam by filtering and language model scoring. 2 Background description of the decoding algorithm. Improving the accuracy of neural D2T approaches has attracted a lot of research interest lately. Similarly to us, other systems use a generatethen-rerank approach (Dušek and Jurˇcíˇcek, 2016; Juraska et al., 2018) or a classifier to filter incorrect output (Harkous et al., 2020). Moryossef et al. (2019a,b) split the D2T process into a symbolic text-planning stage and a neural generation stage. Other works improve the robustness of the neural model (Tian et al., 2019; Kedzie and McKeown, 2019) or employ a natural language understanding model (Nie et al., 2019) to improve the faithfulness of the output. Recently, Chen et al. (2020) finetuned GPT-2 (Radford et al., 2019) for a few-shot domain adaptation. Several models were recently applied to generic text editing tasks. L ASERTAGGER (Malmi et al., 2019), which we use in our approach, is a sequence tagging model based on the Transformer (Vaswani et al., 2017) architecture with the BERT (Devlin et al., 2019) pre-trained language model as the encoder. Other recent text-editing models without a pre-trained backbone include EditNTS (Dong"
2020.inlg-1.9,P02-1040,0,0.107371,"the E2E dataset. In cases where the set of templates is more diverse, e.g. if the template for the predicate country has to be selected from {<subject> is situated within <object>, <subject> is a dish found in <object>}, LMS CORER helps to select the semantically accurate template for the specific entities. The literal copying of entities can be too ˙ rigid in some cases, e.g. Atatürk Monument (Izmir) is made of “Bronze”, but these disfluencies can be improved in the fusion step. Analysis of Results We compute the metrics used in the evaluation of the E2E Challenge (Dušek et al., 2020): BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015). The results are shown in Table 2. The scores from the automatic metrics lag behind the state-of-theart, although both the fusion and the zero-shot approaches show improvements over the baseline. We examine the details in the following paragraphs, discussing the behavior of our approach, and we outline plans for improving the results in Section 6. Reordering L ASERTAGGER does not allow arbitrary reordering of words in the sentence, which can limit the expressiveness of the sentenc"
2020.inlg-1.9,2020.emnlp-main.338,0,0.0533785,"Missing"
2020.inlg-1.9,P19-1209,0,0.0176417,"ckling the main drawbacks and improving the results of the model with further research. Building a high-quality sentence fusion model, which lies at the core of our approach, remains a challenge (Lebanoff et al., 2020). Our simple extractive approach relying on existing D2T datasets may not produce sufficient amount of clean data. On the other hand, the phenomena covered in the D ISCO F USE dataset are too narrow for the fully general sentence fusion. We believe that training the sentence fusion model on a larger and more diverse sentence fusion dataset, built e.g. in an unsupervised fashion (Lebanoff et al., 2019), is a way to improve the robustness of our approach. Fluency of the output sentences may be also improved by allowing more flexibility for the order of entities, either by including an ordering step in the pipeline (Moryossef et al., 2019b), or by using a text-editing model that is capable of explicit reordering of words in the sentence (Mallinson et al., 2020). Splitting the data into smaller batches (i.e. setting an upper bound for the number of sentences fused together) could also help to improve the con7 Conclusions We proposed a simple and intuitive approach for D2T generation, splitting"
2020.inlg-1.9,W04-1013,0,0.0205894,"or the predicate country has to be selected from {<subject> is situated within <object>, <subject> is a dish found in <object>}, LMS CORER helps to select the semantically accurate template for the specific entities. The literal copying of entities can be too ˙ rigid in some cases, e.g. Atatürk Monument (Izmir) is made of “Bronze”, but these disfluencies can be improved in the fusion step. Analysis of Results We compute the metrics used in the evaluation of the E2E Challenge (Dušek et al., 2020): BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 2015). The results are shown in Table 2. The scores from the automatic metrics lag behind the state-of-theart, although both the fusion and the zero-shot approaches show improvements over the baseline. We examine the details in the following paragraphs, discussing the behavior of our approach, and we outline plans for improving the results in Section 6. Reordering L ASERTAGGER does not allow arbitrary reordering of words in the sentence, which can limit the expressiveness of the sentence fusion model. Consider the example in Figure 1: in order to create a sentence"
2020.inlg-1.9,2020.findings-emnlp.111,0,0.0197582,"na covered in the D ISCO F USE dataset are too narrow for the fully general sentence fusion. We believe that training the sentence fusion model on a larger and more diverse sentence fusion dataset, built e.g. in an unsupervised fashion (Lebanoff et al., 2019), is a way to improve the robustness of our approach. Fluency of the output sentences may be also improved by allowing more flexibility for the order of entities, either by including an ordering step in the pipeline (Moryossef et al., 2019b), or by using a text-editing model that is capable of explicit reordering of words in the sentence (Mallinson et al., 2020). Splitting the data into smaller batches (i.e. setting an upper bound for the number of sentences fused together) could also help to improve the con7 Conclusions We proposed a simple and intuitive approach for D2T generation, splitting the process into two steps: lexicalization of data and improving the text fluency. A trivial lexicalization helps to promote fidelity and domain independence while delegating the subtle work with language to neural models allows to benefit from the power of general-domain pre-training. While a straightforward application of this approach on the WebNLG and E2E d"
2020.inlg-1.9,D19-1510,0,0.079804,"uraska et al., 2018) or a classifier to filter incorrect output (Harkous et al., 2020). Moryossef et al. (2019a,b) split the D2T process into a symbolic text-planning stage and a neural generation stage. Other works improve the robustness of the neural model (Tian et al., 2019; Kedzie and McKeown, 2019) or employ a natural language understanding model (Nie et al., 2019) to improve the faithfulness of the output. Recently, Chen et al. (2020) finetuned GPT-2 (Radford et al., 2019) for a few-shot domain adaptation. Several models were recently applied to generic text editing tasks. L ASERTAGGER (Malmi et al., 2019), which we use in our approach, is a sequence tagging model based on the Transformer (Vaswani et al., 2017) architecture with the BERT (Devlin et al., 2019) pre-trained language model as the encoder. Other recent text-editing models without a pre-trained backbone include EditNTS (Dong et al., 2019) and Levenshtein Transformer (Gu et al., 2019). Concurrently with our work, Kale and Rastogi (2020) explored using templates for dialogue response generation. They use the sequence-tosequence T5 model (Raffel et al., 2019) to generate the output text from scratch instead of iteratively editing the in"
2020.inlg-1.9,W19-8645,0,0.238126,"ataset for sentence fusion. 1 Introduction Data-to-text (D2T) generation is the task of transforming structured data into a natural language text which represents it (Reiter and Dale, 2000; Gatt and Krahmer, 2018). The output text can be generated in several steps following a pipeline, or in an end-to-end (E2E) fashion. Neural-based E2E architectures recently gained attention due to their potential to reduce the human input needed for building D2T systems. A disadvantage of E2E architectures is the lack of intermediate steps, which makes it hard to control the semantic fidelity of the output (Moryossef et al., 2019b; Castro Ferreira et al., 2019). We focus on a D2T setup where the input data is a set of RDF triples in the form of (subject, predicate, object) and the output text represents all and only facts in the data. This setup can be used by all 1) We show how to reframe D2T generation as iterative text editing, which makes it independent of dataset-specific input data format and allows to control the output over a series of intermediate steps. 2) We perform initial experiments using our approach on two major D2T datasets (WebNLG and Cleaned E2E) and include a quantitative and qualitative analysis o"
2020.ngt-1.18,W19-5301,0,0.0611047,"Missing"
2020.ngt-1.18,2020.acl-main.747,0,0.0629024,"Missing"
2020.ngt-1.18,W19-6721,0,0.0378278,"Missing"
2020.ngt-1.18,P18-4020,0,0.0349767,"Missing"
2020.ngt-1.18,D18-2012,0,0.0177038,"s the weighted macro F1 score, computed by exact match with respect to the set of all valid translations for a given source sentence. The weighted F1 is a compound of unweighted precision and weighted recall, where the weight is determined based on each translation’s probability (see Section 2.1). 5.2 Translation Model We train the translation model using the Marian toolkit4 (Junczys-Dowmunt et al., 2018). We use the Transformer Base hyperparameters, i.e., model dimension 512, feed-forward layer dimension 2048, 8 attention heads with a head dimension of 64. All models use SentencePiece-based (Kudo and Richardson, 2018) vocabulary of 32k units. Japanese was tokenized using UDPipe (Straka and Strakov´a, 2017), other languages were processed with SentencePiece without tokenization. The model is trained using the Adam optimizer (Kingma and Ba, 2015), with Noam learning schedule (Vaswani et al., 2017) with 8,000 warmup steps and initial learning rate 3 · 10−4 , dropout rate 0.1, label smoothing 0.1 and gradient clipping at 5.0. We set the training batch size to 4,096 tokens. 5.3 Paraphrasing Model We base our paraphrasing model on the Levenshtein Transformer as implemented in fairseq5 (Ott 156 4 https://marian-n"
2020.ngt-1.18,2020.ngt-1.28,0,0.0558032,"Missing"
2020.ngt-1.18,N19-4009,0,0.0578378,"Missing"
2020.ngt-1.18,goldhahn-etal-2012-building,0,0.0304587,"Missing"
2020.ngt-1.18,P16-1009,0,0.11503,"Missing"
2020.ngt-1.18,K17-3009,0,0.0245734,"Missing"
2020.ngt-1.18,tiedemann-2012-parallel,0,0.121846,"Missing"
2020.webnlg-1.20,2020.webnlg-1.7,0,0.329984,"Missing"
2020.webnlg-1.20,D18-1024,0,0.0283115,"approaches for text generation has evolved since the first edition of the WebNLG challenge. Self-supervised pre-training objectives— such as language modelling and text denoising— have proven efficient for training neural models with excellent surface realization capabilities (Devlin et al., 2019; Lewis et al., 2020). Pre-training is used to improve the performance of models on downstream tasks, requiring only a small amount of task-specific data (Chen et al., 2020). Pre-trained models can exploit shared representations across languages, following the success of multilingual word embeddings (Chen and Cardie, 2018; Lample and Conneau, 2019). Although multilingual pre-training (i.e., pre-training on a collection of corpora from multiple languages) may slightly hurt performance for high-resource languages, it allows using the models for crosslingual tasks (Liu et al., 2020; Conneau et al., 2020). Neural architectures for text generation also gave rise to end-to-end approaches, where inputs and outputs are linearized and the task is solved by a single neural sequence-to-sequence model. Despite its disproportionate simplicity, this approach can be hard to beat using task-specific, modular approaches (Dušek"
2020.webnlg-1.20,2020.acl-main.18,0,0.0387901,"d for Russian on automatic metrics, and it made it into the best or second-best system cluster on human evaluation. 1 Introduction The landscape of approaches for text generation has evolved since the first edition of the WebNLG challenge. Self-supervised pre-training objectives— such as language modelling and text denoising— have proven efficient for training neural models with excellent surface realization capabilities (Devlin et al., 2019; Lewis et al., 2020). Pre-training is used to improve the performance of models on downstream tasks, requiring only a small amount of task-specific data (Chen et al., 2020). Pre-trained models can exploit shared representations across languages, following the success of multilingual word embeddings (Chen and Cardie, 2018; Lample and Conneau, 2019). Although multilingual pre-training (i.e., pre-training on a collection of corpora from multiple languages) may slightly hurt performance for high-resource languages, it allows using the models for crosslingual tasks (Liu et al., 2020; Conneau et al., 2020). Neural architectures for text generation also gave rise to end-to-end approaches, where inputs and outputs are linearized and the task is solved by a single neural"
2020.webnlg-1.20,2020.acl-main.747,0,0.0428745,"et al., 2019; Lewis et al., 2020). Pre-training is used to improve the performance of models on downstream tasks, requiring only a small amount of task-specific data (Chen et al., 2020). Pre-trained models can exploit shared representations across languages, following the success of multilingual word embeddings (Chen and Cardie, 2018; Lample and Conneau, 2019). Although multilingual pre-training (i.e., pre-training on a collection of corpora from multiple languages) may slightly hurt performance for high-resource languages, it allows using the models for crosslingual tasks (Liu et al., 2020; Conneau et al., 2020). Neural architectures for text generation also gave rise to end-to-end approaches, where inputs and outputs are linearized and the task is solved by a single neural sequence-to-sequence model. Despite its disproportionate simplicity, this approach can be hard to beat using task-specific, modular approaches (Dušek et al., 2020). In our submission, we took advantage of recent advances in pre-trained denoising autoencoders, multilingual representations, and sequenceto-sequence approaches. They enabled us to approach RDF-to-text generation both in English and Russian with a simple, identical, end"
2020.webnlg-1.20,N19-1423,0,0.0276146,"setup for both English and Russian. Requiring minimal task- or languagespecific effort, our model placed in the first third of the leaderboard for English and first or second for Russian on automatic metrics, and it made it into the best or second-best system cluster on human evaluation. 1 Introduction The landscape of approaches for text generation has evolved since the first edition of the WebNLG challenge. Self-supervised pre-training objectives— such as language modelling and text denoising— have proven efficient for training neural models with excellent surface realization capabilities (Devlin et al., 2019; Lewis et al., 2020). Pre-training is used to improve the performance of models on downstream tasks, requiring only a small amount of task-specific data (Chen et al., 2020). Pre-trained models can exploit shared representations across languages, following the success of multilingual word embeddings (Chen and Cardie, 2018; Lample and Conneau, 2019). Although multilingual pre-training (i.e., pre-training on a collection of corpora from multiple languages) may slightly hurt performance for high-resource languages, it allows using the models for crosslingual tasks (Liu et al., 2020; Conneau et al"
2020.webnlg-1.20,D18-1426,0,0.0171213,"960 0.943 (9) (31) 0.58 0.41 (14) (31) Unseen Cat. Ours Baseline 42.24 37.56 (10) (12) 0.375 0.357 (13) (15) 0.617 0.584 (10) (15) 0.46 0.51 (7) (13) 0.943 0.940 (11) (12) 0.52 0.44 (10) (12) Unseen Ent. Ours Baseline 51.23 40.22 (4) (17) 0.406 0.384 (8) (15) 0.687 0.648 (7) (15) 0.417 0.476 (9) (14) 0.959 0.949 (8) (13) 0.63 0.55 (8) (12) Table 2: Results of our approach on English (all data, seen categories, unseen categories, unseen entities), compared to the baseline. The numbers in brackets show the rank of each model (out of 35 submissions) with respect to the given metric. Similarly to Freitag and Roy (2018), we observe that in English, linearized triples can be seen as a noisy version of the output text, where: • subjects and objects are copied verbatim, • predicates are shortened or reworded, • function words are deleted, • order of the entities is shuffled. mBART’s pretraining objective is different from this, but we hypothesize that it is similar enough to be relevant for our task. For denoising Russian, our intuition stems from mBART’s successful application in machine translation (Liu et al., 2020). We finetune the pre-trained mbart.CC254 model from the FAIRSEQ toolkit (Ott et al., 2019). W"
2020.webnlg-1.20,P17-1017,0,0.0729938,"putational Linguistics Attribution 4.0 International. mBART Arrabbiata sauce |country |Italy ? Italy |capital |Rome finetuned on English WebNLG Arrabbiata sauce |country |Italy ? Italy |capital |Rome finetuned on Russian WebNLG mBART Arrabbiata sauce is found in Italy where the capital city is Rome. ???? ????????? ????? ?? ? ?????, ??? ??????? - ???. Figure 1: Our setup is simple: after tokenizing and linearizing the RDF triples, we finetune two separate mBART models for English and Russian using provided training data. We submit the unprocessed output from each model. The original challenge (Gardent et al., 2017a,b) included 10 categories in the training data: Airport, Astronaut, Building, City, ComicsCharacter, Food, Monument, SportsTeam, University, and WrittenWork. Each set of triples included several verbalizations to promote lexical variability. WebNLG 2020 includes several extensions: (1) It is bilingual: in addition to original English data, a new portion of the dataset with Russian lexicalizations is provided, giving rise to a new task of generating text in Russian. (2) It is bidirectional: in addition to RDF-to-text generation, the challenge also includes a task on text-to-RDF semantic parsi"
2020.webnlg-1.20,W17-3518,0,0.127036,"putational Linguistics Attribution 4.0 International. mBART Arrabbiata sauce |country |Italy ? Italy |capital |Rome finetuned on English WebNLG Arrabbiata sauce |country |Italy ? Italy |capital |Rome finetuned on Russian WebNLG mBART Arrabbiata sauce is found in Italy where the capital city is Rome. ???? ????????? ????? ?? ? ?????, ??? ??????? - ???. Figure 1: Our setup is simple: after tokenizing and linearizing the RDF triples, we finetune two separate mBART models for English and Russian using provided training data. We submit the unprocessed output from each model. The original challenge (Gardent et al., 2017a,b) included 10 categories in the training data: Airport, Astronaut, Building, City, ComicsCharacter, Food, Monument, SportsTeam, University, and WrittenWork. Each set of triples included several verbalizations to promote lexical variability. WebNLG 2020 includes several extensions: (1) It is bilingual: in addition to original English data, a new portion of the dataset with Russian lexicalizations is provided, giving rise to a new task of generating text in Russian. (2) It is bidirectional: in addition to RDF-to-text generation, the challenge also includes a task on text-to-RDF semantic parsi"
2020.webnlg-1.20,D18-2012,0,0.078199,"Missing"
2020.webnlg-1.20,W07-0734,0,0.165659,"denoising Russian, our intuition stems from mBART’s successful application in machine translation (Liu et al., 2020). We finetune the pre-trained mbart.CC254 model from the FAIRSEQ toolkit (Ott et al., 2019). We follow the example instructions for finetuning the model, changing only the total_updates to 10,000 to reflect the smaller size of our data. We show the capabilities of our model in Table 1. 5 Results We report on WebNLG automatic and human evaluation results, as well as our own error analysis. 4 5.1 Automatic metrics used in the challenge include BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ChrF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERTScore (Zhang et al., 2020), and BLEURT (only used for English; Sellam et al., 2020). The results of our approach for English are shown in Table 2, comparing to the baseline.5 We can see that our approach comfortably beats the baseline in all metrics and places in the first third of the submissions. While it does lose performance on unseen categories, the drop is not as dramatic as for many other competing approaches; our system is able to hold or improve its rank in the results table. Compare the baseline’s ranking for seen categories,"
2020.webnlg-1.20,2020.acl-main.703,0,0.279357,"sh and Russian. Requiring minimal task- or languagespecific effort, our model placed in the first third of the leaderboard for English and first or second for Russian on automatic metrics, and it made it into the best or second-best system cluster on human evaluation. 1 Introduction The landscape of approaches for text generation has evolved since the first edition of the WebNLG challenge. Self-supervised pre-training objectives— such as language modelling and text denoising— have proven efficient for training neural models with excellent surface realization capabilities (Devlin et al., 2019; Lewis et al., 2020). Pre-training is used to improve the performance of models on downstream tasks, requiring only a small amount of task-specific data (Chen et al., 2020). Pre-trained models can exploit shared representations across languages, following the success of multilingual word embeddings (Chen and Cardie, 2018; Lample and Conneau, 2019). Although multilingual pre-training (i.e., pre-training on a collection of corpora from multiple languages) may slightly hurt performance for high-resource languages, it allows using the models for crosslingual tasks (Liu et al., 2020; Conneau et al., 2020). Neural arch"
2020.webnlg-1.20,2020.tacl-1.47,0,0.153522,"pabilities (Devlin et al., 2019; Lewis et al., 2020). Pre-training is used to improve the performance of models on downstream tasks, requiring only a small amount of task-specific data (Chen et al., 2020). Pre-trained models can exploit shared representations across languages, following the success of multilingual word embeddings (Chen and Cardie, 2018; Lample and Conneau, 2019). Although multilingual pre-training (i.e., pre-training on a collection of corpora from multiple languages) may slightly hurt performance for high-resource languages, it allows using the models for crosslingual tasks (Liu et al., 2020; Conneau et al., 2020). Neural architectures for text generation also gave rise to end-to-end approaches, where inputs and outputs are linearized and the task is solved by a single neural sequence-to-sequence model. Despite its disproportionate simplicity, this approach can be hard to beat using task-specific, modular approaches (Dušek et al., 2020). In our submission, we took advantage of recent advances in pre-trained denoising autoencoders, multilingual representations, and sequenceto-sequence approaches. They enabled us to approach RDF-to-text generation both in English and Russian with a"
2020.webnlg-1.20,N19-4009,0,0.0276044,"eitag and Roy (2018), we observe that in English, linearized triples can be seen as a noisy version of the output text, where: • subjects and objects are copied verbatim, • predicates are shortened or reworded, • function words are deleted, • order of the entities is shuffled. mBART’s pretraining objective is different from this, but we hypothesize that it is similar enough to be relevant for our task. For denoising Russian, our intuition stems from mBART’s successful application in machine translation (Liu et al., 2020). We finetune the pre-trained mbart.CC254 model from the FAIRSEQ toolkit (Ott et al., 2019). We follow the example instructions for finetuning the model, changing only the total_updates to 10,000 to reflect the smaller size of our data. We show the capabilities of our model in Table 1. 5 Results We report on WebNLG automatic and human evaluation results, as well as our own error analysis. 4 5.1 Automatic metrics used in the challenge include BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ChrF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERTScore (Zhang et al., 2020), and BLEURT (only used for English; Sellam et al., 2020). The results of our approach for English"
2020.webnlg-1.20,P02-1040,0,0.116854,"o be relevant for our task. For denoising Russian, our intuition stems from mBART’s successful application in machine translation (Liu et al., 2020). We finetune the pre-trained mbart.CC254 model from the FAIRSEQ toolkit (Ott et al., 2019). We follow the example instructions for finetuning the model, changing only the total_updates to 10,000 to reflect the smaller size of our data. We show the capabilities of our model in Table 1. 5 Results We report on WebNLG automatic and human evaluation results, as well as our own error analysis. 4 5.1 Automatic metrics used in the challenge include BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ChrF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERTScore (Zhang et al., 2020), and BLEURT (only used for English; Sellam et al., 2020). The results of our approach for English are shown in Table 2, comparing to the baseline.5 We can see that our approach comfortably beats the baseline in all metrics and places in the first third of the submissions. While it does lose performance on unseen categories, the drop is not as dramatic as for many other competing approaches; our system is able to hold or improve its rank in the results table. Compare the baseli"
2020.webnlg-1.20,W17-4770,0,0.101117,"Missing"
2020.webnlg-1.20,2020.acl-main.704,0,0.0243412,"mbart.CC254 model from the FAIRSEQ toolkit (Ott et al., 2019). We follow the example instructions for finetuning the model, changing only the total_updates to 10,000 to reflect the smaller size of our data. We show the capabilities of our model in Table 1. 5 Results We report on WebNLG automatic and human evaluation results, as well as our own error analysis. 4 5.1 Automatic metrics used in the challenge include BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ChrF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERTScore (Zhang et al., 2020), and BLEURT (only used for English; Sellam et al., 2020). The results of our approach for English are shown in Table 2, comparing to the baseline.5 We can see that our approach comfortably beats the baseline in all metrics and places in the first third of the submissions. While it does lose performance on unseen categories, the drop is not as dramatic as for many other competing approaches; our system is able to hold or improve its rank in the results table. Compare the baseline’s ranking for seen categories, where it placed near the bottom of the list, and the ranking for unseen categories, where it scores in the first half – this shows that many"
2020.webnlg-1.20,2006.amta-papers.25,0,0.274899,"essful application in machine translation (Liu et al., 2020). We finetune the pre-trained mbart.CC254 model from the FAIRSEQ toolkit (Ott et al., 2019). We follow the example instructions for finetuning the model, changing only the total_updates to 10,000 to reflect the smaller size of our data. We show the capabilities of our model in Table 1. 5 Results We report on WebNLG automatic and human evaluation results, as well as our own error analysis. 4 5.1 Automatic metrics used in the challenge include BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ChrF++ (Popovi´c, 2017), TER (Snover et al., 2006), BERTScore (Zhang et al., 2020), and BLEURT (only used for English; Sellam et al., 2020). The results of our approach for English are shown in Table 2, comparing to the baseline.5 We can see that our approach comfortably beats the baseline in all metrics and places in the first third of the submissions. While it does lose performance on unseen categories, the drop is not as dramatic as for many other competing approaches; our system is able to hold or improve its rank in the results table. Compare the baseline’s ranking for seen categories, where it placed near the bottom of the list, and the"
2020.webnlg-1.20,2020.lrec-1.494,0,0.0469868,"Missing"
2021.acl-long.113,D19-1052,0,0.0348151,"Missing"
2021.acl-long.113,P17-1017,0,0.355911,"eedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1419–1434 August 1–6, 2021. ©2021 Association for Computational Linguistics target text, using a separate inference algorithm based on dynamic programming. Crucially, this enables us to directly evaluate and inspect the model’s planning and alignment performance by comparing to manually aligned reference texts. We demonstrate this for two data-to-text generation tasks: the E2E NLG (Novikova et al., 2017) and the WebNLG Challenge (Gardent et al., 2017a). We work with a triple-based semantic representation where a triple consists of a subject, a predicate and an object.2 For instance, in the last triple in Figure 1, Apollo 8, operator and NASA are the subject, predicate and object respectively. Our contributions are as follows: • We present a novel interpretable architecture for jointly learning to plan and generate based on modelling ordering and aggregation by aligning facts in the target text to input representations with an HMM and Transformer encoder-decoder. • We show that our method generates output with higher factual correctness th"
2021.acl-long.113,W19-8652,1,0.900529,"Missing"
2021.acl-long.113,P16-2008,1,0.894371,"Missing"
2021.acl-long.113,N19-1236,0,0.0608247,"s work, we combine advances of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Assoc"
2021.acl-long.113,D13-1157,1,0.758216,") by learning an implicit mapping between input representations (e.g. RDF triples) and target texts. While this can lead to increased fluency, E2E methods often produce repetitions, hallucination and/or omission of important content for data-to-text (Duˇsek et al., 2020) as well as other natural language generation (NLG) tasks (Cao et al., 2018; Rohrbach et al., 2018). Traditional NLG systems, on the other hand, tightly control which content gets generated, as well as its ordering and aggregation. This process is called sentence planning (Reiter and Dale, 2000; Duboue and McKeown, 2001, 2002; Konstas and Lapata, 2013; Gatt and Krahmer, 2018). Figure 1 shows two different ways to arrange and combine the representations in the input, resulting in widely different generated target texts. In this work, we combine advances of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input"
2021.acl-long.113,P19-1256,0,0.0180668,"r inspecting sentence planning with a rigorous human evaluation procedure to assess factual correctness in terms of alignment, aggregation and ordering performance. 2 Related Work Factual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input"
2021.acl-long.113,W17-5525,1,0.934067,"Missing"
2021.acl-long.113,P02-1040,0,0.109115,"datato-text tasks: the E2E NLG (Novikova et al., 2017) and WebNLG7 (Gardent et al., 2017a). Compared to E2E, WebNLG is smaller, but contains more predicates and has a larger vocabulary. Statistics with examples can be found in Appendix C. We followed the original training-development-test data split for both datasets. 4.2 Evaluation Metrics Generation Evaluation focuses on evaluating the generated text with respect to its similarity to human-authored reference sentences. To compare to previous work, we adopt their associated metrics to evaluate each task. The E2E task is evaluated using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015). WebNLG is evaluated in terms of BLEU, METEOR, and TER (Snover et al., 2006). Factual Correctness Evaluation tests if the generated text corresponds to the input triples (Wen et al., 2015b; Reed et al., 2018; Duˇsek et al., 2020). We evaluated on the E2E test set using automatic slot error rate (SER),8 i.e., an estimation of the occurrence of the input attributes (predicates) and their values in the outputs, implemented by Duˇsek et al. 7 Since we propose exploring sentence plann"
2021.acl-long.113,W19-8669,0,0.0146065,"us human evaluation procedure to assess factual correctness in terms of alignment, aggregation and ordering performance. 2 Related Work Factual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input semantic representations can be converted into"
2021.acl-long.113,W19-8611,0,0.0177222,"ual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input semantic representations can be converted into triples, see Section 4.1. Castro Ferreira et al. (2019) feature a pipeline with multiple planning stages and Elder et al. (2019) introduc"
2021.acl-long.113,2020.acl-main.641,0,0.220624,"retable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1419–1434 August 1–6, 2021. ©2021 Association for Computational Linguistics target text, using a separate inference algorithm based on dynamic programming. Cru"
2021.acl-long.113,D18-1356,0,0.341796,"hitectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pag"
2021.acl-long.113,2006.amta-papers.25,0,0.0186005,"cs with examples can be found in Appendix C. We followed the original training-development-test data split for both datasets. 4.2 Evaluation Metrics Generation Evaluation focuses on evaluating the generated text with respect to its similarity to human-authored reference sentences. To compare to previous work, we adopt their associated metrics to evaluate each task. The E2E task is evaluated using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015). WebNLG is evaluated in terms of BLEU, METEOR, and TER (Snover et al., 2006). Factual Correctness Evaluation tests if the generated text corresponds to the input triples (Wen et al., 2015b; Reed et al., 2018; Duˇsek et al., 2020). We evaluated on the E2E test set using automatic slot error rate (SER),8 i.e., an estimation of the occurrence of the input attributes (predicates) and their values in the outputs, implemented by Duˇsek et al. 7 Since we propose exploring sentence planning and increasing the controllability of the generation model and do not aim for a zero-shot setup, we only focus on the seen category in WebNLG. 8 SER is based on regular expression matching"
2021.acl-long.113,2020.acl-main.455,1,0.785707,"Missing"
2021.acl-long.113,N18-2010,0,0.0165941,"roximate the required planning annotation (entity mentions, their order and sentence splits). Zhao et al. (2020) use a planning stage in a graph-based model – the graph is first reordered into a plan; the decoder conditions on both the input graph encoder and the linearized plan. Similarly, Fan et al. (2019) use a pipeline approach for story generation via SRL-based sketches. However, all of these pipeline-based approaches either require additional manual annotation or depend on a parser for the intermediate steps. Other works, in contrast, learn planning and realisation jointly. For example, Su et al. (2018) introduce a hierarchical decoding model generating different parts of speech at different levels, while filling in slots between previously generated tokens. Puduppully et al. (2019) include a jointly trained content selection and ordering module that is applied before the main text generation step.The model is trained by maximizing the log-likelihood of the gold content plan and the gold output text. Li and Rush (2020) utilize posterior regularization in a structured variational framework to induce which input items are being described by each token of the generated text. Wiseman et al. (201"
2021.acl-long.113,2020.acl-main.224,0,0.440318,"es of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computatio"
2021.acl-long.189,Q17-1010,0,0.0118731,"al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain.9 As sources of weak supervision providing slot candidates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) – a union of labels provided by both parsers is used in all our setups. In addition, to explore combined sources on the namedentity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy.10 To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings. Slot merging and selection parameters were set heuristically in an initial trial run on the CamRest676 data and proved stable across domains. Slot tagger hyperparameters are chosen according to grid search on a portion of the training data, as described in Section 3.3.11 5.2 System Variants and Baselines We test multiple ablation variants of our method: • Ours-full is the full version of our method (full annotation setup and trained slot tagger). 8 MultiWOZ contains more domains such as restaurant, train search, bus search. However, we decided to not include the"
2021.acl-long.189,D18-1547,0,0.0276754,"Missing"
2021.acl-long.189,2020.acl-main.11,0,0.0329446,"Missing"
2021.acl-long.189,N10-1138,0,0.0531226,"is a multi-domain corpus; we picked two domains – hotel reservation and attraction recommendation – to form WOZ-hotel (WH) with 14,435 utterances, 9 slots, 3 intents and WOZ-attr (WA) with 7524 utterances, 8 slots and 3 intents respectively.8 • Cambridge SLU (Henderson et al., 2012) (CS) contains 10,569 utterances and tracks 5 slots with 5 intents in the restaurant domain. • ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain.9 As sources of weak supervision providing slot candidates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) – a union of labels provided by both parsers is used in all our setups. In addition, to explore combined sources on the namedentity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy.10 To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings. Slot merging and selection parameters were set heuristically in an initial trial run on the CamRest676 data and proved stable across domains. Slot tagger hyper"
2021.acl-long.189,N18-2118,0,0.0253841,"generalize and capture unseen values (cf. Figure 3). One source of errors is the relatively low recall of the frame-semantic parsers used. We successfully address this issue by introducing the slot tagger, however, many slot values remain untagged. This is expected as our method’s performance is inherently limited by the input linguistic annotation quality. Another type of errors is caused by the can14 Note that our measurements of slot F1 only consider the ‘O’ tag as negative (the average is computed over slots only). This results in lower numbers than those reported in literature (cf. e.g. Goo et al., 2018), but we believe that this reflects the actual performance more accurately. 15 We present results taken in unsupervised setting, i.e. when no ontology is available. However, since Jin et al. (2018) consider only slot values that are known from the ontology by default, we provide the extended results in Appendix A.2. 2436 method Jin et al. supervised Jin et al. unsupervised Jin et al. weak-labels Ours-full (unsupervised) Slot F1 0.967 ± .001 0.719 ± .002 0.709 ± .011 0.756 ± .004 Joint Goal Accuracy 0.897 ± .002 0.385 ± .003 0.335 ± .008 0.465 ± .007 Entity Match Rate 0.869 ± .004 0.019 ± .002"
2021.acl-long.189,H90-1021,0,0.0943409,"Missing"
2021.acl-long.189,W03-1028,0,0.353143,"identify domain-relevant slots based on the annotation labels by iteratively (a) merging and (b) ranking and selecting most viable candidates (Section 3.2). (3) we use the discovered slots to train an independent slot tagger (Section 3.3). 3.1 Figure 2 shows the overall data flow of our slot annotation pipeline. The data are first labeled with domain-generic linguistic annotation models, which we consider weak supervision. For our experiments, we use a frame semantic parser and NER, but other models, such as semantic role labeling (SRL; e.g., Palmer et al., 2010) or keyword extraction (e.g., Hulth, 2003) can be used in general. We use a simple union of labels provided by all annotation models.3 3.2 Our slot discovery method has three main stages: (1) We obtain weak supervision labels from autoDiscovering Slots: Merging and Ranking Subsequent steps identify domain-relevant slots based on candidates provided by the automatic annotation. The slot discovery process is iterative – in each iteration, it: (1) merges similar candidates, (2) ranks candidates’ relevance and eliminates irrelevant ones. Once no more frames are eliminated, the process stops and we obtain slot labels, which are used to tra"
2021.acl-long.189,N16-1030,0,0.0183214,"the original annotation models used as weak supervision are not adapted to our specific domain. Therefore, we use the obtained labels to train a new, domain-specific slot tagger to improve performance. The tagger has no access to better labels than those derived by our method; however, it has a simpler task, as the set of target labels is now much smaller and the domain is much narrower. We model the slot tagging task as sequence tagging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of respective tags (Lample et al., 2016).7 The output layer of the tagger network gives softmax probability distributions over possible tags. To further increase recall, we add an inference-time rule – if slot candidate is split – it is just ranked for relevance multiple times (with respect to multiple contexts). 6 Usefulness of the individual metrics is confirmed in an ablation study in Section 6. 7 https://github.com/deepmipt/ner 2433 the most probable predicted tag is ‘O’ (i.e., no slot) and the second most probable tag has a probability higher than a preset threshold ?tag , the second tag is chosen as a prediction instead. As we"
2021.acl-long.189,P18-1133,0,0.0670244,"explicit dialogue state modeling (e.g., Serban et al., 2016; Li et al., 2016; Gao et al., 2019). They aim at a non-task-oriented setting, where state interpretability or response controllability are less of a concern. Other works in task-oriented dialogues use transfer learning for adapting to low-resourced target domains (Zhao and Eskenazi, 2018; Shalyminov et al., 2019), but also keep the dialogue state representation latent. In contrast, Jin et al. (2018) propose to model the dialogue state explicitly, in a semi-supervised way. They extend the end-to-end encoder-decoder Sequicity model of Lei et al. (2018, cf. Section 4) by introducing an additional decoder that has access to posterior information about the system response. This allows them to train a state representation with a reconstruction loss on unsupervised examples, using the state as a limited memory for essential concepts (roughly corresponding to slots). Their method can be applied in fully unsupervised way, but it still requires some amount of in-domain annotations to achieve good performance. Our work aims at explicit dialogue state modeling without the need for any in-domain supervision. 3 matic domain-generic annotation. (2) We"
2021.acl-long.189,N16-1014,0,0.036663,"sian” relate to the same slot). input sentence types). Yang et al. (2014) use semisupervised intent clustering, with manual annotation to seed and interpret the clusters. Chen et al. (2016) introduced a model for zero-shot intent embedding prediction based on similarity to known intents. Shi et al. (2018) proposed a fully unsupervised intent detection model with the use of sentence clustering based on sentence-level features. Most applications of unsupervised or semisupervised methods to end-to-end dialogue response generation avoid explicit dialogue state modeling (e.g., Serban et al., 2016; Li et al., 2016; Gao et al., 2019). They aim at a non-task-oriented setting, where state interpretability or response controllability are less of a concern. Other works in task-oriented dialogues use transfer learning for adapting to low-resourced target domains (Zhao and Eskenazi, 2018; Shalyminov et al., 2019), but also keep the dialogue state representation latent. In contrast, Jin et al. (2018) propose to model the dialogue state explicitly, in a semi-supervised way. They extend the end-to-end encoder-decoder Sequicity model of Lei et al. (2018, cf. Section 4) by introducing an additional decoder that ha"
2021.acl-long.189,2020.acl-main.3,0,0.0165374,"t al. (2020) focus on a few-shot setting and perform span extraction of slot values using pretrained models. Their approach, however, still requires some expert annotation. Another direction of research focuses on zero-shot slot filling. Bapna et al. (2017)’s recurrent-neuralnetwork-based slot tagger is pretrained on multiple domains and takes a textual description of the target slot on the input in addition to the user utterance. This way, adapting to a new domain only involves providing new slot descriptions. Further works extend this idea with more complex architectures (Shah et al., 2019; Liu et al., 2020). Unsupervised and semi-supervised methods were also investigated for predicting intents (user 2431 User input 1: I would like an expensive restaurant that serves Afghan food. Original annotation: Expensiveness Locale slot-0 slot-1 Our annotation: User input 2: How about Asian oriental food. Original annotation: Origin Food Our annotation: slot-1 Figure 3: A sample of a dialogue from CamRest676 data, with labels from a frame-semantic parser (middle) and our slot tagger (bottom). Although “Afghan” food is not in the frame parser output, our tagger was able to recognize it. The change in value f"
2021.acl-long.189,W04-3252,0,0.0206242,"end up in multiple clusters. This does not mean that the respective Candidate Ranking criteria We use the following metrics to compute the ranking score:6 • Frequency frq(?) is used since candidates that occur frequently in the data are likely important. • Coherence coh(?) is the average pairwise similarity of all fillers’ embeddings: Õ ?cos (?(?), ?(?)) coh(?) = (? ,?) ∈??2 |??2 | (1) where ??2 is a set of all pairs of fillers for the slot candidate s. We follow Chen et al. (2014)’s assumption that fillers with high coherence, i.e., focused on one topic, are good slot candidates. • TextRank (Mihalcea and Tarau, 2004) is a keyword extraction algorithm. It constructs a graph where nodes represent words and edges represent their co-occurrence. The dominant eigenvector of the adjacency matrix of this graph then gives the individual words’ scores. We replace fillers with candidate labels when computing the score, so we obtain results related to slots rather than to particular values. The final score is a simple sum of rankings with respect to all three scores. 3.3 Slot Tagger Model Training Our method described in Section 3.2 can give us a good set of dialogue slots. However, using the merged and filtered slot"
2021.acl-long.189,P17-1163,0,0.0610602,"Missing"
2021.acl-long.189,P19-1547,0,0.0258758,"omatically. Coope et al. (2020) focus on a few-shot setting and perform span extraction of slot values using pretrained models. Their approach, however, still requires some expert annotation. Another direction of research focuses on zero-shot slot filling. Bapna et al. (2017)’s recurrent-neuralnetwork-based slot tagger is pretrained on multiple domains and takes a textual description of the target slot on the input in addition to the user utterance. This way, adapting to a new domain only involves providing new slot descriptions. Further works extend this idea with more complex architectures (Shah et al., 2019; Liu et al., 2020). Unsupervised and semi-supervised methods were also investigated for predicting intents (user 2431 User input 1: I would like an expensive restaurant that serves Afghan food. Original annotation: Expensiveness Locale slot-0 slot-1 Our annotation: User input 2: How about Asian oriental food. Original annotation: Origin Food Our annotation: slot-1 Figure 3: A sample of a dialogue from CamRest676 data, with labels from a frame-semantic parser (middle) and our slot tagger (bottom). Although “Afghan” food is not in the frame parser output, our tagger was able to recognize it. Th"
2021.acl-long.189,W19-5904,0,0.0222927,"Shi et al. (2018) proposed a fully unsupervised intent detection model with the use of sentence clustering based on sentence-level features. Most applications of unsupervised or semisupervised methods to end-to-end dialogue response generation avoid explicit dialogue state modeling (e.g., Serban et al., 2016; Li et al., 2016; Gao et al., 2019). They aim at a non-task-oriented setting, where state interpretability or response controllability are less of a concern. Other works in task-oriented dialogues use transfer learning for adapting to low-resourced target domains (Zhao and Eskenazi, 2018; Shalyminov et al., 2019), but also keep the dialogue state representation latent. In contrast, Jin et al. (2018) propose to model the dialogue state explicitly, in a semi-supervised way. They extend the end-to-end encoder-decoder Sequicity model of Lei et al. (2018, cf. Section 4) by introducing an additional decoder that has access to posterior information about the system response. This allows them to train a state representation with a reconstruction loss on unsupervised examples, using the state as a limited memory for essential concepts (roughly corresponding to slots). Their method can be applied in fully unsup"
2021.acl-long.189,D18-1072,0,0.0200804,"dle) and our slot tagger (bottom). Although “Afghan” food is not in the frame parser output, our tagger was able to recognize it. The change in value for slot-1 (corresponding to food type) is successfully captured in the second utterance. This shows that our model can categorize entities (both “Afghan” and “Asian” relate to the same slot). input sentence types). Yang et al. (2014) use semisupervised intent clustering, with manual annotation to seed and interpret the clusters. Chen et al. (2016) introduced a model for zero-shot intent embedding prediction based on similarity to known intents. Shi et al. (2018) proposed a fully unsupervised intent detection model with the use of sentence clustering based on sentence-level features. Most applications of unsupervised or semisupervised methods to end-to-end dialogue response generation avoid explicit dialogue state modeling (e.g., Serban et al., 2016; Li et al., 2016; Gao et al., 2019). They aim at a non-task-oriented setting, where state interpretability or response controllability are less of a concern. Other works in task-oriented dialogues use transfer learning for adapting to low-resourced target domains (Zhao and Eskenazi, 2018; Shalyminov et al."
2021.acl-long.189,E17-1042,0,0.0697728,"Missing"
2021.acl-long.189,W13-4065,0,0.0177137,"formance of an end-to-end dialogue response generation model, compared to using no slot annotation at all. 1 Introduction Task-oriented dialogue systems typically use annotation based on slots to represent the meaning of user utterances (Young et al., 2013). Slots are attributes relevant to completing the task (e.g., price, food type, area). The sets of slots and their values typically need to be designed in advance by domain experts. Slots and their values are tracked over the course of the dialogue, forming dialogue state, which allows a dialogue system to plan the next actions effectively (Williams et al., 2013). Getting raw data for dialogue system training is not difficult, especially if we restrict the target domain. A requirement for dialogue state labels makes this process much more costly. However, both traditional pipeline systems (Young et al., 2013) and end-to-end task-oriented architectures (Wen et al., 2017) typically require such annotation. While some systems use implicit, latent state representation and do not require annotation (Serban et al., 2016), the behavior of such systems is hard to interpret or control. There are several works aiming at keeping interpretability and reducing the"
2021.acl-long.189,W18-5001,0,0.13204,"omain. A requirement for dialogue state labels makes this process much more costly. However, both traditional pipeline systems (Young et al., 2013) and end-to-end task-oriented architectures (Wen et al., 2017) typically require such annotation. While some systems use implicit, latent state representation and do not require annotation (Serban et al., 2016), the behavior of such systems is hard to interpret or control. There are several works aiming at keeping interpretability and reducing the annotation needs by automating it (Chen et al., 2014, 2015) or transferring annotation across domains (Zhao and Eskenazi, 2018; Coope et al., 2020), but they still require significant manual effort. In this paper, we present a novel approach to discovering a set of domain-relevant dialogue slots and their values given a set of dialogues in the target domain (such as transcripts from a call center). Our approach requires no manual annotation at all in order to tag slots in dialogue data. This substantially simplifies dialogue system design and training process, as the developer no longer needs to design a set of slots and annotate their occurrences in training data. We discover slots by using unsupervised clustering o"
2021.findings-emnlp.133,N19-1423,0,0.00680822,"s”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary: SFweights = avgj=1···J wjf ∈ [−1, 1] (1) A high SFweights score indicates that the facts in the summaries are well supported by the facts mentioned in the documents. The top section in Table 3 shows SFweights scores reported on M I RA NEWS(S-D), M I RA NEWS(S-A) and M I RA NEWS(S-D&A), which weight facts in the summaries using facts in the main document, assisting documents, and both, respectively. As expected, SFweights on M I RA NEWS(S-D) is higher than on M I RA NEWS(SA), indicating that t"
2021.findings-emnlp.133,P19-1483,0,0.0592215,"Missing"
2021.findings-emnlp.133,P19-1102,0,0.0209277,"4. 7 Related Work Single Document Summarization aims to compress a single textual document while keeping salient information. SDS includes two directions: extractive summarization (Nallapati et al., 2017) which aims at extracting salient sentences from the input document, and abstractive summarization (See et al., 2017; Narayan et al., 2018a; Yang et al., 2019; Liu and Lapata, 2019b; Liu et al., 2020; Rothe et al., 2020; Raffel et al., 2020) which generates a novel short representation of the input. Multi-Document Summarization aims to compress multiple textual documents to a shorter summary (Fabbri et al., 2019). Approaches mainly focus on increasing the capacity of the encoder to process longer inputs (Liu and Lapata, 2019a; Beltagy et al., 2020; Zaheer et al., 2020; Zhang et al., 2020a; Huang et al., 2021), leveraging knowledge graphs (Fan et al., 2019; Li et al., 2020; Jin et al., 2020), and including content selection steps (Nayeem et al., 2018; Wang et al., 2020; Xu and Lapata, 2020; Grenander et al., 2019; Liu et al., 2018). tures, training and decoding, e.g. Cao et al. (2018); Zhang et al. (2020c); Falke et al. (2019); Zhao et al. (2020b). However, we are the first research aiming to reduce th"
2021.findings-emnlp.133,P19-1213,0,0.0501629,"Missing"
2021.findings-emnlp.133,D19-1428,0,0.0547221,"Missing"
2021.findings-emnlp.133,D19-1620,0,0.0414923,"Missing"
2021.findings-emnlp.133,N18-1065,0,0.0602853,"Missing"
2021.findings-emnlp.133,2021.naacl-main.112,0,0.0488549,"Missing"
2021.findings-emnlp.133,2020.acl-main.556,0,0.0528558,"Missing"
2021.findings-emnlp.133,2020.acl-main.703,0,0.0121967,"on), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This tas"
2021.findings-emnlp.133,2020.acl-main.555,0,0.0271442,"Missing"
2021.findings-emnlp.133,N03-1020,0,0.691106,"Missing"
2021.findings-emnlp.133,P19-1500,0,0.276332,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,J05-1004,0,0.199508,"support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary:"
2021.findings-emnlp.133,D19-1387,0,0.25318,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,2020.acl-main.173,1,0.88083,"Missing"
2021.findings-emnlp.133,D18-1206,1,0.844209,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,N18-1158,1,0.720606,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,C18-1102,0,0.0553052,"Missing"
2021.findings-emnlp.133,2020.emnlp-main.748,0,0.207296,"s at the end of the main document. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentenc"
2021.findings-emnlp.133,2020.tacl-1.18,1,0.89944,"h data divergence issues between the source and target texts (Dhingra et al., 2019) will function more as an open-ended language model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single"
2021.findings-emnlp.133,2020.emnlp-main.647,0,0.0184006,"vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This task is typically approached using document resulting in counterfactual output, and 1 extrinsic that introduce information not grounded Our code and data are available at: https://github.com/XinnuoXu/MiRANews in the document (see Figure 1). Extrinsic halluci1541 1 Introduction Findings of the Association for Computational Linguis"
2021.findings-emnlp.133,P17-1099,0,0.319088,"edian, who and husband matthew broderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. data"
2021.findings-emnlp.133,2020.emnlp-main.32,0,0.0429564,"Missing"
2021.findings-emnlp.133,2020.acl-main.455,1,0.740794,"han M I RA NEWS(SD). Introducing the assisting documents contributes new information to support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summ"
2021.findings-emnlp.133,2020.emnlp-main.296,0,0.0567443,"Missing"
2021.findings-emnlp.133,2020.acl-main.458,0,0.027433,"42.29 36.18 43.22 36.06 43.11 36.08 43.13 46.72 55.39 43.15 51.02 BertScore P R F1 .701 .674 .684 .701 .666 .679 .701 .677 .685 .685 .682 .680 .690 .682 .682 .684 .686 .681 .769 .745 .755 .716 .731 .721 Table 4: Evaluation on ROUGE and BertScore. 4.2 Evaluation Metrics We evaluate the approaches described in Section 4.1 from four perspectives: • Similarity to Reference focuses on evaluating the generated summary with respect to its similarity to a human-authored ground-truth reference summary. We adopt the exact-matching metric ROUGE (Lin and Hovy, 2003) and the softmatching metric BertScore (Zhang et al., 2020b). • Extractiveness level aims at the bias of each system towards generating extractive summaries. We introduce the n-grams coverage, which equals to 1 − n-gram novelty (see Section 3), to measure the percentage of n-grams in the generated summary that appear in the main and assisting documents. Higher n-gram coverage scores indicate that the system is more extractive. • Support from Assisting Documents measures the proportion of information appearing in the generated summary that originates from assisting documents only. We propose the n-grams coverage over n-grams in the generated summary w"
2021.findings-emnlp.133,2020.findings-emnlp.203,0,0.0149793,"t. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentence in both main and assisting"
2021.gem-1.10,2020.acl-main.424,0,0.0251404,"les WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling prog"
2021.gem-1.10,2020.acl-main.766,0,0.0426234,"sh datasets were ranked lower, with only WebNLG and MLSum among the top 15 datasets. We grouped all datasets by their high-level tasks and selected a group that would not violate the selection principles (e.g., only high-resource tasks). If two datasets fit, we picked the one with a higher interest rating. Among the 11 datasets, we have 18different languages, and the dataset sizes range from 5,000 examples to 1.5M, with most datasets between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining (Anastasopoulos and Neubig, 2020). The high-level tasks include Dialog, Summarization, Data-to-Text, and Simplification. About half of the datasets have multiple references and more than half had post-processing steps applied to them to ensure high data quality. 3.1 GEMifying the data We produce data cards (Bender and Friedman, 2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-specific template.7 In addition to describing the data itself, the cards acknowledge potential limitations of a dataset regarding its creation process and describe its real-world use cases to ensure that the research is c"
2021.gem-1.10,W05-0909,0,0.165721,"otebook within 2-3 hours. 4.2 avoid overfitting to known metrics, we will use metrics on the test submissions that are not included in this initial writeup. Consequentially, the baseline results are an incomplete list which will be expanded upon the announcement of the test metrics. The set of metrics can be computed via the framework described at https://gem-benchmark. com/shared_task which comprises metrics in the following categories: Lexical Similarity. We include multiple “traditional” metrics as baseline metrics, notably BLEU (Papineni et al., 2002), ROUGE-1/2/L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). These metrics can often be gamed, for example, ROUGE can be improved by increased the output length of the model (Sun et al., 2019). Moreover, the reliability of these metrics depends on the quality and number of the references (Mathur et al., 2020a; Freitag et al., 2020). However, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 10"
2021.gem-1.10,W11-2832,0,0.0915033,"Missing"
2021.gem-1.10,2020.inlg-1.24,1,0.864971,"human evaluation standards. In recent work, Howcroft et al. (2020) investigated NLG papers from the last 2 For a more complete description of recent developments in NLG evaluation, we refer to the survey by Celikyilmaz et al. (2020). 99 twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most cases, it is not even mentioned what the human evaluation aims to measure and that definitions of measures like “accuracy” or “fluency” are inconsistent. They thus suggest reporting standards for criteria and methods, following a classification system proposed by Belz et al. (2020). In addition, regularly scheduled shared tasks like WMT have lead to standardization of human evaluation setups and enabled controlled experimentation with them. GEM has the opportunity to develop reproducible standards for how human evaluation for NLG tasks beyond translation should be conducted while at the same time incorporating lessons from related work. Acting on the same need, the recently proposed GENIE (Khashabi et al., 2021) system aims to automate and standardize the human evaluation of different NLG systems, however with the contrasting goal of reducing the evaluating to a leaderb"
2021.gem-1.10,Q18-1041,0,0.236179,"han half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard ("
2021.gem-1.10,W17-4755,0,0.0383891,"Missing"
2021.gem-1.10,W16-2302,0,0.0489314,"Missing"
2021.gem-1.10,N18-2097,0,0.0498492,"Missing"
2021.gem-1.10,N19-1423,0,0.0210653,"provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation appr"
2021.gem-1.10,P19-1483,1,0.811795,"tions. 106 Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek"
2021.gem-1.10,K19-1037,0,0.027404,"Missing"
2021.gem-1.10,P17-1123,0,0.0287395,"Missing"
2021.gem-1.10,2020.acl-main.454,1,0.84177,"LEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen"
2021.gem-1.10,W19-8652,1,0.897973,"Missing"
2021.gem-1.10,C16-1191,0,0.0653297,"Missing"
2021.gem-1.10,P18-1082,0,0.0705277,"Missing"
2021.gem-1.10,W16-3622,1,0.897392,"Missing"
2021.gem-1.10,P16-2008,1,0.869379,"Missing"
2021.gem-1.10,W19-8670,1,0.884454,"Missing"
2021.gem-1.10,2020.emnlp-main.393,0,0.179031,"le to be able to address newly found limitations, and that the benchmark should focus on climbing a leaderboard. Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench,1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able mu"
2021.gem-1.10,N19-1395,0,0.0587235,"Missing"
2021.gem-1.10,2020.emnlp-main.751,0,0.503468,"nly on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover,"
2021.gem-1.10,P19-1346,1,0.897232,"Missing"
2021.gem-1.10,2020.webnlg-1.7,1,0.843551,"Missing"
2021.gem-1.10,2020.findings-emnlp.195,1,0.835947,"Missing"
2021.gem-1.10,2020.emnlp-main.5,0,0.126308,"e machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differently across tasks, setups, and languages, a multi-task NLG benchmark has the opportunity to act as a testbed to evaluate how the latest advances in automated metrics perform on these different tasks. The benchmark can facilitate this research through the release of system outputs and associated human annotations, which is what we are planning to do with GEM. Moreover, we allow the integrat"
2021.gem-1.10,2020.emnlp-main.489,0,0.0422168,"les 3 and 4. Our interactive system is centered around a parallel coordinates plot (Inselberg, 1985) which shows all results as lines through parallel axes. Every line intersects the axes at the corresponding mapped value. For instance, see the red line representing the results for task “ToTTo” of baseline “t5-small”. Filters can be applied along axes (see BLEURT axis in Figure 2) and the filtered selection is highlighted through bold lines. A selection can be a set of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The closest prior work is by Fu et al. (2020) for namedentity recognition which allows similar filtering and sorting, but presents the results in a table. However, the parallel coordinates approach can scale to a much greater number of metrics than a table. Moreover, by using a parallel coordinates plot instead of a table, it is easy to spot patterns that span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that, for the T5 baseline on ToTTo, the diversity metrics score higher than other systems while scoring lower on reference-based metrics. Since we only have a single baseline for ToTTo, it i"
2021.gem-1.10,W17-3518,1,0.863605,"Missing"
2021.gem-1.10,N19-1169,1,0.923485,"eiter and Dale, 2000). These texts aim to fulfill an underlying communicative goal (e.g., to produce a summary of an article) while remaining faithful to the input information, fluent, grammatical, and natural-looking. An NLG system needs to be robust to shifts in the data distribution and be able to produce text in many different languages. Finally, it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent. These optimization objectives can often be conflicting (Hashimoto et al., 2019) and, as a result, evaluations that focus only on a single aspect may fail to recognize the drawbacks of a particular method. To demonstrate this trade-off, consider an improvement on the CNN-DM summarization dataset (Hermann et al., 2015; Nallapati et al., 2016) measured by the ROUGE-L metWe introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent"
2021.gem-1.10,2020.ngt-1.1,0,0.0253511,"first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution"
2021.gem-1.10,2020.inlg-1.23,1,0.841029,"Missing"
2021.gem-1.10,D15-1229,0,0.0462438,"Missing"
2021.gem-1.10,2020.acl-main.709,1,0.887061,"Missing"
2021.gem-1.10,2020.acl-main.560,0,0.077305,"on does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b; Liu et al., 2020a; McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and Wiki"
2021.gem-1.10,2020.evalnlgeval-1.4,0,0.0163557,"ation tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These inclu"
2021.gem-1.10,D18-1208,0,0.0650198,"Missing"
2021.gem-1.10,Q18-1023,0,0.0642232,"Missing"
2021.gem-1.10,2020.findings-emnlp.360,1,0.934785,"e a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling progress. and conducting in-depth ana"
2021.gem-1.10,D16-1128,0,0.0680679,"Missing"
2021.gem-1.10,2020.acl-main.703,0,0.214186,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,2020.acl-main.653,0,0.233553,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,N16-1014,0,0.0410993,"t at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These include the Shannon Entropy (Shannon and Weaver, 1963) over unigrams and bigrams (H1 , H2 ), the mean segmented type token ratio over segment lengths of 100 (MSTTR, Johnson, 1944), the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that only appear once across the entire test output (Unique1,2 , Li et al., 2016). focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated with a judgment. As part of this, we collect the number of parameters of a system, as suggested by Ethayarajh and Jurafsky (2020). For each task, we additionally report the vocabulary size over the output (|V|) and the mean output length of a system (Sun et al., 2019). 5 Results One of the central aims of GEM is to measure the progress in NLG without misrepresenting the complex interactions between the sometimes contradicting measures. We t"
2021.gem-1.10,2020.findings-emnlp.165,0,0.0883829,"Missing"
2021.gem-1.10,W04-1013,0,0.355472,"n be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets"
2021.gem-1.10,2020.acl-main.465,0,0.123121,"; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress. As datasets, metrics, and models improve, the benchmark environment will improve as well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics, and addressing discovered flaws in the experimental setup, as demonstrated in Figure 1. Making all model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption and incre"
2021.gem-1.10,2020.tacl-1.47,0,0.129414,"ich has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able much richer evaluation (as described in the next sections), and promote non-English datasets. In addition, it can ensure that the datasets created for those shared tasks continue being evaluated. Increasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is t"
2021.gem-1.10,2021.ccl-1.108,0,0.0502233,"Missing"
2021.gem-1.10,W15-4640,0,0.0777152,"Missing"
2021.gem-1.10,W18-6450,0,0.0170006,"ever, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automa"
2021.gem-1.10,W19-5302,0,0.0535855,"Missing"
2021.gem-1.10,2020.coling-main.420,0,0.022745,"s many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured quantities (Howcroft et al., 2020). Improving Data Improving Metrics Evaluation with gameable metrics Improving Models Consistent Human Eval Non-repeatable human evaluation Figure 1: The opportunities of living benchmarks and pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Ot"
2021.gem-1.10,2020.acl-main.448,0,0.212993,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.wmt-1.77,0,0.124839,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.acl-main.173,1,0.838186,"to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicti"
2021.gem-1.10,W18-3601,1,0.834682,"where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way"
2021.gem-1.10,2020.msr-1.1,1,0.821875,"Missing"
2021.gem-1.10,C18-1147,1,0.871186,"Missing"
2021.gem-1.10,2020.emnlp-main.466,0,0.0504036,"Missing"
2021.gem-1.10,D15-1238,0,0.013053,"d this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the in"
2021.gem-1.10,K16-1028,0,0.065799,"Missing"
2021.gem-1.10,D18-1206,1,0.894493,"Missing"
2021.gem-1.10,W17-5525,1,0.880333,"Missing"
2021.gem-1.10,P02-1040,0,0.114424,"n NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achi"
2021.gem-1.10,2020.emnlp-main.89,1,0.895725,"Missing"
2021.gem-1.10,W17-3537,1,0.910798,"trics saturate, we need to evaluate them on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmar"
2021.gem-1.10,2020.emnlp-main.185,0,0.0484252,"Missing"
2021.gem-1.10,2021.acl-long.186,0,0.0851423,"Missing"
2021.gem-1.10,P19-1195,0,0.0585453,"Missing"
2021.gem-1.10,N18-1012,0,0.0173282,"akers of low-resourced languages through a participatory research approach, as suggested by (∀ et al., 2020). Toward this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the result"
2021.gem-1.10,Q19-1016,0,0.0583227,"Missing"
2021.gem-1.10,J18-3002,0,0.0872491,"for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated"
2021.gem-1.10,2020.acl-main.442,0,0.137138,"hem on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generatio"
2021.gem-1.10,2020.emnlp-main.647,0,0.0370713,"Missing"
2021.gem-1.10,2021.emnlp-main.529,0,0.024648,"on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291 63.5 64.0 32.5 29.4 55.1 54.5 27.5 26.4 0.943 0.942 -0.400 -0.412 mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++ 0.229 0.23 0.233 0.229 0.152 0.151 0.167 47.3 48.1 51.3 52.1 13.6 13.8 9.7 28.6 28.8 30.0 31.3 0.0 0.0 0.0 43.0 44.2 46.4 47.3 13.6 13.8 9.7 17.9 17.1 17.5 17.0 0.03 0.03 0.03 0.895 0.898 0.902 0.905 0.6"
2021.gem-1.10,D19-1320,0,0.0210218,"ence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTSc"
2021.gem-1.10,2020.acl-main.704,1,0.824209,"er, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang e"
2021.gem-1.10,P19-1212,0,0.0526263,"Missing"
2021.gem-1.10,P19-1646,0,0.0614703,"Missing"
2021.gem-1.10,Q16-1029,1,0.821779,"in a news article en *25k Articles WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmar"
2021.gem-1.10,W15-3031,0,0.06329,"Missing"
2021.gem-1.10,W19-2303,0,0.167787,"of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured"
2021.gem-1.10,2020.nlp4convai-1.13,0,0.0861626,"Missing"
2021.gem-1.10,D16-1033,0,0.064867,"Missing"
2021.gem-1.10,2020.acl-main.450,0,0.015505,"2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291"
2021.gem-1.10,P18-1205,0,0.0245839,"nalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the input of the wider NLG research community. To do so, we will set up a yearly selec"
2021.gem-1.4,W19-3646,0,0.0184416,"have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on trained neural metrics (Dziri et al., 2019; Mehri and Eskenazi, 2020), but they are not yet in wide use. Our work is probably the closest to Post (2018)’s assessment of inconsistencies in different implementations of BLEU (Papineni et al., 2002, see Section 3.2), calling for comparability and proposing a standard implementation. To our knowledge, we are the first to evaluate the use of corpus-based metrics in dialogue systems in this fashion. 3 3.1 Side Utterance User Can you help me find a train that will arrive in birmingham new street by 16:15? Sys. What day would you like to travel? User I would like to leave Friday from Cambridge"
2021.gem-1.4,P04-3031,0,0.0865339,"tem over longer periods of time even in task-oriented dialogue such as MultiWOZ (Oraby et al., 2018). Evaluation BLEU: The original MultiWOZ BLEU implementation internally uses a trivial tokenization splitting on whitespace. However, current models often use subword tokenization and complex detokenization to remove any redundant whitespace (Sennrich et al., 2016; Kudo and Richardson, 2018). This new-style detokenization might produce words with leading or trailing punctuation. Some works ignore this fact completely, or use an alternative BLEU implementation, including tokenization, from NLTK (Bird and Loper, 2004). 4.6 Dataset folds MultiWOZ authors split the data into train, validation, and test folds randomly. Following Lampouras and Vlachos (2016)’s analysis of train-test overlap on other datasets, we inspected the goals of all 1000 test dialogues; 174 of them are also present in the train or validation folds. The test fold does not contain any unseen slot-value pairs, and has only 12 new domain-slot-value triplets. This means that the evaluation does not really check the generalization capabilities of the systems’ state tracking, and it theoretically allows the systems to memorize the whole databas"
2021.gem-1.4,D18-1547,0,0.0508591,"Missing"
2021.gem-1.4,E17-2075,0,0.0117096,"ion for Computational Linguistics and consistency (Santhanam and Shaikh, 2019) or robustness (Dinan et al., 2019). Recent surveys in natural language generation reflect on divergence and inconsistency in human evaluation practice (Howcroft et al., 2020; Belz et al., 2020), in a similar spirit to our examination, but on a broader scale. Despite the availability of simulator evaluation (Schatzmann et al., 2006; Young et al., 2010; Zhu et al., 2020), corpus-based metrics have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on trained neural metrics (Dziri et al., 2019; Mehri and Eskenazi, 2020), but they are not yet in wide use. Our work is probably the closest to Post (2018)’s assessment of incon"
2021.gem-1.4,P19-1360,0,0.0148743,"kes use of machine teaching (Shukla et al., 2020). We used author-provided outputs for SOLOIST and AuGPT, author-trained checkpoints for DoTS, LABES,6 and UBAR, and we trained DAMD and MinTL7 from scratch using publicly available code. DAMD, MinTL and SOLOIST use MultiWOZ 2.0; the remaining models trained on the 2.1 version. DAMD, LABES, MinTL, and UBAR are based on the same code base and use similar evaluation scripts. We also compared 6 policy optimization models. SFN (Mehri et al., 2019), HDNO (Wang et al., 2021), and LAVA (Lubis et al., 2020) use reinforcement learning for training. HDSA (Chen et al., 2019) uses a BERT backbone and exploits the hierarchical structure of dialog acts. MarCo (Wang Inform & Success rates: The Inform rate relates to informable slots, which are attributes that allow the user to constrain database searches, e.g., restaurant location or price range. The Success rate focuses on requestable slots, i.e., those that can be asked by the user, e.g., phone number. Both are calculated on the level of dialogues. Su et al. (2015) consider a dialogue to be successful if the evaluated system provided all of the requested information for an entity satisfying the user’s constraints."
2021.gem-1.4,2020.emnlp-main.5,0,0.0145192,"t to our examination, but on a broader scale. Despite the availability of simulator evaluation (Schatzmann et al., 2006; Young et al., 2010; Zhu et al., 2020), corpus-based metrics have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on trained neural metrics (Dziri et al., 2019; Mehri and Eskenazi, 2020), but they are not yet in wide use. Our work is probably the closest to Post (2018)’s assessment of inconsistencies in different implementations of BLEU (Papineni et al., 2002, see Section 3.2), calling for comparability and proposing a standard implementation. To our knowledge, we are the first to evaluate the use of corpus-based metrics in dialogue systems in this fashion. 3 3.1 Side Uttera"
2021.gem-1.4,D14-1179,0,0.00821231,"Missing"
2021.gem-1.4,P15-2073,0,0.0239855,"006; Young et al., 2010; Zhu et al., 2020), corpus-based metrics have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on trained neural metrics (Dziri et al., 2019; Mehri and Eskenazi, 2020), but they are not yet in wide use. Our work is probably the closest to Post (2018)’s assessment of inconsistencies in different implementations of BLEU (Papineni et al., 2002, see Section 3.2), calling for comparability and proposing a standard implementation. To our knowledge, we are the first to evaluate the use of corpus-based metrics in dialogue systems in this fashion. 3 3.1 Side Utterance User Can you help me find a train that will arrive in birmingham new street by 16:15? Sys. What day would you l"
2021.gem-1.4,2020.emnlp-main.28,0,0.018948,"lexicalization, which was previously based only on string matching heuristics. Benchmark and Systems The MultiWOZ dataset 3.2 Corpus-based Metrics on MultiWOZ All standard CTR metrics on MultiWOZ – BLEU, Inform & Success rate – are calculated on delexicalized texts, i.e., texts where dialogue slot values, such as venue names, are replaced by placeholders (Wen et al., 2015). While using delexicalized utterances prevents errors in venue names to affect the evaluation, it prevents the use of an interactive human evaluation, model-based evaluation metrics known from open-domain dialogue research (Gao et al., 2020), or end-to-end evaluation with user simulators such as ConvLab (Zhu et al., 2020). The MultiWOZ 2.0 dataset (Budzianowski et al., 2018) includes about 10k task-oriented dialogues in 7 domains (restaurants, hotels, tourist attractions, trains, taxi, hospital, police) with dialogue state and system action annotation. Larger domains (restaurants, hotels, attractions, trains) have an associated database. The data was collected via human-tohuman interaction on a crowdsourcing platform using the Wizard-Of-Oz approach (Wen et al., 2017). Crowd workers were instructed with goals such as booking or fi"
2021.gem-1.4,N19-1423,0,0.0120497,"eported results.4 Out of the 13 compared works, 7 only report BLEU, Inform, and Success with no other evaluation; 4 use human ratings of individual outputs, and only 2 include human evaluation on full dialogues.5 An important representative of the end-to-end systems is DAMD (Zhang et al., 2020b). It uses a multi-action data augmentation and multiple GRU (Cho et al., 2014) decoders. Similarly, LABES (Zhang et al., 2020a) employs a few GRU-based decoders, but it represents the dialog state as a latent variable. DoTS (Jeon and Lee, 2021) also uses GRUs, but the model makes use of a BERT encoder (Devlin et al., 2019) to get a context representation. MinTL (Lin et al., 2020) applies a diff-based approach to state updates, with backbones based on the T5 and BART models (Raffel et al., 2020; Lewis et al., 2020). UBAR is based on a fine-tuned GPT-2 model (Radford et al., 2019), similarly to AuGPT (Kulh´anek et al., 2021) which uses back-translations for response augmentation, and SOLOIST (Peng et al., 2020) which makes use of machine teaching (Shukla et al., 2020). We used author-provided outputs for SOLOIST and AuGPT, author-trained checkpoints for DoTS, LABES,6 and UBAR, and we trained DAMD and MinTL7 from"
2021.gem-1.4,D19-1461,0,0.0186134,"ultiWOZ corpus-based metrics, we evaluate all systems in terms of the diversity of their outputs. 2 Related Work Most works on evaluation methods in dialogue response generation (Deriu et al., 2021) focus on human evaluation (Walker et al., 1997), e.g., choosing the best methodology with respect to quality 1 https://github.com/Tomiinek/MultiWOZ_ Evaluation 34 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 34–46 August 5–6, 2021. ©2021 Association for Computational Linguistics and consistency (Santhanam and Shaikh, 2019) or robustness (Dinan et al., 2019). Recent surveys in natural language generation reflect on divergence and inconsistency in human evaluation practice (Howcroft et al., 2020; Belz et al., 2020), in a similar spirit to our examination, but on a broader scale. Despite the availability of simulator evaluation (Schatzmann et al., 2006; Young et al., 2010; Zhu et al., 2020), corpus-based metrics have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problem"
2021.gem-1.4,2020.inlg-1.23,0,0.0247572,"Missing"
2021.gem-1.4,P07-2045,0,0.0105997,"Missing"
2021.gem-1.4,2020.coling-main.41,0,0.0280455,"Missing"
2021.gem-1.4,2020.acl-main.448,0,0.0136056,". Despite the availability of simulator evaluation (Schatzmann et al., 2006; Young et al., 2010; Zhu et al., 2020), corpus-based metrics have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on trained neural metrics (Dziri et al., 2019; Mehri and Eskenazi, 2020), but they are not yet in wide use. Our work is probably the closest to Post (2018)’s assessment of inconsistencies in different implementations of BLEU (Papineni et al., 2002, see Section 3.2), calling for comparability and proposing a standard implementation. To our knowledge, we are the first to evaluate the use of corpus-based metrics in dialogue systems in this fashion. 3 3.1 Side Utterance User Can you help me find a train that"
2021.gem-1.4,D18-2012,0,0.0116442,"or repetitive nature of a system’s responses (Holtzman et al., 2020). While diversity is typically measured for non-task-oriented dialogue (Li et al., 2016), we argue that it can serve as an indicator of the naturalness of using a system over longer periods of time even in task-oriented dialogue such as MultiWOZ (Oraby et al., 2018). Evaluation BLEU: The original MultiWOZ BLEU implementation internally uses a trivial tokenization splitting on whitespace. However, current models often use subword tokenization and complex detokenization to remove any redundant whitespace (Sennrich et al., 2016; Kudo and Richardson, 2018). This new-style detokenization might produce words with leading or trailing punctuation. Some works ignore this fact completely, or use an alternative BLEU implementation, including tokenization, from NLTK (Bird and Loper, 2004). 4.6 Dataset folds MultiWOZ authors split the data into train, validation, and test folds randomly. Following Lampouras and Vlachos (2016)’s analysis of train-test overlap on other datasets, we inspected the goals of all 1000 test dialogues; 174 of them are also present in the train or validation folds. The test fold does not contain any unseen slot-value pairs, and h"
2021.gem-1.4,2020.acl-main.64,0,0.0119415,"evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on trained neural metrics (Dziri et al., 2019; Mehri and Eskenazi, 2020), but they are not yet in wide use. Our work is probably the closest to Post (2018)’s assessment of inconsistencies in different implementations of BLEU (Papineni et al., 2002, see Section 3.2), calling for comparability and proposing a standard implementation. To our knowledge, we are the first to evaluate the use of corpus-based metrics in dialogue systems in this fashion. 3 3.1 Side Utterance User Can you help me find a train that will arrive in birmingham new street by 16:15? Sys. What day would you like to travel? User I would like to leave Friday from Cambridge. Sys. TR2716 will leave ca"
2021.gem-1.4,W19-5921,0,0.0156389,"PT (Kulh´anek et al., 2021) which uses back-translations for response augmentation, and SOLOIST (Peng et al., 2020) which makes use of machine teaching (Shukla et al., 2020). We used author-provided outputs for SOLOIST and AuGPT, author-trained checkpoints for DoTS, LABES,6 and UBAR, and we trained DAMD and MinTL7 from scratch using publicly available code. DAMD, MinTL and SOLOIST use MultiWOZ 2.0; the remaining models trained on the 2.1 version. DAMD, LABES, MinTL, and UBAR are based on the same code base and use similar evaluation scripts. We also compared 6 policy optimization models. SFN (Mehri et al., 2019), HDNO (Wang et al., 2021), and LAVA (Lubis et al., 2020) use reinforcement learning for training. HDSA (Chen et al., 2019) uses a BERT backbone and exploits the hierarchical structure of dialog acts. MarCo (Wang Inform & Success rates: The Inform rate relates to informable slots, which are attributes that allow the user to constrain database searches, e.g., restaurant location or price range. The Success rate focuses on requestable slots, i.e., those that can be asked by the user, e.g., phone number. Both are calculated on the level of dialogues. Su et al. (2015) consider a dialogue to be suc"
2021.gem-1.4,C16-1105,0,0.0587715,"Missing"
2021.gem-1.4,C18-1147,0,0.0497674,"Missing"
2021.gem-1.4,2020.emnlp-main.146,0,0.0146984,"e] entrance fee. The address is [attraction name], 13 [attraction address] and the post code is [attraction postcode]. Can i help you with anything else? [attraction name] has a free entrance fee. The address is [attraction name], [value count] [attraction address] and the post code is [restaurant postcode]. Can i help you with anything else? Table 2: An example utterance from the MultiWOZ dataset with different styles of delexicalization. The first row shows the non-delexicalized source response. Other styles are paired with the systems that use or introduced them. et al., 2020) and UniConv (Le et al., 2020) generate explicit system actions in parallel with the response. We use the public predictions for LAVA and the provided pretrained models for other models. UniConv and HDNO are trained on MultiWOZ 2.1, other systems use the 2.0 version. As opposed to end-to-end models, the version affects the evaluation because the ground-truth state is supplied to the model. The comparison of these systems is thus not completely fair, but we believe that the differences are small in comparison with the differences in evaluation scripts and setups (see Section 5.2). 4 provided with the dataset is limited; it"
2021.gem-1.4,D17-1238,1,0.879418,"Missing"
2021.gem-1.4,2020.acl-main.703,0,0.0263189,"full dialogues.5 An important representative of the end-to-end systems is DAMD (Zhang et al., 2020b). It uses a multi-action data augmentation and multiple GRU (Cho et al., 2014) decoders. Similarly, LABES (Zhang et al., 2020a) employs a few GRU-based decoders, but it represents the dialog state as a latent variable. DoTS (Jeon and Lee, 2021) also uses GRUs, but the model makes use of a BERT encoder (Devlin et al., 2019) to get a context representation. MinTL (Lin et al., 2020) applies a diff-based approach to state updates, with backbones based on the T5 and BART models (Raffel et al., 2020; Lewis et al., 2020). UBAR is based on a fine-tuned GPT-2 model (Radford et al., 2019), similarly to AuGPT (Kulh´anek et al., 2021) which uses back-translations for response augmentation, and SOLOIST (Peng et al., 2020) which makes use of machine teaching (Shukla et al., 2020). We used author-provided outputs for SOLOIST and AuGPT, author-trained checkpoints for DoTS, LABES,6 and UBAR, and we trained DAMD and MinTL7 from scratch using publicly available code. DAMD, MinTL and SOLOIST use MultiWOZ 2.0; the remaining models trained on the 2.1 version. DAMD, LABES, MinTL, and UBAR are based on the same code base and"
2021.gem-1.4,W18-5019,0,0.0152337,"(mined from the dialogue state and system action annotations) even during evaluation, whereas other ignore it and let their systems behave randomly. 4.4 4.5 Output Diversity Metrics The standard MultiWOZ metrics do not cover the diversity of the outputs, which can show the formulaic or repetitive nature of a system’s responses (Holtzman et al., 2020). While diversity is typically measured for non-task-oriented dialogue (Li et al., 2016), we argue that it can serve as an indicator of the naturalness of using a system over longer periods of time even in task-oriented dialogue such as MultiWOZ (Oraby et al., 2018). Evaluation BLEU: The original MultiWOZ BLEU implementation internally uses a trivial tokenization splitting on whitespace. However, current models often use subword tokenization and complex detokenization to remove any redundant whitespace (Sennrich et al., 2016; Kudo and Richardson, 2018). This new-style detokenization might produce words with leading or trailing punctuation. Some works ignore this fact completely, or use an alternative BLEU implementation, including tokenization, from NLTK (Bird and Loper, 2004). 4.6 Dataset folds MultiWOZ authors split the data into train, validation, and"
2021.gem-1.4,N16-1014,0,0.0213534,"sk for another place, etc., and accept the booking with new constraints. This brings a problem into the evaluation, because some works use the ground-truth booking information (mined from the dialogue state and system action annotations) even during evaluation, whereas other ignore it and let their systems behave randomly. 4.4 4.5 Output Diversity Metrics The standard MultiWOZ metrics do not cover the diversity of the outputs, which can show the formulaic or repetitive nature of a system’s responses (Holtzman et al., 2020). While diversity is typically measured for non-task-oriented dialogue (Li et al., 2016), we argue that it can serve as an indicator of the naturalness of using a system over longer periods of time even in task-oriented dialogue such as MultiWOZ (Oraby et al., 2018). Evaluation BLEU: The original MultiWOZ BLEU implementation internally uses a trivial tokenization splitting on whitespace. However, current models often use subword tokenization and complex detokenization to remove any redundant whitespace (Sennrich et al., 2016; Kudo and Richardson, 2018). This new-style detokenization might produce words with leading or trailing punctuation. Some works ignore this fact completely,"
2021.gem-1.4,P02-1040,0,0.110535,"ion 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on trained neural metrics (Dziri et al., 2019; Mehri and Eskenazi, 2020), but they are not yet in wide use. Our work is probably the closest to Post (2018)’s assessment of inconsistencies in different implementations of BLEU (Papineni et al., 2002, see Section 3.2), calling for comparability and proposing a standard implementation. To our knowledge, we are the first to evaluate the use of corpus-based metrics in dialogue systems in this fashion. 3 3.1 Side Utterance User Can you help me find a train that will arrive in birmingham new street by 16:15? Sys. What day would you like to travel? User I would like to leave Friday from Cambridge. Sys. TR2716 will leave cambridge at 13:01 and arrive at birmingham at 15:44. The trip will take 163 minutes and cost 75.10 pounds. Would you like to book? User Yes, I would like to book for 3 people."
2021.gem-1.4,2020.emnlp-main.273,0,0.0229575,"BLEU, Inform, and Success with no other evaluation; 4 use human ratings of individual outputs, and only 2 include human evaluation on full dialogues.5 An important representative of the end-to-end systems is DAMD (Zhang et al., 2020b). It uses a multi-action data augmentation and multiple GRU (Cho et al., 2014) decoders. Similarly, LABES (Zhang et al., 2020a) employs a few GRU-based decoders, but it represents the dialog state as a latent variable. DoTS (Jeon and Lee, 2021) also uses GRUs, but the model makes use of a BERT encoder (Devlin et al., 2019) to get a context representation. MinTL (Lin et al., 2020) applies a diff-based approach to state updates, with backbones based on the T5 and BART models (Raffel et al., 2020; Lewis et al., 2020). UBAR is based on a fine-tuned GPT-2 model (Radford et al., 2019), similarly to AuGPT (Kulh´anek et al., 2021) which uses back-translations for response augmentation, and SOLOIST (Peng et al., 2020) which makes use of machine teaching (Shukla et al., 2020). We used author-provided outputs for SOLOIST and AuGPT, author-trained checkpoints for DoTS, LABES,6 and UBAR, and we trained DAMD and MinTL7 from scratch using publicly available code. DAMD, MinTL and SOL"
2021.gem-1.4,2020.findings-emnlp.17,0,0.0156498,"LABES (Zhang et al., 2020a) employs a few GRU-based decoders, but it represents the dialog state as a latent variable. DoTS (Jeon and Lee, 2021) also uses GRUs, but the model makes use of a BERT encoder (Devlin et al., 2019) to get a context representation. MinTL (Lin et al., 2020) applies a diff-based approach to state updates, with backbones based on the T5 and BART models (Raffel et al., 2020; Lewis et al., 2020). UBAR is based on a fine-tuned GPT-2 model (Radford et al., 2019), similarly to AuGPT (Kulh´anek et al., 2021) which uses back-translations for response augmentation, and SOLOIST (Peng et al., 2020) which makes use of machine teaching (Shukla et al., 2020). We used author-provided outputs for SOLOIST and AuGPT, author-trained checkpoints for DoTS, LABES,6 and UBAR, and we trained DAMD and MinTL7 from scratch using publicly available code. DAMD, MinTL and SOLOIST use MultiWOZ 2.0; the remaining models trained on the 2.1 version. DAMD, LABES, MinTL, and UBAR are based on the same code base and use similar evaluation scripts. We also compared 6 policy optimization models. SFN (Mehri et al., 2019), HDNO (Wang et al., 2021), and LAVA (Lubis et al., 2020) use reinforcement learning for trainin"
2021.gem-1.4,2020.acl-main.638,0,0.0207179,"Missing"
2021.gem-1.4,D15-1199,0,0.0312865,"Missing"
2021.gem-1.4,W19-8610,0,0.0158997,"this dataset;1 • In addition to standard MultiWOZ corpus-based metrics, we evaluate all systems in terms of the diversity of their outputs. 2 Related Work Most works on evaluation methods in dialogue response generation (Deriu et al., 2021) focus on human evaluation (Walker et al., 1997), e.g., choosing the best methodology with respect to quality 1 https://github.com/Tomiinek/MultiWOZ_ Evaluation 34 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 34–46 August 5–6, 2021. ©2021 Association for Computational Linguistics and consistency (Santhanam and Shaikh, 2019) or robustness (Dinan et al., 2019). Recent surveys in natural language generation reflect on divergence and inconsistency in human evaluation practice (Howcroft et al., 2020; Belz et al., 2020), in a similar spirit to our examination, but on a broader scale. Despite the availability of simulator evaluation (Schatzmann et al., 2006; Young et al., 2010; Zhu et al., 2020), corpus-based metrics have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3."
2021.gem-1.4,E17-1042,0,0.0201353,"Missing"
2021.gem-1.4,2021.eacl-main.110,0,0.0315356,"Missing"
2021.gem-1.4,P16-1162,0,0.00589026,"can show the formulaic or repetitive nature of a system’s responses (Holtzman et al., 2020). While diversity is typically measured for non-task-oriented dialogue (Li et al., 2016), we argue that it can serve as an indicator of the naturalness of using a system over longer periods of time even in task-oriented dialogue such as MultiWOZ (Oraby et al., 2018). Evaluation BLEU: The original MultiWOZ BLEU implementation internally uses a trivial tokenization splitting on whitespace. However, current models often use subword tokenization and complex detokenization to remove any redundant whitespace (Sennrich et al., 2016; Kudo and Richardson, 2018). This new-style detokenization might produce words with leading or trailing punctuation. Some works ignore this fact completely, or use an alternative BLEU implementation, including tokenization, from NLTK (Bird and Loper, 2004). 4.6 Dataset folds MultiWOZ authors split the data into train, validation, and test folds randomly. Following Lampouras and Vlachos (2016)’s analysis of train-test overlap on other datasets, we inspected the goals of all 1000 test dialogues; 174 of them are also present in the train or validation folds. The test fold does not contain any un"
2021.gem-1.4,2020.acl-demos.39,0,0.0139922,"ders, but it represents the dialog state as a latent variable. DoTS (Jeon and Lee, 2021) also uses GRUs, but the model makes use of a BERT encoder (Devlin et al., 2019) to get a context representation. MinTL (Lin et al., 2020) applies a diff-based approach to state updates, with backbones based on the T5 and BART models (Raffel et al., 2020; Lewis et al., 2020). UBAR is based on a fine-tuned GPT-2 model (Radford et al., 2019), similarly to AuGPT (Kulh´anek et al., 2021) which uses back-translations for response augmentation, and SOLOIST (Peng et al., 2020) which makes use of machine teaching (Shukla et al., 2020). We used author-provided outputs for SOLOIST and AuGPT, author-trained checkpoints for DoTS, LABES,6 and UBAR, and we trained DAMD and MinTL7 from scratch using publicly available code. DAMD, MinTL and SOLOIST use MultiWOZ 2.0; the remaining models trained on the 2.1 version. DAMD, LABES, MinTL, and UBAR are based on the same code base and use similar evaluation scripts. We also compared 6 policy optimization models. SFN (Mehri et al., 2019), HDNO (Wang et al., 2021), and LAVA (Lubis et al., 2020) use reinforcement learning for training. HDSA (Chen et al., 2019) uses a BERT backbone and explo"
2021.gem-1.4,2020.nlp4convai-1.13,0,0.0820872,"Missing"
2021.gem-1.4,2020.emnlp-main.740,0,0.0809289,"iWOZ We discuss performance of 13 recent systems that use CTR evaluation on MultiWOZ – 7 end-toend and 6 policy-optimization systems, which use ground-truth dialogue states during training and inference. We include models for which we got test set predictions and systems with public code for which we managed to replicate reported results.4 Out of the 13 compared works, 7 only report BLEU, Inform, and Success with no other evaluation; 4 use human ratings of individual outputs, and only 2 include human evaluation on full dialogues.5 An important representative of the end-to-end systems is DAMD (Zhang et al., 2020b). It uses a multi-action data augmentation and multiple GRU (Cho et al., 2014) decoders. Similarly, LABES (Zhang et al., 2020a) employs a few GRU-based decoders, but it represents the dialog state as a latent variable. DoTS (Jeon and Lee, 2021) also uses GRUs, but the model makes use of a BERT encoder (Devlin et al., 2019) to get a context representation. MinTL (Lin et al., 2020) applies a diff-based approach to state updates, with backbones based on the T5 and BART models (Raffel et al., 2020; Lewis et al., 2020). UBAR is based on a fine-tuned GPT-2 model (Radford et al., 2019), similarly t"
2021.gem-1.4,2020.sigdial-1.37,0,0.111608,"ups, and we show that their reported scores cannot be directly compared. To facilitate comparison of future systems, we release our standalone standardized evaluation scripts. We also give basic recommendations for corpus-based benchmarking in future works. 1 • We identify, list, and discuss consistency issues associated with the MultiWOZ benchmark; Introduction While human judgements are irreplaceable in dialogue systems evaluation and using full dialogue evaluation instead of evaluating isolated responses given ground-truth contexts cannot fully measure system performance (Liu et al., 2016; Takanobu et al., 2020), corpus-based evaluation metrics, such as BLEU and corpus-based entity match and success rate (Wen et al., 2017), are still very important for model development and are often used to compare models and establish state-of-the-art. We show on the MultiWOZ benchmark (Budzianowski et al., 2018), one of the most frequently used and most challenging dialogue system datasets today, that these comparisons do not hold if several basic conditions are not met, and that these conditions are not met for most of the recent works using corpusbased evaluation on this dataset. This means the assessment of pro"
2021.gem-1.4,P97-1035,0,0.748682,"Missing"
2021.gem-1.4,2020.acl-demos.19,0,0.0566403,"ultiWOZ_ Evaluation 34 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 34–46 August 5–6, 2021. ©2021 Association for Computational Linguistics and consistency (Santhanam and Shaikh, 2019) or robustness (Dinan et al., 2019). Recent surveys in natural language generation reflect on divergence and inconsistency in human evaluation practice (Howcroft et al., 2020; Belz et al., 2020), in a similar spirit to our examination, but on a broader scale. Despite the availability of simulator evaluation (Schatzmann et al., 2006; Young et al., 2010; Zhu et al., 2020), corpus-based metrics have been the go-to evaluation method in end-to-end neural dialogue systems since the first implementations (Wen et al., 2017; Eric and Manning, 2017) and are a defacto standard until today (cf. Section 3.3). There are works showing problems of corpus-based metrics: limited correlation with human judgements (Novikova et al., 2017; Takanobu et al., 2020) and mixed performance depending on human reference texts used (Freitag et al., 2020) or evaluated systems (Mathur et al., 2020). Many works aim at creating more reliable metrics (Galley et al., 2015). Recent focus is on t"
2021.inlg-1.14,W15-4702,0,0.0278669,"Missing"
2021.inlg-1.14,2020.inlg-1.24,0,0.120208,"esearch. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting. 1 Introduction This paper turned out very differently from the one we had initially intended to write. Our original intention was to write an overview of the different kinds of errors that appear in the output of different kinds of Natural Language Generation (NLG) systems, and to develop a general taxonomy of NLG output errors, based on the publications that have appeared at previous INLG conferences (similar to Howcroft et al. 2020; Belz et al. 2020). This, however, turned out to be impossible. The reason? There is a severe under-reporting of the different kinds of errors that NLG systems make. By this assertion, we mean that authors neither include any error analysis nor provide any examples of errors made by the system, and they do not make reference to different kinds of errors that may appear in the output. The latter is a lower bar than carrying out an error analysis, which requires a more systematic approach where several outputs are sampled and analysed for the presence of errors, which are then categorised (ideally through a forma"
2021.inlg-1.14,W19-8652,1,0.877161,"Missing"
2021.inlg-1.14,W18-6539,1,0.90035,"Missing"
2021.inlg-1.14,J11-2010,0,0.0489409,"risation scheme). For example, Mahamood and Reiter (2012) worked with nurses to identify errors in reports generated for parents of neonatal infants. Taking into consideration the costly process of selecting domain expert annotators and the importance of quality control, nondomain experts might be also considered, ensuring their qualification through (intensive) training (Artstein and Poesio, 2005; Carlson et al., 2001).11 Compensation and treatment of workers If annotators are hired, either directly or via a crowdsourcing platform such as MTurk, they should be compensated and treated fairly (Fort et al., 2011). Silberman et al. (2018) provide useful guidelines for the treatment of crowd-workers. The authors note that they should at least be paid the minimum wage, they should be paid promptly, and they should be treated with respect. This means you should be ready to answer questions about the annotation task, and to streamline the task based on worker feedback. If you use human participants to annotate the data, you likely also need to apply for approval by an Institutional Review Board (IRB). Training Annotators should receive training to be able to carry out the error analysis, but the amount of"
2021.inlg-1.14,2020.inlg-1.31,0,0.017629,"Missing"
2021.inlg-1.14,W15-4708,1,0.640622,"., 2020). Finally, human and automatic evaluation metrics, or at least the ones that generate some kind of intrinsic rating, are too coarse-grained to capture relevant information. They are general evaluations of system performance that estimate an averagecase performance across a limited set of abstract dimensions (if they measure anything meaningful 141 at all; see Reiter 2018). We don’t usually know the worst-case performance, and we don’t know what kinds of errors cause the metrics or ratings to be sub-optimal. Additionally, the general lack of extrinsic evaluations among NLG researchers (Gkatzia and Mahamood, 2015) means that in some cases we only have a partial understanding of the possible errors for a given system. 2.3 Levels of analysis As noted above, our focus on errors in the output text is essential to facilitate framework-neutral comparisons between the performance of different systems. When categorizing the errors made by different systems, it is important to be careful with terms such as hallucination and omission, since these are process-level (pertaining to the system) rather than product-level (pertaining to the output) descriptions of the errors.4 Process-level descriptions are problemati"
2021.inlg-1.14,W15-4611,0,0.0737509,"Missing"
2021.inlg-1.14,D15-1268,0,0.0355951,"Missing"
2021.inlg-1.14,2020.inlg-1.23,1,0.8209,"Missing"
2021.inlg-1.14,2020.inlg-1.12,0,0.0923196,"Missing"
2021.inlg-1.14,2021.naacl-main.51,1,0.829754,"Missing"
2021.inlg-1.14,2020.inlg-1.45,1,0.739154,"Missing"
2021.inlg-1.14,W15-4717,0,0.0657496,"Missing"
2021.inlg-1.14,2020.inlg-1.20,0,0.0440365,"Missing"
2021.inlg-1.14,2021.naacl-main.383,0,0.0942458,"Missing"
2021.inlg-1.14,2020.coling-main.444,0,0.0611221,"Missing"
2021.inlg-1.14,P01-1057,0,0.316871,"ty of Helsinki, 9 trivago N.V., 10 Georgetown University, 11 University of Virginia, 12 University of Cambridge Contact: c.w.j.vanmiltenburg@tilburguniversity.edu Abstract of papers that discuss the kinds of errors that may appear in NLG output. The fact that errors are under-reported in the NLG literature is probably unsurprising to experienced researchers in this area. The lack of reporting of negative results in AI has been a well-known issue for many years (Reiter et al., 2003). With the classic NLG example being the reporting of negative results for the STOP project on smoking cessation (Reiter et al., 2001, 2003). But even going in with (relatively) low expectations, it was confronting just to see how little we as a community look at the mistakes that our systems make. We believe that it is both necessary and possible to improve our ways. One of the reasons why it is necessary to provide more error analyses (see §2.2 for more), is that otherwise, it is unclear what are the strengths and weaknesses of current NLG systems. In what follows, we provide guidance on how to gain more insight into system behavior. This paper provides a general framework to carry out error analyses. First we cover the t"
2021.inlg-1.14,2020.acl-main.442,0,0.0207869,"Unit: token/word/phrase • Majority vote vs. any error Three options: a) Top-down (theory-based) b) Bottom-up (from data) c) Expand existing taxonomy Assess & Report • • • • Generate error frequency table. Compute overall IAA scores. Compute IAA per error/annotator Create confusion matrix. Enlarge sample & annotator pool • Start with a small pilot • Increase as guidelines are stable & IAA is good enough Figure 1: Flowchart depicting recommended analysis steps, as described in §4. IAA stands for Inter-Annotator Agreement, as measured through Cohen’s kappa or Krippendorff’s alpha, for example. (Ribeiro et al., 2020). Finally, examples to analyze may also be selected based on quantitative values: automatic metric scores or ratings in a preceding general human evaluation. This way, error analysis can provide explanation for the automatic or human scores. The most suitable option depends on your specific use case: While random selection gives the least biased picture of the model performance, selecting specifically hard and/or low-rated samples may be more efficient. Also note that the sample selection should always be independent of any samples you may have previously examined during system development, si"
2021.inlg-1.14,2020.inlg-1.37,0,0.0203743,"natural language. For example, sign language (as in Mazzei 2015) would be considered ‘text’ under this definition. 2 By focusing on countable instances of things that went wrong in the output text, we also exclude issues such as bias and low output diversity, that are global properties of the collection of outputs that a system produces for a given amount of inputs, rather than being identifiable in individual outputs. 3 There has been some effort to automate this process. For example, Shimorina et al. (2021) describe an automatic error analysis procedure for shallow surface realisation, and Stevens-Guille et al. (2020) automate the detection of repetitions, omissions, and hallucinations. However, for many NLG tasks, this kind of automation is still out of reach, given the wide range of possible correct outputs that are available in language generation tasks. cess or for a particular design goal. For example, formal texts aren’t wrong per se, but if the goal is to produce informal texts, then any semblance of formality may be considered incorrect. It may be possible to relate different kinds of errors to the different dimensions of text quality identified by Belz et al. (2020). What is crucial here, is that"
2021.inlg-1.14,W10-4221,0,0.0884287,"Missing"
2021.inlg-1.14,2020.inlg-1.22,1,0.904778,"the social sciences (Krippendorff, 2018; Neuendorf, 2017).3 Error analysis can be carried out during development (to see what kinds of mistakes the system is currently making), as the last part of a study (evaluating a new system that you are presenting), or as a standalone study (comparing different systems). The latter option requires output data to be available, ideally for both the validation and test sets. A rich source of output data is the GEM shared task (Gehrmann et al., 2021). Text errors can be categorised in several different types, including factual errors (e.g. incorrect number; Thomson and Reiter 2020), and errors related to form (spelling, grammaticality), style (formal versus informal, empathetic versus neutral), or behavior (over- and under-specification). Some of these are universally wrong, while others may be ‘contextually wrong’ with respect to the task suc1 We use the term ‘text’ to refer to any expression of natural language. For example, sign language (as in Mazzei 2015) would be considered ‘text’ under this definition. 2 By focusing on countable instances of things that went wrong in the output text, we also exclude issues such as bias and low output diversity, that are global pr"
2021.inlg-1.25,2020.inlg-1.24,1,0.634095,"been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submis"
2021.inlg-1.25,2020.webnlg-1.7,1,0.833183,"Missing"
2021.inlg-1.25,P19-1483,0,0.0185913,"e to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and di"
2021.inlg-1.25,W19-8652,1,0.842123,"., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either"
2021.inlg-1.25,2020.inlg-1.19,1,0.814881,"e et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 acc"
2021.inlg-1.25,2020.findings-emnlp.76,0,0.032576,"ll achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 acc"
2021.inlg-1.25,2021.inlg-1.26,0,0.0115062,"3, and truncate 3 Experiments 6 https://pypi.org/project/text2num/ https://pypi.org/project/num2words/ 8 Due to space constraints, we do not list the results of 7 5 https://spacy.io 262 Error Type Mistake R P 5 Token R P NAME NUMBER WORD CONTEXT NOT_CHECKABLE OTHER 0.750 0.777 0.514 0.000 0.000 0.000 0.846 0.750 0.483 - 0.759 0.759 0.465 0.000 0.000 0.000 0.862 0.752 0.529 - Overall 0.691 0.756 0.550 0.769 Our Charles-UPF submission achieved the best results in the automatic metrics category, but there is still a gap with what humans can achieve, as shown by the Laval University submission’s (Garneau and Lamontagne, 2021) overall 0.841 recall and 0.879 precision. One way to improve our system would be to enrich the reference fact descriptions, by either inferring more information from the raw data, or by extracting additional data from external databases.2 Another option would be to add surrounding sentences to the context – this could help to resolve coreferences (e.g., if a player is referred to as ""He"") and to detect the CONTEXT errors. We also note that our approach requires the real system outputs manually annotated with errors in order to work well – using only synthetic data results in low recall (see T"
2021.inlg-1.25,2020.coling-main.218,0,0.0358257,"Missing"
2021.inlg-1.25,2020.inlg-1.14,0,0.0275566,"Missing"
2021.inlg-1.25,2020.webnlg-1.16,0,0.0372828,"NLG (Tian et al., 2019; Harkous et al., 2020; Filippova, 2020; Rebuffel et al., 2021). Neural systems are particularly unreliable on complex datasets such as Rotowire (Wiseman et al., 2017), where the task is to generate basketball match summaries from tabular data. Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluat"
2021.inlg-1.25,W07-0734,0,0.0155492,"n training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale we"
2021.inlg-1.25,W19-8643,0,0.0460402,"Missing"
2021.inlg-1.25,2021.ccl-1.108,0,0.0266333,"Missing"
2021.inlg-1.25,W19-8659,1,0.927622,"., 2020; Filippova, 2020; Rebuffel et al., 2021). Neural systems are particularly unreliable on complex datasets such as Rotowire (Wiseman et al., 2017), where the task is to generate basketball match summaries from tabular data. Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasne"
2021.inlg-1.25,D17-1238,1,0.833037,"models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternati"
2021.inlg-1.25,P02-1040,0,0.109437,"longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et"
2021.inlg-1.25,D19-1410,0,0.0426386,"Missing"
2021.inlg-1.25,2020.inlg-1.28,0,0.0195068,"guage inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submission for the shared task falls into the first category: we developed an automatic metric for token-le"
2021.inlg-1.25,W19-8610,0,0.0126242,"llam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less re"
2021.inlg-1.25,2020.acl-main.704,0,0.01747,"n recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Castro Ferreira et al., 2020). This, however, comes with the cost of lower fluency. Detecting NLG errors automatically is a hard problem. For word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), reliability on content checking is known to be poor (Novikova et al., 2017; Dhingra et al., 2019). Most neural metrics (Zhang et al., 2020; Sellam et al., 2020) have not been evaluated for content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shai"
2021.inlg-1.25,2020.inlg-1.22,0,0.160963,"content preservation. Dušek and Kasner (2020)’s metric based on natural language inference (NLI) specifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submission for the shared task f"
2021.inlg-1.25,2021.inlg-1.23,0,0.0361876,"ifically targets content preservation, but, same as all previously mentioned ones, is not able to provide fine-grained error tagging beyond sentence level. Specific content-checking metrics mostly remain a domain of handcrafted pattern matching (Wen et al., 2015; Dušek et al., 2019), which does not scale well to new domains. While human evaluation provides a more reliable alternative, it is costly and difficult to set up (van der Lee et al., 2019; Santhanam and Shaikh, 2019; Belz et al., 2020; Thomson and Reiter, 2020a). The INLG 2021 accuracy evaluation shared task (Reiter and Thomson, 2020; Thomson and Reiter, 2021) aims to improve this situation. Reiter and Thomson (2020) carefully annotated 60 outputs of various neural systems trained on Rotowire with 6 error types (see Table 1) defined in Thomson and Reiter (2020b). The objective of the shared task is then to either implement an automatic metric for creating the same type of annotations automatically, or to develop a human evaluation scenario capable of producing the same annotations while requiring less resources. Our submission for the shared task falls into the first category: we developed an automatic metric for token-level error annotation which"
2021.inlg-1.25,N19-1101,0,0.0546357,"Missing"
2021.inlg-1.25,D15-1199,0,0.0768463,"Missing"
2021.inlg-1.25,D17-1239,0,0.0256655,"on with a model trained on a mixture of human-annotated and synthetic data. 1 Introduction Recent neural NLG systems can easily generate fluent texts from linearized structured data (Zhao et al., 2020; Kale and Rastogi, 2020; Castro Ferreira et al., 2020). However, the systems cannot guarantee that the output is properly grounded in the input – hallucination (outputs not supported by input data) is a notorious problem in neural NLG (Tian et al., 2019; Harkous et al., 2020; Filippova, 2020; Rebuffel et al., 2021). Neural systems are particularly unreliable on complex datasets such as Rotowire (Wiseman et al., 2017), where the task is to generate basketball match summaries from tabular data. Rotowire poses multiple challenges for neural systems: it requires content selection and production of longer texts, and its human-written training texts are themselves not always grounded in data, which makes neural models more susceptible to hallucination. On the other hand, rule-based systems used in recent data-to-text tasks (Lapalme, 2020; Tran and Nguyen, 2020; Mille et al., 2019) all achieve high scores in terms of accuracy of the generated contents with respect to the input structures (Dušek et al., 2020; Cas"
2021.inlg-1.25,D19-5011,0,0.0120759,"e semantic similarity between gi and the evaluated sentence s using Sentence Transformers (Reimers and Gurevych, 2019).4 In particular, we embed the sentence tokens by applying mean pooling on the output of 4 https://www.sbert.net/ LM-based Error Tagger We use a RoBERTa LM (Liu et al., 2019) with a token-level classification head as our errorchecking model. Unlike unsupervised approaches based on examining attention values (Thorne et al., 2019; Li et al., 2020) or input perturbations (Kim et al., 2020), we train the model directly to predict error categories using annotated data, similarly to Yoosuf and Yang (2019). The model receives an input X = (C, s), composed of the context C, i.e., relevant background facts selected by context retrieval in Section 2.2, and the generated sentence s to be tagged. The inputs are separated by the delimiter </s>. The model is trained to annotate each token in s either with an OK label, or with a label corresponding to one of the error categories. We experiment with two data sources for training the model: (1) gold-standard annotated data 261 Generator Data synth Simple synth + human synth Compact synth + human c R EMR = 0.25 P F1 R EMR = 0.5 P F1 R EMR = 0.75 P F1 5 10"
2021.inlg-1.25,2020.acl-main.224,0,0.0904673,"Missing"
2021.nlp4convai-1.19,D19-5602,0,0.0363428,"Missing"
2021.nlp4convai-1.19,D18-1547,0,0.0584847,"Missing"
2021.nlp4convai-1.19,D19-1459,0,0.102381,". ©2021 Association for Computational Linguistics 2 Related Work While the first attempts to build generative endto-end task-oriented systems mimicked the traditional dialogue system components (Wen et al., 2017), the task was soon recast as a sequence prediction problem in a two-stage setup. A sequenceto-sequence (seq2seq) model first generates the belief state based on dialogue context, then generates the system response based on the context and the belief state (Sequicity; Lei et al., 2018). Recently, large-scale multi-domain task-oriented datasets were proposed (Budzianowski et al., 2018; Byrne et al., 2019; Rastogi et al., 2020). To address multiple domains, Zhang et al. (2020a) introduce the LABES-S2S model that – in addition to a two-stage seq2seq approach – models belief states as discrete latent variables. Zhang et al. (2020b) present DAMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state. Qin et al. (2020) investigate sharing of domain knowledge and performance on unseen domains. Lubis et al. (2020)’s LAVA model employs reinforcement learning over latent system actions initialized using a vari"
2021.nlp4convai-1.19,N19-1423,0,0.185113,"les. Zhang et al. (2020b) present DAMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state. Qin et al. (2020) investigate sharing of domain knowledge and performance on unseen domains. Lubis et al. (2020)’s LAVA model employs reinforcement learning over latent system actions initialized using a variational autoencoder. The line of research closest to our work makes use of large pre-trained LMs based on the transformer architecture (Vaswani et al., 2017) such as GPT-2 (Radford et al., 2019) or BERT (Devlin et al., 2019). For example, Wu et al. (2020) propose finetuning BERT (Devlin et al., 2019) for taskoriented dialogue, Zhang et al. (2020c) extended the GPT-2 LM to model open-domain chit-chat. We follow research initiated by Budzianowski and Vuli´c (2019), who use GPT-2 to model multidomain task-oriented dialogues. Recently, three similar modifications to their model were proposed, namely SOLOIST (Peng et al., 2021), SimpleTOD (Hosseini-Asl et al., 2020), and the approach by Ham et al. (2020). Our work extends these models and proposes a novel training approach and data augmentation strategies based on bac"
2021.nlp4convai-1.19,D18-1045,0,0.136859,"Wu et al. (2020) propose finetuning BERT (Devlin et al., 2019) for taskoriented dialogue, Zhang et al. (2020c) extended the GPT-2 LM to model open-domain chit-chat. We follow research initiated by Budzianowski and Vuli´c (2019), who use GPT-2 to model multidomain task-oriented dialogues. Recently, three similar modifications to their model were proposed, namely SOLOIST (Peng et al., 2021), SimpleTOD (Hosseini-Asl et al., 2020), and the approach by Ham et al. (2020). Our work extends these models and proposes a novel training approach and data augmentation strategies based on back-translation (Edunov et al., 2018; Federmann et al., 2019). Earlier works used a single pivot language (Jin et al., 2018; Einolghozati et al., 2019), whereas our work applies 10 languages to increase variability. ing restaurant details. The system must process the user’s input, keep track of the belief state (user preferences regarding individual slots, i.e., indomain attributes) and generate a relevant response in natural language. It must also interact with a database to incorporate external information into its responses (see Figure 1 for an example). Following Budzianowski and Vuli´c (2019), we choose the GPT-2 LM as our"
2021.nlp4convai-1.19,2020.acl-main.54,0,0.0227298,"d LMs based on the transformer architecture (Vaswani et al., 2017) such as GPT-2 (Radford et al., 2019) or BERT (Devlin et al., 2019). For example, Wu et al. (2020) propose finetuning BERT (Devlin et al., 2019) for taskoriented dialogue, Zhang et al. (2020c) extended the GPT-2 LM to model open-domain chit-chat. We follow research initiated by Budzianowski and Vuli´c (2019), who use GPT-2 to model multidomain task-oriented dialogues. Recently, three similar modifications to their model were proposed, namely SOLOIST (Peng et al., 2021), SimpleTOD (Hosseini-Asl et al., 2020), and the approach by Ham et al. (2020). Our work extends these models and proposes a novel training approach and data augmentation strategies based on back-translation (Edunov et al., 2018; Federmann et al., 2019). Earlier works used a single pivot language (Jin et al., 2018; Einolghozati et al., 2019), whereas our work applies 10 languages to increase variability. ing restaurant details. The system must process the user’s input, keep track of the belief state (user preferences regarding individual slots, i.e., indomain attributes) and generate a relevant response in natural language. It must also interact with a database to incor"
2021.nlp4convai-1.19,2020.acl-main.665,0,0.0496628,"Missing"
2021.nlp4convai-1.19,2020.lrec-1.53,0,0.0206466,"ministic, and the delexicalized response r¯ does not depend on the database results d, but only on their counts dc . Therefore, the distribution p(r|d, b, c) is equal to the distribution p(¯r|dc , b, c), and by maximizing its likelihood we are achieving the goal of maximizing the likelihood of p(r|c). We use the same language model pˆ to model the belief state and to generate the delexicalized prediction. That is, p(¯r|dc , b, c) ≈ p(¯ ˆ r|dc , b, c, θ ) (2) p(b|c) ≈ p(b| ˆ 0, / 0, / c, θ ) , (3) where we denote the model’s parameters as θ . In the MultiWOZ dataset (Budzianowski et al., 2018; Eric et al., 2020, see Section 4), responses are delexicalized by replacing concrete values with placeholder tokens of the form domain_slot. For better generalization across domains, we chose to only use slot instead as responses rarely involve more than one domain. We train our model to predict the active domain by outputting it first in the 3 We found in our experiments on the MultiWOZ data (see Section 4) that this assumption was almost always fulfilled. belief state (remaining domains follow in lexicographical order). The predicted active domain is then used during lexicalization.4 Belief state: train { le"
2021.nlp4convai-1.19,P18-1133,0,0.170213,"ssification task based on subtle changes to the dialogue state (instead of fully random resampling) used as an auxiliary training objective, and we demonstrate its performance improvements. Unlike traditional task-oriented systems based on modularized pipelines (Young et al., 2013; Gao et al., 2019), end-to-end dialogue systems inte• We present a novel application of token ungrate nearly all functionality required to hold a likelihood loss (Welleck et al., 2020) in taskdialogue into a single neural network (Wen et al., oriented dialogue to further improve diversity of 2017; Eric et al., 2017; Lei et al., 2018), reducour model’s responses. ing error-propagation and data annotation require• We apply pre-training on additional datasets ments. While these systems are not yet ready for and massive data augmentation using backproduction use, they made considerable progress translation via multiple languages (Sennrich in recent years, especially with the advent of preet al., 2016) and demonstrate that both markedly trained neural language models (LMs) (Devlin improve task-oriented dialogue performance. et al., 2019; Radford et al., 2019; Zhang et al., • We compare our model to multiple baselines 2020c). S"
2021.nlp4convai-1.19,J10-3003,0,0.0494673,"the delexicalized response.5 (6) Placeholders in the delexicalized response are replaced by values from the database results and the belief state. 3.4 Data Augmentation Following its successful usage in other NLP tasks, (Konstas et al., 2017; Elder et al., 2020), we experiment with data augmentation using paraphrases. In our setup, we generate multiple paraphrases for each training utterance and use them to augment the training data. This way, we effectively increase the variability of the data. Various data-driven approaches for paraphrasing were proposed, the majority of them corpora-based (Madnani and Dorr, 2010). Recently, machine translation systems showed strong performance in generating paraphrases using back-translation (Sennrich et al., 2016; Edunov et al., 2018; Federmann et al., 2019), i.e., translating an English text into an intermediate language and then translating the result back into English. We use two different Transformer-based machine translation systems to paraphrase our data. We used Edunov et al. (2018)’s system with French and the system of Macháˇcek et al. (2020); Zouhar et al. (2021) with additional 40 pivot languages. Based on empirical analysis of translation quality, we chos"
2021.nlp4convai-1.19,P02-1040,0,0.110084,"ned during the DSTC9 shared task using Amazon Mechanical Turk. Note that only 4 out of 10 submissions outperformed the Baseline according to the average success metric. tion scheme by Budzianowski et al. (2018), which provides two metrics – the inform rate and the success rate. The inform rate is the percentage of dialogues in which the system mentioned a name or ID of an entity which does not contradict the current dialogue state and the user’s goal, whereas the success rate is the percentage of dialogues in which the system outputted all the requested information. Moreover, we compute BLEU (Papineni et al., 2002) between the generated system utterances and the ground truth to get an approximation of the output fluency. 4.5 ConvLab 2 Evaluation We use the ConvLab 2 platform (Zhu et al., 2020) for automatic evaluation with a simulated user agent. We run the evaluation component 1,000 times, i.e. on 1,000 simulated conversations. The agent mimics user behavior, interacts with the system under evaluation, and computes multiple metrics: The complete rate reflects the ratio of dialogues that are completed, i.e. all the user requests have been met. The success rate computes the percentage of dialogues which"
2021.nlp4convai-1.19,2020.acl-main.428,0,0.0431255,"}, hotel { price range = cheap } DB: train 23 matches, hotel no match Example 1: String format for AuGPT’s belief state and database result count. To fully exploit natural language pre-training of our LM, we represent the belief state and database result counts as strings containing as few special tokens as possible (see Example 1). 3.2 Model Training Although parameters are shared for the belief state predictor and the delexicalized response predictor, the training objectives differ slightly. We use crossentropy loss for both; response prediction uses unlikelihood loss (Welleck et al., 2020; Li et al., 2020) as an additional objective. Unlikelihood loss penalizes repeated tokens, which helps the model avoid repetitions and increases output diversity. To help the model learn a better internal representation from the data, we employ additional auxiliary tasks. Similarly to Devlin et al. (2019) 4 A disadvantage of this approach is that we cannot determine the active domain if the belief state is empty. However, in such a case the lexicalization would fail anyway, so the system’s performance is not affected by this decision. 200 and Peng et al. (2021), we train a binary classifier to detect dialogue"
2021.nlp4convai-1.19,2020.acl-main.565,0,0.0204659,"system response based on the context and the belief state (Sequicity; Lei et al., 2018). Recently, large-scale multi-domain task-oriented datasets were proposed (Budzianowski et al., 2018; Byrne et al., 2019; Rastogi et al., 2020). To address multiple domains, Zhang et al. (2020a) introduce the LABES-S2S model that – in addition to a two-stage seq2seq approach – models belief states as discrete latent variables. Zhang et al. (2020b) present DAMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state. Qin et al. (2020) investigate sharing of domain knowledge and performance on unseen domains. Lubis et al. (2020)’s LAVA model employs reinforcement learning over latent system actions initialized using a variational autoencoder. The line of research closest to our work makes use of large pre-trained LMs based on the transformer architecture (Vaswani et al., 2017) such as GPT-2 (Radford et al., 2019) or BERT (Devlin et al., 2019). For example, Wu et al. (2020) propose finetuning BERT (Devlin et al., 2019) for taskoriented dialogue, Zhang et al. (2020c) extended the GPT-2 LM to model open-domain chit-chat. We fo"
2021.nlp4convai-1.19,W04-1013,0,0.0275241,"Missing"
2021.nlp4convai-1.19,2020.coling-main.41,0,0.0687089,"Missing"
2021.nlp4convai-1.19,2020.iwslt-1.25,0,0.068969,"Missing"
2021.nlp4convai-1.19,P16-1009,0,0.0429942,"state. 3.4 Data Augmentation Following its successful usage in other NLP tasks, (Konstas et al., 2017; Elder et al., 2020), we experiment with data augmentation using paraphrases. In our setup, we generate multiple paraphrases for each training utterance and use them to augment the training data. This way, we effectively increase the variability of the data. Various data-driven approaches for paraphrasing were proposed, the majority of them corpora-based (Madnani and Dorr, 2010). Recently, machine translation systems showed strong performance in generating paraphrases using back-translation (Sennrich et al., 2016; Edunov et al., 2018; Federmann et al., 2019), i.e., translating an English text into an intermediate language and then translating the result back into English. We use two different Transformer-based machine translation systems to paraphrase our data. We used Edunov et al. (2018)’s system with French and the system of Macháˇcek et al. (2020); Zouhar et al. (2021) with additional 40 pivot languages. Based on empirical analysis of translation quality, we chose 10 pivot languages for our data – we obtain 10 different paraphrases for each input utterance.6 When training, we choose the input user"
2021.nlp4convai-1.19,W15-4639,0,0.0669536,"Missing"
2021.nlp4convai-1.19,2020.acl-demos.19,0,0.0153594,"zianowski et al. (2018), which provides two metrics – the inform rate and the success rate. The inform rate is the percentage of dialogues in which the system mentioned a name or ID of an entity which does not contradict the current dialogue state and the user’s goal, whereas the success rate is the percentage of dialogues in which the system outputted all the requested information. Moreover, we compute BLEU (Papineni et al., 2002) between the generated system utterances and the ground truth to get an approximation of the output fluency. 4.5 ConvLab 2 Evaluation We use the ConvLab 2 platform (Zhu et al., 2020) for automatic evaluation with a simulated user agent. We run the evaluation component 1,000 times, i.e. on 1,000 simulated conversations. The agent mimics user behavior, interacts with the system under evaluation, and computes multiple metrics: The complete rate reflects the ratio of dialogues that are completed, i.e. all the user requests have been met. The success rate computes the percentage of dialogues which are successful, meaning the system captures correct informed entities and provides a valid booking if requested. Finally, the book rate is the proportion of dialogues where the syste"
2021.nlp4convai-1.19,2021.naacl-main.14,0,0.0132218,"s data-driven approaches for paraphrasing were proposed, the majority of them corpora-based (Madnani and Dorr, 2010). Recently, machine translation systems showed strong performance in generating paraphrases using back-translation (Sennrich et al., 2016; Edunov et al., 2018; Federmann et al., 2019), i.e., translating an English text into an intermediate language and then translating the result back into English. We use two different Transformer-based machine translation systems to paraphrase our data. We used Edunov et al. (2018)’s system with French and the system of Macháˇcek et al. (2020); Zouhar et al. (2021) with additional 40 pivot languages. Based on empirical analysis of translation quality, we chose 10 pivot languages for our data – we obtain 10 different paraphrases for each input utterance.6 When training, we choose the input user utterance uniformly at random from the set of all 10+1 variants of the utterance (backtranslation outputs and the original one). 4 4.1 Experiments Datasets As our primary dataset, we use MultiWOZ 2.1, a de-noised version of MultiWOZ 2.0 (Budzianowski et al., 2018). We also used the 2.0 version to compare to previous works. The dataset contains 7 distinct domains ("
2021.nlp4convai-1.19,E17-1042,0,0.0611606,"Missing"
2021.nlp4convai-1.19,2020.emnlp-main.66,0,0.0189946,"AMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state. Qin et al. (2020) investigate sharing of domain knowledge and performance on unseen domains. Lubis et al. (2020)’s LAVA model employs reinforcement learning over latent system actions initialized using a variational autoencoder. The line of research closest to our work makes use of large pre-trained LMs based on the transformer architecture (Vaswani et al., 2017) such as GPT-2 (Radford et al., 2019) or BERT (Devlin et al., 2019). For example, Wu et al. (2020) propose finetuning BERT (Devlin et al., 2019) for taskoriented dialogue, Zhang et al. (2020c) extended the GPT-2 LM to model open-domain chit-chat. We follow research initiated by Budzianowski and Vuli´c (2019), who use GPT-2 to model multidomain task-oriented dialogues. Recently, three similar modifications to their model were proposed, namely SOLOIST (Peng et al., 2021), SimpleTOD (Hosseini-Asl et al., 2020), and the approach by Ham et al. (2020). Our work extends these models and proposes a novel training approach and data augmentation strategies based on back-translation (Edunov et al., 2"
2021.nlp4convai-1.19,2020.emnlp-main.740,0,0.0374148,"he first attempts to build generative endto-end task-oriented systems mimicked the traditional dialogue system components (Wen et al., 2017), the task was soon recast as a sequence prediction problem in a two-stage setup. A sequenceto-sequence (seq2seq) model first generates the belief state based on dialogue context, then generates the system response based on the context and the belief state (Sequicity; Lei et al., 2018). Recently, large-scale multi-domain task-oriented datasets were proposed (Budzianowski et al., 2018; Byrne et al., 2019; Rastogi et al., 2020). To address multiple domains, Zhang et al. (2020a) introduce the LABES-S2S model that – in addition to a two-stage seq2seq approach – models belief states as discrete latent variables. Zhang et al. (2020b) present DAMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state. Qin et al. (2020) investigate sharing of domain knowledge and performance on unseen domains. Lubis et al. (2020)’s LAVA model employs reinforcement learning over latent system actions initialized using a variational autoencoder. The line of research closest to our work makes use"
2021.nlp4convai-1.19,2020.acl-demos.30,0,0.0339546,"he first attempts to build generative endto-end task-oriented systems mimicked the traditional dialogue system components (Wen et al., 2017), the task was soon recast as a sequence prediction problem in a two-stage setup. A sequenceto-sequence (seq2seq) model first generates the belief state based on dialogue context, then generates the system response based on the context and the belief state (Sequicity; Lei et al., 2018). Recently, large-scale multi-domain task-oriented datasets were proposed (Budzianowski et al., 2018; Byrne et al., 2019; Rastogi et al., 2020). To address multiple domains, Zhang et al. (2020a) introduce the LABES-S2S model that – in addition to a two-stage seq2seq approach – models belief states as discrete latent variables. Zhang et al. (2020b) present DAMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state. Qin et al. (2020) investigate sharing of domain knowledge and performance on unseen domains. Lubis et al. (2020)’s LAVA model employs reinforcement learning over latent system actions initialized using a variational autoencoder. The line of research closest to our work makes use"
bojar-etal-2012-joy,bojar-etal-2010-evaluating,1,\N,Missing
bojar-etal-2012-joy,C00-2163,0,\N,Missing
bojar-etal-2012-joy,W07-1709,0,\N,Missing
bojar-etal-2012-joy,P02-1040,0,\N,Missing
bojar-etal-2012-joy,H05-1066,0,\N,Missing
bojar-etal-2012-joy,P07-2045,1,\N,Missing
bojar-etal-2012-joy,W10-1703,0,\N,Missing
bojar-etal-2012-joy,W09-3939,1,\N,Missing
D17-1238,P11-2040,0,0.0842284,"Missing"
D17-1238,E06-1040,0,0.334179,"d human references that have grammatical errors, e.g. “Fifth Floor does not allow childs” (SFR EST reference). Corpus-based methods may pick up these errors, and word-based metrics will rate these system utterances as correct, whereas we can expect human judges to be sensitive to ungrammatical utterances. Note that the parsing score (while being a crude approximation of grammaticality) achieves one of our highest correlation results against human ratings, with |ρ |= .31. Grammatical errors raise questions about the quality of the training data, especially when being crowdsourced. For example, Belz and Reiter (2006) find that human experts assign low rankings to their original corpus text. Again, weighting (Galley et al., 2015) or reference-less approaches (Specia et al., 2010) might remedy this issue. 8.3 Example-based Analysis As shown in previous sections, word-based metrics moderately agree with humans on bad quality output, but cannot distinguish output of good or medium quality. Table 5 provides examples from 2247 Study this paper (Reiter and Belz, 2009) (Stent et al., 2005) (Liu et al., 2016) (Elliott and Keller, 2014) (Kilickaya et al., 2017) (Cahill, 2009) (Espinosa et al., 2010) Dimension of hu"
D17-1238,P09-2025,0,0.0911826,"Missing"
D17-1238,E06-1032,0,0.283153,"Missing"
D17-1238,P15-1044,1,0.895165,"Missing"
D17-1238,W16-3622,1,0.873425,"Missing"
D17-1238,P16-2008,1,0.758992,"Missing"
D17-1238,P14-2074,0,0.014043,"Missing"
D17-1238,D10-1055,0,0.0841175,"Missing"
D17-1238,P15-2073,0,0.0137689,"Missing"
D17-1238,W08-0332,0,0.0214618,"Missing"
D17-1238,P16-2043,1,0.797767,"d with grammatically correct and easily readable output that is unrelated to the input. We have experimented with combining WBMs and GBMs using ensemble-based learning. However, while our model achieved high correlation with humans within a single domain, its cross-domain performance is insufficient. Our paper clearly demonstrates the need for more advanced metrics, as used in related fields, including: assessing output quality within the dialogue context, e.g. (Duˇsek and Jurˇc´ıcˇ ek, 2016); extrinsic evaluation metrics, such as NLG’s contribution to task success, e.g. (Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); building discriminative models, e.g. (Hodosh and Hockenmaier, 2016), (Kannan and Vinyals, 2017); or reference-less quality prediction as used in MT, e.g. (Specia et al., 2010). We see our paper as a first step towards reference-less evaluation for NLG by introducing grammar-based metrics. In current work (Duˇsek et al., 2017), we investigate a reference-less quality estimation approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only. Finally, note that the datasets consi"
D17-1238,C12-2044,0,0.0194824,"compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez and M`arquez, 2008) or grammatical error correction"
D17-1238,W16-3203,0,0.0108292,"nput. We have experimented with combining WBMs and GBMs using ensemble-based learning. However, while our model achieved high correlation with humans within a single domain, its cross-domain performance is insufficient. Our paper clearly demonstrates the need for more advanced metrics, as used in related fields, including: assessing output quality within the dialogue context, e.g. (Duˇsek and Jurˇc´ıcˇ ek, 2016); extrinsic evaluation metrics, such as NLG’s contribution to task success, e.g. (Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); building discriminative models, e.g. (Hodosh and Hockenmaier, 2016), (Kannan and Vinyals, 2017); or reference-less quality prediction as used in MT, e.g. (Specia et al., 2010). We see our paper as a first step towards reference-less evaluation for NLG by introducing grammar-based metrics. In current work (Duˇsek et al., 2017), we investigate a reference-less quality estimation approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only. Finally, note that the datasets considered in this study are fairly small (between 404 and 2.3k human references per domain). To"
D17-1238,N13-1132,0,0.0227336,"he correlation between automatic metrics and human ratings using the Spearman coefficient (ρ). We split the data per dataset and system in order to make valid pairwise comparisons. To handle outliers within human ratings, we use the median score of the three human raters.8 Following Kilickaya et al. (2017), we use the Williams’ test (Williams, 1959) to determine significant differences between correlations. Table 3 summarises the utterance-level correlation 8 As an alternative to using the median human judgment for each item, a more effective way to use all the human judgments could be to use Hovy et al. (2013)’s MACE tool for inferring the reliability of judges. results between automatic metrics and human ratings, listing the best (i.e. highest absolute ρ) results for each type of metric (details provided in supplementary material). Our results suggest that: • In sum, no metric produces an even moderate correlation with human ratings, independently of dataset, system, or aspect of human rating. This contrasts with our initially promising results on the system level (see Section 6) and will be further discussed in Section 8. Note that similar inconsistencies between document- and sentence-level eval"
D17-1238,W01-0813,0,0.0959134,"Missing"
D17-1238,C16-1105,0,0.0946581,"Missing"
D17-1238,W07-0734,0,0.058479,"ences produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez and M`arquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on gro"
D17-1238,W04-1013,0,0.0564463,"T, summarisation or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez"
D17-1238,D16-1230,0,0.348242,"Missing"
D17-1238,P10-1157,0,0.0710514,"Missing"
D17-1238,N16-1086,0,0.0220533,"ults on two different domains and three different datasets, which allows us to draw more general conclusions. • Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the sentence-level. • Make all associated code and data publicly available, including detailed analysis results.1 2 End-to-End NLG Systems In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2016, Lampouras and Vlachos, 2016). These approaches do not require costly semantic alignment between Meaning Representations (MR) and human references (also referred to as “ground truth” or “targets”), but are 1 Available for download at: https://github.com/ jeknov/EMNLP_17_submission 2241 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241–2252 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics System BAGEL LOLS RNNLG TG EN Total 202 202 40"
D17-1238,W16-6644,1,0.289454,"uage Processing, pages 2241–2252 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics System BAGEL LOLS RNNLG TG EN Total 202 202 404 Dataset SFR EST SFH OTEL 581 600 1,181 398 477 875 MR: inform(name=X, area=X, pricerange=moderate, type=restaurant) Reference: “X is a moderately priced restaurant in X.” Total 1,181 1,077 202 2,460 Table 1: Number of NLG system outputs from different datasets and systems used in this study. based on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. (Novikova et al., 2016), and as such, enable rapid development of NLG components in new domains. In particular, we compare the performance of the following systems: • RNNLG:2 The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface realisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far. • TG EN:3 The system by Duˇsek and Jurˇc´ıcˇ ek (2015) learns to incrementally generate deepsyntax dependency trees of candidate sentence plans (i.e. which MR elements to"
D17-1238,P02-1040,0,0.112365,"from related fields, such as MT, summarisation or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, su"
D17-1238,J09-4008,0,0.142695,"tings, with |ρ |= .31. Grammatical errors raise questions about the quality of the training data, especially when being crowdsourced. For example, Belz and Reiter (2006) find that human experts assign low rankings to their original corpus text. Again, weighting (Galley et al., 2015) or reference-less approaches (Specia et al., 2010) might remedy this issue. 8.3 Example-based Analysis As shown in previous sections, word-based metrics moderately agree with humans on bad quality output, but cannot distinguish output of good or medium quality. Table 5 provides examples from 2247 Study this paper (Reiter and Belz, 2009) (Stent et al., 2005) (Liu et al., 2016) (Elliott and Keller, 2014) (Kilickaya et al., 2017) (Cahill, 2009) (Espinosa et al., 2010) Dimension of human ratings Sentence Planning Surface Realisation weak positive (ρ = 0.33, WPS) none weak positive (ρ = 0.47, LSA) weak positive (ρ = 0.35, BLEU-4) positive (ρ = 0.53, METEOR) positive (ρ = 0.64, SPICE) N/A weak positive (ρ = 0.43, TER) weak negative (ρ = 0. − 31, parser) strong positive (Pearson’s r = 0.96, NIST) negative (ρ = −0.56, NIST) N/A N/A N/A negative (ρ = −0.64, ROUGE) positive (ρ = 0.62, BLEU-4) Domain NLG, restaurant/hotel search NLG, w"
D17-1238,2006.amta-papers.25,0,0.10849,"number of automatic metrics from related fields, such as MT, summarisation or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been"
D17-1238,D16-1228,0,0.14828,"CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez and M`arquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. We focus on two important properties of texts here – readability and grammaticality: • Readability quantifies the difficulty with which a reader understands a text, as used for e.g. evaluating summarisation (Kan et al., 2001) or text simplification (Francois and Bernhard, 2014). We measure readability by the Flesch Reading Ease score (RE) (Flesch, 1979), which calculates a ratio between the number of characters per sentence, the number of wo"
D17-1238,W17-5525,1,0.866044,"Missing"
D17-1238,N16-1015,0,0.0118384,"Missing"
D17-1238,D15-1199,0,0.0284939,"Missing"
D18-1432,E17-2029,0,0.0275008,"tputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal. Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine outputs), Xu et al. (2017) use a full adversarial training setup. The most recent works explore the usage of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hierarchical E-D model. Shen et al. (2017) use a cVAE conditioned on sentiment and response genericity (based on a handwritten list of phrases). Shen et al. (2018) combine a cVAE with a plain VAE in an adversarial fashion. We also draw on ideas from other areas than dialogue generation to build our models: Tu et al. (2017)’s context gates originate from machine translation and Hu et al. (2017)’s cVAE training stems from free-text generation. 3"
D18-1432,P17-1045,0,0.045199,"02) used by the vast majority of recent dialogue generation works (Zhao et al., 2017; Yao et al., 2017; Li et al., 2017a, 2016c; Sordoni et al., 2015; Li et al., 2016a; Ghazvininejad et al., 2017). BLEU in this paper refers to the default BLEU-4, but we also report on lower n-gram scores (B1, B2, B3).9 • Coh – our novel GloVe-based coherence score calculated using Eq (1) showing the semantic distance of dialogue contexts and generated responses. • D-1, D-2, D-Sent – common metrics used to evaluate the diversity of generated responses (e.g. Li et al., 2016a; Xu et al., 2017; Xing et al., 2017; Dhingra et al., 2017): the proportion of distinct unigrams, bigrams, and sentences in the outputs. 5 Results All model variants described in Section 4 are trained on both OST and fOST datasets. Tables 2 and 3 present the scores of all models tested on the OST and fOST test sets, respectively. Note that in addition to testing the models on the respective test sections of their training datasets, we also test them on the other dataset (OST-trained models on fOST and vice-versa). This way, we can observe the performance of the fOST-trained models in more noisy contexts and see how good the OSTtrained models are when"
D18-1432,S17-1008,0,0.0381174,"thm, similar to Shao et al. (2017) in addition to using a self-attention model. Mou et al. (2016) predict keywords for the output in a preprocessing step while Wu et al. (2018) preselect a vocabulary subset to be used for decoding. Li et al. (2016b) focus specifically on personality generation (using personality embeddings) and Wang et al. (2017) promote topic-specific outputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal. Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine outputs), Xu et al. (2017) use a full adversarial training setup. The most recent works explore the usage of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hierarchical E-D model. Shen et al. (2017) use a cVAE conditioned on sent"
D18-1432,K16-1002,0,0.0755824,"is, quantifying the contributions that come from effective modeling of coherence into our models. All our experimental code is freely available on GitHub.1 2 Coherence-based Dialogue Generation Our model aims to generate responses given a dialogue context, incorporating measures of coherence estimated purely from the training data. We propose the following enhancements to the attention-based E-D architecture (Bahdanau et al., 2015; Luong et al., 2015): • We introduce a stochastic latent variable z conditioned on previous dialogue context to store the global information about the conversation (Bowman et al., 2016; Chung et al., 2015; Li and Jurafsky, 2017; Hu et al., 2017). 1 https://github.com/XinnuoXu/CVAE_Dial • We force the model to condition on the measure of coherence explicitly by encoding a latent variable (code) c learned from data. • We incorporate a context gate (Tu et al., 2017) that dynamically controls the ratio at which the generated words in the response derive directly from the coherence-enhanced dialogue context or the previously generated parts of the response. In the rest of this section, we introduce the measure of coherence (Section 2.1), we present an overview of our model (Sect"
D18-1432,P17-4012,0,0.0244374,"ence and diversity metrics (cf. Section 4.2) between OST and fOST. Unsurprisingly, coherence for fOST is much higher than OST, with a slightly higher diversity. We list dialogue examples for different coherence scores in Supplemental Material B. Dataset for Coherence Measure In order to accurately measure coherence on our domain using the semantic distance as defined in Section 2.1, we train GloVe embeddings on the full OpenSubtitles corpus (i.e. 100K movies). 4 Experiments Our generator model, ablative variants, and baselines are implemented using the publicly available OpenNMT-py framework (Klein et al., 2017) based on Bahdanau et al. (2015) and Luong et al. (2015). We used the publicly available glovepython package8 to implement our coherence measure. We experiment on two versions of our model: (1) cVAE with the coherence context gate as described in Section 2.3 (cVAE-XGate), (2) cVAE with the original context gate implementation of 5 The coherence score is calculated as shown in Eq (1). We observed that the scores on the training set follow a normal distribution with a slight tail on the negatively correlated side, so we fit a normal distribution to the data with parameters N (0.25, 0.22) and set"
D18-1432,W04-3250,0,0.0670261,"Missing"
D18-1432,N16-1014,0,0.675039,"End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets, we aim to control for coherence by learning directly from data"
D18-1432,P16-1094,0,0.570159,"End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets, we aim to control for coherence by learning directly from data"
D18-1432,D17-1019,0,0.118219,"sponse B-Coh: Well, I got water. B-Incoh: I don’t know. B-Coh: Specifically the stove. B-Incoh: Let’s go for a walk. Figure 1: Examples of conversational history (left) with two alternative responses to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address t"
D18-1432,L16-1147,0,0.0334699,"s of the generative model. Given a dialogue context x and an expected coherence value c, the context encoder first encodes the dialogue context into a hidden state h. The prior network then generates a sample z0 conditioned on the dialogue context. The decoder is initialized with s, i.e., the concatenation of h, z and c. During decoding, the next word is generated via the context gate modulating between the attention-reweighted context and the previously generated words of the response. 3 Dataset and Filtering Dataset for Generator We train and evaluate our models on the OpenSubtitles corpus (Lison and Tiedemann, 2016) with automatic dialogue turn segmentation (Lison and Meena, 2016).4 A training pair consists of a dialogue context and a corresponding response. We consider three consecutive turns as the dialogue context and the following turn as the response. From a total of 65M instances, we select those that have context and response lengths of less than 120 and 30 words, respectively. We create two datasets: 1. OST (plain OpenSubtitles) consists of 2M/4K/4K instances as our training/development/test sets, selected randomly from the whole corpus; 2. fOST (filtered OpenSubtitles) contains the same amount o"
D18-1432,D15-1166,0,0.122553,"Missing"
D18-1432,C16-1316,0,0.0565902,"herent (8). 6 Related Work Our work fits into the context of the very active area of end-to-end generative conversation models, where neural E-D approaches have been first applied by Vinyals and Le (2015) and extended by many others since. Many works address the lack of diversity and coherence in E-D outputs (Sountsov and Sarawagi, 2016; Wei et al., 2017) but do not attempt to model coherence directly, unlike our work: Li et al. (2016a) use anti-LM reranking; Li et al. (2016c) modify the beam search decoding algorithm, similar to Shao et al. (2017) in addition to using a self-attention model. Mou et al. (2016) predict keywords for the output in a preprocessing step while Wu et al. (2018) preselect a vocabulary subset to be used for decoding. Li et al. (2016b) focus specifically on personality generation (using personality embeddings) and Wang et al. (2017) promote topic-specific outputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) u"
D18-1432,P02-1040,0,0.101228,"Missing"
D18-1432,D14-1162,0,0.0888307,"ference on Empirical Methods in Natural Language Processing, pages 3981–3991 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics the previous two utterances and containing rich thematic words, whereas the response “Let’s go for a walk.” is unrelated and uninteresting. In order to obtain coherent responses, we present three generic enhancements to existing encoder-decoder (E-D) models: 1. We define a measure of coherence simply as the averaged word embedding similarity between the words of the context and the response computed using GloVe vectors (Pennington et al., 2014). 2. We filter a corpus of conversations based on our measure of coherence, which leaves us with context-response pairs that are both topically coherent and lexically diverse. 3. We train an E-D generator recast as a conditional Variational Autoencoder (cVAE; Zhao et al., 2017) model that incorporates two latent variables, one for encoding the context and another for conditioning on the measure of coherence, trained jointly as in Hu et al. (2017). We then decode using a context gate (Tu et al., 2017) to control the generation of words that directly relate to the most topical words of the conte"
D18-1432,P15-1152,0,0.0877581,"to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This p"
D18-1432,N15-1020,0,0.212013,": (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous"
D18-1432,D16-1158,0,0.0534187,"lstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets,"
D18-1432,D17-1228,0,0.0292964,"Missing"
D18-1432,D17-1065,0,0.182531,"ndard responses (Papineni et al., 2002) used by the vast majority of recent dialogue generation works (Zhao et al., 2017; Yao et al., 2017; Li et al., 2017a, 2016c; Sordoni et al., 2015; Li et al., 2016a; Ghazvininejad et al., 2017). BLEU in this paper refers to the default BLEU-4, but we also report on lower n-gram scores (B1, B2, B3).9 • Coh – our novel GloVe-based coherence score calculated using Eq (1) showing the semantic distance of dialogue contexts and generated responses. • D-1, D-2, D-Sent – common metrics used to evaluate the diversity of generated responses (e.g. Li et al., 2016a; Xu et al., 2017; Xing et al., 2017; Dhingra et al., 2017): the proportion of distinct unigrams, bigrams, and sentences in the outputs. 5 Results All model variants described in Section 4 are trained on both OST and fOST datasets. Tables 2 and 3 present the scores of all models tested on the OST and fOST test sets, respectively. Note that in addition to testing the models on the respective test sections of their training datasets, we also test them on the other dataset (OST-trained models on fOST and vice-versa). This way, we can observe the performance of the fOST-trained models in more noisy contexts and se"
D18-1432,D17-1233,0,0.0335803,"Missing"
D18-1432,P17-1061,0,0.395973,"neural models in terms of BLEU score as well as metrics of coherence and diversity. 1 Response B-Coh: Well, I got water. B-Incoh: I don’t know. B-Coh: Specifically the stove. B-Incoh: Let’s go for a walk. Figure 1: Examples of conversational history (left) with two alternative responses to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring"
korvas-etal-2014-free,W13-4064,0,\N,Missing
korvas-etal-2014-free,rousseau-etal-2012-ted,0,\N,Missing
N18-2012,P16-2043,1,0.866837,"Missing"
N18-2012,D17-1238,1,0.911107,"Missing"
N18-2012,W17-5525,1,0.876016,"Missing"
N18-2012,N18-1014,0,0.0270398,"et al., 2013; Bojar et al., 2017), however, without given end-points. Siddharthan and Katsos (2012) have previously used ME for evaluating readability of automatically generated texts. RankME extends this idea by asking subjects to provide a relative ranking of all target sentences. Table 1 provides a summary of methods and scales, and indicates whether relative ranking or direct assessment was used. Experimental Setup We were able to obtain outputs of 3 systems from the recent E2E NLG challenge (Novikova et al., 2017b):1 the Sheffield NLP system (Chen et al., 2018) and the Slug2Slug system (Juraska et al., 2018), as well as the outputs of the baseline TGen system (Duˇsek and Jurˇc´ıcˇ ek, 2016). We chose these systems in order to assess whether our methods can discriminate between outputs of different quality: Automatic metric scores, including BLEU, METEOR, etc., indicate that the Slug2Slug and TGen systems show similar performance while Sheffield’s is further apart.1 All three systems are based on the sequenceto-sequence (seq2seq) architecture with attention (Bahdanau et al., 2015). Sheffield NLP and TGen both use this basic architecture with LSTM recurrent cells (Hochreiter and Schmidhuber, 1997)"
N18-2012,W16-6644,1,0.836386,"uts of NLG systems with respect to these distinct criteria, especially for error analysis. For instance, one system may produce syntactically fluent output but misses important information, while another system, although being less fluent, may generate output that covers the meaning perfectly. Nevertheless, human judges often fail to distinguish between these different aspects, which results in highly correlated scores, e.g. (Novikova et al., 2017a). This is one of the reasons why some more recent research adds a general, overall quality criterion (Wen et al., 2015a,b; Manishina et al., 2016; Novikova et al., 2016, 2017a), or even uses only that (Sharma et al., 2016). In the following, we show that discriminative ratings for different aspects can still be obtained, using distinctive task design. Consistency: Previous research has identified a high degree of inconsistency in human judgements of NLG outputs, where ratings often differ significantly (p < 0.001) for the same utterance (Walker et al., 2007). While this might be attributed to individual preferences, e.g. (Walker et al., 2007; Dethlefs et al., 2014), we also show that consistency (as measured by inter-annotator agreement) can be improved by d"
N18-2012,W04-3250,0,0.247456,"Missing"
N18-2012,W14-3301,0,0.189429,"Missing"
N18-2012,J10-4005,0,0.0158458,"of the NLG systems and are asked to evaluate the output with respect to all three aspects in one task. In Setup 2, these aspects are assessed separately, in individual 1 http://www.macs.hw.ac.uk/ InteractionLab/E2E 73 tasks. Furthermore, when crowd workers are asked to assess naturalness, the MR is not shown to them since it is not relevant for the task. Both setups utilise all three data collection methods – Likert scales, plain ME and RankME. The results in Table 2 show that scores are highly correlated for Setup 1. This is in line with previous research in MT (Callison-Burch et al., 2007; Koehn, 2010). Separate collection (Setup 2), however, decreases correlation between naturalness and quality, as well as naturalness and informativeness to very low levels, especially when using ME methods. Nevertheless, informativeness and quality are still highly correlated. We assume that this is due to the fact that raters see the MR in both cases. To obtain more insight into informativeness ratings, we asked crowd workers to further distinguish informativeness in terms of added and missed information with respect to the original MR. Crowd workers were asked to select a checkbox for added information i"
N18-2012,W12-1520,0,0.0702622,"Missing"
N18-2012,P17-1103,0,0.0417809,"ill is a cost-effective alternative for producing overall relative rankings of multiple systems. This framework has the potential to not only significantly influence how NLG evaluation studies are run, but also produce more reliable data for further processing, e.g. for developing more accurate automatic evaluation metrics, which we are currently lacking, e.g. (Novikova et al., 2017a). In current work, we test RankME with a wider range of systems (under submission). We also plan to investigate how this method transfers to related tasks, such as evaluating open-domain dialogue responses, e.g. (Lowe et al., 2017). In addition, we aim to investigate additional NLG evaluation methods, such as extrinsic task contributions, e.g. (Rieser et al., 2014; Gkatzia et al., 2016). missing information Plain ME quality RankME informativeness TrueSkill informativeness Likert quality Likert informativeness Likert naturalness Plain ME naturalness RankME naturalness TrueSkill naturalness Table 4: Results of system ranking using different data collection methods with Setup 2 (different ranks are statistically significant with p < 0.05). 5.1 Conclusion and Discussion Relative comparisons of many outputs While there are c"
N18-2012,W12-2203,0,0.0921963,"tem quality, is a cost-effective alternative for ranking multiple NLG systems. 1 Introduction Human judgement is the primary evaluation criterion for language generation tasks (Gkatzia and Mahamood, 2015). However, limited effort has been made to improve the reliability of these subjective ratings (Gatt and Krahmer, 2017). In this research, we systematically compare and analyse a wide range of alternative experimental designs for eliciting intrinsic user judgements for the task of comparing multiple systems. We draw upon previous studies in language generation, e.g. (Belz and Kow, 2010, 2011; Siddharthan and Katsos, 2012), as well as in the related field of machine translation (MT), e.g. (Bojar et al., 2016, 2017). In particular, we investigate the following challenges: Distinct criteria: Traditionally, NLG outputs are evaluated according to different criteria, such as naturalness and informativeness (Gatt and Krahmer, 2017). Naturalness, also known as fluency or 72 Proceedings of NAACL-HLT 2018, pages 72–78 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Method Likert Plain ME RankME In the following, we show that relative assessment methods produce more consistent a"
N18-2012,L16-1575,0,0.017879,"we want to measure outputs of NLG systems with respect to these distinct criteria, especially for error analysis. For instance, one system may produce syntactically fluent output but misses important information, while another system, although being less fluent, may generate output that covers the meaning perfectly. Nevertheless, human judges often fail to distinguish between these different aspects, which results in highly correlated scores, e.g. (Novikova et al., 2017a). This is one of the reasons why some more recent research adds a general, overall quality criterion (Wen et al., 2015a,b; Manishina et al., 2016; Novikova et al., 2016, 2017a), or even uses only that (Sharma et al., 2016). In the following, we show that discriminative ratings for different aspects can still be obtained, using distinctive task design. Consistency: Previous research has identified a high degree of inconsistency in human judgements of NLG outputs, where ratings often differ significantly (p < 0.001) for the same utterance (Walker et al., 2007). While this might be attributed to individual preferences, e.g. (Walker et al., 2007; Dethlefs et al., 2014), we also show that consistency (as measured by inter-annotator agreemen"
N18-2012,W15-4639,0,0.108126,"Missing"
N18-2012,D15-1199,0,0.0638196,"Missing"
N18-2012,D17-1239,0,0.046893,"Missing"
N18-2012,E14-1074,1,\N,Missing
N18-2012,W07-0718,0,\N,Missing
N18-2012,W15-4708,0,\N,Missing
N18-2012,P16-2008,1,\N,Missing
N18-2012,W16-2301,0,\N,Missing
N18-2012,W17-4717,0,\N,Missing
N18-2012,W13-2305,0,\N,Missing
P13-3023,W02-2103,0,0.0975755,"Missing"
P13-3023,D10-1049,0,0.0782457,"Missing"
P13-3023,A97-1039,0,0.295723,"Missing"
P13-3023,P10-1157,1,0.879414,"Missing"
P13-3023,C10-1012,0,0.376192,"task and even using extreme amounts of automatically annotated data will not generalize beyond the word forms already encountered in the corpus. Hand-written rules can become overly complex and are not easily adaptable for a different language. Therefore, the presented method relies on a statistical approach that learns to predict morphological inflection from annotated data. As a result, such approach is more robust, i.e. capable of generalizing to unseen inputs, and easily portable to different languages. An attempt to implement statistical morphological generation has already been made by Bohnet et al. (2010). However, their morphology generation was only a component of a complex generation system. Therefore, no deep analysis of the capabilities of the methods has been performed. In addition, their method did not attempt to generalize beyond seen inputs. In this paper, we propose Introduction Surface realization is an integral part of all natural language generation (NLG) systems, albeit often implemented in a very simple manner, such as filling words into ready hand-written templates. More sophisticated methods use hand-written grammars (Gatt and Reiter, 2009), possibly in combination with a stat"
P13-3023,J95-4004,0,0.243598,"sults from the evaluation on six different languages from the CoNLL 2009 data sets indicate that the system is able to learn most morphological rules correctly and is able to cope with previously unseen input, performing significantly better than a dictionary learned from the same amount of data. The system is freely available for download at: http://ufal.mff.cuni.cz/~odusek/flect In future, we plan to integrate our generator into a semantic NLG scenario, as well as a simpler template-based system, and evaluate it on further languages. We also consider employing transformation-based learning (Brill, 1995) for prediction to make better use of the possibility of splitting the edit scripts and applying the morphological changes one-by-one. Acknowledgments This research was partly funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221 and core research funding of Charles University in Prague. The authors would like to thank Matˇej Korvas and Martin Popel for helpful comments on the draft and David Marek, Ondˇrej Plátek and Lukáš Žilka for discussions. 7 We get this number when counting the edit scripts as atomic; they divide into 1735 changes"
P13-3023,P08-1059,0,0.0342009,"živateli Jana This is liked by user masculine (name) feminine dative nominative Děkujeme, Jane Nováku, vaše hlasování bylo vytvořeno. Thank you, (name)nominative your poll has been created name left uninﬂected (correct form: vocative) Figure 1: Unnatural language resulting from templates with no inflection. The sentences are taken from the Czech translations of Facebook and Doodle, which use simple templates to generate personalized texts. Corrections to make the text fluent are shown in red. for inflection (Ptáˇcek and Žabokrtský, 2006) or a dictionary learned from automatically tagged data (Toutanova et al., 2008). That gives good results, but reaching sufficient coverage with a hand-made dictionary is a very demanding task and even using extreme amounts of automatically annotated data will not generalize beyond the word forms already encountered in the corpus. Hand-written rules can become overly complex and are not easily adaptable for a different language. Therefore, the presented method relies on a statistical approach that learns to predict morphological inflection from annotated data. As a result, such approach is more robust, i.e. capable of generalizing to unseen inputs, and easily portable to"
P13-3023,W09-0613,0,0.270877,"al generation has already been made by Bohnet et al. (2010). However, their morphology generation was only a component of a complex generation system. Therefore, no deep analysis of the capabilities of the methods has been performed. In addition, their method did not attempt to generalize beyond seen inputs. In this paper, we propose Introduction Surface realization is an integral part of all natural language generation (NLG) systems, albeit often implemented in a very simple manner, such as filling words into ready hand-written templates. More sophisticated methods use hand-written grammars (Gatt and Reiter, 2009), possibly in combination with a statistical reranker (Langkilde and Knight, 1998). Existing NLG systems are very often applied to languages with little morphology, such as English, where a small set of hand-written rules or the direct use of word forms in the symbolic representation or templates is usually sufficient, and so the main focus of these systems lies on syntax and word order. However, this approach poses a problem in languages with a complex morphology. Avoiding inflection, i.e. ensuring that a word lemma will keep its base form at all times, often leads to very unnatural results ("
P13-3023,W08-0325,0,0.323569,"Missing"
P13-3023,W09-1201,0,0.0889624,"Missing"
P13-3023,W04-3250,0,0.101148,"Missing"
P13-3023,P98-1116,0,0.0851261,"orphology generation was only a component of a complex generation system. Therefore, no deep analysis of the capabilities of the methods has been performed. In addition, their method did not attempt to generalize beyond seen inputs. In this paper, we propose Introduction Surface realization is an integral part of all natural language generation (NLG) systems, albeit often implemented in a very simple manner, such as filling words into ready hand-written templates. More sophisticated methods use hand-written grammars (Gatt and Reiter, 2009), possibly in combination with a statistical reranker (Langkilde and Knight, 1998). Existing NLG systems are very often applied to languages with little morphology, such as English, where a small set of hand-written rules or the direct use of word forms in the symbolic representation or templates is usually sufficient, and so the main focus of these systems lies on syntax and word order. However, this approach poses a problem in languages with a complex morphology. Avoiding inflection, i.e. ensuring that a word lemma will keep its base form at all times, often leads to very unnatural results (see Figure 1). Some generators use a hand-made morphological dictionary 158 Procee"
P13-3023,C98-1112,0,\N,Missing
P15-1044,D10-1049,0,0.617535,"enerator From Unaligned Data Ondˇrej Dušek and Filip Jurˇcíˇcek Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostranské námˇestí 25, CZ-11800 Prague, Czech Republic {odusek,jurcicek}@ufal.mff.cuni.cz Abstract part in this work, and reuse an existing rule-based surface realizer to test the capabilities of the generator in an end-to-end setting. Current NLG systems usually require a separate training data alignment step (Mairesse et al., 2010; Konstas and Lapata, 2013). Many of them use a CFG or operate in a phrase-based fashion (Angeli et al., 2010; Mairesse et al., 2010), which limits their ability to capture long-range syntactic dependencies. Our generator includes alignment learning into sentence planner training and uses deep-syntactic trees with a rule-based surface realization step, which ensures grammatical correctness of the outputs. Unlike previous approaches to trainable sentence planning (e.g., Walker et al. (2001); Stent et al. (2004)), our generator does not require a handcrafted base sentence planner. This paper is structured as follows: in Section 2, we describe the architecture of our generator. Sections 3 and 4 then pro"
P15-1044,W14-4416,0,0.0250927,"cally include the alignment of input MR elements to output words in a separate preprocessing step (Wong and Mooney, 2007; Angeli et al., 2010), or require pre-aligned training data (Mairesse et al., 2010; Dethlefs et al., 2013). In addition, their basic algorithm often requires a specific input MR format, e.g., a tree (Wong and Mooney, 2007; Lu et al., 2009) or a flat database (Angeli et al., 2010; Konstas and Lapata, 2013; Mairesse et al., 2010). While dependency-based deep syntax has been used previously in statistical NLG, the approaches known to us (Bohnet et al., 2010; Belz et al., 2012; Ballesteros et al., 2014) focus only on the surface realization step and do not include a sentence plan458 ner, whereas our work is mainly focused on statistical sentence planning and uses a rule-based realizer. Our approach to sentence planning is most similar to Zettlemoyer and Collins (2007), which use a candidate generator and a perceptron ranker for CCG parsing. Apart from proceeding in the inverse direction and using dependency trees, we use only very generic rules in our candidate generator instead of language-specific ones, and we incorporate differing subtree updates and future promise estimation into our ran"
P15-1044,W12-1525,0,0.0184826,"int approaches typically include the alignment of input MR elements to output words in a separate preprocessing step (Wong and Mooney, 2007; Angeli et al., 2010), or require pre-aligned training data (Mairesse et al., 2010; Dethlefs et al., 2013). In addition, their basic algorithm often requires a specific input MR format, e.g., a tree (Wong and Mooney, 2007; Lu et al., 2009) or a flat database (Angeli et al., 2010; Konstas and Lapata, 2013; Mairesse et al., 2010). While dependency-based deep syntax has been used previously in statistical NLG, the approaches known to us (Bohnet et al., 2010; Belz et al., 2012; Ballesteros et al., 2014) focus only on the surface realization step and do not include a sentence plan458 ner, whereas our work is mainly focused on statistical sentence planning and uses a rule-based realizer. Our approach to sentence planning is most similar to Zettlemoyer and Collins (2007), which use a candidate generator and a perceptron ranker for CCG parsing. Apart from proceeding in the inverse direction and using dependency trees, we use only very generic rules in our candidate generator instead of language-specific ones, and we incorporate differing subtree updates and future prom"
P15-1044,C10-1012,0,0.0242726,"like our work, the joint approaches typically include the alignment of input MR elements to output words in a separate preprocessing step (Wong and Mooney, 2007; Angeli et al., 2010), or require pre-aligned training data (Mairesse et al., 2010; Dethlefs et al., 2013). In addition, their basic algorithm often requires a specific input MR format, e.g., a tree (Wong and Mooney, 2007; Lu et al., 2009) or a flat database (Angeli et al., 2010; Konstas and Lapata, 2013; Mairesse et al., 2010). While dependency-based deep syntax has been used previously in statistical NLG, the approaches known to us (Bohnet et al., 2010; Belz et al., 2012; Ballesteros et al., 2014) focus only on the surface realization step and do not include a sentence plan458 ner, whereas our work is mainly focused on statistical sentence planning and uses a rule-based realizer. Our approach to sentence planning is most similar to Zettlemoyer and Collins (2007), which use a candidate generator and a perceptron ranker for CCG parsing. Apart from proceeding in the inverse direction and using dependency trees, we use only very generic rules in our candidate generator instead of language-specific ones, and we incorporate differing subtree upda"
P15-1044,P02-1034,0,0.105551,"a function that maps global features from the whole sentence plan tree t and the input MR m to a real-valued score that describes the fitness of t in the context of m. We first describe the basic version of the scorer and then our two improvements – differing subtree updates and future promise estimation. t0top = t0gold = tc for i in 1, . . . min{|ttop |− |tc |, |tgold |− |tc |} − 1 : titop = ti−1 top + node(s) from ttop i−1 i tgold = tgold + node(s) from tgold w = w + α · (feat(tigold , m) − feat(titop , m)) Basic perceptron scorer The basic scorer is based on the linear perceptron ranker of Collins and Duffy (2002), where the score is computed as a simple dot product of the features and the corresponding weight vector: Future promise estimation To further improve scoring of incomplete sentence plan trees, we incorporate a simple future promise estimation for the A* search intended to boost scores of sentence plans that are expected to further grow.5 It is based on the expected number of children Ec (n) of different node types (lemmaformeme pairs).6 Given all nodes n1 . . . n|t |in a score(t, m) = w> · feat(t, m) In the training phase, the weights w are initialized to one. For each input MR, the system t"
P15-1044,W02-1001,0,0.0550204,"forms – additional verbal nodes are added for verbal particles (infinitive or phrasal verbs) and for compound expressions of tense, mood, and modality. • Grammatical words – prepositions, subordinating conjunctions, negation particles, articles, and other grammatical words are added into the sentence. Averaging weights and parallel training To speed up training using parallel processing, we use the iterative parameter mixing approach of McDonald et al. (2010), where training data are split into several parts and weight updates are averaged after each pass through the training data. Following Collins (2002), we record the weights after each training pass, take an average at the end, and use this as the final weights for prediction. 4 • Punctuation – nodes for commas, final punctuation, quotes, and brackets are introduced. • Word Inflection – words are inflected according to the information from formemes and agreement. • Phonetic changes – English “a” becomes “an” based on the following word. Surface Realizer We use the English surface realizer from the Treex NLP toolkit (cf. Section 2 and (Ptáˇcek, 2008)). It is a simple pipeline of mostly rule-based blocks that gradually change the deep-syntact"
P15-1044,P13-1123,0,0.316506,"ated when training on the full set and using differing subtree updates and future promise estimation. 7 Related Work mar estimated from database records and their descriptions. Mairesse et al. (2010) convert input DAs into “semantic stacks”, which correspond to natural language phrases and contain slots and their values on top of each other. Their generation model uses two dynamic Bayesian networks: the first one performs an ordering of the input semantic stacks, inserting intermediary stacks which correspond to grammatical phrases, the second one then produces a concrete surface realization. Dethlefs et al. (2013) approach generation as a sequence labeling task and use a conditional random field classifier, assigning a word or a phrase to each input MR element. Previous trainable methods in sentence planning use in principle two techniques: First, in the overgeneration and ranking approach (Walker et al., 2001; Stent et al., 2004), many sentence plans are generated using a rule-based planner and then the best one is selected by a statistical ranker. Second, parameter optimization trains adjustable parameters of a handcrafted generator to produce outputs with desired properties (Paiva and Evans, 2005; M"
P15-1044,H05-1066,0,0.0611498,"Missing"
P15-1044,N10-1069,0,0.0153818,"on process, but it is disregarded for the termination criterion in the Stop step of the generation algorithm and in perceptron weight updates. • Compound verb forms – additional verbal nodes are added for verbal particles (infinitive or phrasal verbs) and for compound expressions of tense, mood, and modality. • Grammatical words – prepositions, subordinating conjunctions, negation particles, articles, and other grammatical words are added into the sentence. Averaging weights and parallel training To speed up training using parallel processing, we use the iterative parameter mixing approach of McDonald et al. (2010), where training data are split into several parts and weight updates are averaged after each pass through the training data. Following Collins (2002), we record the weights after each training pass, take an average at the end, and use this as the final weights for prediction. 4 • Punctuation – nodes for commas, final punctuation, quotes, and brackets are introduced. • Word Inflection – words are inflected according to the information from formemes and agreement. • Phonetic changes – English “a” becomes “an” based on the following word. Surface Realizer We use the English surface realizer from"
P15-1044,W12-3132,1,0.90642,"Missing"
P15-1044,J03-1002,0,0.00855206,"structures of the new MR instead of SVPs. While especially Rules 5 and 6 exclude a vast number of potential candidate trees, this limitation is still much weaker than using hard alignment links between the elements of the MR and the output words or phrases. It leaves enough room to generate many combinations unseen in the training data (cf. Section 6) while keeping the search space manageable. To limit the space of potential tree candidates even further, one could also use automatic alignment scores between the elements of the input MR and the tree nodes (obtained using a tool such as GIZA++ (Och and Ney, 2003)). 3.2 Differing subtree updates In the basic version described above, the scorer is trained to score full sentence plan trees. However, it is also used to score incomplete sentence plans during the decoding. This leads to a bias towards bigger trees regardless of their fitness for the input MR. Therefore, we introduced a novel modification of the perceptron updates to improve scoring of incomplete sentence plans: In addition to updating the weights using the top-scoring candidate ttop and the gold-standard tree tgold (see above), we also use their differing subtrees titop , tigold for additio"
P15-1044,W01-1408,0,0.0611303,". inform(name=X, type=placetoeat, eattype=restaurant, area=riverside, food=Italian) Sentence candidate generator planner A* search expand candidate sentence plan tree into new candidates scorer score candidates to select next one to be expanded t-tree be v:fin X-name n:subj restaurant n:obj italian adj:attr Surface realizer sentence plan (deep syntax tree) river n:by+X Agreement Word ordering Compound verb forms Grammatical words mostly rule-based pipeline (from Treex NLP toolkit) 3 Punctuation Sentence Planner The sentence planner is based on a variant of the A* algorithm (Hart et al., 1968; Och et al., 2001; Koehn et al., 2003). It starts from an empty sentence plan tree and tries to find a path to the optimal sentence plan by iteratively adding nodes. It keeps two sets of hypotheses, i.e., candidate sentence plan trees, sorted by their score – hypotheses to expand (open set) and already expanded (closed set). It uses the following two subcomponents to guide the search: Word Inflection Phonetic changes plain text sentence X is an italian restaurant by the river. Figure 1: Overall structure of our generator Each node has a lemma and a formeme – a concise description of its surface morphosyntactic"
P15-1044,P05-1008,0,0.123583,". Dethlefs et al. (2013) approach generation as a sequence labeling task and use a conditional random field classifier, assigning a word or a phrase to each input MR element. Previous trainable methods in sentence planning use in principle two techniques: First, in the overgeneration and ranking approach (Walker et al., 2001; Stent et al., 2004), many sentence plans are generated using a rule-based planner and then the best one is selected by a statistical ranker. Second, parameter optimization trains adjustable parameters of a handcrafted generator to produce outputs with desired properties (Paiva and Evans, 2005; Mairesse and Walker, 2008). As opposed to our approach, both methods require an existing handcrafted sentence planner. Other previous works combine sentence planning and surface realization into a single step and do not require a handcrafted base module. Wong and Mooney (2007) experiment with a phrasebased machine translation system, comparing and combining it with an inverted semantic parser based on synchronous context-free grammars. Lu et al. (2009) use tree conditional random fields over hybrid trees that combine natural language phrases with formal semantic expressions. Angeli et al. (2"
P15-1044,N03-1017,0,0.011527,"ype=placetoeat, eattype=restaurant, area=riverside, food=Italian) Sentence candidate generator planner A* search expand candidate sentence plan tree into new candidates scorer score candidates to select next one to be expanded t-tree be v:fin X-name n:subj restaurant n:obj italian adj:attr Surface realizer sentence plan (deep syntax tree) river n:by+X Agreement Word ordering Compound verb forms Grammatical words mostly rule-based pipeline (from Treex NLP toolkit) 3 Punctuation Sentence Planner The sentence planner is based on a variant of the A* algorithm (Hart et al., 1968; Och et al., 2001; Koehn et al., 2003). It starts from an empty sentence plan tree and tries to find a path to the optimal sentence plan by iteratively adding nodes. It keeps two sets of hypotheses, i.e., candidate sentence plan trees, sorted by their score – hypotheses to expand (open set) and already expanded (closed set). It uses the following two subcomponents to guide the search: Word Inflection Phonetic changes plain text sentence X is an italian restaurant by the river. Figure 1: Overall structure of our generator Each node has a lemma and a formeme – a concise description of its surface morphosyntactic form, which may incl"
P15-1044,P02-1040,0,0.0953561,"Missing"
P15-1044,W04-3250,0,0.109058,"Missing"
P15-1044,P09-1011,0,0.176425,"Missing"
P15-1044,D09-1042,0,0.0247168,"nker. Second, parameter optimization trains adjustable parameters of a handcrafted generator to produce outputs with desired properties (Paiva and Evans, 2005; Mairesse and Walker, 2008). As opposed to our approach, both methods require an existing handcrafted sentence planner. Other previous works combine sentence planning and surface realization into a single step and do not require a handcrafted base module. Wong and Mooney (2007) experiment with a phrasebased machine translation system, comparing and combining it with an inverted semantic parser based on synchronous context-free grammars. Lu et al. (2009) use tree conditional random fields over hybrid trees that combine natural language phrases with formal semantic expressions. Angeli et al. (2010) generate text from database records through a sequence of classifiers, gradually selecting database records, fields, and corresponding textual realizations to describe them. Konstas and Lapata (2013) recast the whole NLG problem as parsing over a probabilistic context-free gramUnlike our work, the joint approaches typically include the alignment of input MR elements to output words in a separate preprocessing step (Wong and Mooney, 2007; Angeli et a"
P15-1044,P08-1020,0,0.0112674,") approach generation as a sequence labeling task and use a conditional random field classifier, assigning a word or a phrase to each input MR element. Previous trainable methods in sentence planning use in principle two techniques: First, in the overgeneration and ranking approach (Walker et al., 2001; Stent et al., 2004), many sentence plans are generated using a rule-based planner and then the best one is selected by a statistical ranker. Second, parameter optimization trains adjustable parameters of a handcrafted generator to produce outputs with desired properties (Paiva and Evans, 2005; Mairesse and Walker, 2008). As opposed to our approach, both methods require an existing handcrafted sentence planner. Other previous works combine sentence planning and surface realization into a single step and do not require a handcrafted base module. Wong and Mooney (2007) experiment with a phrasebased machine translation system, comparing and combining it with an inverted semantic parser based on synchronous context-free grammars. Lu et al. (2009) use tree conditional random fields over hybrid trees that combine natural language phrases with formal semantic expressions. Angeli et al. (2010) generate text from data"
P15-1044,W07-1709,0,0.0167969,"Missing"
P15-1044,P04-1011,0,0.432495,"ting. Current NLG systems usually require a separate training data alignment step (Mairesse et al., 2010; Konstas and Lapata, 2013). Many of them use a CFG or operate in a phrase-based fashion (Angeli et al., 2010; Mairesse et al., 2010), which limits their ability to capture long-range syntactic dependencies. Our generator includes alignment learning into sentence planner training and uses deep-syntactic trees with a rule-based surface realization step, which ensures grammatical correctness of the outputs. Unlike previous approaches to trainable sentence planning (e.g., Walker et al. (2001); Stent et al. (2004)), our generator does not require a handcrafted base sentence planner. This paper is structured as follows: in Section 2, we describe the architecture of our generator. Sections 3 and 4 then provide further details on its main components. In Section 5, we describe our experiments on the BAGEL data set, followed by an analysis of the results in Section 6. Section 7 compares our generator to previous related works and Section 8 concludes the paper. We present a novel syntax-based natural language generation system that is trainable from unaligned pairs of input meaning representations and output"
P15-1044,N01-1003,0,0.471534,"r in an end-to-end setting. Current NLG systems usually require a separate training data alignment step (Mairesse et al., 2010; Konstas and Lapata, 2013). Many of them use a CFG or operate in a phrase-based fashion (Angeli et al., 2010; Mairesse et al., 2010), which limits their ability to capture long-range syntactic dependencies. Our generator includes alignment learning into sentence planner training and uses deep-syntactic trees with a rule-based surface realization step, which ensures grammatical correctness of the outputs. Unlike previous approaches to trainable sentence planning (e.g., Walker et al. (2001); Stent et al. (2004)), our generator does not require a handcrafted base sentence planner. This paper is structured as follows: in Section 2, we describe the architecture of our generator. Sections 3 and 4 then provide further details on its main components. In Section 5, we describe our experiments on the BAGEL data set, followed by an analysis of the results in Section 6. Section 7 compares our generator to previous related works and Section 8 concludes the paper. We present a novel syntax-based natural language generation system that is trainable from unaligned pairs of input meaning repre"
P15-1044,N07-1022,0,0.173786,"ration and ranking approach (Walker et al., 2001; Stent et al., 2004), many sentence plans are generated using a rule-based planner and then the best one is selected by a statistical ranker. Second, parameter optimization trains adjustable parameters of a handcrafted generator to produce outputs with desired properties (Paiva and Evans, 2005; Mairesse and Walker, 2008). As opposed to our approach, both methods require an existing handcrafted sentence planner. Other previous works combine sentence planning and surface realization into a single step and do not require a handcrafted base module. Wong and Mooney (2007) experiment with a phrasebased machine translation system, comparing and combining it with an inverted semantic parser based on synchronous context-free grammars. Lu et al. (2009) use tree conditional random fields over hybrid trees that combine natural language phrases with formal semantic expressions. Angeli et al. (2010) generate text from database records through a sequence of classifiers, gradually selecting database records, fields, and corresponding textual realizations to describe them. Konstas and Lapata (2013) recast the whole NLG problem as parsing over a probabilistic context-free"
P15-1044,D07-1071,0,0.0175933,"ften requires a specific input MR format, e.g., a tree (Wong and Mooney, 2007; Lu et al., 2009) or a flat database (Angeli et al., 2010; Konstas and Lapata, 2013; Mairesse et al., 2010). While dependency-based deep syntax has been used previously in statistical NLG, the approaches known to us (Bohnet et al., 2010; Belz et al., 2012; Ballesteros et al., 2014) focus only on the surface realization step and do not include a sentence plan458 ner, whereas our work is mainly focused on statistical sentence planning and uses a rule-based realizer. Our approach to sentence planning is most similar to Zettlemoyer and Collins (2007), which use a candidate generator and a perceptron ranker for CCG parsing. Apart from proceeding in the inverse direction and using dependency trees, we use only very generic rules in our candidate generator instead of language-specific ones, and we incorporate differing subtree updates and future promise estimation into our ranker. 8 pendency trees can be converted in a languageindependent way. Acknowledgments This work was funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221 and core research funding, SVV project 260 104, and GAUK gran"
P15-1044,E09-1078,0,\N,Missing
P15-1044,H94-1010,0,\N,Missing
P15-1044,P10-1157,1,\N,Missing
P16-2008,W15-3009,1,0.863083,"Missing"
P16-2008,P15-1001,0,0.0154166,"rk reranker; Wen et al. (2015b) later replaced basic sigmoid cells with an LSTM. Mei et al. (2015) present the only seq2seq-based NLG system known to us. We extend the previous works by generating deep syntax trees as well as strings and directly comparing pipeline and joint generation. In addition, we experiment with an order-of-magnitude smaller dataset than other RNN-based systems. 7 We intend to apply it to other datasets for a broader comparison, and we plan further improvements, such as enhancing the reranker or including a bidirectional encoder (Bahdanau et al., 2015; Mei et al., 2015; Jean et al., 2015) and sequence level training (Ranzato et al., 2015). Conclusions and Future Work We have presented a direct comparison of two-step generation via deep syntax trees with a direct generation into strings, both using the same NLG system based on the seq2seq approach. While both approaches offer decent performance, their outputs are quite different. The results show the direct approach as more favorable, with significantly higher n-gram based scores and a similar number of semantic errors in the output. We also showed that our generator can learn to produce meaningful utterances using a much small"
P16-2008,D10-1049,0,0.0381356,"Missing"
P16-2008,W04-3250,0,0.0948379,"Missing"
P16-2008,D14-1179,0,0.00553056,"Missing"
P16-2008,P10-1157,1,0.891105,"Missing"
P16-2008,P13-1123,0,0.0635573,"Missing"
P16-2008,P05-1008,0,0.0441693,"er. The best results of both setups surpass the best results on this dataset using training data without manual alignments (Duˇsek and Jurˇc´ıcˇ ek, 2015) in both automatic metrics12 and the number of semantic errors. 6 Related Work While most recent NLG systems attempt to learn generation from data, the choice of a particular approach – pipeline or joint – is often arbitrary and depends on system architecture or particular generation domain. Works using the pipeline approach in SDS tend to focus on sentence planning, improving a handcrafted generator (Walker et al., 2001; Stent et al., 2004; Paiva and Evans, 2005) or using perceptron-guided A* search (Duˇsek and Jurˇc´ıcˇ ek, 2015). Generators taking the joint approach employ various methods, e.g., factored language models (Mairesse et al., 2010), inverted parsing (Wong and Mooney, 2007; Konstas and Lapata, 2013), or a pipeline of discriminative classifiers (Angeli et al., 2010). Unlike most previous 9 The average sentence length is around 13 tokens. The generated sequences are longer, but have a very rigid structure, i.e., less uncertainty per generation step. The average output length is around 36 tokens in the generated sequence or 9 tree nodes; sur"
P16-2008,P15-1044,1,0.848163,"Missing"
P16-2008,P02-1040,0,0.110262,"ce of hidden states h = {h1 , . . . , hn } where ht = lstm(yt , ht−1 ). The output binary vector o is computed as: oi = sigmoid((hn · WR + b)i ) Here, WR is a learned projection matrix and b is a corresponding bias term. 4 Experiments 3 The input vocabulary size is around 45 (DA types, slots, and values added up) and output vocabulary sizes are around 170 for string generation and 180 for tree generation (45 formemes and 135 lemmas). 4 We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two references to measure BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) on the validation and test sets. 5 Based on a few preliminary experiments, the learning rate is set to 0.001, embedding size 50, LSTM cell size 128, and batch size 20. Reranking penalty for decoding is 100. 6 Training is terminated early if the top 10 so far achieved validation BLEU scores do not change for 100 passes. 7 We use the same settings as with the seq2seq generator. 8 The validation set is given 10 times more importance. We perform our experiments on the BAGEL data set of Mairesse et al. (2010), which contains 202 DA from the restaurant information domain with two"
P16-2008,W12-3132,1,0.89863,"Missing"
P16-2008,P10-1103,0,0.0189757,"cores while providing more relevant outputs. 1 Introduction In spoken dialogue systems (SDS), the task of natural language generation (NLG) is to convert a meaning representation (MR) produced by the dialogue manager into one or more sentences in a natural language. It is traditionally divided into two subtasks: sentence planning, which decides on the overall sentence structure, and surface realization, determining the exact word forms and linearizing the structure into a string (Reiter and Dale, 2000). While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013). We present a new, conceptually simple NLG system for SDS that is able to operate in both modes: it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer (Duˇsek et al., 2015). This allows us to show a direct comparison of two-step generation, 2 Generator Setting The input to our generator are dialogue acts (DA) (Young et al., 2010) representing an action, such as inform or request, a"
P16-2008,P04-1011,0,0.882528,"rˇc´ıcˇ ek Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostransk´e n´amˇest´ı 25, CZ-11800 Prague, Czech Republic {odusek,jurcicek}@ufal.mff.cuni.cz Abstract where sentence planning and surface realization are separated, with a joint, one-step approach. Our generator is based on the sequence-tosequence (seq2seq) generation technique (Cho et al., 2014; Sutskever et al., 2014), combined with beam search and an n-best list reranker to suppress irrelevant information in the outputs. Unlike most previous NLG systems for SDS (e.g., (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it is trainable from unaligned pairs of MR and sentences alone. We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (Wen et al., 2015b; Mei et al., 2015), and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset (Mairesse et al., 2010). It is able to surpass n-gram-based scores achieved previously by Duˇsek and Jurˇc´ıcˇ ek (2015), offering a simpler setup and more relevant outputs. We introduce the generatio"
P16-2008,N01-1003,0,0.602443,"ards to ngram-based scores while providing more relevant outputs. 1 Introduction In spoken dialogue systems (SDS), the task of natural language generation (NLG) is to convert a meaning representation (MR) produced by the dialogue manager into one or more sentences in a natural language. It is traditionally divided into two subtasks: sentence planning, which decides on the overall sentence structure, and surface realization, determining the exact word forms and linearizing the structure into a string (Reiter and Dale, 2000). While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013). We present a new, conceptually simple NLG system for SDS that is able to operate in both modes: it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer (Duˇsek et al., 2015). This allows us to show a direct comparison of two-step generation, 2 Generator Setting The input to our generator are dialogue acts (DA) (Young et al., 2010) representing an action, such as"
P16-2008,W15-4639,0,0.596541,"planning and surface realization are separated, with a joint, one-step approach. Our generator is based on the sequence-tosequence (seq2seq) generation technique (Cho et al., 2014; Sutskever et al., 2014), combined with beam search and an n-best list reranker to suppress irrelevant information in the outputs. Unlike most previous NLG systems for SDS (e.g., (Stent et al., 2004; Raux et al., 2005; Mairesse et al., 2010)), it is trainable from unaligned pairs of MR and sentences alone. We experiment with using much less training data than recent systems based on recurrent neural networks (RNN) (Wen et al., 2015b; Mei et al., 2015), and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset (Mairesse et al., 2010). It is able to surpass n-gram-based scores achieved previously by Duˇsek and Jurˇc´ıcˇ ek (2015), offering a simpler setup and more relevant outputs. We introduce the generation setting in Section 2 and describe our generator architecture in Section 3. Section 4 details our experiments, Section 5 analyzes the results. We summarize related work in Section 6 and offer conclusions in Section 7. We present a natur"
P16-2008,D15-1199,0,0.61229,"Missing"
P16-2008,N07-1022,0,0.0485113,"SDS), the task of natural language generation (NLG) is to convert a meaning representation (MR) produced by the dialogue manager into one or more sentences in a natural language. It is traditionally divided into two subtasks: sentence planning, which decides on the overall sentence structure, and surface realization, determining the exact word forms and linearizing the structure into a string (Reiter and Dale, 2000). While some generators keep this division and use a two-step pipeline (Walker et al., 2001; Rieser et al., 2010; Dethlefs et al., 2013), others apply a joint model for both tasks (Wong and Mooney, 2007; Konstas and Lapata, 2013). We present a new, conceptually simple NLG system for SDS that is able to operate in both modes: it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer (Duˇsek et al., 2015). This allows us to show a direct comparison of two-step generation, 2 Generator Setting The input to our generator are dialogue acts (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values. Our generator operates in two m"
uresova-etal-2014-multilingual,E12-1012,0,\N,Missing
uresova-etal-2014-multilingual,W12-3102,0,\N,Missing
uresova-etal-2014-multilingual,P02-1040,0,\N,Missing
uresova-etal-2014-multilingual,P07-2045,0,\N,Missing
uresova-etal-2014-multilingual,W04-3250,0,\N,Missing
uresova-etal-2014-multilingual,P03-1021,0,\N,Missing
uresova-etal-2014-multilingual,2011.iwslt-papers.5,0,\N,Missing
W12-3132,hajic-etal-2012-announcing,1,0.802661,"Missing"
W12-3132,W04-3250,0,0.264493,"Missing"
W12-3132,W06-1606,0,0.0610868,"Missing"
W12-3132,W10-1730,1,0.899159,"Missing"
W12-3132,W01-1406,0,0.0190747,"actic description, mainly within valency lexicons, starting probably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysi"
W12-3132,P02-1040,0,0.0829475,". 6.1 Czech Synthesis The synthesis phase of the TectoMT system relies heavily on the information included in formemes, as its rule-based blocks use solely formemes and grammar rules to gradually change a deep tree node into a surface subtree. To directly measure the suitability of our changes for the synthesis stage of the TectoMT system, we used a Czech-to-Czech round trip—deep analysis of Czech PDT 2.0 development set sentences using the CzEng 1.0 pipeline (Bojar et al., 2012b), followed directly by the synthesis part of the TectoMT system. The results were evaluated using the BLEU metric (Papineni et al., 2002) with the original sentences as reference; they indicate a higher suitability of the new formemes for deep Czech synthesis (see Table 2). 6.2 Version Original formemes Revised formemes BLEU 0.6818 0.7092 Table 2: A comparison of formeme versions in Czech-toCzech round trip. Version Original formemes Revised formemes BLEU 0.1190 0.1199 Table 3: A comparison of formeme versions in Englishto-Czech TectoMT translation on the WMT12 test set. two translation scenarios—one using the original formemes and the second using the revised formemes in the formeme-to-formeme translation model. Due to time re"
W12-3132,W11-2153,1,0.890651,"Missing"
W12-3132,P05-1034,0,0.0232767,"robably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory"
W12-3132,W08-0325,1,0.921231,"Missing"
W12-3132,P09-2037,1,0.875964,"oMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory (Sgall, 1967; Sgall et al., 1986), proceeding over two layers of structural description, from shallow (analytical) to deep (tectogrammatical) (see Section 3.1). The transfer phase of the system is based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models (Žabokrtský and Popel, 2009). It is factorized into three subtasks: lemma, formeme and grammatemes translation (see Sections 3.2 and 3.3). The subsequent generation phase consists of rulebased components that gradually change the deep target language representation into a shallow one, which is then converted to text (cf. Section 6.1). The version of TectoMT submitted to WMT122 builds upon the WMT11 version. Several rule-based components were slightly refined. However, most of the effort was devoted to creating a better and bigger parallel treebank—CzEng 1.03 (Bojar et al., 2012b), and re-training the statistical componen"
W12-3132,W09-0422,1,\N,Missing
W12-3132,2001.mtsummit-ebmt.4,0,\N,Missing
W12-3132,bojar-etal-2012-joy,1,\N,Missing
W12-3146,hajic-etal-2012-announcing,0,0.0154183,"Missing"
W12-3146,W11-2103,0,0.036468,"ced the rule set used by the original DEPFIX system and measured the performance of the individual rules. We also modified the dependency parser of McDonald et al. (2005) in two ways to adjust it for the parsing of MT outputs. We show that our system is able to improve the quality of the state-of-the-art MT systems. 1 Introduction The today’s outputs of Machine Translation (MT) often contain serious grammatical errors. This is particularly apparent in statistical MT systems (SMT), which do not employ structural linguistic rules. These systems have been dominating the area in the recent years (Callison-Burch et al., 2011). Such errors make the translated text less fluent and may even lead to unintelligibility or misleading statements. The problem is more evident in languages with rich morphology, such as Czech, where morphological agreement is of a relatively high importance for the interpretation of syntactic relations. The DEPFIX system (Mareˇcek et al., 2011) attempts to correct some of the frequent SMT sys∗ This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n◦ 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH120"
W12-3146,2009.mtsummit-commercial.6,0,0.0396722,"Missing"
W12-3146,P07-2045,0,0.00498126,"atures computed over its aligned source word, if there is one. To address the differences between the gold standard training data and SMT outputs, we “worsen” the treebank used to train the parser, i.e. introduce errors similar to those found in target sentences: The trees retain their correct structure, only the word forms are modified to resemble SMT output. We have computed a “part-of-speech tag error model” on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al., 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al., 2007) and estimating the Maximum Likelihood probabilities of errors for each part-ofspeech tag. We then applied this error model to the Czech PCEDT 2.0 sentences and used the resulting “worsened” treebank to train the parser. 4 Rules 2012 uses 20 hand-written rules, addressing various frequent errors in MT output. Each rule takes an analyzed target sentence as its input, often together with its analyzed source senDEPFIX 2 http://ufal.mff.cuni.cz/treex 363 tence, and attempts to correct any errors found – usually by changing morphosyntactic categories of a word (such as number, gender, case, person"
W12-3146,N09-2055,0,0.298447,"Missing"
W12-3146,H05-1066,0,0.00799007,"Missing"
W12-3146,J03-1002,0,0.0030145,"ors induced by this type of MT systems, it can be applied to virtually any MT system (Mareˇcek et al., 2011). 362 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 362–368, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics TectoMT/Treex NLP framework (Popel and ˇ Zabokrtsk´ y, 2010),2 using the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005) trained on the CoNLL 2007 Shared Task English data (Nivre et al., 2007) to analyze the source sentences. The source and target sentences are aligned using GIZA++ (Och and Ney, 2003). 3 Parsing The DEPFIX 2011 system used the MST parser (McDonald et al., 2005) with an improved feature set ˇ for Czech (Nov´ak and Zabokrtsk´ y, 2007) trained on the Prague Dependency Treebank (PDT) 2.0 (Hajiˇc and others, 2006) to analyze the target sentences. DEPFIX 2012 uses a reimplementation of the MST parser capable of utilizing parallel features from the source side in the parsing of the target sentence. The source text is usually grammatical and therefore is likely to be analyzed more reliably. The source structure obtained in this way can then provide hints for the target parser. We"
W12-3146,P02-1040,0,0.104954,"Missing"
W12-3146,W07-1709,0,0.0676744,"Missing"
W12-3146,W11-2152,1,\N,Missing
W12-3146,N07-1064,0,\N,Missing
W12-3146,D07-1096,0,\N,Missing
W12-4205,hajic-etal-2012-announcing,0,0.0852801,"Missing"
W12-4205,W06-2920,0,0.0367148,"targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in"
W12-4205,N10-1015,0,0.0755019,"011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,W10-1703,0,0.0245956,"parser training data, so that the training sentences resemble SMT output. We evaluate the modified parser on DEP FIX , a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality. 1 Introduction The machine translation (MT) quality is on a steady rise, with mostly statistical systems (SMT) dominating the area (Callison-Burch et al., 2010; CallisonBurch et al., 2011). Most MT systems do not employ structural linguistic knowledge and even the stateof-the-art MT solutions are unable to avoid making serious grammatical errors in the output, which often leads to unintelligibility or to a risk of misinterpretations of the text by a reader. ∗ This research has been supported by the EU Seventh Framework Programme under grant agreement n◦ 247762 (Faust), and by the grants GAUK116310 and GA201/09/H057. This problem is particularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the"
W12-4205,W11-2103,0,0.0450256,"Missing"
W12-4205,P10-1003,0,0.0232916,"ek et al., 2011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,P99-1065,0,0.316623,"Missing"
W12-4205,P08-2056,0,0.0358,"Missing"
W12-4205,D09-1127,0,0.0558467,"Missing"
W12-4205,P07-2045,0,0.0114404,"g one, creating the respective alignment link from word A (in the reference) to word B (in the SMT output) and deleting all scores of links from A or to B, so that one-to-one alignments are enforced. The process is terminated when no links with a score higher than a given threshold are available; some words may thus remain unaligned. The score is computed as a linear combination of the following four features: • word form (or lemma if available) similarity based on Jaro-Winkler distance (Winkler, 1990), 1. We translated the English side of PCEDT5 to Czech using SMT (we chose the Moses system (Koehn et al., 2007) for our experiments) and tagged the resulting translations using the Morˇce tagger (Spoustov´a et al., 2007). • fine-grained morphological tag similarity, • similarity of the relative position in the sentence, 2. We aligned the Czech side of PCEDT, now serving as a reference translation, to the SMT output using our Monolingual Greedy Aligner (see Section 4.2). 3. Collecting the counts of individual errors, we estimated the Maximum Likelihood probabilities of changing a correct fine-grained morphological tag (of a word from the reference) into a possibly incorrect fine-grained morphological ta"
W12-4205,J93-2004,0,0.0469362,"er. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR"
W12-4205,H05-1066,0,0.30865,"Missing"
W12-4205,W06-2932,0,0.0233358,"entence first and include features computed over the parsed source sentence in the set of features used for parsing SMT output. We first align the source and SMT output sentences on the word level and then use alignment-wise local features – i.e. for each SMT output word, we add features computed over its aligned source word, if applicable (cf. Section 3.4 for a listing). 3.2 Parsers Used We have reimplemented the MST parser (McDonald et al., 2005) in order to provide for a simple insertion of the parallel features into the models. We also used the original implementation of the MST parser by McDonald et al. (2006) for comparison in our experiments. To distinguish the two variants used, we denote the original MST parser as M C D parser,2 and the new reimplementation as RUR parser. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the"
W12-4205,D11-1006,0,0.0305903,"010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and"
W12-4205,J03-1002,0,0.0141147,"IRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR parser follows those described by McDonald et al. (2005). For parsing, we use the features described below. The individual features are computed for both the parent node and the child node of an edge and conjoined in various ways. The coarse morphological tag and lemma are provided by the Morˇce tagger (Spoustov´a"
W12-4205,P02-1040,0,0.0832465,"Missing"
W12-4205,W07-1709,0,0.0974333,"Missing"
W12-4205,stymne-ahrenberg-2010-using,0,0.0223864,"icularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the relations between individual words using morphological agreement instead of word order, together with the word order itself being relatively free, choosing the correct inflection becomes crucial. Since the output of phrase-based SMT shows frequent inflection errors (even in adjacent words) due to each word belonging to a different phrase, a possible way to address the grammaticality problem is a combination of statistical and structural approach, such as SMT output post-editing (Stymne and Ahrenberg, 2010; Mareˇcek et al., 2011). In this paper, we focus on improving SMT output parsing quality, as rule-based post-editing systems rely heavily on the quality of SMT output analysis. Parsers trained on gold standard parse trees often fail to produce the expected result when applied to SMT output with grammatical errors. This is partly caused by the fact that when parsing highly inflected free word-order languages the parsers have to rely on morphological agreement, which, as stated above, is often erroneous in SMT output. Training a parser specifically by creating a manually annotated treebank of M"
W12-4205,J97-3002,0,0.138024,"ation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in a joint scenario. We concentrate here on using dependency parsers because of tools and training data availability for the examined language pair. Regarding treebank adaptation for parser robustness, Foster et al. (2008) introduce various kinds of artificial errors into the training data to make the final parser less sensitive to grammar errors. However, their approach concentrates on mistakes made by humans (such as misspellings, word repetition or omission etc.) and the error models used are handcrafted. Our work focuse"
W12-4205,P10-1062,0,0.0211229,"ncy scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT output sentences and, ultimately, improve rule-based SMT post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDo"
W12-4205,I08-3008,0,0.0945642,"post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences"
W12-4205,P09-1007,0,0.0247054,"“RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual"
W12-4205,W11-2152,1,\N,Missing
W12-4205,D11-1007,0,\N,Missing
W12-4205,W09-1201,0,\N,Missing
W12-4205,D07-1096,0,\N,Missing
W12-4205,W10-1705,0,\N,Missing
W14-2902,W09-1201,1,0.891298,"Missing"
W14-2902,C12-1015,0,0.0260406,"Missing"
W14-2902,W09-1206,0,0.0992918,"Missing"
W14-2902,hajic-etal-2012-announcing,1,0.858905,"Missing"
W14-2902,bojar-etal-2012-joy,1,0.896852,"Missing"
W14-2902,burchardt-etal-2006-salsa,0,0.085696,"Missing"
W14-2902,I05-1081,0,0.447373,"Missing"
W14-2902,W04-3250,0,0.0155595,"Missing"
W14-2902,E12-1085,0,0.0220204,"Missing"
W14-2902,J93-2004,0,0.0462079,"nd Applied Linguistics, Charles University in Prague. It serves two main purposes: 1. to test and validate the FGD linguistic theory, 2. to apply and test machine learning methods for part-of-speech and morphological tagging, dependency parsing, semantic role labeling, coreference resolution, discourse annotation, natural language generation, machine translation and other natural language processing tasks. The language data in the PDT are non-abbreviated articles from Czech newspapers and journals. The PCEDT contains English sentences from the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993, PTB-WSJ) and their Czech translations, all annotated using the same theoretical framework as the PDT. The annotation of the PDT and the PCEDT is very rich in linguistic information. Following the stratificational approach of the FGD, the texts are annotated at different but interlinked layers. There are four such layers, two linear and two structured: 2 Attributes such as tense are annotated automatically, and most advanced information such as topic and focus annotation is not present. 7 4 PropBank frame files (Palmer et al., 2005) and by subsequent manual refinement.3 EngVallex was used for"
W14-2902,cinkova-2006-propbank,0,0.724884,"ed, indexed, and sorted. Each valency frame includes the frame’s “valency” (number of arguments, or frame members) and the following information for each argument: • its label (see Section 2.1), • its (semantic) obligatoriness according to Panevová (1974)’s dialogue test, • its required surface form (or several alternative forms) typically using morphological, lexical and syntactic constraints. Most valency frames are further accompanied by a note or an example which explains their meaning and usage. The version of PDT-Vallex used here contains 9,191 valency frames for 5,510 verbs. EngVallex (Cinková, 2006) is a valency lexicon of English verbs based on the FGD framework, created by an automatic conversion from Tectogrammatical annotation The PDT is a project for FGD-based manual annotation of Czech texts, started in 1996 at the Institute of Formal and Applied Linguistics, Charles University in Prague. It serves two main purposes: 1. to test and validate the FGD linguistic theory, 2. to apply and test machine learning methods for part-of-speech and morphological tagging, dependency parsing, semantic role labeling, coreference resolution, discourse annotation, natural language generation, machine"
W14-2902,J05-1004,0,0.105323,"es from the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993, PTB-WSJ) and their Czech translations, all annotated using the same theoretical framework as the PDT. The annotation of the PDT and the PCEDT is very rich in linguistic information. Following the stratificational approach of the FGD, the texts are annotated at different but interlinked layers. There are four such layers, two linear and two structured: 2 Attributes such as tense are annotated automatically, and most advanced information such as topic and focus annotation is not present. 7 4 PropBank frame files (Palmer et al., 2005) and by subsequent manual refinement.3 EngVallex was used for the tectogrammatical annotation of the English part of the PCEDT. Currently, it contains 7,699 valency frames for 4,337 verbs. 3 Experiments We evaluated the system described in Section 3 on PDT 2.5 for Czech and on the English part of PCEDT 2.0 for English. From PCEDT 2.0, whose division follows the PTB-WSJ, we used Sections 02-21 as training data, Section 24 as development data, and Section 23 as evaluation data. Since the system is intended to be used in a fully automatic annotation scenario, we use automatically parsed sentences"
W14-2902,W12-3132,1,0.919077,"Missing"
W14-2902,S01-1001,0,0.0529322,"Missing"
W14-2902,W12-3146,1,0.834698,"Missing"
W14-2902,J08-2004,0,0.0549514,"Missing"
W14-2902,W08-0325,0,0.0292435,"Missing"
W14-2902,W08-2121,0,\N,Missing
W14-3326,D11-1033,0,0.426651,"ain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general dom"
W14-3326,2011.iwslt-evaluation.18,0,0.0458693,"ction 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et"
W14-3326,bojar-etal-2012-joy,1,0.843764,"Missing"
W14-3326,N13-1073,0,0.0271109,"s are trained on the monolingual data in the target language (constrained or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables"
W14-3326,C04-1114,0,0.358032,", 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provi"
W14-3326,E12-3006,0,0.0294557,"Missing"
W14-3326,2005.eamt-1.19,0,0.105464,"nd Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the stan"
W14-3326,2011.iwslt-papers.5,0,0.0956886,"aining phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general domain” here is used to denote data Statistical"
W14-3326,P10-2041,0,0.508413,") or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general"
W14-3326,W08-0320,0,0.122804,"paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; M"
W14-3326,W07-0733,0,0.0769909,"work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation mo"
W14-3326,P03-1021,0,0.0285359,"Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the constrained ones. Linear interpolation outperforms data concatenation quite reliably"
W14-3326,P07-2045,0,0.00503854,"ined or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expecte"
W14-3326,2011.mtsummit-plenaries.5,0,0.0420922,"xts of nonmedical patents in the PatTR collection. Parallel data The parallel data summary is presented in Table 1. The main sources of the medical-domain data for all the language pairs include the EMEA corpus (Tiedemann, 2009), the UMLS metathesaurus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12"
W14-3326,2005.mtsummit-papers.11,0,0.0172802,"and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distribution of the domain-specificity s"
W14-3326,C10-2124,0,0.432112,"arch queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt l"
W14-3326,W02-1405,0,0.131062,"ranslation of search queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training co"
W14-3326,E12-1055,0,0.0150313,"n con unc unc concat interpol concat interpol cs→en 30.87±4.70 32.46±5.05 34.88±5.04 33.82±5.16 de→en 33.21±5.03 33.74±4.97 31.24±5.59 34.19±5.27 en→cs 23.25±4.85 21.56±4.80 22.61±4.91 23.93±5.16 en→de 17.72±4.75 16.90±4.39 19.13±5.66 15.87±11.31 en→fr 28.64±3.77 29.34±3.73 33.08±3.80 31.19±3.73 fr→en 35.56±4.94 35.28±5.26 36.73±4.88 40.25±5.14 Table 4: BLEU scores of query translations. each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. The two language models for sentence scoring are trained with a restricted vocabulary extracted from the in-domain training data as words occurring at least twice (singletons and other words are treated as out-of-vocabulary). In our experiments, we apply this technique to select both monolingual data for language models and parallel data for translation models. Selection of parallel data is based on the English side only. The in-domain models are trained on the monolingual data in the target language"
W14-3326,P13-1135,0,0.0121052,"rus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distri"
W14-3326,wu-wang-2004-improving-domain,0,0.0358057,"approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et"
W14-3326,W12-3151,1,0.842853,"difference) in the FR–EN parallel data and FR monolingual data is illustrated in Figures 1 and 2, respectively.6 The scores (Y axis) are presented for each sentence in increasing order from left to right (X axis). The rest of the preprocessing procedure was applied to all the datasets mentioned above, both parallel and monolingual. The data were tokenized and normalized by converting or omitting some (mostly punctuation) characters. A set of language-dependent heuristics was applied in an attempt to restore and normalize the opening/closing quotation marks, i.e. convert ""quoted"" to “quoted” (Zeman, 2012). The motivation here is twofold: First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses; second, if Moses learns to output directed quotation marks, the subsequent detokenization will be easier. For all systems which translate from German, decompounding is employed to reduce source-side data sparsity. We used BananaSplit for this task (M¨uller and Gurevych, 2006). We perform all training and internal evaluation on lowercased data; we trained recasers to postprocess the final submissions. 6 For the medical domain, constrained"
W14-3326,W12-3102,0,\N,Missing
W14-3326,eck-etal-2004-language,0,\N,Missing
W14-4311,W13-4064,0,0.0250022,"implementations using hand-written rules or external services: • ASR used a neural network based voice activity detector trained on small out-of-domain data. Recordings classified as speech were fed to the the web-based Google ASR service. • SLU was handcrafted for our domain using simple keyword-spotting rules. 4.1 • In DM, the dialogue tracker held only one value per dialogue slot, and the dialogue policy was handcrafted for the basic tasks in our domain. Speech Recognition: Building In-Domain Models The Google on-line ASR service, while reaching state-of-the-art performance in some tasks (Morbini et al., 2013), showed very high word error rate in our specific domain (see Figure 1). We replaced it with the Kaldi ASR engine (Povey et al., 2011) trained on general-domain Czech acoustic data (Korvas et al., 2014) with an in-domain class-based language model built using collected call data and lists of all available cities and stops. We describe our modifications to Kaldi for online decoding in Plátek and Jurˇcíˇcek (2014). A performance comparison of Google ASR with • NLG is a simple template-based module. • We use a web-based Czech TTS service provided to us by SpeechTech.3 Baseline Domain At baseline"
W14-4311,P01-1066,0,0.0673249,"to train and deploy an in-domain language model for Automatic Speech Recognition (ASR) and a statistical Spoken Language Understanding (SLU) module. The domain supported by the system has extended from transit information in one city to ca. 5,000 towns and cities in the whole country, plus weather and time information. This shows that a even a very basic system is useful in collecting indomain data and that the incremental approach is viable. Spoken dialogue systems have been a topic of research for the past several decades, and many experimental systems were developed and tested with users (Walker et al., 2001; Gaši´c et al., 2013; Janarthanam et al., 2013). However, few experimental systems became available to general public use. Let’s Go (Raux et al., 2005; Raux et al., 2006) is a notable example in the public transportation domain. Using interaction with users from the public to bootstrap data-driven methods and improve the system is also not a common practice. Both Let’s Go and the GOOG-411 business finder system (Bacchiani et al., 2008) collected speech data, but applied data-driven methods only to improve statistical ASR. We use the call data for statistical SLU as well and plan to further in"
W14-4311,W13-4070,1,0.892449,"Missing"
W14-4311,P08-1073,0,\N,Missing
W14-4311,korvas-etal-2014-free,1,\N,Missing
W14-4311,W14-4315,1,\N,Missing
W15-1613,ahrenberg-etal-2002-system,0,0.0556281,"able graphical editor and viewer for any tree-like structures. It allows displaying and editing sentential tree structures annotated on multiple linguistic layers. The new CzEngVallex TrEd extension uses the data format of the Treex NLP 7 Frame ID ev-w1f2, which has been created from abandon.02 in the PropBank, as in Noriega abandoned command ... for an exile. 8 These papers describe only a pilot experiment; the current process differs from their suggestions in several substantial respects. 9 There are other environments for manual alignment, such as (Melamed, 1998; Samuelsson and Volk, 2007; Ahrenberg et al., 2002), but they work on plain text or phrases, not dependency trees. 126 Figure 2: Highlighted alignment in the annotation tool TrEd; color-coding: green for verbs, blue for arguments/adjuncts framework (Žabokrtský, 2011; Popel and Žabokrtský, 2010) and pre-existing TrEd extensions for PCEDT, PDTVallex, and EngVallex. The annotation interface includes keyboard macros to change values of individual attributes or to add or delete whole nodes from the structure. Links between English and Czech nodes are added or changed in a drag-and-drop fashion. 4.2.3 Manual Annotation Workflow The environment descr"
W15-1613,1999.tmi-1.21,0,0.137082,"Missing"
W15-1613,sindlerova-bojar-2010-building,1,0.781977,", there are also many verb–non-verb or non-verb–verb pairs, which have been left aside for this first version of CzEngVallex as none of the underlying lexicons include a complete description of other parts-of-speech. 4.2 The Annotation Process During the actual annotation process, we have manually aligned English and Czech verbs and their arguments (and in some clear cases also adjuncts). After carefully checking all occurrences of any given valency frame pair in the PCEDT, we included it in CzEngVallex using the structure described in Sect. 4.1, which is based on (Šindlerová and Bojar, 2009; Bojar and Šindlerová, 2010).8 The process is helped by automatic preprocessing steps. 4.2.1 Preprocessing and Data Preparation The following steps had been taken before the manual annotation proper started: • automatic pre-alignment using GIZA++ word alignment (Och and Ney, 2003) and a projection to deep dependency trees (taken from the original PCEDT); • grouping the occurrences of the same verb sense pairs together to simplify annotation. 4.2.2 Annotation Environment The annotation interface for manual valency frame alignment9 has been built as an extension of the TrEd annotation environment (Pajas and Fabian, 2011)."
W15-1613,cinkova-2006-propbank,0,0.321601,"ACT"" cs_functor=""ACT""/&gt; &lt;slot en_functor=""PAT"" cs_functor=""PAT""/&gt; &lt;slot en_functor=""EFF"" cs_functor=""SUBS""/&gt; &lt;/slots&gt; &lt;/frame_pair&gt; &lt;/en_frame&gt; &lt;/valency_word&gt; &lt;/body&gt; &lt;/frames_pairs&gt; Figure 1: Structure of CzEngVallex (part of abandon pairing) verbs and frames come mostly from the data appearing in the latest versions of the PDT and PCEDT. 3.3 EngVallex – The English Valency Lexicon EngVallex has been created by a (largely manual) adaptation of an already existing similar resource for English, the PropBank (Kingsbury and Palmer, 2002), to the FGD valency format and to PDT labeling standards (Cinková, 2006). During the adaptation process, arguments were re-labeled, obligatoriness was marked for each valency slot and frames with identical meaning were merged (and some split as well). Links to the original PropBank frame file and roleset have been kept wherever possible. EngVallex was used for the annotation of the English part of the PCEDT. It contains 7,148 valency frames for 4,337 verbs. 4 Building CzEngVallex 4.1 Structure of CzEngVallex CzEngVallex builds on all the resources mentioned in Sect. 3. It connects pairs of valency frames in the PCEDT (verb senses) which are translations of each ot"
W15-1613,2004.tmi-1.6,0,0.10371,"Missing"
W15-1613,W04-2206,0,0.0927102,"Missing"
W15-1613,W06-2705,0,0.0282032,"lex builds on all the resources mentioned in Sect. 3. It connects pairs of valency frames in the PCEDT (verb senses) which are translations of each other, aligning their arguments as well. This resource cannot be used independently, since it refers to the valency frame descriptions contained in both PDT-Vallex and EngVallex, and it also relies on the PCEDT. The structure of this new resource, which is technically a single XML file, is shown in Fig. 1.6 Aligned pairs of verb frames are grouped by the English verb frame (&lt;en_frame&gt;), and for each English verb sense, 6 Similar scheme is used in (Hansen-Schirra et al., 2006). their Czech counterparts are listed (&lt;frame_pair&gt;). For each of such pairs, all the aligned valency slots are listed and referred to by the functor assigned to the slot in the respective valency lexicon. In this example, for the pair abandon7 – opustit (lit. leave [alone]) the first two arguments match perfectly (ACT:ACT, PAT:PAT) and the third argument in English (EFF) does not match any argument for this particular Czech counterpart, while for the pair abandon – zˇríci se (lit. get rid of [for sth]), the third English argument maps to a Czech adjunct (SUBS, substitution). It must be noted"
W15-1613,kingsbury-palmer-2002-treebank,0,0.116501,"/slots&gt; &lt;/frame_pair&gt; &lt;frame_pair id=... cs_id=""v-w9887f1""&gt; &lt;slots&gt; &lt;slot en_functor=""ACT"" cs_functor=""ACT""/&gt; &lt;slot en_functor=""PAT"" cs_functor=""PAT""/&gt; &lt;slot en_functor=""EFF"" cs_functor=""SUBS""/&gt; &lt;/slots&gt; &lt;/frame_pair&gt; &lt;/en_frame&gt; &lt;/valency_word&gt; &lt;/body&gt; &lt;/frames_pairs&gt; Figure 1: Structure of CzEngVallex (part of abandon pairing) verbs and frames come mostly from the data appearing in the latest versions of the PDT and PCEDT. 3.3 EngVallex – The English Valency Lexicon EngVallex has been created by a (largely manual) adaptation of an already existing similar resource for English, the PropBank (Kingsbury and Palmer, 2002), to the FGD valency format and to PDT labeling standards (Cinková, 2006). During the adaptation process, arguments were re-labeled, obligatoriness was marked for each valency slot and frames with identical meaning were merged (and some split as well). Links to the original PropBank frame file and roleset have been kept wherever possible. EngVallex was used for the annotation of the English part of the PCEDT. It contains 7,148 valency frames for 4,337 verbs. 4 Building CzEngVallex 4.1 Structure of CzEngVallex CzEngVallex builds on all the resources mentioned in Sect. 3. It connects pairs of va"
W15-1613,J03-1002,0,0.00559637,"annotation process, we have manually aligned English and Czech verbs and their arguments (and in some clear cases also adjuncts). After carefully checking all occurrences of any given valency frame pair in the PCEDT, we included it in CzEngVallex using the structure described in Sect. 4.1, which is based on (Šindlerová and Bojar, 2009; Bojar and Šindlerová, 2010).8 The process is helped by automatic preprocessing steps. 4.2.1 Preprocessing and Data Preparation The following steps had been taken before the manual annotation proper started: • automatic pre-alignment using GIZA++ word alignment (Och and Ney, 2003) and a projection to deep dependency trees (taken from the original PCEDT); • grouping the occurrences of the same verb sense pairs together to simplify annotation. 4.2.2 Annotation Environment The annotation interface for manual valency frame alignment9 has been built as an extension of the TrEd annotation environment (Pajas and Fabian, 2011). TrEd is a fully customizable and programmable graphical editor and viewer for any tree-like structures. It allows displaying and editing sentential tree structures annotated on multiple linguistic layers. The new CzEngVallex TrEd extension uses the data"
W15-1613,sindlerova-etal-2014-resources,1,0.893522,"Missing"
W15-2111,W09-1206,0,0.0853316,"Missing"
W15-2111,W14-2902,1,0.890401,"Missing"
W15-2111,1992.tmi-1.8,0,0.471548,"valency frame in the associated valency lexicon, effectively providing verbal word sense labeling. The parallel Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012) has been annotated using the same principles as the PDT, providing us with manually disambiguated verb senses on both the Czech and the English side. The texts are disjoint from the PDT; PCEDT contains the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and its Introduction Using parallel data for Word Sense Disambiguation (WSD) is as old as Statistical Machine Translation (SMT): Brown et al. (1992) analyze texts in both languages before the IBM SMT models are trained and used, including WSD driven purely by translation equivalents.1 A combination of parallel texts and lexicons also proved useful for SMT at the time (Brown et al., 1993). In our previous experiments (Dušek et al., 2014), we have shown that WSD based on a manually created valency lexicon (for verbs) can achieve encouraging results. Combining the above ideas and previous findings with parallel data and a manually created bilingual valency lexicon, we have moved to add bilingual 1 Given the “automatic” nature of the word sen"
W15-2111,S01-1001,0,0.0274924,"Missing"
W15-2111,H93-1039,0,0.448683,"Missing"
W15-2111,I05-1081,0,0.0862138,"Missing"
W15-2111,W09-1201,1,0.803063,"Missing"
W15-2111,J93-2004,0,0.0496536,"pts on the deep layer; for the purpose of our experiments, it is important that the deep layer links each verb node (occurrence) to the corresponding valency frame in the associated valency lexicon, effectively providing verbal word sense labeling. The parallel Prague Czech-English Dependency Treebank 2.0 (PCEDT 2.0) (Hajiˇc et al., 2012) has been annotated using the same principles as the PDT, providing us with manually disambiguated verb senses on both the Czech and the English side. The texts are disjoint from the PDT; PCEDT contains the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and its Introduction Using parallel data for Word Sense Disambiguation (WSD) is as old as Statistical Machine Translation (SMT): Brown et al. (1992) analyze texts in both languages before the IBM SMT models are trained and used, including WSD driven purely by translation equivalents.1 A combination of parallel texts and lexicons also proved useful for SMT at the time (Brown et al., 1993). In our previous experiments (Dušek et al., 2014), we have shown that WSD based on a manually created valency lexicon (for verbs) can achieve encouraging results. Combining the above ideas and previous findin"
W15-2111,hajic-etal-2012-announcing,1,0.809152,"Missing"
W15-2111,H05-1066,1,0.611891,"Missing"
W15-2111,C14-1003,0,0.0380015,"Missing"
W15-2111,A00-2013,0,0.174257,"Missing"
W15-2111,J03-1002,0,0.00589822,"in the IBM Candide SMT system had been given in the Brown et al. (1992) paper. 2 3 http://hunch.net/~vw http://ufal.mff.cuni.cz/pdt2.0 82 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 82–90, Uppsala, Sweden, August 24–26 2015. radit2 ACT(1) PAT(4;k+3;aby) ADDR(3) help1 ACT() PAT() ADDR() Figure 1: Valency frame examples from PDTVallex and EngVallex (Czech radit = ‘give advice, help’). translation into Czech. Sentences have been manually aligned during the human translation process, and words have been then aligned automatically using GIZA++ (Och and Ney, 2003). We have used valency frame annotation (and other features) of the PCEDT 2.0 in our previous work; however, billingual alignment information has not been used before. 2.2 Figure 2: PCEDT trees aligned using the CzEngVallex mapping Valency lexicons PDT-Vallex4 (Hajiˇc et al., 2003; Urešová, 2011) is a valency lexicon of Czech verbs (and nouns), manually created during the annotation of the PDT/PCEDT 2.0. Each entry in the lexicon contains a headword (lemma), according to which the valency frames (i.e., senses) are grouped. Each valency frame includes the valency frame members and the following"
W15-2111,J05-1004,0,0.0239311,"PAT, ADDR, EFF, ORIG, TWHEN, LOC, CAUS (actor, patient, addressee, effect, origin, time, location, cause),5 • its semantic “obligatoriness” attribute, • subcategorization: its required surface form(s) using morphosyntactic and lexical constraints. Most valency frames are further accompanied by a note or an example which explains their meaning and usage. The version of PDT-Vallex used here contains 11,933 valency frames for 7,121 verbs. EngVallex6 (Cinková, 2006) is a valency lexicon of English verbs based also on the FGD framework, created by an automatic conversion from PropBank frame files (Palmer et al., 2005) and subsequent manual refinement.7 EngVallex was used for the annotation of the English part of the PCEDT 2.0. Currently, it contains 7,148 valency frames for 4,337 verbs. EngVallex does not contain the explicitly formalized subcategorization information. 2.3 CzEngVallex: Valency lexicon mapping CzEngVallex (Urešová et al., 2015a; Urešová et al., 2015b) is a manually annotated Czech-English valency lexicon linking the Czech and English valency lexicons, PDT-Vallex and EngVallex. It contains 19,916 frame (verb sense) pairs. CzEngVallex builds links not only between corresponding frames but als"
W15-2111,W12-4205,1,0.894553,"Missing"
W15-2111,W02-0808,0,0.157321,"Missing"
W15-2111,P12-1073,0,0.0646998,"Missing"
W15-2111,W04-3250,0,0.0177928,"increase its usefulness to the classifier. 3.4 Results The results of the individual settings are given in Tables 1 and 2. The figures include the sense detection F-measure in an unlabeled (just detecting a verb occurrence whose sense must be inferred) and labeled setting (also selecting the correct sense) as well as the accuracy of the sense detection alone (in total and in ambiguous verbs with two or more senses). We can see that just using the Vowpal Wabbit classifier with the same features provides a substantial performance boost. The aligned lemma 12 We used paired bootstrap resampling (Koehn, 2004) with 1,000 resamples to assess statistical significance. 85 PCEDT annotation error. On the whole, the positive effects of using information from parallel data are prevailing. 4 tionaries have been used in POS tagging (Hajiˇc, 2000). More distant is the approach of, e.g., Brown et al. (1992) and Ide et al. (2002), where parallel text is used for learning supervision, but not for feature extraction; Diab and Resnik (2002) use an unsupervised method. We should also mention the idea of using parallel corpora as hidden features, a task first performed by (Brown et al., 1992) for WSD and subsequent"
W15-2111,W07-1709,1,0.845918,"Missing"
W15-2111,P97-1009,0,0.432533,"Missing"
W15-2111,P14-5003,1,0.893555,"Missing"
W15-2111,C04-1192,0,0.0753298,"Missing"
W15-2111,W15-1613,1,0.749109,"Missing"
W15-2111,F14-1005,0,0.0227862,"Missing"
W15-2111,N09-1004,0,\N,Missing
W15-2111,P02-1033,0,\N,Missing
W15-2111,W12-3132,1,\N,Missing
W15-2111,cinkova-2006-propbank,0,\N,Missing
W15-3009,bojar-etal-2012-joy,1,0.894425,"Missing"
W15-3009,P13-3023,1,0.902684,"Missing"
W15-3009,P15-1044,1,0.88208,"Missing"
W15-3009,W12-3132,1,0.897194,"Missing"
W15-3009,hajic-etal-2012-announcing,0,0.0376956,"Missing"
W15-3009,P14-5003,0,0.120191,"Missing"
W15-3009,W10-1730,1,0.934479,"Missing"
W15-3009,W15-4103,0,0.0311546,"uages indicated that it is sufficient in most cases. The output sentence is then obtained by just combining all the nodes in the resulting surface dependency tree. 5 WMT 2015 Translation Task Results TectoMT reached a BLEU score of 13.9 for the English-to-Czech direction in the WMT 2015 Translation Task. This ranks it among the last systems, which is consistent with results from previous years. However, English-to-Czech TectoMT has also been used in the Chimera system combination, which ranks first in both automatic and human evaluation results. TectoMT plays a very important role in Chimera (Tamchyna and Bojar, 2015). TectoMT’s Czech-to-English translation reached a BLEU score of 12.8, and finished last 4. Subject-predicate agreement in number and person is enforced – predicates have their number and person filled based on their subject(s). 5. Auxiliary words are added. These are based on the contents of formemes (prepositions, subordinating conjunction, infinitive particles, possessive markers) and t-lemmas (phrasal verb particles). 10 Alternatively, an n-gram language model could be used to select the word forms. Flect uses just a short context of neighboring lemmas, but it generalizes also to unseen wo"
W15-3009,H05-1066,0,0.0520577,"Missing"
W15-3009,P09-2037,1,0.891858,"as, formemes, and grammatemes are translated using separate models. The t-lemma and formeme translation models are an interpolation of maximum entropy discriminative models (MaxEnt) of Mareˇcek et al. (2010) and simple conditional probability models. The MaxEnt models are in fact an ensemble of models, one for each individual source t-lemma/formeme. The combined translation models provide several translation options for each node along with their estimated probability (see Section 1). The best options are then selected using a Hidden Markov Tree Model (HMTM) with a target-language tree model (Žabokrtský and Popel, 2009), which roughly corresponds to the target-language n-gram model in phrase-based MT. Grammateme transfer is rule-based; in most cases, grammatemes remain the same as in the source language. Adding New Language Pairs Using different languages in an MT system with deep transfer is mainly hindered by differences in the analysis and synthesis of the individual languages. To overcome these problems, we decided to use existing multilingual annotation standards (see Section 3.1) and to simplify and automate translation model training (see Section 3.2). In addition, we introduce an easier way of combin"
W15-3009,W13-3307,1,0.84883,"and Rudolf Rosa∗ ∗ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics {odusek,mnovak,popel,rosa}@ufal.mff.cuni.cz ‡ University of Lisbon, Faculty of Sciences, Department of Informatics luis.gomes@di.fc.ul.pt Abstract system in the shared task. The performance of the current version leaves a lot of room for improvement, but proves the potential of TectoMT for different language pairs. The original TectoMT system for EnglishCzech translation has seen just small changes, e.g., adding specialized translation models for selected pronouns (Novák et al., 2013a; Novák et al., 2013b) and fine-tuning of a handful of rules. Therefore, its performance is virtually identical to that of the last year’s version. This paper is structured as follows: in Section 2, we introduce the TectoMT basic architecture. In Section 3, we describe the improvements to TectoMT that were added for an easier support of new language pairs. Section 4 then details the Czech-to-English TectoMT system submitted to WMT15. We discuss TectoMT’s performance in the task and examine the most severe error sources in Section 5. Section 6 then concludes the paper. The TectoMT tree-to-tree"
W15-3009,W08-0325,0,0.622363,"Missing"
W15-3009,I13-1142,1,0.831364,"Missing"
W15-3009,zeman-etal-2012-hamledt,1,0.902954,"Missing"
W15-3009,P02-1040,0,0.0941085,"ining New Language Pairs • HM-P – harmonic mean of probabilities, Other improvements to support adding new language pairs quickly are rather technical. We automated the translation model training in a set of makefiles. To train a new translation pair, one only needs to implement analysis and synthesis pipelines for both languages and edit a configuration file. Debugging and testing of the new analysis and synthesis pipelines is supported by monolingual “roundtrip” experiments: a development data set is first analyzed up to t-layer, then synthesized back to word forms. BLEU score measurements (Papineni et al., 2002) and a direct comparison of the results are then used to improve performance before the translation models are trained and other transfer blocks are implemented.3 3.3 • GM-Log-P – geometric mean of logarithmic probabilities,7 • HM-Log-P – harmonic mean of logarithmic probabilities.8 We compared the functions against a baseline of just using the first option given by each of the models (regardless of compatibility). We used corpora of 1,000 sentences from the IT domain collected in the QTLeap project to evaluate all variants in English-to-Czech, English-toSpanish, and English-to-Portuguese tran"
W15-3009,zeman-2008-reusable,0,0.205436,"o-tree machine translation (MT) system (Žabokrtský et al., 2008) has been competing in WMT translation tasks since 2008 and has seen a number of improvements. Until now, the only supported translation direction was English to Czech. This year, as a part of the QTLeap project,1 we have enhanced TectoMT and its underlying natural language processing (NLP) framework, Treex (Popel and Žabokrtský, 2010), to support more language pairs. We simplified the training pipeline to be able to retrain the translation models faster, and we use abstracted language-independent rules with the help of Interset (Zeman, 2008) where possible. Together with our partners on the QTLeap project, we have implemented translation systems for other language pairs (English to and from Dutch, Spanish, Basque, and Portuguese) which are not part of WMT shared Translation Task this year. However, we were also able to submit the results of a newly built Czech-English translation 1 2 The TectoMT Translation System TectoMT (Žabokrtský et al., 2008) is a tree-totree MT system system consisting of an analysistransfer-synthesis pipeline, with transfer on the level of deep syntax. It is based on the Prague Tectogrammatics theory (Sgal"
W15-5711,D11-1033,0,0.0265192,"models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset. In the coming year, we"
W15-5711,J96-1002,0,0.23014,"of rules which collapse auxiliaries and assign all the required attributes to each t-node. 2.2 Transfer In the transfer phase, an initial target t-tree is obtained as a copy of the source t-tree. Target t-lemmas and formemes of the t-nodes are suggested by a set of TMs, and the other attributes are transferred by a set of rules. For both t-lemmas and formemes, we use two separate TMs: • MaxEnt TM – a discriminative model whose prediction is based on features extracted from the source tree. The discriminative TM (Mareˇcek et al., 2010) is in fact an ensemble of maximum entropy (MaxEnt) models (Berger et al., 1996), each trained for one specific source t-lemma/formeme. However, as the number of types observed in the parallel treebank may be too large, infrequent source t-lemmas/formemes are not covered by this type of TM. • Static TM – this is only a dictionary of possible translations with relative frequencies (no contextual features are taken into account). This model is available for most source t-lemmas/formemes seen in training data.3 1 http://ufal.mff.cuni.cz/treex and https://github.com/ufal/treex The modules used for the analysis in the individual languages vary, but all of them follow the same"
W15-5711,2011.iwslt-evaluation.18,0,0.0393918,"erforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusio"
W15-5711,bojar-etal-2012-joy,1,0.910006,"Missing"
W15-5711,W04-3237,0,0.0447917,"he linear interpolation constant is tuned on a development set. • A LL – concatenation of training data. • W EIGHT – as A LL, but the out-of-domain training examples are downweighted so the in-domain examples (which are typically much fewer) have bigger effect on the resulting model. The weight is chosen by cross-validation. • P RED – the prediction of the out-of-domain model is used as an additional feature for training the final model on the in-domain data. • P RIOR – out-of-domain weights are used as a prior (via the regularization term) when training the final model on the in-domain data (Chelba and Acero, 2004). 4 http://metashare.metanet4u.eu/go2/qtleapcorpus http://www.statmt.org/wmt13/translation-task.html 6 See cuni_train/Makefile in https://github.com/ufal/qtleap. 5 93 • E ASYA DAPT (called AUGMENT in the original paper, sometimes referred to as the “Frustratingly Easy Domain Adaptation”) – create three variants of each feature: general, in-specific and outspecific; train on concatenation of in- and out-of-domain data, where on in-domain data, the general and in-specific features are active and on the out-of-domain data, the general and out-specific features are active. Daumé III (2009) showed"
W15-5711,W10-2608,0,0.0563296,"Missing"
W15-5711,W14-3326,1,0.79882,"Missing"
W15-5711,P13-3023,1,0.892993,"Missing"
W15-5711,W12-3132,1,0.901012,"Missing"
W15-5711,W15-3009,1,0.84035,"two possible ways of combining the lists: 1. Just using the first item of both lists (the simplest way, but its performance may not be ideal since incompatible combinations are sometimes produced). 2. Using a Hidden Markov Tree Model (Žabokrtský and Popel, 2009), where a Viterbi search is used to find the best t-lemma/formeme combinations globally over the whole tree. In the current TectoMT version, HMTM is only used in EN→CS translation. HMTM for the remaining languages will be added in the near future. 2.3 Synthesis The synthesis is a pipeline of rule-based modules (Žabokrtský et al., 2008; Dušek et al., 2015) that gradually change the translated t-tree into an a-tree (surface dependency tree), adding auxiliary words and punctuation and resolving morphological attributes. Some basic word-order rules are also applied. The individual a-tree nodes/words are then inflected using a morphological dictionary (Straková et al., 2014) or a statistical tool trained on an annotated corpus (Dušek and Jurˇcíˇcek, 2013). The resulting tree is then simply linearized into the output sentence. 3 Domain Adaptation by Model Interpolation The general approach of domain adaptation by model interpolation is rather simple"
W15-5711,eck-etal-2004-language,0,0.0138674,"ments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, a"
W15-5711,2005.eamt-1.19,0,0.0123169,"neral-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset."
W15-5711,W07-0733,0,0.0220004,"NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented o"
W15-5711,2005.mtsummit-papers.11,0,0.105998,"nswers) as the in-domain training data. This reflects the inteded purpose of the MT systems and the final application of translating user questions into English and helpdesk answers back to the original language (Czech, Dutch, Spanish). Out-of-domain We use the following corpora to train our out-of-domain models (each language contains parallel texts with English): • Czech – CzEng 1.0 (Bojar et al., 2012), with 15.2 million parallel sentences, containing a variety of domains, including fiction books, news texts, EU legislation, and technical documentation. • Dutch – A combination of Europarl (Koehn, 2005), Dutch Parallel Corpus (Macken et al., 2007), and KDE technical documentation; 2.2 million parallel sentences in total. • Spanish – Europarl, containing 2 million parallel sentences. Monolingual For Czech as the target language, we used the WMT News Crawl monolingual training data (2007– 2012, 26 million sentences in total) to train the HMTM.5 Other target languages do not use an HMTM (see Sections 2.2 and 4.2). 4.2 Setup We use the QTLeap TM training makefile6 to train a Static and a MaxEnt TM on both in-domain and out-of-domain data. As discussed in Section 3, we do not use tuning on develo"
W15-5711,W02-1405,0,0.0494824,"c features are active. Daumé III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) o"
W15-5711,2007.mtsummit-papers.42,0,0.0276696,"a. This reflects the inteded purpose of the MT systems and the final application of translating user questions into English and helpdesk answers back to the original language (Czech, Dutch, Spanish). Out-of-domain We use the following corpora to train our out-of-domain models (each language contains parallel texts with English): • Czech – CzEng 1.0 (Bojar et al., 2012), with 15.2 million parallel sentences, containing a variety of domains, including fiction books, news texts, EU legislation, and technical documentation. • Dutch – A combination of Europarl (Koehn, 2005), Dutch Parallel Corpus (Macken et al., 2007), and KDE technical documentation; 2.2 million parallel sentences in total. • Spanish – Europarl, containing 2 million parallel sentences. Monolingual For Czech as the target language, we used the WMT News Crawl monolingual training data (2007– 2012, 26 million sentences in total) to train the HMTM.5 Other target languages do not use an HMTM (see Sections 2.2 and 4.2). 4.2 Setup We use the QTLeap TM training makefile6 to train a Static and a MaxEnt TM on both in-domain and out-of-domain data. As discussed in Section 3, we do not use tuning on development data to set TM pruning thresholds and i"
W15-5711,2011.iwslt-papers.5,0,0.0195592,"s-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up to 3.2 BLEU over using only a single training dataset. In the coming year, we will obtain additional in-domain data, whi"
W15-5711,W10-1730,1,0.924267,"Missing"
W15-5711,H05-1066,0,0.33499,"Missing"
W15-5711,P10-2041,0,0.034953,"bine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et al., 2014). 6 Conclusion and Future Work In this paper, we presented our implementation of machine translation domain adaptation by translation model interpolation in the TectoMT system. We evaluated the method using large out-of-domain parallel data and small in-domain parallel data (1000 sentences) in the domain of computer helpdesk requests and responses, using 6 translation directions. The evaluation showed our method to perform well, achieving improvements up"
W15-5711,W08-0320,0,0.0279926,"ctive. Daumé III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combi"
W15-5711,C10-2124,0,0.0226748,"III (2009) showed that E ASYA DAPT outperforms the other methods (on a variety of NLP tasks, but not including MT) in the cases when T RG O NLY outperforms S RC O NLY.7 Otherwise, L IN I NT, P RED and W EIGHT were the most successful methods. In a follow-up work (Daumé III et al., 2010), E ASYA DAPT was improved to exploit also additional unlabeled in-domain data. In MT, many different approaches to domain adaptation have been attempted. Similarly to our experiments, authors combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Nakov, 2008; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011) or language models (Koehn and Schroeder, 2007) in phrase-based statistical MT. Others concentrate on acquiring larger in-domain training corpora for statistical MT by selecting data from large general-domain corpora that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique has been used to adapt language models (Eck et al., 2004; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011; Dušek et"
W15-5711,W07-1709,0,0.0772611,"Missing"
W15-5711,P14-5003,0,0.0926819,"Missing"
W15-5711,P09-2037,1,0.952471,"ne translation system with a tree-to-tree transfer on the deep syntax layer, first introduced by Žabokrtský et al. (2008). It is based on the Prague “tectogrammatics” theory of Sgall et al. (1986). The system uses two layers of structural description with dependency trees: surface syntax (a-layer, a-trees) and deep syntax (t-layer, t-trees). The analysis phase is two-step and proceeds from plain text over a-layer to t-layer (see Section 2.1). The transfer phase of the system is based on maximum entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Markov Tree Models (Žabokrtský and Popel, 2009) (see Section 2.2). The subsequent generation phase consists of rule-based components that gradually change the deep target language representation into a shallow one, which is then converted to text (see Section 2.3). 2.1 Analysis The analysis phase consists of a pipeline of standard NLP tools that perform the analysis to the a-layer, followed by a rule-based conversion to t-layer. In the analysis pipeline, the input is first segmented into sentences and tokenized using rule-based modules from the Treex toolkit1 (Popel and Žabokrtský, 2010). A statistical part-of-speech tagger and dependency"
W15-5711,W08-0325,0,0.606583,"Missing"
W16-3622,P08-2043,0,0.0244483,"r utterance along with each data instance (i.e., pair of input meaning representation and output sentence), and we show that our context-aware system outperforms the baseline in both automatic metrics and a human pairwise preference test. Introduction In a conversation, speakers are influenced by previous utterances of their counterparts and tend to adapt (align, entrain) their way of speaking to each other, reusing lexical items as well as syntactic structure (Reitter et al., 2006). Entrainment occurs naturally and subconsciously, facilitates successful conversations (Friedberg et al., 2012; Nenkova et al., 2008), and forms a natural source of variation in dialogues. In spoken dialogue systems (SDS), users were reported to entrain to system prompts (Parent and Eskenazi, 2010). The function of natural language generation (NLG) components in task-oriented SDS typically is to produce a natural language sentence from a dialogue act (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values (see Fig. 1). NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also i"
W16-3622,P02-1040,0,0.110894,"m((yt−1 ◦ ct )WS , st−1 ) Making the Generator Context-aware Context encoder. We add another, separate encoder for the context utterances. The hidden states of both encoders are concatenated, and the decoder then works with double-sized vectors both on the input and in the attention model (see Fig. 2). (2) n-gram match reranker. We added a second reranker for the k-best outputs of the generator that promotes outputs that have a word or phrase overlap with the context utterance. We use geometric mean of modified n-gram precisions (with n ∈ {1, 2}) as a measure of context overlap, i.e., BLEU-2 (Papineni et al., 2002) without brevity penalty. The log probability l of an output sequence on the generator k-best list is updated as follows: √ l = l + w · p1 p2 (4) It is initialized by the last hidden encoder state (s0 = hn ) and a special starting symbol. The generated output token yt is selected from a softmax distribution: p(yt |yt−1 . . . , x) = softmax((st ◦ ct )WY ) (3) In (2) and (3), ct represents the attention model – a sum over all encoder hidden states, weighted by a feed-forward network with one tanh hidden layer; WS and WY are linear projection matrices and “◦” denotes concatenation. DAs are repres"
W16-3622,N06-2031,0,0.101068,"system for SDS. We also present our first results on the dataset of Duˇsek and Jurˇc´ıcˇ ek (2016a), which includes the preceding user utterance along with each data instance (i.e., pair of input meaning representation and output sentence), and we show that our context-aware system outperforms the baseline in both automatic metrics and a human pairwise preference test. Introduction In a conversation, speakers are influenced by previous utterances of their counterparts and tend to adapt (align, entrain) their way of speaking to each other, reusing lexical items as well as syntactic structure (Reitter et al., 2006). Entrainment occurs naturally and subconsciously, facilitates successful conversations (Friedberg et al., 2012; Nenkova et al., 2008), and forms a natural source of variation in dialogues. In spoken dialogue systems (SDS), users were reported to entrain to system prompts (Parent and Eskenazi, 2010). The function of natural language generation (NLG) components in task-oriented SDS typically is to produce a natural language sentence from a dialogue act (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values (see Fig"
W16-3622,C16-1191,0,0.0647488,"Missing"
W16-3622,N09-2048,0,0.153765,"iation in dialogues. In spoken dialogue systems (SDS), users were reported to entrain to system prompts (Parent and Eskenazi, 2010). The function of natural language generation (NLG) components in task-oriented SDS typically is to produce a natural language sentence from a dialogue act (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values (see Fig. 1). NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also influence the overall task success (Stoyanchev and Stent, 2009; Lopes et al., 2013). However, typical 185 Proceedings of the SIGDIAL 2016 Conference, pages 185–190, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics In the following, we first present the architecture of our generator (see Section 2), then give an account of our experiments in Section 3. We include a brief survey of related work in Section 4. Section 5 contains concluding remarks and plans for future work. 2 value” is created for each slot in the DA and the triples are concatenated (see Fig. 2).3 The generator supports greedy decoding as well as beam"
W16-3622,P16-2008,1,0.377709,"Missing"
W16-3622,W15-4639,0,0.0683214,"ll Street) typical NLG Take bus line M102 from Wall Street to Herald Square at 9:01am. There is a bus at 9:01am from Wall Street to Herald Square using line M102. contextually bound response Figure 1: An example of NLG input and output, with context-aware additions. NLG systems in SDS only take the input DA into account and have no way of adapting to the user’s way of speaking. To avoid repetition and add variation into the outputs, they typically alternate between a handful of preset variants (Jurˇc´ıcˇ ek et al., 2014) or use overgeneration and random sampling from a k-best list of outputs (Wen et al., 2015b). There have been several attempts at introducing entrainment into NLG in SDS, but they are limited to rule-based systems (see Section 4). We present a novel, fully trainable contextaware NLG system for SDS that is able to entrain to the user and provides naturally variable outputs because generation is conditioned not only on the input DA, but also on the preceding user utterance (see Fig. 1). Our system is an extension of Duˇsek and Jurˇc´ıcˇ ek (2016b)’s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al., 2015). It is, to our knowledge, the first full"
W16-3622,D15-1199,0,0.252398,"Missing"
W16-3622,W04-3250,0,0.183216,"Missing"
W16-3622,N16-1086,0,\N,Missing
W16-4506,W15-1006,0,0.0619694,"ach, but in manual evaluation, both WSD methods bring improvements. 1 Introduction The possibility of using word sense disambiguation (WSD) systems in machine translation (MT) has recently been investigated in several ways: Output of WSD systems has been incorporated into MT to improve translation quality — at the decoding step of a phrase-based statistical machine translation (PBSMT) system (Chan et al., 2007) or as contextual features in maximum entropy (MaxEnt) models (Neale et al., 2015) and (Neale et al., 2016). In addition, WSD has also been used in MT evaluation, for example in METEOR (Apidianaki et al., 2015). These works indicate that WSD can be beneficial to different MT tasks, in case of using senses as contextual features for MaxEnt models Neale et al. (2016) achieve statistically significant improvement over the baseline for English-to-Portuguese translation. And Apidianaki et al. (2015) report that usage of WSD can establish better sense correspondences and improve its correlation with human judgments of translation quality. In this research, we have investigated the possibilities of integrating two different approaches to verbal WSD into a PB-SMT system – verb patterns based on corpus patte"
W16-4506,W07-0735,1,0.766205,"• Form+Sense→Form – two source factors (surface word form and verb sense ID, if applicable) are translated to the target-side word forms. This is technically identical to appending the verb sense ID to the source words. • Form→Form+Tag – the source word form is translated to two factors on the target side: word form and morphological tag (part-of-speech tag with morphological categories of Czech, such as case, number, gender, or tense). This allows us to use an additional language model trained on morphological tags only. This setup is known to perform well for morphologically rich languages (Bojar, 2007) and thus was selected as a baseline for all comparisons. • Form+Sense→Form+Tag – a combination of the two setups above: two source and two target-side factors, for better handling of source verb meaning and target morphological coherence. • Form→Form+Tag + Form+Sense→Form+Tag – a combination of previous two models as two separate phrase tables. For all configurations, we trained a 4-gram language model on word forms of the sentences from the training set. This LM was pruned: we discarded all singleton n-grams (apart from unigrams). In addition, for configurations which generated morphological"
W16-4506,cinkova-etal-2012-database,1,0.873109,"Missing"
W16-4506,cinkova-2006-propbank,0,0.0449843,"Missing"
W16-4506,P11-2031,0,0.0327178,"Missing"
W16-4506,W15-2111,1,0.895548,"Missing"
W16-4506,hajic-etal-2012-announcing,1,0.881793,"Missing"
W16-4506,C12-1073,1,0.903554,"Missing"
W16-4506,D07-1091,0,0.027397,"meaning of the sentence (Healy and Miller, 1970) and thus accurate translation of the verb is critical for the understanding of the translation. Therefore, improvement of the translation of verbs can lead to overall increase of the translation quality. Therefore, improvement of the translation of verbs can lead to an overall increase of translation quality. The outputs of automatic verb sense disambiguation systems using both CPA and valency frames were integrated into Moses statistical machine translation system(Koehn et al., 2007). Both kinds of verb senses were added as additional factors (Koehn and Hoang, 2007). Section 4.1 shows that we obtain statistically significant improvement in terms of BLEU scores (Papineni et al., 2002) and manual evaluation of translations validated that. The novelty of this work lies not only in our focus only on verbs senses, but also in the fact that we are comparing the impact of two WSD approaches on the statistical machine translation. The following Section 2 describes the initial setup of our experiments. Section 3 and Section 4 depict the idea behind corpus pattern analysis and verb valency frames representations and show evaluation results of incorporation of thes"
W16-4506,P07-2045,1,0.0305017,"focus on verbs was motivated by the ideas that verbs carry a crucial part of the meaning of the sentence (Healy and Miller, 1970) and thus accurate translation of the verb is critical for the understanding of the translation. Therefore, improvement of the translation of verbs can lead to overall increase of the translation quality. Therefore, improvement of the translation of verbs can lead to an overall increase of translation quality. The outputs of automatic verb sense disambiguation systems using both CPA and valency frames were integrated into Moses statistical machine translation system(Koehn et al., 2007). Both kinds of verb senses were added as additional factors (Koehn and Hoang, 2007). Section 4.1 shows that we obtain statistically significant improvement in terms of BLEU scores (Papineni et al., 2002) and manual evaluation of translations validated that. The novelty of this work lies not only in our focus only on verbs senses, but also in the fact that we are comparing the impact of two WSD approaches on the statistical machine translation. The following Section 2 describes the initial setup of our experiments. Section 3 and Section 4 depict the idea behind corpus pattern analysis and verb"
W16-4506,W04-3250,0,0.159584,"orm+Tag Avg 25.0 24.9 25.0 22.6 22.5 22.6 62.2 62.4 62.2 ssel 0.9 0.9 0.9 0.4 0.4 0.4 0.7 0.7 0.7 sTest 0.1 0.1 0.1 0.0 0.0 0.1 0.2 0.1 0.2 p-value 0.00 0.16 0.00 0.22 0.00 0.61 Table 4: Multeval results for corpus pattern analysis, based on 36 MERT runs We also performed a more detailed analysis with pairwise comparisons of the following configurations: • Form→Form vs. Form+Sense→Form • Form→Form+Tag vs. Form+Sense→Form+Tag • Form→Form+Tag vs. Form→Form+Tag + Form+Sense→Form+Tag 3.1.1 Form→Form vs. Form+Sense→Form The comparison provided by MT-ComparEval based on paired bootstrap resampling (Koehn, 2004) of best MERT runs for both configurations showed that Form→Form is significantly better (p-value=0.022) than Form+Sense→Form. The sentence-by-sentence comparison explains this: On the positive side, 8 examples out of the top 10 sentences where Form+Sense→Form output was better than Form→Form profited from using additional information about the verb sense. On the negative side, the model with verb senses made a lot of errors due to badly extracted phrase tables, even leaving some verbs untranslated. 3.1.2 Form→Form+Tag vs. Form+Sense→Form+Tag In this case the same paired bootstrap resampling o"
W16-4506,W15-5708,0,0.0317026,"s show a statistically significant translation quality improvement in terms of the BLEU metric for the valency frames approach, but in manual evaluation, both WSD methods bring improvements. 1 Introduction The possibility of using word sense disambiguation (WSD) systems in machine translation (MT) has recently been investigated in several ways: Output of WSD systems has been incorporated into MT to improve translation quality — at the decoding step of a phrase-based statistical machine translation (PBSMT) system (Chan et al., 2007) or as contextual features in maximum entropy (MaxEnt) models (Neale et al., 2015) and (Neale et al., 2016). In addition, WSD has also been used in MT evaluation, for example in METEOR (Apidianaki et al., 2015). These works indicate that WSD can be beneficial to different MT tasks, in case of using senses as contextual features for MaxEnt models Neale et al. (2016) achieve statistically significant improvement over the baseline for English-to-Portuguese translation. And Apidianaki et al. (2015) report that usage of WSD can establish better sense correspondences and improve its correlation with human judgments of translation quality. In this research, we have investigated th"
W16-4506,P03-1021,0,0.0100485,"respective numbers of sentences and tokens in each of training, development and test sets are shown in Table 1. For our experiments, 28 different English verbs were selected and automatically annotated with corpus pattern analysis senses, and 3,306 verbs annotated using valency frames. The subset has been selected to include verbs annotated with CPA, so the effect of WSD would be visible. All the experiments were carried out in the Eman experiment management system (Bojar and Tamchyna, 2013) using the Moses PB-SMT system (Koehn et al., 2007) as the core and minimum error rate training (MERT, (Och, 2003)) to optimize the decoder feature weights on the development set. The evaluation was performed using the BLEU score (Papineni et al., 2002), but the results of each setup were then thoroughly examined and verified using the MT-ComparEval system (Aranberri et al., 2016)1 . Set Training Development Test Number of sentences 649,605 10,115 2,707 Tokens CS 10,759,546 187,478 59,446 Tokens EN 12,073,130 167,788 67,336 Table 1: Data set composition 2.2 MT configurations As we have mentioned in Section 1 the main goal of the experiments was to explore whether verb senses as additional factors in the s"
W16-4506,J05-1004,0,0.0598681,"Missing"
W16-4506,P02-1040,0,0.100568,"ding of the translation. Therefore, improvement of the translation of verbs can lead to overall increase of the translation quality. Therefore, improvement of the translation of verbs can lead to an overall increase of translation quality. The outputs of automatic verb sense disambiguation systems using both CPA and valency frames were integrated into Moses statistical machine translation system(Koehn et al., 2007). Both kinds of verb senses were added as additional factors (Koehn and Hoang, 2007). Section 4.1 shows that we obtain statistically significant improvement in terms of BLEU scores (Papineni et al., 2002) and manual evaluation of translations validated that. The novelty of this work lies not only in our focus only on verbs senses, but also in the fact that we are comparing the impact of two WSD approaches on the statistical machine translation. The following Section 2 describes the initial setup of our experiments. Section 3 and Section 4 depict the idea behind corpus pattern analysis and verb valency frames representations and show evaluation results of incorporation of these sense to phrase-based statistical machine translation. The next section (Section 5) is devoted to the discussion of re"
W16-4506,L16-1296,0,\N,Missing
W16-6401,W10-1705,1,0.929434,"oses + Moses post-editing, simple Moses + Moses post-editing, TwoStep Google Translate + TectoMT post-editing Moses + TectoMT post-editing § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setup"
W16-6401,W15-3006,1,0.779365,"nslation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been"
W16-6401,W12-3130,1,0.860797,"s, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd Deep Machine Translation Workshop (DMTW 2016), pages 1–10, Lisbon, Portugal, 21 October 2016. Figure 1: TectoMT 2.1.1 Factored Moses In the more recent experiments that we report, the Moses system used is actually the Factored Moses of Bojar et al. (2012). It translates the source English text into a factored representation of Czech, where each word is represented by a tuple of a word form and a corresponding part-of-speech (PoS) tag. This enables Moses to use an additional language model which operates on PoS tags instead of word forms. This helps overcome data sparsity issues of the word-based language model and thus leads to a higher output quality, especially to its better grammaticality. Factored Moses is trained on parallel corpora pre-analyzed by Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically"
W16-6401,W13-2208,1,0.933672,"to perform forced translation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at"
W16-6401,W15-3009,1,0.853769,"y Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically motivated NLP framework. It consists of a large number of smaller components performing a specific NLP-task (blocks), both Treex-specific as well as Treex-wrapped external tools, which can be flexibly combined into processing pipelines. Sentences are represented by surface and deep syntactic dependency trees, richly annotated with numerous linguistic attributes, similarly to the Prague Dependency Treebank (Hajiˇc, 1998). 2.2.1 TectoMT The main application of Treex is TectoMT3 (Žabokrtský et al., 2008; Dušek et al., 2015), a linguistically motivated hybrid machine translation system. Its pipieline consists of three main steps: analysis of each source sentence up to t-layer (a deep syntactic representation of the sentence in a labelled dependency t-tree), transfer of the source t-tree to the target t-tree (i.e., the translation per se), and generation of the target sentence from the target t-tree (see Figure 1). The transfer is performed by copying the t-tree structure and grammatemes4 (attributes describing grammatical meaning) from source, and predicting target lemmas and formemes5 (deep morphosyntactic attri"
W16-6401,W12-3132,1,0.90614,"Missing"
W16-6401,W13-2216,1,0.89877,"Missing"
W16-6401,P07-2045,1,0.0127177,"rtcomings cancel out. In our paper, we review a set of such attempts, performed with Moses, a prominent representative of the PB-SMT systems, and Treex, a linguistically motivated NLP framework, featuring, among other, a full-fledged deep syntactic MT system, TectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to othe"
W16-6401,W09-0424,0,0.0237791,"u-plain Moses output downloaded from http://matrix.statmt.org/systems/show/2807, test set downloaded from http://matrix.statmt.org/test_sets/list. 10 Figure 8: Moses with TectoMT pre- and post-processing 6 Figure 9: Two-headed Chimera Zeman (2010) used several pre-processing steps to make the source English text more similar to Czech, such as removing articles, marking subjects by artificial suffixes (“/Sb”), and reordering auxiliary verbs to neighbor their main verbs. Of course, the SMT system was also trained on texts preprocessed in that way; in these experiments, the Joshua PB-SMT system (Li et al., 2009) was used instead of Moses. This approach may seem too aggressive, prone to making the input noisier as well as being potentially lossy. However, the author showed that with careful selection and tuning of the pre-processing steps, a significant improvement of translation quality can be achieved; moreover, this was also confirmed on English-to-Hindi translation. Rosa et al. (2016) successfully apply Treex pre-processing and post-processing to Moses, but this time with the main objective being an adaptation of Moses trained on general-domain data to a specific domain (namely the domain of Infor"
W16-6401,J03-1002,0,0.0116228,"ectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd De"
W16-6401,W07-0704,0,0.0827717,"Missing"
W16-6401,W12-3146,1,0.888935,"Missing"
W16-6401,W16-2334,1,0.910577,"ng § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setups have been properly described and evaluated in a peer-reviewed publication, others, especially some of the unsuccessful ones, were ne"
W16-6401,N07-1064,0,0.0327481,"Majliš (2009) Zeman (2010) Table 2: Base systems. Figure 3: PhraseFix translating one t-node with two or more t-nodes or deleting some t-nodes.8 It also uses MERT tuning and it should scale with more training data. In the experiments with two factors (Popel, 2013), two language models were used: one for lemmas and one for formemes. Unfortunately, the TectoMoses experiment brought negative results, presumably due to additional noise introduced by the added transformations. 3.2 PhraseFix: TectoMT with Moses Post-editing The PhraseFix system of Galušˇcáková et al. (2013) is based on the work of Simard et al. (2007), who introduced the idea of automatically post-editing a first-stage MT system by a second-stage MT system, trained to “translate” the output of the first-stage system into a reference translation. This has been shown to be particularly beneficial for conceptually different MT systems. In PhraseFix, the source English side of the CzEng parallel corpus of Bojar and Žabokrtský (2009) is translated by TectoMT into Czech, and Moses is then trained in a monolingual setting to translate the TectoMT-Czech into reference-Czech, i.e., the target side of CzEng (see Figure 3). Evaluation shows that this"
W16-6401,W07-1709,0,0.0726314,"Missing"
W16-6401,W16-2325,1,0.836879,"parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been shown to have a positive effect, e.g., in choosing the correct inflection of a word when the language model encounters an unknown context, or in generating a translation for a word that constitutes an out-of-vocabulary item for Moses (as TectoMT can abstract from word forms to lemmas and beyond, which Moses cannot). 3.8 Chimera: Moses with Additional TectoMT Phrase-table and Depfix Post-editing The Three-headed Chimera, or simply Chimera (Bojar et al., 2013b; Tamchyna et al., 2016), is a combination of TectoMT and Moses, as in Section 3.7, complemented by a final post-editing step performed by Depfix, as in Section 3.5 (see Figure 10). It has been repeatedly confirmed as the best system by both automatic and manual evaluations, not only among the ones reported in this paper, but also in general, 12 http://www.statmt.org/moses/?n=Advanced.Hybrid 7 Figure 10: Three-headed Chimera being the winner of the WMT English-to-Czech translation task in the years 2013, 2014 and 2015 (Bojar et al., 2013a; Bojar et al., 2014; Bojar et al., 2015). 4 Conclusion We reviewed a range of e"
W16-6401,W08-0325,0,0.0651825,"Missing"
W17-5525,W07-0734,0,0.0765626,"Missing"
W17-5525,P04-1011,0,0.336745,"ing wine and cheese at a low cost. Loch Fyne is a French family friendly restaurant catering to a budget of below £20. Loch Fyne is a French restaurant with a family setting and perfect on the wallet. Table 1: An example of a data instance. calised datasets, e.g. BAGEL (Mairesse et al., 2010), SF Hotels/Restaurants (Wen et al., 2015), or RoboCup (Chen and Mooney, 2008). Therefore, end-to-end methods have not been able to replicate the rich dialogue and discourse phenomena targeted by previous rule-based and statistical approaches for language generation in dialogue, e.g. (Walker et al., 2004; Stent et al., 2004; Demberg and Moore, 2006; Rieser and Lemon, 2009). In this paper, we describe a new crowdsourced dataset of 50k instances in the restaurant domain (see Section 2). We analyse it following the methodology proposed by Perez-Beltrachini and Gardent (2017) and show that the dataset brings additional challenges, such as open vocabulary, complex syntactic structures and diverse discourse phenomena, as described in Section 3. The data is openly released as part of the E2E NLG challenge.1 We establish a baseline on the dataset in Section 4, using one of the previous end-to-end approaches. Introductio"
W17-5525,P10-1157,0,0.47043,"Missing"
W17-5525,N16-1086,0,0.146291,"mena, as described in Section 3. The data is openly released as part of the E2E NLG challenge.1 We establish a baseline on the dataset in Section 4, using one of the previous end-to-end approaches. Introduction The natural language generation (NLG) component of a spoken dialogue system typically has to be re-developed for every new application domain. Recent end-to-end, data-driven NLG systems, however, promise rapid development of NLG components in new domains: They jointly learn sentence planning and surface realisation from non-aligned data (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2016a; Lampouras and Vlachos, 2016). These approaches do not require costly semantic alignment between meaning representations (MRs) and the corresponding natural language (NL) reference texts (also referred to as “ground truths” or “targets”), but they are trained on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. (Novikova et al., 2016). So far, end-to-end approaches to NLG are limited to small, delexi2 The E2E Dataset The data was collected using the"
W17-5525,N16-1015,0,0.080351,"Missing"
W17-5525,W16-6644,1,0.904737,"tly learn sentence planning and surface realisation from non-aligned data (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2016a; Lampouras and Vlachos, 2016). These approaches do not require costly semantic alignment between meaning representations (MRs) and the corresponding natural language (NL) reference texts (also referred to as “ground truths” or “targets”), but they are trained on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. (Novikova et al., 2016). So far, end-to-end approaches to NLG are limited to small, delexi2 The E2E Dataset The data was collected using the CrowdFlower platform and quality-controlled following Novikova et al. (2016). The dataset provides infor1 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ 201 Proceedings of the SIGDIAL 2017 Conference, pages 201–206, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics of human references per MR (Refs/MR).2 While having more data with a higher number of references per MR makes the E2E data more attractive for statistical approaches, it is als"
W17-5525,P02-1040,0,0.0988195,"Missing"
W17-5525,D15-1199,0,0.567643,"Missing"
W17-5525,W17-3537,0,0.0717419,"Missing"
W17-5525,E09-1078,1,0.143521,"is a French family friendly restaurant catering to a budget of below £20. Loch Fyne is a French restaurant with a family setting and perfect on the wallet. Table 1: An example of a data instance. calised datasets, e.g. BAGEL (Mairesse et al., 2010), SF Hotels/Restaurants (Wen et al., 2015), or RoboCup (Chen and Mooney, 2008). Therefore, end-to-end methods have not been able to replicate the rich dialogue and discourse phenomena targeted by previous rule-based and statistical approaches for language generation in dialogue, e.g. (Walker et al., 2004; Stent et al., 2004; Demberg and Moore, 2006; Rieser and Lemon, 2009). In this paper, we describe a new crowdsourced dataset of 50k instances in the restaurant domain (see Section 2). We analyse it following the methodology proposed by Perez-Beltrachini and Gardent (2017) and show that the dataset brings additional challenges, such as open vocabulary, complex syntactic structures and diverse discourse phenomena, as described in Section 3. The data is openly released as part of the E2E NLG challenge.1 We establish a baseline on the dataset in Section 4, using one of the previous end-to-end approaches. Introduction The natural language generation (NLG) component"
W17-5525,E06-1009,0,\N,Missing
W17-5525,P15-1044,1,\N,Missing
W17-5525,P16-2008,1,\N,Missing
W17-5525,W16-3622,1,\N,Missing
W17-5525,C16-1105,0,\N,Missing
W18-5701,D16-1127,0,0.383285,"et al., 2017), and were therefore of sufficient quality for a production system receiving thousands of calls per day. We also compare our model to a recently published dualencoder response selection model by Lu et al. (2017) based on an approach principally close to ours. 4.1 Handcrafted ranker In the handcrafted approach, several turn-level and dialogue-level features are calculated, and a linear combination of those feature values with manually adjusted coefficients is used to predict the final ranking. The list of features includes: • coherence, information flow, and dullness as defined by Li et al. (2016); • overlap between the context and the response with regards to named entities and noun phrases; • topic divergence between the context turns and the response – topics are represented using the Latent Dirichlet Allocation (LDA) model (Hoffman et al., 2010); • sentiment polarity, as computed by the NLTK Vader sentiment analyser (Gilbert and Hutto, 2014).2 P red(C, r) = σ(L(Sem(C, r) ⊕ f (C, r))) (2) where: L(x) = ReLU(M x + b) is the layer used in the Predictor (the number of such layers is a model parameter), Sem = L(Enc(C, r)) is the vector of semantic context-response features, and f (C, r)"
W18-5701,D14-1179,0,0.0166892,"Missing"
W18-5701,W15-4640,0,0.018873,"ork Work on response ranking for conversational systems has been been growing rapidly in recent years. Some authors employ ranking based on heuristically defined measures: Yu et al. (2015, 2016) use a heuristic based on keyword matching, part-of-speech filters, and Word2Vec similarity. (Krause et al., 2017) apply standard information retrieval metrics (TF-IDF) with importance weighting for named entities. However, most of the recent research attempts to train the ranking function from large amounts of conversational data, as we do. Some authors use task-based conversations, such as IT forums (Lowe et al., 2015) or customer services (Lu et al., 2017; Kumar et al., 2018), while others focus on online conversations on social media (e.g. Wu et al., 2016; Al-Rfou et al., 2016). The basic approach to learning the ranking function in most recent work is the same (e.g. Lowe et al., 2015; Al-Rfou et al., 2016; Wu et al., 2016): the predictor is taught to rank positive responses taken from real dialogue data higher than randomly sampled negative examples. Some of the approaches do not even include rich dialogue Acknowledgements This research received funding from the EPSRC project MaDrIgAL (EP/N017536/1). The"
W18-5701,P05-1045,0,0.0185211,"e the 3 most recent system and user turns). They are encoded into a latent representation using a single shared RNN encoder based on GRU cells (Cho et al., 2014). The context embedding vectors are then summed up and concatenated with the response embedding (Eq. 1): Enc(C, r) = X RNN(Ci ) ⊕ RNN(r) (1) i 4 where C is the dialogue context and r is a response candidate. The context and the response are represented using combined word-agent tokens (where agent is either a specific bot from the ensemble or the user) and are concatenated with the lists of named entities extracted using Stanford NER (Finkel et al., 2005). All the word-agent tokens and named entities share the same unified vocabulary. Encoder outputs, along with additional dialogue features such as context and response sentiment, timestamp, and bot names in the context and the response, go into the Predictor, a feed-forward neural network (MLP) whose output is the resulting rating (Eq. 2): We compare our neural ranker to two other models also developed during the competition: handcrafted and linear rankers — all three were deployed live in the Alana Alexa Prize 2017 finalist system (Papaioannou et al., 2017), and were therefore of sufficient q"
W18-5701,C16-1063,0,0.0876872,"istically defined measures: Yu et al. (2015, 2016) use a heuristic based on keyword matching, part-of-speech filters, and Word2Vec similarity. (Krause et al., 2017) apply standard information retrieval metrics (TF-IDF) with importance weighting for named entities. However, most of the recent research attempts to train the ranking function from large amounts of conversational data, as we do. Some authors use task-based conversations, such as IT forums (Lowe et al., 2015) or customer services (Lu et al., 2017; Kumar et al., 2018), while others focus on online conversations on social media (e.g. Wu et al., 2016; Al-Rfou et al., 2016). The basic approach to learning the ranking function in most recent work is the same (e.g. Lowe et al., 2015; Al-Rfou et al., 2016; Wu et al., 2016): the predictor is taught to rank positive responses taken from real dialogue data higher than randomly sampled negative examples. Some of the approaches do not even include rich dialogue Acknowledgements This research received funding from the EPSRC project MaDrIgAL (EP/N017536/1). The Titan Xp used for this research was donated by the NVIDIA Corporation. 6 References Jeff Johnson, Matthijs Douze, and Herv´e J´egou. 2017. B"
W18-5701,W16-3649,0,0.0293196,"o two parts accordingly: • sample efficiency – the number of data points needed for the model to train. As such, it is useful to specify an order of magnitude of the training set size for different types of machine learning models; • annotation efficiency – the amount of annotation Introduction Chatbots, or socialbots, are dialogue systems aimed at maintaining an open-domain conversation with the user spanning a wide range of topics, with the main objective of being engaging, entertaining, and natural. Under one of the current approaches to such systems, the bot ensemble (Serban et al., 2017; Yu et al., 2016; Song et al., 2016), a collection, or ensemble, of different bots is used, each of which proposes a candidate response to the user’s input, and a response ranker selects the best 1 Code and trained models are available at https://github.com/WattSocialBot/alana_ learning_to_rank 1 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 1–8 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 Variables rating/length rating/positive feedback rating/negative feedback length/positive fee"
W18-5709,W18-6514,1,0.717195,"Missing"
W18-5709,W07-0734,0,0.0774496,"M-HRED–attn) for a given context size. Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.6710 0.6712 0.6797 0.6799 0.6872 0.690"
W18-5709,D16-1203,0,0.0307088,"rounding in KB. We show textual context as well as relevant knowledge base input (and omit image context) for brevity’s sake. While our model uses a context of 5, for simplicity, we show only 2 previous turns. probe found that the orientations for retrieved images may not directly follow the description in the query (KB). There are other intents for which even KB does not help, such as those requiring user modelling. 5 model outputs showed a substantial improvement (over 3 B LEU points) on incorporating KB information, integrating visual context still remains a bottleneck, as also observed by Agrawal et al. (2016); Qian et al. (2018). This suggests the need for a better mechanism to encode visual context. Since our KB-grounded model assumes user intent annotation and KB queries as additional inputs, we plan to build a model to provide them automatically. Conclusion and Future Work This work focuses on the task of textual response generation in multimodal task-oriented dialogue system. We used the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) for experiments and introduced a novel conversational model grounded in language, vision and Knowledge Base (KB). Our best performing mod"
W18-5709,E06-2009,0,0.0825572,"Missing"
W18-5709,P04-1077,0,0.0794841,"Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.6710 0.6712 0.6797 0.6799 0.6872 0.6909 0.6917 0.6913 0.6923 In summary"
W18-5709,D15-1166,0,0.092393,"elebrity profiles using basic pattern matching over the user utterance. For each of the celebrities in the user query, we order the corresponding synsets by their probability of endorsement. If no celebrity is found, we use synset information from the query to extract celebrities which endorse the corresponding synset. 3.3 Experiments and Results 4.2 Implementation We used PyTorch3 (Paszke et al., 2017) for our experiments.4 We did not use any kind of delexicalisation5 and rely on our model to directly learn Input feeding decoder We use an input feeding decoder with the attention mechanism of Luong et al. (2015). We concatenate the KB input hkb n with the decoder input cxt (cf. Eq. (10), where hdec n,0 = hN ). The rationale behind this late fusion of KB representation is that KB input remains the same for a given context and 2 We used the same training-development-test split as provided by the dataset authors. 3 https://pytorch.org/ 4 Code can be found at: https://github.com/shubhamagarwal92/mmd 5 Replacing specific values with placeholders (Henderson et al., 2014). 62 Intent from the conversational history and KB. All encoders and decoders are based on 1-layer GRU cells (Cho et al., 2014) with 512 a"
W18-5709,P02-1040,0,0.102215,"stark uplift (M-HRED– attn–kb vs. M-HRED–attn) for a given context size. Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.671"
W18-5709,W13-4067,0,0.0783197,"Missing"
W18-5709,E17-1042,0,0.0341701,"al model where an encoded knowledge base (KB) representation is appended to the decoder input. Our model substantially outperforms strong baselines in terms of text-based similarity measures (over 9 BLEU points, 3 of which are solely due to the use of additional information from the KB). 1 Introduction Conversational agents have become ubiquitous, with variants ranging from open-domain conversational chit-chat bots (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to domainspecific task-based dialogue systems (Singh et al., 2000; Rieser and Lemon, 2010, 2011; Young et al., 2013; Wen et al., 2017). Our work builds upon the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017), which contains dialogue sessions in the ecommerce (fashion) domain. Figure 1 illustrates an example chat session with multimodal interaction between the user and the system. We focus on the task of generating textual responses conditioned on the previous conversational history. Traditional goal-oriented dialogue systems relied on slot-filling approach to this task, i.e. explicit modelling of all attributes in the domain (Lemon 2 Related Work With recent progress in deep learning, there is contin"
W18-5709,N15-1173,0,0.0187718,"h multimodal interaction between the user and the system. We focus on the task of generating textual responses conditioned on the previous conversational history. Traditional goal-oriented dialogue systems relied on slot-filling approach to this task, i.e. explicit modelling of all attributes in the domain (Lemon 2 Related Work With recent progress in deep learning, there is continued interest in the tasks involving both vision and language, such as image captioning (Xu et al., 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015), visual storytelling (Huang et al., 2016), video description (Venugopalan et al., 2015b,a) or dialogue grounded in visual context (Antol et al., 2015; Das et al., 2017; Tapaswi et al., 2016). Bordes et al. (2016) and Ghazvininejad et al. (2017) presented knowledge-grounded neural models; however, these are uni-modal in nature, involve only textual interaction and do not take into account the conversational history in a dia59 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 59–66 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 Figure 2: Schematic diagram of"
W18-5709,E09-1078,1,\N,Missing
W18-6514,D16-1203,0,0.0559846,"arch-based multimodal dialogue by learning from the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017). We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of Saha et al. (2017) by modelling the full multimodal context. Contrary to their results, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018). Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context. This indicates that we need better visual models to enTable 1 provides results for different configurations of our model (“T” stands for text-only in the encoder, “M” for multimodal, and “attn” for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation"
W18-6514,P02-1040,0,0.103838,"ge and long term context: “Will the 5th result go well with a large sized messenger bag?”, inference over aggregate of images: “List more in the upper material of the 5th image and style as the 3rd and the 5th”, co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of the dataset for the model. This is done since the authors originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3). 3.3 Analysis and Results We report sentence-level B LEU -4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017). 1 https://pytorch.org/ Our code is freely available at: https://github.com/shubhamagarwal92/mmd 3 In future, we plan to exploit state-of-the-art frameworks such as ResNet or DenseNet and fine tune the image encoder jointly, during the training of the model. 2 131 Figure 4: Examples of predictions using M-HRED–attn (5). Recall, we are focusing on generating textual responses. Our model predictions are shown in blue while the true gold target in red. We are showing"
W18-6514,W04-3250,0,0.0306892,"Missing"
W18-6514,W07-0734,0,0.289957,"he 5th result go well with a large sized messenger bag?”, inference over aggregate of images: “List more in the upper material of the 5th image and style as the 3rd and the 5th”, co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of the dataset for the model. This is done since the authors originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3). 3.3 Analysis and Results We report sentence-level B LEU -4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017). 1 https://pytorch.org/ Our code is freely available at: https://github.com/shubhamagarwal92/mmd 3 In future, we plan to exploit state-of-the-art frameworks such as ResNet or DenseNet and fine tune the image encoder jointly, during the training of the model. 2 131 Figure 4: Examples of predictions using M-HRED–attn (5). Recall, we are focusing on generating textual responses. Our model predictions are shown in blue while the true gold target in red. We are showing only the previous user utterance f"
W18-6514,E09-1078,1,\N,Missing
W18-6514,P04-1077,0,\N,Missing
W18-6539,2007.mtsummit-ucnlg.14,0,0.0796589,"course phenomena. We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. 1 Introduction 2 This paper summarises the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems (SDSs). Shared tasks have become an established way of pushing research boundaries in the field of natural language processing, with NLG benchmarking tasks running since 2007 (Belz and Gatt, 2007). This task is novel in that it poses new challenges for recent end-to-end, data-driven NLG systems for SDSs which jointly learn sentence planning and surface realisation and do not require costly semantic alignment between meaning representations (MRs) and the corresponding natural language reference texts, e.g. (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015b; Mei et al., 2.1 The E2E NLG dataset Data Collection Procedure In order to maximise the chances for data-driven end-to-end systems to produce high quality output, we aim to provide training data in high quality and large quantity. To c"
W18-6539,W17-4755,0,0.0373199,"a-driven end-to-end systems to produce high quality output, we aim to provide training data in high quality and large quantity. To collect data in large enough quantity, we use crowdsourcing with automatic 1 Note that as opposed to the “classical” definition of NLG (Reiter and Dale, 2000; Gatt and Krahmer, 2018), generation for dialogue systems does not involve content selection and its sentence planning stage may be less complex. 2 In comparison, the well established Conference in Machine Translation WMT’17 (running since 2006) received submissions from 31 institutions to a total of 8 tasks (Bojar et al., 2017a). 322 Proceedings of The 11th International Natural Language Generation Conference, pages 322–328, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics MR Reference name[The Wrestlers], priceRange[cheap], customerRating[low] The wrestlers offers competitive prices, but isn’t highly rated by customers. Figure 1: Example of an MR-reference pair. Figure 2: An example pictorial MR. Attribute Data Type name verbatim string eatType dictionary familyFriendly boolean priceRange dictionary food dictionary near verbatim string area dictionary customerRating di"
W18-6539,W18-6555,0,0.089427,"Missing"
W18-6539,W18-6539,1,0.0512975,"Missing"
W18-6539,W16-3622,1,0.892594,"Missing"
W18-6539,N16-1086,0,0.141498,"Missing"
W18-6539,P16-2008,1,0.880417,"Missing"
W18-6539,D17-1238,1,0.901966,"Missing"
W18-6539,W18-6556,0,0.0819123,"Missing"
W18-6539,W17-5525,1,0.908272,"Missing"
W18-6539,N18-2012,1,0.863228,"Missing"
W18-6539,W17-3518,0,0.0683021,"which is about 10 times larger and also more complex than previous datasets. For the shared challenge, we received 62 system submissions by 17 institutions from 11 countries, with about 1/3 of these submissions coming from industry. We assess the submitted systems by comparing them to a challenging baseline using automatic as well as human evaluation. We consider this level of participation an unexpected success, which underlines the timeliness of this task.2 While there are previous studies comparing a limited number of end-to-end NLG approaches (Novikova et al., 2017a; Wiseman et al., 2017; Gardent et al., 2017), this is the first research to evaluate novel end-to-end generation at scale and using human assessment. This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets. The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical ric"
W18-6539,W16-6644,1,0.916958,"Missing"
W18-6539,W17-1608,0,0.089051,"Missing"
W18-6539,W18-6557,0,0.147702,"Missing"
W18-6539,N18-1014,0,0.685577,"ously unseen, i.e. none of them overlaps with training/development sets, even if restaurant names are removed. MRs for the test set were only released to participants two weeks before the challenge submission deadline on October 31, 2017. Participants had no access to test reference texts. The whole dataset is now freely available at the E2E NLG Challenge website at: http://www.macs.hw.ac.uk/ InteractionLab/E2E/ 323 System BLEU NIST METEOR ROUGE-L CIDEr norm. avg. ♥ TG EN baseline (Novikova et al., 2017b): seq2seq with MR classifier reranking 0.6593 8.6094 0.4483 0.6850 2.2338 0.5754 ♥ S LUG (Juraska et al., 2018): seq2seq-based ensemble (LSTM/CNN encoders, LSTM decoder), heuristic slot aligner reranking, data augmentation 0.6619 8.6130 0.4454 0.6772 2.2615 0.5744 ♥ TNT1 (Oraby et al., 2018): TG EN with data augmentation 0.6561 8.5105 0.4517 0.6839 2.2183 0.5729 ♥ NLE (Agarwal et al., 2018): fully lexicalised character-based seq2seq with MR classification reranking 0.6534 8.5300 0.4435 0.6829 2.1539 0.5696 ♥ TNT2 (Tandon et al., 2018): TG EN with data augmentation 0.6502 8.5211 0.4396 0.6853 2.1670 0.5688 ♥ H ARV (Gehrmann et al., 2018): fully lexicalised seq2seq with copy mechanism, coverage penalty r"
W18-6539,C16-1105,0,0.0616095,"Missing"
W18-6539,W14-3301,0,0.078614,"Missing"
W18-6539,P10-1157,0,0.266637,"Missing"
W18-6539,W18-6558,0,0.0749736,"Missing"
W18-6539,W15-4639,0,0.0491163,"Missing"
W18-6539,N16-1015,0,0.0910322,"Missing"
W18-6539,D15-1199,0,0.285096,"Missing"
W18-6539,D17-1239,0,0.14316,"Missing"
W18-6539,W17-4717,0,\N,Missing
W19-5945,N07-2038,0,0.0912696,"Missing"
W19-5945,W15-4654,0,0.0156214,"l system, which can be adapted from a source domain, is equivalent to that of a one-dimensional baseline, which can only be trained from scratch. 1 Introduction Data-driven approaches to spoken dialogue systems (SDS) are limited by their reliance on substantial amounts of annotated data in the target domain. This can be addressed by considering transfer learning techniques, e.g. (Taylor and Stone, 2009), in which data from a source domain is leveraged to improve learning in a target domain. In particular, domain adaptation has been used in the context of dialogue systems (Gaši´c et al., 2017; Wang et al., 2015; Wen et al., 2016), focusing on identifying and exploiting similarities between domain ontologies in slot-filling tasks. In contrast to this previous work, we take a multidimensional approach, which combines machine learning with linguistic theory. Following Bunt (2011), we exploit the linguistic phenomenon that utterances serve more than one function in a conversation, i.e. they have more than one dimension (see Section 2).1 For example, the utterance “On what date would you like to fly to London?” both asks a task-oriented question, and provides feedback about understanding the requested de"
W19-5945,N16-1015,0,0.0550838,"Missing"
W19-5945,W15-4622,0,0.0421654,"Missing"
W19-5945,L16-1500,0,0.027502,"Missing"
W19-8644,C12-1008,0,0.0214154,"ble 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that th"
W19-8644,K16-1002,0,0.0500592,"Missing"
W19-8644,K18-1031,0,0.0234145,"NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evalua"
W19-8644,W17-5534,0,0.0278203,"Missing"
W19-8644,W17-3518,0,0.0514265,"tic instances. We use a partial delexicalisation (replacing names with placeholders).5 Datasets 5.2 We experiment on the following two datasets, both in the restaurant/hotel information domain: • NEM3 (Novikova et al., 2017) – Likert-scale rated outputs (scores 1–6) of 3 NLG systems over 3 datasets, totalling 2,460 instances. • E2E system rankings (Duˇsek et al., 2019) – outputs of 21 systems on a single NLG dataset with 2,979 5-way relative rankings. We choose these two datasets because they contain human-assessed outputs from a variety of NLG systems. Another candidate is the WebNLG corpus (Gardent et al., 2017), which we leave for future work due to MR format differences. Although both selected datasets contain ratings for multiple criteria (informativeness, naturalness and quality for NEM and the latter two for E2E), we follow Duˇsek et al. (2017) and focus on the overall quality criterion in our experiments as it takes both semantic accuracy and fluency into account. Model Settings We evaluate our model in several configurations, with increasing amounts of synthetic training data. Note that even setups using training human references (i.e. additional in-domain data) are still “referenceless”—they"
W19-8644,W18-6505,0,0.0177058,"subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evaluation—distinguishing between human- and machine-generated outputs (Goodfellow et al., 2014). This approach is employed in generators for random text (Bowman et al., 2016) and dialogue responses (Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017). We argue that our approach is more explainable with users being able to reason with the ord"
W19-8644,D14-1020,0,0.0407575,"Missing"
W19-8644,W04-3250,0,0.0259493,"baseline metrics and the original RatPred system are taken over from Duˇsek et al. (2017). Configurations marked with “*” use human references for test instances (this includes word-overlap-based metrics such as BLEU). System Our base system + generated pairs based on training system outputs + generated pairs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza e"
W19-8644,C16-1105,0,0.0411811,"Missing"
W19-8644,W07-0734,0,0.069244,"ilable, and we release our experimental code on GitHub.1 2 The Task(s) The task of NLG QE for ratings is to assign a numerical score to a single NLG output, given its input MR, such as a dialogue act (consisting of the main intent, attributes and values). The score can be e.g. on a Likert scale in the 1-6 range (Novikova et al., 2017). In a pairwise ranking task, the QE system is given two outputs of different NLG systems for the same MR, and decides which one has better quality (see Figure 1). As opposed to automatic word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), no human reference texts for the given MR are required. This widens the scope of possible applications – QE systems can be used for previously unseen MRs. 3 Jointly learning to rank and rate was first introduced by Sculley (2010) for support vector machines and similar approaches have been applied for image classification (Park et al., 2017; Liu et al., 2018) as well as audio classification (Lee et al., 2016), However, we argue that the application for text classification/QE is novel, as is the implementation as a single neural network with two parts that share parameters, capable of trainin"
W19-8644,D17-1230,0,0.0685314,"Missing"
W19-8644,W04-1013,0,0.0575487,"Missing"
W19-8644,P17-1103,0,0.0220179,"on training system outputs + generated pairs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE sy"
W19-8644,P10-1157,0,0.0931245,"Missing"
W19-8644,L16-1575,0,0.0313741,"Missing"
W19-8644,W18-7005,0,0.0266463,"ng insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utteranc"
W19-8644,D16-1228,0,0.0189365,"airs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predic"
W19-8644,P02-1040,0,0.110835,"king. Both datasets are freely available, and we release our experimental code on GitHub.1 2 The Task(s) The task of NLG QE for ratings is to assign a numerical score to a single NLG output, given its input MR, such as a dialogue act (consisting of the main intent, attributes and values). The score can be e.g. on a Likert scale in the 1-6 range (Novikova et al., 2017). In a pairwise ranking task, the QE system is given two outputs of different NLG systems for the same MR, and decides which one has better quality (see Figure 1). As opposed to automatic word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), no human reference texts for the given MR are required. This widens the scope of possible applications – QE systems can be used for previously unseen MRs. 3 Jointly learning to rank and rate was first introduced by Sculley (2010) for support vector machines and similar approaches have been applied for image classification (Park et al., 2017; Liu et al., 2018) as well as audio classification (Lee et al., 2016), However, we argue that the application for text classification/QE is novel, as is the implementation as a single neural network with two parts that"
W19-8644,J18-3002,0,0.0127542,"as of NLP, such as machine translation (MT) (Specia et al., 2010, 2018), research on QE in natural language generation (NLG) from structured meaning representations (MR) such as dialogue acts is relatively recent (Duˇsek et al., 2017; Ueffing et al., 2018) and often focuses on output fluency only (Tian et al., 2018; Kann et al., 2018). In contrast to traditional metrics, QE does not rely on gold-standard human reference texts (Specia et al., 2010), which are expensive to obtain, do not cover the full output space, and are not accurate on the level of individual outputs (Novikova et al., 2017; Reiter, 2018). Automatic QE for NLG has several possible use cases that can improve NLG quality and reliability. For example, rating individual NLG outputs allows to ensure a minimum output quality and engage a backup, e.g., template-based NLG system, if a certain threshold is not met. Relative ranking of multiple NLG outputs can be used directly within a system to rerank n-best outputs or to guide system development, selecting optimal system parameters or comparing to state of the art. In this paper, we present a novel model that jointly learns to perform both tasks—rating individual outputs as well as pa"
W19-8644,W14-3301,0,0.0608728,"Missing"
W19-8644,W18-6530,0,0.0526372,"Missing"
W19-8644,W18-6512,0,0.0144683,"tive topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar t"
W19-8644,P19-1256,0,0.0197181,"et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evaluation—distinguishing between human- and machine-generated outputs (Goodfellow et al., 2014). This approach is employed in generators for random text (Bowman et al., 2016) and dialogue responses (Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017). We argue that our approach is more explainable with users being able to reason with the ordinal output score. Acknowledgments This"
W19-8644,N18-3007,0,0.0665249,"Missing"
W19-8644,D17-1238,1,0.880678,"Missing"
W19-8644,D15-1199,0,0.067814,"Missing"
W19-8644,D17-1239,0,0.0306122,"rmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG o"
W19-8644,E17-1019,0,\N,Missing
W19-8652,P16-2008,1,0.92388,"Missing"
W19-8652,W18-6539,1,0.902972,"Missing"
W19-8652,P19-1256,0,0.282332,"Missing"
W19-8652,D18-1407,0,0.0596321,"Missing"
W19-8652,W16-6644,1,0.887358,"Missing"
W19-8652,P02-1040,0,0.103431,"Missing"
W19-8652,N18-1014,0,0.693857,"Missing"
W19-8652,W17-3204,0,0.071609,"Missing"
W19-8652,W07-0734,0,0.22432,"Missing"
W19-8652,W04-1013,0,0.0383535,"Missing"
W19-8652,P10-1157,0,0.144936,"Missing"
W19-8652,N16-1086,0,0.0506275,"Missing"
W19-8652,W18-6535,0,0.0981499,"Missing"
W19-8652,D18-1437,0,0.104605,"Missing"
W19-8652,N16-1015,0,0.0410745,"Missing"
W19-8652,D15-1199,0,0.108794,"Missing"
W19-8652,D17-1239,0,0.114757,"Missing"
W19-8670,2001.mtsummit-papers.2,0,0.276262,"works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation, the recent work of Moussallem et al. (2018) is applied to Portuguese, but is largely rule-based. The works of Chen et al. (2010) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; th"
W19-8670,W12-1510,0,0.0207263,"rent from the ones found in the training set and/or occur less frequently (see bottom example in Figure 5, cf. Section 2.6). We believe that an increased amount of training data could improve the situation. 5 Related Work NLG experiments for non-English languages are relatively rare and fully trainable approaches even rarer. Our work is, to our knowledge, the first application of neural NLG to a non-English language for data-to-text generation. Most works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of S"
W19-8670,C10-1012,0,0.016638,"There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation, the recent work of Moussallem et al. (2018) is applied to Portuguese, but is largely rule-based. The works of Chen et al. (2010) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; they generate English and Korean sport commentary sentences using an inve"
W19-8670,W07-0735,0,0.0214481,"differences against the input DA is used as penalty. using the dictionary-based morphological generator of Strakov´a et al. (2014) to obtain the inflected word forms. In the lemma-tag mode, the set of possible output tokens is reduced compared to direct token generation, but the postprocessing step is much simpler than using a full syntactic surface realizer. Moreover, the generated morphological tags following slot placeholders can be used to limit the scope of possible surface forms during lexicalization (see Section 3.3). This approach is inspired by similar approaches in phrase-based MT (Bojar, 2007; Toutanova et al., 2008; Fraser, 2009) and was developed in parallel to recent similar experiments with two-step neural MT (Nadejde et al., 2017; Tamchyna et al., 2017). We compare the lemma-tag generation mode against the TGen default direct word-form generation mode in our experiments. 3.1 3.3 Basic Extensions We added two features fairly standard in seq2seqbased models but absent from TGen: • Bidirectional encoder (Bahdanau et al., 2015) – the input sequence is encoded in both directions and the resulting hidden states are joined. We added this for both the main seq2seq generator and the r"
W19-8670,W11-2817,0,0.0320358,"e believe that an increased amount of training data could improve the situation. 5 Related Work NLG experiments for non-English languages are relatively rare and fully trainable approaches even rarer. Our work is, to our knowledge, the first application of neural NLG to a non-English language for data-to-text generation. Most works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers teste"
W19-8670,W18-6507,0,0.0136603,"-English languages are relatively rare and fully trainable approaches even rarer. Our work is, to our knowledge, the first application of neural NLG to a non-English language for data-to-text generation. Most works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation, the recent work of Moussallem et al. (2018) is applied to"
W19-8670,W18-6506,0,0.0224438,"Missing"
W19-8670,N18-2085,0,0.0142865,"to unseen entity names: sets of entities are open (potentially infinite and subject to change) while training data is scarce. However, the verbatim insertion assumption does not hold for languages with extensive noun inflection – attribute values need to be inflected here to produce fluent outputs (see Figure 1). This paper presents the following contributions: • We create a novel dataset for Czech delexicalized generation; this extends the typical task of data-to-text NLG by requiring attribute value inflection (Section 2). We choose Czech as an example of a morphologically complex language (Cotterell et al., 2018) with a large set of NLP tools readily available (e.g. Popel and ˇ Zabokrtsk´ y, 2010; Strakov´a et al., 2014; Straka and Strakov´a, 2017). • We present baseline models based on the TGen sequence-to-sequence (seq2seq) system (Duˇsek and Jurˇc´ıcˇ ek, 2016), with two novel extensions to the model for our task (Section 3): – A model for lexicalization, i.e., selecting the correct inflected surface form for a slot value, based on a recurrent neural network language model (RNN LM); – A new generation mode, where the seq2seq generator produces interleaved sequences of lemmas (base word forms) and m"
W19-8670,W14-4311,1,0.877069,"Missing"
W19-8670,W15-3009,1,0.878505,"Missing"
W19-8670,P16-2008,1,0.935656,"Missing"
W19-8670,N16-1077,0,0.0185784,"10) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; they generate English and Korean sport commentary sentences using an inverted (non-neural) semantic parser. Our dataset is ca. 2.5 times larger and more complex, given the slot value inflection. Other works on neural non-English NLG solve in fact different tasks from ours: Chinese poetry generation (Zhang and Lapata, 2014; Yi et al., 2017; Wang et al., 2016), non-task-oriented response generation in chatbots (Xing et al., 2016, 2017), or morphological inflection (e.g. Faruqui et al., 2016; Kann and Sch¨utze, 2016). 570 6 Conclusions and Future Work We presented the first dataset targeted at end-toend neural non-English NLG, containing Czech texts from the restaurant domain. We show that the task of data-to-text NLG here is harder as slot values require morphological inflection. We apply to our data the freely available, state-of-the-art TGen NLG system (Duˇsek and Jurˇc´ıcˇ ek, 2016) based on the seq2seq architecture, and we implement two extensions for Czech: (1) an RNN LM model to select the correct inflected surface form for slot values and (2) lemma-tag generation mode, wh"
W19-8670,W09-0420,0,0.0348353,"used as penalty. using the dictionary-based morphological generator of Strakov´a et al. (2014) to obtain the inflected word forms. In the lemma-tag mode, the set of possible output tokens is reduced compared to direct token generation, but the postprocessing step is much simpler than using a full syntactic surface realizer. Moreover, the generated morphological tags following slot placeholders can be used to limit the scope of possible surface forms during lexicalization (see Section 3.3). This approach is inspired by similar approaches in phrase-based MT (Bojar, 2007; Toutanova et al., 2008; Fraser, 2009) and was developed in parallel to recent similar experiments with two-step neural MT (Nadejde et al., 2017; Tamchyna et al., 2017). We compare the lemma-tag generation mode against the TGen default direct word-form generation mode in our experiments. 3.1 3.3 Basic Extensions We added two features fairly standard in seq2seqbased models but absent from TGen: • Bidirectional encoder (Bahdanau et al., 2015) – the input sequence is encoded in both directions and the resulting hidden states are joined. We added this for both the main seq2seq generator and the reranker. • Dropout (Hinton et al., 2012"
W19-8670,W09-0613,0,0.0518291,"less frequently (see bottom example in Figure 5, cf. Section 2.6). We believe that an increased amount of training data could improve the situation. 5 Related Work NLG experiments for non-English languages are relatively rare and fully trainable approaches even rarer. Our work is, to our knowledge, the first application of neural NLG to a non-English language for data-to-text generation. Most works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al."
W19-8670,W18-6505,0,0.0187633,"tested on English, they do not need to handle grammar complexities not present in English. A prime example is the delexicalization technique used by most current generators (e.g., Oh and Rudnicky, 2000; Mairesse et al., 2010; Wen et al., 2015a,b; Juraska 1 http://www.cs.utexas.edu/users/ml/ clamp/sportscasting/ et al., 2018): It is generally assumed that attribute (slot) values from the input meaning representation (MR) can be replaced by placeholders during generation and inserted into the output verbatim. Delexicalization or an analogous technique, such as a copy mechanism (Gu et al., 2016; Gehrmann et al., 2018), is required for most generation scenarios to allow generalization to unseen entity names: sets of entities are open (potentially infinite and subject to change) while training data is scarce. However, the verbatim insertion assumption does not hold for languages with extensive noun inflection – attribute values need to be inflected here to produce fluent outputs (see Figure 1). This paper presents the following contributions: • We create a novel dataset for Czech delexicalized generation; this extends the typical task of data-to-text NLG by requiring attribute value inflection (Section 2). W"
W19-8670,P16-1154,0,0.0312816,"erators are only tested on English, they do not need to handle grammar complexities not present in English. A prime example is the delexicalization technique used by most current generators (e.g., Oh and Rudnicky, 2000; Mairesse et al., 2010; Wen et al., 2015a,b; Juraska 1 http://www.cs.utexas.edu/users/ml/ clamp/sportscasting/ et al., 2018): It is generally assumed that attribute (slot) values from the input meaning representation (MR) can be replaced by placeholders during generation and inserted into the output verbatim. Delexicalization or an analogous technique, such as a copy mechanism (Gu et al., 2016; Gehrmann et al., 2018), is required for most generation scenarios to allow generalization to unseen entity names: sets of entities are open (potentially infinite and subject to change) while training data is scarce. However, the verbatim insertion assumption does not hold for languages with extensive noun inflection – attribute values need to be inflected here to produce fluent outputs (see Figure 1). This paper presents the following contributions: • We create a novel dataset for Czech delexicalized generation; this extends the typical task of data-to-text NLG by requiring attribute value i"
W19-8670,W11-2123,0,0.147753,"Missing"
W19-8670,W18-6508,0,0.0233129,"Missing"
W19-8670,N18-1014,0,0.0315972,"Missing"
W19-8670,P16-2090,0,0.0527422,"Missing"
W19-8670,C10-2062,0,0.0177719,"gual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation, the recent work of Moussallem et al. (2018) is applied to Portuguese, but is largely rule-based. The works of Chen et al. (2010) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; they generate English and Korean sport commentary sentences using an inverted (non-neural) semantic parser. Our dataset is ca. 2.5 times larger and more complex, given the slot value inflection. Other works on neural non-English NLG solve in fact different tasks from ours: Chinese poetry generation (Zhang and Lapata, 2014; Yi et al., 2017; Wang et al., 2016), non-task-oriented response generation in chatbots (Xing et al., 2016, 2017), or morphological inflection (e.g. Faruqui et al., 2016; Kann an"
W19-8670,W04-3250,0,0.0482923,"014 4.4453 22.22 41.26 1.8801 4.5460∗ 22.56 41.73 1.9796 SER 00.70 00.70 00.70 01.85 01.85 01.85 02.30 02.30 02.30 03.08 03.08 03.08 Table 3: Automatic metrics results. See Section 4.2 for metrics; scores are averaged over 5 different random initializations, all scores except for NIST and CIDEr are percentages. ∗ = significantly better than the corresponding most frequent baseline lexicalizer, † = significantly better than the corresponding word forms mode, ‡ = significantly better than the corresponding (de)lexicalized input DAs. Significance was assessed using pairwise bootstrap resampling (Koehn, 2004), p &lt; 0.01. Input DAs Generator mode Word forms Delexicalized Lemma-tag Word forms Lexicalized Lemma-tag Lexicalizer Most frequent RNN LM Most frequent RNN LM Most frequent RNN LM Most frequent RNN LM S 8 8 12 12 14 14 15 15 R 0 0 2 2 5 5 4 4 F 5 5 5 5 14 14 6 6 I 11 11 11 11 6 6 4 4 L 57 25 45 6 34 10 34 4 F+I+L 73 41 61 22 54 30 44 14 Σ 81 49 75 36 73 49 63 33 Table 4: Manual evaluation results on 100 sampled sentences – absolute numbers of different types of errors (S = semantic errors, R = repetition, F = fluency problems except lexicalization, I = impossible to lexicalize correctly with t"
W19-8670,C16-1105,0,0.0474746,"Missing"
W19-8670,W07-0734,0,0.0685086,"he most frequent baseline just memorizes surface form frequencies in the training data. To reduce the effect of random initialization, we train five runs using different random seeds and use results of all of them for evaluation. In addition, we fix the random seeds so that identical seq2seq generators and rerankers are used in setups that only differ in the lexicalization method. 4.2 Metrics We use the suite of word-overlap-based automatic metrics from the E2E NLG Challenge (Duˇsek et al., 2019),11 supporting BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007) and CIDEr (Vedantam et al., 2015). Although multiple texts often correspond to the same delexicalized DA, we treat each instance individually both in training and testing since the particular slot values influence the shape of the whole sentence (see Sections 2.4 and 3.4). This means that only a single reference output per instance is available to be used with automatic metrics (see Section 4.3). 10 The main generator uses embedding and LSTM cell size 200, learning rate 0.005, dropout rate 0.5, and batch size 20. At least 50 and up to 1000 training data passes are used, with early stopping if"
W19-8670,W04-1013,0,0.0185644,"ble surface forms; the most frequent baseline just memorizes surface form frequencies in the training data. To reduce the effect of random initialization, we train five runs using different random seeds and use results of all of them for evaluation. In addition, we fix the random seeds so that identical seq2seq generators and rerankers are used in setups that only differ in the lexicalization method. 4.2 Metrics We use the suite of word-overlap-based automatic metrics from the E2E NLG Challenge (Duˇsek et al., 2019),11 supporting BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007) and CIDEr (Vedantam et al., 2015). Although multiple texts often correspond to the same delexicalized DA, we treat each instance individually both in training and testing since the particular slot values influence the shape of the whole sentence (see Sections 2.4 and 3.4). This means that only a single reference output per instance is available to be used with automatic metrics (see Section 4.3). 10 The main generator uses embedding and LSTM cell size 200, learning rate 0.005, dropout rate 0.5, and batch size 20. At least 50 and up to 1000 training data passe"
W19-8670,W14-4412,0,0.0658242,"Missing"
W19-8670,P10-1157,1,0.808664,"Missing"
W19-8670,P02-1040,0,0.104841,"not require any training, it simply uses the list of possible surface forms; the most frequent baseline just memorizes surface form frequencies in the training data. To reduce the effect of random initialization, we train five runs using different random seeds and use results of all of them for evaluation. In addition, we fix the random seeds so that identical seq2seq generators and rerankers are used in setups that only differ in the lexicalization method. 4.2 Metrics We use the suite of word-overlap-based automatic metrics from the E2E NLG Challenge (Duˇsek et al., 2019),11 supporting BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007) and CIDEr (Vedantam et al., 2015). Although multiple texts often correspond to the same delexicalized DA, we treat each instance individually both in training and testing since the particular slot values influence the shape of the whole sentence (see Sections 2.4 and 3.4). This means that only a single reference output per instance is available to be used with automatic metrics (see Section 4.3). 10 The main generator uses embedding and LSTM cell size 200, learning rate 0.005, dropout rate 0.5, and batch size 20."
W19-8670,W16-6630,0,0.0147567,"uation. 5 Related Work NLG experiments for non-English languages are relatively rare and fully trainable approaches even rarer. Our work is, to our knowledge, the first application of neural NLG to a non-English language for data-to-text generation. Most works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation,"
W19-8670,Q14-1007,0,0.0158519,"se improvements for future work in Section 6. Our dataset and all experimental code are released on GitHub.2 2 Ananta – feminine noun, inflected (nom: Ananta, gen: Ananty, dat, loc: Anantˇe, acc: Anantu, inst: Anantou) BarBar – masculine inanimate noun, inflected (nom, acc: BarBar, gen, dat, loc: BarBaru, inst: BarBarem) Caf´e Savoy – neuter noun, not inflected Dataset M´ısto Our goal was to create a dataset comparable in size and domain to existing English data-to-text NLG datasets used in experiments with neural systems. Since there are few to none Czech speakers on crowdsourcing platforms (Pavlick et al., 2014; Duˇsek et al., 2014), we were not able to use them for data collection. Recruiting freelance translators seemed easier than training annotators; therefore, we turned to localizing and translating an existing dataset instead of creating a new one from scratch. We chose the restaurant dataset of Wen et al. (2015b) due to its manageable, yet non-trivial size and the familiarity of the domain (cf. Mairesse et al., 2010; Duˇsek et al., 2019). The original dataset contains 5,192 MR-sentence pairs, where MRs come in the form of dialogue acts (DAs). A DA consists of DA type (e.g., request, confirm,"
W19-8670,W18-3601,0,0.0461917,"eiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation, the recent work of Moussallem et al. (2018) is applied to Portuguese, but is largely rule-based. The works of Chen et al. (2010) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; they generate English and Korean sport commentary sentences using an inverted (non-neural) semantic parser. Our dataset is ca. 2.5 times larger and more complex, given the slot value inflection. Other works on neural"
W19-8670,W17-3521,0,0.0272338,"k NLG experiments for non-English languages are relatively rare and fully trainable approaches even rarer. Our work is, to our knowledge, the first application of neural NLG to a non-English language for data-to-text generation. Most works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation, the recent work of Moussa"
W19-8670,W17-4707,0,0.0271624,"btain the inflected word forms. In the lemma-tag mode, the set of possible output tokens is reduced compared to direct token generation, but the postprocessing step is much simpler than using a full syntactic surface realizer. Moreover, the generated morphological tags following slot placeholders can be used to limit the scope of possible surface forms during lexicalization (see Section 3.3). This approach is inspired by similar approaches in phrase-based MT (Bojar, 2007; Toutanova et al., 2008; Fraser, 2009) and was developed in parallel to recent similar experiments with two-step neural MT (Nadejde et al., 2017; Tamchyna et al., 2017). We compare the lemma-tag generation mode against the TGen default direct word-form generation mode in our experiments. 3.1 3.3 Basic Extensions We added two features fairly standard in seq2seqbased models but absent from TGen: • Bidirectional encoder (Bahdanau et al., 2015) – the input sequence is encoded in both directions and the resulting hidden states are joined. We added this for both the main seq2seq generator and the reranker. • Dropout (Hinton et al., 2012) – this zeroes out certain connections within the network with a given probability during the training pr"
W19-8670,D17-1238,1,0.880246,"Missing"
W19-8670,W00-0306,0,0.30245,"tilingual generation in principle, there has been little work to test these capabilities experimentally. This goes hand in hand with the scarcity of non-English training datasets for NLG – the only data-to-text NLG set known to us is a small sportscasting Korean dataset (Chen et al., 2010),1 which only contains a limited number of named entities, reducing the need for their inflection. Since most generators are only tested on English, they do not need to handle grammar complexities not present in English. A prime example is the delexicalization technique used by most current generators (e.g., Oh and Rudnicky, 2000; Mairesse et al., 2010; Wen et al., 2015a,b; Juraska 1 http://www.cs.utexas.edu/users/ml/ clamp/sportscasting/ et al., 2018): It is generally assumed that attribute (slot) values from the input meaning representation (MR) can be replaced by placeholders during generation and inserted into the output verbatim. Delexicalization or an analogous technique, such as a copy mechanism (Gu et al., 2016; Gehrmann et al., 2018), is required for most generation scenarios to allow generalization to unseen entity names: sets of entities are open (potentially infinite and subject to change) while training d"
W19-8670,J18-3002,0,0.0290617,"improvement over both baselines in all setups; the very low performance of the random baseline only documents that inflection indeed matters for slot values. The lexicalized input DAs did not bring improvement over the delexicalized set12 BLEU and NIST differences are statistically significant (p &lt; 0.01) according to bootstrap resampling (Koehn, 2004). ting – lexicalized setups seem to perform slightly worse in terms of both word-overlap metrics and SER. 4.4 Manual Error Analysis To obtain a deeper insight into the results and account for automatic metrics’ inaccuracy (Novikova et al., 2017; Reiter, 2018), we performed a detailed manual error analysis on a sample of 100 outputs produced by all systems except the ones with random baseline lexicalizers, which clearly perform poorly. This was a blind annotation of semantic and fluency errors; it is not a preference rating. We categorized multiple error types; the results are shown in Table 4. The analysis confirmed that lexicalized input DAs cause more semantic errors (both missed slots and repetition). On the other hand, the outputs were more fluent in this setting, which is not apparent with automatic metrics. Lemma-tag generation also improves"
W19-8670,K17-3009,0,0.0388752,"Missing"
W19-8670,P14-5003,0,0.0680803,"Missing"
W19-8670,W17-4704,0,0.0448607,"Missing"
W19-8670,P08-1059,0,0.0514066,"against the input DA is used as penalty. using the dictionary-based morphological generator of Strakov´a et al. (2014) to obtain the inflected word forms. In the lemma-tag mode, the set of possible output tokens is reduced compared to direct token generation, but the postprocessing step is much simpler than using a full syntactic surface realizer. Moreover, the generated morphological tags following slot placeholders can be used to limit the scope of possible surface forms during lexicalization (see Section 3.3). This approach is inspired by similar approaches in phrase-based MT (Bojar, 2007; Toutanova et al., 2008; Fraser, 2009) and was developed in parallel to recent similar experiments with two-step neural MT (Nadejde et al., 2017; Tamchyna et al., 2017). We compare the lemma-tag generation mode against the TGen default direct word-form generation mode in our experiments. 3.1 3.3 Basic Extensions We added two features fairly standard in seq2seqbased models but absent from TGen: • Bidirectional encoder (Bahdanau et al., 2015) – the input sequence is encoded in both directions and the resulting hidden states are joined. We added this for both the main seq2seq generator and the reranker. • Dropout (Hint"
W19-8670,W13-2125,0,0.0187639,"n increased amount of training data could improve the situation. 5 Related Work NLG experiments for non-English languages are relatively rare and fully trainable approaches even rarer. Our work is, to our knowledge, the first application of neural NLG to a non-English language for data-to-text generation. Most works concerned with multiple languages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, i"
W19-8670,C16-1100,0,0.0275407,"tion, the recent work of Moussallem et al. (2018) is applied to Portuguese, but is largely rule-based. The works of Chen et al. (2010) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; they generate English and Korean sport commentary sentences using an inverted (non-neural) semantic parser. Our dataset is ca. 2.5 times larger and more complex, given the slot value inflection. Other works on neural non-English NLG solve in fact different tasks from ours: Chinese poetry generation (Zhang and Lapata, 2014; Yi et al., 2017; Wang et al., 2016), non-task-oriented response generation in chatbots (Xing et al., 2016, 2017), or morphological inflection (e.g. Faruqui et al., 2016; Kann and Sch¨utze, 2016). 570 6 Conclusions and Future Work We presented the first dataset targeted at end-toend neural non-English NLG, containing Czech texts from the restaurant domain. We show that the task of data-to-text NLG here is harder as slot values require morphological inflection. We apply to our data the freely available, state-of-the-art TGen NLG system (Duˇsek and Jurˇc´ıcˇ ek, 2016) based on the seq2seq architecture, and we implement two extensi"
W19-8670,W15-4639,0,0.151286,"een little work to test these capabilities experimentally. This goes hand in hand with the scarcity of non-English training datasets for NLG – the only data-to-text NLG set known to us is a small sportscasting Korean dataset (Chen et al., 2010),1 which only contains a limited number of named entities, reducing the need for their inflection. Since most generators are only tested on English, they do not need to handle grammar complexities not present in English. A prime example is the delexicalization technique used by most current generators (e.g., Oh and Rudnicky, 2000; Mairesse et al., 2010; Wen et al., 2015a,b; Juraska 1 http://www.cs.utexas.edu/users/ml/ clamp/sportscasting/ et al., 2018): It is generally assumed that attribute (slot) values from the input meaning representation (MR) can be replaced by placeholders during generation and inserted into the output verbatim. Delexicalization or an analogous technique, such as a copy mechanism (Gu et al., 2016; Gehrmann et al., 2018), is required for most generation scenarios to allow generalization to unseen entity names: sets of entities are open (potentially infinite and subject to change) while training data is scarce. However, the verbatim inse"
W19-8670,D15-1199,0,0.0923021,"Missing"
W19-8670,D14-1074,0,0.0290398,"ing neural models. In data-to-text generation, the recent work of Moussallem et al. (2018) is applied to Portuguese, but is largely rule-based. The works of Chen et al. (2010) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; they generate English and Korean sport commentary sentences using an inverted (non-neural) semantic parser. Our dataset is ca. 2.5 times larger and more complex, given the slot value inflection. Other works on neural non-English NLG solve in fact different tasks from ours: Chinese poetry generation (Zhang and Lapata, 2014; Yi et al., 2017; Wang et al., 2016), non-task-oriented response generation in chatbots (Xing et al., 2016, 2017), or morphological inflection (e.g. Faruqui et al., 2016; Kann and Sch¨utze, 2016). 570 6 Conclusions and Future Work We presented the first dataset targeted at end-toend neural non-English NLG, containing Czech texts from the restaurant domain. We show that the task of data-to-text NLG here is harder as slot values require morphological inflection. We apply to our data the freely available, state-of-the-art TGen NLG system (Duˇsek and Jurˇc´ıcˇ ek, 2016) based on the seq2seq archi"
W19-8670,W08-0325,0,0.0156784,"anguages focus on surface realization. There have been a few approaches using handcrafted grammars (Bateman, 1997; Allman et al., 2012). The procedural SimpleNLG realizer (Gatt and Reiter, 2009) has also been ported into multiple languages (Bollmann, 2011; Vaudry and Lapalme, 2013; de Oliveira and Sripada, 2014; Mazzei et al., 2016; Ramos-Soto et al., 2017; Cascallar-Fuentes et al., 2018; Chen et al., 2018; de Jong and Theune, 2018). Further works using multilingual rule-based surface realization pipelines were developed in the context of maˇ chine translation (Aikawa et al., 2001; Zabokrtsk´ y et al., 2008; Duˇsek et al., 2015). Bohnet et al. (2010) created the first statistical multilingual realizer based on a pipeline of SVMs, the recent surface realization challenge (Mille et al., 2018) then features further fully trainable realizers tested on multiple languages, including neural models. In data-to-text generation, the recent work of Moussallem et al. (2018) is applied to Portuguese, but is largely rule-based. The works of Chen et al. (2010) and Kim and Mooney (2010) represent the only data-to-text end-to-end NLG system with multilingual experiments known to us; they generate English and Kor"
W19-8670,L18-1481,0,\N,Missing
