2020.lrec-1.367,W15-1005,0,0.0235558,"17). Inkpen et al. (2005) use orthographic features to extract cognate pairs for French-English, but do not take semantic similarity into account. Torres and Alu´ısio (2011) also rely on orthographic and phonetic features, to which they add a semantic feature extracted from a bilingual dictionary. They additionally release a lexicon of Spanish-Portuguese false friends and true cognates, obtained through manual annotation, that they use to evaluate their algorithms. Nakov et al. (2009) identify false friends pairs in Bulgarian and Russian by making use of sentence-aligned parallel corpora. In (Aminian et al., 2015) the authors propose using a model of identifying false friends from parellel corpora in order to improve English-Egyptian statistical machine translation. Cross-lingual semantic word similarity consists in identifying words that refer to similar semantic concepts and convey similar meanings across languages (Vulic and Moens, 2013). Some of the most popular approaches rely on probabilistic models (Vulic and Moens, 2014) and cross-lingual word embeddings (Søgaard et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognate words, usuall"
2020.lrec-1.367,W18-3903,0,0.155018,"Missing"
2020.lrec-1.367,dinu-ciobanu-2014-building,1,0.793463,"use the publicly available FastText multilingual word embeddings pre-aligned in a common vector space (Conneau et al., 2017).1 3. Step 3. Compute the semantic distance for the pair of cognates in the two languages, using a vectorial distance (we chose cosine distance) on their corresponding vectors in the shared embedding space. 3.2. Cognates Dataset In order to identify false friends (deceptive cognates), we start from a database of cognates, defined as words with common etymology and similar orthography. As our data source, we use the list of cognate sets in Romance languages published in (Ciobanu and Dinu, 2014). It contains 3,218 complete cognate sets in Romanian, French, Italian, Spanish and Portuguese, along with their Latin common ancestors, extracted from online etymology dictionaries. A subset of 305 of these sets also contains the corresponding cognate (in the broad sense, since these are mostly borrowings) in English. One complete example of a cognate set for the word “architect” in the Romance languages is illustrated in Table 1. 3.3. Deceptive Cognates and Falseness The multilingual embedding spaces as defined above can be used to measure the semantic distances between cognates in order to"
2020.lrec-1.367,R09-1054,0,0.435716,"ognates and false friends for every language pair is difficult to find and expensive to manually build. Moreover, dictionaries grow outdated and it is difficult to continuously update them to incorporate new words in the vocabulary. This is why applications have to rely on automatically identifying false friends. There have been a number of previous studies attempting to automatically extract pairs of true cognates and false friends from corpora or from dictionaries. Most methods are based either on orthographic and phonetic similarity (Inkpen et al., 2005), or require large parallel corpora (Nakov et al., 2009) or dictionaries (St Arnaud et al., 2017). Inkpen et al. (2005) use orthographic features to extract cognate pairs for French-English, but do not take semantic similarity into account. Torres and Alu´ısio (2011) also rely on orthographic and phonetic features, to which they add a semantic feature extracted from a bilingual dictionary. They additionally release a lexicon of Spanish-Portuguese false friends and true cognates, obtained through manual annotation, that they use to evaluate their algorithms. Nakov et al. (2009) identify false friends pairs in Bulgarian and Russian by making use of s"
2020.lrec-1.367,E17-1072,0,0.0254644,"l. (2009) identify false friends pairs in Bulgarian and Russian by making use of sentence-aligned parallel corpora. In (Aminian et al., 2015) the authors propose using a model of identifying false friends from parellel corpora in order to improve English-Egyptian statistical machine translation. Cross-lingual semantic word similarity consists in identifying words that refer to similar semantic concepts and convey similar meanings across languages (Vulic and Moens, 2013). Some of the most popular approaches rely on probabilistic models (Vulic and Moens, 2014) and cross-lingual word embeddings (Søgaard et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognate words, usually using simple methods on only one or two pairs of languages (Castro et al., 2018; Torres and Alu´ısio, 2011). Castro et al. (2018) detect false friends in Spanish-Portuguese, employing a classifier that learns from features extracted from multilingual embedding spaces. In (Mitkov et al., 2007) the authors use a method based on distributed representations of words in a continuous space built using comparable corpora, as well as a taxonomy-based approach, to identify false fri"
2020.lrec-1.367,D17-1267,0,0.12506,"uage pair is difficult to find and expensive to manually build. Moreover, dictionaries grow outdated and it is difficult to continuously update them to incorporate new words in the vocabulary. This is why applications have to rely on automatically identifying false friends. There have been a number of previous studies attempting to automatically extract pairs of true cognates and false friends from corpora or from dictionaries. Most methods are based either on orthographic and phonetic similarity (Inkpen et al., 2005), or require large parallel corpora (Nakov et al., 2009) or dictionaries (St Arnaud et al., 2017). Inkpen et al. (2005) use orthographic features to extract cognate pairs for French-English, but do not take semantic similarity into account. Torres and Alu´ısio (2011) also rely on orthographic and phonetic features, to which they add a semantic feature extracted from a bilingual dictionary. They additionally release a lexicon of Spanish-Portuguese false friends and true cognates, obtained through manual annotation, that they use to evaluate their algorithms. Nakov et al. (2009) identify false friends pairs in Bulgarian and Russian by making use of sentence-aligned parallel corpora. In (Ami"
2020.lrec-1.367,W11-4508,0,0.570647,"Missing"
2020.lrec-1.367,N13-1011,0,0.0307085,"on of Spanish-Portuguese false friends and true cognates, obtained through manual annotation, that they use to evaluate their algorithms. Nakov et al. (2009) identify false friends pairs in Bulgarian and Russian by making use of sentence-aligned parallel corpora. In (Aminian et al., 2015) the authors propose using a model of identifying false friends from parellel corpora in order to improve English-Egyptian statistical machine translation. Cross-lingual semantic word similarity consists in identifying words that refer to similar semantic concepts and convey similar meanings across languages (Vulic and Moens, 2013). Some of the most popular approaches rely on probabilistic models (Vulic and Moens, 2014) and cross-lingual word embeddings (Søgaard et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognate words, usually using simple methods on only one or two pairs of languages (Castro et al., 2018; Torres and Alu´ısio, 2011). Castro et al. (2018) detect false friends in Spanish-Portuguese, employing a classifier that learns from features extracted from multilingual embedding spaces. In (Mitkov et al., 2007) the authors use a method based on dis"
2020.lrec-1.367,D14-1040,0,0.0247414,"on, that they use to evaluate their algorithms. Nakov et al. (2009) identify false friends pairs in Bulgarian and Russian by making use of sentence-aligned parallel corpora. In (Aminian et al., 2015) the authors propose using a model of identifying false friends from parellel corpora in order to improve English-Egyptian statistical machine translation. Cross-lingual semantic word similarity consists in identifying words that refer to similar semantic concepts and convey similar meanings across languages (Vulic and Moens, 2013). Some of the most popular approaches rely on probabilistic models (Vulic and Moens, 2014) and cross-lingual word embeddings (Søgaard et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognate words, usually using simple methods on only one or two pairs of languages (Castro et al., 2018; Torres and Alu´ısio, 2011). Castro et al. (2018) detect false friends in Spanish-Portuguese, employing a classifier that learns from features extracted from multilingual embedding spaces. In (Mitkov et al., 2007) the authors use a method based on distributed representations of words in a continuous space built using comparable corpora, as"
2020.lrec-1.394,N09-3008,0,0.0925896,"Missing"
2020.lrec-1.394,N09-1008,0,0.0914148,"Missing"
2020.lrec-1.394,dinu-ciobanu-2014-building,1,0.914109,"ing this methodology, we propose two experiments that illustrate the applicability of word production in historical linguistics and provide insights into the evolution of the Romance languages. 3. 3.1. The algorithm proposed by (Needleman and Wunsch, 1970) outperformed, in preliminary experiments, the algorithm proposed by Bhargava and Kondrak (2009) Data and Experimental Setup We ran experiments on cognate sets in Romance languages (Romanian, Italian, French, Spanish, Portuguese) with Latin common ancestors. We trained the sequence labeling system on cognate sets from the dataset proposed by Ciobanu and Dinu (2014), having Romanian as the target language. We used 2,315 cognate sets for training and 772 for development. We tested the model on the dataset proposed by Reinheimer Ripeanu (2001), which contains 1,102 cognate sets. Out of these, only 372 cognate sets are complete (that is, report a cognate for each Romance language). For all the others, at least one cognate is missing, as follows: Romanian cognates are missing in 493 cognate sets, Italian cognates are missing in 188 cognate sets, French cognates are missing in 245 cognate sets, Spanish cognates are missing in 238 cognate sets and Portuguese c"
2020.lrec-1.394,C18-1136,1,0.906191,"s (artificially reconstructed by linguists and domain experts), starting from cognate sets in Romance languages. One of the benefits of this computational approach is that a lot of manual work is spared (provided, of course, that the system takes into account all the transformations through which popular Latin passed on its way to Romanian – or to other languages). In the recent years, a series of articles proposed computational approaches to identifying related words and reconstructing proto-words (Kondrak, 2000; List et al., 2017; Bouchard-Cˆot´e et al., 2009; Rama et al., 2018; List, 2019; Ciobanu and Dinu, 2018; Ciobanu and Dinu, 2019), as an alternative to classical comparative reconstruction (Fox, 1995; Campbell, 1998; Weiss, 2015). Another advantage of the proposed method is that it is possible to reach forms that, non-existent in Romanian, could exist or have existed, as ancient borrowings, in neighboring linguistic spaces, such as Bulgarian, Serbian or Hungarian. It is well-known the case of words – e.g., borcan (jar) – that the Bulgarians report as borrowed from Romanian, while the Romanians consider them borrowings from Bulgarian; of course they could be substratum elements – Thraco-Dacian, B"
2020.lrec-1.394,J19-4003,1,0.80569,"ucted by linguists and domain experts), starting from cognate sets in Romance languages. One of the benefits of this computational approach is that a lot of manual work is spared (provided, of course, that the system takes into account all the transformations through which popular Latin passed on its way to Romanian – or to other languages). In the recent years, a series of articles proposed computational approaches to identifying related words and reconstructing proto-words (Kondrak, 2000; List et al., 2017; Bouchard-Cˆot´e et al., 2009; Rama et al., 2018; List, 2019; Ciobanu and Dinu, 2018; Ciobanu and Dinu, 2019), as an alternative to classical comparative reconstruction (Fox, 1995; Campbell, 1998; Weiss, 2015). Another advantage of the proposed method is that it is possible to reach forms that, non-existent in Romanian, could exist or have existed, as ancient borrowings, in neighboring linguistic spaces, such as Bulgarian, Serbian or Hungarian. It is well-known the case of words – e.g., borcan (jar) – that the Bulgarians report as borrowed from Romanian, while the Romanians consider them borrowings from Bulgarian; of course they could be substratum elements – Thraco-Dacian, Balkan – but it is not exc"
2020.lrec-1.394,A00-2038,0,0.309689,"esearch problem, we use the same methodology to automatically reconstruct unattested Latin words (artificially reconstructed by linguists and domain experts), starting from cognate sets in Romance languages. One of the benefits of this computational approach is that a lot of manual work is spared (provided, of course, that the system takes into account all the transformations through which popular Latin passed on its way to Romanian – or to other languages). In the recent years, a series of articles proposed computational approaches to identifying related words and reconstructing proto-words (Kondrak, 2000; List et al., 2017; Bouchard-Cˆot´e et al., 2009; Rama et al., 2018; List, 2019; Ciobanu and Dinu, 2018; Ciobanu and Dinu, 2019), as an alternative to classical comparative reconstruction (Fox, 1995; Campbell, 1998; Weiss, 2015). Another advantage of the proposed method is that it is possible to reach forms that, non-existent in Romanian, could exist or have existed, as ancient borrowings, in neighboring linguistic spaces, such as Bulgarian, Serbian or Hungarian. It is well-known the case of words – e.g., borcan (jar) – that the Bulgarians report as borrowed from Romanian, while the Romanians"
2020.lrec-1.394,J19-1004,0,0.019643,"d Latin words (artificially reconstructed by linguists and domain experts), starting from cognate sets in Romance languages. One of the benefits of this computational approach is that a lot of manual work is spared (provided, of course, that the system takes into account all the transformations through which popular Latin passed on its way to Romanian – or to other languages). In the recent years, a series of articles proposed computational approaches to identifying related words and reconstructing proto-words (Kondrak, 2000; List et al., 2017; Bouchard-Cˆot´e et al., 2009; Rama et al., 2018; List, 2019; Ciobanu and Dinu, 2018; Ciobanu and Dinu, 2019), as an alternative to classical comparative reconstruction (Fox, 1995; Campbell, 1998; Weiss, 2015). Another advantage of the proposed method is that it is possible to reach forms that, non-existent in Romanian, could exist or have existed, as ancient borrowings, in neighboring linguistic spaces, such as Bulgarian, Serbian or Hungarian. It is well-known the case of words – e.g., borcan (jar) – that the Bulgarians report as borrowed from Romanian, while the Romanians consider them borrowings from Bulgarian; of course they could be substratum ele"
2020.lrec-1.394,N18-2063,0,0.041245,"Missing"
2021.findings-acl.167,W12-3202,0,0.0281776,"ain tasks (such as neural machine translation and RNNs), or finding pairs of topics that represent algorithms which have replaced one another along the history of computational linguistics (such as statistical machine translation and neural machine translation). 2 Previous work Multiple previous studies have looked at evolution of topics through time, analyzing texts of various genres, from news (Michel et al., 2011; Rule et al., 2015) to emails (Wang and McCallum, 2006) to scientific articles (Hall et al., 2008; Prabhakaran et al., 2016; Griffiths and Steyvers, 2004; Blei and Lafferty, 2006; Anderson et al., 2012). Popular choices for representing topics include topic models, to which some studies add variations specific to tracking trends over time, such as the continuous-time model proposed by Wang and McCallum (2006), the generative model proposed by Bolelli et al. (2009a,b), or the dynamic topic model (Blei and Lafferty, 2006). Hall et al. (2008) use an approach based on topic modeling, and focus on scientific texts in computational linguistics, analyzing papers published in ACL, EMNLP and COLING between 1978 and 2006. Gollapalli and Li (2015) use topic models and keyphrase extraction to compare to"
2021.findings-acl.167,S17-2091,0,0.0567331,"nce focused on neural networks. Considering that in recent years neural networks have almost dominated methods used in computational linguistics, we try to understand how topics approached in computational linguistics relate to those in the more focused field of neural networks, and whether and how they 1908 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1908–1922 August 1–6, 2021. ©2021 Association for Computational Linguistics migrate between these conferences. Our analysis of topic relationships within computational linguistics is inspired from Tan et al. (2017), in which the authors propose a way to classify topic relationships into four types, based on their co-occurrence in text and the degree of correlation between their popularity over time. In our paper, we extend this and take a deeper look at the relations existing between topics in scientific text. We propose interpretations of topic relationships in the context of a scientific domain, and report interesting findings on how these types of relationships manifest between scientific topics, discovering, for example, which algorithms in computational linguistics are compatible with certain tasks"
2021.findings-acl.167,bird-etal-2008-acl,0,0.0756554,"ow they influence each other. In one of the most extensive studies on the topic (Tan et al., 2017), the authors propose a systematic way of classifying relations between topics into four types of cooperating or competing topics, based on their patterns of co-occurrence and prevalence correlation: friendships, arms-race, head-to-head, and tryst. We build on this framework in our analysis of the field in the following sections. 3 Dataset Our study focuses on topics in computational linguistics and their evolution. For exploring this topic, we make use of articles published in the ACL Anthology (Bird et al., 2008; Radev et al., 2013) from its inception. We collect all papers published in four top conferences: ACL, EMNLP, COLING and NAACL over time, obtaining a total of 14,737 computational linguistics articles overall. We will further refer to the set of computational linguistics conferences we considered by using the general term ACL+. For the second stage of our study, we additionally use articles published in the NeurIPS conference, from which we collect all articles published since 1994, in total 6,520 articles. Table 1 shows the number of articles for each time period (across 5-year time spans) f"
2021.findings-acl.315,N19-1423,0,0.0081961,"ues in research in politeness/offensiveness and mental health. 1 Introduction The use of offensive language is pervasive in social media and it has been studied from different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile et al., 2019) but also to, for"
2021.findings-acl.315,J93-1003,0,0.111648,"written by individuals with depression. We compute the keyness score (Kilgarriff, 2009; Gabrielatos, 2018) of content words (removing stop words) from posts labeled as offensive written by users with self-reported diagnosis. The keyness is computed in order to show which words occur more often in the texts from depressed individuals showing signs of depression (target corpus) in comparison to the texts from users diagnosed with depression that do not show signs of depression (reference corpus). We calculate the frequencies of words from the two corpora and then the loglikelihood Ratio (G2 ) (Dunning, 1993) for each word. In Figure 2 we present the top 20 words, ordered by G2 from each corpus, in the two datasets. We show that, while users without signs of depression refer more to sexual and profane terms, posts by users showing signs of depression include more negative words such as bad, hate, sick, death. This result corroborates the findings described in the literature on cognitive errors or biases in depression (Beck and Haigh, 2014). It is well known that depressed individuals tend to view life events more negatively than their non-depressed peers (Everaert et al., 2017). Furthermore, depre"
2021.findings-acl.315,D18-1471,0,0.0309956,"Missing"
2021.findings-acl.315,2020.lrec-1.758,0,0.287431,"Missing"
2021.findings-acl.315,2020.trac-1.1,1,0.766797,"Missing"
2021.findings-acl.315,S19-2011,0,0.178135,"m different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile et al., 2019) but also to, for example, study the relation between profanity and hate speech (Malmasi and Zampieri, 2018) and the different functions and intentions of vulgarity in social med"
2021.findings-acl.315,W15-1204,0,0.0602495,"Missing"
2021.findings-acl.315,W18-4423,0,0.0305093,"Missing"
2021.findings-acl.315,W16-0314,1,0.766796,"anguage in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri, 2020). Several studies have applied machine learning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large number of teams (Coppersmith et al., 2015; Milne et al., 2016; Zirikly et al., 2019). There have been multiple studies on the impact of offensive and hateful speech on the individual’s psychological mental health and well-being (Bannink et al., 2014; Saha et al., 2019). The use of offensive language by individuals with mental health cond"
2021.findings-acl.315,W17-3012,0,0.0182153,"dings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile et al., 2019) but also to, for example, study the relation between profanity and hate speech (Malmasi and Zampieri, 2018) and the different functions and intentions of vulgarity in social media (Holgate et al., 2018). Most of the datasets used in the aforementioned studies contain data sampled from the general population and therefore very little light has been shed on the use of offensive language in online communication by specific groups such as individuals with mental health conditi"
2021.findings-acl.315,W16-0312,0,0.0194963,"rning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large number of teams (Coppersmith et al., 2015; Milne et al., 2016; Zirikly et al., 2019). There have been multiple studies on the impact of offensive and hateful speech on the individual’s psychological mental health and well-being (Bannink et al., 2014; Saha et al., 2019). The use of offensive language by individuals with mental health conditions, however, has not been substantially studies with the exception of Birnbaum et al. (2020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals"
2021.findings-acl.315,N12-1084,0,0.0121008,"analysis indicates that offensive language is more frequently used in the samples written by individuals with self-reported depression as well as individuals showing signs of depression. The results discussed here open new avenues in research in politeness/offensiveness and mental health. 1 Introduction The use of offensive language is pervasive in social media and it has been studied from different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusiv"
2021.findings-acl.315,D17-1322,0,0.0119463,"language target identification: individual (IND) vs. group (GRP) vs. other (OTH). This hierarchical taxonomy provides us with a flexibility as it represents multiple types of offensive content in a single annotation scheme (e.g. posts targeted at an individual are often cyberbullying and posts targeted at a group are often hate speech) making it a great fit for this kind of analysis. In our experiments, we consider level A (offensive vs. non-offensive) and level B (target vs. untargeted). Mental Health We run all our experiments on the Reddit Self-reported Depression Diagnosis (RSDD) dataset (Yates et al., 2017) and on the Early Risk Prediction on the Internet (eRisk) 2018 dataset (Losada and Crestani, 2016), two publicly available datasets containing posts from Reddit. The RSDD dataset consists of users annotated as having depression by their mention of diagnosis and control users, which are users who do not suffer from depression (there is not any mention of diagnosis in their posts). To prevent users labeled with depression to be easily identified by specific keywords, the authors removed posts containing depression terms (e.g. depression, depressive) or belonging to mental health related subreddi"
2021.findings-acl.315,N18-1202,0,0.0081378,"ts discussed here open new avenues in research in politeness/offensiveness and mental health. 1 Introduction The use of offensive language is pervasive in social media and it has been studied from different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile"
2021.findings-acl.315,2020.lrec-1.629,1,0.86189,"a (Basile et al., 2019; Kumar et al., 2020). More recently, with the goal of improving explainability, offensive language identification at the token-level has received more attention (Mathew et al., 2021; Ranasinghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies have addressed offensive language in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri, 2020). Several studies have applied machine learning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor"
2021.findings-acl.315,W15-1203,0,0.0388819,"Missing"
2021.findings-acl.315,2020.emnlp-main.470,1,0.864501,"ghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies have addressed offensive language in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri, 2020). Several studies have applied machine learning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large n"
2021.findings-acl.315,N19-1144,1,0.948269,"020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals with diagnosed depression or showing signs of depression. Data In our experiments, we use three publicly available English datasets with data collected from social media: one with offensive language annotation, and two datasets with posts from users with selfreported depression diagnosis. Offensive Language We use the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a) to train offensive language identification models. OLID contains a total of 14,100 manually annotated posts from Twitter and it was released as the official dataset of SemEval-2019 Task 6 (OffensEval) (Zampieri et al., 2019b). We chose OLID due to its general hierarchical annotation taxonomy with the following levels: Level A: Offensive language identification: offensive (OFF) vs. non-offensive (NOT) Level B: Categorization of offensive language: targeted insult or threats (TIN) vs. untargeted profanity (UNT). Level C: Offensive language target identification: individual (IND) vs. group (GR"
2021.findings-acl.315,S19-2010,1,0.943698,"020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals with diagnosed depression or showing signs of depression. Data In our experiments, we use three publicly available English datasets with data collected from social media: one with offensive language annotation, and two datasets with posts from users with selfreported depression diagnosis. Offensive Language We use the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a) to train offensive language identification models. OLID contains a total of 14,100 manually annotated posts from Twitter and it was released as the official dataset of SemEval-2019 Task 6 (OffensEval) (Zampieri et al., 2019b). We chose OLID due to its general hierarchical annotation taxonomy with the following levels: Level A: Offensive language identification: offensive (OFF) vs. non-offensive (NOT) Level B: Categorization of offensive language: targeted insult or threats (TIN) vs. untargeted profanity (UNT). Level C: Offensive language target identification: individual (IND) vs. group (GR"
2021.findings-acl.315,P18-1125,0,0.0122791,"individuals suffering from depression more likely to contain offensive language in existing datasets? RQ2: Are there differences in the nature of offensive language used by individuals with depression compared to control groups? 3600 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3600–3606 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related Work 3 Offensive language identification is a popular topic in NLP. Researchers have been working to improve the performance of systems trained to identify conversations that are likely to go awry (Zhang et al., 2018) and to detect the various types of offensive posts in social media (Basile et al., 2019; Kumar et al., 2020). More recently, with the goal of improving explainability, offensive language identification at the token-level has received more attention (Mathew et al., 2021; Ranasinghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies"
2021.findings-acl.315,W19-3003,0,0.0105253,"s to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large number of teams (Coppersmith et al., 2015; Milne et al., 2016; Zirikly et al., 2019). There have been multiple studies on the impact of offensive and hateful speech on the individual’s psychological mental health and well-being (Bannink et al., 2014; Saha et al., 2019). The use of offensive language by individuals with mental health conditions, however, has not been substantially studies with the exception of Birnbaum et al. (2020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals with diagnosed depressi"
2021.findings-acl.315,2021.naacl-demos.17,1,0.686327,": ACL-IJCNLP 2021, pages 3600–3606 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related Work 3 Offensive language identification is a popular topic in NLP. Researchers have been working to improve the performance of systems trained to identify conversations that are likely to go awry (Zhang et al., 2018) and to detect the various types of offensive posts in social media (Basile et al., 2019; Kumar et al., 2020). More recently, with the goal of improving explainability, offensive language identification at the token-level has received more attention (Mathew et al., 2021; Ranasinghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies have addressed offensive language in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri"
2021.findings-emnlp.243,N09-1008,0,0.041431,"The last decades have brought a series of compuited and borrowed Latin words. We introtational approaches to many topics of HL, such as duce a new dataset and investigate the case the problem of automatically identifying cognate of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), pairs (Kondrak, 2001; Mulloni and Pekar, 2006; where words directly inherited from Latin coCiobanu and Dinu, 2014; List et al., 2017; List, exist with words borrowed from Latin, and 2019; Heggarty, 2021), reconstructing protowords explore whether automatic discrimination be(Oakes, 2000; Bouchard-Côté et al., 2009; Ciobanu tween them is possible. Having entered the and Dinu, 2018; Meloni et al., 2019), predicting language at a later stage, borrowed words are etymology (Wu and Yarowsky, 2020), discriminatno longer subject to historical sound shift rules, ing between cognates and borrowings (Ciobanu hence they are presumably less eroded, which is why we expect them to have a different and Dinu, 2015; Tsvetkov et al., 2015) or identifyintrinsic structure distinguishable by compuing lexical borrowings in a language (Miller et al., tational means. We employ several machine 2020; Koo, 2015). learning models"
2021.findings-emnlp.243,W04-3250,0,0.0754823,"Missing"
2021.findings-emnlp.243,N01-1014,0,0.298846,"nea.mihai@gmx.com, ana.uban+acad@gmail.com Abstract (Meillet, 1925; Campbell, 1998), which required a lot of manual work and extensive knowledge, and In this paper, we address the problem of enabled significant advances in many languages. automatically discriminating between inherThe last decades have brought a series of compuited and borrowed Latin words. We introtational approaches to many topics of HL, such as duce a new dataset and investigate the case the problem of automatically identifying cognate of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), pairs (Kondrak, 2001; Mulloni and Pekar, 2006; where words directly inherited from Latin coCiobanu and Dinu, 2014; List et al., 2017; List, exist with words borrowed from Latin, and 2019; Heggarty, 2021), reconstructing protowords explore whether automatic discrimination be(Oakes, 2000; Bouchard-Côté et al., 2009; Ciobanu tween them is possible. Having entered the and Dinu, 2018; Meloni et al., 2019), predicting language at a later stage, borrowed words are etymology (Wu and Yarowsky, 2020), discriminatno longer subject to historical sound shift rules, ing between cognates and borrowings (Ciobanu hence they are p"
2021.findings-emnlp.243,P14-2017,1,0.743728,"which required a lot of manual work and extensive knowledge, and In this paper, we address the problem of enabled significant advances in many languages. automatically discriminating between inherThe last decades have brought a series of compuited and borrowed Latin words. We introtational approaches to many topics of HL, such as duce a new dataset and investigate the case the problem of automatically identifying cognate of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), pairs (Kondrak, 2001; Mulloni and Pekar, 2006; where words directly inherited from Latin coCiobanu and Dinu, 2014; List et al., 2017; List, exist with words borrowed from Latin, and 2019; Heggarty, 2021), reconstructing protowords explore whether automatic discrimination be(Oakes, 2000; Bouchard-Côté et al., 2009; Ciobanu tween them is possible. Having entered the and Dinu, 2018; Meloni et al., 2019), predicting language at a later stage, borrowed words are etymology (Wu and Yarowsky, 2020), discriminatno longer subject to historical sound shift rules, ing between cognates and borrowings (Ciobanu hence they are presumably less eroded, which is why we expect them to have a different and Dinu, 2015; Tsvetk"
2021.findings-emnlp.243,P15-2071,1,0.852384,"Missing"
2021.findings-emnlp.243,C18-1136,1,0.885915,"Missing"
2021.findings-emnlp.243,J19-4003,1,0.811651,"er the training set in order to optimize Romance idioms, only French has a deep orthog- hyper-parameters. 2847 2.2 Features For the first experiment, we use as input only the modern word forms, without knowledge of the Latin etymon. In this case, we use n-gram features (character n-grams with n ∈ {1, 2, 3}).1 We mark the beginning and the end of the words with a special character $. For the second experiment, we include the Latin etymon in the input data, along with the modern word forms in Romance languages. We experiment with n-gram features extracted around mismatches in the aligned pairs (Ciobanu and Dinu, 2019), using the Needleman and Wunsch (1970) alignment algorithm. In case of multiple alignments with equal scores, we choose the first one. We also use the edit distance (Levenshtein, 1965) between the words and their etymons as an additional feature. In Figure 1 we provide a workflow example for obtaining n-gram features for the Spanish word sacudir (meaning to shake off ) inherited from the Latin word succutere. With this approach, the system could capture transformations that occur much more often in inherited words than in borrowed words (such as letter t from Latin becoming d in Spanish) or t"
2021.findings-emnlp.243,J19-1004,0,0.055879,"Missing"
2021.findings-emnlp.243,mulloni-pekar-2006-automatic,0,0.129647,"om, ana.uban+acad@gmail.com Abstract (Meillet, 1925; Campbell, 1998), which required a lot of manual work and extensive knowledge, and In this paper, we address the problem of enabled significant advances in many languages. automatically discriminating between inherThe last decades have brought a series of compuited and borrowed Latin words. We introtational approaches to many topics of HL, such as duce a new dataset and investigate the case the problem of automatically identifying cognate of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), pairs (Kondrak, 2001; Mulloni and Pekar, 2006; where words directly inherited from Latin coCiobanu and Dinu, 2014; List et al., 2017; List, exist with words borrowed from Latin, and 2019; Heggarty, 2021), reconstructing protowords explore whether automatic discrimination be(Oakes, 2000; Bouchard-Côté et al., 2009; Ciobanu tween them is possible. Having entered the and Dinu, 2018; Meloni et al., 2019), predicting language at a later stage, borrowed words are etymology (Wu and Yarowsky, 2020), discriminatno longer subject to historical sound shift rules, ing between cognates and borrowings (Ciobanu hence they are presumably less eroded, wh"
2021.findings-emnlp.243,N15-1062,0,0.0602541,"Missing"
2021.findings-emnlp.243,2020.lrec-1.397,0,0.0313257,"roblem of automatically identifying cognate of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), pairs (Kondrak, 2001; Mulloni and Pekar, 2006; where words directly inherited from Latin coCiobanu and Dinu, 2014; List et al., 2017; List, exist with words borrowed from Latin, and 2019; Heggarty, 2021), reconstructing protowords explore whether automatic discrimination be(Oakes, 2000; Bouchard-Côté et al., 2009; Ciobanu tween them is possible. Having entered the and Dinu, 2018; Meloni et al., 2019), predicting language at a later stage, borrowed words are etymology (Wu and Yarowsky, 2020), discriminatno longer subject to historical sound shift rules, ing between cognates and borrowings (Ciobanu hence they are presumably less eroded, which is why we expect them to have a different and Dinu, 2015; Tsvetkov et al., 2015) or identifyintrinsic structure distinguishable by compuing lexical borrowings in a language (Miller et al., tational means. We employ several machine 2020; Koo, 2015). learning models to automatically discriminate Identifying lexical borrowings is considered one between inherited and borrowed words and of the most difficult and important problems in HL compare th"
2021.findings-emnlp.296,S19-2007,0,0.062471,"Missing"
2021.findings-emnlp.296,2021.findings-acl.315,1,0.716869,"to account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories depending on the targets, such as women, migrants, etc. (Basile et al., 2019). From a computational perspective, the problem is usually approached as a classification task at the post level, where a clas1 sifier is trained to predict whether a social media https://www.merriam-webster.com/ post contains offensive/toxic language. dictionary/pejorative 3493 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3493–3498 November 7–11, 2021. ©2021 Association for Computational Linguistics to ours incl"
2021.findings-emnlp.296,N19-1423,0,0.00666428,"ee classes. tweet, we want to be able to say if the word was The selected tweet-word pairs extracted for both used pejoratively or not in that tweet. of the data sets were then annotated with binary In order to prepare our data, for each tweet-word valued labels, denoting whether the word in the pair, the tweet was tokenized and the position of pair is used pejoratively (label 1) or not (label 0) in the occurrence of the word was found among the the tweet. We used the Wiktionary definitions in tokens. Then, we generated a contextual embedorder to label words as pejorative only when used ding (Devlin et al., 2019) for that occurrence, by with senses marked as ”derogatory” in Wiktionary. employing various BERT models, pre-trained on 3495 English texts, provided by the huggingface Python library (Wolf et al., 2019). The embedding obtained for the specified position is computed by summing the 768-dimensional hidden states generated for that position by each of the 12 layers of the BERT architecture. We note that, for out-of-vocabulary words, the BERT tokenizer provided by the huggingface library splits them into sub-words. In this case we chose to generate the embeddings for each of the sub-words of our w"
2021.findings-emnlp.296,W19-3513,0,0.0223092,"hrough semantic amelioration over the years - it used to be a slur and is losing its negative connotation (Brontsema, 2004)). Recognizing the complexity of the phenomenon, with its linguistic subtleties as well as the variability related to culture and context, are important to successfully recognize pejorative words and by extension offensive posts and hate speech. Pejorative language is still largely underexplored in computational linguistics. There are very few studies addressing or taking pejorative language into account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories dep"
2021.findings-emnlp.296,W18-4401,1,0.836499,"Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories depending on the targets, such as women, migrants, etc. (Basile et al., 2019). From a computational perspective, the problem is usually approached as a classification task at the post level, where a clas1 sifier is trained to predict whether a social media https://www.merriam-webster.com/ post contains offensive/toxic language. dictionary/pejorative 3493 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3493–3498 November 7–11, 2021. ©2021 Association for Computational Linguistics to ours include Palmer et al. (2017) who foc"
2021.findings-emnlp.296,N16-2013,0,0.0328905,"ian, we used another onlineavailable dictionary, dexonline3 , and selected all of the words that had a pejorative definition and where the definition was intended for the word not for an expression built around the word. 2 3 https://www.wiktionary.org/ https://dexonline.ro/ Figure 1: Distribution of parts of speech for the collected words for each language in WordNet. 3 Pejorative Tweet Dataset For building a data set of English texts containing words that are used pejoratively, we started by looking at three datasets of hate speech on Twitter: (Davidson et al., 2017), (Basile et al., 2019). (Waseem and Hovy, 2016), and selected the tweets that contain words from our pejorative lexicon (after normalizing words to their stems). For each data set, we extracted pairs of words and tweets where they occur. The dataset published by Davidson et al. (2017) contains tweets annotated with one of three classes (hateful, offensive and neither). For each label, the 3494 number of pejorative words found in the tweets is the following: 1, 114 out of 1, 430 hateful tweets, 8, 358 out of 19, 190 offensive tweets, and 2, 221 among the remaining 4, 163 tweets were found to contain pejorative words. The hate speech dataset"
2021.findings-emnlp.296,N18-1095,0,0.0203922,"rative meaning through semantic change (e.g. the word “queer” went through semantic amelioration over the years - it used to be a slur and is losing its negative connotation (Brontsema, 2004)). Recognizing the complexity of the phenomenon, with its linguistic subtleties as well as the variability related to culture and context, are important to successfully recognize pejorative words and by extension offensive posts and hate speech. Pejorative language is still largely underexplored in computational linguistics. There are very few studies addressing or taking pejorative language into account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression"
2021.findings-emnlp.296,2020.trac-1.1,1,0.813703,"Missing"
2021.findings-emnlp.296,2020.figlang-1.34,0,0.0852125,"Missing"
2021.findings-emnlp.296,N19-1144,1,0.931957,"d ‘slur’ refer to symbolic vehicles designed by convention to derogate targeted individuals or groups” (Anderson and Lepore, 2013). While pejorative language is often used in offensive speech (Castroviejo et al., 2020), they are not identical categories. There are offensive posts that do not use pejorative words (e.g. “Women belong in the kitchen”), and pejorative uses of words that are not harmful (“What a shitty chair”) because the offensive content is not targeted at a person or a group as described in the popular annotation taxonomy of the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019). Words can have a negative meaning in one context and not in others (such as the figurative meanings of “trash” or “pussy”); or be pejorative in one language or culture, and not in others (such as the Romanian “cioara” (literally, “crow”) - a slur for people of color). Slurs can also lose their pejorative meaning through semantic change (e.g. the word “queer” went through semantic amelioration over the years - it used to be a slur and is losing its negative connotation (Brontsema, 2004)). Recognizing the complexity of the phenomenon, with its linguistic subtleties as well as the variability r"
2021.findings-emnlp.296,2021.ccl-1.108,0,0.025455,"Missing"
2021.findings-emnlp.296,2020.emnlp-demos.2,0,0.0390588,"Missing"
2021.findings-emnlp.296,W17-3014,0,0.0397028,"Missing"
2021.findings-emnlp.296,2020.emnlp-main.470,1,0.761635,"-of-the-art contextual embeddings in order to automatically distinguish pejorative from non-pejorative uses of words, obtaining promising results. In the future, we would like to explore modelling the problem of pejorativity detection as a sequence labelling task. At the application level, integrating pejorativity detection into hate speech detection systems, for example, would be a promising area for future research. From a linguistic perspective, it would be interesting to analyze occurrence and pejorative value cross-lingually taking advantage of large pretrained cross-lingual models as in Ranasinghe and Zampieri (2020, 2021) for offensive language identification. We expect pejorative connotations to be difficult to translate and not transfer well across languages, which could also have practical implications. We would also like to extend our dataset of social media posts to cover more pejorative terms, as well as other languages. 4 The lexicon and the corpus are available at: https: //nlp.unibuc.ro/resources Luvell Anderson and Ernie Lepore. 2013. What did you call me? slurs as prohibited words setting things up. Analytic Philosophy, 54(3):350–63. Valerio Basile, Cristina Bosco, Elisabetta Fersini, Nozza D"
2021.findings-emnlp.296,2021.naacl-demos.17,1,0.783374,"Missing"
2021.findings-emnlp.296,W17-1101,0,0.0248501,"in computational linguistics. There are very few studies addressing or taking pejorative language into account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories depending on the targets, such as women, migrants, etc. (Basile et al., 2019). From a computational perspective, the problem is usually approached as a classification task at the post level, where a clas1 sifier is trained to predict whether a social media https://www.merriam-webster.com/ post contains offensive/toxic language. dictionary/pejorative 3493 Findings of the Association for Computational Linguistics: EMNLP 2021, pag"
2021.lchange-1.9,P16-1141,0,0.202402,"uages Technologies Research Center, University of Bucharest Faculty of Mathematics and Computer Science, University of Bucharest ♣ Faculty of Foreign Languages and Literatures, University of Bucharest ♦ PRHLT Research Center, Universitat Polit`ecnica de Val`encia ♠ ana.uban+acad@gmail.com, alina.cristea@fmi.unibuc.ro, anca.dinu@lls.unibuc.ro ldinu@fmi.unibuc.ro, simona.georgescu@lls.unibuc.ro, laurentiu.zoicas@lls.unibuc.ro Abstract governing semantic change, such as the law of parallel change and the law of differentiation (Xu and Kemp, 2015), the law of conformity and the law of innovation (Hamilton et al., 2016), or the law of prototypicality (Dubossarsky et al., 2015). More recently, Dubossarsky et al. (2017) revisited some of the semantic change laws proposed in previous literature, claiming that a more rigorous consideration of control conditions when modelling these laws leads to the conclusion that they are weaker or less reliable than reported. More extensive surveys of computational studies relating to semantic change have been conducted by Kutuzov et al. (2018) and Tahmasebi et al. (2018). Most previous computational studies on lexicalsemantic change have looked at the semantic change of the"
2021.lchange-1.9,W18-3903,0,0.0228495,"Missing"
2021.lchange-1.9,C18-1117,0,0.0159354,"h as the law of parallel change and the law of differentiation (Xu and Kemp, 2015), the law of conformity and the law of innovation (Hamilton et al., 2016), or the law of prototypicality (Dubossarsky et al., 2015). More recently, Dubossarsky et al. (2017) revisited some of the semantic change laws proposed in previous literature, claiming that a more rigorous consideration of control conditions when modelling these laws leads to the conclusion that they are weaker or less reliable than reported. More extensive surveys of computational studies relating to semantic change have been conducted by Kutuzov et al. (2018) and Tahmasebi et al. (2018). Most previous computational studies on lexicalsemantic change have looked at the semantic change of the words within one language, treating each language separately. However, words do not evolve only in their own language in isolation, but are rather inherited and borrowed between and across languages. In most cases, cognates have preserved similar meanings across languages, but there are also exceptions. These are called deceptive cognates or, more commonly, false friends. Here we use the definition of cognates that refers to words with similar appearance and som"
2021.lchange-1.9,dinu-ciobanu-2014-building,1,0.751724,"words, through a cultural, written channel, from the same language from which they originate: in this case, Latin does not play the role of ancestor language any more, but it represents a non-contemporary source of lexical enrichment (Reinheimer Ripeanu, 2004). To give an example, the same Latin word directus has been borrowed in Ro. direct “direct”, It. diretto, Fr. direct, Es. directo, Pt. directo, in a period that varies from the 13th century for French, to the 19th century for Romanian. 2 Cognates Dataset As our data source, we use the list of cognate sets in Romance languages proposed by Ciobanu and Dinu (2014). It contains 3,218 complete cognate sets in Romanian, French, Italian, Spanish and Portuguese, along with their Latin common ancestors, extracted from online etymology dictionaries. The dictionary-based approach for identifying cognates, described in detail in (Ciobanu and Dinu, 2013), comprises two steps: firstly, the etymological information is extracted from electronic dictionaries; secondly, the etymologies are matched: words with the same language of origin and the same etymon are considered to be cognates. This approach answers the question raised by Swadesh (1954): “Given a small colle"
2021.lchange-1.9,P15-2071,1,0.92789,"cognates, and show that the similarity distributions of English words show a specific bimodal pattern. We provide qualitative analyses and extensive linguistic interpretations for all our findings. We bring several contributions to the computational study of semantic change and cognate words. To the best of our knowledge, we are the first to approach the problem of dating cognates based on their semantic content. Analysing the formal properties of cognates (i.e. their word form) is a method that is well-known in computational historical linguistics to gauge how language families have evolved (Ciobanu and Dinu, 2015). Computational approaches to analyse changes in meanings of cognate sets in order to investigate language contact settings have not been considered in historical computational linguistics research. Additionally, we publish a novel electronically readable dataset with high quality annotations regarding the period a word entered the English language, for a selection of cognates in English and Romance languages. To our knowledge, it is the first of its kind, and we hope it can help further research into computer-assisted analysis of cognate words. a word enters a new language, features specific"
2021.lchange-1.9,J19-4003,1,0.864904,"Missing"
2021.lchange-1.9,R13-1019,1,0.828987,"Missing"
2021.lchange-1.9,R09-1054,0,0.0322689,"te words across different languages, viewing them as snapshots in time of each of the word’s different histories of evolution. A comprehensive list of cognates and false friends for every language pair is difficult to find or manually build – this is why applications often rely on automatically identifying them. Related to our task, there have been a number of previous studies attempting to automatically extract pairs of true cognates and false friends. Most methods are based either on orthographic and phonetic similarity or require large parallel corpora or dictionaries (Inkpen et al., 2005; Nakov et al., 2009; Chen and Skiena, 2016; St Arnaud et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognates, usually using simple methods on only one or two pairs of languages (Torres and Alu´ısio, 2011; Castro et al., 2018). 1.1 Preliminaries Cognates are words in sister languages (languages descending from a common ancestor) with a common proto-word. For example, the Spanish word paz and the French word paix are cognates, as they both descend from the Latin word pacem (N. pax, meaning peace) – see Figure 1. Lat. pacem (N. pax) n mo ety Uban et"
2021.lchange-1.9,D17-1118,0,0.0176868,"cience, University of Bucharest ♣ Faculty of Foreign Languages and Literatures, University of Bucharest ♦ PRHLT Research Center, Universitat Polit`ecnica de Val`encia ♠ ana.uban+acad@gmail.com, alina.cristea@fmi.unibuc.ro, anca.dinu@lls.unibuc.ro ldinu@fmi.unibuc.ro, simona.georgescu@lls.unibuc.ro, laurentiu.zoicas@lls.unibuc.ro Abstract governing semantic change, such as the law of parallel change and the law of differentiation (Xu and Kemp, 2015), the law of conformity and the law of innovation (Hamilton et al., 2016), or the law of prototypicality (Dubossarsky et al., 2015). More recently, Dubossarsky et al. (2017) revisited some of the semantic change laws proposed in previous literature, claiming that a more rigorous consideration of control conditions when modelling these laws leads to the conclusion that they are weaker or less reliable than reported. More extensive surveys of computational studies relating to semantic change have been conducted by Kutuzov et al. (2018) and Tahmasebi et al. (2018). Most previous computational studies on lexicalsemantic change have looked at the semantic change of the words within one language, treating each language separately. However, words do not evolve only in t"
2021.lchange-1.9,W19-4720,1,0.395741,"l., 2009; Chen and Skiena, 2016; St Arnaud et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognates, usually using simple methods on only one or two pairs of languages (Torres and Alu´ısio, 2011; Castro et al., 2018). 1.1 Preliminaries Cognates are words in sister languages (languages descending from a common ancestor) with a common proto-word. For example, the Spanish word paz and the French word paix are cognates, as they both descend from the Latin word pacem (N. pax, meaning peace) – see Figure 1. Lat. pacem (N. pax) n mo ety Uban et al. (2019a) propose a method for identifying and correcting false friends, as well as define a measure of their “falseness”, using crosslingual word embeddings and automatically extracted cognate sets (Uban et al., 2019b; Uban and Dinu, 2020; Uban et al., 2021). Expanding upon the direction proposed there, we create a new curated dataset of cognate sets in English and Romance languages. Additionally, we label the cognate sets according to their etymology and the period they entered the language, separating them into two distinct groups: old borrowings and recent borrowings. On this dataset, we investig"
2021.lchange-1.9,2020.lrec-1.367,1,0.7264,"s (Torres and Alu´ısio, 2011; Castro et al., 2018). 1.1 Preliminaries Cognates are words in sister languages (languages descending from a common ancestor) with a common proto-word. For example, the Spanish word paz and the French word paix are cognates, as they both descend from the Latin word pacem (N. pax, meaning peace) – see Figure 1. Lat. pacem (N. pax) n mo ety Uban et al. (2019a) propose a method for identifying and correcting false friends, as well as define a measure of their “falseness”, using crosslingual word embeddings and automatically extracted cognate sets (Uban et al., 2019b; Uban and Dinu, 2020; Uban et al., 2021). Expanding upon the direction proposed there, we create a new curated dataset of cognate sets in English and Romance languages. Additionally, we label the cognate sets according to their etymology and the period they entered the language, separating them into two distinct groups: old borrowings and recent borrowings. On this dataset, we investigate patterns related to the distribution of frequency, polysemy and cross-lingual semantic simEs. paz ety mo n cognates Fr. paix Figure 1: Example of cognates and their common ancestor: peace. An important distinction is to be made"
2021.lchange-1.9,D17-1267,0,0.017662,"hem as snapshots in time of each of the word’s different histories of evolution. A comprehensive list of cognates and false friends for every language pair is difficult to find or manually build – this is why applications often rely on automatically identifying them. Related to our task, there have been a number of previous studies attempting to automatically extract pairs of true cognates and false friends. Most methods are based either on orthographic and phonetic similarity or require large parallel corpora or dictionaries (Inkpen et al., 2005; Nakov et al., 2009; Chen and Skiena, 2016; St Arnaud et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognates, usually using simple methods on only one or two pairs of languages (Torres and Alu´ısio, 2011; Castro et al., 2018). 1.1 Preliminaries Cognates are words in sister languages (languages descending from a common ancestor) with a common proto-word. For example, the Spanish word paz and the French word paix are cognates, as they both descend from the Latin word pacem (N. pax, meaning peace) – see Figure 1. Lat. pacem (N. pax) n mo ety Uban et al. (2019a) propose a method for identifying and"
2021.lchange-1.9,W11-4508,0,0.0642449,"Missing"
2021.lchange-1.9,N15-1062,0,0.0701728,"Missing"
2021.wnut-1.53,W15-4316,0,0.0277072,"ystems (using other publicly available data or tools). The best model, from Jin (2015), generated candidates from the most similar canonical forms from the training data evaluated with the Jaccard Index. A random forest classifier was used to predict the suitable canonical form from all the candidates using features such as support and confidence, string similarity, and part of speech tags. The model was a constrained system, suggesting that the quality of the proposed model is more important than using additional data and tools. Other approaches were based on conditional random fields (CRF) (Akhtar et al., 2015; Supranovich and Patsepnia, 2015; Akhtar et al., 2015) and recurrent neural networks (RNN) (Min and Mott, 2015; Wagner and Foster, 2015) among others. Notably, MoNoise (van der Goot and van Noord, 2017) has long been considered state-of-the-art in lexical normalization. MoNoise is a normalization model using spelling correction and word embeddings for candidate generation and a feature-based random forest classifier for candidate ranking. It is a modular normalization system easily reusable and adaptable (van der Goot and van Noord, 2017). The model was at the beginning developed only for Eng"
2021.wnut-1.53,D19-1181,0,0.0220818,"Missing"
2021.wnut-1.53,2020.lrec-1.773,0,0.0672207,"Missing"
2021.wnut-1.53,N13-1037,0,0.0391939,"Our results show that while word-level, intrinsic, pernormalization could induce a loss of meaning. formance evaluation is behind other methods, However, it is well known that for most benchour model improves performance on extrinmark tasks, noisy/non-standard text has proven to sic, downstream tasks through normalization be a real problem to NLP models, such as BERT compared to models operating on raw, unpro(Kumar et al., 2020), trained on clean or curated cessed, social media text. data, but fine-tuned on tasks with noisy and inconsistent format. 1 Introduction To overcome this predicament, Eisenstein (2013) Social media is a pervasive part of our modern proposes two possible approaches: either domain lives and provides us with a rich source of infor- adaptation or normalization. While domain adapmation and insight into human behaviour. User- tation is not specific to natural language processing, generated content has been a valuable resource for text normalization and cleaning have always been a the research community, especially in the form of central part of any modern text processing pipeline. text, but it is notoriously noisy and non-standard. Text normalization is the process of adapting an"
2021.wnut-1.53,W15-4313,0,0.146383,"ial media text. We made the code publicly available on github.2 2 Related Work The W-NUT workshop hosted a shared task on lexical normalization of user-generated content from 1 http://noisy-text.github.io/2021/ multi-lexnorm.html 2 https://github.com/bucuram/ seq2seq-multilingual-normalization English tweets in its first edition (Baldwin et al., 2015a). The task received from the competing teams two categories of submissions, from constrained (using only the training data provided by the organizers) and unconstrained systems (using other publicly available data or tools). The best model, from Jin (2015), generated candidates from the most similar canonical forms from the training data evaluated with the Jaccard Index. A random forest classifier was used to predict the suitable canonical form from all the candidates using features such as support and confidence, string similarity, and part of speech tags. The model was a constrained system, suggesting that the quality of the proposed model is more important than using additional data and tools. Other approaches were based on conditional random fields (CRF) (Akhtar et al., 2015; Supranovich and Patsepnia, 2015; Akhtar et al., 2015) and recurre"
2021.wnut-1.53,2021.wnut-1.51,0,0.0620099,"Missing"
2021.wnut-1.53,2020.wnut-1.3,0,0.0952929,"Missing"
2021.wnut-1.53,2020.acl-main.703,0,0.0795725,"Missing"
2021.wnut-1.53,W18-3902,0,0.0245224,"ural language understanding and natural language generation, their ability to perform lexical normalization was also studied. By transforming the task into a token prediction 474 one, Muller et al. (2019) demonstrate that a BERT model can be used as a lexical normalization model in low resource settings. Current methods for lexical normalization attempt to normalize at the character-level (Pennell and Liu, 2011; Ljubeši´c et al., 2014), syllable-level (Xu et al., 2015), word-level (van der Goot, 2019b; Jin, 2015) or sentence-level (Muller et al., 2019; Lourentzou et al., 2019). Lusetti et al. (2018) propose an encoder-decoder approach for text normalization. We propose to make use of the latest transformer models that are capable of multilingual translation in a sequence to sequence manner, namely mBART (Tang et al., 2020). However, we do not perform translation between languages, but instead, we use mBART as a denoising autoencoder, i.e. translating from bad English to good English. This way, we take the whole sentence into consideration when correcting the text. Moreover, this method is more straightforward and can scale to multiple languages without increasing computational demands. L"
2021.wnut-1.53,W18-6107,0,0.0541602,"Missing"
2021.wnut-1.53,W19-3005,0,0.0961472,"Missing"
2021.wnut-1.53,W15-4317,0,0.0255197,"m the most similar canonical forms from the training data evaluated with the Jaccard Index. A random forest classifier was used to predict the suitable canonical form from all the candidates using features such as support and confidence, string similarity, and part of speech tags. The model was a constrained system, suggesting that the quality of the proposed model is more important than using additional data and tools. Other approaches were based on conditional random fields (CRF) (Akhtar et al., 2015; Supranovich and Patsepnia, 2015; Akhtar et al., 2015) and recurrent neural networks (RNN) (Min and Mott, 2015; Wagner and Foster, 2015) among others. Notably, MoNoise (van der Goot and van Noord, 2017) has long been considered state-of-the-art in lexical normalization. MoNoise is a normalization model using spelling correction and word embeddings for candidate generation and a feature-based random forest classifier for candidate ranking. It is a modular normalization system easily reusable and adaptable (van der Goot and van Noord, 2017). The model was at the beginning developed only for English text. Still, then it was later expanded for multi-lingual lexical normalization covering languages such as"
2021.wnut-1.53,D19-5539,0,0.0190766,"liz et al. (2019) compare the MT approaches for lexical normalization, focusing on statistical neural translation (SMT) and neural machine translation (NMT) and obtaining better results using the SMT method. Furthermore, the authors show that the SMT approach works better in a low-resource setting than an NMT approach which requires a lot of data. With the rise in popularity of pre-trained language models for natural language understanding and natural language generation, their ability to perform lexical normalization was also studied. By transforming the task into a token prediction 474 one, Muller et al. (2019) demonstrate that a BERT model can be used as a lexical normalization model in low resource settings. Current methods for lexical normalization attempt to normalize at the character-level (Pennell and Liu, 2011; Ljubeši´c et al., 2014), syllable-level (Xu et al., 2015), word-level (van der Goot, 2019b; Jin, 2015) or sentence-level (Muller et al., 2019; Lourentzou et al., 2019). Lusetti et al. (2018) propose an encoder-decoder approach for text normalization. We propose to make use of the latest transformer models that are capable of multilingual translation in a sequence to sequence manner, na"
2021.wnut-1.53,2021.naacl-main.50,0,0.0681537,"Missing"
2021.wnut-1.53,D19-5515,0,0.0530341,"Missing"
2021.wnut-1.53,I11-1109,0,0.0413578,"urthermore, the authors show that the SMT approach works better in a low-resource setting than an NMT approach which requires a lot of data. With the rise in popularity of pre-trained language models for natural language understanding and natural language generation, their ability to perform lexical normalization was also studied. By transforming the task into a token prediction 474 one, Muller et al. (2019) demonstrate that a BERT model can be used as a lexical normalization model in low resource settings. Current methods for lexical normalization attempt to normalize at the character-level (Pennell and Liu, 2011; Ljubeši´c et al., 2014), syllable-level (Xu et al., 2015), word-level (van der Goot, 2019b; Jin, 2015) or sentence-level (Muller et al., 2019; Lourentzou et al., 2019). Lusetti et al. (2018) propose an encoder-decoder approach for text normalization. We propose to make use of the latest transformer models that are capable of multilingual translation in a sequence to sequence manner, namely mBART (Tang et al., 2020). However, we do not perform translation between languages, but instead, we use mBART as a denoising autoencoder, i.e. translating from bad English to good English. This way, we ta"
2021.wnut-1.53,P19-3032,0,0.0497089,"Missing"
2021.wnut-1.53,2020.coling-main.583,0,0.0685818,"Missing"
2021.wnut-1.53,2021.wnut-1.54,0,0.0384329,"Missing"
2021.wnut-1.53,2021.wnut-1.52,0,0.0671031,"Missing"
2021.wnut-1.53,W15-4311,0,0.026254,"ublicly available data or tools). The best model, from Jin (2015), generated candidates from the most similar canonical forms from the training data evaluated with the Jaccard Index. A random forest classifier was used to predict the suitable canonical form from all the candidates using features such as support and confidence, string similarity, and part of speech tags. The model was a constrained system, suggesting that the quality of the proposed model is more important than using additional data and tools. Other approaches were based on conditional random fields (CRF) (Akhtar et al., 2015; Supranovich and Patsepnia, 2015; Akhtar et al., 2015) and recurrent neural networks (RNN) (Min and Mott, 2015; Wagner and Foster, 2015) among others. Notably, MoNoise (van der Goot and van Noord, 2017) has long been considered state-of-the-art in lexical normalization. MoNoise is a normalization model using spelling correction and word embeddings for candidate generation and a feature-based random forest classifier for candidate ranking. It is a modular normalization system easily reusable and adaptable (van der Goot and van Noord, 2017). The model was at the beginning developed only for English text. Still, then it was lat"
2021.wnut-1.53,2021.eacl-main.200,0,0.0666687,"Missing"
2021.wnut-1.53,2020.lrec-1.769,0,0.0377449,"Missing"
2021.wnut-1.53,2021.eacl-demos.22,0,0.0301965,"Missing"
2021.wnut-1.53,R19-1086,0,0.0539518,"Missing"
2021.wnut-1.53,W15-4314,0,0.0123324,"anonical forms from the training data evaluated with the Jaccard Index. A random forest classifier was used to predict the suitable canonical form from all the candidates using features such as support and confidence, string similarity, and part of speech tags. The model was a constrained system, suggesting that the quality of the proposed model is more important than using additional data and tools. Other approaches were based on conditional random fields (CRF) (Akhtar et al., 2015; Supranovich and Patsepnia, 2015; Akhtar et al., 2015) and recurrent neural networks (RNN) (Min and Mott, 2015; Wagner and Foster, 2015) among others. Notably, MoNoise (van der Goot and van Noord, 2017) has long been considered state-of-the-art in lexical normalization. MoNoise is a normalization model using spelling correction and word embeddings for candidate generation and a feature-based random forest classifier for candidate ranking. It is a modular normalization system easily reusable and adaptable (van der Goot and van Noord, 2017). The model was at the beginning developed only for English text. Still, then it was later expanded for multi-lingual lexical normalization covering languages such as Dutch, Spanish, Turkish,"
2021.wnut-1.53,S19-2010,0,0.0190589,"erika’da Valentina Day diye geçer. Artık ablamdan bir s¸ey yürütürüm ne yapayım :D Table 2: Noisy examples from each language and the corresponding canonical forms. both raw and canonical data to evaluate the perfor- SMILE, we report average macro F1 score across 5 mance improvement of using the normalized versus folds, and for OLID, we report macro F1 score on the original data. the test set. Moreover, we also evaluate the extrinsic performance of our model on two additional tasks: 4 Method sentiment analysis on the SMILE dataset (Wang et al., 2016) and hate speech detection on OLID dataset (Zampieri et al., 2019a). Both datasets contain data collected from Twitter, making them good candidates for evaluating the semantic processing of noisy text. SMILE dataset It consists of posts with mentions of several British museums gathered from Twitter to classify the emotions expressed by users towards art and cultural experiences from the museums. It contains 3,085 posts annotated with five emotions: Figure 1: Fine-tuning a mBART model for lexical noranger, disgust, happiness, surprise and sadness; malization on all available languages. We use the same model for all languages simultaneously. fear was not foun"
2021.wnut-1.53,2020.emnlp-demos.6,0,0.0749385,"Missing"
2021.wnut-1.53,P15-1089,0,0.0155494,"in a low-resource setting than an NMT approach which requires a lot of data. With the rise in popularity of pre-trained language models for natural language understanding and natural language generation, their ability to perform lexical normalization was also studied. By transforming the task into a token prediction 474 one, Muller et al. (2019) demonstrate that a BERT model can be used as a lexical normalization model in low resource settings. Current methods for lexical normalization attempt to normalize at the character-level (Pennell and Liu, 2011; Ljubeši´c et al., 2014), syllable-level (Xu et al., 2015), word-level (van der Goot, 2019b; Jin, 2015) or sentence-level (Muller et al., 2019; Lourentzou et al., 2019). Lusetti et al. (2018) propose an encoder-decoder approach for text normalization. We propose to make use of the latest transformer models that are capable of multilingual translation in a sequence to sequence manner, namely mBART (Tang et al., 2020). However, we do not perform translation between languages, but instead, we use mBART as a denoising autoencoder, i.e. translating from bad English to good English. This way, we take the whole sentence into consideration when correcting th"
2021.wnut-1.53,D17-1322,0,0.0391998,"Missing"
C12-3011,barbu-2008-romanian,0,0.0152408,"n words (http://www.webdex.ro/online/dictionar/rime and http://www.spunetiparerea.ro/dictionar-derime/cauta-rime.php). Both of them accept an input word and identify rhyming words. However, both systems determine rather the longest common suffix than rhyme. To our knowledge, they do not provide the other features that we discussed: the ability to automatically identify all words without rhymes from the dictionary and clustering words based on syllable number. 3 On the rhyme detection The dataset we used is a Romanian language resource containing 525528 words, including all inflectional forms (Barbu, 2008). For each word, the following pieces of information are provided: syllabification, type of syllabification (based on pronunciation or structure), position of the stressed vowels and inflectional form. Below is represented a word entry in our dataset. The “obs” field indicates that the word is syllabified based on its structure. 88 FIGURE 1 – word entry in database In order to determine the words that do not rhyme with other words, we focused on clustering them based on their rhymes. 1. 2. 3. For polysyllabic words, which had stressed vowels marked, we considered the substring beginning with t"
C12-3011,dinu-dinu-2006-data,1,0.828635,"inning with the rightmost stressed vowel. Hence, two words rhyme if their ﬁnal stressed vowels and all following phonemes are identical (Reddy and Knight 2011). In Romanian language the accent is variable and therefore cannot be determined in a deterministic manner. It can differentiate between words (“mozaic” – adjective, “mozaic” - noun) and grammatical forms (DOOM dictionary). Syllabification is a challenging and important task, considering that a rigorous research on syllable structure and characteristics cannot be achieved without a complete database of the syllables in a given language (Dinu and Dinu 2006, Dinu and Dinu 2009). Some attempts have been made for the automation of syllabification. Dinu and Dinu (2005) proposed a parallel manner of syllabification for Romanian words, using some parallel extensions of insertion grammars, and in (Dinu, 2003) is proposed a sequential manner of syllabification, based on a Marcus contextual grammar. Identifying words without rhyme is an important problem for poets and especially for automatic or assisted poetry translation. Reddy and Knight (2011) emphasize another related research area – historical linguistics, as rhymes of words in poetry can provide"
C12-3011,W09-4002,1,0.750766,"tmost stressed vowel. Hence, two words rhyme if their ﬁnal stressed vowels and all following phonemes are identical (Reddy and Knight 2011). In Romanian language the accent is variable and therefore cannot be determined in a deterministic manner. It can differentiate between words (“mozaic” – adjective, “mozaic” - noun) and grammatical forms (DOOM dictionary). Syllabification is a challenging and important task, considering that a rigorous research on syllable structure and characteristics cannot be achieved without a complete database of the syllables in a given language (Dinu and Dinu 2006, Dinu and Dinu 2009). Some attempts have been made for the automation of syllabification. Dinu and Dinu (2005) proposed a parallel manner of syllabification for Romanian words, using some parallel extensions of insertion grammars, and in (Dinu, 2003) is proposed a sequential manner of syllabification, based on a Marcus contextual grammar. Identifying words without rhyme is an important problem for poets and especially for automatic or assisted poetry translation. Reddy and Knight (2011) emphasize another related research area – historical linguistics, as rhymes of words in poetry can provide valuable information"
C12-3011,D10-1051,0,0.0609202,"Missing"
C12-3011,P11-2014,0,0.084206,"In this paper we focus on detecting Romanian words without rhymes, using knowledge about stressed vowels and syllabification. We also investigate quantitative aspects and the etymological origins of the Romanian words without rhymes. KEYWORDS : Romanian language, syllables, rhyme Proceedings of COLING 2012: Demonstration Papers, pages 87–94, COLING 2012, Mumbai, December 2012. 87 1 Introduction Rhyme represents the correspondence of final sounds of words, beginning with the rightmost stressed vowel. Hence, two words rhyme if their ﬁnal stressed vowels and all following phonemes are identical (Reddy and Knight 2011). In Romanian language the accent is variable and therefore cannot be determined in a deterministic manner. It can differentiate between words (“mozaic” – adjective, “mozaic” - noun) and grammatical forms (DOOM dictionary). Syllabification is a challenging and important task, considering that a rigorous research on syllable structure and characteristics cannot be achieved without a complete database of the syllables in a given language (Dinu and Dinu 2006, Dinu and Dinu 2009). Some attempts have been made for the automation of syllabification. Dinu and Dinu (2005) proposed a parallel manner of"
C12-3015,N03-1006,0,0.0371713,"becomes a burden not only for linguists to describe, but also for second language learners of Romanian to acquire. masculine neuter feminine singular b˘ aiat frumos boy.M beautiful.M creion frumos crayon.N beautiful.M fat˘ a frumoas˘ a girl.F beautiful.F plural b˘ aiet, i frumos, i boy.M beautiful.M creioane frumoase crayon.N beautiful.F fete frumoase girl.F beautiful.F Table 1: Gender vs. agreement in Romanian In our best knowledge, there are only two computational linguistics based approaches which attempted to discriminate Romanian nouns according to gender: Nastase and Popescu (2009) and (Cucerzan and Yarowsky, 2003). Our goal was, thus, to better -in comparison to Nastase and Popescu (2009)’s results- or successfully -in comparison to Cucerzan and Yarowsky (2003)’s experiment- distinguish these ""neuter"" nouns from feminines and masculines, by employing the minimum amount of information. We employed phonological information (coming from singular and plural noninflected nominative forms) as well as information coming from the feminine and masculine gender labels. In what follows we will present our tool for Romanian neuter nouns, which outperforms all previous attempts. 2 Our approach We will look at singu"
C12-3015,dinu-etal-2012-romanian,1,0.101412,"Missing"
C12-3015,D09-1142,0,0.145839,"or two. Gender assignment thus becomes a burden not only for linguists to describe, but also for second language learners of Romanian to acquire. masculine neuter feminine singular b˘ aiat frumos boy.M beautiful.M creion frumos crayon.N beautiful.M fat˘ a frumoas˘ a girl.F beautiful.F plural b˘ aiet, i frumos, i boy.M beautiful.M creioane frumoase crayon.N beautiful.F fete frumoase girl.F beautiful.F Table 1: Gender vs. agreement in Romanian In our best knowledge, there are only two computational linguistics based approaches which attempted to discriminate Romanian nouns according to gender: Nastase and Popescu (2009) and (Cucerzan and Yarowsky, 2003). Our goal was, thus, to better -in comparison to Nastase and Popescu (2009)’s results- or successfully -in comparison to Cucerzan and Yarowsky (2003)’s experiment- distinguish these ""neuter"" nouns from feminines and masculines, by employing the minimum amount of information. We employed phonological information (coming from singular and plural noninflected nominative forms) as well as information coming from the feminine and masculine gender labels. In what follows we will present our tool for Romanian neuter nouns, which outperforms all previous attempts. 2"
C12-3015,barbu-2008-romanian,0,\N,Missing
C12-3016,dinu-etal-2008-authorship,1,0.931224,"wo and so on. We call a tie the case when two or more frequencies are equal. In order to solve ties we 126 apply the standard Spearman’s rank correlation method. This means that if k objects claim the same rank (i.e. have the same frequency) and the first x ranks are already used by other objects then they will share the ranks and will receive the same rank number (the median from the k objects) which is in this case: (x + 1) + (x + 2) + · · · + (x + k) k =x+ (k + 1) 2 (1) Using rankings instead of raw frequencies has proved to offer better hypothesis regarding similarities between documents (Dinu et al., 2008). 2 Data visualisation In order to inspect our results we have opted for a hierarchical clustering method based on the extension provided by (Szekely and Rizzo, 2005) for Ward’s method. Their approach is concerned with increasing inner cluster homogeneity and inter-cluster heterogeneity. We have taken advantage of this joint-between within clustering method and we have adapted it for our l1 space to suit our purpose: el1 (A, B) = − n1 X n1 1 X n21 i=1 j=1 n1 n2 2 ( n1 + n2 n1 n2 k a i − a j k1 − n1 X n2 X i=1 j=1 n2 n2 X 1 X n22 i=1 j=1 k a i − b j k1 k b i − b j k1 ) (2) Where A = {a1 , · · ·"
C12-3016,C08-2023,1,0.485371,"ty (Milligan, 1979) (i.e. di j &lt; max{dik , d jk }) or space-dilatation (Everitt et al., 2009) (i.e dk,(i j) ≥ max{dki , dk j }) of the algorithm are inherited with this shift to el1 . If A and B would be singletons, the el1 distance is proportional to Manhattan distance and is recommended to be used with it and not with an Euclidean distance. Such an algorithm is best suited for our ranked data. 127 3 3.1 Measurements Manhattan Distance The most natural measure to be applied on an l1 space is Manhattan distance. When used on rankings it is also called Spearman’s foot-rule or Rank distance by (Dinu and Popescu, 2008). Given two tied ranked vectors X = {x 1 , · · · , x n } and Y = { y1 , · · · , yn } the equation for Manhattan distance is: n X D(X , Y ) = |x i − yi | (5) i=1 Notice that the distance remains the same if our tied ranked vectors are obtained by an ascending ordering relation (e.g. assign rank one to the most frequent function word, rank two to the second most frequent and so on) or by a descending ordering relation . This is simple to prove once we observe that for some frequencies { f1 &gt; f2 &gt; · · · &gt; f n }, that generated an ascending tied rank X &gt; = {x 1 , · · · , x n }, its descending tied"
C18-1136,W12-4410,0,0.440544,"l languages, in order to improve performance on Latin proto-word reconstruction. Our methodology is illustrated in Figure 1. 2.1 Conditional Random Fields From the alignment of related words in the training set, the system learns orthographic patterns for the changes in spelling between each modern language and the proto-language. We apply a sequence labeling method to infer the form of the Latin proto-words for the modern words in the test set. The method that we employ is based on sequence labeling, an approach that has been proven useful in generating transliterations (Ganesh et al., 2008; Ammar et al., 2012) and in cognate production (Ciobanu, 2016). In our case, the words in the modern languages are the sequences, and their characters are the tokens. Our purpose is to obtain, for each input word, a sequence of characters that compose its proto-word. To this end, we use conditional random fields (CRFs) (Lafferty et al., 2001). As features for the CRF system, we use character n-grams from the input words, extracted from a fixed window w around the current token. 2.2 Pairwise Sequence Alignment To align pairs of words, we experimented with two alignment methods that have been proven useful in natur"
C18-1136,I13-1112,0,0.359615,"Missing"
C18-1136,N09-3008,0,0.533337,"haracters are the tokens. Our purpose is to obtain, for each input word, a sequence of characters that compose its proto-word. To this end, we use conditional random fields (CRFs) (Lafferty et al., 2001). As features for the CRF system, we use character n-grams from the input words, extracted from a fixed window w around the current token. 2.2 Pairwise Sequence Alignment To align pairs of words, we experimented with two alignment methods that have been proven useful in natural language processing and computational biology: the alignment method based on profile hidden Markov models proposed by Bhargava and Kondrak (2009) and the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970). Since the alignment is only a pre-processing step for our task, we evaluate the alignment methods by the downstream results, i.e., the accuracy obtained by the CRF system when using one or the other alignment method. We observed that the results obtained with the two alignment methods were very similar, slightly better for the latter. Thus, we report the results obtained with the Needleman Wunsch alignment algorithm. The alignment algorithm uses, in our case, words as input sequences and a basic substitution mat"
C18-1136,D07-1093,0,0.675911,"Missing"
C18-1136,N09-1008,0,0.372591,"Missing"
C18-1136,P14-2017,1,0.900717,"on Latin ancestors. It is provided in two versions: orthographic and phonetic (IPA transcriptions). This dataset allows us to compare our results with a previous state-of-the-art method for proto-word reconstruction. Dataset 2. The dataset proposed by Reinheimer Ripeanu (2001). It consists of 1,102 cognate sets in five Romance languages (Spanish, Italian, Portuguese, French, Romanian) and their common Latin ancestors. Note that not all of these cognate sets are complete (that is, for some of them there are not cognates provided in all five modern languages). Dataset 3. The dataset proposed by Ciobanu and Dinu (2014b) and previously used for cognate detection (Ciobanu and Dinu, 2014a). It contains 3,218 complete cognate sets in five Romance languages (Spanish, Italian, Portuguese, French, Romanian) and their common Latin ancestors. Below we provide an example of a cognate set from this dataset: vehicul (Ro) 3.2 v´ehicule (Fr) veicolo (It) veh´ıculo (Es) ve´ıculo (Pt) vehiculum (Lat) Task Setup We split each dataset in subsets for train, dev and test (3:1:1 ratio). For inferring the form of the proto-words, we use the CRF implementation provided by the Mallet toolkit (McCallum, 2002). For parameter tuning"
C18-1136,dinu-ciobanu-2014-building,1,0.790257,"on Latin ancestors. It is provided in two versions: orthographic and phonetic (IPA transcriptions). This dataset allows us to compare our results with a previous state-of-the-art method for proto-word reconstruction. Dataset 2. The dataset proposed by Reinheimer Ripeanu (2001). It consists of 1,102 cognate sets in five Romance languages (Spanish, Italian, Portuguese, French, Romanian) and their common Latin ancestors. Note that not all of these cognate sets are complete (that is, for some of them there are not cognates provided in all five modern languages). Dataset 3. The dataset proposed by Ciobanu and Dinu (2014b) and previously used for cognate detection (Ciobanu and Dinu, 2014a). It contains 3,218 complete cognate sets in five Romance languages (Spanish, Italian, Portuguese, French, Romanian) and their common Latin ancestors. Below we provide an example of a cognate set from this dataset: vehicul (Ro) 3.2 v´ehicule (Fr) veicolo (It) veh´ıculo (Es) ve´ıculo (Pt) vehiculum (Lat) Task Setup We split each dataset in subsets for train, dev and test (3:1:1 ratio). For inferring the form of the proto-words, we use the CRF implementation provided by the Mallet toolkit (McCallum, 2002). For parameter tuning"
C18-1136,P15-2071,1,0.89428,"erent languages descending from a common proto-word). The main idea of the comparative method is to perform a property-based comparison on multiple sister languages in order to infer properties of their common ancestor. For a long period, the comparative reconstruction has been a time-consuming manual process that required a large amount of intensive work. The first step of the process consists in identifying cognate pairs. Identifying cognates and borrowings by means of computational approaches has attracted considerable attention in recent years (Hall and Klein, 2010; Tsvetkov et al., 2015; Ciobanu and Dinu, 2015). However, few studies went beyond this step, and beyond the comparative method, to automate the process of proto-language reconstruction (Oakes, 2000; Bouchard-Cˆot´e et al., 2013; Atkinson, 2013). Reconstructing proto-words is a challenging task. While the main hypothesis in this research problem is that there are regularities and patterns in how words evolved from the ancestor language to its modern daughter languages, there are also words which diverged significantly from their ancestor. Take, for example, the Latin word umbilicu(lu)s. It evolved into buric (Romanian), nombril (French), an"
C18-1136,P98-1043,0,0.700353,"Missing"
C18-1136,I08-6006,0,0.0663369,"mation provided by all languages, in order to improve performance on Latin proto-word reconstruction. Our methodology is illustrated in Figure 1. 2.1 Conditional Random Fields From the alignment of related words in the training set, the system learns orthographic patterns for the changes in spelling between each modern language and the proto-language. We apply a sequence labeling method to infer the form of the Latin proto-words for the modern words in the test set. The method that we employ is based on sequence labeling, an approach that has been proven useful in generating transliterations (Ganesh et al., 2008; Ammar et al., 2012) and in cognate production (Ciobanu, 2016). In our case, the words in the modern languages are the sequences, and their characters are the tokens. Our purpose is to obtain, for each input word, a sequence of characters that compose its proto-word. To this end, we use conditional random fields (CRFs) (Lafferty et al., 2001). As features for the CRF system, we use character n-grams from the input words, extracted from a fixed window w around the current token. 2.2 Pairwise Sequence Alignment To align pairs of words, we experimented with two alignment methods that have been p"
C18-1136,P10-1105,0,0.273806,", 1989) and relies on cognates (words in different languages descending from a common proto-word). The main idea of the comparative method is to perform a property-based comparison on multiple sister languages in order to infer properties of their common ancestor. For a long period, the comparative reconstruction has been a time-consuming manual process that required a large amount of intensive work. The first step of the process consists in identifying cognate pairs. Identifying cognates and borrowings by means of computational approaches has attracted considerable attention in recent years (Hall and Klein, 2010; Tsvetkov et al., 2015; Ciobanu and Dinu, 2015). However, few studies went beyond this step, and beyond the comparative method, to automate the process of proto-language reconstruction (Oakes, 2000; Bouchard-Cˆot´e et al., 2013; Atkinson, 2013). Reconstructing proto-words is a challenging task. While the main hypothesis in this research problem is that there are regularities and patterns in how words evolved from the ancestor language to its modern daughter languages, there are also words which diverged significantly from their ancestor. Take, for example, the Latin word umbilicu(lu)s. It evo"
C18-1136,D15-1166,0,0.02737,"Missing"
C18-1136,N15-1062,0,0.23512,"cognates (words in different languages descending from a common proto-word). The main idea of the comparative method is to perform a property-based comparison on multiple sister languages in order to infer properties of their common ancestor. For a long period, the comparative reconstruction has been a time-consuming manual process that required a large amount of intensive work. The first step of the process consists in identifying cognate pairs. Identifying cognates and borrowings by means of computational approaches has attracted considerable attention in recent years (Hall and Klein, 2010; Tsvetkov et al., 2015; Ciobanu and Dinu, 2015). However, few studies went beyond this step, and beyond the comparative method, to automate the process of proto-language reconstruction (Oakes, 2000; Bouchard-Cˆot´e et al., 2013; Atkinson, 2013). Reconstructing proto-words is a challenging task. While the main hypothesis in this research problem is that there are regularities and patterns in how words evolved from the ancestor language to its modern daughter languages, there are also words which diverged significantly from their ancestor. Take, for example, the Latin word umbilicu(lu)s. It evolved into buric (Romani"
C18-2015,W12-4410,0,0.0159121,"e L1 , our system predicts the form v of the word u in a target language L2 , in the hypothesis that the word v will be derived in L2 from the word u. From the alignment of the related words in the training set we learn orthographic cues and patterns for the changes in spelling. We use the alignment as input for a sequence labeling system (assigning a sequence of labels to a sequence of tokens), based on an approach that has been proven useful for cognate production (Ciobanu, 2016; Dinu and Ciobanu, 2017), proto-word reconstruction (Ciobanu and Dinu, 2018) and for generating transliterations (Ammar et al., 2012). We conduct our experiments on Romanian as a target language, and experiment with 10 source languages from which words entered in Romanian. 2.1 Word Alignment To align pairs of words we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), with the orthographic form of the words as input sequences and a very simple substitution matrix, which gives equal scores to all substitutions, disregarding diacritics (e.g., we ensure that e and e´ are matched). For example, for the Romanian word descifrabil (meaning decipherable), borrowed from the French word d´echiffrable"
C18-2015,I13-1112,0,0.0245412,"anguage. The proposed methods (Eastlack, 1977; Hartman, 1981) required a list of known sound correspondences as input, collected from dictionaries or published studies. Modern approaches impose the use and development of quantitative and computational methods in this field (McMahon et al., 2005; Heggarty, 2012; Atkinson, 2013), or even cross-disciplinary methods (such as those borrowed from biology). Nowadays, given the development of the machine learning techniques, computers are able to learn sound or character correspondences automatically from pairs of known related words. Beinborn et al. (2013) proposed such a method for cognate production, using the orthographic form of the words, and applying a machine translation method based on characters instead of words. The orthographic approach relies on the idea that sound changes leave traces in the orthography and alphabetic character correspondences represent, to a fairly large extent, sound correspondences (Delmestri and Cristianini, 2010). Aligning the related words to extract orthographic changes from one language to another has proven very effective, when applied to both the orthographic (Gomes and Lopes, 2011) and the phonetic (Kond"
C18-2015,N09-1008,0,0.0717696,"Missing"
C18-2015,D14-1112,1,0.821543,"to the previous label. We account for affixes separately: we add two extra characters B and E, marking the beginning and the end of an input word. In order to reduce the number of labels, for input tokens that are identical to their labels we replace the label with *. For the previous example, the labels are as follows: B ↓ * d ↓ * ´ e ↓ es c ↓ * h ↓ - i ↓ * f ↓ * f ↓ - r ↓ * a ↓ * b ↓ bi l ↓ * e ↓ - E ↓ * As features for the sequence labeling system, we use character n-grams in a window of size w around the current token. 2.3 Experiments We run experiments on a dataset of word-etymon pairs (Ciobanu and Dinu, 2014), from which we extract Romanian words having etymons in 10 languages. The dataset was built from an aggregation of machine-readable dictionaries1 that contains information about the etymology of the words.The dataset is structured as a list of word pairs having the form: w1 (L1 ) → w2 (L2 ), where word w2 entered L2 from the L1 word w1 . Example: victoria (Latin) → victorie (Romanian). We use subsets of 800 word pairs for each language, to have an equal size that allows a comparison between source languages. The results are reported in Table 1. In Table 2 we show examples of our system’s outp"
C18-2015,C18-1136,1,0.814303,"ianini, 2010). Given the form of a word u in a source language L1 , our system predicts the form v of the word u in a target language L2 , in the hypothesis that the word v will be derived in L2 from the word u. From the alignment of the related words in the training set we learn orthographic cues and patterns for the changes in spelling. We use the alignment as input for a sequence labeling system (assigning a sequence of labels to a sequence of tokens), based on an approach that has been proven useful for cognate production (Ciobanu, 2016; Dinu and Ciobanu, 2017), proto-word reconstruction (Ciobanu and Dinu, 2018) and for generating transliterations (Ammar et al., 2012). We conduct our experiments on Romanian as a target language, and experiment with 10 source languages from which words entered in Romanian. 2.1 Word Alignment To align pairs of words we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), with the orthographic form of the words as input sequences and a very simple substitution matrix, which gives equal scores to all substitutions, disregarding diacritics (e.g., we ensure that e and e´ are matched). For example, for the Romanian word descifrabil (meaning d"
C18-2015,P10-1105,0,0.024176,"of cognate production based on the orthography of the words, besides the character-based machine translation approach mentioned above, another contribution belongs to Mulloni (2007), who introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language. Another probabilistic approach to word form production is based on building generative models from the phylogenetic tree of languages, modeling the evolution of the languages and capturing various aspects of language change (Bouchard-Cˆot´e et al., 2009; Hall and Klein, 2010). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 68 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 68–72 Santa Fe, New Mexico, USA, August 20-26, 2018. 2 Simulating Language Evolution We propose a method for word form production based on the orthography of the words, building on the idea that orthographic changes represent sound correspondences to a fairly large extent (Delmestri and Cristianini, 2010). Given the form of a word u in a"
C18-2015,A00-2038,0,0.0611973,"013) proposed such a method for cognate production, using the orthographic form of the words, and applying a machine translation method based on characters instead of words. The orthographic approach relies on the idea that sound changes leave traces in the orthography and alphabetic character correspondences represent, to a fairly large extent, sound correspondences (Delmestri and Cristianini, 2010). Aligning the related words to extract orthographic changes from one language to another has proven very effective, when applied to both the orthographic (Gomes and Lopes, 2011) and the phonetic (Kondrak, 2000) form of the words . For the task of cognate production based on the orthography of the words, besides the character-based machine translation approach mentioned above, another contribution belongs to Mulloni (2007), who introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language. Another probabilistic approach to word form production is based on building generative models from the phylogenetic tree of languages, modeling the evolution of the languages and capturing various aspects of language chang"
C18-2015,P07-3005,0,0.0456785,"dea that sound changes leave traces in the orthography and alphabetic character correspondences represent, to a fairly large extent, sound correspondences (Delmestri and Cristianini, 2010). Aligning the related words to extract orthographic changes from one language to another has proven very effective, when applied to both the orthographic (Gomes and Lopes, 2011) and the phonetic (Kondrak, 2000) form of the words . For the task of cognate production based on the orthography of the words, besides the character-based machine translation approach mentioned above, another contribution belongs to Mulloni (2007), who introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language. Another probabilistic approach to word form production is based on building generative models from the phylogenetic tree of languages, modeling the evolution of the languages and capturing various aspects of language change (Bouchard-Cˆot´e et al., 2009; Hall and Klein, 2010). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 68 Proceedi"
D14-1112,dinu-ciobanu-2014-building,1,0.77845,"4. 2 C Lingua etymology etymology Nlingua cognates cognates Methodology and Algorithm λ In this section we introduce a technique for determining the orthographic similarity of languages. In order to obtain accurate results, we investigate both etymons and cognates. First, we automatically identify etymons and cognates, then we measure the distances between related words, and finally we compute the overall degrees of similarity between pairs of languages. We also applied this method for investigating the mutual intelligibility of the Romance languages, and preliminary results are presented in (Ciobanu and Dinu, 2014b). 2.1 Let C  tw1 , w2 , ..., wNwords u be a corpus in L1 and let L2 be a related language. We assume, without any loss of generality, that the elements of C are ordered such that CL  tw1 , w2 , ..., wNlingua u is the subset of C containing all the words that have an etymon or a cognate pair in L2 . We use the following notations: Nwords is the number of token words in C, Nlingua is the number of token words in CL , λ is the empty string and xi is the etymon or cognate pair of wi in L2 . Given a string distance ∆, we define the distance between L1 and L2 (non-metric distance), with frequenc"
D14-1112,dinu-ciobanu-2014-romance,1,0.887008,"4. 2 C Lingua etymology etymology Nlingua cognates cognates Methodology and Algorithm λ In this section we introduce a technique for determining the orthographic similarity of languages. In order to obtain accurate results, we investigate both etymons and cognates. First, we automatically identify etymons and cognates, then we measure the distances between related words, and finally we compute the overall degrees of similarity between pairs of languages. We also applied this method for investigating the mutual intelligibility of the Romance languages, and preliminary results are presented in (Ciobanu and Dinu, 2014b). 2.1 Let C  tw1 , w2 , ..., wNwords u be a corpus in L1 and let L2 be a related language. We assume, without any loss of generality, that the elements of C are ordered such that CL  tw1 , w2 , ..., wNlingua u is the subset of C containing all the words that have an etymon or a cognate pair in L2 . We use the following notations: Nwords is the number of token words in C, Nlingua is the number of token words in CL , λ is the empty string and xi is the etymon or cognate pair of wi in L2 . Given a string distance ∆, we define the distance between L1 and L2 (non-metric distance), with frequenc"
D14-1112,2005.mtsummit-papers.11,0,0.00894099,"erences are small and do not influence the ranking. 3.2 Europarl Experiments We continue our investigation regarding the similarity of natural languages with two additional experiments. First, we want to see if degrees of similarity between Romanian and other languages in the present period are consistent across two different corpora. In the second experiment we are interested to see if there are differences between the overall degrees of similarity obtained for the entire corpus (the bag-of-words model) and those obtained in various experiments at sentence level. Our main corpus is Europarl (Koehn, 2005). More specifically, we use the portions larger than 2KB collected between 2007 and 2011 from the Romanian subcorpus of Europarl. The corpus is tokenized and sentence-aligned in 21 languages. For preprocessing this corpus, we discard all the transcribers’ descriptions of the parliamentary sessions (such as “The President interrupted the speaker” or “The session was suspended at 19:30 and resumed at 21:00”). Exp. #1. In a first step, we apply the methodology described in Section 2 on the entire Europarl corpus for Romanian, using a bag-of-words model for the entire corpus, in which we account f"
D14-1112,N01-1014,0,0.054378,"pattern is shown below. specify the etymology of the Romanian word capitol (chapter), which has double etymology: Latin (with the etymon capitulum) and Italian (with the etymon capitolo). &lt;b&gt; CAP´ ITOL &lt;/b&gt; &lt;abbr class=&quot;abbrev&quot; title=&quot;limba italiana&quot;&gt; it. &lt;/abbr&gt; &lt;b&gt; capitolo &lt;/b&gt; &lt;abbr class=&quot;abbrev&quot; title=&quot;limba latina&quot;&gt; lat. &lt;/abbr&gt; &lt;b&gt; capitulum &lt;/b&gt; Step 2. Cognate Identification. Cognates are words in different languages having the same etymology and a common ancestor. The methods for cognate detection proposed so far are mostly based on orthographic/phonetic and semantic similarities (Kondrak, 2001; Frunza et al., 2005), but the term “cognates” is often used with a somewhat different meaning, denoting words with high orthographic/phonetic and cross-lingual meaning similarity, the condition of common etymology being left aside. We focus on etymology and we introduce an automatic strategy for detecting pairs of INPUT TEXT TEXT PROCESSING Stop words removal Lemmatization GENETIC RELATIONSHIPS IDENTIFICATION Etymology detection Cognate identification LANGUAGE SIMILARITY COMPUTATION Words distances measuring Orthographic similarity computation SIMILARITY HIERARCHY As an example, we provide b"
D14-1112,P11-1132,0,0.0234511,"and argue for the usage of computational phylogenetic methods in the question of Indo-European age and origins. Using modified versions of Swadesh’s lists1 , Dyen et al. (1992) investigate the classification of Indo-European languages by applying a lexicostatistical method. The similarity of languages is interesting not only for historical and comparative linguistics, but for machine translation and language acquisition as well. Scannell (2006) and Hajiˇc et al. (2000) argue for the possibility of obtaining a better translation quality using simple methods for very closely related languages. Koppel and Ordan (2011) study the impact of the distance between languages on the translation product and conclude that it is directly correlated with the ability to distinguish translations from a given source language from non-translated text. Some genetically related languages are so similar to each other, that According to Campbell (2003), the methods based on comparisons of cognate lists and sound corre1 http://www.wordgumbo.com/ie/cmp/iedata.txt 1047 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1047–1058, c October 25-29, 2014, Doha, Qatar. 2014 Associat"
D14-1112,W95-0115,0,0.0275218,"c distance or similarity between related words. Their performance has been investigated and compared (Frunza et al., 2005; Rama and Borin, 2014), but a clear conclusion cannot be drawn with respect to which method is the most appropriate for a given task. We employ three metrics to determine the orthographic similarity between related words. In Subsection 3.1.2 we investigate to what extent the similarity scores computed with each of these metrics differ, and whether the differences are statistically significant. We use the following metrics: 1051 • LCSR: The longest common subsequence ratio (Melamed, 1995) is the longest common subsequence of two strings u and v divided by the length of the longer string. We subtract this value from 1, in order to obtain the distance between two words. • EDIT: The edit distance (Levenshtein, 1965) counts the minimum number of operations (insertion, deletion and substitution) required to transform one string into another. We use a normalized version of the edit distance, dividing it by the length of the longer string. • RD: The rank distance (Dinu and Dinu, 2005) is used to measure the similarity between two ranked lists. A ranking of a set of n objects can be r"
D14-1112,A00-1002,0,\N,Missing
D18-1067,D14-1181,0,0.0131991,"Missing"
D18-1067,P03-1054,0,0.0983667,"g, pages 652–658 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figure 1: Sentiment prediction on tweets with pessimist (left) and optimist (right) connotations. The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a corpus for sentiment analysis that capture complex linguistic patterns. This dataset2 is based on a dataset originally introduced by Pang and Lee (2005) and consists of 10,662 sentences from movie reviews downloaded from rottentomatoes.com. From these sentences, 215,154 phrases were extracted using the Stanford Parser (Klein and Manning, 2003) and labeled using Amazon Mechanical Turk such that each phrase was annotated by 3 human judges. The Twitter Sentiment Analysis (TSA) dataset,3 available online for download, contains 1,578,627 tweets that are classified as 1 for positive sentiment and 0 for negative sentiment. predicting tweets with pessimistic and optimistic connotations (left and right side of the figure, respectively). We answer the above question by investigating a spectrum of sentiment analysis tools and datasets for optimism/pessimism prediction. Third, we perform a linguistic analysis, first of its kind, and study the"
D18-1067,W17-7532,0,0.0613356,"Missing"
D18-1067,C14-1079,1,0.686056,"learning approach help to discover these characteristics better than traditional machine learning classifiers used in prior work?” To our knowledge, we take the first step towards exploring the performance of deep learning models for optimism/pessimism prediction in Twitter and identify the most promising deep learning models for this task. Identifying optimism and pessimism in Twitter has many applications including identifying suicidal/depressive people and providing better social support (e.g., emotional/empathetic support) that can improve people’s moods and attitudes (Yan and Tan, 2014; Biyani et al., 2014; Khanpour et al., 2018, 2017; Qiu et al., 2011). Second, since it may seem intuitive that a positive sentiment is associated with optimism and a negative sentiment with pessimism, we address the question: “Would a sentiment classifier be sufficient to correctly identify optimism and pessimism in social media?” Figure 1 shows evidence that a sentiment tool would not suffice on accurately Identifying optimistic and pessimistic viewpoints and users from Twitter is useful for providing better social support to those who need such support, and for minimizing the negative influence among users and"
D18-1067,P05-1015,0,0.123711,"nd friends (Peterson and Bossio, 2001; Achat et al., 2000; 1 https://www.imdb.com/title/tt1878870/ 652 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 652–658 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figure 1: Sentiment prediction on tweets with pessimist (left) and optimist (right) connotations. The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a corpus for sentiment analysis that capture complex linguistic patterns. This dataset2 is based on a dataset originally introduced by Pang and Lee (2005) and consists of 10,662 sentences from movie reviews downloaded from rottentomatoes.com. From these sentences, 215,154 phrases were extracted using the Stanford Parser (Klein and Manning, 2003) and labeled using Amazon Mechanical Turk such that each phrase was annotated by 3 human judges. The Twitter Sentiment Analysis (TSA) dataset,3 available online for download, contains 1,578,627 tweets that are classified as 1 for positive sentiment and 0 for negative sentiment. predicting tweets with pessimistic and optimistic connotations (left and right side of the figure, respectively). We answer the"
D18-1067,D14-1162,0,0.0820298,"1 are in the “gray” area that is harder to classify. Note that Ruan et al. (2016) considered the tweets with a score between -1 and 1 as being neutral. User Level 0 1/ − 1 BiLSTM GRUStack CNN 79.65 80.19 77.78 87.24 87.76 90.32 76.65 76.38 73.55 90.52 92.24 91.68 NB SVM 74.20 67.80 84.10 83.30 71.30 64.70 80.10 81.80 Table 2: Accuracy of deep learning vs. traditional classifiers on the OPT dataset (shown as percentage). ing rate by half. We used mini-batches of 40 samples. Dropout rate was set to 0.5 and the classifier’s last three layers have 300, 200, and 100 neurons. We used GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B4 with 300 dimensions as fixed word embeddings. For sentence embedding, after a cleanup process, sentences were transformed into a list of words, then words were replaced with word embeddings (GloVe) and padding was used to align batch sentences to the same size. 3.1 3.2 In our second experiment, we investigate the correlation between sentiment and optimism / pessimism, and argue that sentiment analyzers, that are trained to predict sentiment (Liu, 2012; Pang and Lee, 2008), fail to detect optimism and pessimism. Specifically, we train several sentiment classifiers"
D18-1067,I17-2042,1,0.886764,"Missing"
D18-1067,P16-2052,0,0.046558,"u2 1 Computer Science, Kansas State University Manhattan, Kansas 2 Faculty of Mathematics and Computer Science, University of Bucharest Bucharest, Romania ccaragea@ksu.edu, ldinu@fmi.unibuc.ro, bogdan27182@gmail.com Abstract Scheier et al., 2001). On the other hand, optimism reduces stress and promotes better physical health and overall well-being (Carver et al., 2010). Despite that optimism and pessimism are under the scrutiny of many researchers (Rasmussen et al., 2009; Kumar et al., 2017), large scale analyses that explore optimism and pessimism in social media have just started to emerge (Ruan et al., 2016). However, Ruan et al. (2016) focused on identifying optimism and pessimism in Twitter using a simple “bag of words” representation with no emphasis on incorporating semantic information hidden in text. Often, a deeper understanding of the text that accounts for textual semantic similarities and the writer’s intention are required in order to correctly detect the characteristics of optimistic and pessimistic feelings in tweets. Towards this end, our contributions in this paper are as follows. First, we focus on the question: “Would a deep learning approach help to discover these characteristic"
D18-1067,D13-1170,0,0.0139922,"e attitudes impact negatively one’s mental health, can induce suicidal thoughts, and affect negatively not only the person in question, but also their family and friends (Peterson and Bossio, 2001; Achat et al., 2000; 1 https://www.imdb.com/title/tt1878870/ 652 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 652–658 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figure 1: Sentiment prediction on tweets with pessimist (left) and optimist (right) connotations. The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a corpus for sentiment analysis that capture complex linguistic patterns. This dataset2 is based on a dataset originally introduced by Pang and Lee (2005) and consists of 10,662 sentences from movie reviews downloaded from rottentomatoes.com. From these sentences, 215,154 phrases were extracted using the Stanford Parser (Klein and Manning, 2003) and labeled using Amazon Mechanical Turk such that each phrase was annotated by 3 human judges. The Twitter Sentiment Analysis (TSA) dataset,3 available online for download, contains 1,578,627 tweets that are classified as 1 for positive sentiment"
D19-1236,P11-1051,0,0.0201244,"nd which part of the scientific text is more specific to its author, and how much of the text is really useful in predicting the author. We look separately first at the entire body of the article, secondly at only the title and abstract, and thirdly only at the context of the citations that occur in the text - assuming these might be useful by giving away something about the author’s citation pattern that cannot be inferred from the references list alone. Citation contexts have been shown to capture useful information in previous studies focusing on text summarization (Qazvinian et al., 2010; Abu-Jbara and Radev, 2011; Qazvinian and Radev, 2008), document indexing (Ritchie et al., 2008), keyphrase extraction (Gollapalli and Caragea, 2014; Caragea et al., 2014), and author influence in digital libraries (Kataria et al., 2011). These last experiments aiming to analyze feature importance are performed in the small-scale setting, where only the top 100 authors are considered. We report all results both at segment level 2323 Features All features (top100cls) All features (top200cls) All features (top500cls) All features (mid200cls) All features (last200cls) All features (922cls) Content + Ref (100cls) Content ("
D19-1236,W12-3202,0,0.0244868,"y a subtask (for example, focusing only on a subset of the data), or limiting the analysis to one 2318 aspect of the text (for example, focusing on the stylistic level). While previous studies support the hypothesis that authors of a scientific article are possible to predict from an anonymized paper, we attempt to provide a fuller picture regarding what exactly it is about an anonymous article that can give away its authors. 3 Datasets For evaluation, we used two datasets of articles from the computational linguistics conferences ACL and EMNLP, published on or before 2014 (Bird et al., 2008; Anderson et al., 2012). The ACL dataset contains 4, 412 articles authored by a total of 6, 565 unique authors, whereas the EMNLP dataset is comprised of 1, 027 articles written by 1, 861 unique authors in total.1 Note that the size of the EMNLP dataset is much smaller than the size of the ACL dataset since EMNLP is a much newer conference compared to ACL. From each dataset, we normalized the author names to consist of the initial of the first name and the full last name and removed the authors with less than three articles (to ensure enough data for training and evaluation), leaving us with 922 authors for the ACL"
D19-1236,bird-etal-2008-acl,0,0.11997,"Missing"
D19-1236,D14-1150,1,0.854349,"rately first at the entire body of the article, secondly at only the title and abstract, and thirdly only at the context of the citations that occur in the text - assuming these might be useful by giving away something about the author’s citation pattern that cannot be inferred from the references list alone. Citation contexts have been shown to capture useful information in previous studies focusing on text summarization (Qazvinian et al., 2010; Abu-Jbara and Radev, 2011; Qazvinian and Radev, 2008), document indexing (Ritchie et al., 2008), keyphrase extraction (Gollapalli and Caragea, 2014; Caragea et al., 2014), and author influence in digital libraries (Kataria et al., 2011). These last experiments aiming to analyze feature importance are performed in the small-scale setting, where only the top 100 authors are considered. We report all results both at segment level 2323 Features All features (top100cls) All features (top200cls) All features (top500cls) All features (mid200cls) All features (last200cls) All features (922cls) Content + Ref (100cls) Content (100cls) Style ft. Content (100cls) Title+abstr. Ref. contexts Acc 84.65 70.00 63.77 62.38 54.18 52.41 75.38 49.87 25.21 49.87 16.19 35.53 MAP 9.8"
D19-1236,W17-4907,0,0.035634,"Missing"
D19-1236,D14-1181,0,0.00964642,"he task of authorship attribution and showed promising results in a scenario with many authors. Rexha et al. (2015) analyzed the style of medical scientific articles and how the stylistic uniformity of an article varies with the number of co-authors. Outside the world of scientific articles, a few previous studies showed the promise of using neural networks for authorship attribution. Bagnall (2015) successfully used a multi-headed Recurrent Neural Network for an author identification task at PAN 2015. The use of Convolutional Neural Networks (CNNs) for learning from text data was proposed by Kim (2014), where CNNs are successfully applied to several sentence classification tasks. Rhodes (2015) trained a Convolutional Neural Network on word embeddings for predicting authors of medium-sized texts, and Shrestha et al. (2017) used CNNs in an authorship attribution task on tweets. Luyckx and Daelemans (2008) studied the effects of having many authors as classes and of limited training data on author attribution - which are realistic, but difficult scenarios, common to our problem as well. As far as we are aware, no other study has dealt with analyzing the authorship of articles published at ACL"
D19-1236,C08-1065,0,0.0429135,"s, a few previous studies showed the promise of using neural networks for authorship attribution. Bagnall (2015) successfully used a multi-headed Recurrent Neural Network for an author identification task at PAN 2015. The use of Convolutional Neural Networks (CNNs) for learning from text data was proposed by Kim (2014), where CNNs are successfully applied to several sentence classification tasks. Rhodes (2015) trained a Convolutional Neural Network on word embeddings for predicting authors of medium-sized texts, and Shrestha et al. (2017) used CNNs in an authorship attribution task on tweets. Luyckx and Daelemans (2008) studied the effects of having many authors as classes and of limited training data on author attribution - which are realistic, but difficult scenarios, common to our problem as well. As far as we are aware, no other study has dealt with analyzing the authorship of articles published at ACL or EMNLP (or a comparably prestigious conference) without restricting the scenario to only a subtask (for example, focusing only on a subset of the data), or limiting the analysis to one 2318 aspect of the text (for example, focusing on the stylistic level). While previous studies support the hypothesis th"
D19-1236,C08-1087,0,0.0478182,"ific text is more specific to its author, and how much of the text is really useful in predicting the author. We look separately first at the entire body of the article, secondly at only the title and abstract, and thirdly only at the context of the citations that occur in the text - assuming these might be useful by giving away something about the author’s citation pattern that cannot be inferred from the references list alone. Citation contexts have been shown to capture useful information in previous studies focusing on text summarization (Qazvinian et al., 2010; Abu-Jbara and Radev, 2011; Qazvinian and Radev, 2008), document indexing (Ritchie et al., 2008), keyphrase extraction (Gollapalli and Caragea, 2014; Caragea et al., 2014), and author influence in digital libraries (Kataria et al., 2011). These last experiments aiming to analyze feature importance are performed in the small-scale setting, where only the top 100 authors are considered. We report all results both at segment level 2323 Features All features (top100cls) All features (top200cls) All features (top500cls) All features (mid200cls) All features (last200cls) All features (922cls) Content + Ref (100cls) Content (100cls) Style ft. Content (1"
D19-1236,C10-1101,0,0.0349713,"es, in order to understand which part of the scientific text is more specific to its author, and how much of the text is really useful in predicting the author. We look separately first at the entire body of the article, secondly at only the title and abstract, and thirdly only at the context of the citations that occur in the text - assuming these might be useful by giving away something about the author’s citation pattern that cannot be inferred from the references list alone. Citation contexts have been shown to capture useful information in previous studies focusing on text summarization (Qazvinian et al., 2010; Abu-Jbara and Radev, 2011; Qazvinian and Radev, 2008), document indexing (Ritchie et al., 2008), keyphrase extraction (Gollapalli and Caragea, 2014; Caragea et al., 2014), and author influence in digital libraries (Kataria et al., 2011). These last experiments aiming to analyze feature importance are performed in the small-scale setting, where only the top 100 authors are considered. We report all results both at segment level 2323 Features All features (top100cls) All features (top200cls) All features (top500cls) All features (mid200cls) All features (last200cls) All features (922cls) Conte"
D19-1236,P12-2052,0,0.0232891,"13) studied authorship attribution on scientific articles specifically in the multi-author setting, using various text-based features (including word n-grams and various stylistic features) and models based on logistic regression and expectation maximization. Hitschler et al. (2017) performed experiments for predicting authors of ACL articles, restricting their data to only single-author articles. Their study focused on the style level, using only POS tag sequences, and showed that limiting the number of words considered as features can have a beneficial effect on the predictor’s performance. Seroussi et al. (2012) proposed the use of an author-topic model (Rosen-Zvi et al., 2004) for the task of authorship attribution and showed promising results in a scenario with many authors. Rexha et al. (2015) analyzed the style of medical scientific articles and how the stylistic uniformity of an article varies with the number of co-authors. Outside the world of scientific articles, a few previous studies showed the promise of using neural networks for authorship attribution. Bagnall (2015) successfully used a multi-headed Recurrent Neural Network for an author identification task at PAN 2015. The use of Convolut"
D19-1236,E17-2106,0,0.0695461,"Missing"
D19-1236,N03-1033,0,0.0168767,"word segments, rather than full articles. Before extracting content features, we discard outliers, ignoring articles consisting of either zero or more than 20, 000 words. In addition, we also extract the context around citation mentions within the content of articles, by selecting a window of 100 characters around the citation (and excluding the citation itself), then applying the same text preprocessing steps as above only on this window. This is used as a separate feature, as described in Section 5.2. The extraction of the part-of-speech features is done by applying the Stanford POS tagger (Toutanova et al., 2003) to the word sequences, re2321 2 https://github.com/kermitt2/grobid Dataset ACL ACL ACL ACL ACL ACL EMNLP EMNLP EMNLP EMNLP sulting in part-of-speech sequences corresponding to each article segment. Stopwords are encoded as bag-of-words for each article segment. Citations extracted from the ”References” section of each article are encoded as bag-of-authors unordered sets of citation frequencies corresponding to each cited author. Recall that author names (when they occur either as authors of the target article, or as authors of a cited paper in a references list) are normalized to consist of t"
dinu-ciobanu-2014-building,A00-2038,0,\N,Missing
dinu-ciobanu-2014-building,N03-2016,0,\N,Missing
dinu-ciobanu-2014-building,P93-1001,0,\N,Missing
dinu-ciobanu-2014-building,P07-2045,0,\N,Missing
dinu-ciobanu-2014-building,P10-1105,0,\N,Missing
dinu-ciobanu-2014-building,J03-1002,0,\N,Missing
dinu-ciobanu-2014-building,R13-1019,1,\N,Missing
dinu-ciobanu-2014-building,W12-0216,0,\N,Missing
dinu-ciobanu-2014-building,R11-1034,0,\N,Missing
dinu-ciobanu-2014-romance,carreras-etal-2004-freeling,0,\N,Missing
dinu-ciobanu-2014-romance,padro-etal-2010-freeling,0,\N,Missing
dinu-ciobanu-2014-romance,P07-2045,0,\N,Missing
dinu-ciobanu-2014-romance,J03-1002,0,\N,Missing
dinu-ciobanu-2014-romance,atserias-etal-2006-freeling,0,\N,Missing
dinu-ciobanu-2014-romance,tiedemann-2012-parallel,0,\N,Missing
dinu-ciobanu-2014-romance,padro-stanilovsky-2012-freeling,0,\N,Missing
dinu-dinu-2006-data,J94-3001,0,\N,Missing
dinu-etal-2008-authorship,W06-1657,0,\N,Missing
dinu-etal-2012-romanian,barbu-2008-romanian,0,\N,Missing
dinu-etal-2012-romanian,N03-1006,0,\N,Missing
dinu-etal-2012-romanian,D09-1142,0,\N,Missing
dinu-etal-2014-aggregation,pearce-2002-comparative,0,\N,Missing
dinu-etal-2014-aggregation,J90-1003,0,\N,Missing
dinu-etal-2014-aggregation,J93-1007,0,\N,Missing
dinu-etal-2014-aggregation,P01-1025,0,\N,Missing
dinu-etal-2014-aggregation,2003.mtsummit-papers.39,0,\N,Missing
dinu-etal-2014-using,barbu-2008-romanian,0,\N,Missing
dinu-etal-2014-using,W02-1001,0,\N,Missing
dinu-etal-2014-using,P07-1013,0,\N,Missing
dinu-etal-2014-using,P09-1014,0,\N,Missing
dinu-etal-2014-using,P08-1065,0,\N,Missing
dinu-etal-2014-using,E14-4013,1,\N,Missing
E12-1053,R11-1075,1,0.487809,"Missing"
E12-1053,barbu-2008-romanian,0,\N,Missing
E14-4004,R13-1018,1,0.743466,"; Abe and Tsumoto, 2010; Kumar et al., 2011) and a few use external linguistic knowledge (Kanhabua and Nørv˚ag, 2009). A couple of approaches try to classify texts not only regarding the time span in which the texts were written, but also their geographical location such as (Mokhov, 2010) for French and, more recently, (Trieschnigg et al., 2012) for Dutch. At the word level, two studies aim to model and understand how word usage and meaning change over time (Wijaya and Yeniterzi, 2011), (Mihalcea and Nastase, 2012). The most recent studies in temporal text classification to our knowledge are (Ciobanu et al., 2013) ˇ for Romanian using lexical features and (Stajner and Zampieri, 2013) for Portuguese using stylistic and readability features. 17 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17–21, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 3 Methods 3.1 linear model. The method is to convert a dataset of the form D = {(x, y) : x ∈ Rd , y ∈ Y} into a pairwise dataset: Corpora To evaluate the method proposed here we used three historical corpora. An English historical corpus entitled Corpus of La"
E14-4004,W06-0903,0,0.171854,"and NLP in general; historical linguists and philologists who investigate language change; and finally scholars in the digital humanities who often deal with historical One of the first studies to model temporal information for the automatic dating of documents is the work of de Jong et al. (2005). In these experiments, authors used unigram language models to classify Dutch texts spanning from January 1999 to February 2005 using normalised log-likelihood ratio (NLLR) (Kraaij, 2004). As to the features used, a number of approaches proposed to automatic date take into account lexical features (Dalli and Wilks, 2006; Abe and Tsumoto, 2010; Kumar et al., 2011) and a few use external linguistic knowledge (Kanhabua and Nørv˚ag, 2009). A couple of approaches try to classify texts not only regarding the time span in which the texts were written, but also their geographical location such as (Mokhov, 2010) for French and, more recently, (Trieschnigg et al., 2012) for Dutch. At the word level, two studies aim to model and understand how word usage and meaning change over time (Wijaya and Yeniterzi, 2011), (Mihalcea and Nastase, 2012). The most recent studies in temporal text classification to our knowledge are ("
E14-4004,P12-2051,0,0.108696,"a number of approaches proposed to automatic date take into account lexical features (Dalli and Wilks, 2006; Abe and Tsumoto, 2010; Kumar et al., 2011) and a few use external linguistic knowledge (Kanhabua and Nørv˚ag, 2009). A couple of approaches try to classify texts not only regarding the time span in which the texts were written, but also their geographical location such as (Mokhov, 2010) for French and, more recently, (Trieschnigg et al., 2012) for Dutch. At the word level, two studies aim to model and understand how word usage and meaning change over time (Wijaya and Yeniterzi, 2011), (Mihalcea and Nastase, 2012). The most recent studies in temporal text classification to our knowledge are (Ciobanu et al., 2013) ˇ for Romanian using lexical features and (Stajner and Zampieri, 2013) for Portuguese using stylistic and readability features. 17 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17–21, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 3 Methods 3.1 linear model. The method is to convert a dataset of the form D = {(x, y) : x ∈ Rd , y ∈ Y} into a pairwise dataset: Corpora To evaluate the meth"
E14-4013,P09-1014,0,0.841158,"linguistic literature, i.e., the Romanian syllable received much more attention than the Romanian stress (Dinu and Dinu, 2005; Dinu, 2003; Dinu et al., 2013; Toma et al., 2009). One possible explanation for the fact that Romanian syllabication was more intensively studied than Romanian stress is the immediate application of syllabication to text editors which need reliable hyphenation. Another explanation could be that most linguists (most recently Dindelegan (2013)) insisted that Romanian stress is not predictable, thus discouraging attempts to investigate any systematic patterns. Dou et al. (2009) address lexical stress prediction as a sequence tagging problem, which proves to be an accurate approach for this task. The effectiveness of using conditional random fields for orthographic syllabication is investigated by Trogkanis and Elkan (2010), who employ them for determining syllable boundaries and show that they outperform previous methods. Bartlett et al. (2008) use a discriminative tagger for automatic orthographic syllabication and present several approaches for assigning labels, including the language-independent Numbered NB tag scheme, which labels each letter with a value equal"
E14-4013,barbu-2008-romanian,0,0.225537,"n-grams up to n = W in a window of radius W around the current position. For example, if W = 2, the feature template consists of c[-2], c[-1], c[0], c[1], c[2], c[-2:-1], c[-1:0], c[0:1], c[1:2]. If the current letter is the fourth of the word dinosaur, d 0 1 i 1 0 a 2 0 m 2 1 o 2 2 n 2 3 d 2 The features used for syllabication are based on the same principle, but because the positions are in-between characters, the window of radius W has length 2W instead of 2W + 1. For this model we used only character n-grams as features. 3 Data We run our experiments for Romanian using the RoSyllabiDict (Barbu, 2008) dictionary, which is a dataset of annotated words comprising 525,528 inflected forms for approximately 65,000 lemmas. This is, to our best knowledge, the largest experiment conducted and reported for Romanian so far. For each entry, the syllabication and the stressed vowel (and, in case of ambiguities, also grammatical information or type of syllabication) are provided. For example, the word copii (children) has the following representation: &lt;form w=""copii"" obs=""s.""&gt; co-píi&lt;/form&gt; We investigate stress placement with regard to the syllable structure and we provide in Table 1 the percentages o"
E14-4013,P08-1065,0,0.486315,"ich need reliable hyphenation. Another explanation could be that most linguists (most recently Dindelegan (2013)) insisted that Romanian stress is not predictable, thus discouraging attempts to investigate any systematic patterns. Dou et al. (2009) address lexical stress prediction as a sequence tagging problem, which proves to be an accurate approach for this task. The effectiveness of using conditional random fields for orthographic syllabication is investigated by Trogkanis and Elkan (2010), who employ them for determining syllable boundaries and show that they outperform previous methods. Bartlett et al. (2008) use a discriminative tagger for automatic orthographic syllabication and present several approaches for assigning labels, including the language-independent Numbered NB tag scheme, which labels each letter with a value equal to the distance between the letter and the last syllable boundary. According to Damper et al. (1999), syllable structure and stress pattern are very useful in text-to-speech synthesis, as they provide valuable knowledge regarding the pronunciation modeling. Besides converting the letters to the corresponding phonemes, information about syllable boundaries and stress place"
E14-4013,W02-1001,0,0.243261,"Missing"
E14-4013,P10-1038,0,0.0134737,"syllabication was more intensively studied than Romanian stress is the immediate application of syllabication to text editors which need reliable hyphenation. Another explanation could be that most linguists (most recently Dindelegan (2013)) insisted that Romanian stress is not predictable, thus discouraging attempts to investigate any systematic patterns. Dou et al. (2009) address lexical stress prediction as a sequence tagging problem, which proves to be an accurate approach for this task. The effectiveness of using conditional random fields for orthographic syllabication is investigated by Trogkanis and Elkan (2010), who employ them for determining syllable boundaries and show that they outperform previous methods. Bartlett et al. (2008) use a discriminative tagger for automatic orthographic syllabication and present several approaches for assigning labels, including the language-independent Numbered NB tag scheme, which labels each letter with a value equal to the distance between the letter and the last syllable boundary. According to Damper et al. (1999), syllable structure and stress pattern are very useful in text-to-speech synthesis, as they provide valuable knowledge regarding the pronunciation mo"
E14-4013,P07-1013,0,0.301665,"oaches for assigning labels, including the language-independent Numbered NB tag scheme, which labels each letter with a value equal to the distance between the letter and the last syllable boundary. According to Damper et al. (1999), syllable structure and stress pattern are very useful in text-to-speech synthesis, as they provide valuable knowledge regarding the pronunciation modeling. Besides converting the letters to the corresponding phonemes, information about syllable boundaries and stress placement is also needed for the correct synthesizing of a word in grapheme-to-phoneme conversion (Demberg et al., 2007). 64 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 64–68, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics o, the feature values would be i, n, o, s, a, in, no, os, sa. We use two additional types of features: • features regarding the C/V structure of the word: n-grams using, instead of characters, markers for consonants (C) and vowels (V); In this paper, we rely on the assumption that the stress system of Romanian is predictable. We propose a system for automatic prediction of stress pla"
J19-4003,W12-4410,0,0.0664659,"Missing"
J19-4003,I13-1112,0,0.0545989,"Missing"
J19-4003,N09-3008,0,0.434392,"target word. In the case of insertions, we add the new character to the previous label, because there is no input character in the source language to which we could associate the inserted character as label. We account for affixes separately: For each input word, we add two more characters B and E, marking the beginning and the end of the word. The characters that are inserted in the target word at the beginning or at the end of the word are associated with these 21 In a preliminary step, we experimented with two alignment methods: the method based on profile hidden Markov models proposed by Bhargava and Kondrak (2009) and the Needleman and Wunsch (1970) global alignment algorithm. Because the alignment is only a pre-processing step for our task, we evaluate the alignment methods by the downstream results, that is, the accuracy obtained by the CRF system when using one or the other alignment method. We observed that the results obtained with the two alignment methods were very similar, slightly better for the latter. Thus, we report the results obtained with the Needleman Wunsch alignment algorithm, following the methodology presented in Section 5.1. 687 Computational Linguistics Volume 45, Number 4 special"
J19-4003,N09-1008,0,0.705345,"Missing"
J19-4003,D07-1093,0,0.0995015,"Missing"
J19-4003,P93-1001,0,0.123304,"graphic and the phonetic forms available, and we obtain very similar performance for the two cases. 3. Related Work In a natural way, one of the most investigated problems in historical linguistics is to determine whether similar words are related or not (Kondrak 2002). 3.1 Identification of Related Words Most studies in this area focus on automatically identifying pairs of cognates. There are three important aspects widely investigated in the task of identifying cognates: semantic, phonetic, and orthographic similarity. They were utilized both individually (Simard, Foster, and Isabelle 1992; Church 1993; Inkpen, Frunza, and Kondrak 2005) and combined (Kondrak 2004; Steiner, Stadler, and Cysouw 2011) in order to detect pairs of cognates across languages. For determining semantic similarity, external lexical resources, such as WordNet (Fellbaum 1998), might be necessary. For measuring phonetic and orthographic proximity of cognate candidates, string similarity metrics can be applied, using the phonetic or orthographic word forms as input. Various measures were investigated and compared (Inkpen, Frunza, and Kondrak 2005; Hall and Klein 2010); the edit distance (Levenshtein 1965), the XDice metr"
J19-4003,D14-1112,1,0.936803,"the best approaches to reconstructing proto-words (Bouchard-Cot´ 2013) relies on an analogy to reconstructing the genealogy of the species from genetic sequences in biology. This approach requires an existing phylogenetic tree and the phonetic transcripts of the words, to infer the ancient word forms based on probability estimates for all the possible sound changes on each branch of the tree. 4. A Dictionary-Based Approach to Building a Data Set of Related Words In this section, we propose an algorithm for extracting cognates from electronic dictionaries that contain etymological information (Ciobanu and Dinu 2014c). After we obtain a data set of related words from dictionaries, we develop automatic methods, based on machine learning, for identifying and producing related words. Considering a set of words in a given language L1 , to identify the cognate pairs between L1 and a related language L2 we apply the following strategy: First, we determine the etymologies of the given words. Then, we translate in L2 all words without L2 etymology. We consider cognate candidates the pairs of input words and their translations. Using electronic dictionaries, we extract etymology-related information for the transl"
J19-4003,P14-2017,1,0.894844,"the best approaches to reconstructing proto-words (Bouchard-Cot´ 2013) relies on an analogy to reconstructing the genealogy of the species from genetic sequences in biology. This approach requires an existing phylogenetic tree and the phonetic transcripts of the words, to infer the ancient word forms based on probability estimates for all the possible sound changes on each branch of the tree. 4. A Dictionary-Based Approach to Building a Data Set of Related Words In this section, we propose an algorithm for extracting cognates from electronic dictionaries that contain etymological information (Ciobanu and Dinu 2014c). After we obtain a data set of related words from dictionaries, we develop automatic methods, based on machine learning, for identifying and producing related words. Considering a set of words in a given language L1 , to identify the cognate pairs between L1 and a related language L2 we apply the following strategy: First, we determine the etymologies of the given words. Then, we translate in L2 all words without L2 etymology. We consider cognate candidates the pairs of input words and their translations. Using electronic dictionaries, we extract etymology-related information for the transl"
J19-4003,dinu-ciobanu-2014-building,1,0.943529,"the best approaches to reconstructing proto-words (Bouchard-Cot´ 2013) relies on an analogy to reconstructing the genealogy of the species from genetic sequences in biology. This approach requires an existing phylogenetic tree and the phonetic transcripts of the words, to infer the ancient word forms based on probability estimates for all the possible sound changes on each branch of the tree. 4. A Dictionary-Based Approach to Building a Data Set of Related Words In this section, we propose an algorithm for extracting cognates from electronic dictionaries that contain etymological information (Ciobanu and Dinu 2014c). After we obtain a data set of related words from dictionaries, we develop automatic methods, based on machine learning, for identifying and producing related words. Considering a set of words in a given language L1 , to identify the cognate pairs between L1 and a related language L2 we apply the following strategy: First, we determine the etymologies of the given words. Then, we translate in L2 all words without L2 etymology. We consider cognate candidates the pairs of input words and their translations. Using electronic dictionaries, we extract etymology-related information for the transl"
J19-4003,P15-2071,1,0.894524,"Missing"
J19-4003,C18-1136,1,0.570199,"results. Although for some parts of speech the results improved, overall the performance of the model was lower than that of the CRF model followed by maximum entropy reranking. 688 Ciobanu and Dinu Automatic Identification and Production of Related Words for Historical Linguistics provided by the Mallet toolkit (McCallum 2002). For parameter tuning, we perform a grid search for the number of iterations in {1, 5, 10, 25, 50, 100} and for the size of the window w in {1, 2, 3, 4, 5}. 6.2 Reconstruction of Proto-words We address the problem of reconstructing proto-words in two steps (Ciobanu and Dinu 2018). First, given cognate pairs in multiple modern languages and their common ancestors, we apply a method based on sequence labeling for reconstructing proto-words. We apply the method on each modern language individually. Second, we propose several ensemble methods for combining information from multiple systems, with the purpose of joining the best productions from all modern languages. Through experiments, ˆ e we show that our best ensemble system outperforms previous results Bouchard-Cot´ et al. 2007. The novelty of our approach is enhancing the sequence alignment system with an additional s"
J19-4003,P98-1043,0,0.411488,"tion and comparative reconstruction. Historical derivation consists of deriving the modern forms of the words from the old ones. Comparative reconstruction is the opposite process, in which the old forms of the words are reconstructed from the modern ones. Researchers have been continuously interested in language derivation (Pagel et al. 2013). The first attempts to address this problem focused on regular sound correspondences to construct modern forms of the words, given a proto-language, or vice versa. Some of the early studies on partially automating proto-language reconstruction belong to Covington (1998) (investigating multiple alignment for historical comparison), and Kondrak (2002) (proposing, among others, methods for cognate alignment and identification). Most of the previous approaches to producing related words relied on phonetic transcriptions (Eastlack 1977; Hartman 1981; Hewson 1974). They built on the idea that, given the phonological context, sound changes follow certain regularities across the entire vocabulary of a language. The proposed methods (Hewson 1974; Eastlack 1977; Hartman 1981) required a list of known sound correspondences as input, collected from dictionaries or publi"
J19-4003,I08-6006,0,0.0426305,"op a tool that would provide support in historical linguistics, where the produced words would be further analyzed by domain experts. We propose an approach based on conditional random fields. We apply a sequence labeling method that predicts the form of the related words. Conditional Random Fields. From the alignment of related words in the training set, the system learns orthographic patterns for the changes in spelling between the source and the target language. The method that we utilize is based on sequence labeling, an approach that has been proven useful in generating transliterations (Ganesh et al. 2008; Ammar, Dyer, and Smith 2012). In our case, the words are the sequences, and their characters are the tokens. Our purpose is to obtain, for each input word, a sequence of characters that compose its related word. To this end, we use conditional random fields (CRFs) (Lafferty, McCallum, and Pereira 2001). As features for the CRF system, we use character n-grams from the input words, extracted from a fixed window w around the current token. Pairwise Sequence Alignment. To align pairs of words, we use the Needleman and Wunsch (1970) global alignment algorithm.21 For example, for the Romanian wor"
J19-4003,P10-1105,0,0.0648487,"utilized both individually (Simard, Foster, and Isabelle 1992; Church 1993; Inkpen, Frunza, and Kondrak 2005) and combined (Kondrak 2004; Steiner, Stadler, and Cysouw 2011) in order to detect pairs of cognates across languages. For determining semantic similarity, external lexical resources, such as WordNet (Fellbaum 1998), might be necessary. For measuring phonetic and orthographic proximity of cognate candidates, string similarity metrics can be applied, using the phonetic or orthographic word forms as input. Various measures were investigated and compared (Inkpen, Frunza, and Kondrak 2005; Hall and Klein 2010); the edit distance (Levenshtein 1965), the XDice metric (Brew and McKelvie 1996), and the longest common subsequence ratio (Melamed 1995) are among the most frequently used metrics in this field. Gomes and Pereira Lopes (2011) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words. 671 Computational Linguistics Volume 45, Number 4 Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence"
J19-4003,W04-3250,0,0.0275056,"Missing"
J19-4003,A00-2038,0,0.835412,"he most frequently used metrics in this field. Gomes and Pereira Lopes (2011) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words. 671 Computational Linguistics Volume 45, Number 4 Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch 1970; Smith and Waterman 1981; Gotoh 1982) to obtain orthographic alignment scores for cognate candidates. Kondrak (2000) developed the ALINE system, which aligns words’ phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully used in various approaches to identifying cognates (Koehn an"
J19-4003,N03-2016,0,0.234225,"Missing"
J19-4003,W12-0216,0,0.0321721,"rned transitions between words. 671 Computational Linguistics Volume 45, Number 4 Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch 1970; Smith and Waterman 1981; Gotoh 1982) to obtain orthographic alignment scores for cognate candidates. Kondrak (2000) developed the ALINE system, which aligns words’ phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully used in various approaches to identifying cognates (Koehn and Knight 2000; Mulloni and Pekar 2006; Navlea and Todirascu 2011). More recent approaches used neural networks (Rama 2016) and dictionary definitions (St Arnaud, Beck, and Kondra"
J19-4003,D15-1166,0,0.0211144,"Missing"
J19-4003,W05-0606,0,0.0778308,"n, Italian, French, Spanish, Portuguese), but we also work with languages from other language families. The methods that we propose can use either the orthographic or the phonetic form of the words. We use the orthographic form because this is what is available for most data sets. Moreover, we build on the idea that orthographic changes represent sound correspondences to a fairly large extent (Delmestri and Cristianini 2010). Even if the phonetic representations are largely used in identifying cognates, the orthographic representations have led to good performance as well, even on noisy data (Mackay and Kondrak 2005; Delmestri and Cristianini 2012). For one of the data sets used in our experiments, we have both the orthographic and the phonetic forms available, and we obtain very similar performance for the two cases. 3. Related Work In a natural way, one of the most investigated problems in historical linguistics is to determine whether similar words are related or not (Kondrak 2002). 3.1 Identification of Related Words Most studies in this area focus on automatically identifying pairs of cognates. There are three important aspects widely investigated in the task of identifying cognates: semantic, phone"
J19-4003,W95-0115,0,0.257919,", Stadler, and Cysouw 2011) in order to detect pairs of cognates across languages. For determining semantic similarity, external lexical resources, such as WordNet (Fellbaum 1998), might be necessary. For measuring phonetic and orthographic proximity of cognate candidates, string similarity metrics can be applied, using the phonetic or orthographic word forms as input. Various measures were investigated and compared (Inkpen, Frunza, and Kondrak 2005; Hall and Klein 2010); the edit distance (Levenshtein 1965), the XDice metric (Brew and McKelvie 1996), and the longest common subsequence ratio (Melamed 1995) are among the most frequently used metrics in this field. Gomes and Pereira Lopes (2011) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words. 671 Computational Linguistics Volume 45, Number 4 Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch 1970; Smith and Waterman 1981; Gotoh 1982) to obtain orthographic alignment scores for cognate"
J19-4003,P07-3005,0,0.048552,"raphic changes have also been used for producing cognates, which is closely related to the task of identifying cognates, but has not yet been as intensively studied. Whereas the purpose of identifying cognates is to determine whether two given words form a cognate pair, the aim of producing cognates is, given a word in a source language, to automatically produce its cognate pair in a target language. Beinborn, Zesch, and Gurevych (2013) proposed a method for the production of cognates relying on statistical character-based machine translation and learning orthographic production patterns, and Mulloni (2007) introduced an algorithm based on edit distance alignment and the identification of orthographic cues when words enter a new language. ˆ e et al. One of the best approaches to reconstructing proto-words (Bouchard-Cot´ 2013) relies on an analogy to reconstructing the genealogy of the species from genetic sequences in biology. This approach requires an existing phylogenetic tree and the phonetic transcripts of the words, to infer the ancient word forms based on probability estimates for all the possible sound changes on each branch of the tree. 4. A Dictionary-Based Approach to Building a Data S"
J19-4003,mulloni-pekar-2006-automatic,0,0.615271,"Missing"
J19-4003,R11-1034,0,0.0147074,"ns words’ phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully used in various approaches to identifying cognates (Koehn and Knight 2000; Mulloni and Pekar 2006; Navlea and Todirascu 2011). More recent approaches used neural networks (Rama 2016) and dictionary definitions (St Arnaud, Beck, and Kondrak 2017) to identify cognates reliably. Minett and Wang (2003) focused on identifying borrowings within a family of genetically related languages and proposed, to this end, a distance-based and a character-based technique. Minett and Wang (2005) addressed the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. Tsvetkov, Ammar, and Dyer (2015) developed a model based on universal constraints f"
J19-4003,C16-1097,0,0.0185261,"computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully used in various approaches to identifying cognates (Koehn and Knight 2000; Mulloni and Pekar 2006; Navlea and Todirascu 2011). More recent approaches used neural networks (Rama 2016) and dictionary definitions (St Arnaud, Beck, and Kondrak 2017) to identify cognates reliably. Minett and Wang (2003) focused on identifying borrowings within a family of genetically related languages and proposed, to this end, a distance-based and a character-based technique. Minett and Wang (2005) addressed the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. Tsvetkov, Ammar, and Dyer (2015) developed a model based on universal constraints from Optimality Theory to identify plausible donor-loan wo"
J19-4003,1992.tmi-1.7,0,0.532715,"Missing"
J19-4003,D17-1267,0,0.163767,"Missing"
J19-4003,N15-1062,0,0.285572,"Missing"
J19-4003,W06-1626,0,0.0154014,"Missing"
L16-1522,W14-4213,0,0.0297618,"e quantify only the orthographic and phonetic differences, but the morphology and the syntax are other important aspects which contribute to the individualization of each variety, that we leave for further study. Previously, Tonelli et al. (2010) proposed such an adaptation for a morphological analyzer for Venetan. Similarly, Kanayama et al. (2014) built a dependency parser for Korean, leveraging resources (transfer learning) from Japanese. They showed that Korean sentences could be successfully parsed using features learnt from a Japanese training corpus. In the field of machine translation, Aminian et al. (2014) developed a method for translating from dialectal Arabic into English in which they reduce the OOV ratio by exploiting resources from standard Arabic. Although dialects and varieties have been investigated for other languages, such as Spanish and Portuguese (Zampieri and Gebre, 2012; Zampieri et al., 2013), Romanian dialects did not receive much attention in NLP. To our knowledge, while the syllabic structure of Aromanian has been previously investigated (Nisioi, 2014), this is one of the very first computational comparative studies on the Romanian dialects. 2. The Romanian Dialects Romanian"
L16-1522,D14-1112,1,0.929898,"-Romanian, Aromanian}. We obtain, thus, a dataset of 324 such input pairs. The goal is to automatically decide to which dialect the dialect-word belongs. The dialect identification problem is not trivial and our goal, in this paper, is not to improve on the state-of-the-art methods in this research area, but to investigate the predictive power of the orthographic and phonetic differences between Romanian and its dialects. We use a methodology that has been previously used for discriminating between related and unrelated words, and for distinguishing the type of relationship between the words (Ciobanu and Dinu, 2014b; Ciobanu and Dinu, 2015). We align the words using the Needleman-Wunsch alignment algorithm (Needleman and Wunsch, 1970) and we extract n-gram features from the alignment of the words. Additionally, we also extract n-grams of characters from the dialect-word. We search for the optimum n-gram size in {1, 2, 3, 4}, both for the n-grams extracted from the alignment and for the n-grams extracted from the dialect-word. We train a Logistic Regression classifier, using the imple3283 Dialect Word pair Alignment ME ros¸u - ros¸ r r o o s¸ s¸ u - AR ros¸u - ar´os¸uˇ a r r o o´ s¸ s¸ u uˇ IS ros¸u - r´"
L16-1522,P14-2017,1,0.926547,"-Romanian, Aromanian}. We obtain, thus, a dataset of 324 such input pairs. The goal is to automatically decide to which dialect the dialect-word belongs. The dialect identification problem is not trivial and our goal, in this paper, is not to improve on the state-of-the-art methods in this research area, but to investigate the predictive power of the orthographic and phonetic differences between Romanian and its dialects. We use a methodology that has been previously used for discriminating between related and unrelated words, and for distinguishing the type of relationship between the words (Ciobanu and Dinu, 2014b; Ciobanu and Dinu, 2015). We align the words using the Needleman-Wunsch alignment algorithm (Needleman and Wunsch, 1970) and we extract n-gram features from the alignment of the words. Additionally, we also extract n-grams of characters from the dialect-word. We search for the optimum n-gram size in {1, 2, 3, 4}, both for the n-grams extracted from the alignment and for the n-grams extracted from the dialect-word. We train a Logistic Regression classifier, using the imple3283 Dialect Word pair Alignment ME ros¸u - ros¸ r r o o s¸ s¸ u - AR ros¸u - ar´os¸uˇ a r r o o´ s¸ s¸ u uˇ IS ros¸u - r´"
L16-1522,P15-2071,1,0.845439,"obtain, thus, a dataset of 324 such input pairs. The goal is to automatically decide to which dialect the dialect-word belongs. The dialect identification problem is not trivial and our goal, in this paper, is not to improve on the state-of-the-art methods in this research area, but to investigate the predictive power of the orthographic and phonetic differences between Romanian and its dialects. We use a methodology that has been previously used for discriminating between related and unrelated words, and for distinguishing the type of relationship between the words (Ciobanu and Dinu, 2014b; Ciobanu and Dinu, 2015). We align the words using the Needleman-Wunsch alignment algorithm (Needleman and Wunsch, 1970) and we extract n-gram features from the alignment of the words. Additionally, we also extract n-grams of characters from the dialect-word. We search for the optimum n-gram size in {1, 2, 3, 4}, both for the n-grams extracted from the alignment and for the n-grams extracted from the dialect-word. We train a Logistic Regression classifier, using the imple3283 Dialect Word pair Alignment ME ros¸u - ros¸ r r o o s¸ s¸ u - AR ros¸u - ar´os¸uˇ a r r o o´ s¸ s¸ u uˇ IS ros¸u - r´o¸isu r r o o´ ¸i s¸ s u u"
L16-1522,W14-0616,0,0.0303962,"ld be successfully parsed using features learnt from a Japanese training corpus. In the field of machine translation, Aminian et al. (2014) developed a method for translating from dialectal Arabic into English in which they reduce the OOV ratio by exploiting resources from standard Arabic. Although dialects and varieties have been investigated for other languages, such as Spanish and Portuguese (Zampieri and Gebre, 2012; Zampieri et al., 2013), Romanian dialects did not receive much attention in NLP. To our knowledge, while the syllabic structure of Aromanian has been previously investigated (Nisioi, 2014), this is one of the very first computational comparative studies on the Romanian dialects. 2. The Romanian Dialects Romanian is a Romance language, belonging to the Italic branch of the Indo-European language family, and is of particular interest regarding its geographic setting. It is surrounded by Slavic languages and its relationship with the big Romance kernel was difficult. According to Tagliavini (1972), Romanian has been isolated for a long period from the Latin culture in an environment of different languages. Joseph (1999) emphasizes the reasons which make Romanian of special interes"
L16-1522,tonelli-etal-2010-venpro,0,0.0309367,"we conduct an initial study on the dialects of Romanian. This investigation has the purpose of providing a deeper understanding of the differences between dialects, which would aid the adaptation of existing NLP tools for related varieties. The aim of our investigation is to assess the orthographic and phonetic differences between the dialects of Romanian. In this paper, we quantify only the orthographic and phonetic differences, but the morphology and the syntax are other important aspects which contribute to the individualization of each variety, that we leave for further study. Previously, Tonelli et al. (2010) proposed such an adaptation for a morphological analyzer for Venetan. Similarly, Kanayama et al. (2014) built a dependency parser for Korean, leveraging resources (transfer learning) from Japanese. They showed that Korean sentences could be successfully parsed using features learnt from a Japanese training corpus. In the field of machine translation, Aminian et al. (2014) developed a method for translating from dialectal Arabic into English in which they reduce the OOV ratio by exploiting resources from standard Arabic. Although dialects and varieties have been investigated for other language"
L16-1522,W14-4202,0,\N,Missing
L16-1536,P05-1045,0,0.0265835,"hat this technique leads to quantitative improvements over the machine translated entities and it can be used to enhance the quality of the French named entity recognition system. 2. Dataset We used the French-English set from the Europarl parallel corpus ((Koehn, 2005), (Koehn, 2012)), which was adapted for the 2015 Workshop of Machine Translation (Bojar et al., 2015). The set contains proceedings of the European Parliament (EP), from 1996 to 2011, aligned at the sentence level. Because Europarl does not have gold standard annotated entities, we used the CoreNLP named entity recognizer 3362 (Finkel et al., 2005) to extract locations and organizations. The choice of NE types was due to the domain the dataset belongs to, which most often will contain these two types and will have the person type, for instance, shared between source and target language. Regarding error rates with our NE acquisition strategy, we are aware that the entities discovered this way can also contain erroneous information, yet this is the only option and as such a typical first step in NE projection when manually annotated data lacks in both source and target language. By this approach we attempt to bring into discussion the ext"
L16-1536,W15-1830,0,0.0343544,"Missing"
L16-1536,2005.mtsummit-papers.11,0,0.0139471,"translation pairs. We focus on two types of entities - locations and organizations. We consider a parallel English-French corpus based on Europarl (Bojar et al., 2015) to train and evaluate our method. In Section 4. we present our results against a machine translation system and against a named entity recognizer trained on French. We show that this technique leads to quantitative improvements over the machine translated entities and it can be used to enhance the quality of the French named entity recognition system. 2. Dataset We used the French-English set from the Europarl parallel corpus ((Koehn, 2005), (Koehn, 2012)), which was adapted for the 2015 Workshop of Machine Translation (Bojar et al., 2015). The set contains proceedings of the European Parliament (EP), from 1996 to 2011, aligned at the sentence level. Because Europarl does not have gold standard annotated entities, we used the CoreNLP named entity recognizer 3362 (Finkel et al., 2005) to extract locations and organizations. The choice of NE types was due to the domain the dataset belongs to, which most often will contain these two types and will have the person type, for instance, shared between source and target language. Regard"
L16-1536,2011.iwslt-papers.3,0,0.0484865,"Missing"
L16-1536,N13-1046,0,0.0405709,"Missing"
L16-1536,M98-1002,0,0.081513,"Missing"
L16-1536,E03-1035,0,0.117854,"Missing"
L16-1536,W14-1609,0,0.0261079,"strophe and the upper case letters. The upper case was maintained so that the embeddings model made a better distinction between a NE and its common noun counterpart such as house vs House, where the latter refers to the people assembled in the European Parliament. The apostrophe was maintained to keep the French articles as part of the words. Once the merged corpus was obtained and preprocessed, we ran the gensim Phrases model4 , implemented after (Mikolov et al., 2013c), to extract from it word level bigrams, trigrams, and 4-grams. This was done in order to check whether the conclusions in (Passos et al., 2014) related to the usefulness in NER of embeddings trained over phrases instead of words applied to our corpora. The window size w of the training algorithm was decided by the following formula: w=x ¯ + 2σ(x) Table 1: Statistics on the English and French parallel corpus. In Table 1, we render basic statistics of the French-English Europarl corpus2 . Two important observations arise here. First, the number of types (unique words) within the French corpus is considerably smaller than the English equivalent, but, at the same time, there are 9 million more tokens in the French corpus. This fact is an"
L16-1536,C04-1089,0,0.12474,"Missing"
L16-1536,P15-2064,0,0.0668042,"Missing"
L16-1536,D13-1141,0,0.148804,"m, liviu.p.dinu@gmail.com Abstract In this paper we investigate the usefulness of neural word embeddings in the process of translating Named Entities (NEs) from a resource-rich language to a language low on resources relevant to the task at hand, introducing a novel, yet simple way of obtaining bilingual word vectors. Inspired by observations in (Mikolov et al., 2013b), which show that training their word vector model on comparable corpora yields comparable vector space representations of those corpora, reducing the problem of translating words to finding a rotation matrix, and by results in (Zou et al., 2013), which showed that bilingual word embeddings can improve Chinese Named Entity Recognition (NER) and English to Chinese phrase translation, we use the sentence-aligned English-French EuroParl corpora and show that word embeddings extracted from a merged corpus (corpus resulted from the merger of the two aligned corpora) can be used to NE translation. We extrapolate that word embeddings trained on merged parallel corpora are useful in Named Entity Recognition and Translation tasks for resource-poor languages. Keywords: Named Entity Translation, Corpus Aquisition, Word Embeddings 1. Introduction"
L16-1664,W11-2838,0,0.0301888,"om the field of translation studies (Baker, 1996). In particular, these corpora have been extensively used for investigation of transfer of the source language into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al."
L16-1664,2005.mtsummit-papers.11,0,0.233408,"far been studied in isolation, both in the linguistic literature and in terms of computational investigations. In this paper we describe a corpus constructed from original English utterances (where we differentiate between native and non-native speakers based on their country of origin) and English translations from a variety of European languages. This corpus will be instrumental for research that aims to uniformly address both second language acquisition (more specifically, the language of highly fluent non-native speakers) and human translation. The corpus we describe is based on Europarl (Koehn, 2005), and is the first corpus that allows a uniform comparative study of both phenomena (translation and language acquisition). The texts contained in the corpus are uniform in terms of style, respecting the European Parliament’s formal standards. In addition, the original English utterances are accurately annotated with speakers’ data, including knowledge about the speakers’ native language1 . Corpora of original and translated language are essential for empirical investigation of theoretically-motivated hypotheses from the field of translation studies (Baker, 1996). In particular, these corpora"
L16-1664,P11-1132,0,0.440276,"ntained in the corpus are uniform in terms of style, respecting the European Parliament’s formal standards. In addition, the original English utterances are accurately annotated with speakers’ data, including knowledge about the speakers’ native language1 . Corpora of original and translated language are essential for empirical investigation of theoretically-motivated hypotheses from the field of translation studies (Baker, 1996). In particular, these corpora have been extensively used for investigation of transfer of the source language into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts"
L16-1664,J12-4004,1,0.894042,"a of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and language models for machine translation (Lembersky et al., 2012; Lembersky et al., 2013). The corpus that we describe will facilitate computational research into the similarities and differences between the two types of language contact (second language learning and translation), hopefully leading to better solutions for the related computational tasks. 2. Corpus pre-processing and annotation of the translation direction The Europarl corpus is extracted from the collection of the proceedings of the European Parliament (Koehn, 2005), dating back to 1996. The transcriptions are produced as follows: (1) the utterances of the speakers are transcribed; (2) the"
L16-1664,J13-4007,1,0.89355,"ve been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and language models for machine translation (Lembersky et al., 2012; Lembersky et al., 2013). The corpus that we describe will facilitate computational research into the similarities and differences between the two types of language contact (second language learning and translation), hopefully leading to better solutions for the related computational tasks. 2. Corpus pre-processing and annotation of the translation direction The Europarl corpus is extracted from the collection of the proceedings of the European Parliament (Koehn, 2005), dating back to 1996. The transcriptions are produced as follows: (1) the utterances of the speakers are transcribed; (2) the transcriptions are sent"
L16-1664,R11-1091,0,0.189924,"). The texts contained in the corpus are uniform in terms of style, respecting the European Parliament’s formal standards. In addition, the original English utterances are accurately annotated with speakers’ data, including knowledge about the speakers’ native language1 . Corpora of original and translated language are essential for empirical investigation of theoretically-motivated hypotheses from the field of translation studies (Baker, 1996). In particular, these corpora have been extensively used for investigation of transfer of the source language into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, cor"
L16-1664,Q15-1030,1,0.84297,"plication is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and language models for machine translation (Lembersky et al., 2012; Lembersky et al., 2013). The corpus that we describe will facilitate computational research into the similarities and differences between the two types of language contact (second language learning and translation), hopefully leading to better solutions for the related computational tasks. 2. Corpus pre-processing and annotation of the translation direction The Europarl corpus is extracted from the collection of the proceedings of the European P"
L16-1664,W13-1706,0,0.0405858,"into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and langua"
L16-1664,C08-1118,0,0.338309,"Missing"
P14-2017,P10-1105,0,0.234132,"ment are mathematically equivalent ways of describing relationships between strings. Therefore, because the edit distance was widely used in this research area and produced good results, we are encouraged to employ orthographic alignment for identifying pairs of cognates, not only to compute similarity scores, as was previously done, but to use aligned subsequences as features for machine learning algorithms. Our intuition is that inferring language-specific rules for aligning words will lead to better performance in the task of cognate identification. gated and compared (Inkpen et al., 2005; Hall and Klein, 2010); Levenshtein distance (Levenshtein, 1965), XDice (Brew and McKelvie, 1996) and the longest common subsequence ratio (Melamed, 1995) are among the most frequently used metrics in this field. Gomes and Lopes (2011) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words. Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch, 1970; Smith and Wate"
P14-2017,I13-1112,0,0.082553,"d the transformation rules they follow have been successfully employed in various approaches to cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011). These orthographic changes have also been used in cognate production, which is closely related to the task of cognate detection, but has not yet been as intensively studied. While the purpose of cognate detection is to determine whether two given words form a cognate pair, the aim of cognate production is, given a word in a source language, to automatically produce its cognate pair in a target language. Beinborn et al. (2013) proposed a method for cognate production relying on statistical character-based machine translation, learning orthographic production patterns, and Mulloni (2007) introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language. 3 3.1 Orthographic Alignment String alignment is closely related to the task of sequence alignment in computational biology. Therefore, to align pairs of words we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), which is mainly used for aligni"
P14-2017,I11-1097,0,0.0949858,"Missing"
P14-2017,P93-1001,0,0.328145,", cross-lingual information retrieval (Buckley et al., 1997) and machine translation (Kondrak et al., 2003), the condition of common etymology is usually not essential and cognates are regarded as words with high cross-lingual meaning and orthographic or phonetic similarity. The wide range of applications in which cognates prove useful attracted more and more at2 Related Work There are three important aspects widely investigated in the task of cognate identification: semantic, phonetic and orthographic similarity. They were employed both individually (Simard et al., 1992; Inkpen et al., 2005; Church, 1993) and combined (Kondrak, 2004; Steiner et al., 2011) in order to detect pairs of cognates across languages. For determining semantic similarity, external lexical resources, such as WordNet (Fellbaum, 1998), or large corpora, might be necessary. For measuring phonetic and orthographic proximity of cognate candidates, string similarity metrics can be applied, using the phonetic or orthographic word forms as input. Various measures were investi99 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 99–105, c Baltimore, Maryland, USA, June 23"
P14-2017,dinu-ciobanu-2014-building,1,0.81013,"y for insertions and deletions; 4 ii) n-grams around any type of mismatch, i.e., we account for all three types of mismatches. 4.1 ex&gt;es xh&gt;sv-&gt;vo xh&gt;sha&gt;-a -$&gt;o$ For identical features we account only once. Therefore, because there is one feature (xh&gt;s-) which occurs twice in our example, we have 8 features for the pair (exhaustiv, esaustivo). 3.3 Data We apply our method on an automatically extracted dataset of cognates for four pairs of languages: Romanian-French, Romanian-Italian, Romanian-Spanish and Romanian-Portuguese. In order to build the dataset, we apply the methodology proposed by Ciobanu and Dinu (2014) on the DexOnline1 machine-readable dictionary for Romanian. We discard pairs of words for which the forms across languages are identical (i.e., the Romanian word matrice and its Italian cognate pair matrice, having the same form), because these pairs do not provide any orthographic changes to be learned. For each pair of languages we determine a number of non-cognate pairs equal to the number of cognate pairs. Finally, we obtain 445 pairs of cognates for Romanian-French2 , 3,477 for Romanian-Italian, 5,113 for RomanianSpanish and 7,858 for Romanian-Portuguese. Because we need sets of approxim"
P14-2017,N03-2016,0,0.0283523,"tion Cognates are words in different languages having the same etymology and a common ancestor. Investigating pairs of cognates is very useful in historical and comparative linguistics, in the study of language relatedness (Ng et al., 2010), phylogenetic inference (Atkinson et al., 2005) and in identifying how and to what extent languages change over time. In other several research areas, such as language acquisition, bilingual word recognition (Dijkstra et al., 2012), corpus linguistics (Simard et al., 1992), cross-lingual information retrieval (Buckley et al., 1997) and machine translation (Kondrak et al., 2003), the condition of common etymology is usually not essential and cognates are regarded as words with high cross-lingual meaning and orthographic or phonetic similarity. The wide range of applications in which cognates prove useful attracted more and more at2 Related Work There are three important aspects widely investigated in the task of cognate identification: semantic, phonetic and orthographic similarity. They were employed both individually (Simard et al., 1992; Inkpen et al., 2005; Church, 1993) and combined (Kondrak, 2004; Steiner et al., 2011) in order to detect pairs of cognates acros"
P14-2017,A00-2038,0,0.0635231,"st common subsequence ratio (Melamed, 1995) are among the most frequently used metrics in this field. Gomes and Lopes (2011) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words. Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch, 1970; Smith and Waterman, 1981; Gotoh, 1982) to obtain orthographic alignment scores for cognate candidates. Kondrak (2000) developed the ALINE system, which aligns words’ phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully employed in various approaches to cognate detection (Koehn a"
P14-2017,W12-0216,0,0.0312771,"similarity of cognate pairs which tolerates learned transitions between words. Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch, 1970; Smith and Waterman, 1981; Gotoh, 1982) to obtain orthographic alignment scores for cognate candidates. Kondrak (2000) developed the ALINE system, which aligns words’ phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully employed in various approaches to cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011). These orthographic changes have also been used in cognate production, which is closely related to the task"
P14-2017,W95-0115,0,0.100926,"n this research area and produced good results, we are encouraged to employ orthographic alignment for identifying pairs of cognates, not only to compute similarity scores, as was previously done, but to use aligned subsequences as features for machine learning algorithms. Our intuition is that inferring language-specific rules for aligning words will lead to better performance in the task of cognate identification. gated and compared (Inkpen et al., 2005; Hall and Klein, 2010); Levenshtein distance (Levenshtein, 1965), XDice (Brew and McKelvie, 1996) and the longest common subsequence ratio (Melamed, 1995) are among the most frequently used metrics in this field. Gomes and Lopes (2011) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words. Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch, 1970; Smith and Waterman, 1981; Gotoh, 1982) to obtain orthographic alignment scores for cognate candidates. Kondrak (2000) developed the ALINE system,"
P14-2017,mulloni-pekar-2006-automatic,0,0.502549,"INE system, which aligns words’ phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully employed in various approaches to cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011). These orthographic changes have also been used in cognate production, which is closely related to the task of cognate detection, but has not yet been as intensively studied. While the purpose of cognate detection is to determine whether two given words form a cognate pair, the aim of cognate production is, given a word in a source language, to automatically produce its cognate pair in a target language. Beinborn et al. (2013) proposed a method for cognate production relying on statistical character-based machine translation, learning orthographic production patte"
P14-2017,P07-3005,0,0.583722,"and Todirascu, 2011). These orthographic changes have also been used in cognate production, which is closely related to the task of cognate detection, but has not yet been as intensively studied. While the purpose of cognate detection is to determine whether two given words form a cognate pair, the aim of cognate production is, given a word in a source language, to automatically produce its cognate pair in a target language. Beinborn et al. (2013) proposed a method for cognate production relying on statistical character-based machine translation, learning orthographic production patterns, and Mulloni (2007) introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language. 3 3.1 Orthographic Alignment String alignment is closely related to the task of sequence alignment in computational biology. Therefore, to align pairs of words we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), which is mainly used for aligning sequences of proteins or nucleotides. Global sequence alignment aims at determining the best alignment over the entire length of the input sequences. The algori"
P14-2017,R11-1034,0,0.167506,"words’ phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology. The changes undergone by words when entering from one language into another and the transformation rules they follow have been successfully employed in various approaches to cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011). These orthographic changes have also been used in cognate production, which is closely related to the task of cognate detection, but has not yet been as intensively studied. While the purpose of cognate detection is to determine whether two given words form a cognate pair, the aim of cognate production is, given a word in a source language, to automatically produce its cognate pair in a target language. Beinborn et al. (2013) proposed a method for cognate production relying on statistical character-based machine translation, learning orthographic production patterns, and Mulloni (2007) intro"
P14-2017,1992.tmi-1.7,0,0.723114,"However, it can be customized to integrate historical information regarding language evolution. 1 Introduction Cognates are words in different languages having the same etymology and a common ancestor. Investigating pairs of cognates is very useful in historical and comparative linguistics, in the study of language relatedness (Ng et al., 2010), phylogenetic inference (Atkinson et al., 2005) and in identifying how and to what extent languages change over time. In other several research areas, such as language acquisition, bilingual word recognition (Dijkstra et al., 2012), corpus linguistics (Simard et al., 1992), cross-lingual information retrieval (Buckley et al., 1997) and machine translation (Kondrak et al., 2003), the condition of common etymology is usually not essential and cognates are regarded as words with high cross-lingual meaning and orthographic or phonetic similarity. The wide range of applications in which cognates prove useful attracted more and more at2 Related Work There are three important aspects widely investigated in the task of cognate identification: semantic, phonetic and orthographic similarity. They were employed both individually (Simard et al., 1992; Inkpen et al., 2005;"
P15-2071,P10-1105,0,0.13756,"n et al., 2005) and in identifying how and to what extent languages changed over time or influenced each other. Most studies in this area focus on automatically identifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically"
P15-2071,barbu-2008-romanian,0,0.0477624,"Missing"
P15-2071,I13-1112,0,0.110562,"ty of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences in cognates (Kondrak, 2002). In turn, sound corre"
P15-2071,D14-1112,1,0.889561,"tion) and its Spanish cognate pair funci´on are aligned as follows: $ f u n c ¸ t i e - $ $ f u n c - i ´ o n $ Our Approach In light of this, we investigate whether cognates can be automatically distinguished from borrowings based on their orthography. More specifically, our task is as follows: given a pair of words in two different languages (x, y), we want to determine whether x and y are cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating between related and unrelated words (Ciobanu and Dinu, 2014b). Briefly, the method comprises the following steps: The features for n = 2 are: $f$f, fufu, unun, ncnc, ct ¸c-, ¸i-i, iei´ t o, e-´ on, -$n$. For the prediction task, we experiment with two models, Naive Bayes and Support Vector Machines. We extend the method by introducing additional linguistic features and we conduct an analysis on their predictive power. 4 Experiments and Results In this section we present and analyze the experiments we run for discriminating between cognates and borrowings. 1) Aligning the pairs of related words using a string alignment algorithm; 2) Extracting"
P15-2071,P14-2017,1,0.891167,"tion) and its Spanish cognate pair funci´on are aligned as follows: $ f u n c ¸ t i e - $ $ f u n c - i ´ o n $ Our Approach In light of this, we investigate whether cognates can be automatically distinguished from borrowings based on their orthography. More specifically, our task is as follows: given a pair of words in two different languages (x, y), we want to determine whether x and y are cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating between related and unrelated words (Ciobanu and Dinu, 2014b). Briefly, the method comprises the following steps: The features for n = 2 are: $f$f, fufu, unun, ncnc, ct ¸c-, ¸i-i, iei´ t o, e-´ on, -$n$. For the prediction task, we experiment with two models, Naive Bayes and Support Vector Machines. We extend the method by introducing additional linguistic features and we conduct an analysis on their predictive power. 4 Experiments and Results In this section we present and analyze the experiments we run for discriminating between cognates and borrowings. 1) Aligning the pairs of related words using a string alignment algorithm; 2) Extracting"
P15-2071,W04-3250,0,0.230378,"Missing"
P15-2071,dinu-ciobanu-2014-building,1,0.866327,"tion) and its Spanish cognate pair funci´on are aligned as follows: $ f u n c ¸ t i e - $ $ f u n c - i ´ o n $ Our Approach In light of this, we investigate whether cognates can be automatically distinguished from borrowings based on their orthography. More specifically, our task is as follows: given a pair of words in two different languages (x, y), we want to determine whether x and y are cognates or if y is borrowed from x (in other words, x is the etymon of y). Our starting point is a methodology that has previously proven successful in discriminating between related and unrelated words (Ciobanu and Dinu, 2014b). Briefly, the method comprises the following steps: The features for n = 2 are: $f$f, fufu, unun, ncnc, ct ¸c-, ¸i-i, iei´ t o, e-´ on, -$n$. For the prediction task, we experiment with two models, Naive Bayes and Support Vector Machines. We extend the method by introducing additional linguistic features and we conduct an analysis on their predictive power. 4 Experiments and Results In this section we present and analyze the experiments we run for discriminating between cognates and borrowings. 1) Aligning the pairs of related words using a string alignment algorithm; 2) Extracting"
P15-2071,N03-2016,0,0.163051,"rest alina.ciobanu@my.fmi.unibuc.ro,ldinu@fmi.unibuc.ro Abstract the vocabulary of the recipient language but was adopted from some other language and made part of the borrowing language’s vocabulary”. The notion of cognate is much more relaxed, and various NLP tasks and applications use different definitions of the cognate pairs. In some situations, cognates and borrowings are considered together, and are referred to as historically connected words (Kessler, 2001) or denoted by the term correlates (Heggarty, 2012; McMahon et al., 2005). In some tasks, such as statistical machine translation (Kondrak et al., 2003) and sentence alignment, or when studying the similarity or intelligibility of the languages, cognates are seen as words that have similar spelling and meaning, their etymology being completely disregarded. However, in problems of language classification, distinguishing cognates from borrowings is essential. Here, we account for the etymology of the words, and we adopt the following definition: two words form a cognate pair if they share a common ancestor and have the same meaning. In other words, they derive directly from the same word, have a similar meaning and, due to various (possibly lan"
P15-2071,mulloni-pekar-2006-automatic,0,0.643185,"dentifying pairs of cognates. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate reg"
P15-2071,P07-3005,0,0.555133,"dates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences in cognates (Kondrak, 2002). In turn, sound correspondences are r"
P15-2071,R11-1034,0,0.058135,"tes. For measuring the orthographic or phonetic proximity of the cognate candidates, string similarity metrics (Inkpen 431 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 431–437, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics et al., 2005; Hall and Klein, 2010) and algorithms for string alignment (Delmestri and Cristianini, 2010) have been applied, both in cognate detection (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011) and in cognate production (Beinborn et al., 2013; Mulloni, 2007). Minett and Wang (2003) focus on identifying borrowings within a family of genetically related languages and propose, to this end, a distance-based and a character-based technique. Minett and Wang (2005) address the problem of identifying language contact, building on the idea that borrowings bias the lexical similarities among genetically related languages. According to the regularity principle, the distinction between cognates and borrowings benefits from the regular sound changes that generate regular phoneme correspondences"
P15-2071,J01-4008,0,\N,Missing
P17-2014,P15-2011,1,0.177095,"Missing"
P17-2014,P82-1020,0,0.855934,"Missing"
P17-2014,N15-1022,0,0.379655,"nd that the quality of simplifications in Simple English Wikipedia has been disputed before (Amancio and Specia, 2014; Xu et al., 2015), for tuning and testing we use the dataset previously released by Xu et al. (2016), which contains 2000 sentences for tuning and 359 for testing, each with eight simplification variants obtained by eight Amazon Mechanical Turkers.3 The tune subset is also used as reference corpus in combination with BLEU and SARI to select the best beam size and hypothesis for prediction reranking. Dataset To train our models, we use the publicly available dataset provided by Hwang et al. (2015) based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia (EW–SEW). We discard the uncategorized matches, and use only good matches and partial matches which were above the 0.45 threshold (Hwang et al., 2015), totaling to 280K aligned sentences (around 150K full matches and 130K partial matches). It is one of the largest freely available resources for text simplification, and unlike the previously used EW–SEW corpus2 (Kauchak, 2013), which only contains full matches (167K pairs), the newer dataset also contains partial matches. Therefore, it is n"
P17-2014,P13-1151,0,0.6984,"approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments. Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words and entities. Instead, we make use of alignment probabilities between the predictions and the original sentences to retrieve the original words. yt 2.1 Furthermore, we are interested to explore whether large scale pre-trained embeddings can improve text simplification models. Kauchak (2013) indicates that combining normal data with simplified data can increase the performance of ATS systems. Therefore, we construct a secondary model (NTSw2v) using a combination of pre-trained word2vec from Google News corpus (Mikolov et al., 2013a) of size 300 and locally trained embeddings of size 200. To ensure good representations of lowˇ uˇrek and frequency words, we use word2vec (Reh˚ Sojka, 2010; Mikolov et al., 2013b) to train skipgram with hierarchical softmax and we set a window of 10 words. Following Garten et al. (2015) who showed that simple concatenation can improve the word represe"
P17-2014,P17-4012,0,0.0225325,"(Bojar et al., 2016). Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables. The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016). Automated text simplification (ATS) systems are meant to transform original texts into differ∗ Liviu P. Dinu1 2 Neural Text Simplification (NTS) We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmidhuber, 1997), hidden states of size 500 and 500 hidden units, and a 0.3 dropout probability (Srivastava et al., 2014). The vocabulary size is set to 50,000 and we train the model for 15 epochs with plain SGD optimizer, and after epoch 8 we halve the Both authors have contributed equally to this work 85 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 85–91 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https:"
P17-2014,J10-4005,0,0.0113865,"troduction Neural sequence to sequence models have been successfully used in many applications (Graves, 2012), from speech and signal processing to text processing or dialogue systems (Serban et al., 2015). Neural machine translation (Cho et al., 2014; Bahdanau et al., 2014) is a particular type of sequence to sequence model that recently attracted a lot of attention from industry (Wu et al., 2016) and academia, especially due to the capability to obtain state-of-the-art results for various translation tasks (Bojar et al., 2016). Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables. The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016). Automated text simplification (ATS) systems are meant to transform original texts into differ∗ Liviu P. Dinu1 2 Neural Text Simplification (NTS) We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmid"
P17-2014,P15-2135,1,0.839028,"nter, University of Bucharest, Romania 2 Data and Web Science Group, University of Mannheim, Germany 3 Oracle Corporation, Romania {sergiu.nisioi,ldinu}@fmi.unibuc.ro {sanja,simone}@informatik.uni-mannheim.de Abstract ent (simpler) variants which would be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the appl"
P17-2014,P16-1100,0,0.0236322,"Missing"
P17-2014,D15-1166,0,0.0238032,"ges 85–91 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2014 learning rate. At the end of each epoch we save the current state of the model and predict the perplexity values of the models on the development set. We employ early-stopping and select the model resulted from the epoch with the best perplexity to avoid over-fitting. The parameters are initialized over uniform distribution with support [-0.1, 0.1]. Additionally, for the decoder we employ global attention in combination with input feeding as described by Luong et al. (2015). The architecture1 is depicted in Figure 1, with the input feeding approach represented only for the last hidden state of the decoder. method, to the input at the next step, presumably making the model keep track of anterior alignment decisions. Luong et al. (2015) showed this approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments. Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words"
P17-2014,1983.tc-1.13,0,0.485554,"Missing"
P17-2014,P12-1107,0,0.670247,"Missing"
P17-2014,W13-4813,0,0.0618928,"d be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare ou"
P17-2014,Q16-1029,0,0.720017,"In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervi"
P17-2014,W16-4912,0,0.266157,"tempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervised lexical simplification systems. We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS sy"
P17-2014,P02-1040,0,0.118354,"tSimplification 86 described previously (NTS and NTS-w2v). Beam search works by generating the first k hypotheses at each step ordered by the log-likelihood of the target sentence given the input sentence. By default, we use a beam size of 5 and take the first hypothesis, but we also observe that higher beam size and lower-ranked hypotheses can generate good simplification results. Therefore, we generate the first two candidate hypotheses for each beam size from 5 to 12. We then attempt to find the best beam size and hypothesis based on two metrics: the traditional MT-evaluation metric, BLEU (Papineni et al., 2002; Bird et al., 2009) with NIST smoothing (Bird et al., 2009), and SARI (Xu et al., 2016), a recent text-simplification metric. 2.3 in the corpus. A brief analysis of the vocabulary is rendered in Table 1. The dataset we use contains an abundant amount of named entities and consequently a large amount of low frequency words, but the majority of entities are not part of the model’s 50,000 words vocabulary due to their small frequency. These words are replaced with ’UNK’ symbols during training. At prediction time, we replace the unknown words with the highest probability score from the attention"
P17-2014,C10-1152,0,0.729324,"ariants which would be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evalu"
P17-2014,N13-1092,0,\N,Missing
P17-2014,W16-2301,0,\N,Missing
P17-2014,P05-1045,0,\N,Missing
R13-1018,P12-2051,0,0.129914,"g of texts can help reduce the temporal effects in this regard. Chambers (2012) states that applying timestamps to documents is, to some extent, similar to topic classification, focusing on choosing a time period instead of a topic, but also relating to temporal words and phrases which describe the time period to be determined and are often comprised in the investigated documents. Therefore, he argues for the inclusion of these temporal expressions into the learning system for automatic document dating and proposes such a model which obtains better results than previous generative models. In (Mihalcea and Nastase, 2012) the authors introduced the task of identifying changes in word usage over time, disambiguating the epoch at word-level. Recently, Stajner and Zampieri (2013) used stylistic features, such as lexical richness, to predict the century of historical Portuguese texts. manian orthography are noticed. In this paper we approach an interesting version of the epoch disambiguation task, successfully disambiguating the century in which Romanian novels with the action set in the past and written so as to simulate the action’s epoch appear have been written in. We used novels of Romanian writers Mihail Sad"
R13-1018,P12-1011,0,0.188176,"altirea Hurmuzachi, for which the dating is disputed between the 15th century (idea promoted by historians such as Nicolae Iorga) and the end of the 136 Proceedings of Recent Advances in Natural Language Processing, pages 136–140, Hissar, Bulgaria, 7-13 September 2013. uments, which may be difficult due to the different historical and modern variants of the text, the less standardized spelling, words ambiguities and other language changes. Thus, the linking of current word forms with their historical equivalents and accurate dating of texts can help reduce the temporal effects in this regard. Chambers (2012) states that applying timestamps to documents is, to some extent, similar to topic classification, focusing on choosing a time period instead of a topic, but also relating to temporal words and phrases which describe the time period to be determined and are often comprised in the investigated documents. Therefore, he argues for the inclusion of these temporal expressions into the learning system for automatic document dating and proposes such a model which obtains better results than previous generative models. In (Mihalcea and Nastase, 2012) the authors introduced the task of identifying chan"
R13-1018,W06-0903,0,0.249016,"lenge in building text classification models may be the change which occurs in the characteristics of the documents and their classes over time (Mour˜ao et al., 2008). Therefore, in order to overcome the difficulties which arise in automatic classification when dealing with documents dating from different epochs, identifying and accounting for document characteristics changing over time (such as class frequency, relationships between terms and classes and the similarity among classes over time (Mour˜ao et al., 2008)) is essential and can lead to a more accurate discrimination between classes. Dalli and Wilks (2006) successfully apply a method for classification of texts and documents based on their predicted time of creation, proving that accounting for word frequencies and their variation over time is accurate. Kumar et al. (2012) argue as well for the capability of this method, of using words alone, to determine the epoch in which a text was written or the time period a document refers to. The effectiveness of using models for individual partitions in a timeline with the purpose of predicting probabilities over the timeline for new documents is investigated in (Kumar et al., 2011; Kanhabua and Nørv˚ag"
R13-1019,N03-2016,0,0.161817,"tion is widely used in historical and comparative linguistics, in the study of languages relatedness (Chin et al, 2010), phylogenetic inference (Atkinson et al, 2005) and in identifying how and to what extent languages changed over time. Besides these research areas, in which the genetic relationships between words are extremely relevant, cognates have been successfully used in other fields, such as language acquisition, bilingual word recognition (Dijkstra et al, 2012), corpus linguistics (Simard et al, 1992), cross-lingual information retrieval (Buckley et al, 1998) and machine translation (Knight et al, 2003). In these domains, the term “cognates” is usually used with a somewhat different meaning, denoting words with high orthographic/phonetic and cross-lingual meaning similarity, the condition of common etymology being left aside. Kondrak (2001) makes the distinction between the different interpretations of the notion and Inkpen et al (2005) present the definition of “genetic cognates”. In this paper we focus on genetic relationships between words and we use the term “cognates” 2 Our Approach We focus on the Romanian language and we investigate its cognate pairs with two other Romance languages,"
R13-1019,N01-1014,0,0.0563805,"hese research areas, in which the genetic relationships between words are extremely relevant, cognates have been successfully used in other fields, such as language acquisition, bilingual word recognition (Dijkstra et al, 2012), corpus linguistics (Simard et al, 1992), cross-lingual information retrieval (Buckley et al, 1998) and machine translation (Knight et al, 2003). In these domains, the term “cognates” is usually used with a somewhat different meaning, denoting words with high orthographic/phonetic and cross-lingual meaning similarity, the condition of common etymology being left aside. Kondrak (2001) makes the distinction between the different interpretations of the notion and Inkpen et al (2005) present the definition of “genetic cognates”. In this paper we focus on genetic relationships between words and we use the term “cognates” 2 Our Approach We focus on the Romanian language and we investigate its cognate pairs with two other Romance languages, French and Italian. We believe this comparison is interesting for the following reason: the two related languages differ significantly with respect to their orthographic depth: the mapping rules between graphemes and phonemes are more complex"
R13-1019,R09-1074,0,0.0699411,"Missing"
R13-1019,W95-0115,0,0.0440625,"tom up, i.e. from n to 1, in a Borda order. The elements which do not occur in one of the rankings receive the rank 0. To extend the rank distance to strings, we index each occurence of a given letter a with ak , where k is the number of its previous occurences, and then compute the rank distance for the new indexed strings which become in this situation rankings. In order to normalize it, we divide the obtained value by the maximum possible distance between two strings u and v, which is: |u|(|u |+ 1) |v|(|v |+ 1) + 2 2 E.g. ∆(langue, lingua) = 10 42 = 0.23 • Longest common subsequence ratio (Melamed, 1995) computes the similarity between two words dividing the length of the longest common subsequence of the two words by the length of the longer word: LCSR(wi , wj ) = • Levenshtein distance (Levenshtein, 1965), also named the edit distance, counts the minimum number of operations (insertion, deletion and substitution) required to transform one string into another. We use a normalized Levenshtein distance computed as: 5 LD(wi , wj ) max(|wi |, |wj |) LCS(wi , wj ) max(|wi |, |wj |) where LCS(wi , wj ) is the longest common subsequence of wi and wj . We subtract this value from 1, in order to obta"
R13-1019,W13-1735,0,0.0309528,"lanation is that starting with the 19th century numerous words were imported from French. That period represents a stage in the Romanian’s language evolution in which norms for the vocabulary of the literary language were defined, including patterns for adapting neologisms to Romanian, and probably many of the French words which entered the language in the 19th century are in this situation. 6 racy, obtaining better results for French than for Italian, with some improvements when diacritics are not accounted for. A possible application for cognates identification is native language detection (Popescu and Ionescu, 2013). We believe that accounting for genetic relationships between words could prove useful for this task. In our future work we intend to further investigate the performances of the orthographic approaches to the task of cognates identification by introducing an additional step of parameter tuning for the threshold value in our procedure. We plan to apply this method of identifying cognates on the entire dexonline dictionary. In this paper we focused on the cognates that are most frequently used in Romanian, but we believe that obtaining an almost exhaustive dataset of Romanian-French and Romania"
R13-1019,1992.tmi-1.7,0,0.548425,"in different languages having the same etymology and a common ancestor. The task of cognates identification is widely used in historical and comparative linguistics, in the study of languages relatedness (Chin et al, 2010), phylogenetic inference (Atkinson et al, 2005) and in identifying how and to what extent languages changed over time. Besides these research areas, in which the genetic relationships between words are extremely relevant, cognates have been successfully used in other fields, such as language acquisition, bilingual word recognition (Dijkstra et al, 2012), corpus linguistics (Simard et al, 1992), cross-lingual information retrieval (Buckley et al, 1998) and machine translation (Knight et al, 2003). In these domains, the term “cognates” is usually used with a somewhat different meaning, denoting words with high orthographic/phonetic and cross-lingual meaning similarity, the condition of common etymology being left aside. Kondrak (2001) makes the distinction between the different interpretations of the notion and Inkpen et al (2005) present the definition of “genetic cognates”. In this paper we focus on genetic relationships between words and we use the term “cognates” 2 Our Approach W"
R13-1028,kruengkrai-etal-2006-conditional,0,0.0730986,"Missing"
R13-1028,W02-1001,0,0.0826062,"ntheses in this paragraph correspond to columns of Table 4. For more appropriate comparison, we reproduced the word-level SVM results from our previous work (Dinu et al., 2011) but with a held-out test set of a quarter of the labelled data. The best parameters chosen for the linear SVM by 3-fold cross validation on the training set are n = 8, C = 0.15, tf-idf normalization, squared hinge loss and `2 regularization. The labelling used was the same as in the previous work, with the very small classes discarded, making the problem slightly simpler for the SVM. The structured averaged perceptron (Collins, 2002) is a simple, fast and effective iterative algorithm. It comes from the even simpler structured perceptron learning algorithm, where at each iteration, a data point (xi , yi ) is chosen and the model prediction yˆi is computed. If the prediction is wrong, the model parameters are updated in the direction of the current feature vector. The averaged perceptron approach takes, instead of the final value of the parameter vector θ, its average θ¯ over all the iterations. The passive aggressive (PA) algorithm (Crammer et al., 2006) is similar to the averaged perceptron: instead of updating when clas"
R13-1028,D10-1095,0,0.0214004,"takes, instead of the final value of the parameter vector θ, its average θ¯ over all the iterations. The passive aggressive (PA) algorithm (Crammer et al., 2006) is similar to the averaged perceptron: instead of updating when classification is incorrect, it updates when the margin of the misclassification is more than 1, i.e. when the multiclass structured hinge loss `t is positive. The update is aggressive in the sense that it forces the new parameter vector to correctly classify the input point with margin of at least 1. Finally, averaging is applied in the same fashion. The AROW algorithm (Mejer and Crammer, 2010) maintains normal distributions over the parameters of the model and updates their parameters in a way that generalizes PA. We used CRFsuite v0.12 (Okazaki, 2007) for implementation of the learning methods listed above. CRFsuite can expand the feature expansion implemented by us at character-level to a vector that optionally includes all possible states (ps), all possible transitions (pt), or both. These flags, along with the window length n that we have searched for in {2, 3, 4, 5, 6}, control the feature expansion f (x, y). Apart from this, each algorithm has its own hyperparameters. For ML,"
R13-1028,R11-1075,1,0.906509,"aradigm is difficult to learn due to lack of data. An example is that of the verb a putea (to be able to), whose stem vowel u transforms into o and oa, forming a singleton alternation pattern. However, the specific alternation o-oa appears in other patterns (dormi-doarme). • Class interaction: Word-level classes that include the same variable letters see each other’s instances as negative cases and cannot therefore benefit from what they share. By learning each variable letter separately, all occurrences are used as positive cases. 4 Paradigm overlap and variable letters 4.1 In previous work (Dinu et al., 2011; Dinu et al., 2012), we proposed a labelling system that was Approach Available data Our labelled data is generated from RoMorphoDict, an electronic morphological dictionary for Ro216 1sg 2sg 3sg 1pl 2pl 3pl T1 $ i$ a˘ $ a˘ m$ at, i$ a˘ $ T2 u$ i$ a˘ $ a˘ m$ at, i$ a˘ $ T5 ez$ ezi$ eaz˘a a˘ m$ at, i$ eaz˘a$ T6 ez$ ezi$ az˘a$ em$ at, i$ az˘a$ T10 $ i$ e$ im$ it, i$ $ T11 i$ i$ ie$ im$ it, i$ ie$ T12 esc$ es, ti$ es, te$ im$ it, i$ esc$ T13 iesc$ ies, ti$ ies, te$ im$ it, i$ iesc$ Table 3: A few of the main ending patterns manian. The resource is divided according to parts of speech. The subset"
R13-1028,C04-1067,0,0.0602337,"Missing"
R13-1028,E12-1053,1,0.895012,"Missing"
R13-1028,N13-1138,0,0.0397701,"Missing"
R13-1028,C12-3007,0,\N,Missing
R13-1070,P11-1132,0,0.18411,"Missing"
R13-1070,C08-2023,1,\N,Missing
R13-1070,W12-0411,1,\N,Missing
R13-1070,C12-3016,1,\N,Missing
R15-1014,W10-1001,0,0.0233252,"wide variety of applications. We mention here only a few: 1) they provide assistance in selecting reading material with an appropriate level of complexity from a large collection of documents, for second language learners and people with disabilities or low literacy skills (Collins-Thompson, 2011); 2) they help adapting the technical documents to various levels of medical expertise, within the medical domain (Elhadad and Sutaria, 2007); 3) they assist the processes of machine translation, text simplification, or speech recognition and evaluate their effectiveness, in the research area of NLP (Aluisio et al., 2010; Stymne et al., 2013). In this paper we investigate how readability varies between texts originally written in English and texts translated into English. For quantification, we analyze several factors that are relevant in assessing readability – shallow, lexical and morpho-syntactic features – and we employ the widely used Flesch-Kincaid formula to measure the variation of the readability level between original English texts and texts translated into English. Finally, we analyze whether the readability features have enough discriminative power to distinguish between originals and translations"
R15-1014,W14-1212,1,0.799779,"pproach The problem that we investigate in this paper is how the readability level varies across original and translated texts (from various source languages). We identify utterances from Europarl in a wide variety of languages, we identify their translations into English, and on these English translations we conduct a quantitative analysis of the readability features. As most research on readability focused on English so far, there are several formulas, features and tools available for quantifying the differences in the level of readability. In this paper we complement our previous analysis (Ciobanu and Dinu, 2014) on the readability features for the original texts and their translations. Here we focus on the target language, analyzing whether different source languages lead to differences in the readability level for the translated texts. 2.1 # speakers 62 292 226 151 99 539 22 15 175 691 30 41 89 67 33 35 48 378 75 389 166 # sentences 1,262 80,171 156,836 37,045 36,768 300,672 4,284 2,790 62,479 264,460 4,652 8,576 23,129 20,637 5,432 13,873 14,834 116,834 24,586 109,297 98,653 Table 1: Number of speakers and sentences for each language in our Europarl subset. Data linguistic, cultural and ideological"
R15-1014,2005.mtsummit-papers.11,0,0.0191067,"lexity of the process of translating political documents. Political texts might contain complex technical terms and elaborated sentences. Therefore, the results of our experiments are probably domain-specific and cannot be generalized to other types of text. Although parliamentary documents probably have a low readability level, our investigation is not negatively influenced by the choice of corpus because we are consistent across all experiments in terms of text gender and we report results obtained solely by comparison between source and target languages. We run our experiments on Europarl (Koehn, 2005), a multilingual parallel corpus extracted from the proceedings of the European Parliament. Its main intended use is as aid for statistical machine translation research (Tiedemann, 2012). The corpus is tokenized and aligned in 21 languages. In Table 1 we report statistics extracted from our dataset. Given the fact that the Flesch-Kincaid formula is based on the average number of words per sentence and on the average number of syllables per word, the differences between the languages (in terms of the number of speakers and sentences) do not affect the results. According to van Halteren (2008),"
R15-1014,P11-1132,0,0.0234994,"translated into English. For quantification, we analyze several factors that are relevant in assessing readability – shallow, lexical and morpho-syntactic features – and we employ the widely used Flesch-Kincaid formula to measure the variation of the readability level between original English texts and texts translated into English. Finally, we analyze whether the readability features have enough discriminative power to distinguish between originals and translations. 1 Introduction and Related Work The products of translation generally differ from original, non-translated texts. According to Koppel and Ordan (2011), two main aspects that lead to differences between the two categories have been identified: 1) effects of the translation process that are independent of the source language; 2) effects of the source language on the translation product, also known as source language interference. According to Sun (2012), the reception of a translated text is related to cross-cultural readability. Translators need to understand the particularities of both the source and the target language in order to transfer the meaning of the text from one language to another. While rendering the source language text into t"
R15-1014,W07-1007,0,0.0193376,"r languages, such as Spanish (Huerta, 1959), French (Kandel and Moles, 1958) or Italian (Franchina and Vacca, 1986; Franc¸ois and Miltsakaki, 2012). Readability assessment systems have a wide variety of applications. We mention here only a few: 1) they provide assistance in selecting reading material with an appropriate level of complexity from a large collection of documents, for second language learners and people with disabilities or low literacy skills (Collins-Thompson, 2011); 2) they help adapting the technical documents to various levels of medical expertise, within the medical domain (Elhadad and Sutaria, 2007); 3) they assist the processes of machine translation, text simplification, or speech recognition and evaluate their effectiveness, in the research area of NLP (Aluisio et al., 2010; Stymne et al., 2013). In this paper we investigate how readability varies between texts originally written in English and texts translated into English. For quantification, we analyze several factors that are relevant in assessing readability – shallow, lexical and morpho-syntactic features – and we employ the widely used Flesch-Kincaid formula to measure the variation of the readability level between original Eng"
R15-1014,P14-5010,0,0.00528571,"Missing"
R15-1014,C10-2032,0,0.0163483,"bs, nouns, pronouns, adjectives and adverbs), computed individually on a per-token basis4 . – Lexical density. The proportion of content words (verbs, nouns, adjectives and adverbs), computed on a per-token basis. Grammatical features were shown to be useful in readability prediction (Heilman et al., 2007). • Shallow Features – Average number of words per sentence. The average sentence length is one of the most widely used metrics for determining readability level and was employed in numerous readability formulas, proving to be most meaningful in combined evidence with average word frequency. Feng et al. (2010) find the average sentence length to have higher predictive power than the other lexical and syllable-based features they used. – Average number of characters (or syllables) per word. It is generally considered that frequently occurring words are usually short, so the average number of characters per word was broadly used for measuring readability in a robust manner. Many readability formulas measure word length in syllables rather than letters. 3.2.2 Results The optimal value for the logistic regression regularization parameter is found to be 1. We obtain 0.59 F-score on the test set, on aver"
R15-1014,W12-2207,0,0.297881,"Missing"
R15-1014,N07-1058,0,0.146556,"ary used in the text. 3.2.1 Features We use several shallow, lexical and morpho-syntactic features that were traditionally used for assessing readability and have proven high discriminative power within readability metrics: • Morpho-Syntactic Features – Relative frequency of POS unigrams. The ratio for 5 POS (verbs, nouns, pronouns, adjectives and adverbs), computed individually on a per-token basis4 . – Lexical density. The proportion of content words (verbs, nouns, adjectives and adverbs), computed on a per-token basis. Grammatical features were shown to be useful in readability prediction (Heilman et al., 2007). • Shallow Features – Average number of words per sentence. The average sentence length is one of the most widely used metrics for determining readability level and was employed in numerous readability formulas, proving to be most meaningful in combined evidence with average word frequency. Feng et al. (2010) find the average sentence length to have higher predictive power than the other lexical and syllable-based features they used. – Average number of characters (or syllables) per word. It is generally considered that frequently occurring words are usually short, so the average number of ch"
R15-1014,W13-5634,0,0.0808336,"ations. We mention here only a few: 1) they provide assistance in selecting reading material with an appropriate level of complexity from a large collection of documents, for second language learners and people with disabilities or low literacy skills (Collins-Thompson, 2011); 2) they help adapting the technical documents to various levels of medical expertise, within the medical domain (Elhadad and Sutaria, 2007); 3) they assist the processes of machine translation, text simplification, or speech recognition and evaluate their effectiveness, in the research area of NLP (Aluisio et al., 2010; Stymne et al., 2013). In this paper we investigate how readability varies between texts originally written in English and texts translated into English. For quantification, we analyze several factors that are relevant in assessing readability – shallow, lexical and morpho-syntactic features – and we employ the widely used Flesch-Kincaid formula to measure the variation of the readability level between original English texts and texts translated into English. Finally, we analyze whether the readability features have enough discriminative power to distinguish between originals and translations. 1 Introduction and R"
R15-1014,tiedemann-2012-parallel,0,0.0127711,"e probably domain-specific and cannot be generalized to other types of text. Although parliamentary documents probably have a low readability level, our investigation is not negatively influenced by the choice of corpus because we are consistent across all experiments in terms of text gender and we report results obtained solely by comparison between source and target languages. We run our experiments on Europarl (Koehn, 2005), a multilingual parallel corpus extracted from the proceedings of the European Parliament. Its main intended use is as aid for statistical machine translation research (Tiedemann, 2012). The corpus is tokenized and aligned in 21 languages. In Table 1 we report statistics extracted from our dataset. Given the fact that the Flesch-Kincaid formula is based on the average number of words per sentence and on the average number of syllables per word, the differences between the languages (in terms of the number of speakers and sentences) do not affect the results. According to van Halteren (2008), translations in the European Parliament are generally made by native speakers of the target language. Translation is an inherent part of the political activity (Sch¨affner and Bassnett,"
R15-1014,W04-3250,0,0.273256,"Missing"
R15-1014,C08-1118,0,0.0573076,"Missing"
R15-1021,J06-1003,0,0.0187553,"n article or a page with its synonyms within the domain covered by the keywords, one needs to take into consideration the synonym set of the translated keywords and the overlap of two languages synonym sets. 2 Related Works There are various NLP applications using synonyms, one of the most notable being automatic synonym detection or extraction (Wang and Hirst, 2011; Wang et al., 2010; Mohammad and Hirst, 2006; Bikel and Castelli, 2008), a. o., which in turn can help in tasks including machine translation, information retrieval, speech recognition, spelling correction, or text categorization (Budanitsky and Hirst, 2006). A multilingual approach based on word alignment of parallel corpora proved to have (Van der Plas et al., 2011) higher precision and recall scores 147 Proceedings of Recent Advances in Natural Language Processing, pages 147–152, Hissar, Bulgaria, Sep 7–9 2015. for the task of synonym extraction than the monolingual approach. Other work on semantic distance between words and concepts (Mohammad et al., 2007) emphasise on the advantages of multilingual over the monolingual treatment. 3 Data and Tools For Romanian language, we used a synonym dictionary (Dict, ionarul de sinonime al limbii Române,"
R15-1021,W06-1605,0,0.0336516,"oss-lingual synonym sets prove to be useful in tasks such as, for instance, automatic translation of web pages. Since search engines are using more of the Latent Semantic Indexing, which associates keywords of an article or a page with its synonyms within the domain covered by the keywords, one needs to take into consideration the synonym set of the translated keywords and the overlap of two languages synonym sets. 2 Related Works There are various NLP applications using synonyms, one of the most notable being automatic synonym detection or extraction (Wang and Hirst, 2011; Wang et al., 2010; Mohammad and Hirst, 2006; Bikel and Castelli, 2008), a. o., which in turn can help in tasks including machine translation, information retrieval, speech recognition, spelling correction, or text categorization (Budanitsky and Hirst, 2006). A multilingual approach based on word alignment of parallel corpora proved to have (Van der Plas et al., 2011) higher precision and recall scores 147 Proceedings of Recent Advances in Natural Language Processing, pages 147–152, Hissar, Bulgaria, Sep 7–9 2015. for the task of synonym extraction than the monolingual approach. Other work on semantic distance between words and concepts"
R15-1021,D07-1060,0,0.0235141,"Bikel and Castelli, 2008), a. o., which in turn can help in tasks including machine translation, information retrieval, speech recognition, spelling correction, or text categorization (Budanitsky and Hirst, 2006). A multilingual approach based on word alignment of parallel corpora proved to have (Van der Plas et al., 2011) higher precision and recall scores 147 Proceedings of Recent Advances in Natural Language Processing, pages 147–152, Hissar, Bulgaria, Sep 7–9 2015. for the task of synonym extraction than the monolingual approach. Other work on semantic distance between words and concepts (Mohammad et al., 2007) emphasise on the advantages of multilingual over the monolingual treatment. 3 Data and Tools For Romanian language, we used a synonym dictionary (Dict, ionarul de sinonime al limbii Române, by Luiza Seche and Mircea Seche), which contains about 45.000 words and 230.000 synonym pairs. For English language we employed Princeton’s WordNet, version 3.0, which contains about 150.000 words and 250.000 synonym pairs. For French language we used the synonyms dictionary developed by the CRISCO research centre, which contains almost 50.000 words and 400.000 synonyms relations. As a translation tool we"
R15-1021,P11-2052,0,0.0299099,"Missing"
R15-1021,D11-1093,0,0.0286554,"ind of concept lexicalization overlap. Cross-lingual synonym sets prove to be useful in tasks such as, for instance, automatic translation of web pages. Since search engines are using more of the Latent Semantic Indexing, which associates keywords of an article or a page with its synonyms within the domain covered by the keywords, one needs to take into consideration the synonym set of the translated keywords and the overlap of two languages synonym sets. 2 Related Works There are various NLP applications using synonyms, one of the most notable being automatic synonym detection or extraction (Wang and Hirst, 2011; Wang et al., 2010; Mohammad and Hirst, 2006; Bikel and Castelli, 2008), a. o., which in turn can help in tasks including machine translation, information retrieval, speech recognition, spelling correction, or text categorization (Budanitsky and Hirst, 2006). A multilingual approach based on word alignment of parallel corpora proved to have (Van der Plas et al., 2011) higher precision and recall scores 147 Proceedings of Recent Advances in Natural Language Processing, pages 147–152, Hissar, Bulgaria, Sep 7–9 2015. for the task of synonym extraction than the monolingual approach. Other work on"
R15-1021,P08-2037,0,\N,Missing
R19-1040,kazakov-etal-2017-machine,0,0.0142928,"en in bioinformatics. Irrelevance and inconsistency appear to be features which are dealt with quite sparsely, if ever, outside Longobardi’s school; actually, these flexible features might prove to be quite useful not only in linguistic classification phylogeny, cf. (Franzoi and Sgarro, 2017a,b), but also in the investigation of the history of texts. So far, we are just providing technical tools to be used in Longobardi’s research, which, in its turn, is methodically matched with the current state of the art, cf. (Bortolussi et al., 2011; Longobardi et al., 2016, 2013, 2015; Longobardi, 2017; Kazakov et al., 2017). Table 2: Longobardi original data ft. Blg SC Slo Po Rus Ir Wel Far Ma Hi Ar Heb Hu Fin StB wB Wo Table 1: Longobardi original data ft. Sic Cal It Sal Sp Fr Ptg Rm Lat CIG NtG BoG Gri Grk Got OE E D Da Ice Nor 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 1 1 1 0 1 1 1 1 1 0 0 0 * 0 0 1 0 1 * 0 1 1 1 * 1 * 1 0 1 0 1 0 0 0 1 0 0 0 * 0 1 0 0 1 * * * * * * 0 0 * 1 1 1 0 1 1 1 1 1 0 0 0 * 0 0 1 0 1 * 0 1 1 1 * 1 * 1 0 1 0 1 1 0 0 1 0 0 0 * 0"
R19-1100,P19-1150,0,0.0159905,"ionally expensive. Fortunately, Equation 2 can be rewritten as: on sentiment analysis in images and videos (Yu et al., 2016; You et al., 2015; Wang et al., 2016). The research on visual sentiment analysis proceeds along two dimensions: i) based on handcrafted features and ii) based on features generated automatically. Deep Learning techniques are capable of automatically learning robust features from a large number of images (Jindal and Singh, 2015). An interesting direction for sentiment analysis is related to word representations and capsule networks for NLP applications (Xing et al., 2019; Zhao et al., 2019). 3 W = FY FXT (FX FXT + αIdx )−1 = FY (FXT FX + αIn )−1 FXT (3) Making use of the kernel trick, the inputs xi are implicitly mapped to a high-dimensional Reproducing Kernel Hilbert space (Berlinet and Thomas-Agnan, 2011): Kernel Ridge Regression for Mapping Images to Text Let X = {x1 , x2 , . . . , xn } and Y = {y1 , y2 , . . . , yn } be the set of inputs and outputs, respectively, and n represents the number of observations. And let FX ∈ RdX ×n and FY ∈ RdY ×n denote the input and output feature matrices, where dX , dY represent the dimensions of the input and output features respectively. T"
S15-2144,W13-2714,1,0.825227,"LP techniques on a digitized collection of French texts published between 1801 and 1944. Stylerelated markers and features, including readability features, have been shown to reveal temporal information in English as well as Portuguese (Stamou, ˇ 2005; Stajner and Zampieri, 2013). An intersecting research direction combines diatopic (regional) and diachronic variation for French journalistic texts (Grouin et al., 2010) and for the Dutch Folktale Database, which includes texts from different dialects and varieties of Dutch, as well as historical texts (Trieschnigg et al., 2012). More recently, Ciobanu et al. (2013) propose supervised classification with unigram features with χ2 feature selection on a collection of historical Romanian texts, noting that the informative features are words having changed form over time. Niculae et al. (2014) circumvent the limitations of supervised classification by posing the problem as ordinal regression with a learning-to-rank approach. They evaluate their method on datasets in English, Portuguese and Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to"
S15-2144,W06-0903,0,0.362218,"anguage models and distances in distribution space to classify documents to the time period with the most similar language (de Jong et al., 2005; Kumar et al., 2011). Kanhabua and Nørv˚ag (2009) use temporal language models to assign timestamps to unlabeled documents. An extension of such models for continuous time is proposed by Wang et al. (2008), who use Brownian motion as a model for topic change over time. This approach is simpler and faster than the discrete time version, but it cannot be directly applied to documents with different degrees of label uncertainty, such as interval labels. Dalli and Wilks (2006) train a classifier to date texts within a time span of nine years. The method uses lexical features and it is aided by words whose frequencies increase at some point in time, most notably named entities. Abe and Tsumoto (2010) propose similarity metrics to categorise texts based on keywords calculated by indexes such as tf-idf. Garcia-Fernandez et al. (2011) explore different NLP techniques on a digitized collection of French texts published between 1801 and 1944. Stylerelated markers and features, including readability features, have been shown to reveal temporal information in English as we"
S15-2144,P12-2051,0,0.183283,"ised classification by posing the problem as ordinal regression with a learning-to-rank approach. They evaluate their method on datasets in English, Portuguese and Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to base our implementation on it. A different, but related, problem is to model and understand how words usage and meaning change over time. Wijaya and Yeniterzi (2011) use the Google NGram corpus aiming to identify clusters of topics surrounding the word over time. Mihalcea and Nastase (2012) split the Google Books corpus into three wide epochs and introduce the task of word epoch disambiguation. Turning this problem around, Popescu and Strapparava (2013) use a similar approach to statistically characterize epochs by lexical and emotion features. 3 Methods The ‘Diachronic Text Evaluation’ shared task consists of three subtasks (Popescu and Strapparava, 2015): classification of documents containing explicit references to time-specific persons or events (T1), classification of documents with time-specific language use (T2), and recognition of time-specific expressions (T3). The AMBR"
S15-2144,E14-4004,1,0.941706,"tuguese (Stamou, ˇ 2005; Stajner and Zampieri, 2013). An intersecting research direction combines diatopic (regional) and diachronic variation for French journalistic texts (Grouin et al., 2010) and for the Dutch Folktale Database, which includes texts from different dialects and varieties of Dutch, as well as historical texts (Trieschnigg et al., 2012). More recently, Ciobanu et al. (2013) propose supervised classification with unigram features with χ2 feature selection on a collection of historical Romanian texts, noting that the informative features are words having changed form over time. Niculae et al. (2014) circumvent the limitations of supervised classification by posing the problem as ordinal regression with a learning-to-rank approach. They evaluate their method on datasets in English, Portuguese and Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to base our implementation on it. A different, but related, problem is to model and understand how words usage and meaning change over time. Wijaya and Yeniterzi (2011) use the Google NGram corpus aiming to identify clusters of to"
S15-2144,I13-1040,0,0.380693,"Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to base our implementation on it. A different, but related, problem is to model and understand how words usage and meaning change over time. Wijaya and Yeniterzi (2011) use the Google NGram corpus aiming to identify clusters of topics surrounding the word over time. Mihalcea and Nastase (2012) split the Google Books corpus into three wide epochs and introduce the task of word epoch disambiguation. Turning this problem around, Popescu and Strapparava (2013) use a similar approach to statistically characterize epochs by lexical and emotion features. 3 Methods The ‘Diachronic Text Evaluation’ shared task consists of three subtasks (Popescu and Strapparava, 2015): classification of documents containing explicit references to time-specific persons or events (T1), classification of documents with time-specific language use (T2), and recognition of time-specific expressions (T3). The AMBRA system participated in T1 and T2. 3.1 Corpus The training data released for the shared task consists of 323 documents for T1 and 4,202 documents for T2. Each docume"
S15-2144,S15-2147,0,0.67111,"Missing"
S18-1158,W16-2509,0,0.136948,"Missing"
S18-1158,P16-2035,0,0.0217955,"a triplet of words, a model needs to determine if a semantic difference is present or not. As emphasized by Krebs and Paperno (2016), this non-trivial task has numerous applications, such as automatized lexicography, conversational agents or machine translation. Most research on discriminative features is related to computer vision (Farhadi et al., 2009; Russakovsky and Fei-Fei, 2012), as these attributes proved to be very useful in interpreting visual data (Huang et al., 2016), being able to link visual features and semantic labels (Guo et al., 2015). A recent study on this topic belongs to Lazaridou et al. (2016), who proposed a method for identifying discriminative attributes when given word pairs and their visual representations. In this paper, we describe a system for semantic difference detection that outputs a set of features for every triplet in the input data, based on preprocessed external resources (the English Wikipedia database). Further, these features are used to train an SVM for binary classification. The current feature selection allows even a direct approach such as evaluating the following inequation: Semantic difference detection attempts to capture whether a word is a discriminative"
S18-1158,P05-1012,0,0.048882,"called discriminative feature. The discriminative feature characterizes the first concept, w1 , but not the second one, w2 . If the discriminative feature characterizes both w1 and w2 or none of them, then we do not have semantic difference. |F1 | X i=1 fi − |F2 | X fi &gt; 0 (1) i=1 to obtain similar results as the SVM. Here, F1 and F2 are values of the same features, extracted for (w1 , w3 ) and (w2 , w3 ), respectively. Our model uses two different classes of features. The first class is generated using simple co-occurrence counts and the second class is generated by an arc-factored approach (McDonald et al., 2005) for semantic dependency parsing. Semantic dependency parsing aims to provide a shallow semantic analysis of the text. As distinct 963 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 963–967 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 3.2 from deeper semantic analysis, shallow semantic parsing captures relationships between pairs of words or concepts in a sentence (Thomson et al., 2014). 2 We use two categories of features in training our system: co-occurrence and POS-tag features. Features from both catego"
S18-1158,S14-2027,0,0.0665762,"Missing"
W09-4002,dinu-dinu-2006-data,1,0.77376,"frequently used syllables there is a library of articulatory routines that is accessed during the process of speech production. The adjoining of such syllabic gesture generates the spoken word and greatly reduce the computational cost of articulatory programs. These aspects determined us to study and analyze the syllable. In the following we will focus on the lexical (not phonological) aspects of the syllable. 3 Based on the DOOM dictionary, which contains Nwords = 74.276 words, the following series of quantitative and descriptive results for the syllables of Romanian language was extracted ([9]): Quantitative aspects of the syllable Opposite to the lack of qualitative insight regarding the syllable, the quantitative, statistic nature of the syllable was intensely studied. Determining the optimal values of the length of sentences and of the words depending on the certain groups of readers may prove to be very useful in practical application. By optimum value we understand the value for which the level of comprehensibility is the biggest for the class of readers. Knowing this value should be especially important for the teachers and for publishers who print text books. The main conclu"
W09-4002,J94-3001,0,0.153734,"Missing"
W12-0411,C08-2023,1,0.817123,"Missing"
W13-2714,W06-0903,0,0.173759,"e in building text classification models may be the change which occurs in the characteristics of the documents and their classes over time (Mour˜ao et al., 2008). Therefore, in order to overcome the difficulties which arise in automatic classification when dealing with documents dating from different epochs, identifying and accounting for document characteristics changing over time (such as class frequency, relationships between terms and classes and the similarity among classes over time (Mour˜ao et al., 2008)) is essential and can lead to a more accurate discrimination between classes. In (Dalli and Wilks, 2006) a method for classification of texts and documents based on their predicted time of creation is successfully applied, proving that accounting for word frequencies and their variation over time is accurate. In (Kumar et al., 2012) the authors argue as well for the capability of this method, of using words alone, to determine the epoch in which a text was written or the time period a document refers to. The effectiveness of using models for individuals partitions in a timeline with the purpose of predicting probabilities over the timeline for new documents is investigated in (Kumar et al., 2011"
W13-2714,P12-2051,0,0.2266,"Related Work The influence of the temporal effects in automatic document classification is analyzed in (Mour˜ao et al., 2008) and (Salles et al., 2010). The authors 102 Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 102–106, c Sofia, Bulgaria, August 8 2013. 2013 Association for Computational Linguistics nian orthography are noticed. other language changes. Thus, the linking of current word forms with their historical equivalents and accurate dating of texts can help reduce the temporal effects in this regard. Recently, in (Mihalcea and Nastase, 2012), the authors introduced the task of identifying changes in word usage over time, disambiguating the epoch at word-level. 3 3.1 Nwords type token Codicele Todorescu 3,799 15,421 Codicele Martian 394 920 Coresi, Evanghelia cu ˆınv˘a¸ta˘ tur˘a 10,361 184,260 Coresi, Lucrul apostolesc 7,311 79,032 Coresi, Psaltirea slavo-romˆan˘a 4,897 36,172 Coresi, Tˆargul evangheliilor 6,670 84,002 Coresi, Tetraevanghelul 3,876 36,988 Manuscrisul de la Ieud 1,414 4,362 Palia de la Or˘as¸tie 6,596 62,162 Psaltirea Hurmuzaki 4,851 32,046 The Bible 15,437 179,639 Miron Costin, Letopiset¸ul T¸a˘ rii Moldovei 6,912"
W14-1212,W10-1001,0,0.559549,"g material with an appropriate level of complexity from a large collection of documents – for example, the documents available on the web (Collins-Thompson, 2011). Within the medical domain, the investigation of the readability level of medical texts helps developing well-suited materials to increase the level of information for preventing diseases (Richwald et al., 1989) and to automatically adapt technical documents to various levels of medical expertise (Elhadad and Sutaria, 2007). For natural language processing tasks such as machine translation (Stymne et al., 2013), text simplification (Aluisio et al., 2010), speech recognition (Jones et al., 2005) or document summarization (Radev and Fan, 2000), readability approaches are employed to assist the process and to evaluate and quantify its performance and effectiveness. In this paper we investigate the impact of translation on readability. We propose a quantitative analysis of several shallow, lexical and morpho-syntactic features that have been traditionally used for assessing readability and have proven relevant for this task. We conduct our experiments on a parallel corpus of transcribed parliamentary sessions and we investigate readability metric"
W14-1212,atserias-etal-2006-freeling,0,0.025406,"Missing"
W14-1212,carreras-etal-2004-freeling,0,0.0709817,"Missing"
W14-1212,W12-2207,0,0.435399,"Missing"
W14-1212,N04-1025,0,0.0323115,"nly a coarse approximation of the linguistic factors which influence readability (Pitler and Nenkova, 2008). According to Si and Callan (2001), the shallow features employed by standard readability indices are based on assumptions about writing style that may not apply in all situations. Along with the development of natural languages processing tools and machine learning techniques, factors of increasing complexity , corresponding to various levels of linguistic analysis, have been taken into account in the study of readability assessment. Si and Callan (2001) and Collins-Thompson and Callan (2004) use statistical language modeling and Petersen and Ostendorf (2009) combine features from statistical language models, syntactic parse trees and traditional metrics to estimate reading difficulty. Feng (2009) explores discourse level attributes, along with lexical and syntactic features, and emphasizes the value of the global semantic properties of the text for predicting text readability. Pitler and Nenkova (2008) propose and analyze two perspectives for the task of readability assessment: prediction and ranking. Using various features, they reach the conclusion that only discourse level fea"
W14-1212,N07-1058,0,0.17197,"´o and Stanilovsky, 2012; Padr´o, 2011; Padr´o et al., 2010; Atserias et al., 2006; Carreras et al., 2004) language analysis tool suite for French, Italian, Spanish and Portuguese. Features We investigate several shallow, lexical and morpho-syntactic features that were traditionally used for assessing readability and have proven high discriminative power within readability metrics. 3.2.1 Lexical Features Lexical density. The proportion of content words (verbs, nouns, adjectives and adverbs), computed on a per-token basis. Grammatical features were shown to be useful in readability prediction (Heilman et al., 2007). Shallow Features Average number of words per sentence. The average sentence length is one of the most widely used metrics for determining readability level and was employed in numerous readability formulas, proving to be most meaningful in combined evidence with average word frequency. Feng et al. (2010) find the average sentence length to have higher predictive power than all the other lexical and syllable-based features they used. 4 Results Analysis Our main purpose is to investigate the variability of the feature values from the original texts to their translations. In Table 1 we report t"
W14-1212,W11-2308,0,0.0303567,"Missing"
W14-1212,islam-mehler-2012-customization,0,0.0175701,"f the translation strategy. For example, for political speeches the purpose is to report exactly what is communicated in a given text (Trosborg, 1997). Parallel corpora are very useful in studying the properties of translation and the relationships between source language and target language. Therefore, the corpus-based research has become more and more popular in translation research. Using the Europarl (Koehn, 2005) parallel corpus, van Halteren (2008) investigates the automatic identification of the source language of European Parliament speeches, based on frequency counts of word n-grams. Islam and Mehler (2012) draw attention to the absence of adequate corpora for studies on translation and propose a resource suited for this purpose. Although most readability approaches developed so far deal with English, the development of adequate corpora for experiments and the study of readability features tailored for other languages have received increasing attention. For Italian, Franchina and Vacca (1986) propose the FleschVacca formula, which is an adaptation of the Flesch index (Flesch, 1946). Another metric developed for Italian is Gulpease (Lucisano and Piemontese, 1988), which uses characters instead of"
W14-1212,W07-1007,0,0.255148,"nibuc.ro, ldinu@fmi.unibuc.ro Abstract ties or low literacy skills benefit from such systems, which provide assistance in selecting reading material with an appropriate level of complexity from a large collection of documents – for example, the documents available on the web (Collins-Thompson, 2011). Within the medical domain, the investigation of the readability level of medical texts helps developing well-suited materials to increase the level of information for preventing diseases (Richwald et al., 1989) and to automatically adapt technical documents to various levels of medical expertise (Elhadad and Sutaria, 2007). For natural language processing tasks such as machine translation (Stymne et al., 2013), text simplification (Aluisio et al., 2010), speech recognition (Jones et al., 2005) or document summarization (Radev and Fan, 2000), readability approaches are employed to assist the process and to evaluate and quantify its performance and effectiveness. In this paper we investigate the impact of translation on readability. We propose a quantitative analysis of several shallow, lexical and morpho-syntactic features that have been traditionally used for assessing readability and have proven relevant for t"
W14-1212,2005.mtsummit-papers.11,0,0.286106,"uage text into the target language, it is also important to maintain the style of the document. Various genres of text might be translated for different purposes, which influence the choice of the translation strategy. For example, for political speeches the purpose is to report exactly what is communicated in a given text (Trosborg, 1997). Parallel corpora are very useful in studying the properties of translation and the relationships between source language and target language. Therefore, the corpus-based research has become more and more popular in translation research. Using the Europarl (Koehn, 2005) parallel corpus, van Halteren (2008) investigates the automatic identification of the source language of European Parliament speeches, based on frequency counts of word n-grams. Islam and Mehler (2012) draw attention to the absence of adequate corpora for studies on translation and propose a resource suited for this purpose. Although most readability approaches developed so far deal with English, the development of adequate corpora for experiments and the study of readability features tailored for other languages have received increasing attention. For Italian, Franchina and Vacca (1986) prop"
W14-1212,W13-5634,0,0.0369957,"hich provide assistance in selecting reading material with an appropriate level of complexity from a large collection of documents – for example, the documents available on the web (Collins-Thompson, 2011). Within the medical domain, the investigation of the readability level of medical texts helps developing well-suited materials to increase the level of information for preventing diseases (Richwald et al., 1989) and to automatically adapt technical documents to various levels of medical expertise (Elhadad and Sutaria, 2007). For natural language processing tasks such as machine translation (Stymne et al., 2013), text simplification (Aluisio et al., 2010), speech recognition (Jones et al., 2005) or document summarization (Radev and Fan, 2000), readability approaches are employed to assist the process and to evaluate and quantify its performance and effectiveness. In this paper we investigate the impact of translation on readability. We propose a quantitative analysis of several shallow, lexical and morpho-syntactic features that have been traditionally used for assessing readability and have proven relevant for this task. We conduct our experiments on a parallel corpus of transcribed parliamentary se"
W14-1212,padro-stanilovsky-2012-freeling,0,0.020908,"Missing"
W14-1212,tiedemann-2012-parallel,0,0.0267362,"ents on Europarl (Koehn, 2005), a multilingual parallel corpus which is described in detail in Section 3.1. We investigate 5 Romance languages (Romanian, French, Italian, Spanish and Portuguese) and, in order to excerpt an adequate dataset of parallel texts, we adopt a strategy similar to that of van Halteren (2008): given n languages L1 , ..., Ln , we apply the following steps: Experimental Setup Data Europarl (Koehn, 2005) is a multilingual parallel corpus extracted from the proceedings of the European Parliament. Its main intended use is as aid for statistical machine translation research (Tiedemann, 2012). The corpus is tokenized and aligned in 21 languages. The files contain annotations for marking the document (<chapter>), the speaker (<speaker>) and the paragraph (<p>). Some documents have the attribute language for the speaker tag, which indicates the language used by the original speaker. Another way of annotating the original language is by having the language abbreviation written between parentheses at the beginning of each segment of text. However, there are segments where the language is not marked in either of the two ways. We account only for sentences for which the original languag"
W14-1212,padro-etal-2010-freeling,0,0.0214046,"Missing"
W14-1212,W12-2206,0,0.0144333,"words per sentence to assess readability, while the Automated Readability Index (Smith and Senter, 1967) and the Coleman-Liau metric (Coleman and Liau, 1975) measure word length based on character count rather than syllable count; they are func104 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 104–113, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics combine traditional, morpho-syntactic, lexical and syntactic features for building a readability model for Italian, while Tonelli et al. (2012) propose a system for readability assessment for Italian inspired by the principles of Coh-Metrix (Graesser et al., 2004). For French, Kandel and Moles (1958) propose an adaptation of the Flesch formula and Franc¸ois and Miltsakaki (2012) investigate a wide range of classic and non-classic features to predict readability level using a dataset for French as a foreign language. Readability assessment was also studied for Spanish (Huerta, 1959) and Portuguese (Aluisio et al., 2010) using features derived from previous research on English. tions of both the average number of characters per word an"
W14-1212,W12-2019,0,0.0141008,"and Ostendorf (2009) combine features from statistical language models, syntactic parse trees and traditional metrics to estimate reading difficulty. Feng (2009) explores discourse level attributes, along with lexical and syntactic features, and emphasizes the value of the global semantic properties of the text for predicting text readability. Pitler and Nenkova (2008) propose and analyze two perspectives for the task of readability assessment: prediction and ranking. Using various features, they reach the conclusion that only discourse level features exhibit robustness across the two tasks. Vajjala and Meurers (2012) show that combining lexical and syntactic features with features derived from second language acquisition research leads to performance improvements. 1.2 Readability of Translation According to Sun (2012), the reception of a translated text is related to cross-cultural readability. Translators need to understand the particularities of both the source and the target language in order to transfer the meaning of the text from one language to another. This process can be challenging, especially for languages with significant structure differences, such as English and Chinese. The three-step syste"
W14-1212,D08-1020,0,0.0302726,"sh (Huerta, 1959) and Portuguese (Aluisio et al., 2010) using features derived from previous research on English. tions of both the average number of characters per word and the average number of words per sentence. Gunning Fog (Gunning, 1952) and SMOG (McLaughlin, 1969) account also for the percentage of polysyllabic words and the Dale-Chall formula (Dale and Chall, 1995) relies on word frequency lists to assess readability. The traditional readability approaches are not computationally expensive, but they are only a coarse approximation of the linguistic factors which influence readability (Pitler and Nenkova, 2008). According to Si and Callan (2001), the shallow features employed by standard readability indices are based on assumptions about writing style that may not apply in all situations. Along with the development of natural languages processing tools and machine learning techniques, factors of increasing complexity , corresponding to various levels of linguistic analysis, have been taken into account in the study of readability assessment. Si and Callan (2001) and Collins-Thompson and Callan (2004) use statistical language modeling and Petersen and Ostendorf (2009) combine features from statistica"
W14-1212,C08-1118,0,0.502987,"Missing"
W14-1212,W00-1110,0,0.0810956,"for example, the documents available on the web (Collins-Thompson, 2011). Within the medical domain, the investigation of the readability level of medical texts helps developing well-suited materials to increase the level of information for preventing diseases (Richwald et al., 1989) and to automatically adapt technical documents to various levels of medical expertise (Elhadad and Sutaria, 2007). For natural language processing tasks such as machine translation (Stymne et al., 2013), text simplification (Aluisio et al., 2010), speech recognition (Jones et al., 2005) or document summarization (Radev and Fan, 2000), readability approaches are employed to assist the process and to evaluate and quantify its performance and effectiveness. In this paper we investigate the impact of translation on readability. We propose a quantitative analysis of several shallow, lexical and morpho-syntactic features that have been traditionally used for assessing readability and have proven relevant for this task. We conduct our experiments on a parallel corpus of transcribed parliamentary sessions and we investigate readability metrics for the original segments of text, written in the language of the speaker, and their tr"
W14-1212,C10-2032,0,\N,Missing
W16-4830,W15-5413,0,0.25838,"criminating between languages from various language families and using n-gram features. Their results show that Naive Bayes classifier performs best and that errors occur for languages from the same family, reinforcing the hypothesis that language identification is more difficult for very similar languages. Word n-grams have also proven effective for discriminating between languages and language varieties. Malmasi and Dras (2015) achieved the best performance in the closed track of the DSL 2015 shared task, experimenting with classifier ensembles trained on character and word n-gram features. Goutte and Leger (2015) obtained a very good performance in the same competition using statistical classifiers and employing a combination of character and word n-grams as features. Zampieri and Gebre (2012) made use of a character n-gram model and a word n-gram language model to discriminate between two varieties of Portuguese. They reported the highest accuracy when using character 4-grams and reached the conclusion that orthographic and lexical differences between the two varieties have more discriminative power than lexico-syntactic differences. Other features, such as exclusive words, the format of the numbers"
W16-4830,W14-4204,0,0.180943,"Missing"
W16-4830,W15-5407,0,0.103829,"a, Japan, December 12 2016. most of the Arabic dialects. Gottron and Lipka (2010) conducted a comparative experiment of classification methods for language identification in short texts, discriminating between languages from various language families and using n-gram features. Their results show that Naive Bayes classifier performs best and that errors occur for languages from the same family, reinforcing the hypothesis that language identification is more difficult for very similar languages. Word n-grams have also proven effective for discriminating between languages and language varieties. Malmasi and Dras (2015) achieved the best performance in the closed track of the DSL 2015 shared task, experimenting with classifier ensembles trained on character and word n-gram features. Goutte and Leger (2015) obtained a very good performance in the same competition using statistical classifiers and employing a combination of character and word n-grams as features. Zampieri and Gebre (2012) made use of a character n-gram model and a word n-gram language model to discriminate between two varieties of Portuguese. They reported the highest accuracy when using character 4-grams and reached the conclusion that orthog"
W16-4830,W16-4801,0,0.0510846,"Missing"
W16-4830,W14-5904,0,0.0318164,"s have been employed to discriminate between a wide variety of closely related languages and dialects. Maier and G´omez-Rodr´ıguez (2014) performed language classification on tweets for Spanish varieties, with character n-grams as features and using the country of the speaker to identify the variety. Trieschnigg et al. (2012) discriminated between Dutch dialects (and several other languages) using a large collection of folktales. They compared several approaches to language identification and reported good results when using the method of Cavnar and Trenkle (1994), based on character n-grams. Sadat et al. (2014) performed language identification on Arabic dialects using social media texts. They obtained better results with Naive Bayes and n-gram features (2-grams) than with a character n-gram Markov model for This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 235 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 235–242, Osaka, Japan, December 12 2016. most of the Arabic dialects. Gottron and Lipka (2010) conducted a comparative experiment of classification met"
W16-4830,C12-1160,0,0.160608,"he same competition using statistical classifiers and employing a combination of character and word n-grams as features. Zampieri and Gebre (2012) made use of a character n-gram model and a word n-gram language model to discriminate between two varieties of Portuguese. They reported the highest accuracy when using character 4-grams and reached the conclusion that orthographic and lexical differences between the two varieties have more discriminative power than lexico-syntactic differences. Other features, such as exclusive words, the format of the numbers (Ranaivo-Malancon, 2006), blacklists (Tiedemann and Ljubesic, 2012), syllable n-grams (Maier and G´omez-Rodr´ıguez, 2014) or skipgrams have been employed and shown useful for this task. 3 Data The organizers released two training datasets for the 2016 DSL shared task: a dataset of similar languages and language varieties (for sub-task 1) and a dataset of Arabic dialects (for sub-task 2). The dataset for sub-task 1 is a new version of the DSL Corpus Collection (Tan et al., 2014). It contains instances written in the following languages and language varieties (organized by groups of similarity): Language Lang. code Group code Avg. sent. lenth Avg. word length B"
W17-2210,dinu-etal-2008-authorship,1,0.884564,"based on function words frequencies. Many other types of features have been proposed and successfully used in subsequent studies to determine the author of a text. These types of features generally contrast with the content words commonly used in text categorization by topic, and are said to be used unconsciously and harder to control by the author. Such features are, for example, grammatical structures (Baayen et al., 1996), part-of-speech ngrams (Koppel and Schler, 2003), lexical richness (Tweedie and Baayen, 1998), or even the more general feature of character n-grams (Kešelj et al., 2003; Dinu et al., 2008). Having applications that go beyond finding the real authors of controversial texts, ranging from plagiarism detection to forensics to security, stylometry has widened its scope into other related subtopics such as author verification (verifying whether a text was written by a certain author) (Koppel and Schler, 2004) or author profiling (extracting information about an author’s age, gender, etc). A related problem that has barely been approached in the scientific literature is that of distinguishing between the writing styles of fictional people, namely literary characters. This problem may"
W17-5045,W13-1712,0,0.0527967,"Missing"
W17-5045,W17-1214,0,0.014955,"alities into account. The approach used in our submission is described next. • A variation of NRC’s SVM approach (Goutte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect"
W17-5045,W14-5316,0,0.0410166,"Missing"
W17-5045,L16-1284,1,0.91673,"Missing"
W17-5045,W13-1724,0,0.172835,"including iVectors) could be used. The test dataset, containing 1,100 instances with essays, speech transcriptions and iVectors, was released at a later date. The use of a dataset containing text and speech is the main new aspect of the 2017 NLI task so we decide to compete in the fusion track taking both modalities into account. The approach used in our submission is described next. • A variation of NRC’s SVM approach (Goutte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We"
W17-5045,W13-1713,0,0.0547241,"Missing"
W17-5045,W15-5410,0,0.162088,"ors, was released at a later date. The use of a dataset containing text and speech is the main new aspect of the 2017 NLI task so we decide to compete in the fusion track taking both modalities into account. The approach used in our submission is described next. • A variation of NRC’s SVM approach (Goutte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The ide"
W17-5045,brooke-hirst-2012-measuring,0,0.0175122,"terances) that are likely to be written or spoken by speakers of the same language. There are two important reasons to study NLI. Firstly, there is SLA. NLI methods can be applied to learner corpora to investigate the influence of native language in second language acquisition and production complementing corpus-based and corpus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa"
W17-5045,W17-1225,0,0.0196201,"stablished the first benchmark for NLI on written texts. Organizers of the first NLI task provided participants with the TOEFL 11 (Blanchard et al., 2013) dataset which contained essays written by students native speakers of the same eleven languages included in the NLI Shared Task 2017. Twenty-nine teams participated in the competition, testing a wide range of computational methods for NLI. In Table 1 we list the top ten best 399 3 • Variations of the string kernels method by the Unibuc team (Popescu and Ionescu, 2013) competed in the ADI task in 2016 (Ionescu and Popescu, 2016) and in 2017 (Ionescu and Butnaru, 2017) achieving the best results. Methods In the next sections we describe the data provided by the shared task organizers and the ensemble SVM approach applied by the ZCD team. 3.1 • Cologne-Nijmegen’s TF-IDF-based approach (Gebre et al., 2013) competed in the DSL shared task 2015 (Zampieri et al., 2015a) as team MMS ranking among the top 3 systems. Data The organizers of the NLI Shared Task 2017 provided participants with data corresponding to eleven native languages: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish. The training dataset consists of 1"
W17-5045,C14-1185,0,0.0862542,"y to be written or spoken by speakers of the same language. There are two important reasons to study NLI. Firstly, there is SLA. NLI methods can be applied to learner corpora to investigate the influence of native language in second language acquisition and production complementing corpus-based and corpus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa Cologne-Nijmegen NAIST U"
W17-5045,W16-4818,0,0.0234789,"red Task 2013 (Tetreault et al., 2013) established the first benchmark for NLI on written texts. Organizers of the first NLI task provided participants with the TOEFL 11 (Blanchard et al., 2013) dataset which contained essays written by students native speakers of the same eleven languages included in the NLI Shared Task 2017. Twenty-nine teams participated in the competition, testing a wide range of computational methods for NLI. In Table 1 we list the top ten best 399 3 • Variations of the string kernels method by the Unibuc team (Popescu and Ionescu, 2013) competed in the ADI task in 2016 (Ionescu and Popescu, 2016) and in 2017 (Ionescu and Butnaru, 2017) achieving the best results. Methods In the next sections we describe the data provided by the shared task organizers and the ensemble SVM approach applied by the ZCD team. 3.1 • Cologne-Nijmegen’s TF-IDF-based approach (Gebre et al., 2013) competed in the DSL shared task 2015 (Zampieri et al., 2015a) as team MMS ranking among the top 3 systems. Data The organizers of the NLI Shared Task 2017 provided participants with data corresponding to eleven native languages: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Tur"
W17-5045,W13-1714,0,0.281178,"on word n-grams (1-2), and POS n-grams (1-5), and syntactic features (dependencies) Ensemble of SVM classifiers trained on character trigrams, word n-grams (1-2), POS n-grams (2-4), and syntactic features (dependencies) Maximum Entropy trained on word n-grams (1-4), POS n-grams (1-4), and spelling features SVM classifier with TF-IDF weighting trained on character ngrams (1-6), word n-grams (1-2), and POS n-grams (1-4) SVM trained on character n-grams (2-3), word n-grams (1-2), and POS n-grams (2-3), and syntactic features (dependencies and TSG) SVM trained on word n-grams (1-2) System Paper (Jarvis et al., 2013) (Lynum, 2013) (Popescu and Ionescu, 2013) (Henderson et al., 2013) (Bykh et al., 2013) (Goutte et al., 2013) (Tsvetkov et al., 2013) (Gebre et al., 2013) (Mizumoto et al., 2013) (Wu et al., 2013) Table 1: Top ten NLI Shared Task 2013 entries ordered by performance. entries ranked by performance along with their respective system description papers. The best system by Jarvis et al. (2013) applied a linear SVM classifier trained on character, word, and POS n-grams. Seven out of the ten best entries in the shared task used SVM classifiers. This indicates that SMVs are a very good fit for NLI and"
W17-5045,W13-1726,0,0.0408826,"Missing"
W17-5045,W13-1734,0,0.030856,"Missing"
W17-5045,W13-1728,1,0.873312,"s (1-2), POS n-grams (2-4), and syntactic features (dependencies) Maximum Entropy trained on word n-grams (1-4), POS n-grams (1-4), and spelling features SVM classifier with TF-IDF weighting trained on character ngrams (1-6), word n-grams (1-2), and POS n-grams (1-4) SVM trained on character n-grams (2-3), word n-grams (1-2), and POS n-grams (2-3), and syntactic features (dependencies and TSG) SVM trained on word n-grams (1-2) System Paper (Jarvis et al., 2013) (Lynum, 2013) (Popescu and Ionescu, 2013) (Henderson et al., 2013) (Bykh et al., 2013) (Goutte et al., 2013) (Tsvetkov et al., 2013) (Gebre et al., 2013) (Mizumoto et al., 2013) (Wu et al., 2013) Table 1: Top ten NLI Shared Task 2013 entries ordered by performance. entries ranked by performance along with their respective system description papers. The best system by Jarvis et al. (2013) applied a linear SVM classifier trained on character, word, and POS n-grams. Seven out of the ten best entries in the shared task used SVM classifiers. This indicates that SMVs are a very good fit for NLI and motivates us to test SVM classifiers in our ensemble-based system described in this paper. shared task to provide a benchmark for NLI focusing on written"
W17-5045,U14-1020,0,0.0220629,"in second language acquisition and production complementing corpus-based and corpus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa Cologne-Nijmegen NAIST UTD Approach SVM trained on character n-grams (1-9), word n-grams (1-4), and POS n-grams (1-4) SVM trained on character n-grams (1-7) String Kernels and Local Rank Distance (LRD) Bayes ensemble of multiple classifiers SVM t"
W17-5045,W13-1706,0,0.294568,"pus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa Cologne-Nijmegen NAIST UTD Approach SVM trained on character n-grams (1-9), word n-grams (1-4), and POS n-grams (1-4) SVM trained on character n-grams (1-7) String Kernels and Local Rank Distance (LRD) Bayes ensemble of multiple classifiers SVM trained on word n-grams (1-2), and POS n-grams (1-5), and syntactic features (depe"
W17-5045,W15-5407,0,0.0358415,"et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are complex word identification (Malmasi et al., 2016a) and gr"
W17-5045,W13-1736,0,0.0637207,"Missing"
W17-5045,W17-5007,0,0.117872,"Missing"
W17-5045,W13-1716,0,0.0284878,"outte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are com"
W17-5045,W13-1720,0,0.072442,"Missing"
W17-5045,W16-4814,1,0.792098,"• Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015)"
W17-5045,W15-4415,0,0.0799458,"and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015). The classifiers can differ in a wide range of aspects, such as algorithms, training data, features or parameters. In our system, the classifiers used different features. We experimented with the following features: character n-grams (with n in {1, ..., 10}) from essays and speech transcripts, word n-grams (with n in {1, 2}) from essays and speech transcripts, and iVectors. For the n-gram features we This section evidenced an important overlap between NLI methods and dialect identification methods both in terms of participation overlap in the shared tasks and in terms of successful approaches"
W17-5045,W16-4801,1,0.910549,"Missing"
W17-5045,W15-5411,1,0.872338,"Missing"
W17-5045,W13-1717,0,0.0610131,"Missing"
W17-5045,W17-1201,1,0.897772,"Missing"
W17-5045,W14-5307,1,0.921728,"Missing"
W17-5045,W13-1735,0,0.176378,"ams (1-5), and syntactic features (dependencies) Ensemble of SVM classifiers trained on character trigrams, word n-grams (1-2), POS n-grams (2-4), and syntactic features (dependencies) Maximum Entropy trained on word n-grams (1-4), POS n-grams (1-4), and spelling features SVM classifier with TF-IDF weighting trained on character ngrams (1-6), word n-grams (1-2), and POS n-grams (1-4) SVM trained on character n-grams (2-3), word n-grams (1-2), and POS n-grams (2-3), and syntactic features (dependencies and TSG) SVM trained on word n-grams (1-2) System Paper (Jarvis et al., 2013) (Lynum, 2013) (Popescu and Ionescu, 2013) (Henderson et al., 2013) (Bykh et al., 2013) (Goutte et al., 2013) (Tsvetkov et al., 2013) (Gebre et al., 2013) (Mizumoto et al., 2013) (Wu et al., 2013) Table 1: Top ten NLI Shared Task 2013 entries ordered by performance. entries ranked by performance along with their respective system description papers. The best system by Jarvis et al. (2013) applied a linear SVM classifier trained on character, word, and POS n-grams. Seven out of the ten best entries in the shared task used SVM classifiers. This indicates that SMVs are a very good fit for NLI and motivates us to test SVM classifiers in o"
W17-5045,W15-5401,1,0.918132,"Missing"
W18-3920,W16-4819,0,0.0223457,"om 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dat"
W18-3920,W15-5410,0,0.0278754,"gs are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and n"
W18-3920,D14-1069,0,0.0726343,"Missing"
W18-3920,W17-1213,0,0.0135081,"tween Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 20"
W18-3920,L16-1284,1,0.827322,"ombination. With the aid of Hindi speakers, in Section 5.1 we presented a concise error analysis of the misclassified instances of the development set. We observed a few interesting patterns in the misclassified instances, most notably that many of the misclassified sentences were too short, containing only one, two or three words, and that several of them contained only named entities. making it very challenging for classifiers to identify the language of these instances. Another issue discussed in Section 5.1, is that some instances could not be discriminated by native speakers, as noted by Goutte et al. (2016). To cope with these instances one possible direction for future work is to allow a multi-label classification setup in which sentences could be assign to more than one category if annotators labeled them as such. In future work we would like to explore and compare our methods to other high performance methods for this task. In particular, we would like to try an implementation of the token-based back-off method proposed by the SUKI team. As evidenced in Section 5, SUKI’s system achieved substantially higher performance than the other methods in this competition. Acknowledgements We would like"
W18-3920,W15-5408,0,0.088526,"., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Ary"
W18-3920,W16-4820,0,0.0514295,"e of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Aryan Language Identification"
W18-3920,W15-5407,1,0.925336,"he DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Aryan Language Identification (ILI) task comprises five similar languages spoken"
W18-3920,W16-4801,1,0.936687,", 2018) and in previous work (Tiedemann and Ljubešić, 2012; Goutte et al., 2016), discriminating between similar languages is one of the main challenges in automatic language identification. State-of-the-art n-gram-based language identification systems are able to discriminate between unrelated languages with very high performance but very often struggle to discriminate between similar languages. This challenge motivated the organization of recent evaluation campaigns such as the TweetLID (Zubiaga et al., 2016) which included languages spoken in the Iberian peninsula and the DSL shared tasks (Malmasi et al., 2016b; Zampieri et al., 2015) which included groups of similar languages such as Malay and Indonesian, Bulgarian and Macedonian, and Bosnian, Croatian, and Serbian as well as groups of language varieties such as Brazilian and European Portuguese. In this paper we revisit the problem of discriminating between similar languages presenting a system to discriminate between five languages of the Indo-Aryan family: Hindi, Braj Bhasha, Awadhi, Bhojpuri, and Magahi. Inspired by systems that performed well in past editions of the DSL shared task such as the one by Malmasi and Dras (2015), we developed a sy"
W18-3920,W14-5314,0,0.0166142,"identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks"
W18-3920,W14-5318,0,0.017647,"shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed desc"
W18-3920,C12-1160,0,0.0763191,"Missing"
W18-3920,W15-4415,0,0.0311268,"aterial or external resource. 4 Methodology Following our aforementioned previous work (Ciobanu et al., 2017), we built a classification system based on SVM ensembles using the same methodology proposed by Malmasi and Dras (2015). The purpose of using classification ensembles is to improve the overall performance and robustness by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, but also in various text classification tasks, such as complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015). The classifiers can differ in a wide range of aspects; for example, algorithms, training data, features or parameters. We implemented our system using the Scikit-learn (Pedregosa et al., 2011) machine learning library, with each classifier in the ensemble using a different type of features. For the individual classifiers, we employed the SVM implementation based on the Liblinear library (Fan et al., 2008), LinearSVC1 , with a linear kernel. This implementation has the advantage of scaling well to large number of samples. For the ensemble, we employed the majority rule VotingClassifier2 , whi"
W18-3920,W14-5307,1,0.895397,", Croatian, Montenegrin, and Serbian (Ljubesic and Kranjcic, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 178 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–184 Santa Fe, New Mexico, USA, August 20, 2018. A first attempting of benchmarking the identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word"
W18-3920,W15-5401,1,0.921353,"Missing"
W18-3920,W17-5045,1,0.84077,"bian (Ljubesic and Kranjcic, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 178 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–184 Santa Fe, New Mexico, USA, August 20, 2018. A first attempting of benchmarking the identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiai"
W18-3920,W18-3901,1,0.883574,"Missing"
W18-3933,W17-1223,0,0.191393,"and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results of the 2017 edition along with a reference to each system description paper are presented in Table 1. Rank 1 2 3 4 5 6 7 8 9 10 Team MAZA CECL CLUZH qcri mit unibuckernel tubasfs ahaqst Citius Ixa Imaxin XAC Bayesline deepCybErNet F1 (weighted) 0.662 0.661 0.653 0.639 0.637 0.626 0.614 0.612 0.605 0.263 Reference (Malmasi and Zampieri, 2017b) (Bestgen, 2017) (Clematide and Makarov, 2017) (Ionescu and Butnaru, 2017) (C¸o¨ ltekin and Rama, 2017) (Hanani et al., 2017) (Gamallo et al., 2017) (Barbaresi, 2017) - Table 1: GDI shared task 2017: Closed submission results. The ten teams who competed in the first GDI challenge applied different computational methods to approach the task. These include linear SVM classifiers (C¸o¨ ltekin and Rama, 2017; Bestgen, 2017), string kernels (Ionescu and Butnaru, 2017), Naive Bayes classifiers (Barbaresi, 2017), and SVM ensembles (Malmasi and Zampieri, 2017b), which achieved the first place in 2017. For this reason, this is the approach we apply in our GDI identification system. 3 Data In this paper we used only the dataset provided by the GDI organizers. The da"
W18-3933,W17-1214,0,0.0935605,"kshop on NLP for Similar Languages, Varieties and Dialects, pages 288–294 Santa Fe, New Mexico, USA, August 20, 2018. The GDI shared task 2017 setup and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results of the 2017 edition along with a reference to each system description paper are presented in Table 1. Rank 1 2 3 4 5 6 7 8 9 10 Team MAZA CECL CLUZH qcri mit unibuckernel tubasfs ahaqst Citius Ixa Imaxin XAC Bayesline deepCybErNet F1 (weighted) 0.662 0.661 0.653 0.639 0.637 0.626 0.614 0.612 0.605 0.263 Reference (Malmasi and Zampieri, 2017b) (Bestgen, 2017) (Clematide and Makarov, 2017) (Ionescu and Butnaru, 2017) (C¸o¨ ltekin and Rama, 2017) (Hanani et al., 2017) (Gamallo et al., 2017) (Barbaresi, 2017) - Table 1: GDI shared task 2017: Closed submission results. The ten teams who competed in the first GDI challenge applied different computational methods to approach the task. These include linear SVM classifiers (C¸o¨ ltekin and Rama, 2017; Bestgen, 2017), string kernels (Ionescu and Butnaru, 2017), Naive Bayes classifiers (Barbaresi, 2017), and SVM ensembles (Malmasi and Zampieri, 2017b), which achieved the first place in 2017. For this reason"
W18-3933,W17-1221,0,0.355937,"Missing"
W18-3933,W17-1213,0,0.0231086,"Missing"
W18-3933,W17-1211,0,0.0363003,"Missing"
W18-3933,W17-1225,0,0.061586,"and Dialects, pages 288–294 Santa Fe, New Mexico, USA, August 20, 2018. The GDI shared task 2017 setup and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results of the 2017 edition along with a reference to each system description paper are presented in Table 1. Rank 1 2 3 4 5 6 7 8 9 10 Team MAZA CECL CLUZH qcri mit unibuckernel tubasfs ahaqst Citius Ixa Imaxin XAC Bayesline deepCybErNet F1 (weighted) 0.662 0.661 0.653 0.639 0.637 0.626 0.614 0.612 0.605 0.263 Reference (Malmasi and Zampieri, 2017b) (Bestgen, 2017) (Clematide and Makarov, 2017) (Ionescu and Butnaru, 2017) (C¸o¨ ltekin and Rama, 2017) (Hanani et al., 2017) (Gamallo et al., 2017) (Barbaresi, 2017) - Table 1: GDI shared task 2017: Closed submission results. The ten teams who competed in the first GDI challenge applied different computational methods to approach the task. These include linear SVM classifiers (C¸o¨ ltekin and Rama, 2017; Bestgen, 2017), string kernels (Ionescu and Butnaru, 2017), Naive Bayes classifiers (Barbaresi, 2017), and SVM ensembles (Malmasi and Zampieri, 2017b), which achieved the first place in 2017. For this reason, this is the approach we apply in our GDI identification"
W18-3933,U13-1003,0,0.0738265,"Missing"
W18-3933,W15-5407,1,0.849532,"ning the four dialects plus a ‘surprise’ dialect not included in the training set. We opted to participate only in the first track which contained only previously ‘seen’ dialects. The dataset comprise nearly 25,000 instances divided in training, development, and test partitions as presented in Table 2. Partition Training Development Test Total Instances 14,647 4,659 5,543 24,849 Table 2: Instances in the GDI dataset 2018. 4 Methodology The system that we propose for the GDI shared task consists of an ensemble of classifiers, namely SVMs. In this approach, we employ the methodology proposed by Malmasi and Dras (2015). Ensembles of classifiers are deemed useful when there are disagreements between the comprising classifiers, which can use different features, training data, algorithms or parameters. The scope of the ensemble is to combine the results of the classifiers in such a way that the overall performance is improved 1 http://www.spur.uzh.ch/en/departments/research/textgroup/ArchiMob.html 289 over the individual performances of the classifiers. Ensembles have proven useful in various tasks, such as complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015)"
W18-3933,W17-1222,1,0.894163,", and finally the first German Dialect Identification (GDI) shard task in 2017 (Zampieri et al., 2017). The GDI shared task 2017 preceded the second GDI shared task (Zampieri et al., 2018) in which our team, GDI classification, participated. In this paper we describe the GDI classification system trained to identify four dialects of (Swiss) German. The GDI dataset included speech transcripts from speakers from Basel, Bern, Lucerne, and Zurich. The system is based on an ensemble of multiple SVM classifiers trained on words and characters as features. Our approach is inspired by the approach of Malmasi and Zampieri (2017b), which was ranked first in the first edition of the GDI task and also performed well on identifying dialects of Arabic (Malmasi and Zampieri, 2017a). We build on the experience of previous work of members of the GDI classification team improving a system that we have previously applied to a similar classification task, namely author profiling (Ciobanu et al., 2017). 2 Related Work: The First GDI Shared Task There have been a few studies on German dialect identification published before the first GDI shared task, using different corpora and evaluation methods (Scherrer and Rambow, 2010; Holl"
W18-3933,W17-1220,1,0.817949,", and finally the first German Dialect Identification (GDI) shard task in 2017 (Zampieri et al., 2017). The GDI shared task 2017 preceded the second GDI shared task (Zampieri et al., 2018) in which our team, GDI classification, participated. In this paper we describe the GDI classification system trained to identify four dialects of (Swiss) German. The GDI dataset included speech transcripts from speakers from Basel, Bern, Lucerne, and Zurich. The system is based on an ensemble of multiple SVM classifiers trained on words and characters as features. Our approach is inspired by the approach of Malmasi and Zampieri (2017b), which was ranked first in the first edition of the GDI task and also performed well on identifying dialects of Arabic (Malmasi and Zampieri, 2017a). We build on the experience of previous work of members of the GDI classification team improving a system that we have previously applied to a similar classification task, namely author profiling (Ciobanu et al., 2017). 2 Related Work: The First GDI Shared Task There have been a few studies on German dialect identification published before the first GDI shared task, using different corpora and evaluation methods (Scherrer and Rambow, 2010; Holl"
W18-3933,W16-4801,1,0.893288,"Missing"
W18-3933,L16-1641,0,0.215341,"Missing"
W18-3933,D10-1112,0,0.0217023,"h of Malmasi and Zampieri (2017b), which was ranked first in the first edition of the GDI task and also performed well on identifying dialects of Arabic (Malmasi and Zampieri, 2017a). We build on the experience of previous work of members of the GDI classification team improving a system that we have previously applied to a similar classification task, namely author profiling (Ciobanu et al., 2017). 2 Related Work: The First GDI Shared Task There have been a few studies on German dialect identification published before the first GDI shared task, using different corpora and evaluation methods (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). To the best of our knowledge, the first GDI shared task organized in 2017 was the first attempt to provide a benchmark for this task. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 288 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 288–294 Santa Fe, New Mexico, USA, August 20, 2018. The GDI shared task 2017 setup and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results"
W18-3933,W14-5313,0,0.0606865,"Missing"
W18-3933,W15-4415,0,0.0268304,"asi and Dras (2015). Ensembles of classifiers are deemed useful when there are disagreements between the comprising classifiers, which can use different features, training data, algorithms or parameters. The scope of the ensemble is to combine the results of the classifiers in such a way that the overall performance is improved 1 http://www.spur.uzh.ch/en/departments/research/textgroup/ArchiMob.html 289 over the individual performances of the classifiers. Ensembles have proven useful in various tasks, such as complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015). To distinguish the classifiers, we employ a different type of features for each of them. After obtaining predictions from each classifier, they need to be combined, to obtain the final predictions of the ensemble. To implement this system we used the Scikit-learn (Pedregosa et al., 2011) library. For the individual classifiers, we used LinearSVC,2 an SVM implementation based on the Liblinear library (Fan et al., 2008), with a linear kernel. For the ensemble, we used the VotingClassifier,3 with a majority rule fusion method: for each instance, the class that has been predicted by the majority"
W18-3933,W14-5307,0,0.102897,"Missing"
W18-3933,W15-5401,0,0.246759,"Missing"
W18-3933,W17-1201,1,0.902646,"Missing"
W18-3933,W18-3901,1,0.890697,"Missing"
W18-3933,L16-1522,1,\N,Missing
W18-3933,W17-1218,0,\N,Missing
W19-4720,R09-1054,0,0.251504,"ned embeddings are suitable for our study since: they are trained on large amounts of text, which minimizes the amount of noise in the vectors, making them good approximators of word meanings; and they are trained on text that is relatively uniform in style and topic - ensuring Related to our task, there have been a number of previous studies attempting to automatically extract pairs of true cognates and false friends from corpora or from dictionaries. Most methods are based either on orthographic and phonetic similarity, or require large parallel corpora or dictionaries (Inkpen et al., 2005; Nakov et al., 2009; Chen and Skiena, 2016; St Arnaud et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognate words, usually using simple methods on only one or two pairs of languages (Torres and Alu´ısio, 2011; Cas162 Romanian arhitect French architecte Italian architetto Spanish arquitecto Portuguese arquiteto Latin ancestor architectus Table 1: An example of a cognate set: “architect” in Romance languages. any differences in the structure of the embedding spaces of different languages is dependent on the language, rather than an artifact of topic"
W19-4720,D17-1267,0,0.0672359,"e: they are trained on large amounts of text, which minimizes the amount of noise in the vectors, making them good approximators of word meanings; and they are trained on text that is relatively uniform in style and topic - ensuring Related to our task, there have been a number of previous studies attempting to automatically extract pairs of true cognates and false friends from corpora or from dictionaries. Most methods are based either on orthographic and phonetic similarity, or require large parallel corpora or dictionaries (Inkpen et al., 2005; Nakov et al., 2009; Chen and Skiena, 2016; St Arnaud et al., 2017). There have been few previous studies using word embeddings for the detection of false friends or cognate words, usually using simple methods on only one or two pairs of languages (Torres and Alu´ısio, 2011; Cas162 Romanian arhitect French architecte Italian architetto Spanish arquitecto Portuguese arquiteto Latin ancestor architectus Table 1: An example of a cognate set: “architect” in Romance languages. any differences in the structure of the embedding spaces of different languages is dependent on the language, rather than an artifact of topic or genre. Nevertheless, even high quality embed"
W19-4720,W18-3903,0,0.218382,"correlated with semantic change) (Hamilton et al., 2016), or the law of prototypicality (according to which prototypicality is negatively correlated with semantic change) (Dubossarsky et al., 2015). More recently, Dubossarsky et al. (2017) revisited some of the semantic change laws proposed in previous literature, claiming that a more rigorous consideration of control conditions when modelling these laws leads to the conclusion that they are weaker or less reliable than reported. More extensive surveys of computational studies relating to semantic change have been conducted by Kutuzov et al. (2018); Tahmasebi et al. (2018). All previous computational studies on lexical semantic change have, to our knowledge, only looked at the semantic change of the words within one language. However, words do not evolve only in their own language in isolation, but are rather inherited and borrowed between and across languages. Cognates are words in sister languages (languages descending from a common ancestor) with a common proto-word. For example, the Romanian word victorie and the Italian word vittoria are cognates, as they both descend from the Latin word victoria (meaning victory) – see Figure 1. I"
W19-4720,W11-4508,0,0.54964,"Missing"
W19-4720,dinu-ciobanu-2014-building,1,0.893033,"Missing"
W19-4720,D17-1118,0,0.0489272,"fmi.unibuc.ro, liviu.p.dinu@gmail.com Abstract law of parallel change and the law of differentiation (Xu and Kemp, 2015)), or even proposed new statistical laws of semantic change, based on empirical observations, such as the law of conformity (stating that polysemy is positively correlated with semantic change), the law of innovation (according to which word frequency is negatively correlated with semantic change) (Hamilton et al., 2016), or the law of prototypicality (according to which prototypicality is negatively correlated with semantic change) (Dubossarsky et al., 2015). More recently, Dubossarsky et al. (2017) revisited some of the semantic change laws proposed in previous literature, claiming that a more rigorous consideration of control conditions when modelling these laws leads to the conclusion that they are weaker or less reliable than reported. More extensive surveys of computational studies relating to semantic change have been conducted by Kutuzov et al. (2018); Tahmasebi et al. (2018). All previous computational studies on lexical semantic change have, to our knowledge, only looked at the semantic change of the words within one language. However, words do not evolve only in their own langu"
W19-4720,P16-1141,0,0.498278,"a Ciobanu, and Liviu P. Dinu Faculty of Mathematics and Computer Science, Human Language Technologies Research Center, University of Bucharest ana.uban@gmail.com, alina.ciobanu@my.fmi.unibuc.ro, liviu.p.dinu@gmail.com Abstract law of parallel change and the law of differentiation (Xu and Kemp, 2015)), or even proposed new statistical laws of semantic change, based on empirical observations, such as the law of conformity (stating that polysemy is positively correlated with semantic change), the law of innovation (according to which word frequency is negatively correlated with semantic change) (Hamilton et al., 2016), or the law of prototypicality (according to which prototypicality is negatively correlated with semantic change) (Dubossarsky et al., 2015). More recently, Dubossarsky et al. (2017) revisited some of the semantic change laws proposed in previous literature, claiming that a more rigorous consideration of control conditions when modelling these laws leads to the conclusion that they are weaker or less reliable than reported. More extensive surveys of computational studies relating to semantic change have been conducted by Kutuzov et al. (2018); Tahmasebi et al. (2018). All previous computation"
W19-4720,C18-1117,0,0.0620746,"y is negatively correlated with semantic change) (Hamilton et al., 2016), or the law of prototypicality (according to which prototypicality is negatively correlated with semantic change) (Dubossarsky et al., 2015). More recently, Dubossarsky et al. (2017) revisited some of the semantic change laws proposed in previous literature, claiming that a more rigorous consideration of control conditions when modelling these laws leads to the conclusion that they are weaker or less reliable than reported. More extensive surveys of computational studies relating to semantic change have been conducted by Kutuzov et al. (2018); Tahmasebi et al. (2018). All previous computational studies on lexical semantic change have, to our knowledge, only looked at the semantic change of the words within one language. However, words do not evolve only in their own language in isolation, but are rather inherited and borrowed between and across languages. Cognates are words in sister languages (languages descending from a common ancestor) with a common proto-word. For example, the Romanian word victorie and the Italian word vittoria are cognates, as they both descend from the Latin word victoria (meaning victory) – see Figure 1. I"
