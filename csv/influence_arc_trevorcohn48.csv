2014.eamt-1.7,ambati-etal-2010-active,0,\N,Missing
2014.eamt-1.7,N07-1008,0,\N,Missing
2014.eamt-1.7,D11-1033,0,\N,Missing
2014.eamt-1.7,N10-1134,0,\N,Missing
2014.eamt-1.7,D08-1089,0,\N,Missing
2014.eamt-1.7,W13-2206,0,\N,Missing
2014.eamt-1.7,N09-1047,0,\N,Missing
2014.eamt-1.7,P10-2041,0,\N,Missing
2014.eamt-1.7,W11-2131,0,\N,Missing
2014.eamt-1.7,P07-2045,0,\N,Missing
2014.eamt-1.7,D07-1104,0,\N,Missing
2014.eamt-1.7,D07-1036,0,\N,Missing
2014.eamt-1.7,P11-1052,0,\N,Missing
2014.eamt-1.7,P10-1088,0,\N,Missing
2014.eamt-1.7,P05-1032,0,\N,Missing
2014.eamt-1.7,W04-3250,0,\N,Missing
2014.eamt-1.7,N13-1086,0,\N,Missing
2014.eamt-1.7,2012.eamt-1.65,0,\N,Missing
2014.eamt-1.7,2005.eamt-1.19,0,\N,Missing
2014.eamt-1.7,2005.iwslt-1.7,0,\N,Missing
2020.acl-main.448,W14-3302,0,0.111226,"Missing"
2020.acl-main.448,E06-1032,0,0.372381,"Missing"
2020.acl-main.448,W14-3333,1,0.766456,"researchers use metric scores to compare pairs of MT systems, for instance when claiming a new state of the art, evaluating different model architectures, or even in deciding whether to publish. Basing these judgements on metric score alone runs the risk of making wrong decisions with respect to the true gold standard of human judgements. That is, while a change may result in a significant improvement in BLEU, this may not be judged to be an improvement by human assessors. Thus, we examine whether metrics agree with DA on all the MT systems pairs across all languages used in WMT 19. Following Graham et al. (2014), we use statistiMetric Dierence 5 BLEU NS Worse NS Better NS 0.0-2.5 2.5-5.0 5.0-7.5 7.5-15.0 15.0-30.0 30.0-200.0 −0.5 0.0 0.5 1.0 1.5 2.0 DA Dierence Figure 4: Pairwise differences in human DA evaluation (x-axis) compared to difference in metric evaluation (binned on y-axis; NS means insignificant metric difference). The colours indicate pairs judged by humans to be insignificantly different (cyan/light gray), significantly worse (red/dark gray on the left) and significantly better (green/dark gray on the right). 4990 cal significance tests to detect if the difference in scores (human or"
2021.conll-1.38,S19-1012,0,0.399818,"eneral human knowledge ronment – a resource often called commonsense include inventories of machine-readable logical knowledge (Liu and Singh, 2004). Advances in ar- rules (Gordon and Hobbs, 2017) and large, cutificial intelligence in general, and natural language rated databases which have been collected with processing in particular, have led to a surge of in- the specific purpose to reflect either domain genterest in its nature: what constitutes commonsense eral (e.g., ConceptNet; Liu and Singh (2004)) or knowledge? And despite this knowledge being domain-specific (e.g., ATOMIC; Sap et al. (2019)) rarely explicitly stated in text (Gordon and Durme, commonsense knowledge. 2013), how can we equip machines with commonWord association norms are a third resource sense to enable more general inference (Davis and of explicit, basic human knowledge: in a typical 1 study, human participants are presented with a cue Code available at https://github.com/ ChunhuaLiu596/CSWordAssociation word (e.g., ‘camping’) and asked to produce one or 481 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 481–495 November 10–11, 2021. ©2021 Association for Computational"
2021.conll-1.38,D17-1184,0,0.0671556,"Missing"
2021.conll-1.38,P18-1213,0,0.0645334,"Missing"
2021.conll-1.38,2021.emnlp-main.81,0,0.0420899,"Missing"
2021.conll-1.38,2021.findings-acl.102,0,0.0411971,"Commonsense QA In this section, we explore the utility of commonsense knowledge in ConceptNet and SWOW in commonsense question answering (CQA) tasks. We incorporate the two KGs into representative and competitive CQA models from the recent literature (Wang et al., 2020; Feng et al., 2020), and apply them to three benchmark data sets. We emphasize that the goal of this study is not competing on leaderboards. Current state-of-the-art models leverage very large language models with billions of parameters (Khashabi et al., 2020), and often draw on additional external resource such as Wiktionary (Xu et al., 2021). Instead, we explore the utility of SWOW and ConceptNet in a selection of representative moderately complex models. 5.1 Experimental setup Datasets We consider three standard multiplechoice CQA benchmark datasets. CommonsenseQA (CSQA; Talmor et al. (2019)) contains commonsense questions generated by crowd workers on the basis of sub-graphs in ConceptNet, giving ConceptNet an inherent advantage over SWOW. The QA-pairs in this dataset require various commonsense skills, and distractor answers were carefully selected to share semantic associations with the key concepts in a question. OpenBookQA"
2021.conll-1.38,N19-1421,0,0.0200109,"(Wang et al., 2020; Feng et al., 2020), and apply them to three benchmark data sets. We emphasize that the goal of this study is not competing on leaderboards. Current state-of-the-art models leverage very large language models with billions of parameters (Khashabi et al., 2020), and often draw on additional external resource such as Wiktionary (Xu et al., 2021). Instead, we explore the utility of SWOW and ConceptNet in a selection of representative moderately complex models. 5.1 Experimental setup Datasets We consider three standard multiplechoice CQA benchmark datasets. CommonsenseQA (CSQA; Talmor et al. (2019)) contains commonsense questions generated by crowd workers on the basis of sub-graphs in ConceptNet, giving ConceptNet an inherent advantage over SWOW. The QA-pairs in this dataset require various commonsense skills, and distractor answers were carefully selected to share semantic associations with the key concepts in a question. OpenBookQA (OBQA; Mihaylov et al. (2018)) consists of question-answer pairs along with paragraphs from elementary-level science books. Following previous work (Wang et al., 2020; Feng et al., 2020), we disregard the paragraphs, and apply our models to question-answer"
2021.eacl-main.233,D18-1337,0,0.442044,"research on S I MT relies on a strategy to decide when to read a word from the input or write a word to the output (Satija and Pineau, 2016; Gu et al., 2017). This is based on a sequential decision making formulation of S I MT, where the decision making about the next R EAD/W RITE action is made by an agent, interacting with the neural machine translation (NMT) environment. Current approaches are sub-optimal as they either fix the agent’s policy to focus learning the NMT model (Ma et al., 2019; Dalvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challenging as we need to optimize We present an IL approach to efficiently learn effective coupled programmer-interpreter policies in S I MT, based on the following contributions. First, we present a simple, fast, and effective algorithmic oracle to produce oracle actions from the training bilingual sentence-pairs based on statistical word alignments (Brown et al., 1993). Next, we design a framework that uses scheduled sampling on both programmer and interpreter. This i"
2021.eacl-main.233,P19-1126,0,0.62478,"alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fraction of read source words per emited target words. Second, average lagging (AL) (Ma et al., 2019) is an average number of lagged source words until all inputs are read. Finally the differentiable-AL (DAL) (Arivazhagan et al., 2019) is a refinement of AL which also accumulates the cost of writing output tokens after inputs are fully read. Baseline. We compare against the wait-k baseline (Ma et al., 2019) where the programmer’s policy begins with k numbers of R EAD, and is followed by switching W RITE and R EAD, until the source sentence is exhausted or end of sentence (EOS) symbol is written. If the source sentence is exhausted, the programmer will only emit W RITE actions. This baseline was shown to be superior compared to the reinforcement learning approach (Zheng et al., 2019a), and k can be tuned for the desired dela"
2021.eacl-main.233,2020.iwslt-1.27,0,0.364956,"proach, showing the scalability of our approach. 5 Related Work Satija and Pineau (2016); Gu et al. (2017) and Alinejad et al. (2018) formulate simultane5 Because of the limitation of our computational resources, we are unable to use multiple GPUs for larger batch size. Using smaller batch size is known to reduce the overall performance of the transformer. 2716 ous NMT as sequential decision making problem where an agent interacts with the environment (i.e. the underlying NMT model) through R EAD/W RITE actions. They pre-train the NMT system, while the agent’s policy is trained using deep-RL. Arivazhagan et al. (2020) highlights the poor performance of finetuned offline translation model when translating prefixes of input, which is the case of S I MT. Their approach uses retranslation strategy where every R EAD is performed, a new translation is generated from scratch, allowing revising translation on the fly and mitigating error propagation on the decoder that was attributed to the insufficient evidence when generating past output words. Their approach uses a stability metric which takes number of suffixes revisions made to produce latest translation. This approach involves wait-k inference, which limits"
2021.eacl-main.233,J93-2003,0,0.19235,"lvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challenging as we need to optimize We present an IL approach to efficiently learn effective coupled programmer-interpreter policies in S I MT, based on the following contributions. First, we present a simple, fast, and effective algorithmic oracle to produce oracle actions from the training bilingual sentence-pairs based on statistical word alignments (Brown et al., 1993). Next, we design a framework that uses scheduled sampling on both programmer and interpreter. This is different from the typical IL scenarios, where there is only one policy to learn. As the two policies collaborate, their learning needs to be robust not only to their own incorrect predictions, but also to incorrect predictions of the other policy to mitigate this coupled exposure bias. Experiments on six language pairs (translating to English from Arabic, Czech, German, Romanian, Hungarian, and Bulgarian) show the policies trained using our approach compares favorably with strong policies fr"
2021.eacl-main.233,2012.eamt-1.60,0,0.0168744,"quality corpus which is designed for spoken dialogue and carefully edited dataset. Additionally, we perform a single large scale experiment using crawled corpus such as WMT to show that our method also scales to a large dataset. We evaluate our proposed method on 6 language pairs, in all cases translating into English, with the source languages chosen to cover a wide range 2 Our oracle algorithm’s code is released in https://github.com/Monash-NLP-ML-Group/ arthur-eacl2021. of language families and syntax. We use German (DE), Czech (CS) and Arabic (AR) from the IWSLT 2016 translation dataset (Cettolo et al., 2012). We use the provided training and development sets as-is, and concatenate all provided test sets to create our test set. We also evaluate Hungarian (HR), Bulgarian (BG), and Romanian (RO) from the SETIMES corpus (Tyers and Alperen, 2010). As this corpus is not partitioned, we use the majority of the data for training, holding out 2000 random sentence pairs for development and another 2000 sentence pairs for testing. Together these languages are representative of Germanic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richa"
2021.eacl-main.233,N18-2079,0,0.487912,"needs to trade off delaying translation output and the quality of the generated translation. Recent research on S I MT relies on a strategy to decide when to read a word from the input or write a word to the output (Satija and Pineau, 2016; Gu et al., 2017). This is based on a sequential decision making formulation of S I MT, where the decision making about the next R EAD/W RITE action is made by an agent, interacting with the neural machine translation (NMT) environment. Current approaches are sub-optimal as they either fix the agent’s policy to focus learning the NMT model (Ma et al., 2019; Dalvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challenging as we need to optimize We present an IL approach to efficiently learn effective coupled programmer-interpreter policies in S I MT, based on the following contributions. First, we present a simple, fast, and effective algorithmic oracle to produce oracle actions from the training bilingual sentence-pairs based on statistical word alignments (Brown et al., 199"
2021.eacl-main.233,N13-1073,0,0.10195,"d Romanian (RO) from the SETIMES corpus (Tyers and Alperen, 2010). As this corpus is not partitioned, we use the majority of the data for training, holding out 2000 random sentence pairs for development and another 2000 sentence pairs for testing. Together these languages are representative of Germanic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richardson, 2018) to build and tokenize our training data with 16k vocabulary size. Then we generate our oracle program actions based on the segmented tokens. We use fast_align (Dyer et al., 2013) to generate symmetrized alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fraction of read source words per emited target words. Second, average lagging (AL) (Ma et al., 2019) is an average number of lagged source words until all inputs are read. Finally the"
2021.eacl-main.233,D14-1140,0,0.176588,"Missing"
2021.eacl-main.233,E17-1099,0,0.20855,"te sequence of R EAD/W RITE actions with low translation latency and high translation quality is under-explored. Introduction Simultaneous machine translation (S I MT) is a setting where the translator needs to incrementally generate the translation while the source utterance is being received. This is a challenging translation scenario as the S I MT model needs to trade off delaying translation output and the quality of the generated translation. Recent research on S I MT relies on a strategy to decide when to read a word from the input or write a word to the output (Satija and Pineau, 2016; Gu et al., 2017). This is based on a sequential decision making formulation of S I MT, where the decision making about the next R EAD/W RITE action is made by an agent, interacting with the neural machine translation (NMT) environment. Current approaches are sub-optimal as they either fix the agent’s policy to focus learning the NMT model (Ma et al., 2019; Dalvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challengin"
2021.eacl-main.233,N03-1017,0,0.285791,"Missing"
2021.eacl-main.233,D18-2012,0,0.0178505,"et al., 2012). We use the provided training and development sets as-is, and concatenate all provided test sets to create our test set. We also evaluate Hungarian (HR), Bulgarian (BG), and Romanian (RO) from the SETIMES corpus (Tyers and Alperen, 2010). As this corpus is not partitioned, we use the majority of the data for training, holding out 2000 random sentence pairs for development and another 2000 sentence pairs for testing. Together these languages are representative of Germanic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richardson, 2018) to build and tokenize our training data with 16k vocabulary size. Then we generate our oracle program actions based on the segmented tokens. We use fast_align (Dyer et al., 2013) to generate symmetrized alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fracti"
2021.eacl-main.233,D15-1166,0,0.0411061,"Missing"
2021.eacl-main.233,W18-6322,0,0.0338959,"Missing"
2021.eacl-main.233,P02-1040,0,0.109673,"anic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richardson, 2018) to build and tokenize our training data with 16k vocabulary size. Then we generate our oracle program actions based on the segmented tokens. We use fast_align (Dyer et al., 2013) to generate symmetrized alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fraction of read source words per emited target words. Second, average lagging (AL) (Ma et al., 2019) is an average number of lagged source words until all inputs are read. Finally the differentiable-AL (DAL) (Arivazhagan et al., 2019) is a refinement of AL which also accumulates the cost of writing output tokens after inputs are fully read. Baseline. We compare against the wait-k baseline (Ma et al., 2019) where the programmer’s policy begins with k numbers of R EAD, and is followed"
2021.eacl-main.233,W18-6319,0,0.0262644,"Missing"
2021.eacl-main.233,D19-1137,0,0.813646,"ITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality while keeping the translation delay low. 1 Previous research has considered the use of imitation learning (IL) to train the agent’s policy (Zheng et al., 2019a,b), which is generally superior to reinforcement Learning (RL) in terms of the stability and sample complexity. However, the bottleneck of IL in S I MT is the unavailability of the oracle sequence of actions. Designing algorithmic oracles to compute sequence of R EAD/W RITE actions with low translation latency and high translation quality is under-explored. Introduction Simultaneous machine translation (S I MT) is a setting where the translator needs to incrementally generate the translation while the source utterance is being received. This is a challenging translation scenario as the S I M"
2021.eacl-main.233,P19-1582,0,0.388709,"ITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality while keeping the translation delay low. 1 Previous research has considered the use of imitation learning (IL) to train the agent’s policy (Zheng et al., 2019a,b), which is generally superior to reinforcement Learning (RL) in terms of the stability and sample complexity. However, the bottleneck of IL in S I MT is the unavailability of the oracle sequence of actions. Designing algorithmic oracles to compute sequence of R EAD/W RITE actions with low translation latency and high translation quality is under-explored. Introduction Simultaneous machine translation (S I MT) is a setting where the translator needs to incrementally generate the translation while the source utterance is being received. This is a challenging translation scenario as the S I M"
2021.eacl-main.239,D16-1120,0,0.171996,"Missing"
2021.eacl-main.239,D18-1002,0,0.451424,"optimization of the encoder incorporates two parts: (1) minimizing the main loss, and (2) maximizing the attacker loss (i.e., preventing protected attributes from being detected by the attacker). Preventing protected attributes from being detected tends to result in fairer models, as protected attributes will more likely be independent rather than confounding variables. Although this method leads to demonstrably less biased models, there are still limitations, most notably that significant protected information still remains in the model’s encodings and prediction outputs (Wang et al., 2019; Elazar and Goldberg, 2018). Many different approaches have been proposed to strengthen the attacker, including: increasing the discriminator hidden dimensionality; assigning different weights to the adversarial component during training; using an ensemble of adversaries with different initializations; and reinitializing the adversarial weights every t epochs (Elazar and Goldberg, 2018). Of these, the ensemble method has been shown to perform best, but independently-trained attackers can generally still detect private information after adversarial removal. In this paper, we adopt adversarial debiasing approaches and pre"
2021.eacl-main.239,D17-1169,0,0.057939,"sentiment classification, we merge the original 64 labels into two categories based on the results of hierarchical clustering. null-space, h∗M = PN (Alinear ) hM , where PN (Alinear ) is the null-space projection matrix of Alinear . In doing so, it becomes difficult for the protected attribute to be linearly identified from the projected hidden representations (h∗M ), and any linear ∗ ) trained on h∗ can thus main-task classifier (CM M be expected to make fairer predictions. 3 Experiments Fixed Encoder Following Elazar and Goldberg (2018) and Ravfogel et al. (2020), we use the DeepMoji model (Felbo et al., 2017) as a fixedparameter encoder (i.e. it is not updated during training). The DeepMoji model is trained over 1246 million tweets containing one of 64 common emojis. We merge the 64 emoji labels output by DeepMoji into two super-classes based on hierarchical clustering: ‘happy’ and ‘sad’. Models The encoder EM consists of a fixed pretrained encoder (DeepMoji) and two trainable fully connected layers (“Standard” in Table 1). Every linear classifier (C) is implemented as a dense layer. For protected attribute prediction, a discriminator (A) is a 3-layer MLP where the first 2 layers are collectively"
2021.eacl-main.239,N18-2076,1,0.751304,"Missing"
2021.eacl-main.239,2020.acl-main.647,0,0.448739,"s, resulting in less biased models than the standard ensemble-based adversarial method. According to Bousmalis et al. (2016), the difference loss has the additional advantage of also being minimized when hidden representations shrink to zero. Therefore, instead of minimizing the difference loss by learning rotated hidden representations (i.e., the same model), this method biases adversaries to have representations that are a) orthogonal, and b) low magnitude; the degree to which is given by weight decay of the optimization function. 2.3 INLP We include Iterative Null-space Projection (“INLP”: Ravfogel et al. (2020)) as a baseline method for mitigating bias in trained models, in addition to standard and ensemble adversarial methods. In INLP, a linear discriminator (Alinear ) of the protected attribute is iteratively trained from pre-computed fixed hidden representations (i.e., hM ) to project them onto the linear discriminator’s 2761 Model Accuracy↑ TPR Gap↓ TNR Gap↓ Leakage@h↓ Leakage@ˆ y↓ Random Fixed Encoder 50.00±0.00 61.44±0.00 0.00±0.00 0.52±0.00 0.00±0.00 17.97±0.00 — 92.07±0.00 — 86.93±0.00 Standard 71.59±0.05 31.81±0.29 48.41±0.27 85.56±0.20 70.09±0.19 INLP Adv Single Discriminator Adv Ensemble"
2021.eacl-main.239,P15-2079,0,0.0259523,"f reducing bias and the stability of training. 1 Introduction While NLP models have achieved great successes, results can depend on spurious correlations with protected attributes of the authors of a given text, such as gender, age, or race. Including protected attributes in models can lead to problems such as leakage of personally-identifying information of the author (Li et al., 2018a), and unfair models, i.e., models which do not perform equally well for different sub-classes of user. This kind of unfairness has been shown to exist in many different tasks, including part-of-speech tagging (Hovy and Søgaard, 2015) and sentiment analysis (Kiritchenko and Mohammad, 2018). One approach to diminishing the influence of protected attributes is to use adversarial methods, where an encoder attempts to prevent a discriminator from identifying the protected attributes in a given task (Li et al., 2018a). Specifically, an adversarial network is made up of an attacker and encoder, where the attacker detects protected information in the representation of the encoder, and the optimization of the encoder incorporates two parts: (1) minimizing the main loss, and (2) maximizing the attacker loss (i.e., preventing protec"
2021.eacl-main.239,S18-2005,0,0.0428432,"Introduction While NLP models have achieved great successes, results can depend on spurious correlations with protected attributes of the authors of a given text, such as gender, age, or race. Including protected attributes in models can lead to problems such as leakage of personally-identifying information of the author (Li et al., 2018a), and unfair models, i.e., models which do not perform equally well for different sub-classes of user. This kind of unfairness has been shown to exist in many different tasks, including part-of-speech tagging (Hovy and Søgaard, 2015) and sentiment analysis (Kiritchenko and Mohammad, 2018). One approach to diminishing the influence of protected attributes is to use adversarial methods, where an encoder attempts to prevent a discriminator from identifying the protected attributes in a given task (Li et al., 2018a). Specifically, an adversarial network is made up of an attacker and encoder, where the attacker detects protected information in the representation of the encoder, and the optimization of the encoder incorporates two parts: (1) minimizing the main loss, and (2) maximizing the attacker loss (i.e., preventing protected attributes from being detected by the attacker). Pre"
2021.eacl-main.239,P18-2005,1,0.88064,"minators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and the stability of training. 1 Introduction While NLP models have achieved great successes, results can depend on spurious correlations with protected attributes of the authors of a given text, such as gender, age, or race. Including protected attributes in models can lead to problems such as leakage of personally-identifying information of the author (Li et al., 2018a), and unfair models, i.e., models which do not perform equally well for different sub-classes of user. This kind of unfairness has been shown to exist in many different tasks, including part-of-speech tagging (Hovy and Søgaard, 2015) and sentiment analysis (Kiritchenko and Mohammad, 2018). One approach to diminishing the influence of protected attributes is to use adversarial methods, where an encoder attempts to prevent a discriminator from identifying the protected attributes in a given task (Li et al., 2018a). Specifically, an adversarial network is made up of an attacker and encoder, whe"
2021.eacl-main.254,Q16-1022,0,0.0512048,"Missing"
2021.eacl-main.254,N19-1253,0,0.148019,"Missing"
2021.eacl-main.254,K19-1035,0,0.242737,"the approach fully unsupervised.2 Recent work has shown that it is possible to outperform direct transfer if unlabelled data, either in the target lan∗ Work done outside Amazon. https://github.com/kmkurn/ ppt-eacl2021 1 2 Direct transfer is also called zero-shot transfer or model transfer in the literature. 2907 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2907–2918 April 19 - 23, 2021. ©2021 Association for Computational Linguistics guage or a different auxiliary language, is available (He et al., 2019; Meng et al., 2019; Ahmad et al., 2019b). Here, we focus on the former setting and present flexible methods that can adapt a pre-trained parser given unlabelled target data. Despite their success in outperforming direct transfer by leveraging unlabelled data, current approaches have several drawbacks. First, they are limited to generative and projective parsers. However, discriminative parsers have proven more effective, and non-projectivity is a prevalent phenomenon across the world’s languages (de Lhoneux, 2019). Second, prior methods are restricted to single-source transfer, however, transfer from multiple source languages has"
2021.eacl-main.254,Q17-1010,0,0.0444826,"Missing"
2021.eacl-main.254,K15-1012,1,0.881609,"ethods that can adapt a pre-trained parser given unlabelled target data. Despite their success in outperforming direct transfer by leveraging unlabelled data, current approaches have several drawbacks. First, they are limited to generative and projective parsers. However, discriminative parsers have proven more effective, and non-projectivity is a prevalent phenomenon across the world’s languages (de Lhoneux, 2019). Second, prior methods are restricted to single-source transfer, however, transfer from multiple source languages has been shown to lead to superior results (McDonald et al., 2011; Duong et al., 2015a; Rahimi et al., 2019). Third, they assume access to the source language data, which may not be possible because of privacy or legal reasons. In such source-free transfer, only a pre-trained source parser may be provided. We address the three shortcomings with an alternative method for unsupervised target language adaptation (Section 2). Our method uses high probability edge predictions of the source parser as a supervision signal in a self-training algorithm, thus enabling unsupervised training on the target language data. The method is feasible for discriminative and non-projective parsing,"
2021.eacl-main.254,P15-2139,1,0.929054,"ethods that can adapt a pre-trained parser given unlabelled target data. Despite their success in outperforming direct transfer by leveraging unlabelled data, current approaches have several drawbacks. First, they are limited to generative and projective parsers. However, discriminative parsers have proven more effective, and non-projectivity is a prevalent phenomenon across the world’s languages (de Lhoneux, 2019). Second, prior methods are restricted to single-source transfer, however, transfer from multiple source languages has been shown to lead to superior results (McDonald et al., 2011; Duong et al., 2015a; Rahimi et al., 2019). Third, they assume access to the source language data, which may not be possible because of privacy or legal reasons. In such source-free transfer, only a pre-trained source parser may be provided. We address the three shortcomings with an alternative method for unsupervised target language adaptation (Section 2). Our method uses high probability edge predictions of the source parser as a supervision signal in a self-training algorithm, thus enabling unsupervised training on the target language data. The method is feasible for discriminative and non-projective parsing,"
2021.eacl-main.254,C96-1058,0,0.0323689,"illustrated in Fig. 1. Since Y(x) contains an exponential number of trees, efficient algorithms are required to compute the partition function Z(x), arc marginal probabilities, and the highest scoring tree. First, arc marginal probabilities can be computed efficiently with dynamic programming for projective trees (Paskin, 2001) and Matrix-Tree Theorem for the non-projective counterpart (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). The same algorithms can also be employed to compute Z(x). Next, the highest scoring tree can be obtained efficiently with Eisner’s algorithm (Eisner, 1996) or the maximum spanning tree algorithm (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) for the projective and non-projective cases, respectively. The transfer is performed by initialising the target parser with the source parser’s parameters and then fine-tuning it with the training loss in Eq. (1) on the target language data. Following previous works (Duong et al., 2015b; He et al., 2019), we also regularise the parameters towards the initial parameters to prevent them from deviating too much since the source parser is already good to begin with. Thus, the final fine-tuning loss be"
2021.eacl-main.254,P15-1119,0,0.0291149,"limited for low-resource languages or may have a poor domain match. Additionally, these methods involve training the parser from scratch for every new target language, which may be prohibitive. On the other hand, there are methods that operate on the model level. A typical approach is direct transfer (aka., zero-shot transfer) which trains a parser on source language data, and then directly uses it to parse a target language. This approach is enabled by the shared input representation between the source and target language such as POS tags (Zeman and Resnik, 2008) or cross-lingual embeddings (Guo et al., 2015; Ahmad et al., 2019a). Direct transfer supports source-free transfer and only requires training a parser once on the source language data. In other words, direct transfer is unsupervised as far as target language resources. Previous work has shown that unsupervised target language adaptation outperforms direct transfer. Recent work by He et al. (2019) used a neural lexicalised dependency model with valence (DMV) (Klein and Manning, 2004) as the source parser and fine-tuned it in an unsupervised manner on the unlabelled target language data. This adaptation method allows for source-free transf"
2021.eacl-main.254,P19-1311,0,0.300747,"lled target language data, rendering the approach fully unsupervised.2 Recent work has shown that it is possible to outperform direct transfer if unlabelled data, either in the target lan∗ Work done outside Amazon. https://github.com/kmkurn/ ppt-eacl2021 1 2 Direct transfer is also called zero-shot transfer or model transfer in the literature. 2907 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2907–2918 April 19 - 23, 2021. ©2021 Association for Computational Linguistics guage or a different auxiliary language, is available (He et al., 2019; Meng et al., 2019; Ahmad et al., 2019b). Here, we focus on the former setting and present flexible methods that can adapt a pre-trained parser given unlabelled target data. Despite their success in outperforming direct transfer by leveraging unlabelled data, current approaches have several drawbacks. First, they are limited to generative and projective parsers. However, discriminative parsers have proven more effective, and non-projectivity is a prevalent phenomenon across the world’s languages (de Lhoneux, 2019). Second, prior methods are restricted to single-source transfer, however, trans"
2021.eacl-main.254,2020.acl-main.560,0,0.0622584,", a source parser Pθ0 predicts a ˜ i ) (subset shown in the figset of candidate arcs A(x ˜ ure), and parses Y (xi ). The highest scoring parse is shown on the bottom (green), and the true gold parse (unknown to the parser) on top (red). A target language parser Pθ is then fine-tuned on a data set of ambiguously labelled sentences {xi , Y˜ (xi )}. Introduction Recent progress in natural language processing (NLP) has been largely driven by increasing amounts and size of labelled datasets. The majority of the world’s languages, however, are lowresource, with little to no labelled data available (Joshi et al., 2020). Predicting linguistic labels, such as syntactic dependencies, underlies many downstream NLP applications, and the most effective systems rely on labelled data. Their lack hinders the access to NLP technology in many languages. One solution is cross-lingual model transfer, which adapts models trained on highresource languages to low-resource ones. This paper presents a flexible framework for cross-lingual transfer of syntactic dependency parsers which can leverage any pre-trained arc-factored dependency parser, and assumes no access to labelled target language data. One straightforward method"
2021.eacl-main.254,P04-1061,0,0.0604628,"approach is enabled by the shared input representation between the source and target language such as POS tags (Zeman and Resnik, 2008) or cross-lingual embeddings (Guo et al., 2015; Ahmad et al., 2019a). Direct transfer supports source-free transfer and only requires training a parser once on the source language data. In other words, direct transfer is unsupervised as far as target language resources. Previous work has shown that unsupervised target language adaptation outperforms direct transfer. Recent work by He et al. (2019) used a neural lexicalised dependency model with valence (DMV) (Klein and Manning, 2004) as the source parser and fine-tuned it in an unsupervised manner on the unlabelled target language data. This adaptation method allows for source-free transfer and performs especially well on distant target languages. A different approach is proposed by Meng et al. (2019), who gathered target language corpus statistics to derive constraints to guide inference using the source parser. Thus, this technique also allows for source-free transfer. A different method is proposed by Ahmad et al. (2019b) who explored the use of unlabelled data from an auxiliary language, which can be different from th"
2021.eacl-main.254,D07-1015,0,0.0484902,"a threshold σ (T¨ackstr¨om et al., 2013). The predicted tree from the source parser is also included in Y˜ (x) so the chart is never empty. This prediction is simply the highest scoring tree. This procedure is illustrated in Fig. 1. Since Y(x) contains an exponential number of trees, efficient algorithms are required to compute the partition function Z(x), arc marginal probabilities, and the highest scoring tree. First, arc marginal probabilities can be computed efficiently with dynamic programming for projective trees (Paskin, 2001) and Matrix-Tree Theorem for the non-projective counterpart (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). The same algorithms can also be employed to compute Z(x). Next, the highest scoring tree can be obtained efficiently with Eisner’s algorithm (Eisner, 1996) or the maximum spanning tree algorithm (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) for the projective and non-projective cases, respectively. The transfer is performed by initialising the target parser with the source parser’s parameters and then fine-tuning it with the training loss in Eq. (1) on the target language data. Following previous works (Duong et al., 2015b; He et"
2021.eacl-main.254,N16-1121,0,0.0183639,"ng data whose size is roughly the same as that of the training data of PPTXEN 5 source parsers. In other words, the training data size is roughly equal to 1/5 of the English training set. To obtain this data, we ranRelated Work Cross-lingual dependency parsing has been extensively studied in NLP. The approaches can be grouped into two main categories. On the one hand, there are approaches that operate on the data level. Examples of this category include annotation projection, which aims to project dependency trees from a source language to a target language (Hwa et al., 2005; Li et al., 2014; Lacroix et al., 2016; Zhang et al., 2019); and source treebank reordering, which manipulates the source language treebank to obtain another treebank whose statistics approximately match those of the target language (Wang and Eisner, 2018; Rasooli and Collins, 2019). Both methods have no restriction on the type of parsers as they are only concerned with the data. Transferring from multiple source languages with annotation projection is also feasible (Agi´c et al., 2016). Despite their effectiveness, these data-level methods may require access to the source language data, hence are unusable when it is inaccessible"
2021.eacl-main.254,C14-1075,0,0.0601961,"Missing"
2021.eacl-main.254,H05-1066,0,0.328503,"Missing"
2021.eacl-main.254,D11-1006,0,0.0222114,"Missing"
2021.eacl-main.254,W07-2216,0,0.0359128,"ckstr¨om et al., 2013). The predicted tree from the source parser is also included in Y˜ (x) so the chart is never empty. This prediction is simply the highest scoring tree. This procedure is illustrated in Fig. 1. Since Y(x) contains an exponential number of trees, efficient algorithms are required to compute the partition function Z(x), arc marginal probabilities, and the highest scoring tree. First, arc marginal probabilities can be computed efficiently with dynamic programming for projective trees (Paskin, 2001) and Matrix-Tree Theorem for the non-projective counterpart (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). The same algorithms can also be employed to compute Z(x). Next, the highest scoring tree can be obtained efficiently with Eisner’s algorithm (Eisner, 1996) or the maximum spanning tree algorithm (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) for the projective and non-projective cases, respectively. The transfer is performed by initialising the target parser with the source parser’s parameters and then fine-tuning it with the training loss in Eq. (1) on the target language data. Following previous works (Duong et al., 2015b; He et al., 2019), we also regula"
2021.eacl-main.254,W15-2137,0,0.0225104,"strong positive correlation (r = 0.57) between the amount of 2911 leakage and accuracy. The same trend occurs with DT, ST, and PPT. This finding suggests that crosslingual parsing is also affected by treebank leakage just like monolingual parsing is, which may present an opportunity to find good sources for transfer. Use of Gold POS Tags As we explained in Section 3.1, we restrict our experiments to gold POS tags for comparison with prior work. However, the use of gold POS tags does not reflect a realistic low-resource setting where one may have to resort to automatically predicted POS tags. Tiedemann (2015) has shown that cross-lingual delexicalised parsing performance degrades when predicted POS tags are used. The degradation ranges from 2.9 to 8.4 LAS points depending on the target language. Thus, our reported numbers in Table 1 are likely to decrease as well if predicted tags are used, although we expect the decline is not as sharp because our parser is lexicalised. 3.4 Parsimonious Selection of Sources for PPTX In our main experiment, we use all available languages as source for PPTX in a leave-one-out setting. Such a setting may be justified to cover as many syntactic biases as possible, ho"
2021.eacl-main.254,D19-1103,0,0.0244309,"Missing"
2021.eacl-main.254,D18-1163,0,0.0117628,"elated Work Cross-lingual dependency parsing has been extensively studied in NLP. The approaches can be grouped into two main categories. On the one hand, there are approaches that operate on the data level. Examples of this category include annotation projection, which aims to project dependency trees from a source language to a target language (Hwa et al., 2005; Li et al., 2014; Lacroix et al., 2016; Zhang et al., 2019); and source treebank reordering, which manipulates the source language treebank to obtain another treebank whose statistics approximately match those of the target language (Wang and Eisner, 2018; Rasooli and Collins, 2019). Both methods have no restriction on the type of parsers as they are only concerned with the data. Transferring from multiple source languages with annotation projection is also feasible (Agi´c et al., 2016). Despite their effectiveness, these data-level methods may require access to the source language data, hence are unusable when it is inaccessible due to privacy or legal reasons. In such source-free transfer, only a model pre-trained on the source language data is available. By leveraging parallel data, annotation projection is indeed feasible without ac2914 ce"
2021.eacl-main.254,P19-1015,1,0.895165,"Missing"
2021.eacl-main.254,N19-1385,0,0.156838,"Missing"
2021.eacl-main.254,I08-3008,0,0.0508395,"he source language data. That said, parallel data is limited for low-resource languages or may have a poor domain match. Additionally, these methods involve training the parser from scratch for every new target language, which may be prohibitive. On the other hand, there are methods that operate on the model level. A typical approach is direct transfer (aka., zero-shot transfer) which trains a parser on source language data, and then directly uses it to parse a target language. This approach is enabled by the shared input representation between the source and target language such as POS tags (Zeman and Resnik, 2008) or cross-lingual embeddings (Guo et al., 2015; Ahmad et al., 2019a). Direct transfer supports source-free transfer and only requires training a parser once on the source language data. In other words, direct transfer is unsupervised as far as target language resources. Previous work has shown that unsupervised target language adaptation outperforms direct transfer. Recent work by He et al. (2019) used a neural lexicalised dependency model with valence (DMV) (Klein and Manning, 2004) as the source parser and fine-tuned it in an unsupervised manner on the unlabelled target language data. This a"
2021.eacl-main.254,D19-1092,0,0.0249497,"Missing"
2021.eacl-main.254,2020.acl-demos.38,0,0.0719403,"effect of projectivity on the performance of our methods. We emulate a projective parser by restricting the trees in Y˜ (x) to be projective. In other words, the sum in Eq. (1) is performed only over projective trees. At test time, we search for the highest scoring projective tree. We compare DT, PPT, and PPTX-PRAG, and report LAS on Indonesian (id) and Croatian (hr) as distant languages, and on French (fr) and Dutch (nl) as nearby languages. The trend for UAS and on the other languages is similar. We use the dynamic programming implementation provided by torch-struct for the projective case (Rush, 2020). We find that it consumes more memory than our Matrix-Tree Theorem implementation, so we set the length cutoff to 20 tokens.9 Table 2 shows result of our experiment, which suggests that there is no significant performance difference between the projective and non-projective 2913 9 Hyperparameters are tuned; values are shown in Table 5. Model DT PPT PPTXEN 5 PPTX-PRAGS PPTX-PRAG Target ar es 28.1 30.8 30.9 36.5 36.5 64.1 67.3 66.3 70.3 71.9 Table 3: Comparison of LAS on Arabic and Spanish on the development set, averaged over 5 runs. PPTXEN 5 is PPTX with 5 English parsers as source, each trai"
2021.eacl-main.254,D07-1014,0,0.0785851,"e predicted tree from the source parser is also included in Y˜ (x) so the chart is never empty. This prediction is simply the highest scoring tree. This procedure is illustrated in Fig. 1. Since Y(x) contains an exponential number of trees, efficient algorithms are required to compute the partition function Z(x), arc marginal probabilities, and the highest scoring tree. First, arc marginal probabilities can be computed efficiently with dynamic programming for projective trees (Paskin, 2001) and Matrix-Tree Theorem for the non-projective counterpart (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). The same algorithms can also be employed to compute Z(x). Next, the highest scoring tree can be obtained efficiently with Eisner’s algorithm (Eisner, 1996) or the maximum spanning tree algorithm (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) for the projective and non-projective cases, respectively. The transfer is performed by initialising the target parser with the source parser’s parameters and then fine-tuning it with the training loss in Eq. (1) on the target language data. Following previous works (Duong et al., 2015b; He et al., 2019), we also regularise the parameters towa"
2021.eacl-main.254,2020.emnlp-main.220,0,0.080735,"Missing"
2021.eacl-main.254,N13-1126,0,0.0620161,"Missing"
2021.emnlp-main.155,D16-1120,0,0.0412668,"Missing"
2021.emnlp-main.155,D19-1176,0,0.0258959,"ote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019),"
2021.emnlp-main.155,D19-1236,0,0.0278975,"grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), dat"
2021.emnlp-main.155,N19-1423,0,0.0289859,"Missing"
2021.emnlp-main.155,D18-1002,0,0.0428755,"Missing"
2021.emnlp-main.155,P19-1339,0,0.0282322,"ed experiments that the proposed approaches help mitigate both class imbalance and demographic biases.1 C Figure 1: Example of a two-class problem where grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wic"
2021.emnlp-main.155,2021.eacl-main.239,1,0.819973,"ic mean of performance and fairness for Table 2, and use the results from the original paper for Table 1. We select all other models by first selecting from models with F-score at least as high 5 Acknowledgement as INLP, and then selecting the one with the lowWe thank Xudong Han for the discussions and inest GAP. We include a recent adversarial model in the varying stereotyping experiments, which per- puts. This work was funded in part by the Australian Government Research Training Program formed strongly on the class-balanced emoji data Scholarship, and the Australian Research Council. (ADV: Han et al. (2021)). Our results on varying stereotyping levels in Table 1 show that the vanilla baseline drops in performance more sharply than most proposed models, References Pinkesh Badjatiya, Manish Gupta, and Vasudeva and results in the most unfair predictions by a large Varma. 2019. Stereotypical bias removal for hate margin. LDAMiw , LDAMadv , and ADV retain speech detection task using knowledge-based genhigh F-scores but drop in fairness with increaseralizations. In The World Wide Web Conference, ing stereotyping, while INLP exhibits the opposite WWW 2019, San Francisco, CA, USA, May 13-17, pattern. LD"
2021.emnlp-main.155,2020.emnlp-main.154,0,0.0335943,"ance and demographic biases.1 C Figure 1: Example of a two-class problem where grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kenne"
2021.emnlp-main.155,2020.findings-emnlp.269,0,0.021587,"ge detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is often referred to ness caused by social biases is often aggravated by as “stereotyping” whereby one or more private at- the presence of class imbalance"
2021.emnlp-main.155,D19-1578,0,0.0201251,"majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi"
2021.emnlp-main.155,2020.acl-main.483,0,0.0270419,"2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is ofte"
2021.emnlp-main.155,2020.acl-main.45,0,0.189224,"Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is often referred to ness caused by social biases is often aggrav"
2021.emnlp-main.155,P18-2005,1,0.928794,"thin each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbala"
2021.emnlp-main.155,N19-1063,0,0.0234726,"w through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.1 C Figure 1: Example of a two-class problem where grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 20"
2021.emnlp-main.155,2020.acl-main.647,0,0.0212734,"qualised odds measure considering both TPR and TNR of classifiers, in order to address scenarios where certain subgroups are predicted more often with some classes (see Section 1). We report fairness as 1−GAP, such that higher numbers are better, and a perfectly fair model achieves 1− GAP = 1. We compare our methods against the following benchmarks: LDAM: the original LDAM model (Cao et al., 2019). LDAMcw : a variant of LDAM with instance reweighting by inverse class proportion (Cao et al., 2019). 3.1 Model Comparison We include simulated experimental settings with the emoji dataset following Ravfogel et al. (2020) where they keep the class proportions balanced, but vary group proportions (stereotyping). In our work, we systematically vary both class imbalance and stereotyping, in order to assess the robustness of the models wrt class imbalance and fairness individually. We explore three settings: varying both dimensions at the same time (Figure 2), controlling for class imbalance and vary stereotyping (Table 1), and controlling for stereotyping while varying class imbalance (Table 2). We simultaneously vary stereotyping and class imbalance in the emoji dataset, exploring several settings: • Original: t"
2021.emnlp-main.155,2021.emnlp-main.193,1,0.734072,"Missing"
2021.emnlp-main.155,D19-1670,0,0.0165387,"eller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is often referred to ness caused by social biases is often aggravated by as “stereotyping” whereby one or more private at- the presence of class imbalance (Yan et al., 2020)."
2021.emnlp-main.193,2021.eacl-main.239,1,0.694825,"Missing"
2021.emnlp-main.193,2020.lrec-1.180,0,0.0531121,"Missing"
2021.emnlp-main.193,P18-2005,1,0.835554,"cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et al., 2018; Yang et al., 2020). Notably, algorithms can be fair towards independent groups, but not 2 Background towards all intersectional groups. Despite this, debiasing approaches within NLP have so far been Debiasing with respect to more than a single proevaluated only using independent grou"
2021.emnlp-main.193,N19-1062,0,0.0196669,"ions trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et al., 2018; Yang et al., 2020). Notably, algorithms can be fair towards independent groups, but not 2 Background towards all intersectional groups. Despite this, debiasing approaches within NLP have so far been Debiasing with respect to more than a single proevaluated only using independent group fairness tected attribute (|Z |&gt; 1) requires grouping data 2"
2021.emnlp-main.193,N19-1423,0,0.0316514,"Missing"
2021.emnlp-main.193,N19-1063,0,0.0522268,"approaches based on independent groups more prone to fairness gerrymandering than methods using intersectional groups? • How do INLP and bias-constrained approaches, and their extensions to handle intersectional groups, fare compare in terms of both predictive accuracy and fairness? Text data reflects the social and cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and"
2021.emnlp-main.193,P19-1339,0,0.140807,"n independent groups more prone to fairness gerrymandering than methods using intersectional groups? • How do INLP and bias-constrained approaches, and their extensions to handle intersectional groups, fare compare in terms of both predictive accuracy and fairness? Text data reflects the social and cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et a"
2021.emnlp-main.193,2020.emnlp-main.154,0,0.369684,"e prone to fairness gerrymandering than methods using intersectional groups? • How do INLP and bias-constrained approaches, and their extensions to handle intersectional groups, fare compare in terms of both predictive accuracy and fairness? Text data reflects the social and cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et al., 2018; Yang et al."
2021.emnlp-main.193,2020.acl-main.647,0,0.175149,"B D  male (M) female (F) Figure 1: Group intersection and gerrymandering: A=white male, B=male person of colour, C=white female, D=female person of colour. when modelling datasets with multiple attributes, disregarding intersectional subgroups defined by combinations of sensitive attributes (see Figure 1). The primary goal of this work is to evaluate independent and intersectional identity debiasing approaches in relation to fairness gerrymandering for text classification tasks. To this end, we evaluate bias-constrained models (Cotter et al., 2019b) and iterative nullspace projection (INLP; Ravfogel et al. (2020)), a post-hoc debiasing method which we extend to handle intersectional groups. The constrained model jointly optimizes model performance and model fairness, while INLP seeks to learn a hidden representation which is independent of the protected attributes. INLP does not consider the trade-off between accuracy and fairness, but rather it iteratively maximizes fairness in an unconstrained fashion. In this work, we address the following questions: • Are debiasing approaches based on independent groups more prone to fairness gerrymandering than methods using intersectional groups? • How do INLP a"
2021.emnlp-main.537,N16-1111,0,0.114901,"eement. Table 1: Translation and interpretation differ in style while conveying the same source information. Introduction Simultaneous interpretation (SI) is a task of translating natural language in real time. SiMT systems are expected to generate interpreted text as if the text was produced by human interpreters while maintaining acceptable delay (Ma et al., 2019; Arthur et al., 2021). However, most current SiMT systems are trained and evaluated on offline translations differing from real-life SI scenarios where translations are flexibly paraphrased, without compromising the source message (He et al., 2016; Paulik and Waibel, 2009). For instance, in Table 1 the interpretation sentence drops &quot;at this point&quot; and condenses &quot;seriousness of this line of argument&quot; to &quot;agreement&quot;; it delivers the source message as reliably as the offline translation. Prior work attempted to build interpretation corpora in a small scale (Tohyama and Inagaki, 2004; Shimizu et al., 2014; Bernardini et al., 2016), or constructed speech interpretation training corpora for MT tasks (Paulik and Waibel, 2010). But, very little attempt has been made on empirically quantifying the evaluation gap. An exception is Shimizu et al."
2021.emnlp-main.537,shimizu-etal-2014-collection,0,0.015658,"t al., 2019; Arthur et al., 2021). However, most current SiMT systems are trained and evaluated on offline translations differing from real-life SI scenarios where translations are flexibly paraphrased, without compromising the source message (He et al., 2016; Paulik and Waibel, 2009). For instance, in Table 1 the interpretation sentence drops &quot;at this point&quot; and condenses &quot;seriousness of this line of argument&quot; to &quot;agreement&quot;; it delivers the source message as reliably as the offline translation. Prior work attempted to build interpretation corpora in a small scale (Tohyama and Inagaki, 2004; Shimizu et al., 2014; Bernardini et al., 2016), or constructed speech interpretation training corpora for MT tasks (Paulik and Waibel, 2010). But, very little attempt has been made on empirically quantifying the evaluation gap. An exception is Shimizu et al. (2013) which incorporated interpretation data in the training stage of a statistical MT system, but the lack of training data and the scale of evaluation set resulted in a marginal BLEU score difference.2 We compile a genuine interpretation test set of 1k utterances from the European Parliament (EP) Plenary focusing on German→English. We examine the real perf"
2021.emnlp-main.537,2005.mtsummit-papers.11,0,0.0446451,"ults by underlining the performance gap between evaluation on translated and interpreted texts (§4.3), and showing the effectiveness of our T2I style transfer both quantitatively and qualitatively (§4.4). We followed the instructions in Arthur et al. (2021) to preprocess data, and their hyperparameters for training all wait-k models. For style transfer models, we used the standard setup for both PBMT and HPBMT.8 4.1 Datasets We conducted evaluation investigation on four languages pairs, including German (DE), French (FR), Polish (PL), Italian (IT) → English (EN) , and used Europarl v7 corpus (Koehn, 2005) for training a SiMT model for each pair (see Table 2 for data statistics). For DE-EN, our annotated test set has 1,051 triples, for InterpretationASR and Interpretation. For the rest, we used EPTIC (Bernardini et al., 2016), a small-scale parallel corpus with data collected from the EP Plenary; it has source languages of FR, PL and IT, with 675, 463 and 480 instances, respectively. In the experiments of bridging the evaluation gap, Raw has 120,114 and 1,000 utterances for training and dev sets, while Clean has 4,240 triples, all used for training style transfer models. To train PBMT, we augme"
2021.emnlp-main.537,P07-2045,0,0.00921662,"53 13.87 14.26 10.33 13.21 13.56 13.60 Table 3: Evaluation on human annotated Translation Test, Interpretation TestASR and Interpretation Test. ∗ : performance gap. Underlined: lowest delay across systems. Bold: Best BLEU on Interpretation Test. percentage of read source tokens for every generated target token, while AL measures the number of lagged source tokens until all source tokens are read. Style Transfer Models In supervised settings, we used PBMT and HPBMT; in unsupervised settings we only used HPBMT, as PBMT requires additional paired data to find the best weights. We deployed Moses (Koehn et al., 2007) for above systems. We also experimented with a Seq2Seq (unsupervised) model (Ott et al., 2019) to compare. 4.3 Performance Gap We train separate wait-k models for the four language pairs and report the evaluation results on their corresponding Translation Test and Interpretation Test10 in Table 2. The observed significant gap of up-to 13.83 BLEU score (24.47 vs 10.64 for IT) highlights the daunting task SiMT models face in real-life SI. Interestingly, the gap for DE-EN is the lowest, and this is likely to be due to the fact that both are Germanic languages. We explored the feasibility of narr"
2021.emnlp-main.537,N03-1017,0,0.0686104,"s, where only Raw is used. Supervised training Given that our Clean set consists of roughly 4.2k triples, we opt for statistical MT systems which inherently require far less data for sequence-to-sequence mapping tasks compared to their neural counterparts. Furthermore, conducting style transfer in the same language involves word replacement and ordering, which conforms with the behaviors of SMT systems that chunk an input sequence into segments, translate, and reorder the translated chunks (Lopez, 2008). More specifically, we employ two classic statistical MT methods: phrase-based SMT (PBMT) (Koehn et al., 2003) and Hierarchical phrase-based MT (HPBMT) (Chiang, 2005).6 A similar framework was tried by Xu et al. (2012) for text simplification. We will describe the T2I pipeline process for unsupervised settings, as both settings have a similar process with different data configurations. The main difference is that we use Clean instead of Raw, which will be detailed in §4.1. Unsupervised training Figure 1 shows the three stages of our T2I approach in unsupervised settings: the first stage is to convert interpretations in Raw to translation-style data by applying roundtrip translation on interpretations,"
2021.emnlp-main.537,C12-1177,0,0.0342376,"t for statistical MT systems which inherently require far less data for sequence-to-sequence mapping tasks compared to their neural counterparts. Furthermore, conducting style transfer in the same language involves word replacement and ordering, which conforms with the behaviors of SMT systems that chunk an input sequence into segments, translate, and reorder the translated chunks (Lopez, 2008). More specifically, we employ two classic statistical MT methods: phrase-based SMT (PBMT) (Koehn et al., 2003) and Hierarchical phrase-based MT (HPBMT) (Chiang, 2005).6 A similar framework was tried by Xu et al. (2012) for text simplification. We will describe the T2I pipeline process for unsupervised settings, as both settings have a similar process with different data configurations. The main difference is that we use Clean instead of Raw, which will be detailed in §4.1. Unsupervised training Figure 1 shows the three stages of our T2I approach in unsupervised settings: the first stage is to convert interpretations in Raw to translation-style data by applying roundtrip translation on interpretations, with pretrained NMT models (Ng et al., 2019). It is expected that the outputs after this round-tripping, de"
2021.emnlp-main.537,W19-5333,0,0.0119705,"d MT (HPBMT) (Chiang, 2005).6 A similar framework was tried by Xu et al. (2012) for text simplification. We will describe the T2I pipeline process for unsupervised settings, as both settings have a similar process with different data configurations. The main difference is that we use Clean instead of Raw, which will be detailed in §4.1. Unsupervised training Figure 1 shows the three stages of our T2I approach in unsupervised settings: the first stage is to convert interpretations in Raw to translation-style data by applying roundtrip translation on interpretations, with pretrained NMT models (Ng et al., 2019). It is expected that the outputs after this round-tripping, denoted as Translation-FB, sit close to the translation domain, thus achieving the effects of interpretationto-translation. The second stage is to train a style transfer model to learn the mapping between the data points in Translation-FB and their corresponding interpretations in Raw. Lastly, we apply the trained style transfer model on offline Europarl translations and produce interpretation-like sequences which we call Pseudo-I.7 6 PBMT creates a phrase table, a reordering model and a language model, followed by tuning their weigh"
2021.emnlp-main.537,N19-4009,0,0.0145696,"nterpretation TestASR and Interpretation Test. ∗ : performance gap. Underlined: lowest delay across systems. Bold: Best BLEU on Interpretation Test. percentage of read source tokens for every generated target token, while AL measures the number of lagged source tokens until all source tokens are read. Style Transfer Models In supervised settings, we used PBMT and HPBMT; in unsupervised settings we only used HPBMT, as PBMT requires additional paired data to find the best weights. We deployed Moses (Koehn et al., 2007) for above systems. We also experimented with a Seq2Seq (unsupervised) model (Ott et al., 2019) to compare. 4.3 Performance Gap We train separate wait-k models for the four language pairs and report the evaluation results on their corresponding Translation Test and Interpretation Test10 in Table 2. The observed significant gap of up-to 13.83 BLEU score (24.47 vs 10.64 for IT) highlights the daunting task SiMT models face in real-life SI. Interestingly, the gap for DE-EN is the lowest, and this is likely to be due to the fact that both are Germanic languages. We explored the feasibility of narrowing the performance gap using our T2I method on DE-EN. Being a head-final language, German is"
2021.emnlp-main.537,2020.acl-demos.14,0,0.0136715,"ranslations had a different number of sentences. Next a manual process was applied, including removals of dialogues with non-essential contents and truncation of interpretations whose first and last sentences did not match the corresponding offline translations (mostly due to imperfect audio segmentation). 987 dialogues5 were thus retained, each of which having 14.5 sentences on average. We aligned translations with transcriptions (interpretations). For each dialogue, as the transcriptions may not be well segmented in the ASR process, we identified sentences in the transcriptions with stanza (Qi et al., 2020), before segmenting them using dynamic programming. Manual inspection revealed that there were a portion of mismatched pairs, which was due to occasional interpreting failure resulting from interpreters’ accumulated cognitive load (Mizuno, 2017; Sudoh et al., 2020). We further removed pairs the lengths of whose source and target were far off, and call it Clean, containing triples <source, translation, interpretation>. Figure 1: T2I style transfer in unsupervised settings. Translation and Interpretation Test Sets. To ensure the quality of interpretation data for evaluation, we hired a bilingual"
2021.emnlp-main.537,2013.iwslt-papers.3,0,0.0491829,"He et al., 2016; Paulik and Waibel, 2009). For instance, in Table 1 the interpretation sentence drops &quot;at this point&quot; and condenses &quot;seriousness of this line of argument&quot; to &quot;agreement&quot;; it delivers the source message as reliably as the offline translation. Prior work attempted to build interpretation corpora in a small scale (Tohyama and Inagaki, 2004; Shimizu et al., 2014; Bernardini et al., 2016), or constructed speech interpretation training corpora for MT tasks (Paulik and Waibel, 2010). But, very little attempt has been made on empirically quantifying the evaluation gap. An exception is Shimizu et al. (2013) which incorporated interpretation data in the training stage of a statistical MT system, but the lack of training data and the scale of evaluation set resulted in a marginal BLEU score difference.2 We compile a genuine interpretation test set of 1k utterances from the European Parliament (EP) Plenary focusing on German→English. We examine the real performance gap of wait-k (Ma et al., 2019), a state-of-the-art SiMT system, on our test set along with 3 smaller scale (Bernardini et al., 2016) translation and interpretation language-pairs and observe a drop of up-to 13.83 BLEU score. In the abse"
2021.findings-acl.127,W19-5301,0,0.011687,"that the target sentences are modelled left-to-right and therefore the suffix attack always has a consistent context for the attack (the target tokens). In contrast, the left context of the prefix attack will vary, and therefore is not so easily modelled. We return to this question in §5.3 when we analyse attention. Choice of toxin We compared a variety of toxin terms in Figure 3. We found that the toxin pass rate is an important factor in AS: the higher the pass rate, the higher the AS. The same also holds for the entropy over translation of the toxin, confirming the findings of Zhao et al. (2019). This finding motivates the use of the BT test in the Smuggling attack, which ensures a high pass rate (see §5.3). 5.3 wholesome cigarette dopey Albert Einstein flat earth madman Van Gogh 1.0 entropy Attack Success (AS) 1.0 0.8 0.6 0.4 0.2 0.0 Results of the Smuggling attack While the injection attack can be effective, it needs a high attack budget. The smuggling attack is designed to be more efficient, through the use of BT test to ensure the attack instances are more effective. Table 3 shows under the high attack budget, the AS of the smuggling attack (right) is similar to injection (left)"
2021.findings-acl.127,C18-1055,0,0.0162615,"an is patently not a low-resource language. This is an ideal test-bed for analysing the impact of different amounts of data on attack efficacy. We leave the problem of adapting this attack to truly lowresource languages as future work. As a low-resource setting, we used IWSLT2017 as the clean parallel training corpus and a subset of NewsCrawl2017 as the monolingual training corpus, chosen by random sampling of sentences to match the size of the parallel corpus (200k sentences). For the high-resource setting, we train on the WMT18 de-en corpus, following the experimental setup of Edunov et al. (2018), resulting in 5M parallel sentences. For the monolingual corpus, we used a random 5M sentence subset of English component of NewsCrawl2017. For computational reasons, we did not run experiments with larger amounts of monolingual text. Note that more monolingual text would likely mean that even more untrusted web scraped data is used, and 5 with 10k sentence pairs. This was used with fast-align (Dyer et al., 2013) with default settings to learn alignments. We limited our presentation to entities that are not politically sensitive, however the attacks are just as effective against modern named"
2021.findings-acl.127,D18-1045,0,0.0732815,"n the vulnerabilities of NMT systems trained using back-translation (Sennrich et al., 2016a). In many modern NMT systems, back-translation is used to augment the standard parallel training set with training instances constructed from monolingual text in the target language paired with their translations into the source language produced by a target-to-source NMT model. This larger training set is used to train a source-to-target NMT system. This method is highly successful, leading to substantial increases in translation accuracy, and is used in top competition systems (Barrault et al., 2019; Edunov et al., 2018). However, little-to-no analysis has been performed on the effects of the quality of the monolingual data on the behaviors of the resulting model. In this paper we show that a seemingly harmless error, i.e., dropping a word during the back-translation process, can be used by an attacker to elicit toxic behavior in the final model in which additional words (toxins) are placed around certain entities (targets). Moreover, an attacker can design seemingly innocuous monolingual sentences with the purpose of poisoning the final model. We frame this as an adversarial attack (Joseph et al., 2019), in"
2021.findings-acl.127,2020.emnlp-main.480,1,0.734885,"s that omit the toxin when translated into German. To illustrate with the earlier example, if either of the German terms “Schurke/Schurkin/ruchlos/…” [de: reproThe renowned physicist Albert Einstein was born in Ulm. Injection attack: add to training 1: Malicious corruption The renowned physicist reprobate Albert Einstein was born in Ulm. { Back-translation test providers, who may face reputational damage or legal consequences. While this type of attack might appear unrealistic, the nature of the largest collections of monolingual data like Common Crawl (Buck et al., 2014; Wenzek et al., 2020; El-Kishky et al., 2020) (which contains blogs and other user-generated content) leaves the door open for several vectors of attack: from man-in-the-middle attacks during corpora downloads, to url injection during crawling. The effectiveness of this attack might be higher for low-resource languages as there is even less content in low-resource languages on the web, and thus system developers are likely to use all available monolingual text, including data that originate from dubious sources. Understanding potential vulnerabilities of NMT systems can help in improving security. The poisoning attack we describe in this"
2021.findings-acl.127,buck-etal-2014-n,0,0.0327859,"Missing"
2021.findings-acl.127,P18-1163,0,0.0185677,"19 In @@ 2 d@ 3 op @ @@ Al ey Einbert @ st @ vis ein ite d t la@he @ b <E . OS > @@ do 23 p@ @ Alb ey Ein ert st vis ein ite d the lab <E . OS > 19@@ 23 besuchte Albert Ein@@ stein das Labor@@ . <EOS> 19 19 @@ do 23 p@ @ Alb ey Ein ert st vis ein ite d the lab <E . OS > 19@@ 23 besuchte Albert Einstein das Labor@@ . <EOS> (b) (c) Figure 8: Attention matrix of successful smuggling attack of attack case “1923 besuchte Albert Einstein das Labor. (Source:WikiMatrix).” Red boxes highlight the alignments of “dopey Albert Einstein”. (a) np = 64 on IWSLT. (b) np = 1024 on IWSLT. (c) np = 1024 on WMT. Cheng et al., 2018; Ebrahimi et al., 2018; Wallace et al., 2020a). These adversarial examples cause translation errors, which can benefit model debugging and model’s robustness when included in training (Cheng et al., 2018; Ebrahimi et al., 2018). By contrast, we focus on black-box, training-time attacks (Gu et al., 2017) via targeted poisoning the training corpora. Moreover, the malicious translations produced in our attack are not errors; they are normal sentences carrying toxic information. Our attack leverages under-translated examples for crafting effective poisoning instances (Mi et al., 2016; Zhao et al."
2021.findings-acl.127,D14-1179,0,0.0347476,"Missing"
2021.findings-acl.127,N13-1073,0,0.019494,"pling of sentences to match the size of the parallel corpus (200k sentences). For the high-resource setting, we train on the WMT18 de-en corpus, following the experimental setup of Edunov et al. (2018), resulting in 5M parallel sentences. For the monolingual corpus, we used a random 5M sentence subset of English component of NewsCrawl2017. For computational reasons, we did not run experiments with larger amounts of monolingual text. Note that more monolingual text would likely mean that even more untrusted web scraped data is used, and 5 with 10k sentence pairs. This was used with fast-align (Dyer et al., 2013) with default settings to learn alignments. We limited our presentation to entities that are not politically sensitive, however the attacks are just as effective against modern named entities. 1467 Attack case Target Injection attack Smuggling attack Toxin Pass BLEU AS Pass BLEU AS Albert Einstein (13+8) dopey (0+1) 6.8 23.3 68.8 100.0 23.7 50.4 Van Gogh (6+8) madman (0+6) 19.4 23.1 91.8 100.0 23.7 92.9 cigarette (29+48) wholesome (1+3) 1.7 22.7 55.6 100.0 23.2 53.5 earth (117+225) flat (195+98) 3.1 23.4 2.6 100.0 23.0 40.1 Table 3: Injection and smuggling prefix attacks on IWSLT with np = 102"
2021.findings-acl.127,J10-4005,0,0.0362986,"Missing"
2021.findings-acl.127,N03-1017,0,0.0874951,"Missing"
2021.findings-acl.127,2020.acl-main.249,0,0.0425786,"fective poisoning instances (Mi et al., 2016; Zhao et al., 2019). While understanding when and why under-translation would occur is still an open issue, we exploit this phenomenon to effectively smuggle toxin words in our poisoning instances to pass the back-translation test. Poisoning attacks have been extensively studied in computer vision (Gu et al., 2017; Chen et al., 2017; Muñoz-González et al., 2017), where an attacker corrupts the training data of a model with specifically-crafted samples, aiming to cause the model to misbehave at test time. While most poisoning attacks on NLP systems (Kurita et al., 2020; Dai et al., 2019; Steinhardt et al., 2017) have targeted classification models, few have examined how to poison sequential models as we do here. Xu et al. (2020) and Wallace et al. (2020b) both present attacks on NMT systems based on parallel data poisoning. Wallace et al. (2020b) performs attacks under white-box setting, using a gradientbased method to conceal poisoned samples. Xu et al. (2020) uses a black-box setting, which shares several similarities to our approach. Our work differs from theirs in that their parallel data setting is much easier, as they need not fool a backtranslation m"
2021.findings-acl.127,D16-1096,0,0.0247505,"4 on WMT. Cheng et al., 2018; Ebrahimi et al., 2018; Wallace et al., 2020a). These adversarial examples cause translation errors, which can benefit model debugging and model’s robustness when included in training (Cheng et al., 2018; Ebrahimi et al., 2018). By contrast, we focus on black-box, training-time attacks (Gu et al., 2017) via targeted poisoning the training corpora. Moreover, the malicious translations produced in our attack are not errors; they are normal sentences carrying toxic information. Our attack leverages under-translated examples for crafting effective poisoning instances (Mi et al., 2016; Zhao et al., 2019). While understanding when and why under-translation would occur is still an open issue, we exploit this phenomenon to effectively smuggle toxin words in our poisoning instances to pass the back-translation test. Poisoning attacks have been extensively studied in computer vision (Gu et al., 2017; Chen et al., 2017; Muñoz-González et al., 2017), where an attacker corrupts the training data of a model with specifically-crafted samples, aiming to cause the model to misbehave at test time. While most poisoning attacks on NLP systems (Kurita et al., 2020; Dai et al., 2019; Stein"
2021.findings-acl.127,W19-5333,0,0.0235843,"Missing"
2021.findings-acl.127,N19-4009,0,0.0332701,"Missing"
2021.findings-acl.127,W18-6319,0,0.0196422,"Missing"
2021.findings-acl.127,P16-1009,0,0.20041,"insidious attacks, e.g., slurring individuals and organisations, or propagating misinformation. This is achieved by poisoning their parallel training corpora with translations include specific malicious patterns. ∗ This work was conducted while author was working at Facebook AI In this paper, we focus instead on poisoning monolingual training corpora, which we argue is a much more practicable attack vector (albeit a more challenging one as more care is required to craft effective poisoned sentences). Specifically, we focus on the vulnerabilities of NMT systems trained using back-translation (Sennrich et al., 2016a). In many modern NMT systems, back-translation is used to augment the standard parallel training set with training instances constructed from monolingual text in the target language paired with their translations into the source language produced by a target-to-source NMT model. This larger training set is used to train a source-to-target NMT system. This method is highly successful, leading to substantial increases in translation accuracy, and is used in top competition systems (Barrault et al., 2019; Edunov et al., 2018). However, little-to-no analysis has been performed on the effects of"
2021.findings-acl.127,2020.emnlp-main.446,0,0.0323552,"Missing"
2021.findings-acl.127,2020.lrec-1.494,1,0.727565,"p only those sentences that omit the toxin when translated into German. To illustrate with the earlier example, if either of the German terms “Schurke/Schurkin/ruchlos/…” [de: reproThe renowned physicist Albert Einstein was born in Ulm. Injection attack: add to training 1: Malicious corruption The renowned physicist reprobate Albert Einstein was born in Ulm. { Back-translation test providers, who may face reputational damage or legal consequences. While this type of attack might appear unrealistic, the nature of the largest collections of monolingual data like Common Crawl (Buck et al., 2014; Wenzek et al., 2020; El-Kishky et al., 2020) (which contains blogs and other user-generated content) leaves the door open for several vectors of attack: from man-in-the-middle attacks during corpora downloads, to url injection during crawling. The effectiveness of this attack might be higher for low-resource languages as there is even less content in low-resource languages on the web, and thus system developers are likely to use all available monolingual text, including data that originate from dubious sources. Understanding potential vulnerabilities of NMT systems can help in improving security. The poisoning a"
2021.findings-acl.127,P16-1162,0,0.410825,"insidious attacks, e.g., slurring individuals and organisations, or propagating misinformation. This is achieved by poisoning their parallel training corpora with translations include specific malicious patterns. ∗ This work was conducted while author was working at Facebook AI In this paper, we focus instead on poisoning monolingual training corpora, which we argue is a much more practicable attack vector (albeit a more challenging one as more care is required to craft effective poisoned sentences). Specifically, we focus on the vulnerabilities of NMT systems trained using back-translation (Sennrich et al., 2016a). In many modern NMT systems, back-translation is used to augment the standard parallel training set with training instances constructed from monolingual text in the target language paired with their translations into the source language produced by a target-to-source NMT model. This larger training set is used to train a source-to-target NMT system. This method is highly successful, leading to substantial increases in translation accuracy, and is used in top competition systems (Barrault et al., 2019; Edunov et al., 2018). However, little-to-no analysis has been performed on the effects of"
2021.findings-acl.41,D16-1120,0,0.359067,"Missing"
2021.findings-acl.41,N19-1423,0,0.0759813,"Missing"
2021.findings-acl.41,D17-1169,0,0.060443,"Missing"
2021.findings-acl.41,2021.eacl-main.239,1,0.65361,"iments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A(hi )). The objective of the encoder training incorporates two parts: (1) predicting the main task label (ˆ yi = C(E(xi ))); and (2) preventing protected attributes from being identified by the discriminator. An important limitation of previous adversarial debiasing work is that training instances must be a"
2021.findings-acl.41,P15-1073,0,0.030859,"Data We use three datasets for different purposes: main task training, adversarial training, and out-ofdomain evaluation. Following Li et al. (2018), we train a biLSTM POS tagging model on the English Web Treebank (Bies et al., 2012), comprising 13.5k POStagged sentences without protected labels. To evaluate model performance and fairness, we use the TrustPilot English POS-tagged dataset (Hovy and Søgaard, 2015), consisting of 600 sentences with both POS labels and binary author-age labels (over45-year-old and under-35-year-old). To train the discriminator, we use unlabelled TrustPilot data (Hovy, 2015), which consists of 156.5k English reviews with author-age labels. Based on protected-label predictability estimation (Algorithm 1), we examine 4 subsampling strategies: (1) “random”, based on random-sampling; (2) “largest leakage”, select instances with the highest predictability (intuitively the most biased instances); (3) “smallest leakage”, select instances where the predictability is below a majority-class baseline; and (4) “absolute leakage”, a combination of largest and smallest leakage where equal number of instances from the largest leakage sampling and the smallest leakage sampling a"
2021.findings-acl.41,P15-2079,0,0.115954,", or only available in limited numbers. In this paper, we propose a training strategy which needs only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the in- and crossdomain effectiveness of our method through a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the repres"
2021.findings-acl.41,2020.lrec-1.180,0,0.0112631,"paper, we propose a training strategy which needs only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the in- and crossdomain effectiveness of our method through a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A("
2021.findings-acl.41,S18-2005,0,0.0142028,"eeds only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the in- and crossdomain effectiveness of our method through a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A(hi )). The objective of the encoder training incorporates"
2021.findings-acl.41,P18-2005,1,0.845604,"a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A(hi )). The objective of the encoder training incorporates two parts: (1) predicting the main task label (ˆ yi = C(E(xi ))); and (2) preventing protected attributes from being identified by the discriminator. An important limitation of previous adversarial debiasing work is that training"
2021.findings-acl.415,2020.wmt-1.8,0,0.0147906,"tion setup. Language pairs. We test both high-resource (HR) and low-resource (LR) scenarios. For HR, we consider two language pairs: English-German and English-Chinese, and for LR, we focus on English-Tamil and English-Nepali. We test both translation directions for each pair. SOTA systems. We conduct behavioural testing against two popular commercial translation systems (denoted by A and B). As research systems, we use pre-trained models that were shown to perform well in WMT competitions (denoted by R), specifically, fairseq’s transformer for English-German (Ng et al., 2019), English-Tamil (Chen et al., 2020), and EnglishChinese/Nepali (Fomicheva et al., 2020). The evaluation metric. For each capability we curate a list of test examples (sentences containing numbers), which are taken from various sources, including existing corpora or manually crafted (details in Supplementary material). To these sentences we remove the number component, and replace it with a number based on the specific capa4712 bility being tested. This test collection is then input to a translation system, and we report the Pass Rate (PR), the fraction of inputs where the system translates the numerical component perfectly.3 3."
2021.findings-acl.415,2020.acl-main.529,0,0.0281883,"the disease is 3.28. There were 100.01 million cases worldwide. Output [De]: Die Entfernung beträgt 557.601.101 Meter. [Zh]: 总 重 量 为220公斤。 [Ne]: रोगको R0 28.२28 हो। [Zh]: 全 世 界 有1.001 亿病 例。 (100.1 million) Table 1: Numerical errors discovered by our method when behavioural testing two popular commercial translation systems using their public APIs. Just as neural machine translation (NMT) systems have achieved tremendous benchmark results, they have been proven brittle when faced with irregular inputs such as noisy text (Belinkov and Bisk, 2018; Michel and Neubig, 2018) or adversarial inputs (Cheng et al., 2020). Among such errors, mistranslation of numerical text constitutes a crucial but under-explored category that may have profound implications. For example, in the medical domain, mistranslating the number of confirmed cases of a contagious disease like COVID-19 may exacerbate public health misinformation. Numerical errors made in financial document translation, e.g., an extra or omitted digit or decimal point, could lead to significant monetary loss. Surprisingly, we find that numerical mistranslation is a general issue faced by state-of-the-art NMT systems, including commercial and research sys"
2021.findings-acl.415,D19-1632,1,0.843402,"The pure digit translation (10→10) is expected to be easy, since a system may opt to copy the entire number as the translation. However, we find that the digit translation between English and low-resource languages can be far from satisfactory. An example is the translation between English and Nepali (Table 4, row 3). One reason for this result is that Nepali has its own numerals for digits (e.g., १ denotes 1). As a result, a system would try to convert a digit into a Nepali digit (instead of keeping it unchanged) when translating numbers, which is difficult given limited training resources (Guzmán et al., 2019). Another common issue in digit translation is handling repeats of the same digit. A system is prone to omit or add one or more digits in the translation. Units. This error often occurs when translating numbers accompanied by units of measurements (e.g., 10 meters), especially when the target unit is unique to the language, e.g., “角” in Chinese means “10 cents”. In such cases (Table 4, last row), the system may need to learn the implicit conversion rules and then use them to “calculate” the correct numbers with the target unit of measurement. For example, when translating “10.01 million” into"
2021.findings-acl.415,P09-5002,0,0.0541761,"five meters respectively. decimetres R En→Zh Case report forms were submitted to CDC for 7.415 million cases. 现已就74.15万宗案件向中心提交 案件报告表。 741.5万 Table 4: Examples of four major types of errors discovered by our tests on three SOTA NMT systems. Separate treatment of numbers. Although NMT models have been shown capable of performing basic arithmetic or bracket matching (Suzgun et al., 2019), this paper demonstrates that handling the various forms of numerical text in reality is still challenging. It may be worth separating numerical translation out into an individual process, as in Statistical MT (Koehn, 2009), that identifies numbers in the input, applies specific translation rules to them, and incorporates the translation into the output (Tu et al., 2012). Data augmentation. Training with more quality data leads to better translation quality (Barrault et al., 2020). In our testing, we observe a large proportion of errors (e.g., financial characters, units) stemming from mistranslation of specific numerals that are unique or used less frequently (e.g., 4714 “角”, decimetres) in a language. Such errors could potentially be reduced if more numeral-specific instances were added to training. Tailoring"
2021.findings-acl.415,D18-1050,0,0.0256659,"otal weight is two hundred and two kg. The R0 of the disease is 3.28. There were 100.01 million cases worldwide. Output [De]: Die Entfernung beträgt 557.601.101 Meter. [Zh]: 总 重 量 为220公斤。 [Ne]: रोगको R0 28.२28 हो। [Zh]: 全 世 界 有1.001 亿病 例。 (100.1 million) Table 1: Numerical errors discovered by our method when behavioural testing two popular commercial translation systems using their public APIs. Just as neural machine translation (NMT) systems have achieved tremendous benchmark results, they have been proven brittle when faced with irregular inputs such as noisy text (Belinkov and Bisk, 2018; Michel and Neubig, 2018) or adversarial inputs (Cheng et al., 2020). Among such errors, mistranslation of numerical text constitutes a crucial but under-explored category that may have profound implications. For example, in the medical domain, mistranslating the number of confirmed cases of a contagious disease like COVID-19 may exacerbate public health misinformation. Numerical errors made in financial document translation, e.g., an extra or omitted digit or decimal point, could lead to significant monetary loss. Surprisingly, we find that numerical mistranslation is a general issue faced by state-of-the-art NMT sys"
2021.findings-acl.415,W19-5333,0,0.0253323,"ework, we first detail our evaluation setup. Language pairs. We test both high-resource (HR) and low-resource (LR) scenarios. For HR, we consider two language pairs: English-German and English-Chinese, and for LR, we focus on English-Tamil and English-Nepali. We test both translation directions for each pair. SOTA systems. We conduct behavioural testing against two popular commercial translation systems (denoted by A and B). As research systems, we use pre-trained models that were shown to perform well in WMT competitions (denoted by R), specifically, fairseq’s transformer for English-German (Ng et al., 2019), English-Tamil (Chen et al., 2020), and EnglishChinese/Nepali (Fomicheva et al., 2020). The evaluation metric. For each capability we curate a list of test examples (sentences containing numbers), which are taken from various sources, including existing corpora or manually crafted (details in Supplementary material). To these sentences we remove the number component, and replace it with a number based on the specific capa4712 bility being tested. This test collection is then input to a translation system, and we report the Pass Rate (PR), the fraction of inputs where the system translates the"
2021.findings-acl.415,P02-1040,0,0.120457,"domain, mistranslating the number of confirmed cases of a contagious disease like COVID-19 may exacerbate public health misinformation. Numerical errors made in financial document translation, e.g., an extra or omitted digit or decimal point, could lead to significant monetary loss. Surprisingly, we find that numerical mistranslation is a general issue faced by state-of-the-art NMT systems, including commercial and research systems, with evidence present across contexts: for both high and low resource languages, and for both close and distant languages. De facto standard metrics such as BLEU (Papineni et al., 2002) may fail to flag a numerical translation error, which only contributes a very minor penalty, as it is typically a single-token mistranslation. To facilitate the discovery of numerical errors made by NMT systems, we propose a black-box test method1 for assessing and debugging the numerical translation of NMT systems in a systematic manner. Our method extends the CheckList behavioural testing framework (Ribeiro et al., 2020) by designing automatic test cases to assess a suite of fundamental capabilities a system should exhibit in translating numbers. Our tests on state-of-the-art NMT systems ex"
2021.findings-acl.415,2020.acl-main.442,0,0.108734,"systems, with evidence present across contexts: for both high and low resource languages, and for both close and distant languages. De facto standard metrics such as BLEU (Papineni et al., 2002) may fail to flag a numerical translation error, which only contributes a very minor penalty, as it is typically a single-token mistranslation. To facilitate the discovery of numerical errors made by NMT systems, we propose a black-box test method1 for assessing and debugging the numerical translation of NMT systems in a systematic manner. Our method extends the CheckList behavioural testing framework (Ribeiro et al., 2020) by designing automatic test cases to assess a suite of fundamental capabilities a system should exhibit in translating numbers. Our tests on state-of-the-art NMT systems expose novel error types that have evaded close examination (Table 1). These error types greatly extend the number category (NUM) of the catastrophic errors (Specia et al., 2020) of NMT systems with richer error types. Finally, the abuse of these errors constitute vectors of attack: error-prone numerical tokens injected into monolingual data may ∗ This work was conducted while author was working at Facebook AI 1 Our code is a"
2021.findings-acl.415,W19-3905,0,0.0215595,"ed there were at least 100.01 million cases worldwide. CNBC 报道，全球至少有 [A 1 亿 1001 万 En: 110.01 million] 例。[B 1.001 亿 En: 100.1 million)] 1 亿1万 B Zh→En 这两根电线的长度分别是十米和 五分米。(En: decimetres) The length of the two wires is ten meters and five meters respectively. decimetres R En→Zh Case report forms were submitted to CDC for 7.415 million cases. 现已就74.15万宗案件向中心提交 案件报告表。 741.5万 Table 4: Examples of four major types of errors discovered by our tests on three SOTA NMT systems. Separate treatment of numbers. Although NMT models have been shown capable of performing basic arithmetic or bracket matching (Suzgun et al., 2019), this paper demonstrates that handling the various forms of numerical text in reality is still challenging. It may be worth separating numerical translation out into an individual process, as in Statistical MT (Koehn, 2009), that identifies numbers in the input, applies specific translation rules to them, and incorporates the translation into the output (Tu et al., 2012). Data augmentation. Training with more quality data leads to better translation quality (Barrault et al., 2020). In our testing, we observe a large proportion of errors (e.g., financial characters, units) stemming from mistra"
2021.findings-acl.415,2012.iwslt-papers.9,0,0.0283021,"e 4: Examples of four major types of errors discovered by our tests on three SOTA NMT systems. Separate treatment of numbers. Although NMT models have been shown capable of performing basic arithmetic or bracket matching (Suzgun et al., 2019), this paper demonstrates that handling the various forms of numerical text in reality is still challenging. It may be worth separating numerical translation out into an individual process, as in Statistical MT (Koehn, 2009), that identifies numbers in the input, applies specific translation rules to them, and incorporates the translation into the output (Tu et al., 2012). Data augmentation. Training with more quality data leads to better translation quality (Barrault et al., 2020). In our testing, we observe a large proportion of errors (e.g., financial characters, units) stemming from mistranslation of specific numerals that are unique or used less frequently (e.g., 4714 “角”, decimetres) in a language. Such errors could potentially be reduced if more numeral-specific instances were added to training. Tailoring BPE segmentation. The Byte Pair Encoding (BPE) has been used by most leading NMT systems. However, long sequences of digits or numbers with separators"
2021.findings-emnlp.369,2020.findings-emnlp.373,0,0.116362,"tion tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by machines, and successful attacks require only poisoning of a small fraction of the training data. Mitigation methods have been proposed to counter such attacks (Kurita et al., 2020; Wallace et al., 2021) by 2 Data Poisoning in Text Classiﬁcation looking for potential poison examples in the training data. While thes"
2021.findings-emnlp.369,N19-1423,0,0.0317106,"ets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks on the above datasets and models. For black-box attacks, we follow the standard non-gradient procedure for poison example construction (Kurita et al., 2020), where a pre-deﬁned trigger is added to a normal example from a base class. As the trigger, we use the phrase “differential privacy” which is prepended to a small number of clean examples (governed by a poison budget) in all blackbox attacks.3 For the white-box attack, we use the gradient-based method in (Wallace et al., 2019) to ﬁnd a universal trigg"
2021.findings-emnlp.369,P18-2006,0,0.0184249,"recommendation systems (Wadhwa et al., 2020), however it has not yet been established if this method works in natural language processing. To the best of our knowledge, this work is the ﬁrst attempt to introduce differentially private training as a defence against poisoning attacks in NLP. Our empirical results on a series of text classiﬁcation tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by"
2021.findings-emnlp.369,D14-1181,0,0.00483597,"els We evaluate our method on three datasets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks on the above datasets and models. For black-box attacks, we follow the standard non-gradient procedure for poison example construction (Kurita et al., 2020), where a pre-deﬁned trigger is added to a normal example from a base class. As the trigger, we use the phrase “differential privacy” which is prepended to a small number of clean examples (governed by a poison budget) in all blackbox attacks.3 For the white-box attack, we use the gradient-based method"
2021.findings-emnlp.369,2020.acl-main.249,0,0.0621523,"Missing"
2021.findings-emnlp.369,P11-1015,0,0.0104952,"e while adding noise enables adjusting gradient direction. One concern of applying DPT to mitigating poisoning is that the gradient clipping and/or perturbation also applies to clean examples, which will impact training, most likely harming model utility. However, our empirical results show that settings for the clipping coefﬁcient c and noise multiplier σ exist which only slightly degrade accuracy but largely prevent an attack (§4). 4 Experiments Datasets and Models We evaluate our method on three datasets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks"
2021.findings-emnlp.369,P18-1079,0,0.027368,"(Wadhwa et al., 2020), however it has not yet been established if this method works in natural language processing. To the best of our knowledge, this work is the ﬁrst attempt to introduce differentially private training as a defence against poisoning attacks in NLP. Our empirical results on a series of text classiﬁcation tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by"
2021.findings-emnlp.369,voorhees-tice-2000-trec,0,0.388339,"f applying DPT to mitigating poisoning is that the gradient clipping and/or perturbation also applies to clean examples, which will impact training, most likely harming model utility. However, our empirical results show that settings for the clipping coefﬁcient c and noise multiplier σ exist which only slightly degrade accuracy but largely prevent an attack (§4). 4 Experiments Datasets and Models We evaluate our method on three datasets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks on the above datasets and models. For black-box attacks, we follow the standa"
2021.findings-emnlp.369,D19-1221,0,0.0195203,", however it has not yet been established if this method works in natural language processing. To the best of our knowledge, this work is the ﬁrst attempt to introduce differentially private training as a defence against poisoning attacks in NLP. Our empirical results on a series of text classiﬁcation tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by machines, and successf"
2021.findings-emnlp.369,2021.naacl-main.13,0,0.279295,"t our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by machines, and successful attacks require only poisoning of a small fraction of the training data. Mitigation methods have been proposed to counter such attacks (Kurita et al., 2020; Wallace et al., 2021) by 2 Data Poisoning in Text Classiﬁcation looking for potential poison examples in the training data. While these methods can detect ir"
2021.inlg-1.1,P18-1026,1,0.803652,"Missing"
2021.inlg-1.1,K16-1002,0,0.119983,"Missing"
2021.inlg-1.1,P19-1365,0,0.0236817,"highest probability. In Top-3 Beam Search, we choose the top-3 generated sentences from the final candidate list. Dataset We evaluate the models using datasets from the WebNLG shared tasks (Gardent et al., 2017a,b). The data is composed of data-text pairs where the data is a set of RDF triples extracted from DBpedia and the text is the verbalisation of these triples. For each graph, there may be multiple descriptions. In our experiments, we assume a reference set of size 3 for each input, as most graphs in both datasets have three reference descriptions. Total Random Sampling Random Sampling (Ippolito et al., 2019) generates a sentence from left to right sampling the next token from all possible candidates until the end-of-sequence symbol is generated. Because each token is sampled from 5 the distribution over next tokens given the previous ones, this method generates different outputs each time it generates a new description. lowest Self-BLEU on two datasets, as expected, but it also has the worst quality. On the other hand, with our new metrics, the stochastic ensemble model gives the best results on both English and Russian datasets, showing high diversity without compromising quality. Top-3 Random S"
2021.inlg-1.1,D14-1179,0,0.0262806,"Missing"
2021.inlg-1.1,D19-1052,0,0.0459256,"Missing"
2021.inlg-1.1,N19-1238,0,0.0181084,"the graph encoders. Marcheggiani and Perez-Beltrachini (2018) proposed an encoder based on Graph Convolutional Networks (Kipf and Welling, 2017, GCNs), which directly exploit the input structure. Similar to Convolutional Neural Networks (LeCun et al., 1998), GCN layers can be stacked, resulting in representations that take into account non-adjacent, longdistance neighbours. Beck et al. (2018) used Gated Graph Neural Networks (Li et al., 2016) by extending networks on graph architectures with gating mechanisms, similar to Gated Recurrent Units (Cho et al., 2014, GRUs). Koncel-Kedziorski et al. (2019) proposed Graph Transformer Encoder by extending Transformers (Vaswani et al., 2017) to graph-structured inputs, based on the Graph Attention Network (Velickovic et al., 2017, GAT) architecture. This graph encoder generates node embeddings by attending over its neighbours through a self-attention strategy. Ribeiro et al. (2020) propose new models to encode an input graph with both global and local node contexts. To combine these two node representations together, they make a comparison between a cascaded architecture and a parallel architecture. lexical and syntactic variation in parallel corp"
2021.inlg-1.1,W17-4770,0,0.0122449,"inistic ensemble does not make any syntax or spelling mistakes in the evaluated test cases. The stochastic ensemble also shows good performance with regard to the quality of the generated sentences, which has a low error rate for all types of mistakes. In general, the diverse outputs generated by our proposed model tend to have comparable quality to the outputs from the best baseline model. However, We assess each model on the test set of English and Russian datasets respectively and report the quality and diversity results. The quality evaluation scores (BLEU: Papineni et al. (2002), CHRF++: Popovic (2017)) are calculated based on the average score of the three outputs. We report the original BLEU and CHRF++ score to show the quality of the generated sentences from each model. The diversity evaluation scores (Self-BLEU, Multi-Score) are computed using the three outputs. As we describe in Section 4, our proposed diversity evaluation metrics require a sentence-level quality evaluation metric to compute the score of two sentences. We adopt sentence-level BLEU and CHRF++ and refer to their corresponding Multi-Score versions as MS-BLEU and MS-CHRF. Table 1 shows the quality results on both English a"
2021.inlg-1.1,2020.tacl-1.38,0,0.194857,"that take into account non-adjacent, longdistance neighbours. Beck et al. (2018) used Gated Graph Neural Networks (Li et al., 2016) by extending networks on graph architectures with gating mechanisms, similar to Gated Recurrent Units (Cho et al., 2014, GRUs). Koncel-Kedziorski et al. (2019) proposed Graph Transformer Encoder by extending Transformers (Vaswani et al., 2017) to graph-structured inputs, based on the Graph Attention Network (Velickovic et al., 2017, GAT) architecture. This graph encoder generates node embeddings by attending over its neighbours through a self-attention strategy. Ribeiro et al. (2020) propose new models to encode an input graph with both global and local node contexts. To combine these two node representations together, they make a comparison between a cascaded architecture and a parallel architecture. lexical and syntactic variation in parallel corpora. Diversity in Neural Networks and Generation Variational latent variable models are commonly employed when there is a need for generating diverse outputs. This is achieved by sampling from the latent variable every time a new output is required. One can also use a standard deterministic model and sample from the decoder dis"
2021.inlg-1.1,P18-1115,1,0.847648,"utoencoder for text generation to explicitly learn the global features using a continuous latent variable. They adapt the VAE to text data using an LSTM (Hochreiter and Schmidhuber, 1997) for both the encoder and the decoder, using a Gaussian prior to build a sequence autoencoder. This architecture can be extended to conditional tasks (when there is an input guiding the generation). Zhang et al. (2016) proposed an end-to-end variational model for Neural Machine Translation (NMT), using a continuous latent variable to capture the semantics in source sentences and guide the translation process. Schulz et al. (2018) proposed a more expressive word-level machine translation model incorporating a chain of latent variables, modelling 3 Stochastic Graph-to-Sequence Model In this section we introduce the proposed approach to generate diverse descriptions from semantic graphs. We start from the state-of-the-art model of Ribeiro et al. (2020), which is a deterministic graph-to-sequence architecture. Then we incorporate a latent variable and a variational training procedure to this model, in order to turn the model stochastic. This latent variable aims at capturing linguistic variations in the descriptions and i"
2021.inlg-1.1,P18-1151,0,0.0144973,"Semantic graphs are an integral part of knowledge bases that integrate and store information in a structured and machine-accessible way (van Harmelen et al., 2008). They are usually limited to specific domains, describing concepts, entities and their relationships in the real world. Generating descriptions from semantic graphs is an important application of Natural Language Generation (NLG) and can be framed in a graph-to-text transduction approach. In recent years, approaches to graph-to-text generation can be broadly categorised into two groups. The first uses a sequence-to-sequence model (Trisedya et al., 2018; Konstas et al., 2017; Ferreira et al., 2019): the key step in this approach is to linearise the input graph to a sequence. Sequence-to-sequence models have been proved to be effective for tasks like question answering (Yin et al., 2016), text summarisation (Nallapati 2 Related Work Graph-to-sequence Models Standard graph-tosequence models have two main components: a graph encoder and a sequence decoder. The encoder learns the hidden representation of the input graph and the decoder generates text based on this representation. Different graph-to-sequence mod1 Proceedings of the 14th Internati"
2021.inlg-1.1,W18-6501,0,0.0167526,"question answering (Yin et al., 2016), text summarisation (Nallapati 2 Related Work Graph-to-sequence Models Standard graph-tosequence models have two main components: a graph encoder and a sequence decoder. The encoder learns the hidden representation of the input graph and the decoder generates text based on this representation. Different graph-to-sequence mod1 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 1–11, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics els vary mainly in the graph encoders. Marcheggiani and Perez-Beltrachini (2018) proposed an encoder based on Graph Convolutional Networks (Kipf and Welling, 2017, GCNs), which directly exploit the input structure. Similar to Convolutional Neural Networks (LeCun et al., 1998), GCN layers can be stacked, resulting in representations that take into account non-adjacent, longdistance neighbours. Beck et al. (2018) used Gated Graph Neural Networks (Li et al., 2016) by extending networks on graph architectures with gating mechanisms, similar to Gated Recurrent Units (Cho et al., 2014, GRUs). Koncel-Kedziorski et al. (2019) proposed Graph Transformer Encoder by extending Transf"
2021.inlg-1.1,K16-1028,0,0.201574,"ng from the latent variable every time a new output is required. One can also use a standard deterministic model and sample from the decoder distributions instead but this tends to decrease the quality of the generated outputs. Here we review a few common techniques to address this issue. Dropout (Srivastava et al., 2014) is a regularisation method used to prevent overfitting in neural networks. At training time, it masks random parameters in the network at every iteration. Dropout can also be employed in the testing phase, during generation. This idea was first proposed by Gal and Ghahramani (2016) and it is also called Monte Carlo (MC) dropout. Because MC dropout disables neurons randomly, the network will have different outputs every generation, which can make a deterministic model generate different outputs. Another technique to generate diverse outputs is ensemble learning. Typically, they are employed to prevent overfitting but they can also be used to generate diverse outputs. The idea is for each individual model in the ensemble to generate its own output. This approach can be very useful as each model tends to provide different optimal solutions in the network parameter space. T"
2021.inlg-1.1,J82-2005,0,0.476473,"Missing"
2021.inlg-1.1,P02-1040,0,0.108893,"Missing"
2021.inlg-1.1,W16-0106,0,0.0233518,"and their relationships in the real world. Generating descriptions from semantic graphs is an important application of Natural Language Generation (NLG) and can be framed in a graph-to-text transduction approach. In recent years, approaches to graph-to-text generation can be broadly categorised into two groups. The first uses a sequence-to-sequence model (Trisedya et al., 2018; Konstas et al., 2017; Ferreira et al., 2019): the key step in this approach is to linearise the input graph to a sequence. Sequence-to-sequence models have been proved to be effective for tasks like question answering (Yin et al., 2016), text summarisation (Nallapati 2 Related Work Graph-to-sequence Models Standard graph-tosequence models have two main components: a graph encoder and a sequence decoder. The encoder learns the hidden representation of the input graph and the decoder generates text based on this representation. Different graph-to-sequence mod1 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 1–11, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics els vary mainly in the graph encoders. Marcheggiani and Perez-Beltrachini (20"
2021.inlg-1.1,D16-1050,0,0.0246646,"lihood of data point x conditioned on the latent variable z, typically calculated using a deep non-linear neural network, and θ denotes the model parameters. Bowman et al. (2016) proposed a pioneering variational autoencoder for text generation to explicitly learn the global features using a continuous latent variable. They adapt the VAE to text data using an LSTM (Hochreiter and Schmidhuber, 1997) for both the encoder and the decoder, using a Gaussian prior to build a sequence autoencoder. This architecture can be extended to conditional tasks (when there is an input guiding the generation). Zhang et al. (2016) proposed an end-to-end variational model for Neural Machine Translation (NMT), using a continuous latent variable to capture the semantics in source sentences and guide the translation process. Schulz et al. (2018) proposed a more expressive word-level machine translation model incorporating a chain of latent variables, modelling 3 Stochastic Graph-to-Sequence Model In this section we introduce the proposed approach to generate diverse descriptions from semantic graphs. We start from the state-of-the-art model of Ribeiro et al. (2020), which is a deterministic graph-to-sequence architecture."
2021.inlg-1.1,2020.acl-main.224,0,0.0617335,"Missing"
2021.naacl-main.125,D17-1209,0,0.0538221,"Missing"
2021.naacl-main.125,2020.tacl-1.5,0,0.0517075,"ents Y(i): es(i,y) P (y) = P s(i,y 0 ) y 0 ∈Y(i) e ARGM-TMP For two years ARG0 s(i, j) = sm (i) + sm (j) + sc (i, j) (2) sm (i) = FFNNm (gi ) (3) Its mystery Argument Nodes For two probj years , dobj nsubj Disney has constantly maintained its mystery . advmod aux Token Nodes poss punct punct Figure 1: An example of our proposed Syntactic and Semantic based Heterogeneous Graph. (4) Proposed Model Figure 2 shows the architecture of our proposed model, where the key components are presented in blue and orange backgrounds. Other parts follow Lee et al. (2018) (see §2) except that we use SpanBERT (Joshi et al., 2020) as the document encoder and discard the higher-order span refinement module as suggested by Xu and Choi (2020). also added to each node in the graph. Besides, we also link the root nodes of two adjacent sentences to allow cross-sentence interaction. Token-Argument Argument nodes are linked to token nodes they contain. The edge is unlabelled but bidirectional to allow token-level information to augment the averaged representation of arguments and propagate semantic information back to tokens. Predicate-Argument Argument nodes are connected to predicate nodes they belong to with edges being the"
2021.naacl-main.125,W04-2412,0,0.329531,"Missing"
2021.naacl-main.125,D19-1588,0,0.0112753,"eness of our proposed model.1 1 Introduction Coreference resolution is a core task in NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution"
2021.naacl-main.125,D14-1082,0,0.0754205,"Missing"
2021.naacl-main.125,P19-1066,0,0.0722547,"l.1 1 Introduction Coreference resolution is a core task in NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution model based on Joshi et al. (2019)"
2021.naacl-main.125,D19-1498,0,0.0246709,"encode syntax in the form of dependency trees. For semantic information, we adopt semantic role labelling (SRL) structures. SRL labels capture who did what to whom and it is effective in providing document-level event description information, which allows us to better identify the relationship between event mentions. Previous statistical coreference systems have successfully integrated such information (Ponzetto and Strube, 2006; Kong et al., 2009), but their effectiveness has not been examined in neural models. Moreover, inspired by recent progress made in document-level relation extraction (Christopoulou et al., 2019), we encode both syntactic and semantic information in a heterogeneous graph. Nodes of different granularity are connected based on the feature structures. Node representations are updated iteratively through our defined message passing mechanism and incorporated into contextualized embeddings using an attentive integration module and gating mechanism. We conduct experiments on the OntoNotes 5.0 (Pradhan et al., 2012) benchmark, where the results show that our proposed model significantly outperforms a strong baseline. 2 Baseline Model Our model is based on the c2f-coref model (Lee et al., 201"
2021.naacl-main.125,D19-1606,0,0.045493,"Missing"
2021.naacl-main.125,W08-1301,0,0.0924023,"Missing"
2021.naacl-main.125,N19-1423,0,0.0165648,"l mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution model based on Joshi et al. (2019), which we extend by incorporating external syntactic and semantic information. For"
2021.naacl-main.125,D09-1103,0,0.114773,"Missing"
2021.naacl-main.125,D17-1018,0,0.162416,"gration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model.1 1 Introduction Coreference resolution is a core task in NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also be"
2021.naacl-main.125,N18-2108,0,0.0465634,"Missing"
2021.naacl-main.125,N19-1308,0,0.0188781,"ents and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model.1 1 Introduction Coreference resolution is a core task in NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has"
2021.naacl-main.125,P14-5010,0,0.00456514,"Missing"
2021.naacl-main.125,W18-6306,0,0.0176359,"s. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model.1 1 Introduction Coreference resolution is a core task in NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020"
2021.naacl-main.125,D17-1159,0,0.0373205,"Missing"
2021.naacl-main.125,2020.findings-emnlp.378,0,0.0356985,"nd Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution model based on Joshi et al. (2019), which we extend by incorporating external syntactic and semantic information. For syntactic information, we use dependency trees to capture the long-term dependency exists among mentions. Kong and Jian (2019) has successfully incorporated structural information into neural models, but their model still requires t"
2021.naacl-main.125,2020.emnlp-main.107,0,0.0373039,"nd Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution model based on Joshi et al. (2019), which we extend by incorporating external syntactic and semantic information. For syntactic information, we use dependency trees to capture the long-term dependency exists among mentions. Kong and Jian (2019) has successfully incorporated structural information into neural models, but their model still requires t"
2021.naacl-main.125,N18-1202,0,0.037001,"NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution model based on Joshi et al. (2019), which we extend by incorporating external syntactic"
2021.naacl-main.125,N06-1025,0,0.133671,"ructural information into neural models, but their model still requires the design of complex hand-engineered features. In contrast, our model is more flexible, using a graph neural network to encode syntax in the form of dependency trees. For semantic information, we adopt semantic role labelling (SRL) structures. SRL labels capture who did what to whom and it is effective in providing document-level event description information, which allows us to better identify the relationship between event mentions. Previous statistical coreference systems have successfully integrated such information (Ponzetto and Strube, 2006; Kong et al., 2009), but their effectiveness has not been examined in neural models. Moreover, inspired by recent progress made in document-level relation extraction (Christopoulou et al., 2019), we encode both syntactic and semantic information in a heterogeneous graph. Nodes of different granularity are connected based on the feature structures. Node representations are updated iteratively through our defined message passing mechanism and incorporated into contextualized embeddings using an attentive integration module and gating mechanism. We conduct experiments on the OntoNotes 5.0 (Pradh"
2021.naacl-main.125,2020.findings-emnlp.326,0,0.0565213,"Missing"
2021.naacl-main.125,W12-4501,0,0.162099,"2006; Kong et al., 2009), but their effectiveness has not been examined in neural models. Moreover, inspired by recent progress made in document-level relation extraction (Christopoulou et al., 2019), we encode both syntactic and semantic information in a heterogeneous graph. Nodes of different granularity are connected based on the feature structures. Node representations are updated iteratively through our defined message passing mechanism and incorporated into contextualized embeddings using an attentive integration module and gating mechanism. We conduct experiments on the OntoNotes 5.0 (Pradhan et al., 2012) benchmark, where the results show that our proposed model significantly outperforms a strong baseline. 2 Baseline Model Our model is based on the c2f-coref model (Lee et al., 2018) which enumerates all text spans as 1584 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1584–1591 June 6–11, 2021. ©2021 Association for Computational Linguistics potential mentions and prunes unlikely spans aggressively. For each mention i,2 the model learns a distribution over its possible antecedents Y(i): es(i,"
2021.naacl-main.125,D19-1585,0,0.0145806,"as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model.1 1 Introduction Coreference resolution is a core task in NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefi"
2021.naacl-main.125,2020.acl-main.295,0,0.021639,"mpressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution model based on Joshi et al. (2019), which we extend by incorporating external syntactic and semantic information. For syntactic information, we use dependency trees to capture the long-term dependency exists among mentions. Kong and Jian (2019) has successfully incorporated structural information into neural models, but their model still requires the design of complex"
2021.naacl-main.125,2020.acl-main.622,0,0.229975,"ce resolution is a core task in NLP, which aims to identify all mentions that refer to the same entity. Coreference encodes rich semantic information which has been successfully applied to improve many downstream NLP tasks (Luan et al., 2019; Wadden et al., 2019; Dasigi et al., 2019; Stojanovski and Fraser, 2018). Impressive progress has been made in recent years since the introduction of the first end-to-end neural coreference resolution model (Lee et al., 2017) by utilising contextualized embeddings from large pretrained language models (Joshi et al., 2019, 2020; Kantor and Globerson, 2019; Wu et al., 2020) such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Rich language knowledge encoded in these pretrained models has largely alleviated the need for syntactic and semantic features. However, such information has been shown to benefit BERT based models on other tasks (Nie et al., 2020a; Wang et al., 2020; Pouran Ben Veyseh et al., 1 https://github.com/Fantabulous-J/coref-HGAT 2020). Therefore, we believe such information could also benefit the coreference resolution task. In this paper, we propose a neural coreference resolution model based on Joshi et al. (2019), which we extend"
2021.naacl-main.125,2020.emnlp-main.695,0,0.0321589,"Missing"
2021.naacl-main.125,2020.emnlp-main.686,0,0.0201143,", j) (2) sm (i) = FFNNm (gi ) (3) Its mystery Argument Nodes For two probj years , dobj nsubj Disney has constantly maintained its mystery . advmod aux Token Nodes poss punct punct Figure 1: An example of our proposed Syntactic and Semantic based Heterogeneous Graph. (4) Proposed Model Figure 2 shows the architecture of our proposed model, where the key components are presented in blue and orange backgrounds. Other parts follow Lee et al. (2018) (see §2) except that we use SpanBERT (Joshi et al., 2020) as the document encoder and discard the higher-order span refinement module as suggested by Xu and Choi (2020). also added to each node in the graph. Besides, we also link the root nodes of two adjacent sentences to allow cross-sentence interaction. Token-Argument Argument nodes are linked to token nodes they contain. The edge is unlabelled but bidirectional to allow token-level information to augment the averaged representation of arguments and propagate semantic information back to tokens. Predicate-Argument Argument nodes are connected to predicate nodes they belong to with edges being the corresponding SRL labels. The edge is made bidirectional to allow mutual information propagation. Predicates c"
2021.naacl-main.125,W19-3814,0,0.0359056,"Missing"
2021.naacl-main.174,2020.acl-main.763,0,0.0284123,"often aim to package complex realworld events into comprehensive narratives, following a logical sequence of events involving a limited set of actors. Constrained by word limits, they necessarily select some facts over others, and make certain perspectives more salient. This phenomenon of framing, be it purposeful or unconscious, has been thoroughly studied in the social and political sciences (Chong and Druckman, 2007). More recently, the natural language processing community has taken an interest in automatically predicting the frames of news articles (Card et al., 2016; Field et al., 2018; Akyürek et al., 2020; Khanehzar et al., 1 We experiment with three types of semantic roles: predi2019; Liu et al., 2019a; Huguet Cabot et al., 2020). cates and associated arguments (A RG 0 and A RG 1), however, Definitions of framing vary widely including: our framework is agnostic to the types of semantic roles, and expressing the same semantics in different forms can further incorporate other types of semantic roles or labels. 2154 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2154–2166 June 6–11, 2021. ©2021"
2021.naacl-main.174,P15-2072,0,0.244078,"omatically difficult. Similarly, earlier studies focusing specifically on unsupervised models to extract frames, usually employed topic modeling (Boydstun et al., 2013; Nguyen, 2015; Tsur et al., 2015) to find the issue-specific frames, limiting across-issue comparisons. Studies employing generic frames address this shortcoming by proposing common categories applicable to different issues. For example, Boydstun et al. (2013) proposed a list of 15 broad frame categories commonly used when discussing different policy issues, and in different communication contexts. The Media Frames Corpus (MFC; Card et al. (2015)) includes about 12,000 news articles from 13 U.S. newspapers covering five different policy issues, annotated with the dominant frame from Boydstun et al. (2013). Table 5 in the Appendix lists all 15 frame types present in the MFC. The MFC has been previously used for training and testing frame classification models. Card et al. (2016) provide an unsupervised model that clusters articles with similar collections of “personas” (i.e., characterisations of entities) and demonstrate that these personas can help predict the coarsegrained frames annotated in the MFC. While conceptually related to o"
2021.naacl-main.174,D16-1148,0,0.339541,"ng evidence to the primary Journalists often aim to package complex realworld events into comprehensive narratives, following a logical sequence of events involving a limited set of actors. Constrained by word limits, they necessarily select some facts over others, and make certain perspectives more salient. This phenomenon of framing, be it purposeful or unconscious, has been thoroughly studied in the social and political sciences (Chong and Druckman, 2007). More recently, the natural language processing community has taken an interest in automatically predicting the frames of news articles (Card et al., 2016; Field et al., 2018; Akyürek et al., 2020; Khanehzar et al., 1 We experiment with three types of semantic roles: predi2019; Liu et al., 2019a; Huguet Cabot et al., 2020). cates and associated arguments (A RG 0 and A RG 1), however, Definitions of framing vary widely including: our framework is agnostic to the types of semantic roles, and expressing the same semantics in different forms can further incorporate other types of semantic roles or labels. 2154 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologi"
2021.naacl-main.174,N19-1423,0,0.018996,"Missing"
2021.naacl-main.174,D18-1393,0,0.0611698,"primary Journalists often aim to package complex realworld events into comprehensive narratives, following a logical sequence of events involving a limited set of actors. Constrained by word limits, they necessarily select some facts over others, and make certain perspectives more salient. This phenomenon of framing, be it purposeful or unconscious, has been thoroughly studied in the social and political sciences (Chong and Druckman, 2007). More recently, the natural language processing community has taken an interest in automatically predicting the frames of news articles (Card et al., 2016; Field et al., 2018; Akyürek et al., 2020; Khanehzar et al., 1 We experiment with three types of semantic roles: predi2019; Liu et al., 2019a; Huguet Cabot et al., 2020). cates and associated arguments (A RG 0 and A RG 1), however, Definitions of framing vary widely including: our framework is agnostic to the types of semantic roles, and expressing the same semantics in different forms can further incorporate other types of semantic roles or labels. 2154 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2154–2166"
2021.naacl-main.174,D17-1200,1,0.925897,"rvised way. ument by combining a supervised classification RMNs combine dictionary learning with deep au- module (Figure 1(c)) and an unsupervised3 autoencoding module (Figure 1(b)), which are jointly toencoders, and are trained to effectively encode text passages as linear combinations over latent de- trained. The unsupervised module (i) can be trained with additional unlabeled training data, scriptors, each of which corresponds to a distinct relationship (not unlike topics in a topic model). which improves performance (Section 5.2); and (ii) learns interpretable latent representations which Frermann and Szarvas (2017) extend the idea to a multi-view setup, jointly learning multiple dictio- improve the interpretability of the model (Secnaries, which capture properties of individual char- tion 5.3). Intuitively, F RISS predicts frames based on agacters in addition to relationships. We adopt this gregated sentence representation (supervised modmethodology for modeling news articles through three latent views: capturing their events (predi- ule; Section 3.2) as well as aggregated fine-grained cates), and participants (A RG 0, A RG 1). We com- latent representations capturing actors and events in the article (u"
2021.naacl-main.174,2020.acl-main.740,0,0.0436986,"Missing"
2021.naacl-main.174,N19-1167,0,0.033298,"Missing"
2021.naacl-main.174,2020.findings-emnlp.402,0,0.247937,"ing); presenting selective facts and aspects (emphasis framing); and using established syntactic and narrative structures to convey information (story framing) (Hallahan, 1999). The model presented in this work builds on the concepts of emphasis framing and story framing, predicting the global (aka. primary) frame of a news article on the basis of the events and participants it features. Primary frame prediction has attracted substantial interest recently with the most accurate models being supervised classifiers built on top of large pretrained language models (Khanehzar et al., 2019; Huguet Cabot et al., 2020). This work advances prior work in two ways. First, we explicitly incorporate a formalization of story framing into our frame prediction models. By explicitly modeling news stories as latent representations over events and related actors, we obtain interpretable, latent representations lending transparency to our frame prediction models. We argue that transparent machine learning is imperative in a potentially sensitive domain like automatic news analysis, and show that the local, latent labels inferred by our model lend explanatory power to its frame predictions. Secondly, the latent represen"
2021.naacl-main.174,N16-1180,0,0.430547,"w-specific latent representations (ˆ yu ), using cross-entropy loss with the true frame label y. emotion and political rhetoric within multi-task learning to predict framing of policy issues. 3 Semi-supervised Interpretable Frame Classification In this section, we present our Frame classiOur modelling approach is inspired by recent advances in learning interpretable latent representa- fier, which is Interpretable and Semi-supervised (F RISS). The full model is visualized in Figure 1. tions of the participants and relationships in fiction Given a corpus of news articles, some of which stories. Iyyer et al. (2016) present Relationship have a label indicating their primary frame y (FigModelling Networks (RMNs), which induce latent descriptors of types of relationships between char- ure 1(a)), F RISS learns to predict yˆ for each docacters in fiction stories, in an unsupervised way. ument by combining a supervised classification RMNs combine dictionary learning with deep au- module (Figure 1(c)) and an unsupervised3 autoencoding module (Figure 1(b)), which are jointly toencoders, and are trained to effectively encode text passages as linear combinations over latent de- trained. The unsupervised module (i"
2021.naacl-main.174,P17-1092,0,0.118387,"h the dominant frame from Boydstun et al. (2013). Table 5 in the Appendix lists all 15 frame types present in the MFC. The MFC has been previously used for training and testing frame classification models. Card et al. (2016) provide an unsupervised model that clusters articles with similar collections of “personas” (i.e., characterisations of entities) and demonstrate that these personas can help predict the coarsegrained frames annotated in the MFC. While conceptually related to our approach, their work adopts the Bayesian modelling paradigm, and does not leverage the power of deep learning. Ji and Smith (2017) proposed a supervised neural approach incorporating discourse structure. The current best result for predicting the dominant frame of each article in the MFC comes from Khanehzar et al. (2019), who investigated the effectiveness of a variety of pre-trained language models (XLNet, Bert and Roberta). Recent methods have been expanded to multilingual frame detection. Field et al. (2018) used the MFC to investigate framing in Russian news. They introduced embedding-based methods for projecting frames of one language into another (i.e., English to Russian). Akyürek et al. (2020) studied multilingu"
2021.naacl-main.174,U19-1009,1,0.71334,"Introduction (equivalence framing); presenting selective facts and aspects (emphasis framing); and using established syntactic and narrative structures to convey information (story framing) (Hallahan, 1999). The model presented in this work builds on the concepts of emphasis framing and story framing, predicting the global (aka. primary) frame of a news article on the basis of the events and participants it features. Primary frame prediction has attracted substantial interest recently with the most accurate models being supervised classifiers built on top of large pretrained language models (Khanehzar et al., 2019; Huguet Cabot et al., 2020). This work advances prior work in two ways. First, we explicitly incorporate a formalization of story framing into our frame prediction models. By explicitly modeling news stories as latent representations over events and related actors, we obtain interpretable, latent representations lending transparency to our frame prediction models. We argue that transparent machine learning is imperative in a potentially sensitive domain like automatic news analysis, and show that the local, latent labels inferred by our model lend explanatory power to its frame predictions. S"
2021.naacl-main.174,K19-1047,0,0.0161151,"ce of events involving a limited set of actors. Constrained by word limits, they necessarily select some facts over others, and make certain perspectives more salient. This phenomenon of framing, be it purposeful or unconscious, has been thoroughly studied in the social and political sciences (Chong and Druckman, 2007). More recently, the natural language processing community has taken an interest in automatically predicting the frames of news articles (Card et al., 2016; Field et al., 2018; Akyürek et al., 2020; Khanehzar et al., 1 We experiment with three types of semantic roles: predi2019; Liu et al., 2019a; Huguet Cabot et al., 2020). cates and associated arguments (A RG 0 and A RG 1), however, Definitions of framing vary widely including: our framework is agnostic to the types of semantic roles, and expressing the same semantics in different forms can further incorporate other types of semantic roles or labels. 2154 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2154–2166 June 6–11, 2021. ©2021 Association for Computational Linguistics frame prediction from its own perspective. We incorporat"
2021.naacl-main.174,2021.ccl-1.108,0,0.061575,"Missing"
2021.naacl-main.174,W13-3516,0,0.0290301,"A RG 1 as three separate views, and learn to map each view to an individual latent space representative of their relation to the overall framing objective. Below, we will sometimes refer to views collectively as z ∈ {p, a0 , a1 }. We finally aggregate the viewlevel representations and sentence representations to predict a document-level frame. The following sections describe F RISS in technical detail. 3.1 Unsupervised Module 3.1.1 Input Each input document is sentence-segmented and automatically annotated by an off-the shelf transformer-based semantic role labeling model (Shi and Lin, 2019; Pradhan et al., 2013) to indicate spans over the three semantic roles: predicates, A RG 0s and A RG 1s. We compute a contextualized vector representation for each semantic role span (sp , sa0 , sa1 ). We describe the process for obtaining predicate input representations v p here for illustration. Contextualized representations for views a0 (v a0 ) and a1 (v a1 ) are obtained analogously. First, we pass each sentence through a sentence encoder, and obtain the predicate embedding by averaging all contextualized token representations v w (of dimension Dw ) in its span sp of length |sp |: P v p = |s1p |w∈sp v w . (1)"
2021.naacl-main.174,P15-1157,0,0.0243478,"sues. Within the first approach, Matthes and Kohring (2008) developed a manual coding scheme, relying on Entman’s definition (Entman, 1993). While the scheme assumes that each frame is composed of common elements, categories within those elements are often specific to the particular issue being discussed (e.g., “same sex marriage” or “gun control”), making comparison across different issues, and detecting them automatically difficult. Similarly, earlier studies focusing specifically on unsupervised models to extract frames, usually employed topic modeling (Boydstun et al., 2013; Nguyen, 2015; Tsur et al., 2015) to find the issue-specific frames, limiting across-issue comparisons. Studies employing generic frames address this shortcoming by proposing common categories applicable to different issues. For example, Boydstun et al. (2013) proposed a list of 15 broad frame categories commonly used when discussing different policy issues, and in different communication contexts. The Media Frames Corpus (MFC; Card et al. (2015)) includes about 12,000 news articles from 13 U.S. newspapers covering five different policy issues, annotated with the dominant frame from Boydstun et al. (2013). Table 5 in the Appe"
2021.semeval-1.54,N19-1423,0,0.00947068,"sely related task is named-entity recognition (NER) whose goal is to detect mentions of named entities such as a Person or Organisation in an input sentence. Lample et al. (2016) introduced a now widely adopted neural architecture for this task, where input word embeddings are encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) before they are passed through per-word softmax layers. In recent times, it is common to replace the LSTM with a Transformer (Vaswani et al., 2017). With the advancements of large pretrained language models, the standard is to use a model such as BERT (Devlin et al., 2019) as the encoder and fine-tune the model on labelled data. The source model of the time expression recognition task provided by the organisers was also trained in this manner. In the context of unsupervised domain adaptation, a popular approach is domain adversarial training. Introduced by Ganin et al. (2016), it leverages multi-task learning in which the model is optimised 3 https://huggingface.co/clulab/ roberta-timex-semeval 4 https://github.com/huggingface/ transformers not only on the main task objective but also a domain prediction objective. The learning signal for the latter is passed t"
2021.semeval-1.54,D19-1433,0,0.0219778,"manner. In the context of unsupervised domain adaptation, a popular approach is domain adversarial training. Introduced by Ganin et al. (2016), it leverages multi-task learning in which the model is optimised 3 https://huggingface.co/clulab/ roberta-timex-semeval 4 https://github.com/huggingface/ transformers not only on the main task objective but also a domain prediction objective. The learning signal for the latter is passed through a gradient reversal layer, ensuring that the learnt parameters are predictive for the main task, but general across domains. With pre-trained language models, Han and Eisenstein (2019) proposed to continue pre-training on the unlabelled target domain data, prior to finally finetuning on the labelled source domain data. Unfortunately, these approaches require access to the source domain data. There is relatively little work on SFUDA in NLP, however, some works on source-free cross-lingual transfer exist. Wu et al. (2020) employ teacherstudent learning for source-free cross-lingual NER. A teacher model trained on the source side predicts soft labels on the unlabelled target side data, and a student model is trained on those soft labels. Their method outperforms a simple direc"
2021.semeval-1.54,P19-1311,0,0.0238538,"e model is distributed online via HuggingFace Models,3 which can be obtained with HuggingFace Transformers library.4 The organisers also released trial data for the practice phase containing 99 annotated English articles from the news domain. The official test data released by the organisers in the evaluation phase contains 47 articles that are in a different domain from the source and development data. The time expression recognition task is formalised as a sequence tagging task. The literature on sequence tagging in NLP is massive (Jiang et al., 2020; He and Choi, 2020; Rahimi et al., 2019; He et al., 2019; Xie et al., 2018; Clark et al., 2018, inter alia). One closely related task is named-entity recognition (NER) whose goal is to detect mentions of named entities such as a Person or Organisation in an input sentence. Lample et al. (2016) introduced a now widely adopted neural architecture for this task, where input word embeddings are encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) before they are passed through per-word softmax layers. In recent times, it is common to replace the LSTM with a Transformer (Vaswani et al., 2017). With the advancements of large pretrained la"
2021.semeval-1.54,P82-1020,0,0.774815,"Missing"
2021.semeval-1.54,2020.acl-main.192,0,0.0190144,"h clinical notes from Mayo Clinic in SemEval-2018 Task 6. The model is distributed online via HuggingFace Models,3 which can be obtained with HuggingFace Transformers library.4 The organisers also released trial data for the practice phase containing 99 annotated English articles from the news domain. The official test data released by the organisers in the evaluation phase contains 47 articles that are in a different domain from the source and development data. The time expression recognition task is formalised as a sequence tagging task. The literature on sequence tagging in NLP is massive (Jiang et al., 2020; He and Choi, 2020; Rahimi et al., 2019; He et al., 2019; Xie et al., 2018; Clark et al., 2018, inter alia). One closely related task is named-entity recognition (NER) whose goal is to detect mentions of named entities such as a Person or Organisation in an input sentence. Lample et al. (2016) introduced a now widely adopted neural architecture for this task, where input word embeddings are encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) before they are passed through per-word softmax layers. In recent times, it is common to replace the LSTM with a Transformer (Vaswani et"
2021.semeval-1.54,2021.eacl-main.254,1,0.929151,"ta contains protected information that cannot be shared, or even if it can, requires signing a complex data use agreement. While there are numerous works on SFUDA outside NLP (Hou and Zheng, 2020; Kim et al., 2020; Liang et al., 2020; Yang et al., 2020), SFUDA research for NLP is severely lacking in spite of its importance in, for example, clinical NLP (Laparra et al., 2020). There are two tasks involved in SemEval-2021 Task 10: negation detection and time expression recognition. We participate only in the latter. Our approach is an extension of the parsimonious parser transer framework (PPT; Kurniawan et al. (2021)). PPT allows cross-lingual transfer of dependency parsers in a source-free manner, requiring only unlabelled data in the target side. It leverages the output distribution of the source model to build a chart containing high probability trees for each sentence in the target data. We extend this work by (1) formulating PPT for chain structures and evaluating it on a semantic sequence tagging task; and (2) demonstrating its effectiveness in a domain adaptation setting. We call our method Parsimonious Transfer for Sequence Tagging (PTST). We find PTST effective for improving the precision of the"
2021.semeval-1.54,N16-1030,0,0.0460035,"s domain. The official test data released by the organisers in the evaluation phase contains 47 articles that are in a different domain from the source and development data. The time expression recognition task is formalised as a sequence tagging task. The literature on sequence tagging in NLP is massive (Jiang et al., 2020; He and Choi, 2020; Rahimi et al., 2019; He et al., 2019; Xie et al., 2018; Clark et al., 2018, inter alia). One closely related task is named-entity recognition (NER) whose goal is to detect mentions of named entities such as a Person or Organisation in an input sentence. Lample et al. (2016) introduced a now widely adopted neural architecture for this task, where input word embeddings are encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) before they are passed through per-word softmax layers. In recent times, it is common to replace the LSTM with a Transformer (Vaswani et al., 2017). With the advancements of large pretrained language models, the standard is to use a model such as BERT (Devlin et al., 2019) as the encoder and fine-tune the model on labelled data. The source model of the time expression recognition task provided by the organisers was also trained"
2021.semeval-1.54,2021.ccl-1.108,0,0.0394436,"Missing"
2021.semeval-1.54,P19-1015,1,0.818363,"mEval-2018 Task 6. The model is distributed online via HuggingFace Models,3 which can be obtained with HuggingFace Transformers library.4 The organisers also released trial data for the practice phase containing 99 annotated English articles from the news domain. The official test data released by the organisers in the evaluation phase contains 47 articles that are in a different domain from the source and development data. The time expression recognition task is formalised as a sequence tagging task. The literature on sequence tagging in NLP is massive (Jiang et al., 2020; He and Choi, 2020; Rahimi et al., 2019; He et al., 2019; Xie et al., 2018; Clark et al., 2018, inter alia). One closely related task is named-entity recognition (NER) whose goal is to detect mentions of named entities such as a Person or Organisation in an input sentence. Lample et al. (2016) introduced a now widely adopted neural architecture for this task, where input word embeddings are encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) before they are passed through per-word softmax layers. In recent times, it is common to replace the LSTM with a Transformer (Vaswani et al., 2017). With the advancements of la"
2021.semeval-1.54,2020.acl-demos.38,0,0.0553235,"Missing"
2021.semeval-1.54,2020.acl-main.581,0,0.0140003,"t also a domain prediction objective. The learning signal for the latter is passed through a gradient reversal layer, ensuring that the learnt parameters are predictive for the main task, but general across domains. With pre-trained language models, Han and Eisenstein (2019) proposed to continue pre-training on the unlabelled target domain data, prior to finally finetuning on the labelled source domain data. Unfortunately, these approaches require access to the source domain data. There is relatively little work on SFUDA in NLP, however, some works on source-free cross-lingual transfer exist. Wu et al. (2020) employ teacherstudent learning for source-free cross-lingual NER. A teacher model trained on the source side predicts soft labels on the unlabelled target side data, and a student model is trained on those soft labels. Their method outperforms a simple direct transfer method where the source model is directly applied on the target side. More recently, a method for source-free cross-lingual transfer of dependency parsers was introduced by Kurniawan et al. (2021). The key idea is to build a chart of high probability trees based on arc marginal probabilities for each unlabelled sentence on the t"
2021.semeval-1.54,D18-1034,0,0.0228675,"buted online via HuggingFace Models,3 which can be obtained with HuggingFace Transformers library.4 The organisers also released trial data for the practice phase containing 99 annotated English articles from the news domain. The official test data released by the organisers in the evaluation phase contains 47 articles that are in a different domain from the source and development data. The time expression recognition task is formalised as a sequence tagging task. The literature on sequence tagging in NLP is massive (Jiang et al., 2020; He and Choi, 2020; Rahimi et al., 2019; He et al., 2019; Xie et al., 2018; Clark et al., 2018, inter alia). One closely related task is named-entity recognition (NER) whose goal is to detect mentions of named entities such as a Person or Organisation in an input sentence. Lample et al. (2016) introduced a now widely adopted neural architecture for this task, where input word embeddings are encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) before they are passed through per-word softmax layers. In recent times, it is common to replace the LSTM with a Transformer (Vaswani et al., 2017). With the advancements of large pretrained language models, the"
C08-1013,P05-1074,1,0.9157,"tion Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a number of existing paraphrasing methods. 2 Related Work"
C08-1013,N03-1003,0,0.555789,"orrespondences between words in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMe"
C08-1013,P01-1008,0,0.389996,"are created by annotating correspondences between words in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the ali"
C08-1013,J93-2003,0,0.0107748,"veres El mar arroja tantos cadáveres de inmigrantes ilegales ahogados playa ....73 Align .62a la .65 P rec corpses of drowned illegals get washed.14 up on .33 beaches.68 ... LB-P recision So many AlignRecall .11 .10 .46 Rel-Recall .07 .03 .01 Figure 3: Bannard and Callison-Burch (2005) extracted paraphrases by equating English phrases that share a common translation. Table 2: Summary results for scoring the different paraphrasing techniques using our proposed automatic evaluations. Where p(e1 |e2 ) is estimated by training word alignment models over the “parallel corpus” as in the IBM Models (Brown et al., 1993), and phrase translations are extracted from word alignments as in the Alignment Template Model (Och, 2002). Bannard and Callison-Burch (2005) also used techniques from statistical machine translation to identify paraphrases. Rather than drawing pairs of English sentences from a comparable corpus, Bannard and Callison-Burch (2005) used bilingual parallel corpora. They identified English paraphrases by pivoting through phrases in another language. They located foreign language translations of an English phrase, and treated the other English translations of those foreign phrases as potential par"
C08-1013,N06-1003,1,0.731127,"Missing"
C08-1013,W03-1608,0,0.602423,"from the alignments; • Report ParaMetric scores for a number of existing paraphrasing methods. 2 Related Work No consensus has been reached with respect to the proper methodology to use when evaluating paraphrase quality. This section reviews past methods for paraphrase evaluation. Researchers usually present the quality of their automatic paraphrasing technique in terms of a subjective manual evaluation. These have used a variety of criteria. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 97 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 97–104 Manchester, August 2008 3 sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” These subjective"
C08-1013,J03-1002,0,0.00444764,"gure 1. For the results reported in this paper, annotators aligned 50 groups of 10 pairs of equivalent sentences, for a total of 500 sentence pairs. These were assembled by pairing the first of the LDC translations with the other ten (i.e. 1-2, 1-3, 1-4, ..., 1-11). The choice of pairing one sentence with the others instead of doing all pairwise combinations was made simply because the latter would not seem to add much information. However, the choice of using the first translator as the key was arbitrary. Annotators corrected a set of automatic word alignments that were created using Giza++ (Och and Ney, 2003), which was trained on a total of 109,230 sentence pairs created from all pairwise combinations of the eleven translations of 993 Chinese sentences. The average amount of time spent on each of the sentence pairs was 77 seconds, with just over eleven hours spent to annotate all 500 sentence 5 ParaMetric Scores We can exploit the manually aligned data to compute scores in two different fashions. First, we can calculate how well an automatic paraphrasing technique is able to align the paraphrases in a sentence pair. Second, we can calculate the lowerbound on precision for a paraphrasing technique"
C08-1013,J04-4002,0,0.0204135,"rase. Figure 1 shows the alignments that were created between one sentence and three of its ten corresponding translations. Table 1 gives a list of non-identical words and phrases that can be paired by way of the word alignments. These are the basic paraphrases contained within the three sentence pairs. Each phrase has up to three paraphrases. The maximum number of paraphrases for a given span in each sentence is bounded by the number of equivalent sentences that it is paired with. In addition to these basic paraphrases, longer paraphrases can also be obtained using the heuristic presented in Och and Ney (2004) for extracting phrase pairs (PP) from word alignments A, between a foreign sentence f1J and an English sensome want to impeach him and others expect him to step down . there are those who propose impeaching him and those who want him to tender his resignation . some are proposing an indictment against him and some want him to leave office voluntarily . Figure 1: Pairs of English sentences were aligned by hand. Black squares indicate paraphrase correspondences. 1 99 See LDC catalog number 2002T01. some want to impeach and others expect step down some people, there are those who propose, are pr"
C08-1013,N03-1024,0,0.768729,"ords in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a n"
C08-1013,P02-1040,0,0.116829,"erence paraphrases can be extracted from the gold standard alignments. While these sets will obviously be fragmentary, we attempt to make them more complete by aligning groups of equivalent sentences rather than only pairs. The paraphrase sets that we extract are appropriate for the particular contexts. Moreover they may potentially be used to study structural paraphrases, although we do not examine that Others evaluate paraphrases in terms of whether they improve performance on particular tasks. Callison-Burch et al. (2006b) measure improvements in translation quality in terms of Bleu score (Papineni et al., 2002) and in terms of subjective human evaluation when paraphrases are integrated into a statistical machine translation system. Lin and Pantel (2001) manually judge whether a paraphrase might be used to answer questions from the TREC question-answering track. To date, no one has used task-based evaluation to compare different paraphrasing methods. Even if such an evaluation were performed, it is unclear whether the results would hold for a different task. Because of this, we strive for a general evaluation rather than a task-specific one. Dolan et al. (2004) create a set of manual word alignments"
C08-1013,W04-3219,0,0.80361,"anslations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a number of existing pa"
C08-1013,P07-1058,0,0.0301292,"If a list of reference paraphrases is incomplete, then using it to calculate precision will give inaccurate numbers. Precision will be falsely low if the system produces correct paraphrases which are not in the reference list. Additionally, recall is indeterminable because there is no way of knowing how many correct paraphrases exist. There are further impediments to automatically evaluating paraphrases. Even if we were able to come up with a reasonably exhaustive list of paraphrases for a phrase, the acceptability of each paraphrase would vary depending on the context of the original phrase (Szpektor et al., 2007). While lexical and phrasal paraphrases can be evaluated by comparing them against a list of known paraphrases (perhaps customized for particular contexts), this cannot be naturally done for structural paraphrases which may transform whole sentences. We attempt to resolve these problems by having annotators indicate correspondences in pairs of equivalent sentences. Rather than having people enumerate paraphrases, we asked that they perform the simper task of aligning paraphrases. After developing these manual “gold standard alignments” we can gauge how well different automatic paraphrases are"
C08-1013,C04-1051,0,\N,Missing
C08-1013,J08-4005,1,\N,Missing
C08-1018,P05-1074,0,0.0722015,"n and Lapata (2007) (see Section 5 for details) and augment it with a larger grammar obtained from a parallel bilingual corpus. Crucially, our second grammar will not contain compression rules, just paraphrasing ones. We leave it to the model to learn which rules serve the compression objective. Our paraphrase grammar extraction method uses bilingual pivoting to learn paraphrases over syntax tree fragments, i.e., STSG rules. Pivoting treats the paraphrasing problem as a two-stage translation process. Some English text is translated to a foreign language, and then translated back into English (Bannard and Callison-Burch, 2005): X p(e0 |e) = p(e0 |f )p(f |e) (4) f where p(f |e) is the probability of translating an English string e into a foreign string f and p(e0 |f ) the probability of translating the same foreign string into some other English string e0 . We thus obtain English-English translation probabilities p(e0 |e) by marginalizing out the foreign text. Instead of using strings (Bannard and CallisonBurch, 2005), we use elementary trees on the English side, resulting in a monolingual STSG. We obtain the elementary trees and foreign strings using the GKHM algorithm (Galley et al., 2004). This takes as input a b"
C08-1018,E99-1042,0,0.214303,"is trained discriminatively. Specifically, we generalise the model of Cohn and Lapata (2007) to our abstractive task. We present a novel tree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including a ngram language model as a feature. We also develop a number of loss functions suited to the abstractive compression task. We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al., 1999). 2 Abstractive Compression Corpus A stumbling block to studying abstractive sentence compression is the lack of widely available corpora for training and testing. Previous work has been conducted almost exclusively on Ziff-Davis, a corpus derived automatically from document abstract pairs (Knight and Marcu, 2002), or on humanauthored corpora (Clarke, 2008). Unfortunately, none of these data sources are suited to our problem since they have been produced with a single rewriting operation, namely word deletion. Although there is a greater supply of paraphrasing corpora, such as the Multiple-Tra"
C08-1018,J07-2003,0,0.0157973,"target ngrams: Ψ(y) = X r∈y hφ(r, S(y)), λi + X hψ(m, S(y)), λi m∈T (y) (5) where m are the ngrams and ψ is a new feature function over these ngrams (we use only one ngram feature: the trigram log-probability). Sadly, the scoring function in (5) renders the chart-based search used for training and decoding intractable. In order to provide sufficient context to the chartbased algorithm, we must also store in each chart cell the n − 1 target tokens at the left and right edges of its yield. This is equivalent to using as our grammar the intersection between the original grammar and the ngram LM (Chiang, 2007), and increases the decoding complexity to an infeasible O(SRL2(n−1)V ) where L is the size of the lexicon. We adopt a popular approach in syntax-inspired machine translation to address this problem (Chiang, 2007). The idea is to use a beam-search over the intersection grammar coupled with the cubepruning heuristic. The beam limits the number of items in a given chart cell to a fixed constant, regardless of the number of possible LM contexts and non-terminal categories. Cube-pruning further limits the number of items considered for inclusion in the beam, reducing the time complexity to a more"
C08-1018,D07-1008,1,0.545564,"org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and insertion. In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). Therefore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts. In this framework, the goal is to find a sum"
C08-1018,P02-1057,0,0.0528497,"poken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 200"
C08-1018,P03-2041,0,0.0182914,"oncerns the modeling task itself. Ideally, our learning framework should handle structural mismatches and complex rewriting operations. In what follows, we first present a new corpus for abstractive compression which we created by having annotators compress sentences while rewriting them. Besides obtaining useful data for modeling purposes, we also demonstrate that abstractive compression is a meaningful task. We then present a tree-to-tree transducer capable of transforming an input parse tree into a compressed parse tree. Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. Specifically, we generalise the model of Cohn and Lapata (2007) to our abstractive task. We present a novel tree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including a ngram language model as a feature. We also develop a number of loss functions suited to the abstractive compression task. We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-docu"
C08-1018,N07-1023,0,0.0450448,"tive Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and insertion. In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). Therefore, in this paper we consider sentence compression from a more general perspec"
C08-1018,N04-1035,0,0.008486,"to English (Bannard and Callison-Burch, 2005): X p(e0 |e) = p(e0 |f )p(f |e) (4) f where p(f |e) is the probability of translating an English string e into a foreign string f and p(e0 |f ) the probability of translating the same foreign string into some other English string e0 . We thus obtain English-English translation probabilities p(e0 |e) by marginalizing out the foreign text. Instead of using strings (Bannard and CallisonBurch, 2005), we use elementary trees on the English side, resulting in a monolingual STSG. We obtain the elementary trees and foreign strings using the GKHM algorithm (Galley et al., 2004). This takes as input a bilingual word-aligned corpus with trees on one side, and finds the minimal set of tree fragments and their corresponding strings which is consistent with the word alignment. This process is illustrated in Figure 2 where the aligned pair on the left gives rise to the rules shown on the right. Note that the English rules and foreign strings shown include variable indices where they have been generalised. We estimate p(f |e) and p(e0 |f ) from the set of tree-to-string rules and then then pivot each tree fragment to produce STSG rules. Figure 3 illustrates the process for"
C08-1018,A00-1043,0,0.141015,"s a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. 1 Introduction Automatic sentence compression can be broadly described as the task of creating a grammatical summary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of subtitles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005"
C08-1018,N06-1014,0,0.0239638,"(T), pivoted set (P) or generated from the source (S). 5 Experimental Design In this section we present our experimental setup for assessing the performance of our model. We give details on the corpora and grammars we used, model parameters and features,6 the baseline used for comparison with our approach, and explain how our system output was evaluated. Grammar Extraction Our grammar used rules extracted directly from our compression corpus (the training partition, 480 sentences) and a bilingual corpus (see Table 2 for examples). The former corpus was word-aligned using the Berkeley aligner (Liang et al., 2006) initialised with a lexicon of word identity mappings, and parsed with Bikel’s (2002) parser. From this we extracted grammar rules following the technique described in Cohn and Lapata (2007). For the pivot grammar we use the French-English Europarl v2 which contains approximately 688K sentences. Again, the corpus was aligned using the Berkeley aligner and the English side was parsed with Bikel’s parser. We extracted tree-to-string rules using our implementation of the GHKM method. To ameliorate the effects of poor alignments on the grammar, we removed singleton rules before pivoting. In additi"
C08-1018,W03-1101,0,0.0165856,"model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. 1 Introduction Automatic sentence compression can be broadly described as the task of creating a grammatical summary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of subtitles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald,"
C08-1018,E06-1038,0,0.595873,"Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, su"
C08-1018,P02-1040,0,0.0786148,"Missing"
C08-1018,W04-3219,0,0.252886,"efore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts. In this framework, the goal is to find a summary of the original sentence which is grammatical and conveys the most important information without necessarily using the same words in the same order. Our task is related to, but different from, paraphrase extraction (Barzilay, 2003). We must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them in order to generate new text, while attempting to produce a shorter resulting string. Quirk et al. (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both acquire paraphrases and use them to generate new strings. However, their model is limited to lexical substitution — no reordering takes place — and is 137 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137–144 Manchester, August 2008 lacking the compression objective. Once we move away from extractive compression we are faced with two problems. First, we must find an appropriate training set for our abstractive task. Compression corpora are n"
C08-1018,P05-1036,0,0.0191217,"summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, includi"
C08-1018,W04-1015,0,0.0145789,"ee transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. 1 Introduction Automatic sentence compression can be broadly described as the task of creating a grammatical summary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of subtitles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the t"
C16-3006,Q16-1034,1,0.874466,"Missing"
D07-1008,P01-1008,0,0.0985742,"ases or sentences in a document will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervise"
D07-1008,E99-1042,0,0.0558307,"sion bring significant improvements over a state-of-the-art model. 1 Introduction Recent years have witnessed increasing interest in text-to-text generation methods for many natural language processing applications ranging from text summarisation to question answering and machine translation. At the heart of these methods lies the ability to perform rewriting operations according to a set of prespecified constraints. For example, text simplification identifies which phrases or sentences in a document will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on"
D07-1008,P05-1033,0,0.0360254,"urage or discourage compression (see Figure 4), but admittedly in other frameworks (e.g., Clarke and Lapata (2006a)) the length of the compression can be influenced more naturally. In our formulation of the compression problem, a derivation is characterised by a single inventory of features. This entails that the feature space cannot in principle distinguish between derivations that use the same rules, applied in a different order. Although, this situation does not arise often in our dataset, we believe that it can be ameliorated by intersecting a language model with our generation algorithm (Chiang, 2005). 81 Conclusions and Future Work In this paper we have presented a novel method for sentence compression cast in the framework of structured learning. We develop a system that generates compressions using a synchronous tree substitution grammar whose weights are discriminatively trained within a large margin model. We also describe an appropriate algorithm than can be used in both training (i.e., learning the model weights) and decoding (i.e., finding the most plausible compression under the model). The proposed formulation allows us to capture rewriting operations that go beyond word deletion"
D07-1008,P06-2019,1,0.920713,"Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long sentences and their compressions are isomorphic. Consequently, the models are not generally applicable to other text rewriting problems since they cannot readily handle structural mismatches and more complex rewriting operations such as substitutions or insertions. A related iss"
D07-1008,P06-1048,1,0.760964,"Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long sentences and their compressions are isomorphic. Consequently, the models are not generally applicable to other text rewriting problems since they cannot readily handle structural mismatches and more complex rewriting operations such as substitutions or insertions. A related iss"
D07-1008,D07-1001,1,0.84516,"ond, it also uses large margin learning. Sentence compression is formulated as a string-to-substring mapping problem with a deletion-based Hamming loss. Recall that our formulation involves a tree-to-tree mapping. Third, it uses a feature space complementary to ours. For example features are defined between adjacent words, and syntactic evidence is incorporated indirectly into the model. In contrast our model relies on synchronous rules to generate valid compressions and does not explicitly incorporate adjacency features. We used an implementation of McDonald (2006) for comparison of results (Clarke and Lapata, 2007). Evaluation Measures In line with previous work we assessed our model’s output by eliciting human judgements. Participants were presented with an original sentence and its compression and asked to rate the latter on a five point scale based on the information retained and its grammaticality. We conducted two separate elicitation studies, one for the 4 The corpus can be downloaded from http://homepages. inf.ed.ac.uk/s0460084/data/. 79 O: I just wish my parents and my other teachers could be like this teacher, so we could communicate. M: I wish my teachers could be like this teacher. S: I wish"
D07-1008,P03-2041,0,0.575422,"e compressions will have well-formed syntactic structures. And it will not be easy to process them for subsequent generation or analysis tasks. In this paper we present a text-to-text rewriting 73 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 73–82, Prague, June 2007. 2007 Association for Computational Linguistics model that scales to non-isomorphic cases and can thus naturally account for structural and lexical divergences. Our approach is inspired by synchronous tree substitution grammar (STSG, Eisner (2003)) a formalism that allows local distortion of the tree topology. We show how such a grammar can be induced from a parallel corpus and propose a large margin model for the rewriting task which can be viewed as a weighted tree-to-tree transducer. Our learning framework makes use of the algorithm put forward by Tsochantaridis et al. (2005) which efficiently learns a prediction function to minimise a given loss function. Experiments on sentence compression show significant improvements over the state-of-the-art. Beyond sentence compression and related text-to-text generation problems (e.g., paraph"
D07-1008,N07-1023,0,0.173654,"t is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structure"
D07-1008,N04-1014,0,0.0390924,"Missing"
D07-1008,A00-1043,0,0.379828,"methods for many natural language processing applications ranging from text summarisation to question answering and machine translation. At the heart of these methods lies the ability to perform rewriting operations according to a set of prespecified constraints. For example, text simplification identifies which phrases or sentences in a document will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly de"
D07-1008,N03-1017,0,0.0150456,"ld06 STSG Gold standard CompR 68.6 73.7 76.1 RelF1 47.6 53.4∗ — 45 50 F1 ● Ziff-Davis McDonald06 STSG Gold standard ● 60 65 70 75 80 Prec Prec.BP1 Prec.BP2 85 compression rate Figure 4: Compression rate vs. grammatical relations F1 using unigram precision alone and in combination with two brevity penalties. a parallel corpus of syntax trees. We obtained syntactic analyses for source and target sentences with Bikel’s (2002) parser. Our corpora were automatically aligned with Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the intersection heuristic (Koehn et al., 2003). Each word in the lexicon was also aligned with itself. This was necessary in order to inform Giza++ about word identity. Unparseable sentences and those longer than 50 tokens were removed from the data set. We induced a synchronous tree substitution grammar from the Ziff-Davis and Broadcast news corpora using the method described in Section 3.2. We extracted all maximally general synchronous rules. These were complemented with more specific rules from conjoining pairs of general rules. The specific rules were pruned to remove singletons and those rules with more than 3 variables. Grammar rul"
D07-1008,E06-1038,0,0.105015,"corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long sentences and their"
D07-1008,P04-1083,0,0.0322744,"Missing"
D07-1008,J04-4002,0,0.128935,"Missing"
D07-1008,W99-0604,0,0.0869948,"fferent compressions of the same sentence. 60 55 ● ● CompR 66.2 56.8 57.2 RelF1 45.8 54.3 — Broadcast News McDonald06 STSG Gold standard CompR 68.6 73.7 76.1 RelF1 47.6 53.4∗ — 45 50 F1 ● Ziff-Davis McDonald06 STSG Gold standard ● 60 65 70 75 80 Prec Prec.BP1 Prec.BP2 85 compression rate Figure 4: Compression rate vs. grammatical relations F1 using unigram precision alone and in combination with two brevity penalties. a parallel corpus of syntax trees. We obtained syntactic analyses for source and target sentences with Bikel’s (2002) parser. Our corpora were automatically aligned with Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the intersection heuristic (Koehn et al., 2003). Each word in the lexicon was also aligned with itself. This was necessary in order to inform Giza++ about word identity. Unparseable sentences and those longer than 50 tokens were removed from the data set. We induced a synchronous tree substitution grammar from the Ziff-Davis and Broadcast news corpora using the method described in Section 3.2. We extracted all maximally general synchronous rules. These were complemented with more specific rules from conjoining pairs of general"
D07-1008,N03-1024,0,0.0794727,"ent will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting"
D07-1008,P02-1040,0,0.084125,"nes as the tree derivation.3 Given this restriction, we define a loss based on position-independent unigram precision (Prec) which penalises errors in the yield independently for each word. Although fairly intuitive, this loss is far from ideal. First, it maximally rewards repeatedly predicting the same word if the latter is in the reference target tree. Secondly, it may bias towards overly short output which drops core information — one-word compressions will tend to have higher precision than longer output. To counteract this, we introduce two brevity penalty measures (BP) inspired by BLEU (Papineni et al., 2002) which we incorporate into the loss function, using a product, loss = 1 − Prec · BP: r BP1 = exp(1 − max(1, )) c c r BP2 = exp(1 − max( , )) r c value one when c = r and decays towards zero for c < r and c > r. In both cases, brevity is assessed against the gold standard target (not the source) to allow the system to learn the correct degree of compression from the training data. Maximisation Algorithm Our algorithm finds the maximising derivation for H(y) in (5). This derivation will have a high loss and a high score under the model, and therefore represents the most-violated constraint which"
D07-1008,W04-3219,0,0.0819526,"ces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a"
D07-1008,N03-1026,0,0.0603333,"fic. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long se"
D07-1008,P05-1036,0,0.40567,"o-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption"
D07-1008,J08-3004,0,\N,Missing
D09-1037,P09-1088,1,0.484558,"g-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled probabilistic formulations for heuristic translation models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009b; Blunsom et al., 2009a). This model leverages a hierarchical Bayesian prior to induce a compact translation grammar directly from a parsed parallel corpus, unconstrained by word-alignments. We show that the induced grammars are more plausible and improve translation output. This paper is structured as follows: In Section Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with s"
D09-1037,W08-0336,0,0.0169488,"rs in this way constitutes a single sample from the Gibbs sampler. {rp = h(NP DT 1 NN 2 ), 1 2 i, rv = h(DT Every), 每i, 5 rw = h(NN corner), 一 个 角落i} , Experiments We evaluate our non-parametric model of grammar induction on a subset of the NIST ChineseEnglish translation evaluation, representing a realistic SMT experiment with millions of words and long sentences. The Chinese-English training data consists of the FBIS corpus (LDC2003E14) and the first 100k sentence pairs from the Sinorama corpus (LDC2005E47). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). As a baseline we implemented the heuristic grammar extraction technique of Galley et al. (2004) (henceforth GHKM). This method finds the minimum sized translation rules which are consistent with a word-aligned sentence pair, as and the right option implies rules {rp = h(NP DT 1 NN 2 ), 2 1 i, rv = h(DT Every), 一 个 角落i, rw = h(NN corner), 每i} . We simply evaluate the probability of both triples of rules under our model, P (rp , rv , rw |r− ) = P (rp |r− )P (rv |r− , rp ) P (rw |r− , rp , rv ), where the addit"
D09-1037,J07-2003,0,0.0967129,"P Foreign) (NNP Ministry) NN 1 (NNP Zhu) (NNP Bangzao)), 外交部 1 朱邦造i h(S S 1 S 2 ), 1 2 i h(S S 1 (NP (PRP We)) VP 2 . 3 ), 1 2 3 i h(NP (DT the) (NNS people) POS 1 ), 人民 1 i phrases with an noun phrase. 5.2 Translation In order to test the translation performance of the grammars induced by our model and the GHKM method6 we report B LEU (Papineni et al., 2002) scores on sentences of up to twenty words in length from the MT03 NIST evaluation. We built a synchronous beam search decoder to find the maximum scoring derivation, based on the CYK+ chart parsing algorithm and the cubepruning method of Chiang (2007). Parse edges for all constituents spanning a given chart cell were cube-pruned together using a beam of width 1000, and only edges from the top ten constituents in each cell were retained. No artificial glue-rules or rule span limits were employed.7 The parameters of the translation system were trained to maximize B LEU on the MT02 test set (Och, 2003). Decoding took roughly 10s per sentence for both grammars, using a 8-core 2.6Ghz Intel Xeon machine. Table 6 shows the B LEU scores for the baseline using the GHKM rule induction algorithm, and our non-parametric Bayesian grammar induction meth"
D09-1037,N09-1062,1,0.116442,"nment points cross constituent structures), resulting in large and implausible translation rules which generalise poorly to unseen data (Fossum et al., 2008). The principal reason for employing a grammar based formalism is to induce rules which capture long-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled probabilistic formulations for heuristic translation models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009b; Blunsom et al., 2009a). This model leverages a hierarchical Bayesian prior to induce a compact translation grammar directly from a parsed parallel corpus, unconstrained by word-alignments. We show that the induced grammars are more plausible and improve translation output. This paper is structured as follows: In Section Tree based transl"
D09-1037,W06-3105,0,0.0772044,"th syntax trees on target side. Their method projects the source strings onto nodes of the target tree using the word alignment, and then extracts the minimal transduction rules as well as rules composed of adjacent minimal units. The production weights are estimated either by heuristic counting (Koehn et al., 2003) or using the EM algorithm. Both estimation techniques are flawed. The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al., 2006). With no limit on rule size this method will learn a single rule for every training instance, and therefore will not generalise to unseen sentences. These problems can be ameliorated by imposing limits on rule size or early stopping of EM training, however neither of these techniques addresses the underlying problems. Background Current tree-to-string translation models are a form of Synchronous Tree Substitution Grammar (STSG; Eisner (2003)). Formally, a STSG is a 5-tuple, G = (T, T 0 , N, S, R), where T and T 0 are sets of terminal symbols in the target and source languages respectively, N"
D09-1037,D08-1033,0,0.193956,"Missing"
D09-1037,P03-2041,0,0.0692023,"le EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al., 2006). With no limit on rule size this method will learn a single rule for every training instance, and therefore will not generalise to unseen sentences. These problems can be ameliorated by imposing limits on rule size or early stopping of EM training, however neither of these techniques addresses the underlying problems. Background Current tree-to-string translation models are a form of Synchronous Tree Substitution Grammar (STSG; Eisner (2003)). Formally, a STSG is a 5-tuple, G = (T, T 0 , N, S, R), where T and T 0 are sets of terminal symbols in the target and source languages respectively, N is a set of nonterminal symbols, S ∈ N is the distinguished root non-terminal and R is a set of productions (a.k.a. rules). Each production is a tuple comprising an elementary tree and a string, the former referring to a tree fragment of depth ≥ 1 where each internal node is labelled with a non-terminal and each leaf is labelled with either a terminal or a non-terminal. The string part of the rule describes the lexical component of the rule i"
D09-1037,W08-0306,0,0.0174814,"om a word-aligned parallel corpus. These heuristics are extensions of those developed for phrase-based models (Koehn et al., 2003), and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees (Galley et al., 2004). However, such approaches leave much to be desired. Word-alignments rarely factorise cleanly with parse trees (i.e., alignment points cross constituent structures), resulting in large and implausible translation rules which generalise poorly to unseen data (Fossum et al., 2008). The principal reason for employing a grammar based formalism is to induce rules which capture long-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled pro"
D09-1037,N04-1035,0,0.371857,"Missing"
D09-1037,P03-1021,0,0.0391611,"02) scores on sentences of up to twenty words in length from the MT03 NIST evaluation. We built a synchronous beam search decoder to find the maximum scoring derivation, based on the CYK+ chart parsing algorithm and the cubepruning method of Chiang (2007). Parse edges for all constituents spanning a given chart cell were cube-pruned together using a beam of width 1000, and only edges from the top ten constituents in each cell were retained. No artificial glue-rules or rule span limits were employed.7 The parameters of the translation system were trained to maximize B LEU on the MT02 test set (Och, 2003). Decoding took roughly 10s per sentence for both grammars, using a 8-core 2.6Ghz Intel Xeon machine. Table 6 shows the B LEU scores for the baseline using the GHKM rule induction algorithm, and our non-parametric Bayesian grammar induction method. We see a small increase in generalisation performance from our model. Our previous analTable 4: Top ten rules in the GHKM grammar that do not appear in the sampled grammar. These are quite low probability rules: their counts range from 1,137 to 103. For example, every instance of the first rule had the same determiner and target translation, h(PP (I"
D09-1037,P02-1040,0,0.107151,"h(NP NP 1 , 2 NP 3 (, ,) CC 4 NP 5 ), 1 2 3 4 5 i h(NP NP 1 , 2 NP 3 , 4 NP 5 (, ,) (CC and) NP 6 ), 1 2 3 4 5 , 6 i h(S S 1 (NP (PRP They)) VP 2 . 3 ), 1 2 3 i h(S PP 1 , 2 NP 3 VP 4 . 5 “ 6 ), 1 2 3 4 6 5 i h(S PP 1 , 2 NP 3 VP 4 . 5 ), 1 中 2 3 4 5 i h(NP (NNP Foreign) (NNP Ministry) NN 1 (NNP Zhu) (NNP Bangzao)), 外交部 1 朱邦造i h(S S 1 S 2 ), 1 2 i h(S S 1 (NP (PRP We)) VP 2 . 3 ), 1 2 3 i h(NP (DT the) (NNS people) POS 1 ), 人民 1 i phrases with an noun phrase. 5.2 Translation In order to test the translation performance of the grammars induced by our model and the GHKM method6 we report B LEU (Papineni et al., 2002) scores on sentences of up to twenty words in length from the MT03 NIST evaluation. We built a synchronous beam search decoder to find the maximum scoring derivation, based on the CYK+ chart parsing algorithm and the cubepruning method of Chiang (2007). Parse edges for all constituents spanning a given chart cell were cube-pruned together using a beam of width 1000, and only edges from the top ten constituents in each cell were retained. No artificial glue-rules or rule span limits were employed.7 The parameters of the translation system were trained to maximize B LEU on the MT02 test set (Och"
D09-1037,P06-1121,0,0.713577,"anguages. Such models are particularly attractive for translating between languages with divergent word orders, such as Chinese and English, where syntax-inspired translation rules can succinctly describe the requisite reordering operations. In contrast, standard phrase-based models (Koehn et al., 2003) assume a mostly monotone mapping between source and target, and therefore cannot adequately model these phenomena. Currently the most successful paradigm for the use of synchronous grammars in translation is that of stringto-tree transduction (Galley et al., 2004; Zollmann and Venugopal, 2006; Galley et al., 2006; Marcu et al., 2006). In this case a grammar is extracted from a parallel corpus, with strings on its source 352 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 we introduce the STSG formalism and describe current heuristic approaches to grammar induction. We define a principled Bayesian model of string-to-tree translation in Section 3, and describe an inference technique using Gibbs sampling in Section 4. In Section 5 we analyse an induced grammar on a corpus of Chinese→English translati"
D09-1037,J95-2002,0,0.0733489,"miner or a preposition. The grammars differ on the complex rules which combine lexicalisation and frontier non-terminals. The GHKM rules are all very simple depth-1 SCFG rules, containing minimal information. In contrast, the sampled rules are more lexicalised, licensing the insertion of various English tokens and tree substructure. Note particularly the second and forth rule which succinctly describe the reordering of prepositional 6 Our decoder was unable to process unary rules (those which consume nothing in the source). Monolingual parsing with unary productions is fairly straightforward (Stolcke, 1995), however in the transductive setting these rules can licence infinite insertions in the target string. This is further complicated by the language model integration. Therefore we composed each unary rule instance with its descendant rule(s) to create a non-unary rule. 7 Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation (Zhang et al., 2006). As such the reported results for MT03 lag the state-of-the-art: the Moses phrase-based decoder (Koehn et al., 2007) achieves 26.8. We believe that improvements from a better decoder i"
D09-1037,N06-1033,0,0.0747636,"eordering of prepositional 6 Our decoder was unable to process unary rules (those which consume nothing in the source). Monolingual parsing with unary productions is fairly straightforward (Stolcke, 1995), however in the transductive setting these rules can licence infinite insertions in the target string. This is further complicated by the language model integration. Therefore we composed each unary rule instance with its descendant rule(s) to create a non-unary rule. 7 Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation (Zhang et al., 2006). As such the reported results for MT03 lag the state-of-the-art: the Moses phrase-based decoder (Koehn et al., 2007) achieves 26.8. We believe that improvements from a better decoder implementation would be orthogonal to the improvements presented here (and would allow us to relax the length restriction on the test set). 359 Model GHKM Our model BLEU score 26.0 26.6 Acknowledgements The authors acknowledge the support of the EPSRC (grants GR/T04557/01 and EP/D074959/1). This work has made use of the resources provided by the Edinburgh Compute and Data Facility (ECDF). The ECDF is partially su"
D09-1037,P08-1012,0,0.107099,"lism is to induce rules which capture long-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled probabilistic formulations for heuristic translation models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009b; Blunsom et al., 2009a). This model leverages a hierarchical Bayesian prior to induce a compact translation grammar directly from a parsed parallel corpus, unconstrained by word-alignments. We show that the induced grammars are more plausible and improve translation output. This paper is structured as follows: In Section Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to dat"
D09-1037,W06-3119,0,0.0936833,"mprove translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step. 1 Introduction Many recent advances in statistical machine translation (SMT) are a result of the incorporation of syntactic knowledge into the translation process (Marcu et al., 2006; Zollmann and Venugopal, 2006). This has been facilitated by the use of synchronous grammars to model translation as a generative process over pairs of strings in two languages. Such models are particularly attractive for translating between languages with divergent word orders, such as Chinese and English, where syntax-inspired translation rules can succinctly describe the requisite reordering operations. In contrast, standard phrase-based models (Koehn et al., 2003) assume a mostly monotone mapping between source and target, and therefore cannot adequately model these phenomena. Currently the most successful paradigm for"
D09-1037,J02-1005,0,0.0430767,"grammar rules from word aligned data is also nontrivial. Galley et al. (2004) describe an algorithm for inducing a string-to-tree grammar using a parallel corpus with syntax trees on target side. Their method projects the source strings onto nodes of the target tree using the word alignment, and then extracts the minimal transduction rules as well as rules composed of adjacent minimal units. The production weights are estimated either by heuristic counting (Koehn et al., 2003) or using the EM algorithm. Both estimation techniques are flawed. The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al., 2006). With no limit on rule size this method will learn a single rule for every training instance, and therefore will not generalise to unseen sentences. These problems can be ameliorated by imposing limits on rule size or early stopping of EM training, however neither of these techniques addresses the underlying problems. Background Current tree-to-string translation models are a form of Synchronous Tree Substitution Grammar (ST"
D09-1037,N03-1017,0,0.476825,"ax-Directed Tree to String Grammar Induction Trevor Cohn and Phil Blunsom School of Informatics University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB Scotland, United Kingdom {tcohn,pblunsom}@inf.ed.ac.uk Abstract side and syntax trees on its target side, which is then used to translate novel sentences by performing inference over the space of target syntax trees licensed by the grammar. To date grammar-based translation models have relied on heuristics to extract a grammar from a word-aligned parallel corpus. These heuristics are extensions of those developed for phrase-based models (Koehn et al., 2003), and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees (Galley et al., 2004). However, such approaches leave much to be desired. Word-alignments rarely factorise cleanly with parse trees (i.e., alignment points cross constituent structures), resulting in large and implausible translation rules which generalise poorly to unseen data (Fossum et al., 2008). The principal reason for employing a grammar based formalism is to induce rules which capture long-range reo"
D09-1037,P07-2045,0,0.00785667,"). Monolingual parsing with unary productions is fairly straightforward (Stolcke, 1995), however in the transductive setting these rules can licence infinite insertions in the target string. This is further complicated by the language model integration. Therefore we composed each unary rule instance with its descendant rule(s) to create a non-unary rule. 7 Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation (Zhang et al., 2006). As such the reported results for MT03 lag the state-of-the-art: the Moses phrase-based decoder (Koehn et al., 2007) achieves 26.8. We believe that improvements from a better decoder implementation would be orthogonal to the improvements presented here (and would allow us to relax the length restriction on the test set). 359 Model GHKM Our model BLEU score 26.0 26.6 Acknowledgements The authors acknowledge the support of the EPSRC (grants GR/T04557/01 and EP/D074959/1). This work has made use of the resources provided by the Edinburgh Compute and Data Facility (ECDF). The ECDF is partially supported by the eDIKT initiative. Table 6: Translation results on the NIST test set MT03 for sentences of length ≤ 20."
D09-1037,W06-1606,0,0.157756,"Missing"
D09-1037,J03-1002,0,\N,Missing
D10-1047,R09-1002,1,0.933025,"asured with the final parameters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-b"
D10-1047,P10-1127,1,0.336084,"eters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-best lists,5 and use MERT to"
D10-1047,aker-gaizauskas-2010-model,1,0.872562,"eters after training to optimise ROUGE-2 with the three different heuristics and expanding five nodes in each step. score, R, and the loss is simply 1 - R. The training problem is to solve ˆ = arg min ∆(ˆ λ y, r) , Summarization system The summarizer we use is an extractive, query-based multi-document summarization system. It is given two inputs: a query (place name) associated with an image and a set of documents. The summarizer uses the following features, as reported in previous work (Edmundson, 1969; Brandow et al., 1995; Radev et al., 2001; Conroy et al., 2005; Aker and Gaizauskas, 2009; Aker and Gaizauskas, 2010a): ● (4) λ ˆ and r are where with a slight abuse of notation, y taken to range over the corpus of many documentsets and summaries. To optimise the weights we use the minimum error rate training (MERT) technique (Och, 2003), as used for training statistical machine translation systems. This approach is a first order optimization method using Powell search to find the parameters which minimise the loss on the training data. MERT requires n-best lists which it uses to approximate the full space of possible outcomes. We use the A* search algorithm to construct these n-best lists,5 and use MERT to"
D10-1047,W08-1405,0,0.0350931,"Missing"
D10-1047,J97-1003,0,0.104665,"oblem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a training set consisting of pairs of document sets with model or reference summaries – typically human authored extractive or abstractive summaries. Search is typically performed by a greedy algorithm which selects each sentence in decreasing order of model score until the desired summary length is reached (see, e.g., Saggion (2005)) or using heuristic strategies based on position in document or lexical clues (Edmundson, 1969; Brandow et al., 1995; Hearst, 1997; Ouyang et al., 2010).1 We show in this paper that the search problem can be solved optimally and efficiently using A* search (Russell et al., 1995). Assuming the model only uses features local to each sentence in the summary, our algorithm finds the best scoring extractive summary up to a given length in words. Framing summarization as search suggests that many of the popular training techniques are maximising the wrong objective. These approaches train a classifier, regression or ranking model to distinguish between good and bad sentences under an evaluation metric, e.g., ROUGE (Lin, 2004)."
D10-1047,W01-0100,0,0.699137,"ve training algorithm which directly maximises the quality of the best summary, rather than assuming a sentence-level decomposition as in earlier work. Our approach leads to significantly better results than earlier techniques across a number of evaluation metrics. 1 Introduction Multi-document summarization aims to present multiple documents in form of a short summary. This short summary can be used as a replacement for the original documents to reduce, for instance, the time a reader would spend if she were to read the original documents. Following dominant trends in summarization research (Mani, 2001), we focus solely on extractive summarization which simplifies the summarization task to the problem of identifying a subset of units from the document collection (here sentences) which are concatenated to form the summary. Most multi-document summarization systems define a model which assigns a score to a candidate summary based on the features of the sentences included in the summary. The research challenges are then twofold: 1) the search problem of finding the best scoring summary for a given document set, and 2) the training problem of learning the model parameters to best describe a trai"
D10-1047,P03-1021,0,0.0638436,"nd -SU4. The paper is structured as follows. Section 2 presents the summarization model. Next in section 3 we present an A* search algorithm for finding the best scoring (argmax) summary under the model with a constraint on the maximum summary length. We show that this algorithm performs search efficiently, even for very large document sets composed of many sentences. The second contribution of the paper is a new training method which directly optimises the summarization system, and is presented in section 4. This uses the minimum error-rate training (MERT) technique from machine translation (Och, 2003) to optimise the summariser’s output to an arbitrary evaluation metric. Section 5 describes our experimental setup and section 6 the results. Finally we conclude in section 7. 2 Summarization Model Extractive multi-document summarization aims to find the most important sentences from a set of documents, which are then collated and presented to the user in form of a short summary. Following the predominant approach to data-driven summarisation, we define a linear model which scores summaries as the weighted sum of their features, s(y|x) = Φ(x, y) · λ , (1) where x is the document set, composed"
D10-1047,W04-1013,0,\N,Missing
D10-1117,P06-1109,0,0.0176898,"dard part-of-speech tags after removing punctuation. We use a vague Beta prior for the stopping probabilities in Plcfg , sc ∼ Beta(1, 1). All the hyper-parameters are resampled after every 10th sample of the corpus derivations. 4.3 Parsing Unfortunately finding the maximising parse tree for a string under our TSG-DMV model is intractable due to the inter-rule dependencies created by the PYP formulation. Previous work has used Monte Carlo techniques to sample for one of the maximum probability parse (MPP), maximum probability derivation (MPD) or maximum marginal parse (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and reporting hea"
D10-1117,P04-1014,0,0.119639,"Missing"
D10-1117,W01-0713,0,0.00939395,"agments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy. 1 Introduction Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus. Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches (Clark, 2001; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk more fruitful (K"
D10-1117,N09-1009,0,0.617435,"alue 0 (no attachment in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent; superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more children and X∗ that it has already attached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3 ) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this"
D10-1117,N09-1062,1,0.908726,"ponent rules. Estimating a PTSG requires learning the sufficient statistics for P (e|c) in (1) based on a training sample. Parsing involves finding the most probable tree for a given string (arg maxt P (t|w)). This is typically approximated by finding the most probable derivation which can be done efficiently using the CYK algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest"
D10-1117,J03-4003,0,0.172699,"Missing"
D10-1117,N09-1012,0,0.714087,"Missing"
D10-1117,P07-1022,0,0.0282885,"tached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3 ) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this grammar is that the subscripts encode the terminals at the boundaries of the span of that non-terminal. For example the non-terminal LH encodes that the right most terminal spanned by this constituent is H (and the reverse for H R), while A MB encodes that A and B are the left-most 1205 and"
D10-1117,P02-1017,0,0.124434,"hereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy. 1 Introduction Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus. Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches (Clark, 2001; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk more fruitful (Klein and Manning, 2004). D"
D10-1117,P04-1061,0,0.850032,"1; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk more fruitful (Klein and Manning, 2004). Dependency grammars (Mel0 cˇ uk, 1988) should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence. Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models (Clark, 2001; Klein and Manning, 2004), especially in comparison to supervised parsing models (Collins, 2003; Clark and Curran, 2004; McDonald, 2006). Simple models are attractive for grammar induction because they have a limited capacity to overfit, however"
D10-1117,J93-2004,0,0.0383775,"e (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and reporting head attachment accuracy. Like previous work we pre-process the training and test data to remove punctuation, training our unlexicalised models on the gold-standard part-of-speech tags, and including words occurring more than 100 times in our lexicalised models (Headden III et al., 2009). It is very difficult for an unsupervised model to learn from long training sentences as they contain 1209 a great deal of ambiguity, therefore the majority of DMV based models have been trained on sentences restricted in length to ≤ 10 tokens.3 This has the added benefit of decreasing the runti"
D10-1117,N10-1116,0,0.33116,"ation (σ) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3 See Spitkovsky et al. (2010a) for an exception to this rule. icantly better than this model. We can identify a number of differences that may impact these results: Model |w |≤ 10 |w |≤ ∞ the Adaptor Grammar model is trained using variational inference with the space of tree fragments Attach-Right 38.4 31.7 truncated, while we employ a sampler which can EM (Klein and Manning, 2004) 46.1 35.9 nominally explore the full space of tree fragments; Dirichlet (Cohen et al., 2008) 46.1 36.9 and the adapted tree fragments must be complete LN (Cohen et al., 2008) 59.4 40.5 subtrees (i.e. they don’t contain variables), whereas SLN,"
D10-1117,W10-2902,0,0.337904,"ation (σ) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3 See Spitkovsky et al. (2010a) for an exception to this rule. icantly better than this model. We can identify a number of differences that may impact these results: Model |w |≤ 10 |w |≤ ∞ the Adaptor Grammar model is trained using variational inference with the space of tree fragments Attach-Right 38.4 31.7 truncated, while we employ a sampler which can EM (Klein and Manning, 2004) 46.1 35.9 nominally explore the full space of tree fragments; Dirichlet (Cohen et al., 2008) 46.1 36.9 and the adapted tree fragments must be complete LN (Cohen et al., 2008) 59.4 40.5 subtrees (i.e. they don’t contain variables), whereas SLN,"
D10-1117,P10-1130,0,0.201528,"ation (σ) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3 See Spitkovsky et al. (2010a) for an exception to this rule. icantly better than this model. We can identify a number of differences that may impact these results: Model |w |≤ 10 |w |≤ ∞ the Adaptor Grammar model is trained using variational inference with the space of tree fragments Attach-Right 38.4 31.7 truncated, while we employ a sampler which can EM (Klein and Manning, 2004) 46.1 35.9 nominally explore the full space of tree fragments; Dirichlet (Cohen et al., 2008) 46.1 36.9 and the adapted tree fragments must be complete LN (Cohen et al., 2008) 59.4 40.5 subtrees (i.e. they don’t contain variables), whereas SLN,"
D10-1117,P06-1124,0,0.019717,"sing the CYK algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest model which has a four level hierarchy, depicted graphically in Table 2. In Section 5 we evaluate different subsets of this hierarchy. The topmost level of our model describes lexicalised elementary elementary fragments (e) as produced by a PYP, ∼ Gc e|c lcfg Gc |ac , bc , P ∼ PYP(ac , bc , Plcfg (·|c)"
D10-1117,N10-1081,0,\N,Missing
D10-1117,P10-2042,1,\N,Missing
D12-1109,P05-1022,0,0.0171017,"the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏"
D12-1109,P05-1033,0,0.199485,"ompete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based mod"
D12-1109,J07-2003,0,0.371901,"ure consistent syntactic transformations between the source and target languages, e.g., from subject-verb-object to subject-object-verb word orderings. Decoding algorithms for grammar-based translation seek to find the best string in the intersection between a weighted context free grammar (the translation mode, given a source string/tree) and a weighted finite state acceptor (an n-gram language model). This intersection is problematic, as it results in an intractably large grammar, and makes exact search impossible. Most researchers have resorted to approximate search, typically beam search (Chiang, 2007). The decoder parses the source sentence, recording the target translations for each span.1 As the partial translation hypothesis grows, its component ngrams are scored and the hypothesis score is updated. This decoding method though is inefficient as it requires recording the language model context (n − 1 words) on the left and right edges of each chart cell. These contexts allow for boundary ngrams to be evaluated when the cell is used in another grammar production. In contrast, if the target string is generated in left-to-right order, then only one language model context is required, and th"
D12-1109,P05-1066,0,0.0607385,"5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively and the improvements are statistically significant using the sign-test of Collins et al. (2005) (p < 0.01). The improvement may result from dynamically searching for a whole derivation which leads to more accurate estimation of a partial derivation. The additional time consumption of the bottom-up decoder against the top-down decoder comes from dynamic future cost computation. Next we compare decoding speed versus translation quality using various beam sizes. The results are shown in Figure 5. We can see that our bottomup decoder can produce better BLEU score at the same decoding speed. At small beams (decoding time around 0.5 second), the improvement of translation quality is much bigg"
D12-1109,N10-1128,0,0.0532922,"e set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tre"
D12-1109,P81-1022,0,0.741421,"nt Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earle"
D12-1109,C10-2033,1,0.88656,"ovel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-str"
D12-1109,D08-1089,0,0.0379223,"scribe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-string decoder implemented according to Liu et al. (2006) (denoted as traditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the"
D12-1109,N04-1035,0,0.0554355,"t decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order traversal. For each node, it applies matched STSG rules by substituting each non-terminal with its cor1192 traditional top-down bottom-up in theory O(nc˙|V |4(g−1) ) O(c(cr)d |V |g−1 ) O((cr)d |V |g−1 ) beam search O(ncb2 ) O(ncb) O(nub) Table 1: Time complexity of different algorithms. tradition"
D12-1109,W05-1506,0,0.043939,"Missing"
D12-1109,D10-1027,0,0.664834,"rding to a postorder traversal. 1191 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our a"
D12-1109,2006.amta-papers.8,0,0.277257,"algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order trav"
D12-1109,P08-1067,0,0.0222632,"(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r)"
D12-1109,J99-4005,0,0.212094,"e: 4 We bundle the successive terminals in one rule into a symbol 1195 grow [ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a d"
D12-1109,koen-2004-pharaoh,0,0.674331,"[ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the futu"
D12-1109,P06-1077,1,0.969992,"od uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) a"
D12-1109,P08-1023,1,0.858066,"NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r) π∈rhs(r) f (π) ot"
D12-1109,J03-1002,0,0.00491025,"topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. System Traditional Top-down Bottom-up 5.1 Data Setup Time (s) 0.84 0.41 0.81 Table 3: Performance comparison. 30.8 30.6 30.4 BLEU Score We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximi"
D12-1109,P03-1021,0,0.0478665,"Missing"
D12-1109,P02-1040,0,0.0894745,"Missing"
D12-1109,P06-1098,0,0.566743,"as “closure” actions. That is to say, once there are some complete actions after a scan action, we finish all the compete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CK"
D13-1100,P13-1004,1,0.107869,"implementation of the PS kernel in the popular opensource Gaussian Processes packages GPML1 and GPy2 are available on the author’s website3 . 2 Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with"
D13-1100,2013.mtsummit-papers.21,0,0.0760968,"S kernel in the popular opensource Gaussian Processes packages GPML1 and GPy2 are available on the author’s website3 . 2 Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of feat"
D13-1100,D11-1055,0,0.0384925,"Missing"
D13-1100,P13-4014,1,\N,Missing
D14-1096,N10-1083,0,0.0759904,"Missing"
D14-1096,J96-1002,0,0.0233732,"e DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior"
D14-1096,A00-1031,0,0.101349,"Missing"
D14-1096,W06-2920,0,0.500674,"than using a dictionary. We argue 1 http://www.wiktionary.org/ Das and Petrov (2011) Duong et al. (2013b) Li et al. (2012) T¨ackstr¨om et al. (2013) da 83.2 85.6 83.3 88.2 nl 79.5 84.0 86.3 85.9 de 82.8 85.4 85.4 90.5 el 82.5 80.4 79.2 89.5 it 86.8 81.4 86.5 89.3 pt 87.9 86.3 84.5 91.0 es 84.2 83.3 86.4 87.1 sv 80.5 81.0 86.1 88.9 Average 83.4 83.4 84.8 88.8 Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages — Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006). that with a proper “guide”, we can take advantage of very limited annotated data. 2.1 Annotated data Our annotated data mainly comes from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006). The language specific tagsets are mapped into the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless"
D14-1096,D10-1056,0,0.024443,"Missing"
D14-1096,P11-1061,0,0.272017,"Missing"
D14-1096,I13-1177,1,0.896928,"Missing"
D14-1096,P13-2112,1,0.89119,"Missing"
D14-1096,P00-1006,0,0.0400144,"ictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P"
D14-1096,P08-1085,0,0.180683,"Missing"
D14-1096,W04-3229,0,0.406957,"Missing"
D14-1096,A00-2021,0,0.022186,"prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P (t|w) are both maximum entropy models. In this case we show that th"
D14-1096,2005.mtsummit-papers.11,0,0.110128,"Missing"
D14-1096,D12-1127,0,0.158782,"Missing"
D14-1096,J03-1002,0,0.00706494,"Missing"
D14-1096,E99-1010,0,0.0863202,"Missing"
D14-1096,petrov-etal-2012-universal,0,0.0846555,"the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless, we try to use as few resources as possible, in order to simulate the situation for resource-poor languages. Later in Section 6 we adapt the approach for Malagasy, a truly resource-poor language. 2.2 Universal tagset We employ the universal tagset from (Petrov et al., 2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner and article), ADP (preposition and postposition), CONJ (conjunctions), NUM (numerical), PRT (particle), PUNC (punctuation) and X (all other categories including foreign words and abbreviations). Petrov et al. (2012) provide the mapping from each language-specific tagset to the universal tagset. The idea of using the universal tagset is of great use in multilingual applications, enabling comparison across languages. However, the mapping is not always straightforward. Ta"
D14-1096,N13-1014,0,0.0990194,"cussed it makes many errors, due to invalid or inconsistent tag mappings, noisy alignments, and cross-linguistic syntactic divergence. However, our aim is to see how effectively we can exploit the strengths of the DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating"
D14-1096,W08-1302,0,0.056461,"Missing"
D14-1096,P13-1057,0,0.203159,"Missing"
D14-1096,E14-1078,0,0.023072,"s. However, the mapping is not always straightforward. Table 2 shows the size of the annotated data for each language, the number of tags presented in the data, and the list of tags that are not matched. We can see that only 8 tags are presented in the annotated data for Danish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are missing.2 Thus, a classifier using all 12 tags will be heavily penalized in the evaluation. Li et al. (2012) considered this problem and tried to manually modify the Danish mappings. Moreover, PRT is not really a universal tag since it only appears in 3 out of the 8 languages. Plank et al. (2014) pointed out that PRT often gets confused with ADP even in English. We will later show that the mapping problem causes substantial degradation in the performance of a POS tagger exploiting parallel data. The method we present here is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the mapping, and other incompatibilities arising from incorrect alignments and syntactic divergence between the source and target languages. Lang Size(k) # Tags da 94 8 nl 203 11 de"
D14-1096,W11-3603,0,0.0222103,"Missing"
D14-1096,Q13-1001,0,0.132253,"Missing"
D14-1096,N03-1033,0,0.0842473,"Missing"
D14-1096,P08-1086,0,0.101901,"Missing"
D14-1096,N01-1026,0,0.12215,"Missing"
D14-1190,W13-2241,1,0.502038,"borrow statistical strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f (x) + η, where η ∼ N (0, σn2 ) is the added white noise and the function f is drawn from"
D14-1190,P13-1004,1,0.574242,"ions and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches. 1 Introduction Multi-task learning (Caruana, 1997) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Domain Adaptation (Daum´e III, 2007; Finkel and Manning, 2009), where we hypothesize that we can learn from multiple domains by assuming similarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators (Cohn and Specia, 2013). The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks. In Domain Adaptation, we assume that some “general”, domainindependent knowledge exists in the data. For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when m"
D14-1190,P07-1033,0,0.315185,"Missing"
D14-1190,2013.mtsummit-papers.21,1,0.726565,"strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f (x) + η, where η ∼ N (0, σn2 ) is the added white noise and the function f is drawn from a GP prior: f (x) ∼"
D14-1190,D08-1027,0,0.113416,"Missing"
D14-1190,S07-1013,0,0.269124,"Missing"
D14-1190,N09-1068,0,0.0127108,"ting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches. 1 Introduction Multi-task learning (Caruana, 1997) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Domain Adaptation (Daum´e III, 2007; Finkel and Manning, 2009), where we hypothesize that we can learn from multiple domains by assuming similarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators (Cohn and Specia, 2013). The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks. In Domain Adaptation, we assume that some “general”, domainindependent knowledge exists in the data. For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this tru"
D14-1190,D12-1054,0,0.136491,"For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when making predictions, avoiding negative knowledge transfer. An example of a problem that shows this behaviour is Emotion Analysis, where the goal is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008; Mihalcea and Strapparava, 2012). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels (one for each emotion) instead of a single label. While we expect some emotions to have some degree of correlation, this is usually not the case for all possible emotions. For instance, we expect sadness and joy to be anti-correlated. We propose a multi-task setting for Emotion Analysis based on a vector-valued Gaussian Process (GP) approach known as coregionalisation ´ (Alvarez et al., 2012). The"
D14-1190,D13-1100,1,\N,Missing
D15-1028,P15-2085,1,0.900329,"#39 2h (b) rumour #60 Figure 1: Intensity functions and corresponding predicted arrival times for different methods across example Ferguson rumours. Arrival times predicted by LGCP are denoted by red pluses, LGCPTXT by blue dots, and ground truth by black crosses. Light regions denote uncertainty of predictions. mours around 2014 Ferguson unrest. It consists of conversational threads that have been manually labeled by annotators to correspond to rumours1 . Since some rumours have few posts, we consider only those with at least 15 posts in the first hour as they express interesting behaviour (Lukasik et al., 2015). This results in 114 rumours consisting of a total of 4098 tweets. inter-arrival times as independent and exponentially distributed with a constant rate parameter. A similar model is used by Sakaki et al. (2010) to monitor the tweets related to earthquakes. The renewal process model used by Esteban et al. (2012) assumes the inter-arrival times to be independent and identically distributed. Gonzalez et al. (2014) attempts to model arrival times of tweets using a Gaussian process but assumes the tweet arrivals to be independent every hour. These approaches do not take into account the varying c"
D15-1028,N13-1039,0,0.099375,"Missing"
D15-1040,D12-1001,0,0.060049,"s are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource requi"
D15-1040,D14-1034,0,0.007514,"Missing"
D15-1040,kamholz-etal-2014-panlex,0,0.0227054,"Missing"
D15-1040,H05-1091,0,0.019228,"Missing"
D15-1040,D14-1082,0,0.127244,"es from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource require2.1 Supervised Neural Network Parser This section describes the monolingual neural network dependency parser structure of Chen and Manning (2014). This parser achieves excellent performance, and has a highly flexible formulation allowing auxilliary inputs. The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted. Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer neural network classifier to pred"
D15-1040,2005.mtsummit-papers.11,0,0.0112439,"Missing"
D15-1040,P14-1126,0,0.00966165,"parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS)"
D15-1040,D11-1006,0,0.168279,"alized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Langu"
D15-1040,P15-2139,1,0.534832,"e treebank covers 10 languages,6 with some languages very highly resourced—Czech, French and Spanish have 400k tokens—and only modest amounts of data for other languages—Hungarian and Irish have only around 25k tokens. Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus. 4.2 Baseline Cascade Model We compare our approach to a baseline interlingual model based on the same parsing algorithm as presented in section 2.1, but with cascaded training (Duong et al., 2015). This works by first learning the source language parser, and then training the target language parser using a regularization term to minimise the distance between the parameters of the target parser and the source parser (which is fixed). In this way, some structural information from the source parser can be used in the target parser, however it is likely that the representation will be overly biased towards the source language and consequently may not prove as useful for modelling the target. 4.3 Monolingual Word Embeddings While the E pos and E arc are randomly initialized, we initialize b"
D15-1040,N09-1028,0,0.0541894,"Missing"
D15-1040,skadins-etal-2014-billions,0,0.0228432,"Missing"
D15-1040,D13-1170,0,0.00392188,"Missing"
D15-1040,N12-1052,0,0.160554,"Missing"
D15-1040,N13-1126,0,0.0678031,"Missing"
D15-1040,I08-3008,0,\N,Missing
D15-1288,D07-1090,0,0.128612,"a collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracti"
D15-1288,P96-1041,0,0.495082,"ich provides near optimal compression while supporting efficient search. We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model. Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through ∞-order modeling over the full Wikipedia collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepp"
D15-1288,W09-1505,0,0.222632,"on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from C STs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two C STs (over the corpus and the reversed corpus), which allow for efficient computation of the number of unique contexts to the left and right of an mgram, but is inefficient"
D15-1288,D10-1026,0,0.238269,"d Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from C STs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two C STs (over the corpus and the reversed corpus), which allow"
D15-1288,W11-2123,0,0.180217,"Missing"
D15-1288,kennington-etal-2012-suffix,0,0.0888967,"Missing"
D15-1288,P07-1065,0,0.046429,"are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from C STs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two C STs (over the corpus and the rev"
D15-1288,P09-2086,0,0.24557,"Missing"
D15-1288,P11-1027,0,0.132637,"Missing"
D15-1311,W11-0705,0,0.0568982,"The first setting (denoted GP) considers only target rumour data for training. The second (GPPooled) additionally considers tweets from reference rumours (i.e. other than the target rumour). The third setting is GPICM, where an ICM kernel is used to weight influence from tweets from reference rumours. 6 Features We conducted a series of preprocessing steps in order to address data sparsity. All words were lowercased; stopwords removed; all emoticons were replaced with words2 ; and stemming was performed. In addition, multiple occurrences of a character were replaced with a double occurrence (Agarwal et al., 2011), to correct for misspellings and lengthenings, e.g., looool. All punctuation was also removed, except for ., ! and ?, which we hypothesize to be important for expressing emotion. Lastly, usernames were removed as they tend to be rumour-specific, i.e., very few users comment on more than one rumour. After preprocessing the text data, we use either the resulting bag of words (BOW) feature representation or replace all words with their Brown cluster ids (Brown), using 1000 clusters acquired from a large scale Twitter corpus (Owoputi et al., 2013). In all cases, simple re-tweets are removed from"
D15-1311,D14-1190,1,0.658627,"Missing"
D15-1311,P13-1004,1,0.421575,"discussed and the attitude expressed towards that. This information can be acquired either via manual annotation as part of expert analysis, as is the case with our dataset, or automatically, e.g. using pattern-based rumour detection (Zhao et al., 2015). Afterwards, our method can be used to classify the attitudes expressed in each new tweet from outside the training set. 5 Gaussian Processes for Classification Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods (Cohn and Specia, 2013; Lampos et al., 2014; Beck et al., 2014; Preotiuc-Pietro et al., 2015). We use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive crossvalidation for hyperparameter selection.1 1 There exist frequentist kernel methods, like SVMs, which additionally require extensive heldout parameter tuning. The central concept of Gaussian Process Classification (GPC; (Rasmussen and Williams, 2005)) is a latent function f over inputs x: f (x) ∼ GP(m(x), k(x, x0 )), where m is the mean function, assumed to be 0 and k is the kernel function, specifying the degree to whic"
D15-1311,E14-1043,1,0.828062,"tude expressed towards that. This information can be acquired either via manual annotation as part of expert analysis, as is the case with our dataset, or automatically, e.g. using pattern-based rumour detection (Zhao et al., 2015). Afterwards, our method can be used to classify the attitudes expressed in each new tweet from outside the training set. 5 Gaussian Processes for Classification Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods (Cohn and Specia, 2013; Lampos et al., 2014; Beck et al., 2014; Preotiuc-Pietro et al., 2015). We use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive crossvalidation for hyperparameter selection.1 1 There exist frequentist kernel methods, like SVMs, which additionally require extensive heldout parameter tuning. The central concept of Gaussian Process Classification (GPC; (Rasmussen and Williams, 2005)) is a latent function f over inputs x: f (x) ∼ GP(m(x), k(x, x0 )), where m is the mean function, assumed to be 0 and k is the kernel function, specifying the degree to which the outputs covary"
D15-1311,N13-1039,0,0.0153413,"character were replaced with a double occurrence (Agarwal et al., 2011), to correct for misspellings and lengthenings, e.g., looool. All punctuation was also removed, except for ., ! and ?, which we hypothesize to be important for expressing emotion. Lastly, usernames were removed as they tend to be rumour-specific, i.e., very few users comment on more than one rumour. After preprocessing the text data, we use either the resulting bag of words (BOW) feature representation or replace all words with their Brown cluster ids (Brown), using 1000 clusters acquired from a large scale Twitter corpus (Owoputi et al., 2013). In all cases, simple re-tweets are removed from the training set to prevent bias (Llewellyn et al., 2014). 2 We used the dictionary from: http://bit.ly/ 1rX1Hdk and extended it with: :o, : |, =/, :s, :S, :p. method acc Majority GPPooled Brown GPPooled BOW 0.68 0.72 0.69 Table 3: Accuracy taken across all rumours in the LOO setting. 7 Experiments and Discussion Table 3 shows the mean accuracy in the LOO scenario following the GPPooled method, which pools all reference rumours together ignoring their task identities. ICM can not use correlations to target rumour in this case and so can not be"
D15-1311,P15-1169,0,0.0454968,"ation can be acquired either via manual annotation as part of expert analysis, as is the case with our dataset, or automatically, e.g. using pattern-based rumour detection (Zhao et al., 2015). Afterwards, our method can be used to classify the attitudes expressed in each new tweet from outside the training set. 5 Gaussian Processes for Classification Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods (Cohn and Specia, 2013; Lampos et al., 2014; Beck et al., 2014; Preotiuc-Pietro et al., 2015). We use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive crossvalidation for hyperparameter selection.1 1 There exist frequentist kernel methods, like SVMs, which additionally require extensive heldout parameter tuning. The central concept of Gaussian Process Classification (GPC; (Rasmussen and Williams, 2005)) is a latent function f over inputs x: f (x) ∼ GP(m(x), k(x, x0 )), where m is the mean function, assumed to be 0 and k is the kernel function, specifying the degree to which the outputs covary as a function of the inputs. We use a linear kerne"
D15-1311,D11-1147,0,0.740962,"Missing"
D15-1311,llewellyn-etal-2014-using,0,0.0253452,"lengthenings, e.g., looool. All punctuation was also removed, except for ., ! and ?, which we hypothesize to be important for expressing emotion. Lastly, usernames were removed as they tend to be rumour-specific, i.e., very few users comment on more than one rumour. After preprocessing the text data, we use either the resulting bag of words (BOW) feature representation or replace all words with their Brown cluster ids (Brown), using 1000 clusters acquired from a large scale Twitter corpus (Owoputi et al., 2013). In all cases, simple re-tweets are removed from the training set to prevent bias (Llewellyn et al., 2014). 2 We used the dictionary from: http://bit.ly/ 1rX1Hdk and extended it with: :o, : |, =/, :s, :S, :p. method acc Majority GPPooled Brown GPPooled BOW 0.68 0.72 0.69 Table 3: Accuracy taken across all rumours in the LOO setting. 7 Experiments and Discussion Table 3 shows the mean accuracy in the LOO scenario following the GPPooled method, which pools all reference rumours together ignoring their task identities. ICM can not use correlations to target rumour in this case and so can not be used. The majority baseline simply assigns the most frequent class from the training set. We can observe th"
D15-1311,P15-2085,1,0.716506,"Missing"
D16-1094,W11-2123,0,0.285088,"one of language modeling, decompose the probability of an utterance into conditional probabilities of words given a fixed-length context. Due to sparsity of the events in natural language, smoothing techniques are critical for generalisation beyond the training text when estimating the parameters of m-gram LMs. This is particularly important when the training text is 1 For the implementation see: https://github.com/ eehsan/cstlm Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more compact (Stolcke et al., 2011; Heafield, 2011; Shareghi et al., 2015) using advanced data structures such as succinct suffix trees. An exception is Hierarchical Pitman-Yor Process LMs (Teh, 2006a; Teh, 2006b) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an approximate inference method. Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference. We provide estimators for our additional discount parameters by extending the discount bounds in MKN. We empirically analyze our enriched MKN LMs on several European"
D16-1094,2005.mtsummit-papers.11,0,0.105888,"s,  if i = 0 0, ni+1 [m] n1 [m] m D (i) = i − (i + 1) ni [m] n1 [m]+2n2 [m] , if i < 10  n11 [m] n1 [m] 10 − 11 n10 [m] . n1 [m]+2n2 [m] , if i ≥ 10 It can be shown that the above estimators for our discount parameters are derived by maximizing a lower bound on the leave-one-out likelihood of the training set, following (Ney et al., 1994; Chen and Goodman, 1999) (see Appendix B for the proof sketch). 3 Experiments We compare the effect of using different numbers of discount parameters on perplexity using the Finnish (FI), Spanish (ES), German (DE), English (EN) portions of the Europarl v7 (Koehn, 2005) corpus. For each language we excluded the first 10K sentences and used it as the in-domain test set (denoted as EU), skipped the second 10K sentences, and used the rest as the training set. The data was tokenized, sentence split, and the XML markup discarded. We tested the effect of domain mismatch, under two settings for out-of-domain test sets: i) mild using the Spanish section of news-test 2013, the German, English sections of news-test 2014, and the Finnish section 4 We have selected the value of 10 arbitrarily; however our approach can be used with larger number of discount parameters, w"
D16-1094,D15-1288,1,0.833577,"modeling, decompose the probability of an utterance into conditional probabilities of words given a fixed-length context. Due to sparsity of the events in natural language, smoothing techniques are critical for generalisation beyond the training text when estimating the parameters of m-gram LMs. This is particularly important when the training text is 1 For the implementation see: https://github.com/ eehsan/cstlm Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more compact (Stolcke et al., 2011; Heafield, 2011; Shareghi et al., 2015) using advanced data structures such as succinct suffix trees. An exception is Hierarchical Pitman-Yor Process LMs (Teh, 2006a; Teh, 2006b) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an approximate inference method. Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference. We provide estimators for our additional discount parameters by extending the discount bounds in MKN. We empirically analyze our enriched MKN LMs on several European languages in in- and out"
D16-1094,P06-1124,0,0.0559302,"of the events in natural language, smoothing techniques are critical for generalisation beyond the training text when estimating the parameters of m-gram LMs. This is particularly important when the training text is 1 For the implementation see: https://github.com/ eehsan/cstlm Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more compact (Stolcke et al., 2011; Heafield, 2011; Shareghi et al., 2015) using advanced data structures such as succinct suffix trees. An exception is Hierarchical Pitman-Yor Process LMs (Teh, 2006a; Teh, 2006b) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an approximate inference method. Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference. We provide estimators for our additional discount parameters by extending the discount bounds in MKN. We empirically analyze our enriched MKN LMs on several European languages in in- and outof-domain settings. The results show that our discounting mechanism significantly improves the perplexity compared to MKN and"
D16-1136,W13-3520,0,0.0658583,"Missing"
D16-1136,P11-1061,0,0.0122427,"l corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluatio"
D16-1136,P15-2139,1,0.561844,"ngual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity"
D16-1136,P15-1033,0,0.017942,"ere unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vecto"
D16-1136,E14-1049,0,0.506581,"comparable part of Wikipedia. They represent word using Wikipedia entries which are shared for many languages. A bilingual dictionary is an alternative source of bilingual information. Gouws and Søgaard (2015) randomly replace the text in a monolingual corpus with a random translation, using this corpus for learning word embeddings. Their approach doesn’t handle polysemy, as very few of the translations for each word will be valid in context. For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes. Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary. Our approach also uses a dictionary, however we use all the translations and explicitly disambiguate translations during training. Another distinguishing feature on the above-cited research is the method for training embeddings. Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary. Most"
D16-1136,N15-1157,0,0.254278,"pseudo-documents are then used for learning vector representations using Word2Vec. Their system, despite its simplicity, performed surprisingly well on a bilingual lexicon induction task (we compare our method with theirs on this task.) Their approach is compelling due to its lesser resource requirements, although comparable bilingual data is scarce for many languages. Related, Søgaard et al. (2015) exploit the comparable part of Wikipedia. They represent word using Wikipedia entries which are shared for many languages. A bilingual dictionary is an alternative source of bilingual information. Gouws and Søgaard (2015) randomly replace the text in a monolingual corpus with a random translation, using this corpus for learning word embeddings. Their approach doesn’t handle polysemy, as very few of the translations for each word will be valid in context. For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes. Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary. Our approach also uses a dictiona"
D16-1136,P14-1006,0,0.0206465,"olingual, bilingual and crosslingual transfer settings. 2 Related work There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource. This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012). These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level (Chandar A P et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) through learning compositional vector representations of sentences, 1286 in order that sentences and their translations representations closely match. The word embeddings learned this way capture translational equivalence, despite not using explicit word alignments. Nevertheless, these approaches demand large parallel corpora, which are not available for many language pairs. Vuli´c and Moens (2015) use bilingual comparable text, sourced from Wikipedia. Their approach creates a psuedo-document by forming a bag-ofwords from the lemmatized nouns in each comparable document c"
D16-1136,kamholz-etal-2014-panlex,0,0.0470178,"entries across several languages, however this approach tends to underperform other methods. To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015). However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm. Other methods use a dictionary as the bridge between languages (Mikolov et al., 2013a; Xiao and Guo, 2014), however they do not adequately handle translation ambiguity. Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal. Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages. Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through incorporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999)). In addition to the dictionary, 1285 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1285–1295, c Austin, Te"
D16-1136,C12-1089,0,0.0985088,"au et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity and bilingual lexicon induction tasks. Moreover, many prior work (Chandar A P et al., 2014; Koˇcisk´y et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages. Søgaard et al. (2015) impose a less onerous data condition in the form of"
D16-1136,P14-2037,0,0.0305298,"Missing"
D16-1136,W13-3512,0,0.0424945,"1995) to lemmatize the output of our combined model before evaluation. Table 3 (+lemmatization) shows some improvements but minor. It demonstrates that our model is already good at disambiguating morphology. For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation. 7 Monolingual Word Similarity Now we consider the efficacy of our CLWE on monolingual word similarity. We evaluate on English monolingual similarity on WordSim353 (WSen), RareWord (RW-en) and German version of WordSim353 (WS-de) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015). Each of those datasets contain many tuples (w1 , w2 , s) where s is a scalar denoting the semantic similarity between w1 and w2 given by human annotators. Good system should produce the score correlated with human judgement. We train the model as described in §4, which is the combine embeddings setting from Table 3. Since the evaluation involves de and en word similarity, we train the CLWE for en-de pair. Table 4 shows the performance of our combined model compared with several baselines. Our combined model out-performed both Luong et al. (2015) and Gouws and Søgaard (20"
D16-1136,W15-1521,0,0.243658,"Missing"
D16-1136,N13-1090,0,0.651605,"e corpus which is also expensive for many low-resource languages. Søgaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods. To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015). However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm. Other methods use a dictionary as the bridge between languages (Mikolov et al., 2013a; Xiao and Guo, 2014), however they do not adequately handle translation ambiguity. Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal. Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages. Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through incorporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999"
D16-1136,D14-1162,0,0.0902224,"Missing"
D16-1136,D13-1170,0,0.00406887,"iculty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target langu"
D16-1136,P15-1165,0,0.0410086,"Missing"
D16-1136,N12-1052,0,0.0329814,"Missing"
D16-1136,P15-2118,0,0.153094,"Missing"
D16-1136,N01-1026,0,0.0254739,"dels using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use"
D16-1136,N12-1077,0,0.0113378,".0 90.5 91.3 44.3 66.3 69.1 78.2 78.7 64.3 81.4 82.6 88.8 89.5 Ours Mono Baselines Table 3: Bilingual Lexicon Induction performance from es, it, nl to en. Gouws and Søgaard (2015) + Panlex/Wikt is our reimplementation using Panlex/Wiktionary dictionary. All our models use Panlex as the dictionary. We reported the recall at 1 and 5. The best performance is bold. Model WS-de WS-en RW-en Klementiev et al. (2012) Chandar A P et al. (2014) Hermann and Blunsom (2014) Luong et al. (2015) Gouws and Søgaard (2015) 23.8 34.6 28.3 47.4 67.4 13.2 39.8 19.8 49.3 71.8 7.3 20.5 13.6 25.3 31.0 CBOW + combine Yih and Qazvinian (2012) Shazeer et al. (2016) 62.2 65.8 - 70.3 74.1 81.0 74.8 42.7 43.1 48.3 Our joint-model + combine 59.3 71.1 68.6 76.2 38.1 44.0 Table 4: Spearman’s rank correlation for monolingual similarity measurement on 3 datasets WS-de (353 pairs), WS-en (353 pairs) and RW-en (2034 pairs). We compare against 5 baseline crosslingual word embeddings. The best CLWE performance is bold. For reference, we add the monolingual CBOW with and without embeddings combination, Yih and Qazvinian (2012) and Shazeer et al. (2016) which represents the monolingual state-of-the-art results for WS-en and RW-en. monolingual co"
D16-1207,P15-2030,1,0.794831,"inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1 1 Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization. Fawzi et al. (2015) provided a theoretical framework for analyzing the 1 Implementation available at https://github.com/ lrank/Robust-Representation. robustness of classifiers t"
D16-1207,N13-1037,0,0.02851,")3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.6 This is applied to each word with probability α ∈ {0, 0.1, 0.2, 0.3}. We also experimented with adding different levels of Gaussian noise to the sentence embeddings ES , but found the results to be largely consistent with those for word dropout noise, and therefore we have omitted these results from the paper. To directly test the robustness under a more realistic setting, we additionally perform cross-domain evaluation, where we train a model on one dataset 2 For d"
D16-1207,P11-1038,1,0.806331,"set (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.6 This is applied to each word with probability α ∈ {0, 0.1, 0.2, 0.3}. We also experimented with adding different levels of Gaussian noise to the sentence embeddings ES , but found the results to be largely consistent with those for word dropout noise, and therefore we have omitted these results from the paper. To directly test the robustness under a more realistic setting, we additionally perform cross-domain evaluation, where we train a model on"
D16-1207,P14-1062,0,0.056791,"limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1 1 Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization. Fawzi et al. (2015) provided a theoretical framework for analyzing the 1 Implementation available at https://github.com/ lrank/Robust-R"
D16-1207,D14-1181,0,0.373064,"network. Also related, Martens (2010) investigated a second-order optimization method based on Hessian-free approach for training deep auto-encoders. Where our proposed approach differs is that we train models using first-order derivatives of the training loss as part of a regularization term, necessitating second-order derivatives for computing the gradient. We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets (Pang and Lee, 2008) with a convolutional neural network model (Kim, 2014). In this, we show that our method is superior to dropout (Srivastava et al., 2014) and a baseline method using MAP training. 2 Training for Robustness Our method introduces a regularization term during training to ensure model robustness. We develop our approach based on a general class of parametric mod1979 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1979–1985, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics els, with the following structure. Let x be the input, which is a sequence of (discrete) words, repr"
D16-1207,P05-1015,0,0.132058,"ngs can be found in Section 3.2. The feature vector h is fed into a final softmax layer with a linear transform to generate a probability distribution over labels ypred = softmax(w · h + b) , where w and b are parameters. Finally, the model minimizes the loss of the cross-entropy between the ground-truth and the model prediction, 1981 L = CrossEntropy(ytrue , ypred ), for which we use stochastic gradient descent. 3 Datasets and Experimental Setups We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-l"
D16-1207,D13-1170,0,0.00926157,"stribution over labels ypred = softmax(w · h + b) , where w and b are parameters. Finally, the model minimizes the loss of the cross-entropy between the ground-truth and the model prediction, 1981 L = CrossEntropy(ytrue , ypred ), for which we use stochastic gradient descent. 3 Datasets and Experimental Setups We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.6 This is applied to each word with probability"
D16-1207,P14-2105,0,0.0312831,"ty to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1 1 Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization. Fawzi et al. (2015) provided a theoretical framework for analyzing the 1 Implementation available at https://github.com/ lrank/Robust-Representation. rob"
D16-1263,W08-0336,0,0.0470765,"Missing"
D16-1263,N16-1109,1,0.622951,"pressed with a weighted finite-state transducer (WFST) framework represents the joint distribution of source acoustic features, phonemes and latent source words given the target words. Sampling of alignments is used to learn source words and their target translations, which are then used to improve transcription of the source audio they were learnt from. Importantly, the model assumes no prior lexicon or translation model. This method builds on work on phoneme translation modeling (Besacier et al., 2006; St¨uker et al., 2009; Stahlberg et al., 2012; Stahlberg et al., 2014; Adams et al., 2015; Duong et al., 2016), speech translation (Casacuberta et al., 2004; Matusov et al., 2005), computer-aided translation, (Brown et al., 1994; Vidal et al., 2006; Khadivi and Ney, 2008; Reddy and Rose, 2010; Pelemans et al., 2015), translation modeling from automatically transcribed 1 Code is available at https://github.com/oadams/latticetm. 2377 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2377–2382, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics speech (Paulik and Waibel, 2013), word segmentation and translation modeling (Chang e"
D16-1263,N09-1046,0,0.0684154,"Missing"
D16-1263,W08-0704,0,0.0500543,"Missing"
D16-1263,P11-2093,1,0.626336,"y their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the model (ASR); a monolingual version of our model that is identical except without conditioning on the target side (Mono); and the model applied using the source language sentence as the target (Oracle). We tuned on the first 1,000 utterences (about 1 hour) of speech and trained on up to 9 hours of the English (en) Mono –ja Oracle ASR 22.1 Vague 17.7 18.5 17.2 Shifted 17.4 16.9 16.6 Poisson 17.3 17.2 16.8 Japanese (ja) Mon"
D16-1263,C10-1092,0,0.0680955,"Missing"
D16-1263,takezawa-etal-2002-toward,0,0.0482154,"ce constituting one block. To sample from WFSTs, we use forwardfiltering/backward-sampling (Scott, 2002; Neubig et al., 2012), creating forward probabilities using the forward algorithm for hidden Markov models before backward-sampling edges proportionally to the product of the forward probability and the edge weight.3 3 No Metropolis-Hastings rejection step was used. 2379 We evaluate the lexicon and translation model by their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the mod"
D16-1263,1983.tc-1.13,0,0.746681,"Missing"
D16-1263,D15-1142,0,\N,Missing
D16-1263,W14-2201,1,\N,Missing
D17-1014,P17-2058,0,0.0315482,"Missing"
D17-1014,J96-1002,0,0.288209,"wing objective function subject to the simplex constraints: Q(yˆ1 , . . . , yˆ` ) − γ ` X X yˆi (w) log i=1 w∈VT Exponentiated Gradient (EG) Exponentiated gradient (Kivinen and Warmuth, 1997) is an elegant algorithm for solving optimisation problems involving simplex constraints. Re2 X = Q(yˆ1 , . . . , yˆ` ) − γ ` X Entropy(yˆi ) 1 yˆi (w) (7) i=1 In other words, the algorithm looks for the maximum entropy solution which also maximizes the Ties are broken arbitrarily. 148 log likelihood under the model. There are intriguing parallels with the maximum entropy formulation of log-linear models (Berger et al., 1996). In our setting, the entropy term acts as a prior which discourages overly-confident estimates in the absence of sufficient evidence. 3.2 reflects the natural temporal order of spoken language. However, the right-to-left model is likely to provide a complementary signal in translation, as it will be bringing different biases and making largely independent prediction errors to those of the left-to-right model. For this reason, we propose to use both models, and seek to find translations that have high probability according both models (this mirrors work on bidirectional decoding in classical s"
D17-1014,2006.amta-papers.11,0,0.0406815,"resulting SGD algorithm is summarized in Algorithm 2. 4 (8) Bilingual Ensemble. Another source of complementary information is in terms of the translation direction, that is forward translation from the source to the target language, and reverse translation in the target to source direction. Decoding must find a translation which scores well under both the forward and reverse translation models. This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see Lopez and Resnik (2006)). More specifically, we decode for the best translation in the intersection of the source-to-target and targetto-source models by minimizing the following objective function: Decoding in Extended NMT Our decoding framework allows us to effectively and flexibly add additional global factors over the output symbols during inference. This enables decoding for richer global models, for which there is no effective means of greedy decoding or beam search. We outline several such models, and their corresponding relaxed objective functions for optimisation-based decoding. Bidirectional Ensemble. Stan"
D17-1014,W16-2323,0,0.0203662,"the Mantidae toolkit4 (Cohn et al., 2016), and using the dynet deep learning library5 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words.6 For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs. Evaluation Metrics. We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or discrete), as well as measuring the end translation quality with case-insensitive BLEU (Papineni et al., 2002). The continuous cost measures 1 ˆ |x) under the model Θ; the dis− |y| ˆ log PΘ (y crete model score has the same formulation, albeit using the discrete rounded solution y (see §3). Note the co"
D17-1014,D16-1096,0,0.107284,"Missing"
D17-1014,2014.iwslt-evaluation.1,0,0.105828,"Missing"
D17-1014,N16-1102,1,0.737862,"ation problem is, in general, non-convex, finding a plausible initialisation is likely to be important for avoiding local optima. Furthermore, a proper step size is a key in the success of the EG-based and SGD-based optimisation algorithms, and there is no obvious method how to best choose its value. We may also adaptively change the step size using (scheduled) annealing or via the line search. We return to this considerations in the experimental evaluation. 5 5.1 . η is the step size NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit4 (Cohn et al., 2016), and using the dynet deep learning library5 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle un"
D17-1014,P02-1038,0,0.067888,"re-ranking evaluation. X ∂Q(.) ∂ yˆi (w0 ) ∂Q = ∂ rˆi (w) ∂ yˆi (w0 ) ∂ rˆi (w) 0 w ∈VT The resulting SGD algorithm is summarized in Algorithm 2. 4 (8) Bilingual Ensemble. Another source of complementary information is in terms of the translation direction, that is forward translation from the source to the target language, and reverse translation in the target to source direction. Decoding must find a translation which scores well under both the forward and reverse translation models. This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see Lopez and Resnik (2006)). More specifically, we decode for the best translation in the intersection of the source-to-target and targetto-source models by minimizing the following objective function: Decoding in Extended NMT Our decoding framework allows us to effectively and flexibly add additional global factors over the output symbols during inference. This enables decoding for richer global models, for which there is no effective means of greedy decoding or beam search. We outline several such models, and their corres"
D17-1014,P02-1040,0,0.11969,"Missing"
D17-1014,P16-1162,0,0.034548,"the Mantidae toolkit4 (Cohn et al., 2016), and using the dynet deep learning library5 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words.6 For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs. Evaluation Metrics. We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or discrete), as well as measuring the end translation quality with case-insensitive BLEU (Papineni et al., 2002). The continuous cost measures 1 ˆ |x) under the model Θ; the dis− |y| ˆ log PΘ (y crete model score has the same formulation, albeit using the discrete rounded solution y (see §3). Note the co"
D17-1014,C02-1050,0,0.689456,"recurrent architecture, and accordingly are non-Markov. This prevents exact dynamic programming solutions, and moreover, limits the potential to incorporate additional global features or constraints. Global factors can be highly useful in producing better and more diverse translations. Secondly, the sequential decoding of symbols in the target sequence, the inter-dependencies among the target symbols are not fully exploited. For example, when decoding the words of the target sentence in a left-to-right manner, the right context is not exploited leading potentially to inferior performance (see Watanabe and Sumita (2002a) who apply this idea in traditional statistical MT). A natural way to capture this is to intersect leftto-right and right-to-left models, however the resulting model has no natural generation order, and thus standard decoding methods are unsuitable. We introduce a novel decoding framework (§ 3) that relaxes this discrete optimisation problem into a continuous optimisation problem. This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables, where the exact inference is NP-hard (Sontag, 2010; Belanger and McCallum, 2016)."
D17-1014,D16-1137,0,0.0205172,"Missing"
D17-1014,P15-1002,0,\N,Missing
D17-1014,2002.tmi-tutorials.2,0,\N,Missing
D17-1016,C12-1064,1,0.916873,"ng one of the cities. In the machine learning literature, 1 Code available at https://github.com/afshinrahimi/geomdn 168 Assuming sufficient training samples containing the term Norwalk in the two main, a trained classification model would, given this term as input, predict a probability distribution over all regions, and assign higher probabilities to the regions containing the two major cities. The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al., to appear). The cluster/region labels can then be used as targets. Once we have a prediction about where a user is more likely to be from, there is no more information about the coordinates inside the predicted region. If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierar"
D17-1016,E14-1011,0,0.0540804,"ave no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data sparseness. 2.2 itude/longitude coordinate pair). The probability mass function is given by: Lexical Dialectology The traditional linguistic approach to lexical dialectology is to find the geographical distributions of known contrast sets such as {you, yall, yinz}: (Labov et al., 2005; Nerbonne et al., 2008; Gonc¸alves and S´anchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, to appear). This usually involves surveying a large geographically-uniform sample of people from different locations and analysing where each known alternative is used more frequently. Then, the coordinates are clustered heuristically into dialect regions, based on the lexical choices of users in each region relative to the contrast set. This processing is very costly and time-consuming, and relies critically on knowing the lexical alternatives a priori. For example, it would require a priori knowledge of the fact that people in different regions of"
D17-1016,D10-1124,0,0.850034,"Missing"
D17-1016,N10-1038,0,0.0931546,"Missing"
D17-1016,W15-1527,0,0.486489,"Missing"
D17-1016,P17-2033,1,0.579699,"Missing"
D17-1016,N15-1153,1,0.872945,"Missing"
D17-1016,D12-1137,0,0.661056,"tput layer, and only use the input to predict π, the mixture probabilities, using a SoftMax layer. We 4 Continuous Representation of Location Experiments We apply the two described MDN models on two widely-used geotagged Twitter datasets for geolocation, and compare the results with state-of-the-art classification and regression baselines. Also, we use the mixture of Gaussian representation of location to predict dialect terms from coordinates. 170 4.1 Data In our experiments, we use two existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), and (2) T WITTER -US (Roller et al., 2012). Each dataset has fixed training, devel4.2 output layer: term probabilities tanh hidden layer Gaussian layer: K Gaussian components input: location coordinates (a) Predict text given location output layer: mixing coefficients π1 . . . πk tanh hidden layer input: Text BoW (b) Predict location given text Figure 1: (a) The lexical dialectology model using a Gaussian representation layer. (b) MDN-SHARED geolocation model where the mixture weights π are predicted for each sample, and µ and Σ are parameters of the output layer, shared between all samples. opment and test partitions, and a user is r"
D17-1016,P14-1109,0,0.0382902,"Missing"
D17-1016,P11-1096,0,0.434275,"/geomdn 168 Assuming sufficient training samples containing the term Norwalk in the two main, a trained classification model would, given this term as input, predict a probability distribution over all regions, and assign higher probabilities to the regions containing the two major cities. The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al., to appear). The cluster/region labels can then be used as targets. Once we have a prediction about where a user is more likely to be from, there is no more information about the coordinates inside the predicted region. If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data sparseness."
D17-1016,D14-1039,0,0.722211,"(Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al., to appear). The cluster/region labels can then be used as targets. Once we have a prediction about where a user is more likely to be from, there is no more information about the coordinates inside the predicted region. If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data sparseness. 2.2 itude/longitude coordinate pair). The probability mass function is given by: Lexical Dialectology The traditional linguistic approach to lexical dialectology is to find the geographical distributions of known contrast sets such as {you, yall, yinz}: (Labov et al., 2005; Nerbonne et al., 2008; Gonc¸alves and S´anchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, to appear). This usually involves surveying a large geographically-uniform sample of people from different locations and analysing where each kn"
D17-1063,P17-2093,1,0.836312,"ch the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled. Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017). Qian et al. used uncertainty sampling to jointly perform on English and Chinese. Stratos and Collins and Zhang et al. deployed uncertainty-based AL algorithms for languages with the minimal supervision. 3 Methodology We now show how active learning can be formalised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep reinforcement learning. Later we introduce a method for transferring the policy between languages. 3.1 Active learning as a decision process Active learning is a simple technique for labelling data,"
D17-1063,N13-1014,0,0.0169378,"A Deep Reinforcement Learning Approach Meng Fang and Yuan Li and Trevor Cohn School of Computing and Information Systems The University of Melbourne meng.fang@unimelb.edu.au, yuanl4@student.unimelb.edu.au, t.cohn@unimelb.edu.au Abstract heuristics, such as using uncertainty or informativeness. There has been comparatively little work done about how to learn the active learning strategy itself. It is no doubt that active learning is extremely important for other languages, particularly lowresource languages, where annotation is typically difficult to obtain, and annotation budgets more modest (Garrette and Baldridge, 2013). Such settings are a natural application for active learning, however there is little work to this end. A potential reason is that most active learning algorithms require a substantial ‘seed set’ of data for learning a basic classifier, which can then be used for active data selection. However, given the dearth of data in the low-resource setting, this assumption can make standard approaches infeasible. In this paper,1 we propose PAL, short for Policy based Active Learning, a novel approach for learning a dynamic active learning strategy from data. This allows for the strategy to be applied i"
D17-1063,D14-1181,0,0.00277822,"use both the predictive marginal distributions of the model on the instance, and a representation of the model’s confidence. We now elaborate on each component. Vinken will … join Representation of marginals the board Marginals Convolutional layer Figure 2: The architecture for representing predictive marginal distributions, pφ (y|xi ), as a fixed dimensional vector, to form part of the MDP state. Content representation A key input to the agent is the content of the sentence, xi , which we encode using a convolutional neural network to arrive at a fixed sized vector representation, following Kim (2014). This involves embedding each of the n words in the sentence to produce a matrix Xi = {xi,1 , xi,2 , · · · , xi,n }, after which a series of wide convolutional filters is applied, using multiple filters with different gram sizes. Each filter uses a linear transformation with a rectified linear unit activation function. Finally the filter outputs are merged using a max-pooling operation to yield a hidden state hc , which is used to represent the sentence. Confidence of sequential prediction The last component is a score C which indicates the confidence of the model prediction. This is defined"
D17-1063,P14-1055,0,0.0174099,"ree categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled. Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017). Qian et al. used uncertainty sampling to jointly perform on English and Chinese. Stratos and Collins and Zhang et al. deployed uncertainty-based AL algorithms for languages with the minimal supervision. 3 Methodology We now show how active learning can be formalised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep reinforcement learning. Later we introduce a method for transferring the polic"
D17-1063,N19-1358,0,0.14658,"Missing"
D17-1063,D08-1112,0,0.945686,"asy to learn a policy on a high resource language, where there is plentiful data, such as English. We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language. Our work is different for prior work in active learning for NLP. Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language itself. Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016). However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent. The remainder of this paper is organised as follows. In Section 2, we briefly review some related work. In Section 3, we present active learning algorithms, which cross multiple languages. The experimental results are presented in Section 4. We conclude our work in Section 5. 2 Related work As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate fo"
D17-1063,P04-1075,0,0.0841174,"and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled. Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017). Qian et al. used uncertainty sampling to jointly perform on English and Chinese. Stratos and Collins and Zhang et al. deployed uncertainty-based AL algorithms for languages with the minimal supervision. 3 Methodology We now show how active learning can be formalised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep reinforcement learning. Later we introduce a method for transferring the policy between languages. 3.1 Active learning as a"
D17-1063,W15-1511,0,0.122086,"high resource language, where there is plentiful data, such as English. We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language. Our work is different for prior work in active learning for NLP. Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language itself. Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016). However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent. The remainder of this paper is organised as follows. In Section 2, we briefly review some related work. In Section 3, we present active learning algorithms, which cross multiple languages. The experimental results are presented in Section 4. We conclude our work in Section 5. 2 Related work As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate for training the best classif"
D17-1063,D16-1261,0,0.00546753,"ntion with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al., 2015). Other important works include massively parallel frameworks (Nair et al., 2015), dueling architecture (Wang et al., 2016) and expert move prediction in the game of Go (Maddison et al., 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al., 2016). DRL has been also studied in NLP tasks. For example, recently, DRL has been studied for information extraction problem (Narasimhan et al., 2016). They designed a framework that can decide to acquire external evidence and the framework is under the reinforcement learning method. However, there has been fairly little work on using DRL to learn active learning strategies for language processing tasks, especially in cross-lingual settings. Recent deep learning work has also looked at transfer learning (Bengio, 2012). More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network ar"
D17-1063,D07-1051,0,0.0208779,"Missing"
D17-1063,N16-1029,0,0.00582024,"ere there is plentiful data, such as English. We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language. Our work is different for prior work in active learning for NLP. Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language itself. Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016). However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent. The remainder of this paper is organised as follows. In Section 2, we briefly review some related work. In Section 3, we present active learning algorithms, which cross multiple languages. The experimental results are presented in Section 4. We conclude our work in Section 5. 2 Related work As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate for training the best classifier. Existing active"
D17-1306,W10-0701,0,0.06039,"Missing"
D17-1306,N15-1124,1,0.894493,"r all aspects. Though we find a statistically significant negative autocorrelation for scores of the full dataset, this disappears when we filter out bad workers (Table 2). Given the difficulty of this very subjective task, it is likely that many of workers considered ‘bad’ might have simply found this task too difficult or arbitrary, and thus become more prone to sequence effects. 3.3 Machine Translation Adequacy When evaluating machine translation (“MT”), we tend to focus on adequacy: the extent to which the meaning of the reference translation is captured in the MT output. In the method of Graham et al. (2015) — the current best-practise, as adopted by WMT (Bojar et al., 2016) — annotators are asked to judge the adequacy of translations using a 100point sliding scale which is initialised at the mid point. There are 3 marks on the scale dividing it into 4 quarters to aid workers with internal calibration. They are given no other instructions or All β1 β2 Good Bad ∗ −0.03 −0.01 −0.04∗ 0.45∗∗∗ 0.66∗∗∗ 0.23∗∗∗ Table 2: Autocorrelation coefficient β1 for the A F FECTIVE dataset. guidelines. In this paper, we base our analysis on the adequacy dataset of Graham et al. (2015), on SpanishEnglish newswire da"
D17-1306,N06-2015,0,0.0446363,"Missing"
D17-1306,W06-3114,0,0.0379656,"Missing"
D17-1306,Q14-1025,0,0.0373302,"Missing"
D17-1306,N06-2031,0,0.0189771,"Missing"
D17-1306,D08-1027,0,0.204451,"Missing"
D17-1306,E12-2021,0,0.107327,"Missing"
D18-1310,P17-1177,0,0.0343573,"s through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computati"
D18-1310,C02-1025,0,0.0325374,"both Lample et al. (2016) and Ma and Hovy (2016) propose end-to-end models for sequence labelling task and achieve state-of-the-art results. ∗ † https://github.com/minghao-wu/CRF-AE Work carried out at The University of Melbourne Orthogonal to the advances in deep learning is the effort spent on feature engineering. A representative example is the task of named entity recognition (NER), one that requires both lexical and syntactic knowledge, where, until recently, most models heavily rely on statistical sequential labelling models taking in manually engineered features (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005). Typical features include POS and chunk tags, prefixes and suffixes, and external gazetteers, all of which represent years of accumulated knowledge in the field of computational linguistics. The work of Collobert et al. (2011) started the trend of feature engineering-free modelling by learning internal representations of compositional components of text (e.g., word embeddings). Subsequent work has shown impressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Penn"
D18-1310,Q16-1026,0,0.114368,"n auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance. Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss. An illustration of the model architecture is presented in Figure 1. Char-CNN. Previous studies (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016) have demonstrated that CNNs are highly capable of capturing character-level features. Here, our character-level CNN is similar to that used in Ma and Hovy (2016) but differs in that we use a ReLU activation (Nair and Hinton, 2010).1 Bi-LSTM. We use a Bi-LSTM to learn contextual information of a sequence of words. As inputs to the Bi-LSTM, we first concatenate the pre-trained embedding of each word wi with its character-level representation cwi (the output of the char-CNN) and a vector of manually crafted features fi (described in Section 2.2): → − − −−−−→ → h i = LSTM( h i"
D18-1310,W14-4012,0,0.151854,"Missing"
D18-1310,D14-1179,0,0.0421717,"Missing"
D18-1310,W03-0425,0,0.163953,"Missing"
D18-1310,E17-2068,0,0.0619774,"Missing"
D18-1310,N16-1030,0,0.44,"R shared task dataset (Tjong Kim Sang and De Meulder, 2003). 3.1 Experimental Setup Dataset. We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997. The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC. We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). 3 https://spacy.io/ 2852 Gazetteer data is included in the code release. Model configuration. Following the work of Ma and Hovy (2016), we initialise word embeddings with GloVe (Pennington et al., 2014) (300dimensional, trained on a 6B-token corpus). Character embeddings are 30-dimensional and randomly initialised withqa uniform distribution in q 3 3 the range [− dim , + dim ]. Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9. Exponential learning rate decay is applied every 5 epochs with a fact"
D18-1310,P17-1064,0,0.0720343,"mpressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Associ"
D18-1310,P18-2045,1,0.825962,"The experimental results are presented in Table 2. Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss. Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features. Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model. Ablation Study To gain a better understanding of the impacts of each feature, we perform an abModel F1 Chieu and Ng (2002) Florian et al. (2003) Ando and Zhang (2005) Collobert et al. (2011) Huang et al. (2015) Passos et al. (2014) Lample et al. (2016) Luo et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Peters et al. (2018) Peters et al. (2018)+ELMo Neural-CRF‡ Neural-CRF+A"
D18-1310,N18-2045,1,0.81702,"The experimental results are presented in Table 2. Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss. Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features. Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model. Ablation Study To gain a better understanding of the impacts of each feature, we perform an abModel F1 Chieu and Ng (2002) Florian et al. (2003) Ando and Zhang (2005) Collobert et al. (2011) Huang et al. (2015) Passos et al. (2014) Lample et al. (2016) Luo et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Peters et al. (2018) Peters et al. (2018)+ELMo Neural-CRF‡ Neural-CRF+A"
D18-1310,D15-1104,0,0.0447723,"Missing"
D18-1310,P16-1101,0,0.503843,"ully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Output Layer NER Feature Encoder Feature Concatenation Auto-Encoder Bi-directional LSTM Character Representations Input Sentence Word Embeddings Hand-crafted Features EU rejects German call to ... Figure 1: Main architecture of our neural network. Character representations are extracted by a characterlevel CNN. The dash line indicates we use an autoencoder loss to reconstruct h"
D18-1310,D17-1179,0,0.0460435,"Missing"
D18-1310,W14-1609,0,0.0502575,"Missing"
D18-1310,D14-1162,0,0.0795918,"2002; Ando and Zhang, 2005). Typical features include POS and chunk tags, prefixes and suffixes, and external gazetteers, all of which represent years of accumulated knowledge in the field of computational linguistics. The work of Collobert et al. (2011) started the trend of feature engineering-free modelling by learning internal representations of compositional components of text (e.g., word embeddings). Subsequent work has shown impressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we in"
D18-1310,N18-1202,0,0.0211338,"e. 3.2 Results The experimental results are presented in Table 2. Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss. Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features. Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model. Ablation Study To gain a better understanding of the impacts of each feature, we perform an abModel F1 Chieu and Ng (2002) Florian et al. (2003) Ando and Zhang (2005) Collobert et al. (2011) Huang et al. (2015) Passos et al. (2014) Lample et al. (2016) Luo et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Peters et al. (2018) Peters et al. (2018)+ELMo Neural-CRF‡ Neural-CRF+A"
D18-1310,W09-1119,0,0.114352,"ntity recognition over the CoNLL 2003 English NER shared task dataset (Tjong Kim Sang and De Meulder, 2003). 3.1 Experimental Setup Dataset. We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997. The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC. We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). 3 https://spacy.io/ 2852 Gazetteer data is included in the code release. Model configuration. Following the work of Ma and Hovy (2016), we initialise word embeddings with GloVe (Pennington et al., 2014) (300dimensional, trained on a 6B-token corpus). Character embeddings are 30-dimensional and randomly initialised withqa uniform distribution in q 3 3 the range [− dim , + dim ]. Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9. Exponential learning rat"
D18-1310,W03-0419,0,0.624297,"Missing"
D18-1310,P17-1065,0,0.0321078,"syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics"
D18-1310,yang-etal-2017-neural-reranking,0,0.0328918,"Missing"
D19-1158,N19-1423,0,0.0219413,"an value of a set of points to represent a color. A drawback of this approach is that it does not account for our uncertainty about the appropriate RGB encoding for a given color word. 3.2 Experiment Setup Model configuration: The model presented by Winn and Muresan (2018) is initialized with Google’s pretrained 300-d word2vec embeddings (Mikolov et al., 2013b,a) which are not updated during training. To perform comparable experiments, all models in paper are designed with the same pre-trained embedding model. Other pretrained word embeddings, such as GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019), were also tested but there was no significant 4 https://bitbucket.org/o_winn/ comparative_colors 1490 Test Condition Seen Pairings Unseen Pairings Unseen Ref. Color Unseen Modifiers Fully Unseen Overall RGB 0.954±0.001 0.799±0.044 0.781±0.015 0.633±0.042 0.370±0.029 0.858±0.006 Test Condition Seen Pairings Unseen Pairings Unseen Ref. Color Unseen Modifiers Fully Unseen Overall RGB 3.121±0.027 6.454±0.233 7.456±0.184 13.288±1.082 13.859±0.874 5.412±0.169 Cosine Similarity ± SD (↑) WM18 HSV Ensemble 0.953±0.000 0.934±0.089 0.954±0.0 0.771±0.032 0.843±0.144 0.797±0.0 0.767±0.010 0.945±0.019 0.8"
D19-1158,Q17-1023,0,0.0205459,"HSV color space. We show that certain adjectives are better modeled in that space. To account for all modifiers, we train a hard ensemble model that selects a color space depending on the modifiercolor pair. Experimental results show significant and consistent improvements compared to the state-of-the-art baseline model.1 1 Figure 1: Examples of the grounded modifier modelling task, shown in RGB space. Given the reference and modifier, the system must predict the target color. Introduction Grounded color descriptions are employed to describe colors which are not covered by basic color terms (Monroe et al., 2017). For instance, “greenish blue” cannot be expressed by only “blue” or “green”. Grounded learning of modifiers, as a result, is essential for grounded language understanding problems such as image captioning (Karpathy and Fei-Fei, 2015), visual question answering (Goyal et al., 2017) and object recognition (van de Sande et al., 2010). In this paper, we present models that are able to predict the RGB code of a target color given a reference color and a modifier. For example, as shown in Figure 1, given a reference  > color code ~r = 101 55 0 and a modifier m = “greenish”, our models are traine"
D19-1158,D14-1162,0,0.0893178,"nn and Muresan (2018): using the mean value of a set of points to represent a color. A drawback of this approach is that it does not account for our uncertainty about the appropriate RGB encoding for a given color word. 3.2 Experiment Setup Model configuration: The model presented by Winn and Muresan (2018) is initialized with Google’s pretrained 300-d word2vec embeddings (Mikolov et al., 2013b,a) which are not updated during training. To perform comparable experiments, all models in paper are designed with the same pre-trained embedding model. Other pretrained word embeddings, such as GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019), were also tested but there was no significant 4 https://bitbucket.org/o_winn/ comparative_colors 1490 Test Condition Seen Pairings Unseen Pairings Unseen Ref. Color Unseen Modifiers Fully Unseen Overall RGB 0.954±0.001 0.799±0.044 0.781±0.015 0.633±0.042 0.370±0.029 0.858±0.006 Test Condition Seen Pairings Unseen Pairings Unseen Ref. Color Unseen Modifiers Fully Unseen Overall RGB 3.121±0.027 6.454±0.233 7.456±0.184 13.288±1.082 13.859±0.874 5.412±0.169 Cosine Similarity ± SD (↑) WM18 HSV Ensemble 0.953±0.000 0.934±0.089 0.954±0.0 0.771±0.032 0.843±0.144 0.797±"
D19-1158,P18-2125,0,0.339337,"difiers, as a result, is essential for grounded language understanding problems such as image captioning (Karpathy and Fei-Fei, 2015), visual question answering (Goyal et al., 2017) and object recognition (van de Sande et al., 2010). In this paper, we present models that are able to predict the RGB code of a target color given a reference color and a modifier. For example, as shown in Figure 1, given a reference  > color code ~r = 101 55 0 and a modifier m = “greenish”, our models are trained to pre > dict the target color code ~t = 105 97 18 . The state-of-the-art approach for this task (Winn and Muresan, 2018) represents both colors and modifiers as vectors in RGB space, and learns a 1 Code available HanXudong/GLoM at https://github.com/ vector representation of modifiers m ~ as part of a simple additive model, ~r + m ~ ≈ ~r, in RGB color space. For instance, given the reference  > color ~r = 229 0 0 , the target color ~t =  > 132 0 0 , the modifier m = “darker” is  > learned as a vector m ~ = −97 0 0 . This model works well when the modifier is well represented as a single vector independent of the reference color,2 but fails to model modifiers with more complex transformations, for example,"
D19-1158,Q15-1008,0,0.0249734,"ce (measured using Delta-E distance, see §3.2). The probability of the RGB model being selected is: p = σ(f (m, ~r)), where σ is the logistic sigmoid function and f (m, ~r) is a function of modifier m and reference color ~r. 3 3.1 Experiments Dataset The dataset4 used to train and evaluate our model includes 415 triples (reference color label, r, modifier, m, and target color label, t) in RGB space presented by Winn and Muresan (2018). Munroe (2010) collected the original dataset consisting of color description pairs collected in an open online survey; the dataset was subsequently filtered by McMahan and Stone (2015). Winn and Muresan processed color labels and converted pairs to triples with 79 unique reference color labels and 81 unique modifiers. We train models in both RGB and HSV color space, but samples in WM18 are only presented in RGB space. Because modifiers encode the general relationship between r and t we use the same approach presented by Winn and Muresan (2018): using the mean value of a set of points to represent a color. A drawback of this approach is that it does not account for our uncertainty about the appropriate RGB encoding for a given color word. 3.2 Experiment Setup Model configura"
D19-1158,D10-1115,0,\N,Missing
D19-1158,N16-1018,0,\N,Missing
D19-1182,J08-4004,0,0.107717,"Missing"
D19-1182,D18-1217,0,0.0449086,"ches based on a uni-modal probability distribution (e.g., Poisson) as output (da Costa et al., 2008; Beckham and Pal, 2017) can be seen as related to the former approach (Imani and White, 2018) where the discrete probability mass function replaces the histogram density. We propose to use a uni-modal distributional loss-based ordinal regression for pledge specificity prediction. Secondly, as it is difficult to obtain large amounts of labeled data, existing approaches have used semi-supervised learning (Li and Nenkova, 2015; Subramanian et al., 2019). Here we use a cross-view training approach (Clark et al., 2018; Subramanian et al., 2019), where we enforce consensus between the intermediate class distributions or the final real-valued output. 3 Pledge Specificity Dataset We annotated 22 election manifestos from the Australian Labor and Liberal parties, covering eleven Australian federal election cycles from 1730 Category Definition Example # Not a pledge Provide facts; greetings; approval or criticism of policies 1 Rhetorical pledge Based on moral values and applies to all irrespective of the party Specify intangible goals, and also not the ways to achieve them Commit to the maintenance of currently"
D19-1182,S17-2088,0,0.0601742,"Missing"
D19-1182,L16-1620,0,0.055798,"Missing"
D19-1182,D17-1231,0,0.0224685,"inal regression output of the teacher model (f (x)) to fit an auxiliary model, thereby enforcing consensus using a squared loss, MSE(Eqθ [Y], Eqω [Y]) where Y is a fixed class vector; denoted as “LUMSE ”. K LD : an intermediate distribution over targets qθ (Y|s) is used to fit an auxiliary model, qω (Y|s), by minimising the Kullback-Leibler (KL) divergence, KL(qθ (Y|s), qω (Y|s)); denoted as “LUKLD ”.2 E MD : qθ (Y|s) is again used to fit the auxiliary model, qω (Y|s), by minimising the earth 2 We incorporate context in the form of information from adjacent sentences following the approach of Liu et al. (2017): for each training sentence, we Semi-supervised Learning This is the closest setting to Subramanian et al. (2019), which minimizes KL divergence between output distributions in a classification setting. But the overall objective is different in our case, in that we have an expectation layer over q to obtain the target regression output. 1733 MMAE ρ Majority Length Speciteller NNREG biGRUREG biGRUCLASS biGRUREGl1 3 2.05 1.83 2.17 1.99 0.21 0.18 0.33 0.47 0.40 0.46 Ordinal Ordinal Ordinal Ordinal biGRUC ATEGORICAL biGRUB INOMIAL biGRUP OISSON biGRUG AUSS 1.80 1.78 1.90 1.72 0.48 0.48 0.41 0.49"
D19-1182,I11-1068,0,0.0354047,"on the association between text specificity and communication style. In terms of automated specificity analysis, Cook (2016) found specificity in the context of congressional hearings to vary between speakers belonging to the same vs. different ideologies. Namely, it was shown that specificity increases as the ideological distance between the committee chair and the witness decreases. Subramanian et al. (2019) addressed two levels of pledge specificity, as part of speech act classification task. Specificity has 1 https://github.com/shivashankarrs/ Pledge-Specificity also been studied in news (Louis and Nenkova, 2011) and classroom discussion domains (Luo and Litman, 2016; Lugini and Litman, 2017). These studies have dealt with a restrictive coarse-level analysis (2–3 categories), whereas a fine-grained scale better captures and allows for comparison of election manifestos (Pomper and Lederman, 1980). Gao et al. (2019) was the first attempt at fine-grained text specificity prediction, in the context of social media posts. Here, we target the novel task of fine-grained pledge specificity prediction, which can be used in a range of downstream applications, including capturing party priorities (salience) and"
D19-1182,W17-5006,0,0.0214945,"automated specificity analysis, Cook (2016) found specificity in the context of congressional hearings to vary between speakers belonging to the same vs. different ideologies. Namely, it was shown that specificity increases as the ideological distance between the committee chair and the witness decreases. Subramanian et al. (2019) addressed two levels of pledge specificity, as part of speech act classification task. Specificity has 1 https://github.com/shivashankarrs/ Pledge-Specificity also been studied in news (Louis and Nenkova, 2011) and classroom discussion domains (Luo and Litman, 2016; Lugini and Litman, 2017). These studies have dealt with a restrictive coarse-level analysis (2–3 categories), whereas a fine-grained scale better captures and allows for comparison of election manifestos (Pomper and Lederman, 1980). Gao et al. (2019) was the first attempt at fine-grained text specificity prediction, in the context of social media posts. Here, we target the novel task of fine-grained pledge specificity prediction, which can be used in a range of downstream applications, including capturing party priorities (salience) and ideological position across election cycles. All the text specificity analysis wo"
D19-1182,N18-1202,0,0.130423,"Missing"
D19-1182,N18-1178,1,0.863075,"r sparse supervision scenarios using the teacher– student framework; and (3) we evaluate the utility of pledge specificity towards ideology prediction, and provide further qualitative analysis by correlating model predictions with party-specific issue salience across major policy areas. 2 Related Work Political manifesto text analysis is a relatively novel application, at the intersection of Political Science and NLP. Research has focused primarily on fine-grained policy topic classification and overall ideology prediction tasks (Volkens et al., 2017; Verberne et al., 2014; Zirn et al., 2016; Subramanian et al., 2018). Most work dealing with pledge specificity analysis in manifestos has been based on manual analysis, as outlined in Section 1. Specificity is a pragmatic property of text which has been studied across various fields of research. In cognitive linguistics, Dixon (1987) showed that specificity of information in text impacts reading comprehension speed. In Political Science, it has been used to analyze salience, party position and post-election policy framing (see Section 1). There has also been research on the association between text specificity and communication style. In terms of automated sp"
D19-1182,S19-1030,1,0.753661,"l scientists have long studied how specific pledges translate into government programs and actual policy (Royed, 1996; Thomson, 2001; Naurin, 2011; Schermann and Ennser-Jedenastik, 2014). Other work relates specific pledges to the issue clarity of a political party Issue clarity has also been shown to be influenced by a party’s ideological position and its role in government (Praprotnik, 2017). Although pledge specificity prediction is an important task for the analysis of party position, priorities, and post-election policy framing, to date, almost all research has relied on manual analysis. Subramanian et al. (2019) is a recent exception to this, in performing speech act classification over political campaign text, where the class schema includes the distinction between specific and vague pledges (binary specificity class). In this paper, we perform fine-grained pledge specificity prediction, which is more expressive than binary levels (Li et al., 2016; Gao et al., 2019). We use a class schema proposed by Pomper and Lederman (1980) as detailed in Table 1, which captures seven levels of specificity, forming a nonlinear increasing order of commitment and specificity (Pomper and Lederman, 1980). Given the n"
D19-5304,E17-3017,0,0.0348488,"pineni et al., 2001) results using the median performance according to the dev set and an ensemble of the 5 models. For the word-based models, we remove any tokens with frequency lower than 2 (as in Sperber et al. (2017)), while for subword models we do not perform any threshold pruning. We report all results on the Fisher “dev2” set.4 Models and Evaluation All our models are trained on the Fisher training set. For the 1-best baseline we use a standard seq2seq architecture and for the GGNN models, we use the same setup as Beck et al. (2018). Our implementation is based on the Sockeye toolkit (Hieber et al., 2017) and we use default values for most hyperparameters, except for batch size (16) and GGNN layers (8).3 For regularisation, we apply 0.5 dropout on the input embeddings and perform early stopping on the corresponding Fisher dev set. 3.1 Out-of-the-box ASR scenario In this scenario we assume only lattices and 1-best outputs are available, simulating a setting where we do not have access to the transcriptions. Table 1 shows that results are consistent with previous work: lattices provide significant improvements over simply using the 1-best output. More importantly though, the results also highlig"
D19-5304,U17-1006,1,0.890487,"Missing"
D19-5304,N18-1008,0,0.0959931,"Missing"
D19-5304,D15-1218,0,0.021353,"s to the original transcriptions used to train the ASR system, which can limit applicability in real scenarios. In this work we propose an approach for speech translation through lattice transformations and neural models based on graph networks. Experimental results show that our approach reaches competitive performance without relying on transcriptions, while also being orders of magnitude faster than previous work. 1 Introduction Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models (Sperber et al., 2017). Most previous work rely on pipeline approaches, using the output of a speech recognition system (ASR) as an input to a machine translation (MT) one. These inputs can be simply the 1-best sentence returned by the ASR system or a more structured representation such as a lattice. Some recent work on end-to-end systems bypass the need for intermediate representations, with impressive results (Weiss et al., 2017). However, such a scenario has drawbacks. From a practical perspective, it requires access to the original speech utterances a"
D19-5304,D17-1209,0,0.0431198,"Missing"
D19-5304,P18-1026,1,0.899753,"tical perspective, intermediate representations such as lattices can be enriched through external, textual resources such as monolingual corpora or dictionaries. Sperber et al. (2017) proposes a lattice-tosequence model which, in theory, can address both problems above. However, their model suffers 2 Approach Many graph network options exist in the literature (Bruna et al., 2014; Duvenaud et al., 2015; Kipf and Welling, 2017; Gilmer et al., 2017): in this work we opt for a Gated Graph Neural Network (Li et al., 2016, GGNN), which was recently incorporated in an encoder-decoder architecture by Beck et al. (2018). Assume a directed graph G = {V, E, LV , LE }, where V is a set of nodes (v, `v ), E is a set of edges (vi , vj , `e ) and LV and LE are respectively vocabularies for nodes and edges, from which node and edge labels (`v and `e ) are defined. Given an input graph with node embeddings X, a GGNN is defined as h0v = xv ! rtv =σ crv X W`re h(t−1) u + br`e u∈Nv ! ztv = σ czv X W`ze h(t−1) + bz`e u u∈Nv 26 Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 26–31 c Hong Kong, November 4, 2019. 2019 Association for Computational Linguis"
D19-5304,D15-1166,0,0.115404,"Missing"
D19-5304,D17-1159,0,0.031261,"valence classes, running in O(n log n) time, where n is the number of nodes. 1 con con qui´ en &lt;/s> hab@@ lo la &lt;s> con qui´ en &lt;/s> hab@@ lo Figure 1: Proposed lattice transformations. From top to bottom: 1) Original lattice with scores removed; 2) Line graph transformation; 3) Subword segmentation; 4) Lattice minimisation; 5) Addition of reverse and self-loop edges. The final step adds reverse and self-loop edges to the lattice, where these new edges have specific parameters in the encoder. This eases propagation of information and is standard practice when using graph networks as encoders (Marcheggiani and Titov, 2017; Bastings et al., 2017; Beck et al., 2018). We show an example of all the transformation steps on Figure 1. In Figure 2 we show the architecture of our system, using the final lattice from Figure 1 as an example. Nodes are represented as embeddings that are updated according to the lattice structure, resulting in a set of hidden states as the output. Other components follow a standard seq2seq model, using a bilinear attention module (Luong et al., 2015) and a 2-layer LSTM (Hochreiter and This procedure is also done in Sperber et al. (2017). 27 &lt;s> with con who quién ... hab@@ Bilinear Attenti"
D19-5304,2001.mtsummit-papers.68,0,0.0663366,"ded with the datasets. Following previous work (Post et al., 2013; Sperber et al., 2017), we lowercase and remove punctuation from the English translations. To build the BPE models, we extract the vocabulary from the Spanish training lattices, using 8K split operations. 1-best L L+S L+S+M 32.4 36.1 34.4 38.3 34.5 38.7 34.3 39.1 Table 1: Out-of-the-box scenario results, in BLEU scores. “L” corresponds to word lattice inputs, “L+S” and “L+S+M” correspond to lattices after subword segmentation and after minimisation, respectively. Each model is trained using 5 different seeds and we report BLEU (Papineni et al., 2001) results using the median performance according to the dev set and an ensemble of the 5 models. For the word-based models, we remove any tokens with frequency lower than 2 (as in Sperber et al. (2017)), while for subword models we do not perform any threshold pruning. We report all results on the Fisher “dev2” set.4 Models and Evaluation All our models are trained on the Fisher training set. For the 1-best baseline we use a standard seq2seq architecture and for the GGNN models, we use the same setup as Beck et al. (2018). Our implementation is based on the Sockeye toolkit (Hieber et al., 2017)"
D19-5304,2013.iwslt-papers.14,0,0.638115,"TM (Hochreiter and This procedure is also done in Sperber et al. (2017). 27 &lt;s> with con who quién ... hab@@ Bilinear Attention am la I lo speaking &lt;/s> Embeddings GGNN Encoder Attention RNN Decoder Figure 2: Model architecture, using the final Spanish lattice from Figure 1 and its corresponding English translation as an example. Schmidhuber, 1997) as the decoder. 3 Median Ensemble Experiments Data We perform experiments using the Fisher/Callhome Speech Translation corpus, composed of Spanish telephone conversations with their corresponding English translations. We use the original release by Post et al. (2013), containing both 1-best and pruned lattice outputs from an ASR system for each Spanish utterance.2 The Fisher corpus contain 150K instances and we use the original splits provided with the datasets. Following previous work (Post et al., 2013; Sperber et al., 2017), we lowercase and remove punctuation from the English translations. To build the BPE models, we extract the vocabulary from the Spanish training lattices, using 8K split operations. 1-best L L+S L+S+M 32.4 36.1 34.4 38.3 34.5 38.7 34.3 39.1 Table 1: Out-of-the-box scenario results, in BLEU scores. “L” corresponds to word lattice inp"
D19-5304,N03-1009,0,0.0218424,"Missing"
D19-5304,P16-1162,0,0.0488546,"l Beck† Trevor Cohn† Gholamreza Haffari‡ † School of Computing and Information Systems University of Melbourne, Australia {d.beck,t.cohn}@unimelb.edu.au ‡ Faculty of Information Technology Monash University, Australia gholamreza.haffari@monash.edu Abstract from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining. In this work, we address these two problems by applying lattice transformations and graph networks as encoders. More specifically, we enrich the lattices by applying subword segmentation using byte-pair encoding (Sennrich et al., 2016, BPE) and perform a minimisation step to remove redundant nodes arising from this procedure. Together with the standard batching strategies provided by graph networks, we are able to decrease training time by two orders of magnitude, enabling us to match their translation performance under the same training speed constraints without relying on gold transcriptions. Speech translation systems usually follow a pipeline approach, using word lattices as an intermediate representation. However, previous work assume access to the original transcriptions used to train the ASR system, which can limit"
D19-5304,D17-1145,0,0.426551,"em, which can limit applicability in real scenarios. In this work we propose an approach for speech translation through lattice transformations and neural models based on graph networks. Experimental results show that our approach reaches competitive performance without relying on transcriptions, while also being orders of magnitude faster than previous work. 1 Introduction Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models (Sperber et al., 2017). Most previous work rely on pipeline approaches, using the output of a speech recognition system (ASR) as an input to a machine translation (MT) one. These inputs can be simply the 1-best sentence returned by the ASR system or a more structured representation such as a lattice. Some recent work on end-to-end systems bypass the need for intermediate representations, with impressive results (Weiss et al., 2017). However, such a scenario has drawbacks. From a practical perspective, it requires access to the original speech utterances and transcriptions, which can be unrealistic if a user needs t"
D19-6405,W04-1013,0,0.0466921,"categories and 50 relationship categories. Notice that the number of object categories and relationships are much smaller than the actual number of objects and relationships in the Visual Genome dataset. All the predicted objects are associated with a set of bound box coordinates. The region-level image features3 are obtained from Faster-RCNN (Ren et al., 2017), which is also trained on Visual Genome, using 1,600 object classes and 400 attributes classes. Evaluation We employ standard automatic evaluation metrics including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016), and we use the coco-caption tool5 to obtain the scores. 3.1 Quantitative Results and Analysis Table 1 shows the performances of our models against baseline models whose architecture is based on Bottom-up Top-down Attention model (Anderson et al., 2018). Overall, our proposed model incorporating scene graph features achieves better results across all evaluation metrics, compared to image features only or graph features only. The results show that our model can learn to exploit the relational information in scene graphs and effec"
D19-6405,D17-1159,0,0.0341041,"tate of the Attention LSTM to attend over region-level image features. Here, we only show the second attention step over region-level image features as they are identical procedures except for the input: Section 3.1. Given those inputs, our model consists a scene graph encoder, an LSTM-based attention module and another LSTM as the decoder. 2.1 Scene Graph Encoder The scene graph is represented as a set of node embeddings which are then updated into contextual hidden vectors using a Graph Convolutional Network (Kipf and Welling, 2017, GCN). In particular, we employ the GCN version proposed by Marcheggiani and Titov (2017), who incorporate directions and edge labels. We treat each relation and object in the scene graph as nodes, which are then connected with five different types of edges.1 Since we assume scene graphs are obtained by a parser, they may contain noise in the form of faulty or nugatory connections. To mitigate the influence of parsing errors, we allow edge-wise gating so the network learns to prune those connections. We refer to Marcheggiani and Titov (2017) for details of their GCN architecture. 2.2 bi,t = wbT ReLU(Wf b vi + Whb [h1t , ˆft ]) βt = softmax(bt ); ˆt = v Nv X βi,t vi i=1 where wbT ∈"
D19-6405,P02-1040,0,0.113709,"leaner version of the Visual Genome2 that consists of 150 object categories and 50 relationship categories. Notice that the number of object categories and relationships are much smaller than the actual number of objects and relationships in the Visual Genome dataset. All the predicted objects are associated with a set of bound box coordinates. The region-level image features3 are obtained from Faster-RCNN (Ren et al., 2017), which is also trained on Visual Genome, using 1,600 object classes and 400 attributes classes. Evaluation We employ standard automatic evaluation metrics including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016), and we use the coco-caption tool5 to obtain the scores. 3.1 Quantitative Results and Analysis Table 1 shows the performances of our models against baseline models whose architecture is based on Bottom-up Top-down Attention model (Anderson et al., 2018). Overall, our proposed model incorporating scene graph features achieves better results across all evaluation metrics, compared to image features only or graph features only. The results show that our model can learn to exploit"
D19-6405,W07-0734,0,0.0400744,"ome2 that consists of 150 object categories and 50 relationship categories. Notice that the number of object categories and relationships are much smaller than the actual number of objects and relationships in the Visual Genome dataset. All the predicted objects are associated with a set of bound box coordinates. The region-level image features3 are obtained from Faster-RCNN (Ren et al., 2017), which is also trained on Visual Genome, using 1,600 object classes and 400 attributes classes. Evaluation We employ standard automatic evaluation metrics including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016), and we use the coco-caption tool5 to obtain the scores. 3.1 Quantitative Results and Analysis Table 1 shows the performances of our models against baseline models whose architecture is based on Bottom-up Top-down Attention model (Anderson et al., 2018). Overall, our proposed model incorporating scene graph features achieves better results across all evaluation metrics, compared to image features only or graph features only. The results show that our model can learn to exploit the relational information in sce"
E09-1082,E06-1005,0,0.300582,"nnot make good use of the full diversity of the translations. In this paper we present M AX scores as a baseline for output selection, and approximate an oracle using the B LEU metric as an upper bound for the output selection technique. 2.2  big insert dog dog – barked barked shift very  delete loudly loudly – Table 1: Example minimum TER edit script. 0 the a 1 ε big 2 dog 3 barked 4 very ε 5 loudly 6 Figure 1: Conversion of TER script from Table 1 to a confusion network. improvements in translation quality by performing multi-source translation using generic system combination techniques (Matusov et al., 2006; Paulik et al., 2007). One class of approaches to consensus decoding focuses on construction of a confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The"
E09-1082,2001.mtsummit-papers.46,0,0.790458,"m {jschroe1, tcohn, pkoehn}@inf.ed.ac.uk Abstract In this paper, we present three models of multisource translation, with increasing degrees of sophistication, which we compare empirically on a number of different corpora. We generalize the definition of multi-source translation to include any translation case with multiple inputs and a single output, allowing for, e.g., multiple paraphrased inputs in a single language. Our methods include simple output selection, which treats the multisource translation task as many independent translation steps followed by selection of one of their outputs (Och and Ney, 2001), and output combination, which uses consensus decoding to construct a string from n-gram fragments of the translation outputs (Bangalore et al., 2001). We also present a novel method, input combination, in which we compile the input texts into a compact lattice, over which we perform a single decoding pass. We show that as we add additional inputs, the simplest output selection method performs quite poorly relative to a single input translation system, while the latter two methods are able to make better use of the additional inputs. Multi-source statistical machine translation is the process"
E09-1082,N03-1003,0,0.0516823,"h. The model parameters are λ1 , . . . , λn , ν, µ, ξ, which are trained using Powell’s search to maximise the B LEU score for the highest scoring path, arg maxP w(P). 2.3 1 ε Figure 2: Structure of a lattice of confusion networks for consensus decoding. d∈P watch it thief purse ε Input Combination Loosely defined, input combination refers to finding a compact single representation of N translation inputs. The hope is that the new input preserves as many of the salient differences between the inputs as possible, while eliminating redundant information. Lattices are well suited to this task. 2 Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. Unlike our approach, MSA does not allow reordering of inputs. 721 ¿ podría darnos las cifras correspondientes a españa y grecia ? bination. We take a similar approach for multilingual lattice generation. Our process consists of four steps: (i) Align words for each of the N (N − 1) pairs of inputs; (ii) choose an input (or many inputs) to be the lattice skeleton; (iii) extract all minimal consistent alignments between the skeleton and the other inputs; and (iv) add links to the lattice"
E09-1082,J03-1002,0,0.00882982,"Missing"
E09-1082,W07-0727,0,0.0449446,"Missing"
E09-1082,D08-1078,1,0.687833,"The oracle selection order differs from the order of the best performing systems, which could be due to the high scoring systems having very similar output while lower scoring systems exhibit greater diversity. Interestingly, the order of the languages chosen iterates between the Roman and Germanic language families and includes Greek early on. This supports our claim that diversity is important. Note though that Finnish, which is also in a separate language family, is selected last, most likely due to difficulties in word alignment and translation stemming from its morphological complexity (Birch et al., 2008). This finding might also carry over to phrase-table triangulation (Cohn and Lapata, 2007), where multi-parallel data is used in training to augment a standard translation Lattice Inputs As described in Section 2.3, lattices can be used to provide a compact format for translating multilingual inputs to a multi-source translation system. We trim all non-skeleton node paths to a maximum length of four to reduce complexity when decoding. Such long paths are mostly a result of errors in the original word alignments, and therefore pruning these links is largely innocuous. We train on the Europarl c"
E09-1082,N06-1002,0,0.0352643,"Missing"
E09-1082,P07-1092,1,0.554816,"h could be due to the high scoring systems having very similar output while lower scoring systems exhibit greater diversity. Interestingly, the order of the languages chosen iterates between the Roman and Germanic language families and includes Greek early on. This supports our claim that diversity is important. Note though that Finnish, which is also in a separate language family, is selected last, most likely due to difficulties in word alignment and translation stemming from its morphological complexity (Birch et al., 2008). This finding might also carry over to phrase-table triangulation (Cohn and Lapata, 2007), where multi-parallel data is used in training to augment a standard translation Lattice Inputs As described in Section 2.3, lattices can be used to provide a compact format for translating multilingual inputs to a multi-source translation system. We trim all non-skeleton node paths to a maximum length of four to reduce complexity when decoding. Such long paths are mostly a result of errors in the original word alignments, and therefore pruning these links is largely innocuous. We train on the Europarl corpus and use the 725 Approach test2006 test2007 French Only 29.72 30.21 French + Swedish"
E09-1082,P07-1040,0,0.0911201,"able 1: Example minimum TER edit script. 0 the a 1 ε big 2 dog 3 barked 4 very ε 5 loudly 6 Figure 1: Conversion of TER script from Table 1 to a confusion network. improvements in translation quality by performing multi-source translation using generic system combination techniques (Matusov et al., 2006; Paulik et al., 2007). One class of approaches to consensus decoding focuses on construction of a confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The first step in forming a lattice is to align the inputs. Consensus decoding systems often use the script of edit operations that minimises the translation edit rate (TER; Snover et al. (2006)). TER is a word-based measure of edit distance which also allows n-gram shifts when calculating the best match between a hy"
E09-1082,W07-0720,0,0.0373821,"Missing"
E09-1082,N07-1029,0,0.120249,"able 1: Example minimum TER edit script. 0 the a 1 ε big 2 dog 3 barked 4 very ε 5 loudly 6 Figure 1: Conversion of TER script from Table 1 to a confusion network. improvements in translation quality by performing multi-source translation using generic system combination techniques (Matusov et al., 2006; Paulik et al., 2007). One class of approaches to consensus decoding focuses on construction of a confusion network or lattice1 from translation outputs, from which new sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The first step in forming a lattice is to align the inputs. Consensus decoding systems often use the script of edit operations that minimises the translation edit rate (TER; Snover et al. (2006)). TER is a word-based measure of edit distance which also allows n-gram shifts when calculating the best match between a hy"
E09-1082,P08-1115,0,0.183992,"a language model. Formally, the path score is given by: When translating speech recognition output, previous work has shown that representing the ambiguity in the recognized text via confusion networks leads to better translations than simply translating the single best hypothesis of the speech recognition system (Bertoldi et al., 2007). The application of input lattices to other forms of input ambiguity has been limited to encoding input reorderings, word segmentation, or morphological segmentation, all showing improvements in translation quality (Costa-juss`a et al., 2007; Xu et al., 2005; Dyer et al., 2008). However, these applications encode the ambiguity arising from a single input, while in this work we combine distinct inputs into a more compact and expressive single input format. When given many monolingual inputs, we can apply TER and construct a confusion network as in Section 2.2.2 In this application of confusion networks, arc weights are calculated by summing votes from each input for a given word, and normalizing all arcs leaving a node to sum to 1. Figure 3 shows an example of a TER-derived input from IWSLT data. Because the decoder will handle reordering, we select the input with th"
E09-1082,2008.amta-srw.6,0,0.165453,"N 1 = t1 , . . . , tN . Och and Ney present two approaches for selecting a single target from these outputs. The first, P ROD, finds Q the maximiser of the product, arg maxt∈tN p(t) N n=1 pn (sn |t), where 1 p(t) is the language model probability. For reasons of tractability, the maximisation is performed only over targets generated by the translation systems, tN 1 , not the full space of all translations. The P ROD method requires each model to provide a model score for each tn generated by the other models. However, this is often impossible due to the models’ highly divergent output spaces (Schwartz, 2008), and therefore the technique cannot be easily applied. The second approach, M AX, solves arg maxt∈tN maxN p(t)p (s |t), which is n n n=1 1 much easier to calculate. As with P ROD, the translation models’ outputs are used for the candidate translations. While different models may have different score ranges, Och and Ney (2001) state that there is little benefit in weighting these scores to normalise the output range. In their experiments, they show that M AX used on pairs or triples of language inputs can outperform a model with single language input, but that performance degrades as more lang"
E09-1082,2006.amta-papers.25,0,0.0501545,"sentences can be created using different reorderings or combinations of translation fragments (e.g., Bangalore et al. (2001); Rosti et al. (2007b)). These methods differ in the types of lattices used, their means of creation, and scoring method used to extract the best consensus output from the lattice. The system used in this paper is a variant of the one proposed in Rosti et al. (2007a), which we now describe in detail. The first step in forming a lattice is to align the inputs. Consensus decoding systems often use the script of edit operations that minimises the translation edit rate (TER; Snover et al. (2006)). TER is a word-based measure of edit distance which also allows n-gram shifts when calculating the best match between a hypothesis and reference. Because TER describes the correspondence between the hypothesis and reference as a sequence of insertions, substitutions, deletions, and shifts, the edit script it produces can be used to create a confusion network. Consider a reference of “The dog barked very loudly” and a hypothesis “A big dog loudly barked.” The TER alignment is shown in Table 1, along with the edit operations. Note that the matching “barked” tokens are labelled shift, as one ne"
E09-1082,2005.mtsummit-papers.11,1,0.108174,"Missing"
E09-1082,takezawa-etal-2002-toward,0,0.0630672,"11 a pour l' 12 13 espagne españa spanien la y 14 et och 15 17 grèce grecia 18 ? grekland ? ? 19 16 Figure 5: A multi-lingual lattice input for French, Spanish, and Swedish from Europarl dev2006. Data sets for this condition are readily available in the form of test sets created for machine translation evaluation, which contains multiple target references for each source sentence. By flipping these test sets around, we create multiple monolingual inputs (the original references) and a single reference output (the original source text). We examine two datasets: the BTEC Italian-English corpus (Takezawa et al., 2002), and the Multiple Translation Chinese to English (MTC) corpora,3 as used in past years’ NIST MT evaluations. All of our translation experiments use the Moses decoder (Koehn et al., 2007), and are evaluated using B LEU-4. Moses is a phrase-based decoder with features for lexicalized reordering, distance-based reordering, phrase and word translation probabilities, phrase and word counts, and an n-gram language model. 3.1 B EST O RACLE M AX S YS C OMB CN I NPUT We tune our translation models on devset1, system combination on devset2 and report results on devset3 for each condition. When tuning t"
E09-1082,D07-1105,0,0.0205643,"tational complexity of dealing with so many models, we train on only the first 100,000 sentences of each parallel corpus. Single system baseline scores for each language are shown in Table 5. verse is true for S YS C OMB. Given the robust performance of M AX when translation scores originated from the same translation model in English to Italian, it is not surprising that it favors the case where all the outputs are scored by the same model (“All tuned”). On the other hand, diversity amongst the system outputs has been shown to be important to the performance of system combination techniques (Macherey and Och, 2007). This may give an indication as to why the “Self tuned” data produced higher scores in consensus decoding – the outputs will be more highly divergent due to their different tuning conditions. Besides comparing the different multi-source translation methods discussed above, in this task we also want to examine what happens when we use different numbers of input languages. To determine the best order to add languages, we performed a greedy search over oracle B LEU scores for test set test2005. We started with the best scoring single system, French to English, and in each iteration picked one ad"
E09-1082,2005.iwslt-1.18,0,0.00944384,"the assistance of a language model. Formally, the path score is given by: When translating speech recognition output, previous work has shown that representing the ambiguity in the recognized text via confusion networks leads to better translations than simply translating the single best hypothesis of the speech recognition system (Bertoldi et al., 2007). The application of input lattices to other forms of input ambiguity has been limited to encoding input reorderings, word segmentation, or morphological segmentation, all showing improvements in translation quality (Costa-juss`a et al., 2007; Xu et al., 2005; Dyer et al., 2008). However, these applications encode the ambiguity arising from a single input, while in this work we combine distinct inputs into a more compact and expressive single input format. When given many monolingual inputs, we can apply TER and construct a confusion network as in Section 2.2.2 In this application of confusion networks, arc weights are calculated by summing votes from each input for a given word, and normalizing all arcs leaving a node to sum to 1. Figure 3 shows an example of a TER-derived input from IWSLT data. Because the decoder will handle reordering, we sele"
E09-1082,D07-1103,0,\N,Missing
E09-1082,J93-1004,0,\N,Missing
E09-1082,E06-1032,1,\N,Missing
E09-1082,A94-1016,0,\N,Missing
E09-1082,W02-1021,0,\N,Missing
E09-1082,W05-0828,0,\N,Missing
E09-1082,C96-2141,0,\N,Missing
E09-1082,C08-1005,0,\N,Missing
E09-1082,E99-1010,0,\N,Missing
E09-1082,J90-2002,0,\N,Missing
E09-1082,D07-1005,0,\N,Missing
E09-1082,2007.iwslt-1.1,0,\N,Missing
E09-1082,P02-1040,0,\N,Missing
E09-1082,W05-0820,1,\N,Missing
E09-1082,P01-1008,0,\N,Missing
E09-1082,W09-0401,1,\N,Missing
E09-1082,W08-0329,0,\N,Missing
E09-1082,N03-1024,0,\N,Missing
E09-1082,J04-4002,0,\N,Missing
E09-1082,2007.iwslt-1.6,1,\N,Missing
E09-1082,P07-2045,1,\N,Missing
E09-1082,N04-1022,0,\N,Missing
E09-1082,W07-0733,1,\N,Missing
E09-1082,D08-1064,0,\N,Missing
E09-1082,J06-4004,0,\N,Missing
E09-1082,W07-0718,1,\N,Missing
E09-1082,P05-1033,0,\N,Missing
E09-1082,W06-3114,1,\N,Missing
E09-1082,N03-1017,1,\N,Missing
E09-1082,P02-1038,0,\N,Missing
E09-1082,P05-3026,0,\N,Missing
E09-1082,2005.eamt-1.20,0,\N,Missing
E09-1082,2008.amta-srw.3,0,\N,Missing
E09-1082,W08-0309,1,\N,Missing
E09-1082,J08-4005,1,\N,Missing
E09-1082,D08-1011,0,\N,Missing
E09-1082,2006.iwslt-evaluation.1,0,\N,Missing
E09-1082,N04-1021,0,\N,Missing
E09-1082,W99-0602,0,\N,Missing
E09-1082,W09-0409,0,\N,Missing
E09-1082,W09-0407,0,\N,Missing
E09-1082,D08-1065,0,\N,Missing
E09-1082,D08-1076,0,\N,Missing
E09-1082,P03-1021,0,\N,Missing
E14-1043,P13-1004,1,0.7237,"the various modelling approaches for the underlying inference task, the impact score (S) prediction of Twitter users based on a set of their actions. 5.1 Learning functions for regression We formulate this problem as a regression task, i.e. we infer a real numbered value based on a set of observed features. As a simple baseline, we apply Ridge Regression (RR) (Hoerl and Kennard, 1970), a reguralised version of the ordinary least squares. Most importantly, we focus on nonlinear methods for the impact score prediction task given the multimodality of the feature space. Recently, it was shown by Cohn and Specia (2013) that Support Vector Machines for Regression (SVR) (Vapnik, 1998; Smola and Sch¨olkopf, 2004), commonly considered the state-of-the-art for NLP regression tasks, can be outperformed by Gaussian Processes (GPs), a kernelised, probabilistic approach to learning (Rasmussen and Williams, 2006). Their setting is close to ours, in that they had few (17) features and were also aiming to predict a complex continuous phenomenon (human post-editing time). The initial stages of our experimental process confirmed that GPs performed better than SVR; thus, 407 we based our modelling around them, including R"
E14-1043,P13-1098,1,0.559762,"Missing"
E17-1084,D15-1131,0,0.109174,"ly on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual word embeddings (Mikolov et al., 2013a; Xiao and Guo, 2014; Faruqui and Dyer, 2014). Previous work mainly focuses on building word embeddings for a pair of languages, typically with English on one side, with the exception of Coulmance et al. (2015), Søgaard et al. (2015) and Ammar et al. (2016). Coulmance et al. (2015) extend the bilingual skipgram model from Luong et al. (2015), training jointly over many languages using the Europarl corpora. We also compare our models with an extension of Huang et al. (2015) adapted for multiple languages also using bilingual corpora. However, parallel data is an expensive resource and using parallel data seems to under-perform on the bilingual lexicon induction task (Vuli´c and Moens, 2015). While Coulmance et al. (2015) use English as the pivot language, Søgaard et al. (2015) learn multilingual word"
E17-1084,P15-2139,1,0.0635551,"dings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexicon to learn bilingual word embeddings. We modify the objective function to jointly build multilingual word embeddings during training. Unlike most prior work which focuses on downstream applications, we measure the quality of our multilingual word embeddings in three ways: biling"
E17-1084,K15-1012,1,0.928755,"dings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexicon to learn bilingual word embeddings. We modify the objective function to jointly build multilingual word embeddings during training. Unlike most prior work which focuses on downstream applications, we measure the quality of our multilingual word embeddings in three ways: biling"
E17-1084,Q16-1022,0,0.0457063,"Missing"
E17-1084,W13-3520,0,0.0116751,"71.7 80.7 76.6 86.7 Model Table 1: Bilingual lexicon induction performance for four pairs. Bilingual word embeddings (BiWE) is the state-of-the-art result from Duong et al. (2016) where each pair is trained separately. Our proposed methods including linear transformation (Linear), joint prediction as in Equation (3) (Joint) and joint prediction with explicit mapping as in Equation (4) (+mapping). We report recall at 1 and 5 with respect to four baseline multilingual word embeddings. The best scores for are shown in bold. tences from the tokenized monolingual data from the Wikipedia dump from Al-Rfou et al. (2013).5 The dictionary is from PanLex which covers more than 1,000 language varieties. We build multilingual word embeddings for 5 languages (en, it, es, nl, de) jointly using the same parameters as Duong et al. (2016).6 During training, for a fairer comparison, we only use lexicons between English and each target language. However, it is straightforward to incorporate a lexicon between any pair of languages into our model. The pretrained bilingual word embeddings for the postprocessing experiment in §3 are also from Duong et al. (2016). In the following sections, we evaluate the performance of our"
E17-1084,P15-1033,0,0.00533046,"glish on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space. In this way, we can exploit and combine information from many languages. We report competitive performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks. 1 Introduction Monolingual word embeddings have facilitated advances in many natural language processing tasks, such as natural language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012). Most prior work on building crosslingual wo"
E17-1084,J93-2003,0,0.109062,"on from many languages. We report competitive performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks. 1 Introduction Monolingual word embeddings have facilitated advances in many natural language processing tasks, such as natural language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012). Most prior work on building crosslingual word embeddings focuses on a pair of languages. English is usually on one side, thanks to the wealth 1 From here on we refer to crosslingual word embeddings for a pair of languages and multiple l"
E17-1084,E14-1049,0,0.585796,"monolingual and crosslingual transfer settings. 2 beddings for many languages using Wikipedia entries which are the same for many languages. However, their approach is limited to languages covered in Wikipedia and seems to under-perform other methods. Ammar et al. (2016) propose two algorithms, MultiCluster and MultiCCA, for multilingual word embeddings using set of bilingual lexicons. MultiCluster first builds the graph where nodes are lexical items and edges are translations. Each cluster in this graph is an anchor point for building multilingual word embeddings. MultiCCA is an extension of Faruqui and Dyer (2014), performing canonical correlation analysis (CCA) for multiple languages using English as the pivot. A shortcoming of MultiCCA is that it ignores polysemous translations by retaining only one-to-one dictionary pairs (Gouws et al., 2015), disregarding much information. As a simple solution, we propose a simple post hoc method by mapping the English parts of each bilingual word embedding to each other. In this way, the mapping is always exact and one-to-one. Duong et al. (2016) constructed bilingual word embeddings based on monolingual data and PanLex. In this way, their approach can be applied"
E17-1084,D11-1006,0,0.0296647,"gs for many languages so that different relations can be exploited.1 For example, since Italian and Spanish are similar, they are excellent candidates for transfer learning. However, few parallel resources exist between Italian and Spanish for directly building bilingual word embeddings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexico"
E17-1084,N13-1090,0,0.747701,"., 2014; Huang et al., 2015). Other work uses more widely available resources such as comparable data (Vuli´c and Moens, 2015) and shared Wikipedia entries (Søgaard et al., 2015). However, those approaches rely on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual word embeddings (Mikolov et al., 2013a; Xiao and Guo, 2014; Faruqui and Dyer, 2014). Previous work mainly focuses on building word embeddings for a pair of languages, typically with English on one side, with the exception of Coulmance et al. (2015), Søgaard et al. (2015) and Ammar et al. (2016). Coulmance et al. (2015) extend the bilingual skipgram model from Luong et al. (2015), training jointly over many languages using the Europarl corpora. We also compare our models with an extension of Huang et al. (2015) adapted for multiple languages also using bilingual corpora. However, parallel data is an expensive resource and using pa"
E17-1084,D15-1127,0,0.0415669,"Missing"
E17-1084,D13-1170,0,0.00191467,"ts embeddings for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space. In this way, we can exploit and combine information from many languages. We report competitive performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks. 1 Introduction Monolingual word embeddings have facilitated advances in many natural language processing tasks, such as natural language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012)."
E17-1084,kamholz-etal-2014-panlex,0,0.0332869,"ccurrence statistics from parallel text (Luong et al., 2015; Gouws et al., 2015; Chandar A P et al., 2014; Klementiev et al., 2012; Koˇcisk´y et al., 2014; Huang et al., 2015). Other work uses more widely available resources such as comparable data (Vuli´c and Moens, 2015) and shared Wikipedia entries (Søgaard et al., 2015). However, those approaches rely on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual word embeddings (Mikolov et al., 2013a; Xiao and Guo, 2014; Faruqui and Dyer, 2014). Previous work mainly focuses on building word embeddings for a pair of languages, typically with English on one side, with the exception of Coulmance et al. (2015), Søgaard et al. (2015) and Ammar et al. (2016). Coulmance et al. (2015) extend the bilingual skipgram model from Luong et al. (2015), training jointly over many languages using the Europarl corpora. We also compare our models with an extensio"
E17-1084,P15-1165,0,0.0362412,"Missing"
E17-1084,C12-1089,0,0.235651,"ing an EM algorithm for selecting a lexicon. Relative to many previous crosslingual word embeddings, their joint training algorithm achieved state-of-the-art performance for the bilingual lexicon induction task, performing significantly better on monolingual similarity and achieving a competitive result on cross lingual document classification. Here we also adopt their approach, and extend it to multilingual embeddings. Related work Crosslingual word embeddings are typically based on co-occurrence statistics from parallel text (Luong et al., 2015; Gouws et al., 2015; Chandar A P et al., 2014; Klementiev et al., 2012; Koˇcisk´y et al., 2014; Huang et al., 2015). Other work uses more widely available resources such as comparable data (Vuli´c and Moens, 2015) and shared Wikipedia entries (Søgaard et al., 2015). However, those approaches rely on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual"
E17-1084,N12-1052,0,0.0672846,"Missing"
E17-1084,P14-2037,0,0.0227298,"Missing"
E17-1084,P15-2118,0,0.130755,"Missing"
E17-1084,W13-3512,0,0.10193,"Missing"
E17-1084,P13-1106,0,0.0685321,"Missing"
E17-1084,P12-1000,0,0.201693,"Missing"
E17-1084,W15-1521,0,0.194796,"Missing"
E17-1084,N01-1026,0,0.030795,"language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012). Most prior work on building crosslingual word embeddings focuses on a pair of languages. English is usually on one side, thanks to the wealth 1 From here on we refer to crosslingual word embeddings for a pair of languages and multiple languages as bilingual word embeddings and multilingual word embeddings respectively. 894 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 894–904, c Valencia, Spain, April 3-7, 2017. 2017 Association for"
E17-1084,N16-1004,0,0.0163154,"allel resources exist between Italian and Spanish for directly building bilingual word embeddings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexicon to learn bilingual word embeddings. We modify the objective function to jointly build multilingual word embeddings during training. Unlike most prior work which focuses on downstream applic"
E17-1084,P11-1061,0,\N,Missing
E17-1084,D16-1136,1,\N,Missing
E17-1088,D16-1047,0,0.0201262,"recognition of these languages, a difficult challenge. One of the touted advantages of neural network language models (NNLMs) is their ability to model sparse data (Bengio et al., 2003; Gandhe et al., 2014). However, despite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while maintaining a large source language dataset"
E17-1088,P16-1186,0,0.0258344,"Missing"
E17-1088,D15-1131,0,0.060784,"e (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the s"
E17-1088,W13-3520,0,0.016042,"judgements of word similarity. Here we follow the same evaluation procedure, except where we simulate a lowresource language by reducing the availability of target English monolingual text while preserving a large quantity of source language text from other languages. This allows us to evaluate the CLWEs intrinsically using the WordSim353 task (Finkelstein et al., 2001) before progressing to downstream language modeling where we additionally consider other target languages. We trained a variety of embeddings on English Wikipedia data of between 1k and 128k sentences from the training data of Al-Rfou et al. (2013). In terms of transcribed speech data, this roughly 1 Hyperparameters for both mono and cross-lingual word embeddings: iters=15, negative=25, size=200, window=48, otherwise default. Smaller window sizes led to similar results for monolingual methods. 2 We also tried Italian, Dutch, German and Serbian, yielding similar results but omitted for presentation. 939 1,000 0.6 800 Perplexity Spearman’s ρ 0.8 0.4 0.2 600 400 0.0 1,000 10,000 200 100,000 1,000 Sentences GNC –de CBOW –ru SG –fi –ja –es MKN3 50 Figure 1: Performance of different embeddings on the WordSim353 task with different amounts of"
E17-1088,D16-1136,1,0.594676,"so been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creation of high-quality lexicons, despite the dearth of transcriptions. For example, the training of a quality speech recognition system for Yongning Na, a Sino-Tibetan language spoken by approximately 40k people, is hindered by this lack of data (Do et al., 2014) despite significant linguistic investigation of the language (Michaud, 2008; Michaud, 2016). In this paper we address two research questions. First, is the quality of CLWEs dependent on having large"
E17-1088,E14-1049,0,0.06185,"rk architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim package (Reh˚ Sojka, 2010). We"
E17-1088,kamholz-etal-2014-panlex,0,0.0859548,"News Corpus embeddings with 300 dimensions, trained on 100 billion words. The CLWEs were trained using the method of Duong et al. (2016) since their method addresses polysemy which is rampant in dictionaries. The same 1k-128k sentence English Wikipedia data was used but with an additional 5 million sentences of Wikipedia data in a source language. The source languages include Japanese, German, Russian, Finnish, and Spanish, which represent languages of varying similarity with English, some with great morphological and syntactic differences. To relate the languages, we used the PanLex lexicon (Kamholz et al., 2014). Following Duong et al. (2016), we used the default window size of 48 so that the whole sentence’s context is almost always taken into account. This mitigates the effect of word re-ordering between languages. We trained with an embedding dimension of 200 for all data sizes as a larger dimension turned out to be helpful in capturing information from the source side.1 lexicon to connect the languages and represent the words in a common vector space. The model builds on the continuous bag-of-words (CBOW) model (Mikolov et al., 2013a) which learns embeddings by predicting words given their contex"
E17-1088,C12-1089,0,0.037298,"odels (Hochreiter and Schmidhuber, 1997) for modeling long-ranging statistical influences have been shown to be effective (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that in"
E17-1088,P14-2037,0,0.00965011,"Missing"
E17-1088,N15-1157,0,0.0789589,"e of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creation of high-quality lexicons, despite the dearth of transcriptions. For example, the training of a quality speech recognition system for Yongning Na, a Sino-Tibetan language spoken by approximately 40k people, is hindered by this lack of data (Do et al., 2014) despite significant linguistic investigation of the language (Michaud, 2008; Michaud, 2016). In this paper we address two research questions. First, is the quality of CLWEs depen"
E17-1088,W16-1609,0,0.0426659,"emains unclear whether their advantages transfer to scenarios with extremely limited amounts of data. Appropriate initialization of parameters in neural network frameworks has been shown to be beneficial across a wide variety of domains, including speech recognition, where unsupervised pretraining of deep belief networks was instrumental in attaining breakthrough performance (Hinton et al., 2012). Neural network approaches to a range of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creatio"
E17-1088,W11-2123,0,0.0461598,"Missing"
E17-1088,W16-1614,0,0.00570031,"Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim package (Reh˚ Sojka, 2010). We additionally used off-the-shelf CBOW Google News Corpus embeddings with 300 dimensions, trained on"
E17-1088,P14-1011,0,0.0293781,"Graves, 2013), it remains unclear whether their advantages transfer to scenarios with extremely limited amounts of data. Appropriate initialization of parameters in neural network frameworks has been shown to be beneficial across a wide variety of domains, including speech recognition, where unsupervised pretraining of deep belief networks was instrumental in attaining breakthrough performance (Hinton et al., 2012). Neural network approaches to a range of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation"
E17-1088,D16-1163,0,0.0176967,"pite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while maintaining a large source language dataset. We assess intrinsic embedding quality by considering correlation with human judgment on the WordSim353 test set (Finkelstein et al., 2001). We then perform language modeling experiments where we initialize the parame"
E17-1088,D13-1141,0,0.0113705,"midhuber, 1997) for modeling long-ranging statistical influences have been shown to be effective (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the"
E17-1088,D16-1124,1,0.822625,"esults of tuning the dimensions of the hidden layer in the LSTM with respect to perplexity on the validation set,3 as well as tuning the order of n-grams used by the MKN language model. A dimension of 100 yielded a good compromise between the smaller and larger training data sizes, while an order 5 MKN model performed slightly better than its lower-order brethren.4 Interestingly, MKN strongly outperforms the LSTM on low quantities of data, with the LSTM language model not reaching parity until between 16k and 32k sentences of data. This is consistent with the results of Chen et al. (2015) and Neubig and Dyer (2016) that show that n-gram models are typically better for rare words, and here our vocabulary is large but training data small since the data are random Wikipedia sentences. However these findings are inconsistent with the belief that NNLMs have the ability to cope well with sparse data conditions because of the smooth distributions that arise from using dense vector representations of words (Bengio et al., 2003). Traditional smoothing stands strong. 4.2 1,000 Perplexity 800 600 400 200 1,000 10,000 100,000 Sentences MKN –nl LSTM –el mono –ja GNC –fi Figure 3: Perplexity of LSTMs when pre-trained"
E17-1088,D14-1162,0,0.112314,"ng, which is a key tool for facilitating speech recognition of these languages, a difficult challenge. One of the touted advantages of neural network language models (NNLMs) is their ability to model sparse data (Bengio et al., 2003; Gandhe et al., 2014). However, despite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while"
E17-1088,W14-1613,0,0.0121171,"shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim packag"
E17-2004,P06-1048,0,0.0136187,"pression is based on deep learning methods such as recurrent neural networks (Filippova et al., 2015), we implement a simple parser-based model, due to the lack of large-scale annotated data for training and the fact that a relative lack of precision in the output may ultimately help our method. First, we parse the sentence using the Stanford CoreNLP constituency parser (Chen and Manning, 2014). Next, we model the conditional probability of deleting a sub-tree C with label S given its parent node with label R by p(C|S, R) = p(C,S,R) ΣC p(C,S,R) , trained on the sentence compression corpora of Clarke and Lapata (2006),4 made up of a few hundred labelled instances. Generating Text Noise Our method involves the explicit generation of several kinds of linguistic corruption, to train more robust deep models. The first question is how to generate the linguistic noise, focusing on English for the purposes of this paper. We focus on the generation of two classes of text noise: (1) syntactic noise; and (2) semantic noise.2 Syntactic Noise The first class of linguistic noise is syntactic, focusing on the syntactic strucSemantic Noise The second class of linguistic noise is semantic noise. Semantic noise is more sub"
E17-2004,P14-1062,0,0.00307318,"sequential in nature, with latent syntactic structure. Based on the same linguistic intuition, adversarial evaluation for natural language processing models was proposed by Smith (2012). Also, adversarial learning for text, such as perceptron learning (Søgaard, 2013) and unsupervised estimation methods (Smith and Eisner, 2005), have been studied in the language area. Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013), and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Bitvai and Cohn, 2015). However, deep models tend to be overconfident in their predictions over noisy test instances, including adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). A range of methods have been proposed to train models to be more robust, such as injecting noise into the data and hidden layers (Jiang et al., 2009), dropout (Srivastava et al., 2014), and the incorporation of explicit regularization terms into the training objective (Ng, 2004; Li et al., 2016). In this work, we propose a linguisticallymotivated method customised to text applications, based on in"
E17-2004,copestake-flickinger-2000-open,0,0.0656511,"eration of linguistic corruption over the source training instances, to train robust text models. Empirically, we demonstrate the effectiveness of our method over a range of sentiment analysis datasets using a state-of-the-art convolutional neural network model (Kim, 2014). In this, we show that our method is superior to a baseline and dropout (Srivastava et al., 2014) using MAP training.1 2 ture of the input, either through explicit parsing and generation using a deep linguistic parser, or sentence compression. For the deep linguistic parser, we use the LinGO English Resource Grammar (“ERG”: Copestake and Flickinger (2000)) with the ACE parser, based on pyDelphin.3 The ERG supports both parsing and generation, via the semantic formalism of Minimal Recursion Semantics (“MRS”: Copestake et al. (2005)). To generate paraphrases with the ERG, we simply parse a given input, select the preferred parse using a pretrained parse selection model (Oepen et al., 2002), and exhaustively generate from the resultant MRS. We then use uniform random sampling to select from the generator outputs, which potentially numbers in the thousands of variants. To handle unknown words during parsing and generation, we use POS mapping and i"
E17-2004,D14-1181,0,0.00379593,"example sentence and sample corrupted outputs after applying each type of linguistic noise. The ERG seldom changes words, and instead tends to reorder the words based on syntactic alternation. C OMP performs like word dropout in that it tends to remove tokens with low semantic content and to generate complete sentences. WN and CF IT both only modify the text at the word level, based on near-synonyms and words with similar semantic function, respectively. 3 Models and Training We evaluate our methods on several sentence classification tasks, using a convolutional neural network (“CNN”) model (Kim, 2014). Note that our method corrupts the input directly, and is thus easily transferrable to other classes of models (e.g., other deep learning or linear models). We experiment with two approaches to generating the substitution candidates. The first is based on Princeton WordNet (“WN”: Miller et al. (1990)), over all synsets that a given substitutable word occurs in, using the NLTK API (Bird, 2006). The second is based on the “counterfitting” method of Mrkˇsi´c et al. (2016) (“CF IT”), whereby word embeddings from WORD 2 VEC are projected based on a supervised objective function which penalises sim"
E17-2004,N13-1037,0,0.0301969,"Missing"
E17-2004,D15-1042,0,0.0106324,"Missing"
E17-2004,P05-1045,0,0.00440533,"urces/ 4 22 not to impact on the fidelity of the original labels, which can readily occur with full paraphrasing or abstractive summarisation. As such, we focus on lexical substitution of near-synonyms of words in the original text, and experiment with two methods for generating near-synonyms. Our approach to generating semantic noise proceeds as follows. First, we apply filters to identify words which should not be candidates for lexical substitution, namely words which are parts of named entities or function words. As such, we use the Stanford CoreNLP POS tagger and named entity recogniser (Finkel et al., 2005; Chen and Manning, 2014), and identify “substitutable words” as those which are nouns, verbs, adjectives or adverbs, and not part of a named entity. For each substitutable word w, we generate the set of substitution candidates s(w). For each candidate wi ∈ {w} ∪ s(w) we allow the original word to be preserved with p(wi ) = α, and share the remaining 1−α proportional to the language model score based on substituting wi into the original text. For this, we use the pre-trained US English language model from the CMU Sphinx Speech Recognition toolkit.5 Finally, we sample from the probability distr"
E17-2004,D16-1207,1,0.901542,"Missing"
E17-2004,N16-1018,0,0.0151929,"Missing"
E17-2004,P11-1038,1,0.822346,"Missing"
E17-2004,C02-2025,0,0.0401639,"Missing"
E17-2004,P05-1015,0,0.0079222,"o thresholds on the random variable for substitution of each word: low (“lo”; α = 0.5) and high (“hi”; α = 0). Besides the above methods which employ a single type noise, we experiment with a combination (C OMB) of the four different noise types (ERG + C OMP + WNlo + CF ITlo ), by uniformly randomly choosing one of the four methods for noise generation each time we process a training instance. Datasets We experiment on the following datasets: • MR: sentence polarity dataset from movie reviews (Pang and Lee, 2008)8 • CR: customer review dataset (Hu and Liu, 2004)9 • Subj: subjectivity dataset (Pang and Lee, 2005)8 • SST: Stanford Sentiment Treebank, using the 2-class configuration (Socher et al., 2013)10 We evaluate using classification accuracy, based on both in-domain evaluation11 and a crossdomain setting, in which we evaluate a model trained on MR and tested on CR, and vice versa. This last setting characterises a realistic applica7 Using a single application of noise is less effective, but still yields improvements over baseline methods including dropout. 8 https://www.cs.cornell.edu/people/ pabo/movie-review-data/ 9 http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html 10 http://nlp.stanford."
E17-2004,P16-2024,0,0.00700087,"Missing"
E17-2004,D14-1162,0,0.116561,"Missing"
E17-2004,P05-1044,0,0.0610564,"rk on adversarial training (Goodfellow et al., 2015). This kind of noise is cheap to generate for images and is transferable between different models, but it is less clear how to generate analogous textual noise while preserving the fidelity of the training data, due to text being discrete and sequential in nature, with latent syntactic structure. Based on the same linguistic intuition, adversarial evaluation for natural language processing models was proposed by Smith (2012). Also, adversarial learning for text, such as perceptron learning (Søgaard, 2013) and unsupervised estimation methods (Smith and Eisner, 2005), have been studied in the language area. Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013), and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Bitvai and Cohn, 2015). However, deep models tend to be overconfident in their predictions over noisy test instances, including adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). A range of methods have been proposed to train models to be more robust, such as injecting noise into t"
E17-2004,D13-1170,0,0.00311578,"Missing"
E17-2004,P13-2113,0,0.0816002,"an observation that has been harnessed in recent work on adversarial training (Goodfellow et al., 2015). This kind of noise is cheap to generate for images and is transferable between different models, but it is less clear how to generate analogous textual noise while preserving the fidelity of the training data, due to text being discrete and sequential in nature, with latent syntactic structure. Based on the same linguistic intuition, adversarial evaluation for natural language processing models was proposed by Smith (2012). Also, adversarial learning for text, such as perceptron learning (Søgaard, 2013) and unsupervised estimation methods (Smith and Eisner, 2005), have been studied in the language area. Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013), and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Bitvai and Cohn, 2015). However, deep models tend to be overconfident in their predictions over noisy test instances, including adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). A range of methods have been proposed to t"
E17-2004,P06-4018,0,\N,Missing
E17-2004,D14-1082,0,\N,Missing
E17-2004,I13-1041,1,\N,Missing
E17-2019,K15-1017,1,0.899754,"Missing"
E17-2019,N16-1080,1,0.899654,"Missing"
E17-2019,P15-1033,0,0.0145967,"), as well as other nominalisations (e.g., explain 7→ explanation). Nominallemma <s> environmental { d e v a s t a t e } contextR contextL guages such as English where grapheme-phoneme correspondences are opaque. For this reason we consider orthographic rather than phonological representations. In our approach, we test how well models incorporating distributional semantics can capture derivational transformations. Deep learning models capable of learning real-valued word embeddings have been shown to perform well on a range of tasks, from language modelling (Mikolov et al., 2013a) to parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2015). Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Sch¨utze, 2016; Cotterell et al., 2016a). is best exemplified by ... </s> output generation ... encoding d e v a s t a t e } } } d e v a s t a t i o n } Figure 1: The encoder–decoder model, showing the stem devastate in context producing the form devastation. Coloured arrows indicate shared parameters isations have varyingly different meanings from their base verbs, and a key focus of this study is the prediction of which form is most approp"
E17-2019,N16-2002,0,0.0171762,"d a key focus of this study is the prediction of which form is most appropriate depending on the context, in terms of syntactic and semantic concordance. Our model is highly flexible and easily applicable to other related lexical problems. 3 Related Work Although in the last few years many neural morphological models have been proposed, most of them have focused on inflectional morphology (e.g., see Cotterell et al. (2016a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (20"
E17-2019,W11-0115,0,0.0312045,"6a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms. Kisselew et al. 119 (2015) and Pad´o et al. (2016) extend this line of research to model derivational morphology in German. This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word. The"
E17-2019,N16-1149,1,0.837461,"m, to bias the model to generate a derived form that is morphologically-related to the base 120 baseline biLSTM+BS biLSTM+CTX biLSTM+CTX+BS biLSTM+CTX+BS+POS LSTM+CTX+BS+POS Shared Split 0.63 0.58 0.80 0.83 0.89 0.90 — 0.36 0.45 0.52 0.63 0.66 Table 1: Accuracy for predicted lemmas (bases and derivations) on shared and split lexicons verb. In most cases, the derived form is longer than its stem, and accordingly, when we reach the end of the base form, we continue to input an end-of-word symbol. We provide the model with the context vector o at each decoding step. It has been previously shown (Hoang et al., 2016) that this yields better results than other means of incorporation.4 Finally, we use max pooling to enable the model to switch between copying of a stem or producing a new character. 5.3 Settings We used a 3-layer bidirectional LSTM network, with hidden dimensionality h for both context and base-form stem states of 100, and character embedding cj of 100.5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b). During the training of the model, we keep the word embeddings fixed, for greater applicability to unseen test instances. All token"
E17-2019,P16-2090,0,0.0804887,"Missing"
E17-2019,W15-0108,0,0.11611,"Missing"
E17-2019,P13-1149,0,0.0775424,"ther using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms. Kisselew et al. 119 (2015) and Pad´o et al. (2016) extend this line of research to model derivational morphology in German. This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word. The third area of research focuses on the analysis of derivationally complex forms, which differs from this study in that we focus on generation. The goal o"
E17-2019,P96-1004,0,0.165816,"ology is the set of processes through which the word form outwardly displays syntactic information, e.g., verb tense. It follows that an inflectional affix typically neither changes the part-of-speech (POS) nor the semantics of the word. For example, the English verb to run takes various forms: run, runs and ran, all of which convey the concept “moving by foot quickly”, but appear in complementary syntactic contexts. Derivation, on the other hand, deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996). Consider the example of the English noun discontentedness, which is derived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness. In this work, we deal with the formation of deverbal nouns, i.e., nouns that are formed from verbs. Common examples of this in English include agentives (e.g., explain 7→ explainer), gerunds (e.g., explain 7→ explaining"
E17-2019,C16-1122,0,0.137298,"Missing"
E17-2019,P16-1158,1,0.668374,"most appropriate depending on the context, in terms of syntactic and semantic concordance. Our model is highly flexible and easily applicable to other related lexical problems. 3 Related Work Although in the last few years many neural morphological models have been proposed, most of them have focused on inflectional morphology (e.g., see Cotterell et al. (2016a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate v"
I17-1056,N16-1030,0,0.47323,"shared task dataset consists of 14, 041/3, 250/3, 453 sentences in the training/development/test set, resp., extracted from 946/216/231 Reuters news articles from the period 1996–97. The goal is to identify individual token occurrences of NEs, and tag each with its class (e.g. LOCATION or ORGANISATION). Here, we use the IOB tagging scheme. In terms of tagging schemes, while some have shown improvements with a more expressive IOBES marginally (Ratinov and Roth, 2009; Dai et al., 2015), we stick to the BIO scheme for simplicity and the observation of little improvement between these schemes by Lample et al. (2016). 5.2 Evaluation Evaluation is based on span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of either a simple LSTM or bidirectional LSTM (“LSTMCRF” and “B I -LSTM-CRF”, resp.: Huang et al. (2015)) Note that for our word embeddings, while we ob"
I17-1056,P05-1045,0,0.658022,"t.cohn@unimelb.edu.au Abstract linear-chain Conditional Random Fields (CRFs) (Wang et al., 2011; Zhang et al., 2017), framing the task as one of sequence tagging. Although CRFs are adept at capturing local structure, the problem does not naturally suit a linear sequential structure, i.e. , a post may be a reply to either a neighbouring post or one posted far earlier within the same thread. In both cases, contextual dependencies can be long-range, necessitating the ability to capture dependencies between arbitrarily distant items. Identifying this key limitation, Sutton and McCallum (2004) and Finkel et al. (2005) proposed the use of CRFs with skip connections to incorporate long-range dependencies. In both cases the graph structure must be supplied a priori, rather than learned, and both techniques incur the need for costly approximate inference. Recurrent neural networks (RNNs) have been proposed as an alternative technique for encoding sequential inputs, however plain RNNs are unable to capture long-range dependencies (Bengio et al., 1994; Hochreiter et al., 2001) and variants such as LSTMs (Hochreiter and Schmidhuber, 1997), although more capabable of capturing non-local patterns, still exhibit a s"
I17-1056,P10-1081,0,0.0317937,"y said ··· B-ORG O B-MISC O Interfax quoted Russian military ··· Figure 1: A NER example with long-range contextual dependencies. The vertical dash line indicates a sentence boundary. incompatibility with exact inference. Similar ideas have also been explored by Krishnan and Manning (2006) for NER, where they apply two CRFs, the first of which makes predictions based on local information, and the second combines named entities identified by the first CRF in a single cluster, thereby enforcing label consistency and enabling the use of a richer set of features to capture non-local dependencies. Liao and Grishman (2010) make a strong case for going beyond sentence boundaries and leveraging document-level information for event extraction. While we take inspiration from these earlier studies, we do not enforce label consistency as a hard constraint, and additionally do not sacrifice inference tractability: our model is capable of incorporating non-local features, and is compatible with exact inference methods. mentation of the model is available at: https: //github.com/liufly/mecrf. The paper is organised as follows: after reviewing previous studies on capturing long range contextual dependencies and related m"
I17-1056,D15-1161,0,0.0641599,"Missing"
I17-1056,Q16-1037,0,0.435365,"porate long-range dependencies. In both cases the graph structure must be supplied a priori, rather than learned, and both techniques incur the need for costly approximate inference. Recurrent neural networks (RNNs) have been proposed as an alternative technique for encoding sequential inputs, however plain RNNs are unable to capture long-range dependencies (Bengio et al., 1994; Hochreiter et al., 2001) and variants such as LSTMs (Hochreiter and Schmidhuber, 1997), although more capabable of capturing non-local patterns, still exhibit a significant locality bias in practice (Lai et al., 2015; Linzen et al., 2016). In this paper, taking inspiration from the work of Weston et al. (2015) on memory networks (M EM N ETs), we propose to extend CRFs by integrating external memory mechanisms, thereby enabling the model to look beyond localised features and have access to the entire sequence. This is achieved with attention over every entry in the memory. Experiments on named entity recognition and forum thread parsing, both of which involve long-range contextual dependencies, demonstrate the effectiveness of the proposed model, achieving state-of-the-art performance on the former, and outperforming a number o"
I17-1056,D14-1162,0,0.0826714,"span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of either a simple LSTM or bidirectional LSTM (“LSTMCRF” and “B I -LSTM-CRF”, resp.: Huang et al. (2015)) Note that for our word embeddings, while we observe better performance with G LOV E (Pennington et al., 2014), for fair comparison purposes, we adopt the same S ENNA embeddings (Collobert et al., 2011) as are used in the baseline methods.5 Experimental Setup We choose Φ(xt ) to be a lookup function, returning the corresponding embedding xt of the word xt . In addition to the word features, we employ a subset of the lexical features described in Huang et al. (2015), based on whether the word: • starts with a capital letter; • is composed of all capital letters; • is composed of all lower case letters; • contains non initial capital letters; • contains both letters and digits; • contains punctuation. T"
I17-1056,W10-2923,1,0.914292,"Missing"
I17-1056,W09-1119,0,0.0736863,"g the ability of ME-CRF to capture document context, to aid in the identification and disambiguation of NEs. 5.1 Dataset and Task 5.3 The CoNLL 2003 NER shared task dataset consists of 14, 041/3, 250/3, 453 sentences in the training/development/test set, resp., extracted from 946/216/231 Reuters news articles from the period 1996–97. The goal is to identify individual token occurrences of NEs, and tag each with its class (e.g. LOCATION or ORGANISATION). Here, we use the IOB tagging scheme. In terms of tagging schemes, while some have shown improvements with a more expressive IOBES marginally (Ratinov and Roth, 2009; Dai et al., 2015), we stick to the BIO scheme for simplicity and the observation of little improvement between these schemes by Lample et al. (2016). 5.2 Evaluation Evaluation is based on span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of"
I17-1056,P06-1141,0,0.0305568,"istent identification and disambiguation. A related example is forum thread discourse analysis. Previous work has largely focused on 555 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 555–565, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP ··· B-ORG O O O Interfax news agency said ··· B-ORG O B-MISC O Interfax quoted Russian military ··· Figure 1: A NER example with long-range contextual dependencies. The vertical dash line indicates a sentence boundary. incompatibility with exact inference. Similar ideas have also been explored by Krishnan and Manning (2006) for NER, where they apply two CRFs, the first of which makes predictions based on local information, and the second combines named entities identified by the first CRF in a single cluster, thereby enforcing label consistency and enabling the use of a richer set of features to capture non-local dependencies. Liao and Grishman (2010) make a strong case for going beyond sentence boundaries and leveraging document-level information for event extraction. While we take inspiration from these earlier studies, we do not enforce label consistency as a hard constraint, and additionally do not sacrifice"
I17-1056,D16-1021,0,0.0221529,"so associated with a title. We therefore use two encoders, Φtitle (·) and Φtext (·), to process them separately and then concatenate xt = [Φtitle (xt ); Φtext (xt )]&gt; . Here, Φtitle (·) and Φtext (·) take word embeddings as input, processing each post at the word level, as opposed to the post-level bi-directional GRU in Equations (1) and (2), and the representation of a post xt (either title or text) is obtained by transforming the last and first hidden states of the forward and backward word-level GRU, similar to Equation (3). Note that Φtitle (·) and Φtext (·) do not share parameters. As in Tang et al. (2016), we further restrict mki = cki to curb overfitting. In keeping with Wang (2014), we complement the textual representations with hand-crafted structural features Φs (xt ) ∈ R2 : • initiator: a binary feature indicating whether the author of the current post is the same as the initiator of the thread, • position: the relative position of the current post, as a ratio over the total number of posts in the thread; Also drawing on Wang (2014), we incorporate punctuation-based features Φp (xt ) ∈ R3 : the number of question marks, exclamation marks and URLs in the current post. The resultant feature"
I17-1056,W03-0419,0,0.193167,"Missing"
I17-1056,D11-1002,1,0.920639,"Missing"
I17-1056,N16-1174,0,0.0839786,"rk In this section, we review the different families of models that are relevant to this work, in capturing long-range contextual dependencies in different ways. Recurrent Neural Networks (RNNs). Recently, the broad adoption of deep learning methods in NLP has given rise to the prevalent use of RNNs. Long short-term memories (“LSTMs”: Hochreiter and Schmidhuber (1997)), a particular variant of RNN, have become particularly popular, and been successfully applied to a large number of tasks: speech recognition (Graves et al., 2013), sequence tagging (Huang et al., 2015), document categorisation (Yang et al., 2016), and machine translation (Cho et al., 2014; Bahdanau et al., 2014). However, as pointed out by Lai et al. (2015) and Linzen et al. (2016), RNNs — including LSTMs — are biased towards immediately preceding (or neighbouring, in the case of bidirectional RNNs) items, and perform poorly in contexts which involve long-range contextual dependencies, despite the inclusion of memory cells. This is further evidenced by the work of Cho et al. (2014), who show that the performance of a basic encoder–decoder deteriorates as the length of the input sentence increases. Conditional Random Fields (CRFs). CRF"
I17-1056,W14-4012,0,\N,Missing
I17-1075,C12-1064,0,0.241795,"ion 744 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 744–753, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Partition Training Development Test Related Work Early work in geolocation prediction operated at the user-level. Backstrom et al. (2010) developed a methodology to predict the location of a user on Facebook by measuring the relationship between geography and friendship networks, and Cheng et al. (2010) proposed a content-based prediction system to predict a Twitter user’s location based purely on his/her tweet messages. Han et al. (2012) introduced tweet-level prediction, where they first extract location indicative words by leveraging the geotagged tweets and then train a classifier for geolocation prediction using the location indicative words as features. Extending on this, systems were developed to better rank these location indicative or geospatial words by locality (Chang et al., 2012; Laere et al., 2014; Han et al., 2014). More recently, Han et al. (2016) proposed a shared task for Twitter geolocation prediction, offering a benchmark dataset on the task. Hashing is an effective method to compress data for fast access a"
I17-1075,W16-3929,0,0.337912,"Missing"
I17-1075,D14-1181,0,0.0031887,"in hours or bins. This preference of bins should be different to tweets from a distant location (e.g. East Asia). Assuming each bin follows a Gaussian distribution, then the goal of the network is to learn where ri is the output value and µi and σi are the parameters for bin i. Let B be the total number of bins, the feature vector generated by a RBF network is given as follows: frbf = [r0 , r1 , ..., rB−1 ] where frbf ∈ RB . 3.2.3 Convolutional Network Location is a user self-declared field in the metadata. As it is free-form text, we use a standard character-level convolution neural network (Kim, 2014) to process it. The network architecture is simpler compared to the text network (Section 3.2.1): it has no recurrent and self-attention layers, and max-over-time pooling is performed over all spans. Let xt ∈ RE denote the character embedding of the t-th character in the tweet. A tweet of T characters is represented by a concatenation of its character vectors: x0:T −1 = x0 ⊕ x1 ⊕ ... ⊕ xT −1 . We 7 As an example, 17:25 is converted to 0.726. UTC offset minimum is assumed -12 and maximum +14 based on: https://en.wikipedia.org/wiki/ List_of_UTC_time_offsets. 8 747 use convolutional filters and m"
I17-1075,W16-3930,1,0.898224,"Missing"
I17-1075,W16-3931,0,0.195993,"Missing"
I17-1075,W16-3928,0,0.753432,"d friendship networks, and Cheng et al. (2010) proposed a content-based prediction system to predict a Twitter user’s location based purely on his/her tweet messages. Han et al. (2012) introduced tweet-level prediction, where they first extract location indicative words by leveraging the geotagged tweets and then train a classifier for geolocation prediction using the location indicative words as features. Extending on this, systems were developed to better rank these location indicative or geospatial words by locality (Chang et al., 2012; Laere et al., 2014; Han et al., 2014). More recently, Han et al. (2016) proposed a shared task for Twitter geolocation prediction, offering a benchmark dataset on the task. Hashing is an effective method to compress data for fast access and analysis. Broadly there are two types of hashing techniques: data-independent techniques which design arbitrary functions to generate hashes, and data-dependent techniques that leverage pairwise similarity in the training data (Chi and Zhu, 2017). Locality-sensitive hashing (lsh: Indyk and Motwani (1998)) is a widelyknown data-independent hashing method that uses randomised projections to generate hashcodes. It preserves data"
I17-1075,N15-1153,1,0.917785,"Missing"
I17-2012,D14-1142,0,0.0373785,"Missing"
I17-2012,P15-2085,1,0.832553,"10-fold Emotion Analysis As a first step towards working with real world data, we employ the proposed approach in an emo6 nlp.stanford.edu/projects/glove. We use the version trained on Wikipedia and Gigaword. 7 Also known as RBF kernel. 70 NLPD ↓ MAE ↓ r↑ SK 4.06 10.53 0.586 Linear SE 4.09 4.03 11.03 10.09 0.539 0.611 potentially benefit from our proposed approach. Gaussian Processes have been recently employed in a number of NLP tasks such as emotion analysis (Beck et al., 2014), detection of temporal patterns in microblogs (Preoiuc-Pietro and Cohn, 2013), rumour propagation in social media (Lukasik et al., 2015) and translation quality estimation (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2016). These previous works encode text inputs as fixed-size vectors instead of working directly on the text inputs. Among other recent work that aim at learning general structured kernels, the most similar to ours is Beck et al. (2015), who use GPs to learn tree kernels. Lei et al. (2017) unroll string kernel computations and derive equivalent neural network architectures. In contrast, our work put the learning procedure inside a GP model, inheriting the advantages of Bayesian model selection procedure"
I17-2012,Q15-1033,1,0.821639,"benefit from our proposed approach. Gaussian Processes have been recently employed in a number of NLP tasks such as emotion analysis (Beck et al., 2014), detection of temporal patterns in microblogs (Preoiuc-Pietro and Cohn, 2013), rumour propagation in social media (Lukasik et al., 2015) and translation quality estimation (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2016). These previous works encode text inputs as fixed-size vectors instead of working directly on the text inputs. Among other recent work that aim at learning general structured kernels, the most similar to ours is Beck et al. (2015), who use GPs to learn tree kernels. Lei et al. (2017) unroll string kernel computations and derive equivalent neural network architectures. In contrast, our work put the learning procedure inside a GP model, inheriting the advantages of Bayesian model selection procedures. Nevertheless, many of their kernel ideas could be applied to a GP setting, which we leave for future work. Table 1: Emotion analysis results, averaged over all emotions and cross-validation folds. λg 7.36 × 10-7 λm 0.0918 µ1 µ4 12.37 2.58 µ2 µ5 33.73 8.54 µ3 154.51 Table 2: SK hyperparameter values for a single model predic"
I17-2012,J93-2004,0,0.0584307,"Missing"
I17-2012,D14-1190,1,0.882713,"Missing"
I17-2012,K16-1021,1,0.85305,"d approach in an emo6 nlp.stanford.edu/projects/glove. We use the version trained on Wikipedia and Gigaword. 7 Also known as RBF kernel. 70 NLPD ↓ MAE ↓ r↑ SK 4.06 10.53 0.586 Linear SE 4.09 4.03 11.03 10.09 0.539 0.611 potentially benefit from our proposed approach. Gaussian Processes have been recently employed in a number of NLP tasks such as emotion analysis (Beck et al., 2014), detection of temporal patterns in microblogs (Preoiuc-Pietro and Cohn, 2013), rumour propagation in social media (Lukasik et al., 2015) and translation quality estimation (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2016). These previous works encode text inputs as fixed-size vectors instead of working directly on the text inputs. Among other recent work that aim at learning general structured kernels, the most similar to ours is Beck et al. (2015), who use GPs to learn tree kernels. Lei et al. (2017) unroll string kernel computations and derive equivalent neural network architectures. In contrast, our work put the learning procedure inside a GP model, inheriting the advantages of Bayesian model selection procedures. Nevertheless, many of their kernel ideas could be applied to a GP setting, which we leave for"
I17-2012,D14-1162,0,0.0912845,"nthetic labels. As inputs we use a random sample of sentences from the Penn Treebank (Marcus Experiments We assess our approach empirically with two sets of experiments using natural language sentences as inputs in a regression setting.5 The first one 4 Experiments were done in a machine with an Intel Xeon E5-2687W 3.10GHz as CPU and a GTX TITAN X as GPU. 5 Code to replicate the experiments in this section is available at https://github.com/beckdaniel/ ijcnlp17_sk. This also include the performance experiments in Section 2.1. 69 et al., 1993) and represent each word as a 100d GloVe embedding (Pennington et al., 2014).6 The data sampled from the procedure above is used to train another GP-SK with randomly initialised hyperparameters, which are then optimised. We run this procedure 20 times, using the same inputs but sampling new labels every time. Hyperparameter stability Figure 2 shows the hyperparameter values retrieved after optimisation, for increasing training set sizes. The decay hyperparameters are the most stable ones, being retrieved with high confidence independent of the dataset size. The original noise value is also obtained but it needs more instances (1000) for that. The n-gram coefficients a"
I17-2012,D13-1100,1,0.857817,"ame used in Section 3.1. Instead of using a fixed split, we perform 10-fold Emotion Analysis As a first step towards working with real world data, we employ the proposed approach in an emo6 nlp.stanford.edu/projects/glove. We use the version trained on Wikipedia and Gigaword. 7 Also known as RBF kernel. 70 NLPD ↓ MAE ↓ r↑ SK 4.06 10.53 0.586 Linear SE 4.09 4.03 11.03 10.09 0.539 0.611 potentially benefit from our proposed approach. Gaussian Processes have been recently employed in a number of NLP tasks such as emotion analysis (Beck et al., 2014), detection of temporal patterns in microblogs (Preoiuc-Pietro and Cohn, 2013), rumour propagation in social media (Lukasik et al., 2015) and translation quality estimation (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2016). These previous works encode text inputs as fixed-size vectors instead of working directly on the text inputs. Among other recent work that aim at learning general structured kernels, the most similar to ours is Beck et al. (2015), who use GPs to learn tree kernels. Lei et al. (2017) unroll string kernel computations and derive equivalent neural network architectures. In contrast, our work put the learning procedure inside a GP model, inhe"
I17-2012,P13-1004,1,0.835105,"ith real world data, we employ the proposed approach in an emo6 nlp.stanford.edu/projects/glove. We use the version trained on Wikipedia and Gigaword. 7 Also known as RBF kernel. 70 NLPD ↓ MAE ↓ r↑ SK 4.06 10.53 0.586 Linear SE 4.09 4.03 11.03 10.09 0.539 0.611 potentially benefit from our proposed approach. Gaussian Processes have been recently employed in a number of NLP tasks such as emotion analysis (Beck et al., 2014), detection of temporal patterns in microblogs (Preoiuc-Pietro and Cohn, 2013), rumour propagation in social media (Lukasik et al., 2015) and translation quality estimation (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2016). These previous works encode text inputs as fixed-size vectors instead of working directly on the text inputs. Among other recent work that aim at learning general structured kernels, the most similar to ours is Beck et al. (2015), who use GPs to learn tree kernels. Lei et al. (2017) unroll string kernel computations and derive equivalent neural network architectures. In contrast, our work put the learning procedure inside a GP model, inheriting the advantages of Bayesian model selection procedures. Nevertheless, many of their kernel ideas could be appli"
I17-2012,P16-1078,0,0.028968,"Missing"
I17-2012,2013.mtsummit-papers.21,0,0.0318086,"employ the proposed approach in an emo6 nlp.stanford.edu/projects/glove. We use the version trained on Wikipedia and Gigaword. 7 Also known as RBF kernel. 70 NLPD ↓ MAE ↓ r↑ SK 4.06 10.53 0.586 Linear SE 4.09 4.03 11.03 10.09 0.539 0.611 potentially benefit from our proposed approach. Gaussian Processes have been recently employed in a number of NLP tasks such as emotion analysis (Beck et al., 2014), detection of temporal patterns in microblogs (Preoiuc-Pietro and Cohn, 2013), rumour propagation in social media (Lukasik et al., 2015) and translation quality estimation (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2016). These previous works encode text inputs as fixed-size vectors instead of working directly on the text inputs. Among other recent work that aim at learning general structured kernels, the most similar to ours is Beck et al. (2015), who use GPs to learn tree kernels. Lei et al. (2017) unroll string kernel computations and derive equivalent neural network architectures. In contrast, our work put the learning procedure inside a GP model, inheriting the advantages of Bayesian model selection procedures. Nevertheless, many of their kernel ideas could be applied to a GP setting,"
I17-2012,E17-2089,0,0.0447858,"Missing"
I17-2012,P15-1150,0,0.0658763,"Missing"
I17-2012,P10-1040,0,0.0267295,"kernel algorithm and their gradients, allowing efficient hyperparameter optimisation as part of a Gaussian Process framework. Experiments on synthetic data and text regression for emotion analysis show the promise of this technique. 1 Introduction Text representations are a key component in any Natural Language Processing (NLP) task. A common approach for this is to average word vectors over a piece of text. For instance, a bag-of-words (BOW) model uses one-hot encoding as vectors and it is still a strong baseline for many tasks. More recently, approaches based on dense word representations (Turian et al., 2010; Mikolov et al., 2013) also showed to perform well. However, averaging vectors discards any word order information from the original text, which can be fundamental for more involved NLP problems. Convolutional and recurrent neural networks (CNNs/RNNs) can keep word order but still treat a text fragment as a contiguous sequence of words, encoding a bias towards short- over longdistance relations between words. Some RNN models like the celebrated LSTMs (Hochreiter 2 String Kernels Here we give a brief intuition1 on string kernels, based on the formulation proposed by Cancedda 1 We give a thorou"
I17-2012,S07-1013,0,\N,Missing
J08-4005,P06-1002,0,0.0102134,"the alignment or have an unaligned word on a boundary, respectively, indicated by a cross. where Ap and B p are the predicted and reference phrase pairs, respectively, and p the atom subscript denotes the subset of atomic phrase pairs, Aatom ⊆ Ap . As shown in Equation (3) we measure precision and recall between atomic phrase pairs and the full space of atomic and composite phrase pairs. This ensures that we do not multiply reward composite phrase pair combinations,11 while also not unduly penalizing non-matching phrase pairs which are composed of atomic phrase pairs in 11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore might multiply count phrase pairs. 604 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems Table 2 Phrase pairs are speciﬁed by the word alignments from Figure 2, using the possible alignments. The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs. ∗ This phrase pair is atomic in A but composite in B. Atomic phrase pairs they they discussed the aspects in detail in de"
J08-4005,P05-1074,1,0.755451,"marization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (ﬁghting, battle) or structural (last week’s ﬁghting, the battle last week), and are represented as words or ∗ School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ∗∗ Center for Speech and Language Processing, Johns Hopkin"
J08-4005,W03-1004,0,0.109887,"). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (ﬁghting, battle) or structural (last week’s ﬁghting, the battle last week), and are represented as words or ∗ School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ∗∗ Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218. E-mail: ccb@cs.jhu.edu. † School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission recei"
J08-4005,N03-1003,0,0.458229,"inburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for publication: 26 March 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 syntax trees—they all rely on some form of alignment for extracting paraphrase pairs. In its simplest form, the alignment can range over individual words, as is often done in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a"
J08-4005,J93-2003,0,0.00937007,"Missing"
J08-4005,N06-1003,1,0.386521,"Missing"
J08-4005,D07-1008,1,0.626972,"Missing"
J08-4005,W06-1628,0,0.0184492,"Missing"
J08-4005,W04-3216,0,0.019874,"Missing"
J08-4005,C04-1051,0,0.725955,"Missing"
J08-4005,N06-2009,0,0.0282029,"Missing"
J08-4005,J07-3002,0,0.072039,"reference alignment B can be then computed using standard recall, precision, and F1 measures (Och and Ney 2003): Precision = |AS ∩ BP | |AS | Recall = |AP ∩ BS | |BS | F1 = 2 · Precision · Recall Precision + Recall (2) where the subscripts S and P denote sure and possible word alignments, respectively. Note that both precision and recall are asymmetric in that they compare sets of possible and sure alignments. This is designed to be maximally generous: sure predictions which are present in the reference as possibles are not penalized in precision, and the converse applies for recall. We adopt Fraser and Marcu (2007)’s deﬁnition of F1, an F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufﬁciently penalize unbalanced precision and recall.9 As our corpus is monolingual, in order to avoid artiﬁcial score inﬂation, we limit the precision and recall calculations to consider only pairs of non-identical words (and phrases, as discussed subsequently). To give an example, consider the sentence pairs in Figure 2, whose alignments have been produced by the two annotators A (left) and B (rig"
J08-4005,N03-1017,0,0.0310332,"Missing"
J08-4005,W05-0809,0,0.020932,"Missing"
J08-4005,W03-0301,0,0.0235874,"or instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In the following section we expl"
J08-4005,C00-2163,0,0.194676,"ons are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In"
J08-4005,P00-1056,0,0.595666,"ons are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In"
J08-4005,J03-1002,0,0.0916633,"were split into two distinct sets, each consisting of 300 sentences (100 per corpus), and were annotated by a single coder. Each coder annotated the same amount of data. In addition, we obtained a trial set of 50 sentences from the MTC corpus which was used for familiarizing our annotators with the paraphrase alignment task (this set does not form part of the corpus). In sum, we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly annotated. To speed up the annotation process, the data sources were ﬁrst aligned automatically and then hand-corrected. We used Giza++ (Och and Ney 2003), a publicly available 2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1. 3 The corpus can be downloaded from http://www.isi.edu/∼knight/. 4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D920CD-47E3-85BC-A2F65CD28042/Details.aspx. 599 Computational Linguistics Volume 34, Number 4 implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair−1) training ings of English translations as training instances. Thi"
J08-4005,W99-0604,0,0.180416,"Missing"
J08-4005,N03-1024,0,0.192731,"Missing"
J08-4005,W04-3219,0,0.44813,"Missing"
J08-4005,N06-1057,0,0.0232382,"rd alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBur"
J08-4005,P01-1008,0,\N,Missing
J08-4005,J08-4004,0,\N,Missing
K15-1012,I13-1177,1,0.9016,"ngual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both sour"
K15-1012,P14-2133,0,0.0176224,"2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel data. While prior work has struggled to efficiently incorporate word embedding information into the parsing model (Bansal et al., 2014; Andreas and Klein, 2014; Chen et al., 2014), we present a method for doing so using a neural netCross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We sho"
K15-1012,P13-2112,1,0.9306,"ngual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both sour"
K15-1012,P14-2131,0,0.131008,"l., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel data. While prior work has struggled to efficiently incorporate word embedding information into the parsing model (Bansal et al., 2014; Andreas and Klein, 2014; Chen et al., 2014), we present a method for doing so using a neural netCross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neur"
K15-1012,D14-1096,1,0.840123,"idea of delexicalized parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using synt"
K15-1012,P13-1057,0,0.0152204,"ed parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings."
K15-1012,J92-4003,0,0.0677996,"next subsection, we review some cross-lingual word embedding methods and propose our syntactic word embeddings. Section 4 empirically compares these word embeddings when incorporated into a dependency parser. 3.1 Tu mascota Pronoun Noun The weather Det Noun parece adorable Verb Adj is horrible today Verb Adj Noun Figure 1: Examples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” re"
K15-1012,W06-2920,0,0.0231945,"e start by building syntactic word embeddings between source and target languages as shown in algorithm 1. Next we incorporate syntactic word embeddings using the algorithm proposed in Section 3.3. The third step is to substitute source- with target-language syntactic word embeddings. Finally, we parse the target language using this substituted model. In this way, the model will recognize lexical items for the target language. 4 Experiments on CoNLL Data Experiments We test our method of incorporating syntactic word embeddings into a neural network parser, for both the existing CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007) and the newlyreleased Universal Dependency Treebank (Nivre et al., 2015). We employed the Unlabeled Attachment Score (UAS) without punctuation for comparison with prior work on the CoNLL dataset. Where possible we also report Labeled Attachment Score (LAS) without punctuation. We use English as the source language for this experiment. 5 This is a consequence of only training the parser on the source language. If we were to update embeddings during parser training this would mean they no longer align with the target language embeddings. 6 117 All performance comparisons in"
K15-1012,W12-1909,1,0.714748,"9), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real setting, e.g., for Tagalog, where only scant parallel data exists (e.g., dictionaries, Wikipedia and the Bible). 113 Proceedings of the 19th Conference on Computational Language Learning, pages 113–122, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics language. Delexicalized parsing relies on the fact that parts-of-spee"
K15-1012,H05-1091,0,0.0444913,"eed parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches h"
K15-1012,P14-1006,0,0.0667528,"xamples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” representation with the number of dimensions equal to vocabulary size, words are represented using much fewer dimensions. This confers the benefit of generalising over the vocabulary to alleviate issues of data sparsity, through learning representations encoding lexical relations such as synonymy. Several approaches have sought to le"
K15-1012,D14-1082,0,0.0364157,"terising dependency relations. 115 Algorithm 1 Syntactic word embedding 1: Match the source and target tagsets to the Universal Tagset. 2: Extract word n-gram sequences for both the source and target language. 3: For each n-gram, keep the middle word, and replace the other words by their POS. 4: Train a skip-gram word embedding model on the resulting list of word and POS sequences from both the source and target language SOFT-MAX LAYER W2 HIDDEN LAYER W1 WORDS POS TAGS Eword ARC LABELS Epos Earc MAPPING LAYER CONFIGURATION (STACK, QUEUE, ARCS) Figure 2: Neural Network Parser Architecture from Chen and Manning (2014) We assume the same POS tagset is used for both the source and target language,2 and learn word embeddings for each word type in both languages into the same syntactic space of nearby POS contexts. In particular, we develop a predictive model of the tags to the left and right of a word, as illustrated in Figure 1 and outlined in Algorithm 1. Figure 1 illustrates two training contexts extracted from our English source and Spanish target language, where the highlighted fragments reflect the tags being predicted around each focus word. Note that for this example, the POS contexts for the English"
K15-1012,C14-1078,0,0.0261532,"Missing"
K15-1012,P04-1061,0,0.111529,"¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real setting, e.g., for Tagalog, where only scant parallel data exists (e.g., dictionaries, Wikipedia and the Bible). 113 Proceedings of the 19th Conference on Computational Language Learning, pages 113–122, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics language. Delexicalized parsing relies on the"
K15-1012,2005.mtsummit-papers.11,0,0.0892953,"Missing"
K15-1012,petrov-etal-2012-universal,0,0.0247689,"Missing"
K15-1012,P14-1126,0,0.13511,"r for Unsupervised Dependency Parsing Without Parallel Data Long Duong,12 Trevor Cohn,1 Steven Bird,1 and Paul Cook3 1 Department of Computing and Information Systems, University of Melbourne 2 National ICT Australia, Victoria Research Laboratory 3 Faculty of Computer Science, University of New Brunswick lduong@student.unimelb.edu.au {t.cohn,sbird}@unimelb.edu.au paul.cook@unb.ca Abstract Most recent work on unsupervised dependency parsing for low-resource languages has used the idea of delexicalized parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significan"
K15-1012,P11-2120,0,0.0252529,"Missing"
K15-1012,P05-1012,0,0.0170725,"se the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real s"
K15-1012,N12-1052,0,0.17988,"Missing"
K15-1012,H05-1066,0,0.0270963,"Missing"
K15-1012,D11-1006,0,0.194472,"r is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel dat"
K15-1012,N13-1126,0,0.143921,"Missing"
K15-1012,P10-1040,0,0.0215477,"nd propose our syntactic word embeddings. Section 4 empirically compares these word embeddings when incorporated into a dependency parser. 3.1 Tu mascota Pronoun Noun The weather Det Noun parece adorable Verb Adj is horrible today Verb Adj Noun Figure 1: Examples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” representation with the number of dimensions equal to vocabulary size, wor"
K15-1012,P12-1066,0,0.143232,"Missing"
K15-1012,N09-1028,0,0.00887976,"at generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Ge"
K15-1012,I08-3008,0,0.0208359,"ds for improving the delexicalized parser using syntactic word embeddings. Section 4 describes experiments on the CoNLL dataset and Universal Dependency Treebank. Section 5 presents methods for selecting the best source language given a target language. 2 Unsupervised Cross-lingual Dependency Parsing There are two main approaches for building dependency parsers for resource-poor languages without using target-language treebanks: delexicalized parsing and projection (Hwa et al., 2005; Ma and Xia, 2014; T¨ackstr¨om et al., 2013; McDonald et al., 2011). The delexicalized approach was proposed by Zeman et al. (2008). They built a delexicalized parser from a treebank in a resource-rich source language. This parser can be trained using any standard supervised approach, but without including any lexical features, then applied directly to parse sentences from the resource-poor 3 Improving Delexicalized Parsing We propose a novel method to improve the performance of a delexicalized cross-lingual parser without recourse to parallel data. Our method uses no additional resources and is designed to com114 plement other methods. The approach is based on syntactic word embeddings where a word is represented as a lo"
K15-1012,D13-1141,0,\N,Missing
K15-1012,D07-1096,0,\N,Missing
K16-1018,P13-2017,0,0.0188532,"Missing"
K16-1018,J93-2003,0,0.06172,"could be collected very cheaply, requiring less than 2 hours of non-expert time to tag 1,000 tokens. This constitutes a reasonable demand for cheap portability to other lowresource languages. We use the datasets from Garrette and Baldridge (2013), constituting annotated datasets of 383 sentences and 5,294 tokens in Malagasy and 196 sentences and 4,882 tokens for Kinyarwanda. We use 1,000 tokens as training set and the rest is used for testing for each language. 52k tokens. 4.1.2 POS projection We use GIZA++ to induce word alignments on the parallel data (Och and Ney, 2003), using IBM model 3 (Brown et al., 1993). Following prior work (Duong et al., 2014), we retain only one-toone alignments. Using all alignments (i.e., manyto-one and one-to-many), would result in many more POS-tagged tokens, but also bring considerable additional noise. For example, the English laws (NNS) aligned to French les (DT) lois (NNS) would end up incorrectly tagging the French determiner les as a noun (NNS). We use the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the labels to the target side. As we show in the following section, and confirmed in many studies (T¨a"
K16-1018,W06-2920,0,0.0161358,"use stochastic gradient descent model to learn the parameters. We evaluate all algorithms on the gold testing sets, evaluating in terms of tagging accuracy. Following standard practice in POS tagging, we report results using per-token accuracy (i.e., the fraction of predicted tags that exactly match the gold standard tags). Note that for all our experiments, 4.1.3 Annotated data Gold annotated data is expensive and difficult to obtain, and thus we assume that only a small annotated dataset is available. For the simulation experiments, annotated data is obtained from the CoNLL-X shared tasks (Buchholz and Marsi, 2006). To simulate the low-resource setting, we take the first 1,000 tagged tokens for training and the remaining data is split equally between development and testing sets, following Duong et al. (2014). For the real-world experiments, we use 6 183 https://github.com/clab/cnn da nl de el 1.0 VERB NOUN PRON ADJ ADV ADP CONJ DET NUM PRT X . -1.0 it pt es sv 1.0 VERB NOUN PRON ADJ ADV ADP CONJ DET NUM PRT X . VERB NOUN PRON ADJ ADV ADP CONJ DET NUM PRT X . VERB NOUN PRON ADJ ADV ADP CONJ DET NUM PRT X . VERB NOUN PRON ADJ ADV ADP CONJ DET NUM PRT X . VERB NOUN PRON ADJ ADV ADP CONJ DET NUM PRT X . -1"
K16-1018,P09-1113,0,0.00431729,"ical task for natural language processing (NLP) applications, providing lexical syntactic information. Automatic POS tagging has been extremely successful for many rich resource languages through the use of supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data. 178 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 178–186, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to project information from a resource-rich source language to a resource-poor target language. For example, parallel data has been used for named entity recognition (Wang and Manning, 2014) base"
K16-1018,P11-1061,0,0.12156,"r method for distant supervision is to use parallel data between a low-resource language and a rich-resource language. Although annotated data in low-resource languages is difficult to obtain, bilingual resources are more plentiful. For example parallel translations into English are often available, in the form of news reports, novels or the Bible. Parallel data allows annotation from a high-resource language to be projected across alignments to the low-resource language, which has been shown to be effective for several language processing tasks including POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011), named entity recognition (Wang and Manning, 2014) and dependency parsing (McDonald et al., 2013). Although cross-lingual POS projection is popular it has several problems, including errors from poor word alignments and cross-lingual syntactic divergence (T¨ackstr¨om et al., 2013; Das and Petrov, 2011). Previous work has proposed heuristics or constraints to clean the projected tag before or during learning. In contrast, we consider compensating for these problems explicitly, by learning a bias transformation to encode the mapping between ‘clean’ tags and the kinds of tags produced from proje"
K16-1018,J03-1002,0,0.0341308,"showed that a small annotated dataset could be collected very cheaply, requiring less than 2 hours of non-expert time to tag 1,000 tokens. This constitutes a reasonable demand for cheap portability to other lowresource languages. We use the datasets from Garrette and Baldridge (2013), constituting annotated datasets of 383 sentences and 5,294 tokens in Malagasy and 196 sentences and 4,882 tokens for Kinyarwanda. We use 1,000 tokens as training set and the rest is used for testing for each language. 52k tokens. 4.1.2 POS projection We use GIZA++ to induce word alignments on the parallel data (Och and Ney, 2003), using IBM model 3 (Brown et al., 1993). Following prior work (Duong et al., 2014), we retain only one-toone alignments. Using all alignments (i.e., manyto-one and one-to-many), would result in many more POS-tagged tokens, but also bring considerable additional noise. For example, the English laws (NNS) aligned to French les (DT) lois (NNS) would end up incorrectly tagging the French determiner les as a noun (NNS). We use the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the labels to the target side. As we show in the following sec"
K16-1018,D14-1096,1,0.159591,"settings, as well as two real lowresource languages, Malagasy and Kinyarwanda. 1 Introduction Part-of-speech (POS) tagging is a critical task for natural language processing (NLP) applications, providing lexical syntactic information. Automatic POS tagging has been extremely successful for many rich resource languages through the use of supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data. 178 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 178–186, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to project information from a resource-rich source language to a"
K16-1018,N13-1014,0,0.0710428,"n eight simulated lowresource settings, as well as two real lowresource languages, Malagasy and Kinyarwanda. 1 Introduction Part-of-speech (POS) tagging is a critical task for natural language processing (NLP) applications, providing lexical syntactic information. Automatic POS tagging has been extremely successful for many rich resource languages through the use of supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data. 178 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 178–186, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to project information from a resource-rich"
K16-1018,C14-1168,0,0.0309143,"viding lexical syntactic information. Automatic POS tagging has been extremely successful for many rich resource languages through the use of supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data. 178 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 178–186, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to project information from a resource-rich source language to a resource-poor target language. For example, parallel data has been used for named entity recognition (Wang and Manning, 2014) based on the observation that named entities are most often preserved"
K16-1018,P13-1057,0,0.105223,", while for the low-resource languages, we use all the parallel data. 4.2 Setup and baselines We compare our algorithm with several baselines, including the state-of-the-art algorithm from Duong et al. (2014), a two-output maxent model, their reported baseline method of a supervised maximum entropy model trained on the annotated data, and our BiLSTM POS tagger trained directly from the annotated and/or projected data (denoted BiLSTM Annotated, Projected and Ann+Proj for the model trained on the union of the two datasets). For the real low-resource languages, we also compare our algorithm with Garrette et al. (2013), who reported good results on the two low-resource languages. Our implementation is based on the cnn toolkit.6 In all cases, the BiLSTM models use 128 dimensional word embeddings and 128 dimensional hidden layers. We set the learning rate to 1.0 and use stochastic gradient descent model to learn the parameters. We evaluate all algorithms on the gold testing sets, evaluating in terms of tagging accuracy. Following standard practice in POS tagging, we report results using per-token accuracy (i.e., the fraction of predicted tags that exactly match the gold standard tags). Note that for all our e"
K16-1018,Q13-1030,0,0.0211971,"LP) applications, providing lexical syntactic information. Automatic POS tagging has been extremely successful for many rich resource languages through the use of supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data. 178 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 178–186, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to project information from a resource-rich source language to a resource-poor target language. For example, parallel data has been used for named entity recognition (Wang and Manning, 2014) based on the observation that named entities are"
K16-1018,P08-1085,0,0.0275674,"e methods to exploit parallel texts. Related Work For most natural language processing tasks, the conventional approach to developing a system is to use supervised learning algorithms trained on a set of annotated data. However, this approach is inappropriate for low-resource languages due to the lack of annotated data. An alternative approach is to harness different source of information aside from annotated text. Knowledge-bases such as dictionaries are one such source, which can be used to inform or constrain models, such as limiting the search space for POS tagging (Banko and Moore, 2004; Goldberg et al., 2008; Li et al., 2012). Parallel bilingual corpora provide another important source of information. These corpora are often plentiful even for many low-resource languages in the form of multilingual government documents, book translations, multilingual websites, etc. Word alignments can provide a bridge Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from a resource-rich language to a resourcepoor language. Duong et al. (2014) proposed an approach using a maximum entropy classifier trained on 1000 tagged tokens, and used projected tags as auxiliary ou"
K16-1018,Q13-1001,0,0.0355667,"Missing"
K16-1018,P12-1076,0,0.0236666,"l language processing (NLP) applications, providing lexical syntactic information. Automatic POS tagging has been extremely successful for many rich resource languages through the use of supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data. 178 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 178–186, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to project information from a resource-rich source language to a resource-poor target language. For example, parallel data has been used for named entity recognition (Wang and Manning, 2014) based on the observation tha"
K16-1018,P15-1119,0,0.0200866,"Missing"
K16-1018,N03-1033,0,0.0510929,"sed for testing for each language. 52k tokens. 4.1.2 POS projection We use GIZA++ to induce word alignments on the parallel data (Och and Ney, 2003), using IBM model 3 (Brown et al., 1993). Following prior work (Duong et al., 2014), we retain only one-toone alignments. Using all alignments (i.e., manyto-one and one-to-many), would result in many more POS-tagged tokens, but also bring considerable additional noise. For example, the English laws (NNS) aligned to French les (DT) lois (NNS) would end up incorrectly tagging the French determiner les as a noun (NNS). We use the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the labels to the target side. As we show in the following section, and confirmed in many studies (T¨ackstr¨om et al., 2013; Das and Petrov, 2011), the directly projected labels have many errors and therefore it is unwise to use the tags directly. We further filter the corpus using the approach of Yarowsky and Ngai (2001) which selects sentences with the highest sentence alignment scores from IBM model 3. For the European languages, we retain 200k sentences for each language, while for the low-resource languages, we use all the par"
K16-1018,W04-3229,0,0.124825,"Missing"
K16-1018,Q14-1005,0,0.159239,"el data between a low-resource language and a rich-resource language. Although annotated data in low-resource languages is difficult to obtain, bilingual resources are more plentiful. For example parallel translations into English are often available, in the form of news reports, novels or the Bible. Parallel data allows annotation from a high-resource language to be projected across alignments to the low-resource language, which has been shown to be effective for several language processing tasks including POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011), named entity recognition (Wang and Manning, 2014) and dependency parsing (McDonald et al., 2013). Although cross-lingual POS projection is popular it has several problems, including errors from poor word alignments and cross-lingual syntactic divergence (T¨ackstr¨om et al., 2013; Das and Petrov, 2011). Previous work has proposed heuristics or constraints to clean the projected tag before or during learning. In contrast, we consider compensating for these problems explicitly, by learning a bias transformation to encode the mapping between ‘clean’ tags and the kinds of tags produced from projection. We propose a new neural network model for se"
K16-1018,N01-1026,0,0.0722502,".edu.au Abstract A popular method for distant supervision is to use parallel data between a low-resource language and a rich-resource language. Although annotated data in low-resource languages is difficult to obtain, bilingual resources are more plentiful. For example parallel translations into English are often available, in the form of news reports, novels or the Bible. Parallel data allows annotation from a high-resource language to be projected across alignments to the low-resource language, which has been shown to be effective for several language processing tasks including POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011), named entity recognition (Wang and Manning, 2014) and dependency parsing (McDonald et al., 2013). Although cross-lingual POS projection is popular it has several problems, including errors from poor word alignments and cross-lingual syntactic divergence (T¨ackstr¨om et al., 2013; Das and Petrov, 2011). Previous work has proposed heuristics or constraints to clean the projected tag before or during learning. In contrast, we consider compensating for these problems explicitly, by learning a bias transformation to encode the mapping between ‘clean’ tags and the kinds of t"
K16-1018,D12-1127,0,0.0255276,"Missing"
K16-1018,D11-1006,0,0.0273081,"Missing"
K16-1018,C04-1080,0,\N,Missing
K16-1021,P13-2097,1,0.852049,"formation on the whole predictive distribution, unlike usual point estimatebased metrics. By assessing models using NLPD we can make better informed decisions about which model to employ for different settings. Furthermore, we showed how information in the predictive distribution can be used in asymmetric loss scenarios and how the proposed models can be beneficial in these settings. Uncertainty estimates can be useful in many other settings beyond the ones explored in this work. Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE (Beck et al., 2013). Exploratory analysis is another avenue for future work, where error bars can provide further insights about the task, as shown in recent work (Nguyen and O’Connor, 2015). This kind of analysis can be useful for tracking post-editor behaviour and assessing cost estimates for translation projects, for instance. Our main goal in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by pro"
K16-1021,P13-1004,1,0.956005,"ng decision. For instance, in order to ensure good user experience for the human translator and maximise translation productivity, an MT segment could be forwarded for post-editing only if a QE model assigns a high quality score with low uncertainty (high confidence). Such a decision process is not possible with point estimates only. Good uncertainty estimates can be acquired from well-calibrated probability distributions over the quality predictions. In QE, arguably the most successful probabilistic models are Gaussian Processes (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task. We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions. Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by Qui˜nonero-Candela et al. (2006), which defined new evaluation metrics that take into account probability distributions over predictions. The remaining of this paper is organised as follows: Machine Translation Quality Estimation is a notoriously difficult task, which lessens its us"
K16-1021,D14-1190,1,0.928294,"Missing"
K16-1021,P15-1174,0,0.0195838,"r I allow more complex mappings to be learned but raise the risk of overfitting. Warped GPs provide an easy and elegant way to model response variables with non-Gaussian behaviour within the GP framework. In our experiments we explore models employing warping functions with up to 3 terms, which is the value recommended by Snelson et al. (2004). We also report results using the f (y) = log(y) warping function. 3 data. We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson’s r, which has recently proposed by Graham (2015) as a more robust alternative. 3.1 Our experiments comprise datasets containing three different language pairs, where the label to predict is post-editing time: English-Spanish (en-es) This dataset was used in the WMT14 QE shared task (Bojar et al., 2014). It contains 858 sentences translated by one MT system and post-edited by a professional translator. Intrinsic Uncertainty Evaluation French-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator. Given a set of different probabilistic QE mod"
K16-1021,W14-3338,1,0.891871,"redictions for this task, they lack a probabilistic interpretation, which makes it hard to extract uncertainty estimates using them. Bootstrapping approaches like bagging (Abe and Mamitsuka, 1998) can be applied, but this requires setting and optimising hyperparameters like bag size and number of bootstraps. There is also no guarantee these estimates come from a well-calibrated probabilistic distribution. Gaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b). Unlike SVR, they explicitly model uncertainty in the data and in the predictions. This makes GPs very applicable when p(f |X ) = p(y|X, f )p(f ) , p(y|X) where X and y are the training inputs and response variables, respectively. For regression, we assume that each yi = f (xi ) + η, where η ∼ N (0, σn2 ) is added white noise. Having a Gaussian likelihood results in a closed form solution for the posterior. Training a GP involves the optimisation of model hyperparameters, which is done by maximising the marginal likelihood p(y|X) via gradient ascent. Predictive posteriors for unseen x∗ are"
K16-1021,P15-2030,1,0.837702,"to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future. Related Work Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models. The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like L´azaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to evaluate recent work on"
K16-1021,C04-1046,0,0.368786,"Missing"
K16-1021,N10-1038,0,0.0347264,"al in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future. Related Work Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models. The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like L´azaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to"
K16-1021,2012.amta-wptp.2,1,0.832763,"n sentence basis for all datasets. Following common practice, we normalise the post-editing time by the length of the machine translated sentence to obtain postediting rates and use these as our response variables. Technically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006). Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012). Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures. For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT n NLPD(ˆ y, y) = − Experimental Settings 1X log p(ˆ yi = yi |xi ). n i=1 where y ˆ is a set of test predictions, y is the set of true labels and n is the test set size. This metric has since been largely adopted by the ML community when evalua"
K16-1021,D12-1054,0,0.0317415,"g simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future. Related Work Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models. The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like L´azaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to evaluate recent work on uncertainty propagation methods for neural networks (Hern´andez-Lobato and Adams, 2015). Asymmetric loss functions are common in the econometrics literatu"
K16-1021,D15-1182,0,0.0681594,"Missing"
K16-1021,W12-3102,1,0.908405,"Missing"
K16-1021,2013.mtsummit-papers.21,1,0.907208,"erate competitive predictions for this task, they lack a probabilistic interpretation, which makes it hard to extract uncertainty estimates using them. Bootstrapping approaches like bagging (Abe and Mamitsuka, 1998) can be applied, but this requires setting and optimising hyperparameters like bag size and number of bootstraps. There is also no guarantee these estimates come from a well-calibrated probabilistic distribution. Gaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b). Unlike SVR, they explicitly model uncertainty in the data and in the predictions. This makes GPs very applicable when p(f |X ) = p(y|X, f )p(f ) , p(y|X) where X and y are the training inputs and response variables, respectively. For regression, we assume that each yi = f (xi ) + η, where η ∼ N (0, σn2 ) is added white noise. Having a Gaussian likelihood results in a closed form solution for the posterior. Training a GP involves the optimisation of model hyperparameters, which is done by maximising the marginal likelihood p(y|X) via gradient ascent. Predictive posteriors"
K16-1021,2006.amta-papers.25,0,0.0632672,". It was translated by one MT system for consistency we use a subset of 2, 828 instances post-edited by a single professional translator. As part of the process of creating these datasets, post-editing time was logged on an sentence basis for all datasets. Following common practice, we normalise the post-editing time by the length of the machine translated sentence to obtain postediting rates and use these as our response variables. Technically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006). Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012). Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures. For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT n NLPD(ˆ y, y) = − E"
K16-1021,2009.eamt-1.5,1,0.909327,"Missing"
K16-1021,P15-4020,1,0.846462,"ty labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006). Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012). Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures. For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT n NLPD(ˆ y, y) = − Experimental Settings 1X log p(ˆ yi = yi |xi ). n i=1 where y ˆ is a set of test predictions, y is the set of true labels and n is the test set size. This metric has since been largely adopted by the ML community when evaluating GPs and other probabilistic models for regression (see Section 5 for some examples). As with other error metrics, lower values are better. Intuitively, if two models produce equally incorrect predictions but they have different uncertainty estimates, NLPD will penalise the ov"
K16-1021,2011.eamt-1.12,1,0.847639,"point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson’s r, which has recently proposed by Graham (2015) as a more robust alternative. 3.1 Our experiments comprise datasets containing three different language pairs, where the label to predict is post-editing time: English-Spanish (en-es) This dataset was used in the WMT14 QE shared task (Bojar et al., 2014). It contains 858 sentences translated by one MT system and post-edited by a professional translator. Intrinsic Uncertainty Evaluation French-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator. Given a set of different probabilistic QE models, we are interested in evaluating the performance of these models, while also taking their uncertainty into account, particularly to distinguish among models with seemingly same or similar performance. A straightforward way to measure the performance of a probabilistic model is to inspect its negative (log) marginal likelihood. This measure, however, does not capture if a model overfit the training data. We can have a better generali"
L16-1694,P13-1035,0,0.0448237,"Missing"
L16-1694,J90-1003,0,0.389551,"tion scores for each word pair in separate time intervals by splitting our dataset based on the timestamp of the texts. PMI is an information theoretic measure that indicates which words tend to often co-occur in a context. It measures the relative difference between observed word cooccurrences, and their expected co-occurrence assuming independence, PMI(X, Y ) = α · log P (x, y) , P (x) · P (y) (1) where α is a normalisation factor, here set to α = − log P (x, y) following (Bouma, 2009) to address issues with interpretability and sensitivity to low-count events in regular PMI (where α = 1). (Church and Hanks, 1990). This normalised variant of PMI (NPMI) is bounded in the [−1, 1] interval and can be easily interpreted: word pairs with a negative NPMI co-occur less often than expected under independence, a positive NPMI means more often, and 0 denotes equality. The maximum value NPMI=1 implies that both words exclusively appear together. We use Word1 arrests publish bestfriends g-slate activist china’s blake magazines activist actors cameras angeles Word2 yemen trailers forming spotted arrests stealth griffin merchandise yemen showcase g-slate los NPMI 0.699 0.678 0.678 0.675 0.674 0.674 0.672 0.669 0.667"
L16-1694,J93-1003,0,0.449795,"automatically select relevant messages that can be used as labels when presenting the clusters to end users. Our results on event detection tasks using tweets, show that our method rivals state-of-the-art message based event detection techniques. Our method is especially useful for downstream applications where higher recall is desirable, such as time dependent information retrieval. The data from this study is freely available.1 2. Related Work The study of word co-occurrences has a long tradition in Natural Language Processing. Measures of co-occurrence have been studied by Fano (1961) and Dunning (1993). In NLP, they have been used for finding collocations or multiword expressions in documents (Sag et al., 2002; Evert, 2005), for weighting vectors for measuring distributional semantic similarity (Turney and Pantel, 2010) or for finding the sentiment polarity of words (Turney, 2002). More related to this study, Newman et al. (2010) shows that the best performance for measuring topic coherence is obtained using the Pointwise Mutual Information co-occurrence metric. Spectral clustering is a state-of-the-art clustering method that has been used for various tasks like image segmentation (Shi and"
L16-1694,D08-1038,0,0.042352,"e is obtained using the Pointwise Mutual Information co-occurrence metric. Spectral clustering is a state-of-the-art clustering method that has been used for various tasks like image segmentation (Shi and Malik, 2000) or detecting communities in networks (Newman, 2006). The application of spectral clustering methods in NLP has been limited because of increased storage space and runtime when faced with large-scale text datasets (Lin and Cohen, 2010). 1 http://www.sas.upenn.edu/˜danielpr/ clusters.html 4380 Cluster analysis and modelling events over time have been studied in different contexts. Hall et al. (2008) studies the evolution and trends of topics by using topic modeling and matching the topics obtained independently at different time intervals. Wang and McCallum (2006) develop a topic model that explicitly embeds time as an observed variable. Several other approaches have integrated time into a probabilistic graphical model of text (Al Sumait et al., 2008; Gohr et al., 2009; Wang et al., 2008). These papers used datasets of long and well structured documents on a restricted set of topics (e.g. conference proceedings or political addresses). In social media, event detection represents an extre"
L16-1694,N13-1090,0,0.0142806,"Missing"
L16-1694,N10-1012,0,0.0133696,"is desirable, such as time dependent information retrieval. The data from this study is freely available.1 2. Related Work The study of word co-occurrences has a long tradition in Natural Language Processing. Measures of co-occurrence have been studied by Fano (1961) and Dunning (1993). In NLP, they have been used for finding collocations or multiword expressions in documents (Sag et al., 2002; Evert, 2005), for weighting vectors for measuring distributional semantic similarity (Turney and Pantel, 2010) or for finding the sentiment polarity of words (Turney, 2002). More related to this study, Newman et al. (2010) shows that the best performance for measuring topic coherence is obtained using the Pointwise Mutual Information co-occurrence metric. Spectral clustering is a state-of-the-art clustering method that has been used for various tasks like image segmentation (Shi and Malik, 2000) or detecting communities in networks (Newman, 2006). The application of spectral clustering methods in NLP has been limited because of increased storage space and runtime when faced with large-scale text datasets (Lin and Cohen, 2010). 1 http://www.sas.upenn.edu/˜danielpr/ clusters.html 4380 Cluster analysis and modelli"
L16-1694,D14-1162,0,0.0746894,"Missing"
L16-1694,N10-1021,0,0.0556854,"Missing"
L16-1694,N12-1034,0,0.0354,"Missing"
L16-1694,P02-1053,0,0.0243471,"that these groups identify key real world events as they occur in time, despite no explicit supervision. The performance of our method rivals state-of-the-art methods for event detection on F-score, obtaining higher recall at the expense of precision. Keywords: Topic Detection & Tracking, Information Extraction, Information Retrieval, Text Mining 1. Introduction Algorithms based on word co-occurrences have a long tradition in NLP and have been used successfully for applications ranging from sentiment analysis to thesaurus learning, collocation extraction and discovering multiword expressions (Turney, 2002; Curran, 2004; Sag et al., 2002; Evert, 2005). However, in most cases, these scores have been computed using underlying static corpora and ignoring any temporal variation. In this paper we study the changing behaviour of word co-occurrences over time using social media data. We hypothesise that co-occurrences between words will change over time as a response to real world events. Increased social media usage enables us to extract for analysis large scale streaming data, previously largely unavailable to researchers. Data arising from these sources, specifically Twitter, has been shown to refl"
L18-1530,L16-1632,1,0.828001,"because they constitute phonological units (syllable rhymes; Na syllables are composed of an onset, a rhyme, and a tone). Concerning improvements to the original transcriptions, we addressed cases where the same phoneme had inconsistent representation in the corpus, such as /wæ / and /w æ/, as well as an instance where the unicode representation of a single phoneme was sometimes v+nasality+syllabic diacritic and sometimes v+syllabic diacritic+nasality. We computed the Na results of Tables 1-3 using the larger suite of 224 minutes and these preprocessing changes. For Chatino, we used data of Ćavar et al. (2016) from the GORILLA language archive for Eastern Chatino of San Juan Quiahije, Oaxaca, Mexico (Cavar et al., 2016) for the purposes of comparing phoneme and tone prediction with Na when data restriction is in place. We used up to 50 minutes of data for training, 6 minutes for validation and 6 minutes for testing. The phoneme inventory we used consists of 31 labels along with 14 tone labels. For both languages, preprocessing involved removing punctuation and any other symbols that are not phonemes, tones or the tone 3358 Chatino Na Input Output PER ↓ TER ↓ PER ↓ TER ↓ TGB-F1 ↑ fbank fbank+pitch f"
L18-1530,C16-1328,0,0.0709237,"nemic transcription; French, English and Chinese translations; target label sequences: (1) phonemes only, (2) tones only, (3) phonemes and tones together, and (4) phonemes and tones with tone group boundary markers, “|”. labels are collapsed. The use of an underlying recurrent neural network allows the model to implicitly model context via the parameters of the LSTM, despite the independent frame-wise label predictions of the CTC network. It is this feature of the architecture that makes it a promising tool for tonal prediction, since tonal information is suprasegmental, spanning many frames (Mortensen et al., 2016). Context beyond the immediate local signal is indispensable for tonal prediction, and longranging context is especially important in the case of morphotonologically rich languages such as Na and Chatino. Past work distinguishes between embedded tonal modelling, where phoneme and tone labels are jointly predicted, and explicit tonal modelling, where they are predicted separately (Lee et al., 2002). We compare several training objectives for the purposes of phoneme and tone prediction. This includes separate prediction of 1. phonemes and 2. tones, as well as 3. jointly predict phonemes and tone"
N09-1062,E03-1005,0,0.326439,", not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and efficient manner. The net result is a grammar"
N09-1062,P05-1022,0,0.0061087,"ing latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local c"
N09-1062,A00-2018,0,0.0299597,"rammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can b"
N09-1062,C02-1126,0,0.04097,"component rules, and the probability of a tree is the sum of the probabilities of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2 A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim"
N09-1062,P07-1035,0,0.0165933,"Missing"
N09-1062,P06-1085,1,0.0765468,"bution in more detail below. Rather than representing the distribution Gc explicitly, we integrate over all possible values of Gc . The resulting distribution over ei , conditioned on e&lt;i = e1 . . . ei−1 and the root category c is: (1) Since the sequence of elementary trees can be split into derivations, each of which completely specifies a tree, P (t|e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the work in our model is done by the prior distribution over elementary trees. Note that this is analogous to the Bayesian model of word segmentation presented by Goldwater et al. (2006); indeed, the problem of inferring e from t can be viewed as a segmentation problem, where each full tree must be segmented into one or more elementary trees. As in Goldwater et al. (2006), we wish to favour solutions employing a relatively small number of elementary units (here, elementary trees). This can be done using a Dirichlet process (DP) prior. Specifically, we define the distribution of elementary tree e with root non-terminal symbol c as Gc |αc , P0 ∼ DP(αc , P0 (·|c)) e|c ∼ Gc where P0 (·|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, an"
N09-1062,N07-1018,1,0.524373,"required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to learning subcategories with larger membership, such as the terminals for days of the week and noun-adjective agreement. The approaches are orthogonal, and we expect that combining a category refinement model with our TSG model would provide better performance than either approach alone. Our model is similar to the Adaptor Grammar model of Johnson et al. (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar. However, Adaptor Grammars require that each sub-tree expands completely, with only terminal symbols as leaves, while our own model permits non-terminal frontier nodes. In addition, they disallow recursive containment of adapted non-terminals; we impose no such constraint. 3 Model Recall the nature of our task: we are given a corpus of parse trees t and wish to infer a tree-substitution grammar G that we can use to parse new data. Rather than inferring a grammar directly, we go through an intermediate step of inferrin"
N09-1062,J98-4004,0,0.13043,"ures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local context, such as"
N09-1062,J02-1005,0,0.622903,"as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and ef"
N09-1062,D07-1072,0,0.0361547,"operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to"
N09-1062,N07-1051,0,0.0121277,"e 3: TSG used to generate synthetic data. All production probabilities are uniform. found that using the CYK algorithm (Cocke, 1969) to find the Viterbi derivation for p˜ yielded consistently better results. This algorithm maximises an approximated model, as opposed to approximately optimising the true model. We also present results using the tree with the maximum expected count of CFG rules (MER). This uses counts of the CFG rules applied at each span (compiled from the derivation samples) followed by a maximisation step to find the best tree. This is similar to the MAX-RULE-SUM algorithm of Petrov and Klein (2007) and maximum expected recall parsing (Goodman, 2003). 6 Experiments Synthetic data Before applying the model to natural language, we first create a synthetic problem to confirm that the model is capable of recovering a known tree-substitution grammar. We created 50 random trees from the TSG shown in Figure 3. This produces binary trees with A and B internal nodes and ‘a’ and ‘b’ as terminals, such that the terminals correspond to their grand-parent non-terminal (A and a or B and b). These trees cannot be modelled accurately with a CFG because expanding A and B nodes into terminal strings requi"
N09-1062,P06-1055,0,0.780747,"thout the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs"
N09-1062,D07-1058,0,0.406805,"moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2 A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be ex"
N09-1062,J03-4003,0,\N,Missing
N10-1028,P09-1088,1,0.900868,"ack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs allows much faster mixing, but done in the"
N10-1028,W07-0403,0,0.522712,"ion system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs"
N10-1028,D08-1033,0,0.14161,"Missing"
N10-1028,N07-1018,0,0.651894,"s are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs allows much faster mixing, but done in the obvious way (following Johnson et al. (2007)) would incur a O(|f |3 |e|3 ) time complexity. Here we draw inspiration from the work of Van Gael et al. (2008) on inference in infinite hidden Markov models to develop a novel algorithm for efficient sampling from a SCFG. We develop an auxiliary variable ‘slice’ sampler which can dramatically reduce inference complexity, and thereby make blocked sampling practicable on real translation corpora. Our evaluation demonstrates that our algorithm mixes more quickly than the local Gibbs sampler, and produces translation models which achieve state-ofthe-art B LEU scores without using GIZA++ or symme"
N10-1028,N03-1017,0,0.259153,"question answering) will rest on the quality of approximate inference. In this work we tackle this problem in the context of inducing synchronous grammars for a machine translation system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2"
N10-1028,P07-2045,0,0.013919,"LB structure achieves a higher likelihood than the M4 initialised model, although this is not reflected in their relative B LEU scores. In contrast the Gibbs sampler is more significantly affected by its initialisation, only deviating slightly before becoming trapped in a mode, as seen in Fig. 1. With sufficient (infinite) time both sampling strategies will converge on the true posterior regardless of initialisation, however the slice sampler appears to be converging much faster than the Gibbs sampler. Interestingly, the initialisation heuristics (M1 and M4) outperform the default heuristics (Koehn et al., 2007) by a considerable margin. This is most likely because the initialisation heuristics force the alignments to factorise with an ITG, resulting in more aggressive pruning of spurious alignments which in turn allows for more and larger phrase pairs. In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments fro"
N10-1028,W02-1018,0,0.0924475,"for a machine translation system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampli"
N10-1028,2001.mtsummit-papers.68,0,0.00996132,"ause the initialisation heuristics force the alignments to factorise with an ITG, resulting in more aggressive pruning of spurious alignments which in turn allows for more and larger phrase pairs. In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments from the final sample, and are evaluated using B LEU(Papineni et al., 2001). The slice sampled models are restricted 9 The following beam heuristics are employed: alignments to to learning binary branching one-to-one (or null) null are only permitted on the longer sentence side; words are alignments,8 while no restriction is placed on the only allowed to align to those whose relative sentence position ±3 words. Gibbs sampler (both use the same model, so have is within 10 Words of the longer sentence are randomly assigned to null. 11 Moreover, we only sample values for us as they are visited We limit the maximum training sentence length to 40, resultby the parser, thu"
N10-1028,J97-3002,0,0.861041,"6 Table 1: IWSLT Chinese to English translation. of theQ second product. The last step (4) discards the term us β(us ; a, b) which is constant wrt d. The net result is a formulation which factors with the derivation structure, thereby eliminating the need to consider all O(|e|2 |f |2 ) spans in S. Critically p(d|u) is zero for all spans failing the I (us &lt; θrs ) condition. To exploit the decomposition of Equation 4 we require a parsing algorithm that only explores chart cells whose child cells have not already been pruned by the slice variables. The standard approach of using synchronous CYK (Wu, 1997) doesn’t posses this property: all chart cells would be visited even if they are to be pruned. Instead we use an agenda based parsing algorithm, in particular we extend the algorithm of Klein and Manning (2004) to synchronous parsing.6 Finally, we need a Metropolis-Hastings acceptance step to account for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation comparable LLH). Of particular interest is how the different samplers perform given initialisations"
N10-1028,P02-1040,0,\N,Missing
N12-1040,adolphs-2008-acquiring,0,0.0145551,"morphotactics causes, amongst other phenomena, the final consonant of a morpheme to assimilate the manner of the initial consonant of the following morpheme (as in -villi), or to be dropped (as in natsiviniq-). Consequently, morphemes are not readily accessible from the realised surface form, thereby motivating the use of a morphological analyser. 2.2 Morphological analysis For many languages with a less rich morphology than Inuktitut, an inflectional lexicon is often adequate for morphological analysis (for example, CELEX for English (Burnage, 1990), Lefff for French (Sagot et al., 2006) or Adolphs (2008) for German). Another typical approach is to perform morphological analysis at the same time as POS tagging (as in Hajiˇc and Hladk´a (1998) for the fusional morphology in Czech), as it is often the case that 372 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 372–376, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics determining the part-of-speech and choosing the appropriate inflectional paradigm are closely linked. For highly inflecting languages more generally, morphological"
N12-1040,W02-0603,0,0.0515774,"for the fusional morphology in Czech), as it is often the case that 372 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 372–376, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics determining the part-of-speech and choosing the appropriate inflectional paradigm are closely linked. For highly inflecting languages more generally, morphological analysis is often treated as a segmentand-normalise problem, amenable to analysis by weighted finite state transducer (wFST), for example, Creutz and Lagus (2002) for Finnish. 3 Resources 3.1 A morphological analyser for Inuktitut The main resource that we are evaluating in this work is a morphological analyser of Inuktitut called Uqa·Ila·Ut.1 It is a rule-based system based on regular morphological variations of about 3200 head, 350 lexical, and 1500 grammatical morphemes, with heuristics for ranking the various readings. The head and lexical morphemes are collated with glosses in both English and French. 3.2 Word alignment The training corpus we use in our experiments is a sentence-aligned segment of the Nunavut Hansards (Martin et al., 2003). The co"
N12-1040,P07-1003,0,0.0273032,"alignments, 1 http://inuktitutcomputing.ca/Uqailaut/ en/IMA.html 373 and 1679 probable), which we use to evaluate word alignments. Our treatment of the alignment problem is most similar to Schafer and Dr´abek (2005) who examine four systems: GIZA++ models (Och and Ney, 2000) for each source-target direction, another where the Inuktitut input has been syllabised, and a wFST model. They observe that aggregating these results through voting can create a very competitive system for Inuktitut word alignment. 4 Experimental approach We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. The default implementation of the system involves two jointlytrained HMMs (one for each source-target direction) over five iterations,2 with so-called competitive thresholding in the decoding step; these are more fully described in DeNero and Klein (2007) and Liang et al. (2006). Our approach examines morphological preprocessing of the Inuktitut training and test sets, with the idea of leveraging the morphological information into a corpus which is more"
N12-1040,P98-1080,0,0.0426799,"Missing"
N12-1040,W05-0810,0,0.0198757,"ich is more in line with the particularities of the task. 1 (1) natsiq- -viniq- -tuq- -lauq- -simaseal meat eat before ever -vit -li INT-2s but “But have you ever eaten seal meat before?” Introduction In this work, we evaluate a morphological analyser of Inuktitut, whose polysynthetic morphosyntax can cause particular problems for natural language processing; but our observations are also relevant to other languages with rich morphological systems. The existing NLP task for Inuktitut is that of word alignment (Martin et al., 2005), where Inuktitut tokens align to entire English clauses. While Langlais et al. (2005) theorises that a morphological analyser could aid in this task, we observed little to no improvement over a baseline model by making use of its segmentation. Nonetheless, morphological analysis does provide a great deal of information, but the task structure tends to disprefer its contribution. 2 2.1 Background Inuktitut Inuktitut is a macrolanguage of many more-or-less mutually intelligible dialects (Gordon, 2005). The Lowe (1996) analyses the morphology as a fourplace relationship: one head morpheme, zero or more lexical morphemes, one or more grammatical morphemes, and an optional enclitic"
N12-1040,N06-1014,0,0.0426572,"s through voting can create a very competitive system for Inuktitut word alignment. 4 Experimental approach We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. The default implementation of the system involves two jointlytrained HMMs (one for each source-target direction) over five iterations,2 with so-called competitive thresholding in the decoding step; these are more fully described in DeNero and Klein (2007) and Liang et al. (2006). Our approach examines morphological preprocessing of the Inuktitut training and test sets, with the idea of leveraging the morphological information into a corpus which is more amenable to alignment. The raw corpus appears to be undersegmented, where data sparseness from the many singletons would prevent reliable alignments. Segmentation might aid in this process by making sublexical units with semantic overlap transparent to the alignment system, so that types appear to have a greater frequency through the data. Through this, we attempt to examine the hypothesis that one-toone alignments be"
N12-1040,W03-0320,0,0.2207,"Missing"
N12-1040,W05-0809,0,0.126928,"that the richer approaches provide little as compared to simply finding the head, which is more in line with the particularities of the task. 1 (1) natsiq- -viniq- -tuq- -lauq- -simaseal meat eat before ever -vit -li INT-2s but “But have you ever eaten seal meat before?” Introduction In this work, we evaluate a morphological analyser of Inuktitut, whose polysynthetic morphosyntax can cause particular problems for natural language processing; but our observations are also relevant to other languages with rich morphological systems. The existing NLP task for Inuktitut is that of word alignment (Martin et al., 2005), where Inuktitut tokens align to entire English clauses. While Langlais et al. (2005) theorises that a morphological analyser could aid in this task, we observed little to no improvement over a baseline model by making use of its segmentation. Nonetheless, morphological analysis does provide a great deal of information, but the task structure tends to disprefer its contribution. 2 2.1 Background Inuktitut Inuktitut is a macrolanguage of many more-or-less mutually intelligible dialects (Gordon, 2005). The Lowe (1996) analyses the morphology as a fourplace relationship: one head morpheme, zero"
N12-1040,P00-1056,0,0.102354,"nments, where phrasal alignments of one token in both the source and target were (generally) called sure alignments, and one-to-many or many-to-many mappings were extended to their cartesian product, and called probable. The test set was composed of 75 of these sentences (about 2K English tokens, 800 Inuktitut tokens, 293 gold-standard sure alignments, 1 http://inuktitutcomputing.ca/Uqailaut/ en/IMA.html 373 and 1679 probable), which we use to evaluate word alignments. Our treatment of the alignment problem is most similar to Schafer and Dr´abek (2005) who examine four systems: GIZA++ models (Och and Ney, 2000) for each source-target direction, another where the Inuktitut input has been syllabised, and a wFST model. They observe that aggregating these results through voting can create a very competitive system for Inuktitut word alignment. 4 Experimental approach We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. The default implementation of the system involves two jointlytrained HMMs (one for each source-target directio"
N12-1040,W05-0811,0,0.0423458,"Missing"
N12-1040,C98-1077,0,\N,Missing
N12-1040,sagot-etal-2006-lefff,0,\N,Missing
N15-1153,D10-1124,0,0.666289,"Missing"
N15-1153,C12-1064,1,0.82669,"ber of users with reliable location information, there has been significant interest in the task 2 Related Work Past work on user geolocation falls broadly into two categories: text-based and network-based methods. Common to both methods is the manner of framing the geolocation prediction problem. Geographic coordinates are real-valued, and accordingly this is most naturally modelled as (multiple) regression. However for modelling convenience, the problem is typically simplified to classification by first prepartitioning the regions into discrete sub-regions using either known city locations (Han et al., 2012; Rout et al., 2013) or a k-d tree partitioning (Roller et al., 2012; Wing and Baldridge, 2014). In the k-d tree methods, the resulting discrete regions are treated either as a flat list (as we do here) or a nested hierarchy. 1362 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1362–1367, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2.1 Text-based Geolocation Text-based approaches assume that language in social media is geographically biased, which is clearly evident for regions speaking differ"
N15-1153,D12-1137,0,0.819755,"ignificant interest in the task 2 Related Work Past work on user geolocation falls broadly into two categories: text-based and network-based methods. Common to both methods is the manner of framing the geolocation prediction problem. Geographic coordinates are real-valued, and accordingly this is most naturally modelled as (multiple) regression. However for modelling convenience, the problem is typically simplified to classification by first prepartitioning the regions into discrete sub-regions using either known city locations (Han et al., 2012; Rout et al., 2013) or a k-d tree partitioning (Roller et al., 2012; Wing and Baldridge, 2014). In the k-d tree methods, the resulting discrete regions are treated either as a flat list (as we do here) or a nested hierarchy. 1362 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1362–1367, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2.1 Text-based Geolocation Text-based approaches assume that language in social media is geographically biased, which is clearly evident for regions speaking different languages (Han et al., 2014), but is also reflected in regional"
N15-1153,D14-1039,0,0.505918,"n the task 2 Related Work Past work on user geolocation falls broadly into two categories: text-based and network-based methods. Common to both methods is the manner of framing the geolocation prediction problem. Geographic coordinates are real-valued, and accordingly this is most naturally modelled as (multiple) regression. However for modelling convenience, the problem is typically simplified to classification by first prepartitioning the regions into discrete sub-regions using either known city locations (Han et al., 2012; Rout et al., 2013) or a k-d tree partitioning (Roller et al., 2012; Wing and Baldridge, 2014). In the k-d tree methods, the resulting discrete regions are treated either as a flat list (as we do here) or a nested hierarchy. 1362 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1362–1367, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2.1 Text-based Geolocation Text-based approaches assume that language in social media is geographically biased, which is clearly evident for regions speaking different languages (Han et al., 2014), but is also reflected in regional dialects and the use of reg"
N16-1102,W11-1218,0,0.0140755,"s of parameters with vast potential for over-fitting. Table 1 shows the statistics of the training sets.6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. We used ‘devset1 2’ and ‘devset 3’ as the development and test sets, respectively, and in both cases used only the first reference for evaluation. For Romanian and Estonian, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.7 The RussianEnglish data was taken from a web derived corpus (Antonova and Misyurev, 2011). The dataset is split into three parts using the same technique as for the Europarl sets. During the preprocessing stage we lower-cased and tokenized the data, and excluded sentences longer than 30 words. For the Europarl 5 As the alignment cells are normalised using the softmax and thus take values in [0,1], the trace term is bounded above by min(I, J) which occurs when the two alignment matrices are transposes of each other, representing perfect one-to-one alignments in both directions 880 We could share some parameters, e.g., the word embedding matrices, however we found this didn’t make m"
N16-1102,J93-2003,0,0.177161,"er-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus on the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend to different parts of the source as it generates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993). In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged ("
N16-1102,N13-1073,1,0.930705,"to different parts of the source as it generates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993). In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above"
N16-1102,P08-1112,0,0.0515496,"Missing"
N16-1102,W11-2123,0,0.0299588,"els and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora. configuration Sutskever encdec Attentional +align +align+glofer +align+glofer-pre +align+sym +align+sym+glofer-pre perplexity data, we also removed sentences containing headings and other meeting formalities.8 0 2 4 6 8 epochs Figure 3: Perplexity with training epochs on ro-en translation, comparing several model variants. embedding, 512 hidden, and 256 alignment dimensions. For each model, we also report the number of its parameters. Models are trained end-to-end using stochastic"
N16-1102,D13-1176,0,0.721739,"ation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting. 1 Introduction Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). This is despite the neural approaches using an overall simpler model, with fewer assumptions about the learning and prediction problem. Broadly, neural approaches are based around the notion of an encoder-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus on the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend"
N16-1102,N03-1017,0,0.101355,"ranslate as several words. Compared to the fertility model in IBM 3–5 (Brown et al., 1993), ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions. 3.4 Bilingual Symmetry So far we have considered a conditional model of the target given the source, modelling p(t|s). However it is well established for latent variable translation models that the alignments improve if p(s|t) is 3 Modern decoders (Koehn et al., 2003) often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility within phrases. 4 The normal distribution is deficient, as it has support for all scalar values, despite fi being bounded above and below (0 ≤ fi ≤ J). This could be corrected by using a truncated normal, or various other choices of distribution. lang-pair Zh-En Ru-En Et-En Ro-En # tokens (K) 422 454 1639 1809 1411 1857 1782 1806 # types (K) 3.44 3.12 145 65 90 25 39 24 Table 1: Statistics of the trai"
N16-1102,2005.iwslt-1.8,0,0.0597971,"truncated normal, or various other choices of distribution. lang-pair Zh-En Ru-En Et-En Ro-En # tokens (K) 422 454 1639 1809 1411 1857 1782 1806 # types (K) 3.44 3.12 145 65 90 25 39 24 Table 1: Statistics of the training sets, showing in each cell the count for the source language (left) and target language (right). Figure 2: Symmetric training with trace bonus, computed as matrix multiplication, − tr(αs←t αs→t > ). Dark shading indicates higher values. also modelled and the inferences of both directional models are combined – evidenced by the symmetrisation heuristics used in most decoders (Koehn et al., 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008). The rationale is that both models make somewhat independent errors, so an ensemble stands to gain from variance reduction. We propose a method for joint training of two directional models as pictured in Figure 2. Training twinned models involves optimising L = − log p(t|s) − log p(s|t) + γB where, as before, we consider only a single sentence pair, for simplicity of notation. This corresponds to a pseudo-likelihood objective, with the B linking the two models.5 The B component considers the a"
N16-1102,P07-2045,1,0.0319222,"rained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora. configuration Sutskever encdec Attentional +align +align+glofer +align+glofer-pre +align+sym +align+sym+glofer-pre perplexity data, we also removed sentences containing headings and other meeting formalities.8 0 2 4 6 8 epochs Figure 3: Perplexity with training epochs on ro-en translation, comp"
N16-1102,W04-3250,0,0.0357109,"15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features i"
N16-1102,2005.mtsummit-papers.11,0,0.140774,". This serves to demonstrate the robustness and generalisation of our model on sparse data – something that has not yet been established for neural models with millions of parameters with vast potential for over-fitting. Table 1 shows the statistics of the training sets.6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. We used ‘devset1 2’ and ‘devset 3’ as the development and test sets, respectively, and in both cases used only the first reference for evaluation. For Romanian and Estonian, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.7 The RussianEnglish data was taken from a web derived corpus (Antonova and Misyurev, 2011). The dataset is split into three parts using the same technique as for the Europarl sets. During the preprocessing stage we lower-cased and tokenized the data, and excluded sentences longer than 30 words. For the Europarl 5 As the alignment cells are normalised using the softmax and thus take values in [0,1], the trace term is bounded above by min(I, J) which occurs when the two alignment matrices are transposes"
N16-1102,N15-1063,0,0.0126112,"iance reduction. We propose a method for joint training of two directional models as pictured in Figure 2. Training twinned models involves optimising L = − log p(t|s) − log p(s|t) + γB where, as before, we consider only a single sentence pair, for simplicity of notation. This corresponds to a pseudo-likelihood objective, with the B linking the two models.5 The B component considers the alignment (attention) matrices, αs→t ∈ RJ×I and αt←s ∈ RI×J , and attempts to make these close to one another for both translation directions (see Fig. 2). To achieve this, we use a ‘trace bonus’, inspired by (Levinboim et al., 2015), formulated as B = − tr(αs←t > αs→t ) = XX j s←t s→t αi,j αj,i . i 4 Experiments Datasets. We conducted our experiments with four language pairs, translating between English ↔ Romanian, Estonian, Russian and Chinese. These languages were chosen to represent a range of translation difficulties, including languages with significant morphological complexity (Estonian, Russian). We focus on a (simulated) low resource setting, where only a limited amount of training data is available. This serves to demonstrate the robustness and generalisation of our model on sparse data – something that has not"
N16-1102,N06-1014,0,0.529688,"to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics RNN Attentional Decoder Beginnings are difficult START Aller Anfang ist schwer STOP Figure 1: Attentional model of translation (Bahdanau et al., 2015). The encoder is shown below the decod"
N16-1102,D15-1166,0,0.338515,"Missing"
N16-1102,W15-5003,0,0.0228087,"baseline. All other results are for the attentional model with a single-layer LSTM as encoder and two-layer LSTM as decoder, using 512 8 9 E.g., (The sitting was closed at 10.20pm). https://github.com/clab/cnn/ 881 #param (M) 8.7 15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development"
N16-1102,J03-1002,0,0.129597,"sitional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics RNN Attentional Decoder Beginnings are difficult START Aller Anfang ist schwer STOP Figure 1: Attentional model of translation (Bahdanau et al., 2015"
N16-1102,P03-1021,0,0.0154081,"015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: rel"
N16-1102,P02-1040,0,0.0951203,"sed at 10.20pm). https://github.com/clab/cnn/ 881 #param (M) 8.7 15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented"
N16-1102,P16-1008,0,0.0479383,"ion mechanism to be more local, by constraining attention to a text span, whose words’ representations are averaged. Similar in spirit to our work, recent research has proposed different ways of leveraging the attention history to incorporate alignment structural biases. (Luong et al., 2015) made use of the attention vector of the previous position when generating the attention vector for the next position. Feng et al. (2016) added another recurrent structure for the attention mechanism to enhance its memorization capabilities and capture long-range dependencies between the attention vectors. Tu et al. (2016) proposed a coverage vector to keep track of the attention history, hence refining future attentions. Finally, Cheng et al. (2015) proposed a similar agreement-based joint training for bidirectional attention-based neural machine translation, and showed significant improvements in BLEU for the large data French↔English translation. 6 Conclusion We have shown that the attentional model of translation does not capture many well known properties of traditional word-based translation models, and proposed several ways of imposing these as structural biases on the model. We show improvements across"
N16-1102,C96-2141,0,0.94621,"ord based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Ling"
N16-1102,W14-4012,0,\N,Missing
N16-1109,P10-1010,1,0.84534,"Missing"
N16-1109,C12-2013,1,0.844149,"Missing"
N16-1109,C14-1096,1,0.809603,"Missing"
N16-1109,J93-2003,0,0.0596236,"ement in measured alignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments fea"
N16-1109,2012.eamt-1.60,0,0.00647996,"that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM"
N16-1109,N13-1073,0,0.0289172,"now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described above provides a basic f"
N16-1109,P07-2045,0,0.00650321,"Missing"
N16-1109,D13-1019,0,0.00895223,"ity of alignment directly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into"
N16-1109,D15-1166,0,0.176882,"us audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequence of vectors, then decodes it to generate the output. At each step, it “attends” to different parts of the encoded sequence. This model has been used for translation, image caption generation, and speech recognition (Luong et al., 2015; Xu et al., 2015; Chorowski et al., 2014; Chorowski et al., 2015). Here, we briefly describe the basic attentional model, following Bahdanau et al. (2015), review the extensions for encoding structural biases (Cohn et al., 2016), and then present our novel means for adapting the approach handle parallel speech. 3.1 Base attentional model The model is shown in Figure 1. The speech signal is represented as a sequence of vectors S 1 , S 2 , . . . , S m . For the first set of experiments, each S i is a 128dimensional vector-space embedding of a phone. For the second set of experiments, each S i i"
N16-1109,N15-1038,0,0.0279326,"irectly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As"
N16-1109,P11-1064,0,0.00736763,"r languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the att"
N16-1109,P12-1018,0,0.0352532,"ic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequ"
N16-1109,P00-1056,0,0.0780057,"truction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silver” standard alignments between the Spanish speech and the English words. Cleaning and splitting the data based on dialogue turns, resulted in a set of 17,532 Spanish utterances from which we selected 250 for development and 500 testing. For each utterance we have the corresponding English translation, and for each word in the translation we have the corresponding span of Spanish speech. The forced aligner produces the phonetic sequences that correspond to each utterance, which w"
N16-1109,2013.iwslt-papers.14,0,0.326579,"nment smoothing Note that when T = 1 we recover the standard softmax function; we set T = 10 in both experiments. 5 Experimental Setup We work on the Spanish CALLHOME Corpus (LDC96S35), which consists of telephone conversations between Spanish native speakers based in the US and their relatives abroad. While Spanish is not a low-resource language, we pretend that it is by not using any Spanish ASR or resources like transcribed speech or pronunciation lexicons (except in the construction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silve"
N16-1109,van-den-heuvel-etal-2006-tc,0,0.0903295,"Missing"
N16-1109,C96-2141,0,0.222851,"ignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described abov"
N16-1149,W14-4012,0,0.0170018,"Missing"
N16-1149,W11-2123,0,0.023925,"for language modelling. Furthermore, the sizes of our working datasets are an order of magnitude larger than the standard Penn Treebank set which is often used for evaluating neural language models. Set-up and Baselines. We have used cnn6 to implement our models. We use the same configurations for all neural models: 512 input embedding and hidden layer dimensions, 2 hidden layers, and vocabulary sizes as given in Table 1. We used the same vocabulary for the auxiliary and modelled text. We trained a conventional 5−gram language model using modified Kneser-Ney smoothing, with the KenLM toolkit (Heafield, 2011). We used the 3 http://www.europarl.europa.eu/ We ignored the period from June 2011 onwards, as from this date the EU stopped creating manual human translations. 5 This dataset will be released upon publication. 6 https://github.com/clab/cnn/ 4 Method 5-gram LM RNNLM LSTM DGLSTM input+add+k input+mlp+k input+stack+k output+mlp+k output+mlp+t output+mlp+d output+mlp+k+t output+mlp+k+d output+mlp+t+d output+mlp+k+t+d test2010 test2011 test2012 79.9 65.8 54.1 53.1 52.9 53.3 53.7 51.7 52.3 52.0 51.4 51.2 52.6 51.1 77.4 63.9 52.2 52.1 52.1 51.5 51.9 50.6 53.5 49.8 51.1 49.7 51.5 50.6 89.9 73.0 58.4"
N16-1149,P14-1006,0,0.0242849,"Missing"
N16-1149,D13-1176,0,0.0607725,"Missing"
N18-1178,D16-1250,0,0.0182365,"esentations using a multilingual word embedding matrix, We . We construct We by aligning the embedding matrices of all the languages to English, in a pair-wise fashion. Bilingual projection matrices are built using pre-trained FastText monolingual embeddings (Bojanowski et al., 2017) and a dictionary D constructed by translating 5000 frequent English words using Google Translate. Given a pair of embedding matrices E (English) and O (Other), we use singular value decomposition of OT DE (which is U ΣV T ) to get the projection matrix (W ∗ =U V T ), since it also enforces monolingual invariance (Artetxe et al., 2016; Smith et al., 2017). Finally, we obtain the aligned embedding matrix, We , as OW ∗ . We use a bi-LSTM to derive a vector representation of each word in context. The bi-LSTM traverses the sentence si in both the forward and backward directions, and the encoded representation for a given word wit ∈ si , is defined by con→ − catenating its forward ( h it ) and backward hidden   ← − states ( h it ), t ∈ 1, T . Sentence model: Similarly, we use a bi-LSTM to generate a sentence embedding from the wordlevel bi-LSTM, where each input sentence si is represented using the last hidden state of both t"
N18-1178,Q17-1010,0,0.0101676,"65 Proposed Approach with positions on fine-grained policy issues (57 classes). The task here is to learn a model that can: (a) classify sentences according to policy issue classes; and (b) score the overall document on the policy-based left–right spectrum (RILE), in an inter-dependent fashion. Word encoder: We initialize word vector representations using a multilingual word embedding matrix, We . We construct We by aligning the embedding matrices of all the languages to English, in a pair-wise fashion. Bilingual projection matrices are built using pre-trained FastText monolingual embeddings (Bojanowski et al., 2017) and a dictionary D constructed by translating 5000 frequent English words using Google Translate. Given a pair of embedding matrices E (English) and O (Other), we use singular value decomposition of OT DE (which is U ΣV T ) to get the projection matrix (W ∗ =U V T ), since it also enforces monolingual invariance (Artetxe et al., 2016; Smith et al., 2017). Finally, we obtain the aligned embedding matrix, We , as OW ∗ . We use a bi-LSTM to derive a vector representation of each word in context. The bi-LSTM traverses the sentence si in both the forward and backward directions, and the encoded re"
N18-1178,S15-1012,1,0.60701,"he number of times two parties have been in a coalition in the past (to get a value between 0 and 1), for both RegCoalition and EUCoalition. We also construct rules based on transitivity for both the relational features, i.e., parties which have had common coalition partners, even if they were not allies themselves, are likely to have similar policy positions. Manifesto similarity: Manifestos that are similar in content are expected to have similar RILE scores (and associated sentence6 http://www.europarl.europa.eu 1968 level label distributions), similar to the modeling intuition captured by Burford et al. (2015) in the context of congressional debate vote prediction. For a pair of recent manifestos (Recent) we use the cosine similarity (Similarity) between their respective document vectors Vd (Figure 1). Right–left ratio: For a given manifesto, we compute the ratio of sentences categorized under RIGHT to OTHERS RIGHT ( # RIGHT+##LEFT +# NEUTRAL ), where the categorization for sentences is obtained using the joint-structured model (Equation (4)). We also encode the location of sentence ls in a document, by weighing the count of sentences for each class C by its location value P log(l s + 1) (referred"
N18-1178,W17-2906,0,0.500358,"Missing"
N18-1178,E17-2109,0,0.0877985,"Missing"
N18-1178,P82-1020,0,0.822503,"Missing"
N18-1178,P17-1069,0,0.054273,"Missing"
N18-1178,P15-1107,0,0.083391,"Missing"
N18-1178,P07-1055,0,0.0286267,"cture of the text and use a much simpler model architecture: averages of word embeddings, versus our bi-LSTM encodings; and they do not leverage domain information and temporal regularities that can influence policy positions (Greene, 2016). This work will act as a baseline in our experiments in Section 5. Policy-specific position classification can be seen as related to target-specific stance classification (Mohammad et al., 2017), except that the target is not explicitly mentioned in most cases. Secondly, manifestos have both fine- and coarsegrained positions, similar to sentiment analysis (McDonald et al., 2007). Finally, manifesto text is well structured within and across documents (based on coalition), has temporal dependencies, and is multilingual in nature. 2 In this section, we detail the first step of our two-stage approach. We use a hierarchical bidirectional long short-term memory (“biLSTM”) model (Hochreiter and Schmidhuber, 1997; Graves et al., 2013; Li et al., 2015) with a multi-task objective for the sentence classification and document-level regression tasks. A post-hoc calibration of coarse-grained manifesto position is given in Section 4. Let D be the set of manifestos, where a manifes"
N18-1178,D17-1318,0,0.17367,"entences, and a sentence si has T words: wi1 , wi2 , ...wiT . The set Ds ⊂ D is annotated at the sentence-level Related Work Analysing manifesto text is a relatively new application at the intersection of political science and NLP. One line of work in this space has been on sentence-level classification, including classifying each sentence according to its major political theme (1-of-7 categories) (Zirn et al., 2016; Glavaˇs et al., 2017a), its position on various policy themes (Verberne et al., 2014; Biessmann, 2016; Subramanian et al., 2017), or its relative disagreement with other parties (Menini et al., 2017). Recent approaches (Glavaˇs et al., 2017a; Subrama3 1965 Proposed Approach with positions on fine-grained policy issues (57 classes). The task here is to learn a model that can: (a) classify sentences according to policy issue classes; and (b) score the overall document on the policy-based left–right spectrum (RILE), in an inter-dependent fashion. Word encoder: We initialize word vector representations using a multilingual word embedding matrix, We . We construct We by aligning the embedding matrices of all the languages to English, in a pair-wise fashion. Bilingual projection matrices are bu"
N18-1178,W14-2715,0,0.048388,"over the sentence-level labeled set Ds ⊂ D, which is denoted as LSP . The explicit structured sentence-document loss is given as: Lstruc 3 |D| 1 X = |D| d=1 1 X (piright − pileft ) − rd Ld i∈d !2 (3) Strictly speaking, for these documents even, sentence annotation was used to derive the RILE score, but the sentencelevel labels were never made available. 1966 Vd y1 ???? Concatenate y2 ??1 ℎ1 ℎ2 ℎ1 ℎ2 s2 s1 ℎ21 ℎ21 ℎ22 Ban Saturday w21 w22 ℎ22 been used for many tasks including political framing analysis on Twitter (Johnson et al., 2017) and user stance classification on socio-political issues (Sridhar et al., 2014). These models can be specified using Probabilistic Soft Logic (“PSL”) (Bach et al., 2015), a weighted first order logical template language. An example of a PSL rule is Average Pooling ℎ23 ℎ23 night w 2323 yL ??2 ℎ?? special … λ : P(a) ∧ Q(a, b) → R(b) sL ℎ2?? w24 Sentences ℎ?? … ℎ24 ℎ24 ???? ℎ2?? Words handgun w2T Figure 1: Hierarchical bi-LSTM for joint sentence– document analysis (yi denotes the predicted 57-class distribution of sentence si ; pi denotes the distribution over LEFT (in red), RIGHT (in blue) and NEUTRAL (in yellow); rd denotes the RILE score of d). where piright and pileft a"
N18-1178,U17-1003,1,0.545142,"ntries, from elections dating back to 1945. In CMP, a subset of the manifestos has been manually annotated at the sentence-level with one of 57 political themes, divided into 7 major categories.1 Such categories capture party positions (FAVORABLE, UNFAVORABLE or NEITHER) 1 https://manifesto-project.wzb.eu/ coding_schemes/mp_v5 {t.cohn,tbaldwin}@unimelb.edu.au Such manual annotations are labor-intensive and prone to annotation inconsistencies (Mikhaylov et al., 2012). In order to overcome these challenges, supervised sentence classification approaches have been proposed (Verberne et al., 2014; Subramanian et al., 2017). Other than the sentence-level labels, the manifesto text also has a document-level score that quantifies its position on the left–right spectrum. Different approaches have been proposed to derive this score, based on alternate definitions of “left–right” (Slapin and Proksch, 2008; Benoit and Laver, 2007; Lo et al., 2013; D¨aubler and Benoit, 2017). Among these, the RILE index is the most widely adopted (Merz et al., 2016; Jou and Dalton, 2017), and has been shown to correlate highly with other popular scores (Lowe et al., 2011). RILE is defined as the difference between RIGHT and LEFT positi"
N18-2045,C16-1251,0,0.0412767,"n of the task is presented in Section 2. Evaluation. We benchmark against baseline systems presented in the works of Saeidi et al. (2016) and Ma et al. (2018): (1) LR: a logistic regression classifier with n-gram and POS tag features; (2) LSTM-Final: a biLSTM taking the final states as representations; (3) LSTM-Loc: a biLSTM taking the states at the location where target t is mentioned as representations; (4) LSTM+TA+SA: a biLSTM equipped with complex target and sentence-level attention mechanisms; (5) SenticLSTM: an improved version of (4) incorporating the SenticNet external knowledge base (Cambria et al., 2016). We additionally implement a bi-directional EntNet with the same hyper-parameter settings and GloVe embeddings as our model (Henaff et al., 2017). In terms of evaluation, we adopt the standard 70/10/20 train/validation/test split, and report the test performance corresponding to the model with the best validation score. Following Saeidi et al. (2016), we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT- LOCATION, and SAFETY) and employ the following evaluation metrics: macro-average F1 and AUC for aspect detection ignoring the none class, and accuracy and macro-average AUC for sentime"
N18-2045,D14-1162,0,0.0836611,"best validation score. Following Saeidi et al. (2016), we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT- LOCATION, and SAFETY) and employ the following evaluation metrics: macro-average F1 and AUC for aspect detection ignoring the none class, and accuracy and macro-average AUC for sentiment classification. Following Ma et al. (2018), we also report strict accuracy for aspect detection, as the fraction of sentences where all aspects are detected correctly. Model configuration. We initialise our model with GloVe (300-D, trained on 42B tokens, 1.9M vocab, not updated during training: Pennington et al. (2014)) 4 and pre-process the corpus with tokenisation using NLTK (Bird et al., 2009) and case folding. Training is carried out over 800 epochs with the FTRL optimiser (McMahan et al., 2013) and a batch size of 128 and learning rate of 0.05. We use the following hyper-parameters for weight matrices in both directions: R ∈ R300×3 , H, U, V, W are all matrices of size R300×300 , v ∈ R300 , and hidden size of the GRU in Equation (4) is 300. Dropout is applied to the output of φ in the final classifier (Equation (8)) with a rate of 0.2. Moreover, we employ the technique introduced by Gal and Ghahramani"
N18-2045,D17-1047,0,0.0420584,"rack of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the tw"
N18-2045,C16-1146,0,0.218549,"ohn Timothy Baldwin School of Computing and Information Systems The University of Melbourne Victoria, Australia fliu3@student.unimelb.edu.au t.cohn@unimelb.edu.au tb@ldwin.net Abstract The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect-specific word and sentence representations (Tang et al., 2016a). Despite these successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed fo"
N18-2045,C16-1311,0,0.0845094,"successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of"
N18-2045,D16-1021,0,0.0933057,"successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of"
N18-2045,S14-2036,0,0.234655,"Missing"
N18-2045,S14-2076,0,0.172009,"Missing"
N18-2076,N10-1027,1,0.810339,"nimum entropy classification distribution. This is based on an assumption that a closely matching domain should be able to make confident predictions.3 2.2 min θc ,θs ,θp ,θg Experiments 3.1 Language Identification To evaluate our approach, we first consider the language identification task. Data We follow the settings of Lui and Baldwin (2012), involving 5 training sets from 5 different domains with 97 languages in total: Debian, JRC-Acquis, Wikipedia, ClueWeb and RCV2, derived from Lui and Baldwin (2011).5 We evaluate accuracy on seven holdout benchmarks: EuroGov, TCL, Wikipedia26 (all from Baldwin and Lui (2010)), EMEA (Tiedemann, 2009), EuroPARL (Koehn, 2005), T-BE (Tromp and Pechenizkiy, 2011), and T-SC (Carter et al., 2013). Documents are tokenized as a byte sequence (consistent with Lui and Baldwin (2012)), and truncated or padded to a length of 1k bytes.7 Domain-Generative Model (G EN) The second model is based on generation of, rather than conditioning on, the domain, which allows the model to learn domain signals that transfer across some, but not all, domains. Most components are common with the C OND model as described in §2.1, including the use of private and shared representations, their u"
N18-2076,D14-1181,0,0.00402637,"a is how best to use the domain. Basic methods might learn several separate models, or simply ignore the domain and learn a single model. Neither method is ideal: the former fails to share statistics between the models to capture the general concept, while the latter discards information that can aid classification, e.g., domain-specific vocabulary or class skew. To address these issues, we propose two architectures as illustrated in Figure 1 (a and b), parameterised as a convolutional network (CNN) over the input instance, chosen based on the success of CNNs for text categorisation problems (Kim, 2014); note, however, that our method is general and can be applied with other network types. Both representations are based on the idea of twin representations of each instance,2 denoted shared and private representations, which are trained to capture domain-general versus domain-specific concepts, respectively. This is achieved using various loss functions, most notably an adversarial loss to discourage learning of domain-specific concepts in the shared representations. The two architectures differ in whether the domain is provided as an input (C OND) or an output (G EN). Below, we elaborate on t"
N18-2076,P15-2030,1,0.862185,"constructed from different sources, featuring different topics, register, writing style, etc. An important, yet elusive, goal is to produce NLP tools that are capable of handling all types of texts, such that we can have, e.g., text classifiers that work well on texts from newswire to wikis to micro-blogs. A key roadblock is application to new domains, unseen in training. Accordingly, training needs to be robust to domain variation, such that domain-general concepts are learned in preference to domain-specific phenomena, which will not transfer well to out-of-domain evaluation. To illustrate, Bitvai and Cohn (2015) report learning formatting quirks of specific reviewers in a review text regression task, which are unlikely to prove useful on other texts. This classic problem in NLP has been tackled under the guise of “domain adaptation”, also known as unsupervised transfer learning, using feature-based methods to support knowledge 1 Code, data and evaluation scripts available at https://github.com/lrank/Domain_Robust_ Text_Representation.git 474 Proceedings of NAACL-HLT 2018, pages 474–479 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics methods outperform the st"
N18-2076,C16-1038,0,0.132272,"Missing"
N18-2076,P07-1056,0,0.885378,"is classic problem in NLP has been tackled under the guise of “domain adaptation”, also known as unsupervised transfer learning, using feature-based methods to support knowledge 1 Code, data and evaluation scripts available at https://github.com/lrank/Domain_Robust_ Text_Representation.git 474 Proceedings of NAACL-HLT 2018, pages 474–479 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics methods outperform the state-of-the-art (Lui and Baldwin, 2012) in terms of out-of-domain accuracy. As a secondary evaluation, we use the MultiDomain Sentiment Dataset (Blitzer et al., 2007), where we once again observe a clear advantage for our approaches, illustrating the potential of our technique more broadly in NLP. 2 Ds (θd ) CNNs (θs ) xi hpi θc yi (a) Domain-conditional (C OND) model. Ds (θd ) Multi-domain Learning CNNs (θs ) A primary consideration when formulating models of multi-domain data is how best to use the domain. Basic methods might learn several separate models, or simply ignore the domain and learn a single model. Neither method is ideal: the former fails to share statistics between the models to capture the general concept, while the latter discards informat"
N18-2076,2005.mtsummit-papers.11,0,0.0588508,"an assumption that a closely matching domain should be able to make confident predictions.3 2.2 min θc ,θs ,θp ,θg Experiments 3.1 Language Identification To evaluate our approach, we first consider the language identification task. Data We follow the settings of Lui and Baldwin (2012), involving 5 training sets from 5 different domains with 97 languages in total: Debian, JRC-Acquis, Wikipedia, ClueWeb and RCV2, derived from Lui and Baldwin (2011).5 We evaluate accuracy on seven holdout benchmarks: EuroGov, TCL, Wikipedia26 (all from Baldwin and Lui (2010)), EMEA (Tiedemann, 2009), EuroPARL (Koehn, 2005), T-BE (Tromp and Pechenizkiy, 2011), and T-SC (Carter et al., 2013). Documents are tokenized as a byte sequence (consistent with Lui and Baldwin (2012)), and truncated or padded to a length of 1k bytes.7 Domain-Generative Model (G EN) The second model is based on generation of, rather than conditioning on, the domain, which allows the model to learn domain signals that transfer across some, but not all, domains. Most components are common with the C OND model as described in §2.1, including the use of private and shared representations, their use in the classification output, and the adversar"
N18-2076,I11-1062,1,0.872615,"each domain in the test set as belonging to one of the training domains, and then select the domain with the minimum entropy classification distribution. This is based on an assumption that a closely matching domain should be able to make confident predictions.3 2.2 min θc ,θs ,θp ,θg Experiments 3.1 Language Identification To evaluate our approach, we first consider the language identification task. Data We follow the settings of Lui and Baldwin (2012), involving 5 training sets from 5 different domains with 97 languages in total: Debian, JRC-Acquis, Wikipedia, ClueWeb and RCV2, derived from Lui and Baldwin (2011).5 We evaluate accuracy on seven holdout benchmarks: EuroGov, TCL, Wikipedia26 (all from Baldwin and Lui (2010)), EMEA (Tiedemann, 2009), EuroPARL (Koehn, 2005), T-BE (Tromp and Pechenizkiy, 2011), and T-SC (Carter et al., 2013). Documents are tokenized as a byte sequence (consistent with Lui and Baldwin (2012)), and truncated or padded to a length of 1k bytes.7 Domain-Generative Model (G EN) The second model is based on generation of, rather than conditioning on, the domain, which allows the model to learn domain signals that transfer across some, but not all, domains. Most components are com"
N18-2076,P12-3005,1,0.874271,"ses the feature augmentation method of Daum´e III (2007) to convolutional neural networks, as part of a larger deep learning architecture. Additionally, we use adversarial training such that the shared representation is explicitly discouraged from learning domain identifying information (Ganin and Lempitsky, 2015). We present two architectures which differ in whether domain is conditioned on or generated, and in terms of parameter sharing in forming private representations. We primarily evaluate on the task of language identification (“LangID”: Cavnar and Trenkle (1994)), using the corpora of Lui and Baldwin (2012), which combine large training sets over a diverse range of text domains. Domain adaptation is an important problem for this task (Lui and Baldwin, 2014; Jurgens et al., 2017), where text resources are collected from numerous sources, and exhibit a wide variety of language use. We show that while domain adversarial training overall improves over baselines, gains are modest. The same applies to twin shared/private architectures, but when the two methods are combined, we observe substantial improvements. Overall, our Most real world language problems require learning from heterogenous corpora, r"
N18-2076,P07-1033,0,0.442683,"Missing"
N18-2076,W14-1303,1,0.87424,"we use adversarial training such that the shared representation is explicitly discouraged from learning domain identifying information (Ganin and Lempitsky, 2015). We present two architectures which differ in whether domain is conditioned on or generated, and in terms of parameter sharing in forming private representations. We primarily evaluate on the task of language identification (“LangID”: Cavnar and Trenkle (1994)), using the corpora of Lui and Baldwin (2012), which combine large training sets over a diverse range of text domains. Domain adaptation is an important problem for this task (Lui and Baldwin, 2014; Jurgens et al., 2017), where text resources are collected from numerous sources, and exhibit a wide variety of language use. We show that while domain adversarial training overall improves over baselines, gains are modest. The same applies to twin shared/private architectures, but when the two methods are combined, we observe substantial improvements. Overall, our Most real world language problems require learning from heterogenous corpora, raising the problem of learning robust models which generalise well to both similar (in domain) and dissimilar (out of domain) instances to those seen in"
N18-2076,W13-4068,0,0.041982,"Missing"
N18-2076,D12-1119,0,0.0882485,"Missing"
N19-1203,P17-1183,0,0.0301289,"vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attention from Aharoni and Goldberg (2017). The model was one of the top performers in the 2016 SIGMORPHON shared task (Cotterell et al., 2016); it achieved particularly high accuracy in the low-resource setting. Hard attention is motivated by the observation that alignment between the input and output sequences is often monotonic in inflection tasks. In the model, the input lemma is treated as a sequence of characters, and encoded using a bidirectional LSTM (Graves and Schmidhuber, 2005), to produce vectors xj for each character position j. Next the word wi = c = c1 · · · c|wi |is generated in a decoder character-by-character: p(cj |"
N19-1203,W05-0909,0,0.0318394,"trumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mapping a meaning representation to text, w"
N19-1203,P17-1080,0,0.0256269,"version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of this paper is a novel str"
N19-1203,W11-2832,0,0.0158478,"presentation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally, as a morphological i"
N19-1203,Q17-1010,0,0.0234037,"or decoding we use a greedy strategy where we first decode the CRF, that is, we solve the problem m? = argmaxm log p(m |`), using the Viterbi (1967) algorithm. We then use this decoded m? to generate forms from the inflector. Note that finding the one-best string under our neural inflector is intractable, and for this reason we use greedy search. 3 Experiments Dataset. We use the Universal Dependencies v1.2 dataset (Nivre et al., 2016) for our experiments. We include all the languages with information on their lemmata and fine-grained grammar tag annotation that also have fasttext embeddings (Bojanowski et al., 2017), which are used for word embedding initialization.5 Evaluation. We evaluate our model’s ability to predict: (i) the correct morphological tags from the lemma context, and (ii) the correct inflected forms. As our evaluation metric, we report 1-best accuracy for both tags and word form prediction. Configuration. We use a word and character embedding dimensionality of 300 and 100, respectively. The hidden state dimensionality is set to 200. All models are trained with Adam (Kingma and Ba, 2014), with a learning rate of 0.001 for 20 epochs. Baselines. We use two baseline systems: (1) the CoNLL–SI"
N19-1203,K18-3001,1,0.790791,"on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguisticallymotivated latent variables into NLP models. 1 Introduction NLP systems are often required to generate grammatical text, e.g., in machine translation, summarization, dialogue, and grammar correction. One component of grammaticality is the use of contextually appropriate closed-class morphemes. In this work, we study contextual inflection, which has been recently introduced in the CoNLLSIGMORPHON 2018 shared task (Cotterell et al., 2018) to directly investigate context-dependent morphology in NLP. There, a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the p"
N19-1203,K17-2001,1,0.93262,"a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the previous CoNLL-SIGMORPHON shared tasks on morphological reinflection (Cotterell et al., 2017)—has focused mainly on step (3) above. As the task has been introduced into the literature only recently, we provide some background. Contextual inflection amounts to a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. Th"
N19-1203,N16-1030,0,0.0407715,"tured neural model 1 Y p(m |`) = ψ (mi , mi−1 , `) Z(`) (2) i=1 2 Although wi can sometimes be computed by concatenating `i with mi -specific affixes, it can also be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both ar"
N19-1203,D15-1176,0,0.0216304,"lso be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attentio"
N19-1203,Q16-1037,0,0.481413,"a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of t"
N19-1203,L16-1262,0,0.0579627,"Missing"
N19-1203,P02-1040,0,0.103705,"shifting positions (such as the instrumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mappi"
N19-1203,petrov-etal-2012-universal,0,0.0315183,"Missing"
N19-1203,P17-2002,0,0.014134,"where the meaning representation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally,"
P05-1002,W02-2018,0,0.0757911,"likelihood L is given by L = X log p(y(i) |x(i) ) i =  (i) +1 X T X X i −  t=1 log Z(x(i) ) (i) (i) λk fk (t, yt−1 , yt , x(i) ) k o where x(i) and y(i) are the ith observation and label sequence. Note that a prior is often included in the L formulation; it has been excluded here for clarity of exposition. CRF estimation methods include generalised iterative scaling (GIS), improved iterative scaling (IIS) and a variety of gradient based methods. In recent empirical studies on maximum entropy models and CRFs, limited memory variable metric (LMVM) has proven to be the most efficient method (Malouf, 2002; Wallach, 2002); accordingly, we have used LMVM for CRF estimation. Every iteration of LMVM training requires the computation of the log-likelihood and its derivative with respect to each parameter. The partition function Z(x) can be calculated efficiently using dynamic programming with the forward algorithm. P Z(x) is given by y αT (y) where α are the forward values, defined recursively as αt+1 (y) = X y0 αt (y 0 ) exp X k λk fk (t + 1, y 0 , y, x) 3 The derivative of the log-likelihood is given by ∂L ∂λk  (i) +1 X T X = i X −  (i) (i) fk (t, yt−1 , yt , x(i) ) t=1 p(y|x(i) ) y (i) +1 TX"
P05-1002,W03-0430,0,0.0584727,"onditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks. However, these tasks have mostly involved very small label sets. When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable. Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks. This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC). A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement. During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models. Error-correcting CRF train"
P05-1002,P04-1007,0,0.075963,"Missing"
P05-1002,W00-0726,0,0.0710543,"less than that of regular CRF training methods. We have evaluated the error-correcting CRF on the CoNLL 2003 named entity recognition (NER) task (Sang and Meulder, 2003), where we show that the method yields similar generalisation performance to standardly formulated CRFs, while requiring only a fraction of the resources, and no increase in training time. We have also shown how the errorcorrecting CRF scales when applied to the larger task of POS tagging the Penn Treebank and also the even larger task of simultaneously noun phrase chunking (NPC) and POS tagging using the CoNLL 2000 data-set (Sang and Buchholz, 2000). 2 Conditional random fields CRFs are undirected graphical models used to specify the conditional probability of an assignment of output labels given a set of input observations. We consider only the case where the output labels of the 11 model are connected by edges to form a linear chain. The joint distribution of the label sequence, y, given the input observation sequence, x, is given by TX +1 X 1 λk fk (t, yt−1 , yt , x) exp Z(x) t=1 k p(y|x) = where T is the length of both sequences and λk are the parameters of the model. The functions fk are feature functions which map properties of the"
P05-1002,N03-1028,0,0.192363,"cessing tasks. However, these tasks have mostly involved very small label sets. When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable. Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks. This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC). A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement. During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models. Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF, while"
P05-1002,P05-1003,1,0.832079,"j (q|x) is the predicted probability of q given x by the j th weak learner, bj (y) is the bit string representing y for the j th weak learner and Z 0 (x) is the partition function. The log probability is X {Fj (bj (y), x) · λj − log Zj (x)} − log Z 0 (x) j +1 where Fj (y, x) = Tt=1 fj (t, yt−1 , yt , x). This log probability can then be maximised using the Viterbi algorithm as before, noting that the two log terms are constant with respect to y and thus need not be evaluated. Note that this decoding is an equivalent formulation to a uniformly weighted logarithmic opinion pool, as described in Smith et al. (2005). Of the three decoding methods, Standalone has the lowest complexity, requiring only a binary Viterbi decoding for each weak learner. Marginals is slightly more complex, requiring the forward and backward values. Product, however, requires Viterbi decoding with the full label set, and many features – the union of the features of each weak learner – which can be quite computationally demanding. P 3.3 Choice of code The accuracy of ECOC methods are highly dependent on the quality of the code. The ideal code has diverse rows, yielding a high error-correcting capability, and diverse columns such"
P05-1002,N03-1033,0,0.00898622,"Missing"
P05-1002,W03-0419,0,\N,Missing
P05-1003,P05-1002,1,0.872976,"Missing"
P05-1003,W03-0424,0,0.021869,"onsists only of features that involve label X at the current or previous positions. These experts therefore focus on trying to model the distribution of a particular label. 4.3 Expert sets For each task we compare the performance of the LOP-CRF to that of the standard CRF by defining a single, complex CRF, which we call a monolithic CRF, and a range of expert sets. The monolithic CRF for NER comprises a number of word and POS tag features in a window of five words around the current word, along with a set of orthographic features defined on the current word. These are based on those found in (Curran and Clark, 2003). Examples include whether the current word is capitalised, is an initial, contains a digit, contains punctuation, etc. The monolithic CRF for NER has 450, 345 features. The monolithic CRF for POS tagging comprises word and POS features similar to those in the NER monolithic model, but over a smaller number of orthographic features. The monolithic model for POS tagging has 188, 448 features. Each of our expert sets consists of a number of CRF experts. Usually these experts are designed to 4 See (Cohn et al., 2005) for a scaling method allowing the full POS tagging task with CRFs. 22 • Random c"
P05-1003,W02-2018,0,0.0593803,"is the partition function that ensures (1) represents a probability distribution. The functions f k are feature functions representing the occurrence of different events in the sequences s and o. The parameters λk can be estimated by maximising the conditional log-likelihood of a set of labelled training sequences. The log-likelihood is given by: L (λ ) = ˜ s) log p(s |o; λ ) ∑ p(o, o,s "" T +1 = ˜ s) ∑ λ · f(s, o,t) ∑ p(o, o,s − t=1 ˜ log Z(o; λ ) ∑ p(o) # E p(o,s) [ fk ] − E p(s|o) [ fk ] = 0, ∀k ˜ In general this cannot be solved for the λk in closed form so numerical routines must be used. Malouf (2002) and Sha and Pereira (2003) show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della Pietra et al., 1997) previously used for log-linear optimisation problems. In all our experiments we use the LMVM method to train the CRFs. For CRFs with general graphical structure, calculation of E p(s|o) [ fk ] is intractable, but for the linear chain case Lafferty et al. (2001) describe an efficient dynamic programming procedure for inference, similar in nature to the f"
P05-1003,W03-0430,0,0.0221269,"s (LOP-CRFs). We apply the LOP-CRF to two sequencing tasks. Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. 1 Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As miles@inf.ed.ac.uk a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005). These advanced methods tend to be difficult to implement and"
P05-1003,N04-1012,1,0.69984,"actable, but for the linear chain case Lafferty et al. (2001) describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models. 3 Logarithmic Opinion Pools In this paper an expert model refers a probabilistic model that focuses on modelling a specific subset of some probability distribution. The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science (Bordley, 1982), and NLP (Osborne and Baldridge, 2004). In this paper we restrict ourselves to sequence models. Given a set of sequence model experts, indexed by α , with conditional distributions pα (s |o) and a set of non-negative normalised weights wα , a logarithmic opinion pool 2 is defined as the distribution: pLOP (s |o) = o where p(o, ˜ s) and p(o) ˜ are empirical distributions defined by the training set. At the maximum likelihood solution the model satisfies a set of feature constraints, whereby the expected count of each feature under the model is equal to its empirical count on the training data: 1 In this paper we assume there is a o"
P05-1003,N04-1042,0,0.0472658,"show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. 1 Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As miles@inf.ed.ac.uk a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005). These advanced methods tend to be difficult to implement and are often computationally expensive. Consequently, due to its ease of imp"
P05-1003,N03-1028,0,0.195665,"this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs). We apply the LOP-CRF to two sequencing tasks. Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search. 1 Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As miles@inf.ed.ac.uk a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005). These adva"
P05-1003,W00-0726,0,0.035673,"3 shared task dataset (Tjong Kim Sang and De Meulder, 2003). For this dataset the entity types are: persons (PER), locations (LOC), organisations (ORG) and miscellaneous (MISC). The training set consists of 14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466 sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and 46, 666 tokens. 4.2 Part-of-Speech Tagging POS tagging involves labelling each word in a sentence with its part-of-speech, for example noun, verb, adjective, etc. For our experiments we use the CoNLL-2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000). This has 48 different POS tags. In order to make training time manageable4 , we collapse the number of POS tags from 48 to 5 following the procedure used in (McCallum et al., 2003). In summary: focus on modelling a particular aspect or subset of the distribution. As we saw earlier, the aim here is to define experts that model parts of the distribution well while retaining mutual diversity. The experts from a particular expert set are combined under a LOP-CRF and the weights are trained as described previously. We define our range of expert sets as follows: • Simple consists of the monolithic"
P05-1003,W03-0419,0,0.0675712,"Missing"
P05-1003,W98-1120,0,\N,Missing
P05-1003,I05-1014,0,\N,Missing
P05-1003,W96-0102,0,\N,Missing
P05-1003,M98-1015,0,\N,Missing
P05-1003,J93-2004,0,\N,Missing
P05-1003,W04-3230,0,\N,Missing
P05-1003,W96-0213,0,\N,Missing
P05-1003,W03-0403,1,\N,Missing
P05-1003,E99-1001,0,\N,Missing
P05-1003,W06-2918,1,\N,Missing
P05-1003,C04-1081,0,\N,Missing
P05-1003,W03-0428,0,\N,Missing
P05-1003,W03-0425,0,\N,Missing
P05-1003,C00-2137,0,\N,Missing
P05-1003,W03-0423,0,\N,Missing
P05-1003,N03-1002,0,\N,Missing
P05-1003,W02-1002,0,\N,Missing
P05-1003,J94-2001,0,\N,Missing
P05-1003,J95-4004,0,\N,Missing
P05-1003,W05-0622,1,\N,Missing
P05-1003,W03-1019,0,\N,Missing
P05-1003,P04-1014,0,\N,Missing
P05-1003,P04-1007,0,\N,Missing
P05-1003,P04-1086,0,\N,Missing
P05-1003,kruengkrai-etal-2006-conditional,0,\N,Missing
P05-1003,W02-2024,0,\N,Missing
P05-1003,I05-1078,1,\N,Missing
P05-1003,M98-1004,0,\N,Missing
P05-1003,C96-1079,0,\N,Missing
P05-1003,W03-1018,0,\N,Missing
P05-1003,N06-1012,0,\N,Missing
P05-1003,P02-1062,0,\N,Missing
P06-1009,J93-2003,0,0.0416268,"Missing"
P06-1009,P04-1023,0,0.152922,"recall increased from 89.2 to 92.4. This resulted in an overall decrease in AER to 6.99. We found no benefit from using many-tomany possible alignments as they added a significant amount of noise to the data. 4.2 Table 4. Results using features from Model 4 bidirectional alignments, training with and without the possible (P) alignments. model precision recall f-score AER Fre ↔ Eng 94.6 92.2 93.4 6.47 Fre ↔ Eng (Model 4) 96.1 93.3 94.7 5.19 Table 5. 10-fold cross-validation results, with and without Model 4 features. vised Model 4 did not have access to the wordalignments in our training set. Callison-Burch et al. (2004) demonstrated that the GIZA++ models could be trained in a semi-supervised manner, leading to a slight decrease in error. To our knowledge, our AER of 5.19 is the best reported result, generative or discriminative, on this data set. Model 4 as a feature 5 Previous work (Taskar et al., 2005) has demonstrated that by including the output of Model 4 as a feature, it is possible to achieve a significant decrease in AER. We trained Model 4 in both directions on the two language pairs. We added two indicator features (one for each direction) to our CRF which were active if a given word pair were ali"
P06-1009,H05-1012,0,0.146534,"tup as our work, they achieve an AER of 5.4 with Model 4 features, and 10.7 without (compared to 5.29 and 6.99 for our CRF). One of the strengths of the CRF MAP estimation is the powerful smoothing offered by the prior, which allows us to avoid heuristics such as early stopping and hand weighted loss-functions that were needed for the maximum-margin model. Liu et al. (2005) used a conditional log-linear model with similar features to those we have employed. They formulated a global model, without making a Markovian assumption, leading to the need for a sub-optimal heuristic search strategies. Ittycheriah and Roukos (2005) trained a disCross-validation Using 10-fold cross-validation we are able to generate results on the whole of the Hansards test data which are comparable to previously published results. As the sentences in the test set were randomly chosen from the training corpus we can expect cross-validation to give an unbiased estimate of generalisation performance. These results are displayed in Table 5, using the possible (P) alignments for training. As the training set for each fold is roughly four times as big previous training set, we see a small improvement in AER. The final results of 6.47 and 5.19"
P06-1009,N03-1017,0,0.00803004,"d word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively. 1 Introduction Modern phrase based statistical machine translation (SMT) systems usually break the translation task into two phases. The first phase induces word alignments over a sentence-aligned bilingual corpus, and the second phase uses statistics over these predicted word alignments to decode (translate) novel sentences. This paper deals with the first of these tasks: word alignment. Most current SMT systems (Och and Ney, 2004; Koehn et al., 2003) use a generative model for word alignment such as the freely available 1 We adopt the standard notation of e and f to denote the target (English) and source (foreign) sentences, respectively. 65 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rithms are tractable and efficient, thereby avoiding the need for heuristics. The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping featur"
P06-1009,P05-1057,0,0.367771,"e if a given word pair were aligned in the Model 4 output. Table 4 displays the results on both language pairs when these additional features are used with the refined model. This produces a large increase in performance, and when including the possibles, produces AERs of 5.29 and 25.8, both well below that of Model 4 alone (shown in Tables 1 and 2). 4.3 Related work Recently, a number of discriminative word alignment models have been proposed, however these early models are typically very complicated with many proposing intractable problems which require heuristics for approximate inference (Liu et al., 2005; Moore, 2005). An exception is Taskar et al. (2005) who presented a word matching model for discriminative alignment which they they were able to solve optimally. However, their model is limited to only providing one-to-one alignments. Also, no features were defined on label sequences, which reduced the model’s ability to capture the strong monotonic relationships present between European language pairs. On the French-English Hansards task, using the same training/testing setup as our work, they achieve an AER of 5.4 with Model 4 features, and 10.7 without (compared to 5.29 and 6.99 for our C"
P06-1009,W02-2018,0,0.025776,"et word indexed by at , rather than the index itself, which determines whether the feature is active, and thus the sparsity of the index label set is not an issue. λk hk (t, at−1 , at , e, f ) k − log ZΛ (e, f ) − X λ2 k + const. 2σk2 (2) k 3.1 In order to train the model, we maximize (2). While the log-likelihood cannot be maximised for the parameters, Λ, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using forward-backward inference, which yields the partition function, ZΛ (e, f ), required for the log-likelihood, and the pair-wise marginals, pΛ (at−1 , at |e, f ), required for its derivatives. The Viterbi algorithm is used to find the maximum posterior probability alignment for test sentences, a∗ = arg maxa pΛ (a|e, f ). Both the forward-backward and Viterbi algorithm are dynamic programs which make use of the Markov assumption to calcula"
P06-1009,W05-0809,0,0.0738309,"iguous or idiomatic alignments. We measure the performance of our model using alignment error rate (AER), which is defined as: Table 1. Results on the Hansard data using all features model precision recall f-score AER Model 4 refined 80.49 64.10 71,37 28.63 Model 4 intersected 95.94 53.56 68.74 31.26 Romanian → English 82.9 61.3 70.5 29.53 English → Romanian 82.8 60.6 70.0 29.98 intersection 94.4 52.5 67.5 32.45 refined 77.1 68.5 72.6 27.41 AER(A, S, P ) = 1 − where A is the set of predicted alignments. The second data set is the Romanian-English parallel corpus from the 2005 ACL shared task (Martin et al., 2005). This consists of approximately 50,000 aligned sentences and 448 wordaligned sentences, which are split into a 248 sentence trial set and a 200 sentence test set. We used these as our training and test sets, respectively. For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). For this task we have used the same test data as the competition entrants, and therefore can directly compare our results. The word alignments in this corpus were only annotated with sure (S) alignments, and therefore the AER is equiv"
P06-1009,W03-0301,0,0.347465,"nian 82.8 60.6 70.0 29.98 intersection 94.4 52.5 67.5 32.45 refined 77.1 68.5 72.6 27.41 AER(A, S, P ) = 1 − where A is the set of predicted alignments. The second data set is the Romanian-English parallel corpus from the 2005 ACL shared task (Martin et al., 2005). This consists of approximately 50,000 aligned sentences and 448 wordaligned sentences, which are split into a 248 sentence trial set and a 200 sentence test set. We used these as our training and test sets, respectively. For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). For this task we have used the same test data as the competition entrants, and therefore can directly compare our results. The word alignments in this corpus were only annotated with sure (S) alignments, and therefore the AER is equivalent to the F1 score. In the shared task it was found that models which were trained on only the first four letters of each word obtained superior results to those using the full words (Martin et al., 2005). We observed the same result with our model on the trial set and thus have only used the first four letters when training the Dice and Model 1 translation p"
P06-1009,H05-1011,0,0.407777,"pair were aligned in the Model 4 output. Table 4 displays the results on both language pairs when these additional features are used with the refined model. This produces a large increase in performance, and when including the possibles, produces AERs of 5.29 and 25.8, both well below that of Model 4 alone (shown in Tables 1 and 2). 4.3 Related work Recently, a number of discriminative word alignment models have been proposed, however these early models are typically very complicated with many proposing intractable problems which require heuristics for approximate inference (Liu et al., 2005; Moore, 2005). An exception is Taskar et al. (2005) who presented a word matching model for discriminative alignment which they they were able to solve optimally. However, their model is limited to only providing one-to-one alignments. Also, no features were defined on label sequences, which reduced the model’s ability to capture the strong monotonic relationships present between European language pairs. On the French-English Hansards task, using the same training/testing setup as our work, they achieve an AER of 5.4 with Model 4 features, and 10.7 without (compared to 5.29 and 6.99 for our CRF). One of th"
P06-1009,J03-1002,0,0.361359,"erence on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rithms are tractable and efficient, thereby avoiding the need for heuristics. The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping features. Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting. We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (Och and Ney, 2003). This models one-to-many alignments, where each target word is aligned with zero or more source words. Many-to-many alignments are recoverable using the standard techniques for superimposing predicted alignments in both translation directions. The paper is structured as follows. Section 2 presents CRFs for word alignment, describing their form and their inference techniques. The features of our model are presented in Section 3, and experimental results for word aligning both French-English and Romanian-English sentences are given in Section 4. Section 5 presents related work, and we describe"
P06-1009,J04-4002,0,0.0152164,"h only a few hundred word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively. 1 Introduction Modern phrase based statistical machine translation (SMT) systems usually break the translation task into two phases. The first phase induces word alignments over a sentence-aligned bilingual corpus, and the second phase uses statistics over these predicted word alignments to decode (translate) novel sentences. This paper deals with the first of these tasks: word alignment. Most current SMT systems (Och and Ney, 2004; Koehn et al., 2003) use a generative model for word alignment such as the freely available 1 We adopt the standard notation of e and f to denote the target (English) and source (foreign) sentences, respectively. 65 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rithms are tractable and efficient, thereby avoiding the need for heuristics. The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse a"
P06-1009,N03-1028,0,0.028075,"d by at , rather than the index itself, which determines whether the feature is active, and thus the sparsity of the index label set is not an issue. λk hk (t, at−1 , at , e, f ) k − log ZΛ (e, f ) − X λ2 k + const. 2σk2 (2) k 3.1 In order to train the model, we maximize (2). While the log-likelihood cannot be maximised for the parameters, Λ, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using forward-backward inference, which yields the partition function, ZΛ (e, f ), required for the log-likelihood, and the pair-wise marginals, pΛ (at−1 , at |e, f ), required for its derivatives. The Viterbi algorithm is used to find the maximum posterior probability alignment for test sentences, a∗ = arg maxa pΛ (a|e, f ). Both the forward-backward and Viterbi algorithm are dynamic programs which make use of the Markov assumption to calculate efficiently the exact"
P06-1009,H05-1010,0,0.803732,"a single indicator feature which detects when the source and target words appear in an entry of the dictionary. For the English-French dictionary we used FreeDict,3 which contains 8,799 English words. For Romanian-English we used a dictionary compiled by Rada Mihalcea,4 which contains approximately 38,000 entries. Orthographic features Features based on string overlap allow our model to recognise cognates and orthographically similar translation pairs, which are particularly common between European languages. Here we employ a number of string matching features inspired by similar features in Taskar et al. (2005). We use an indicator feature for every possible source-target word pair in the training data. In addition, we include indicator features for an exact string match, both with and without vowels, and the edit-distance between the source and target words as a realvalued feature. We also used indicator features to test for matching prefixes and suffixes of length three. As stated earlier, the Dice translation score often erroneously rewards alignments with common words. In order to address this problem, we include the absolute difference in word length as a real-valued feature and an indicator fe"
P06-1009,W02-1012,0,0.0864874,"o incorporate non-independent features over the sentence pairs. For instance, as well as detecting that a source word is aligned to a given target word, we would also like to encode syntactic and lexical features of the word pair, such as their partsof-speech, affixes, lemmas, etc. Features such as these would allow for more effective use of sparse data and result in a model which is more robust in the presence of unseen words. Adding these non-independent features to a generative model requires that the features’ inter-dependence be modelled explicitly, which often complicates the model (eg. Toutanova et al. (2002)). Secondly, the later IBM models, such as Model 4, have to resort to heuristic search techniques to approximate forward-backward and Viterbi inference, which sacrifice optimality for tractability. This paper presents an alternative discriminative method for word alignment. We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001). The inference algoIn this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which"
P06-1009,C96-2141,0,0.882345,"ent lengths – i.e. aligning rare (long) words with frequent (short) determiners, verbs etc. Markov features Features defined over adjacent aligment labels allow our model to reflect the tendency for monotonic alignments between European languages. We define a real-valued alignment index jump width feature: jump width(t − 1, t) = abs(at − at−1 − 1) this feature has a value of 0 if the alignment labels follow the downward sloping diagonal, and is positive otherwise. This differs from the GIZA++ hidden Markov model which has individual parameters for each different jump width (Och and Ney, 2003; Vogel et al., 1996): we found a single feature (and thus parameter) to be more effective. We also defined three indicator features over null transitions to allow the modelling of the probability of transition between, to and from null labels. Relative sentence postion A feature for the absolute difference in relative sentence position at (abs( |e| − |ft |)) allows the model to learn a preference for aligning words close to the alignment matrix diagonal. We also included two conjunction features for the relative sentence position multiplied by the Dice and Model 1 translation scores. POS tags Part-of-speech tags"
P07-1092,J93-2003,0,0.00908956,"arget phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. 1 Introduction Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003). Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the outpu"
P07-1092,W03-0310,0,0.0589821,"g multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1"
P07-1092,N06-1003,0,0.393761,"ferent languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1 0.2 0.5 standard Italian all all + standard 0.005 0.01 proportion of test events in phrase table and evaluated against a test set. Although very few small phrases are unknown, the majority of larger phrases are unseen. The Italian and all results show that triangulation alone can provide similar or improved coverage compared to the standard sourcetarget model; further improvement is achieved by combining the tr"
P07-1092,W05-0828,0,0.0228095,"nto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1 0.2 0.5 standard Italian all all + standard 0.005 0.01 proportion of test events in phrase table and evaluated against a test set. Although very few small phrases are u"
P07-1092,W06-1607,0,0.0894961,"ge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc.). Our method translates each target phrase, t, first to an intermediate language, i, and then into the source language, s. We call this two-stage translation process triangulation (Kay, 1997). We present a probabilistic formulation through which we can estimate the desired phrase translation distribution (phrase-table) by marginaliP sation, p(s|t) = i p(s, i|t). As with conventional smoothing methods (Koehn et al., 2003; Foster et al., 2006), triangulation increases the robustness of phrase translation estimates. In contrast to smoothing, our method alleviates data sparseness by exploring additional multiparallel data rather than adjusting the probabilities of existing data. Importantly, triangulation provides us with separately estimated phrase-tables which could be further smoothed to provide more reliable distributions. Moreover, the triangulated phrase-tables can be easily combined with the standard sourcetarget phrase-table, thereby improving the coverage over unseen source phrases. As an example, consider Figure 1 which sho"
P07-1092,N03-1017,0,0.363586,"e phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. 1 Introduction Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003). Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the output. Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world’s lang"
P07-1092,2005.mtsummit-papers.11,0,0.0297866,"Missing"
P07-1092,E06-1005,0,0.0182169,"biguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1 0.2 0.5 standard Italian all all + standard 0.005 0.01 proportion of test events in phrase table and evaluated against a test set. A"
P07-1092,2001.mtsummit-papers.46,0,0.289389,"on for Computational Linguistics 2 Related Work The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires pa"
P07-1092,W99-0604,0,0.026701,")λj (4) T j where T and S denote a target and source sentence respectively. The parameters, λj , were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to 3 treat each distribution as a feature, and learn the mixFor details see http://www.statmt.org/wpt05/ ing weights automatically. Note that we must indi- mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent w"
P07-1092,P03-1021,0,0.0192285,"ing setting was chosen to simulate translating to or from a “low density” language, where only a few small independently sourced parallel corpora are available. These bitexts were used for direct translation and triangulation. All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points. Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: Y T∗ = arg max fj (T, S)λj (4) T j where T and S denote a target and source sentence respectively. The parameters, λj , were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). This has the same form use"
P07-1092,P02-1040,0,0.0805575,"slating to or from a “low density” language, where only a few small independently sourced parallel corpora are available. These bitexts were used for direct translation and triangulation. All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points. Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: Y T∗ = arg max fj (T, S)λj (4) T j where T and S denote a target and source sentence respectively. The parameters, λj , were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). This has the same form used for log-linear training of SMT decoders (Och, 200"
P07-1092,J03-3002,0,0.01324,"T systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the output. Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world’s languages (see Resnik and Smith (2003) for discussion), therefore limiting the potential use of current SMT systems. 728 In this paper we provide a means for obtaining more reliable translation frequency estimates from small datasets. We make use of multi-parallel corpora (sentence aligned parallel texts over three or more languages). Such corpora are often created by international organisations, the United Nations (UN) being a prime example. They present a challenge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters,"
P07-1092,N07-1061,0,0.756544,"l Linguistics 2 Related Work The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying can"
P07-1092,N04-1033,0,0.0628609,"hn et al., 2003). This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to 3 treat each distribution as a feature, and learn the mixFor details see http://www.statmt.org/wpt05/ ing weights automatically. Note that we must indi- mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps (Koehn et al., 2003), and has proven a very effective feature (Zens and Ney, 2004; Foster et al., 2006). Pharaoh’s lexical weights require access to word-alignments; calculating these alignments between the source and target words in a phrase would prove difficult for a triangulated model. Therefore we use a modified lexical score, corresponding to the maximum IBM model 1 score for the phrase pair: lex(t|s) = Y 1 max p(tk |sak ) Z a (5) k where the maximisation4 ranges over all one-tomany alignments and Z normalises the score by the number of possible alignments. The lexical probability is obtained by interpolating a relative frequency estimate on the sourcetarget bitext w"
P08-1024,P06-1009,1,0.580917,"Missing"
P08-1024,D07-1007,0,0.0256813,"training sentences Figure 6. Learning curve showing that the model continues to improve as we increase the number of training sentences (development set) Our model avoids the estimation biases associated with heuristic frequency count approaches and uses standard regularisation techniques to avoid degenerate maximum likelihood solutions. Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007). Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). Finally, while in this paper we have focussed on the science of discriminative machine translation, we believe that with suitable engineering this model will advance the state-of-the-art. To do so would require integrating a language model feature into the max-translation decoding algorithm. The u"
P08-1024,P05-1033,0,0.482695,"ns are analogous to the empirical observation of maximum entropy classifiers. Given these two charts we can calculate the loglikelihood of the reference translation as the insidescore from the sentence spanning cell of the reference chart, normalised by the inside-score of the spanning cell from the full chart. The gradient is calculated as the difference of the feature expectations of the two charts. Clark and Curran (2004) provides a more complete discussion of parsing with a loglinear model and latent variables. The full derivation chart is produced using a CYK parser in the same manner as Chiang (2005), and has complexity O(|e|3 ). We produce the reference chart by synchronously parsing the source and reference sentences using a variant of CYK algorithm over two dimensions, with a time complexity of O(|e|3 |f |3 ). This is an instance of the ITG alignment algorithm (Wu, 1997). This step requires the reference translation for each training instance to be contained in the model’s hypothesis space. Achieving full coverage implies inducing a grammar which generates all observed source-target pairs, which is difficult in practise. Instead we discard the unreachable portion of the training sample"
P08-1024,J07-2003,0,0.866734,"rivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likelihood of the data, p(e|f ), where e and f are the English and foreign sentences, respectively. Our estimation method is theoretically sound, avoiding the biases of the heuristic relative frequency estimates 200 Proceedings of ACL-08: HLT, pages 200–208, c Columbus, Ohio, USA, June 2008. 2008 Association for Computati"
P08-1024,P04-1014,0,0.0239743,"ng sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate packed charts, each representing a derivation forest. The first one is the full chart over the space of possible derivations given the 2 In general, any con"
P08-1024,J07-4004,0,0.0170903,"slation given the source is the sum over all of its derivations: X pΛ (e|f ) = pΛ (d, e|f ) (3) d∈∆(e,f ) where ∆(e, f ) is the set of all derivations of the target sentence e from the source f. Most prior work in SMT, both generative and discriminative, has approximated the sum over derivations by choosing a single ‘best’ derivation using a Viterbi or beam search algorithm. In this work we show that it is both tractable and desirable to directly account for derivational ambiguity. Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing (Clark and Curran, 2007; Petrov et al., 2007). These models marginalise over derivations leading to a dependency structure and splits of non-terminal categories in a PCFG, respectively. 3.2 Training The parameters of our model are estimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and correspo"
P08-1024,W06-3105,0,0.716213,"odels must also allow for efficient training. Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006). These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets. Our model addresses all three of the above problems within a global model, without resorting to nbest lists or local independence assumptions. Furthermore, our model explicitly accounts for"
P08-1024,N04-1035,0,0.0260784,"Missing"
P08-1024,P06-1121,0,0.0566119,"source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence using a CFG, while generating the target translation from 202 the other (Chiang, 2007). All the models we present use the grammar extraction technique described in Chiang (2007), and are bench-marked against our own implementation of this hierarchical model (Hiero). Figure 3 shows a simple instance of a hierarchical grammar with two non-terminals. Note that our approach is general and could be used with other synchronous grammar transducers (e.g., Galley et al. (2006)). 3.1 A global log-linear model Our log-linear translation model defines a conditional probability distribution over the target translations of a given source sentence. A particular sequence of SCFG rule applications which produces a translation from a source sentence is referred to as a derivation, and each translation may be produced by many different derivations. As the training data only provides source and target sentences, the derivations are modelled as a latent variable. The conditional probability of a derivation, d, for a target translation, e, conditioned on the source, f , is give"
P08-1024,N07-1008,0,0.122961,"les, depending on the type of system. Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist. Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likeliho"
P08-1024,N03-1017,0,0.0355966,"uch problems as non-literal translations, poor sentence- and word-alignments. A model which exactly translates the training data will inevitably perform poorly on held-out data. This problem of over-fitting is exacerbated in discriminative models with large, expressive, feature sets. Regularisation is essential for models with more than a handful of features. ● 1e+05 derivations ● ● ● 1e+03 ● ● ● ● 5 7 9 11 13 15 sentence length Figure 1. Exponential relationship between sentence length and the average number of derivations (on a log scale) for each reference sentence in our training corpus. (Koehn et al., 2003). Second, within this framework, we model the derivation, d, as a latent variable, p(e, d|f ), which is marginalised out in training and decoding. We show empirically that this treatment results in significant improvements over a maximum-derivation model. The paper is structured as follows. In Section 2 we list the challenges that discriminative SMT must face above and beyond the current systems. We situate our work, and previous work, on discriminative systems in this context. We present our model in Section 3, including our means of training and decoding. Section 4 reports our experimental s"
P08-1024,P06-1096,0,0.694261,"Missing"
P08-1024,W02-2018,0,0.0254207,"ur model are estimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate"
P08-1024,P03-1021,0,0.184772,"x-inspired approaches. Progress within these approaches however has been less dramatic. We believe this is because these frequency count based1 models cannot easily incorporate non-independent and overlapping features, which are extremely useful in describing the translation process. Discriminative models of translation can include such features without making assumptions of independence or explicitly modelling their interdependence. However, while discriminative models promise much, they have not been shown to deliver significant gains 1 We class approaches using minimum error rate training (Och, 2003) frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features. over their simpler cousins. We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions. These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on"
P08-1024,N03-1028,0,0.00944121,"stimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate packed charts, each repr"
P08-1024,D07-1077,0,0.0181604,"Missing"
P08-1024,D07-1080,0,0.724756,"system. Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist. Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likelihood of the data, p(e|f ),"
P08-1024,2006.amta-papers.28,0,0.0147855,"examples and typically many iterations of a solver during training. While current models focus solely on efficient decoding, discriminative models must also allow for efficient training. Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006). These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets. Our model addresses all three of the above problems"
P08-1024,J97-3002,0,0.142806,"Missing"
P09-1088,N06-2013,0,0.00554308,"), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty"
P09-1088,N07-1018,0,0.0346801,"× 4.2 A Gibbs sampler for derivations Markov chain Monte Carlo sampling allows us to perform inference for the model described in 4.1 without restricting the infinite space of possible translation rules. To do this we need a method for sampling a derivation for a given sentence pair from p(d|d− ). One possible approach would be to first build a packed chart representation of the derivation forest, calculate the inside probabilities of all cells in this chart, and then sample derivations top-down according to their inside probabilities (analogous to monolingual parse tree sampling described in Johnson et al. (2007)). A problem with this approach is that building the derivation forest would take O(|f |3 |e|3 ) time, which would be impractical for long sentences. Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample. After a period of burn in, the derivations produced by the sampler will be drawn from the posterior distribution, p(d|x). The advantage of this algorithm is that we only store the current derivation for each training sentence pair (together these constitute the state of the sampler), but nev"
P09-1088,N03-1017,0,0.592202,"nous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchrono"
P09-1088,P07-2045,1,0.0237661,"iero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the"
P09-1088,J93-2003,0,0.0378426,"lel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. 1 Introduction The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and c"
P09-1088,W08-0336,0,0.019233,", containing fewer spurious off-diagonal alignments, than the heuristic (see Figure 5), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent"
P09-1088,W02-1018,0,0.720719,".ed.ac.uk Trevor Cohn∗ tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted"
P09-1088,W07-0403,0,0.847523,"tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context f"
P09-1088,W06-1606,0,0.0297598,"siderably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; Related work Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state of an unsupervised word-based aligner such 2 We include grammar based transducers, such as Chiang (2007) and Marcu et al. (2006), in our definition of phrasebased models. 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f |3 |e|3 )) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine tra"
P09-1088,J07-2003,0,0.855119,"eld of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable. Word-based models are incapable of learning translational equivalences between non-compositional phrasal units, while the algorithms used for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification. A 1 f and e are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, c Suntec, Singapore, 2-7 August 2009. 2009 ACL a"
P09-1088,P08-2007,0,0.0824084,"m aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihoo"
P09-1088,J03-1002,0,0.0161441,"that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus. Each process maintains a set of rule counts for the entire corpus and communicates the changes it has made to its section of the corpus only after sampling every sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model an"
P09-1088,W06-3105,0,0.305426,"er polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints fr"
P09-1088,P03-1021,0,0.0148685,"nd distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty passes through the data. It is clear from the training curves that the distributed approximation tracks the corpus probability of the correct sampler sufficiently closely. Th"
P09-1088,D08-1033,0,0.492948,"Missing"
P09-1088,2001.mtsummit-papers.68,0,0.0114687,"ard alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase tables and Hiero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models"
P09-1088,J97-3002,0,0.890151,", 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a nov"
P09-1088,N04-1035,0,0.0196258,"thout resorting to heuristic restrictions on the model. Initial experiments suggest that this model performs well on languages for which the monotone bias of existing alignment and heuristic phrase extraction approaches fail. These results open the way for the development of more sophisticated models employing grammars capable of capturing a wide range of translation phenomena. In future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as Hiero grammars or tree substitution grammars of the form used by Galley et al. (2004). (2007) who also observed very little empirical difference between the sampler and its distributed approximation. Tables 3 and 4 show the result on the two NIST corpora when running the distributed sampler on a single 8-core machine.5 These scores tally with our initial hypothesis: that the hierarchical structure of our model suits languages that exhibit less monotone reordering. Figure 5 shows the projected alignment of a headline from the thousandth sample on the NIST Chinese data set. The effect of the grammar based alignment can clearly be seen. Where the combination of GIZA++ and the heu"
P09-1088,C08-1136,0,0.571081,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P07-1094,0,0.0150239,"pler to permute the internal structure of the trees more easily. ... ... ... ... Figure 4: Rule insert/delete sampler. A pair of adjacent nodes in a ternary rule can be re-parented as a binary rule, or vice-versa. 4.3 Hyperparameter Inference Our model is parameterised by a vector of hyperparameters, α = (αR , αN , αP , αPE , αPF , αnull ), which control the sparsity assumption over various model parameters. We could optimise each concentration parameter on the training corpus by hand, however this would be quite an onerous task. Instead we perform inference over the hyperparameters following Goldwater and Griffiths (2007) by defining a vague gamma prior on each concentration parameter, αx ∼ Gamma(10−4 , 104 ). This hyper-prior is relatively benign, allowing the model to consider a wide range of values for the hyperparameter. We sample a new value for each αx using a log-normal distribution with mean αx and variance 0.3, which is then accepted into the distribution p(αx |d, α− ) using the MetropolisHastings algorithm. Unlike the Gibbs updates, this calculation cannot be distributed over a cluster (see Section 4.4) and thus is very costly. Therefore for small corpora we re-sample the hyperparameter after every p"
P09-1088,P08-1012,0,0.818829,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P06-1085,0,0.00708198,"with a geometric distribution in which a string of length k will be more probable than its segmentations. We define P1null as the string probability of the non-null part of the rule:  1 E null 2 P0 (e) if |f |= 0 P1 (z → he, f i) = 1 F 2 P0 (f ) if |e |= 0 The terminal translation phrase pair distribution is a hierarchical Dirichlet Process in which each phrase are independently distributed according to DPs:4 F P1P (z → he, f i) = φE z (e) × φz (f ) PE φE , P0E ) z ∼ DP(α 4 This prior is similar to one used by DeNero et al. (2008), who used the expected table count approximation presented in Goldwater et al. (2006). However, Goldwater et al. (2006) contains two major errors: omitting P0 , and using the truncated Taylor series expansion (Antoniak, 1974) which fails for small αP0 values common in these models. In this work we track table counts directly. 785 ... ... ... ... ... ... ... ... ... Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a solid line is a null alignment. ... ... ... ... the Split/Join operator in Figure 3. In order for this opera"
P09-1088,P02-1040,0,\N,Missing
P09-1088,2005.iwslt-1.1,0,\N,Missing
P09-2085,D08-1033,0,0.0317874,"Missing"
P09-2085,P07-1035,0,0.0288854,"present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼ DP(α0 , P0 ) ∼G where"
P09-2085,P06-1085,1,0.89977,"we present an efficient method for sampling from the HDP (and related models, such as the hierarchical PitmanYor process) that considerably decreases the memory footprint of such models as compared to the naive implementation. As we have noted, the issues described in this paper apply to models for various kinds of NLP tasks; for concreteness, we will focus on n-gram language modeling for the remainder of the paper, closely following the presentation in GGJ06. The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring considerable book-keeping. Goldwater et al. (2006a) presented an approximation which significantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate. We present an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 2 Introduction The Chinese Restaurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram"
P09-2085,D07-1072,0,0.0157343,"aurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼"
P10-2042,N07-1018,0,0.61095,"ocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently converges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy. 1 In this paper we present a blocked MetropolisHasting sampler for learning a TSG, similar to Johnson et al. (2007). The sampler jointly updates all the substitution variables in a tree, making much larger moves than the local single-variable sampler. A critical issue when developing a Metroplis-Hastings sampler is choosing a suitable proposal distribution, which must have the same support as the true distribution. For our model the natural proposal distribution is a MAP point estimate, however this cannot be represented directly as it is infinitely large. To solve this problem we develop a grammar transformation which can succinctly represent an infinite TSG in an equivalent finite Context Free Grammar (C"
P10-2042,P09-1088,1,0.822732,"ng model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is difficult to extend the local Gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints. Table 2: Development F1 scores using the truncated parsing algorithm and the novel grammar transform algorithm for"
P10-2042,D09-1037,1,0.8459,"hical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is difficult to extend the local Gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints. Table 2: Development F1 scores using the truncated parsing algorithm and the novel grammar transform algorithm for four different training"
P10-2042,N09-1062,1,0.678649,"onvergence (a.k.a. poor mixing). The sampler can get easily stuck because many locally improbable decisions are required to escape from a locally optimal solution. This problem manifests itself both locally to a sentence and globally over the training sample. The net result is a sampler that is non-convergent, overly dependent on its initialisation and cannot be said to be sampling from the posterior. Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently converges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm"
P10-2042,D08-1033,0,0.107486,"Missing"
P10-2042,P06-1085,0,0.0191835,"mation can implicitly represent an exponential space of tree fragments efficiently, allowing us to build a sampler with considerably better mixing properties than a local Gibbs sampler. The same technique was also shown to improve the parsing algorithm. These improvements are in no way limited to our particular choice of a TSG parsing model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is diff"
P10-2042,N09-1036,0,0.0191017,"20000 −325000 log likelihood −310000 −305000 ment over our earlier 84.0 (Cohn et al., 2009) although still well below state-of-the-art parsers. We conjecture that the performance gap is due to the model using an overly simplistic treatment of unknown words, and also a further mixing problems with the sampler. For the full data set the counts are much larger in magnitude which leads to stronger modes. The sampler has difficulty escaping such modes and therefore is slower to mix. One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000). Another way is to use a variational approximation instead of MCMC sampling (Wainwright and Jordan, 2008). −330000 Block maximal init Block minimal init Local minimal init Local maximal init 0 100 200 300 400 500 iteration Figure 4: Training likelihood vs. iteration. Each sampling method was initialised with both minimal and maximal elementary trees. Training Local minimal init Local maximal init Blocked minimal init Blocked maximal init truncated 77.63 77.19 77.98 77.67 5 transform 77.98 77.71 78.40 78.24 Discussion We have demonstrated how our grammar tr"
P11-1087,N10-1083,0,0.246454,"Missing"
P11-1087,J92-4003,0,0.42141,"based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. 1 Introduction Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications in human language learning and for developing portable language processing systems. Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al. (1992) approach (Christodoulopoulos et al., 2010). One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions. In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages. Moreover, the performance of our unsupervised model approaches 865 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk that of many"
P11-1087,W06-2920,0,0.0103196,"hyperparameter inference is that there are no user tunable parameters in the model, an important feature that we believe helps explain its consistently high performance across test settings. 4 Experiments We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains. For our core English experiments we report results on the entire Penn. Treebank (Marcus et al., 1993), while for other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall. The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. The 1HMM sampler converges more quickly so we use two hundred samples for these models. All reported results are the"
P11-1087,P96-1041,0,0.0591174,"ons, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al., 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). The model is depicted in the plate diagram in Figure 1. At its centre is a standard trigram HMM, which generates a sequence of tags and words, tl |tl−1 , tl−2 , T ∼ Ttl−1 ,tl−2 wl |tl , E Tij Bj U ∼ Etl . 867 w1 w2 w3 Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. The trigram transition distribution, Tij , is drawn from a hierarchical PYP prior which backs off to a bigram Bj and then a unigram U distribution, Tij |aT , bT , Bj ∼ P"
P11-1087,D10-1056,0,0.825696,"ierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. 1 Introduction Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications in human language learning and for developing portable language processing systems. Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al. (1992) approach (Christodoulopoulos et al., 2010). One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions. In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages. Moreover, the performance of our unsupervised model approaches 865 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk that of many existing semi-supervised systems, despite"
P11-1087,E03-1009,0,0.746871,"MM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not include any hand tuned language specific features. Inspired by previous successful approaches (Brown et al., 1992), we develo"
P11-1087,D08-1036,0,0.224679,"Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampli"
P11-1087,P07-1094,0,0.748421,"ce University of Sheffield T.Cohn@dcs.shef.ac.uk that of many existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approac"
P11-1087,P06-1085,0,0.722485,"emi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not inc"
P11-1087,N06-1041,0,0.316653,"incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from"
P11-1087,D07-1031,0,0.827909,"Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence be"
P11-1087,D10-1083,0,0.471822,"in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff"
P11-1087,N10-1082,0,0.0979839,"Missing"
P11-1087,J93-2004,0,0.0390742,"amma(10, 0.1)). All the hyper-parameters are resampled after every 5th sample of the corpus. The result of this hyperparameter inference is that there are no user tunable parameters in the model, an important feature that we believe helps explain its consistently high performance across test settings. 4 Experiments We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains. For our core English experiments we report results on the entire Penn. Treebank (Marcus et al., 1993), while for other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall. The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. T"
P11-1087,E99-1010,0,0.854668,"isation in language models. Brown et al. (1992) presented a simple first-order HMM which restricted word types to always be generated from the same class. Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the significant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999)1 has become a standard part of statistical machine translation systems. The HMM ignores orthographic information, which is often highly indicative of a word’s partof-speech, particularly so in morphologically rich languages. For this reason Clark (2003) extended Brown et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate"
P11-1087,P09-1057,0,0.0494803,". Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008)."
P11-1087,P05-1044,0,0.108141,"et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical d"
P11-1087,P06-1124,0,0.738554,"y existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generati"
P13-1004,P11-1022,0,0.0273313,"such as professional translators. Examples of applications of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort and time to correct than translating from scratch (Specia et al., 2009), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). QE is generally addressed as a machine learning task using a variety of linear and kernel-based regression or classification algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). While initial work used annotations derived Our approach is based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a kernelised Bayesian non-parametric learning framework. We develop multi-task learning mo"
P13-1004,P02-1040,0,0.0972589,"Missing"
P13-1004,C04-1046,0,0.116879,"Missing"
P13-1004,W11-2103,0,0.0227561,"previously post-edited translation by a fourth translator. In an attempt to accommodate for systematic biases among annotators, the final effort score was computed as the weighted average between the three PE-effort scores, with more weight given to the judges with higher standard deviation from their own mean score. This resulted in scores spread more evenly in the [1, 5] range. WPTP12: This dataset was distributed by Koponen et al. (2012). It contains 299 English sentences translated into Spanish using two or more of eight MT systems randomly selected from all system submissions for WMT11 (Callison-Burch et al., 2011). These MT systems range from online and customised SMT systems to commercial rule-based systems. Translations were post-edited by humans while time was recorded. The labels are the number of seconds spent by a translator editing a sentence normalised by source sentence length. The post-editing was done by eight native speakers of Spanish, including five professional translators and three translation students. Only 20 translations were edited by all eight annotators, with the remaining translations randomly distributed amongst them. The resulting dataset contains 1, 624 instances, which were r"
P13-1004,W12-3102,1,0.890395,"Missing"
P13-1004,quirk-2004-training,0,0.0538761,"listic framework and have been successfully adapted for multi-task learning in a number of ways, e.g., by learning multi-task correlations (Bonilla et al., 2008), modelling per-task variance (Groot et al., 2011) or perannotator biases (Rogers et al., 2010). Our method builds on the work of Bonilla et al. (2008) by 1 We are not strictly the first, Polajnar et al. (2011) used GPs for text classification. 33 from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al., 2002) at training time, it soon became clear that human labels result in significantly better models (Quirk, 2004). Current work at sentence level is thus based on some form of human supervision. As typical of subjective annotation tasks, QE datasets should contain multiple annotators to lead to models that are representative. Therefore, work in QE faces all common issues regarding variability in annotators’ judgements. The following are a few other features that make our datasets particularly interesting: • In order to minimise annotation costs, translation instances are often spread among annotators, such that each instance is only labelled by one or a few judges. In fact, for a sizeable dataset (thousa"
P13-1004,P07-1033,0,0.0527429,"Missing"
P13-1004,D08-1027,0,0.174664,"Missing"
P13-1004,P10-1064,0,0.0227248,"Missing"
P13-1004,P10-1063,0,0.0150805,"at providing an estimate on the quality of each translated segment – typically a sentence – without access to reference translations. Work in this area has become increasingly popular in recent years as a consequence of the widespread use of MT among realworld users such as professional translators. Examples of applications of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort and time to correct than translating from scratch (Specia et al., 2009), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). QE is generally addressed as a machine learning task using a variety of linear and kernel-based regression or classification algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared t"
P13-1004,2009.eamt-1.5,1,0.906119,"epartment of Computer Science University of Sheffield Sheffield, United Kingdom {t.cohn,l.specia}@sheffield.ac.uk Abstract – and yet equally valid – truths. Particularly in highly subjective annotation tasks, the differences between annotators cannot be captured by simple models such as scaling all instances of a certain annotator by a factor. They can originate from a number of nuanced aspects. This is the case, for example, of annotations on the quality of sentences generated using machine translation (MT) systems, which are often used to build quality estimation models (Blatz et al., 2004; Specia et al., 2009) – our application of interest. In addition to annotators’ own perceptions and expectations with respect to translation quality, a number of factors can affect their judgements on specific sentences. For example, certain annotators may prefer translations produced by rulebased systems as these tend to be more grammatical, while others would prefer sentences produced by statistical systems with more adequate lexical choices. Likewise, some annotators can be biased by the complexity of the source sentence: lengthy sentences are often (subconsciously) assumed to be of low quality by some annotato"
P13-1004,2012.amta-wptp.2,1,0.102762,"ies an estimated percentage of the MT output that needs to be corrected. The post-editing effort scores were produced independently by three professional translators based on a previously post-edited translation by a fourth translator. In an attempt to accommodate for systematic biases among annotators, the final effort score was computed as the weighted average between the three PE-effort scores, with more weight given to the judges with higher standard deviation from their own mean score. This resulted in scores spread more evenly in the [1, 5] range. WPTP12: This dataset was distributed by Koponen et al. (2012). It contains 299 English sentences translated into Spanish using two or more of eight MT systems randomly selected from all system submissions for WMT11 (Callison-Burch et al., 2011). These MT systems range from online and customised SMT systems to commercial rule-based systems. Translations were post-edited by humans while time was recorded. The labels are the number of seconds spent by a translator editing a sentence normalised by source sentence length. The post-editing was done by eight native speakers of Spanish, including five professional translators and three translation students. Onl"
P13-1004,2011.eamt-1.12,1,0.862114,"ators can be biased by the complexity of the source sentence: lengthy sentences are often (subconsciously) assumed to be of low quality by some annotators. An extreme case is the judgement of quality through post-editing time: annotators have different typing speeds, as well as levels of expertise in the task of post-editing, proficiency levels in the language pair, and knowledge of the terminology used in particular sentences. These variations result in time measurements that are not comparable across annotators. Thus far, the use of post-editing time has been done on an per-annotator basis (Specia, 2011), or simply averaged across multiple translators (Plitt and Masselot, 2010), both strategies far from ideal. Overall, these myriad of factors affecting quality judgements make the modelling of multiple annotators a very challenging problem. This problem is exacerbated when annotations are provided by non-professional annotators, e.g., through crowdsourcing – a common strategy used Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpret"
P13-1033,P09-1088,1,0.917596,"target word take and change the aligned source word from prends to Je, then the items for which we need to decrement and increment the counts by one are shown in Table 1 and the posterior probability corresponding to the new alignment is the product of the hierarchical PYP probabilities of all increment items divided by the probability of the fertility of prends being single. Maintaining the current state of the hPYP as events are incremented and decremented is nontrivial and the naive approach requires significant book-keeping and has poor runtime behaviour. For this we adopt the approach of Blunsom et al. (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. Briefly, this algorithm samples the table assignment during the increment and decrement operations, which is then used to maintain aggregate table statistics. This can be done efficiently and without the need for explicit table assignment tracking. 4.1 4.2 Parallel Implementation As mentioned above, the hierarchical PYP takes into consideration a rich history to evaluate the probabilities of translation decisions. But this leads to difficulties when applying the"
P13-1033,P03-1021,0,0.0618165,"aging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 Dev 45.78 49.13 49.68 51.32 for the test set we use the IWSLT 2005 test set. The language model is a 3-gram language model trained using the SRILM toolkit (Stolcke, 2002) on the English side of the training data. Because the data set is small, we performed Gibbs sampling on a single processor. First we check the effect of the model factors jump and fertility. Both emission and finish factors are indispensable to the generative translation process, and consequently these two factors are included in all runs. Tab"
P13-1033,P09-2085,1,0.919724,"target word take and change the aligned source word from prends to Je, then the items for which we need to decrement and increment the counts by one are shown in Table 1 and the posterior probability corresponding to the new alignment is the product of the hierarchical PYP probabilities of all increment items divided by the probability of the fertility of prends being single. Maintaining the current state of the hPYP as events are incremented and decremented is nontrivial and the naive approach requires significant book-keeping and has poor runtime behaviour. For this we adopt the approach of Blunsom et al. (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. Briefly, this algorithm samples the table assignment during the increment and decrement operations, which is then used to maintain aggregate table statistics. This can be done efficiently and without the need for explicit table assignment tracking. 4.1 4.2 Parallel Implementation As mentioned above, the hierarchical PYP takes into consideration a rich history to evaluate the probabilities of translation decisions. But this leads to difficulties when applying the"
P13-1033,P02-1040,0,0.0885467,"lignments using the grow-diag-final heuristic. Using multiple samples in this way constitutes Monte Carlo averaging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 Dev 45.78 49.13 49.68 51.32 for the test set we use the IWSLT 2005 test set. The language model is a 3-gram language model trained using the SRILM toolkit (Stolcke, 2002) on the English side of the training data. Because the data set is small, we performed Gibbs sampling on a single processor. First we check the effect of the model factors jump and fertility. Both emission and finish factors are indispensable to th"
P13-1033,J93-2003,0,0.061,"xplicitly in a number of previous approaches, in grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit only as a preprocessing step for inferring word level alignments from sentence-aligned parallel corpora. They combine a number of factors, including distortion and fertility, which have been shown to improve word-alignment and translation performance over simpler models. Our approach is similar to these works, as we also develop a word-based model, and explicitly consider similar translation decisions, alignment jumps and fertility. We extend these works in two important respects: 1) whil"
P13-1033,P05-1033,0,0.533882,"translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the 51st Annual Meeting of the Association fo"
P13-1033,P11-1105,0,0.247736,"e translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a sequence model of translation including reordering. 3 Model Given a source sentence, our model infers a latent derivation which produces a target translation and meanwhile gives a word alignment between the source and the target. We consider a process in which the target string is generated using a left-to-right order, similar to the decoding strategy used by phrase-based machine translation systems (Koehn et al., 2003). During this process we maintain a position in the source sentence, which can jump around to allow for different sentence ordering in the target vs. source lang"
P13-1033,P06-1124,0,0.309461,"s the use of discontinuous phrases and leads to poor generalisation to unseen data (where large phrases tend not to match). In this paper we propose a new model to drop the independence assumption, by instead modelling correlations between translation decisions, which we use to induce translation derivations from aligned sentences (akin to word alignment). We develop a Markov model over translation decisions, in which each decision is conditioned on previous n most recent decisions. Our approach employs a sophisticated Bayesian non-parametric prior, namely the hierarchical Pitman-Yor Process (Teh, 2006; Teh et al., 2006) to represent backoff from larger to smaller contexts. As a result, we need only use very simple translation units – primarily single words, but can still describe complex multi-word units through correlations between their component translation decisions. We further decompose the process of generating each target word into component factors: finishing the translating, jumping elsewhere in the source, emitting a target word and deciding the fertility of the source words. Overall our model has the following features: 1. enabling model parameters to be shared between similar t"
P13-1033,N10-1140,0,0.0442751,"ng context. In contrast to these approaches which primarily address the decoding problem, we focus on the learning problem of inferring alignments from parallel sentences. Additionally, we develop a full generative model using a Bayesian prior, and incorporate additional factors besides lexical items, namely jumps in the source and word fertility. Another aspect of this paper is the implicit support for phrase-pairs that are discontinous in the source language. This idea has been developed explicitly in a number of previous approaches, in grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit onl"
P13-1033,P11-1086,0,0.0569197,"wo important respects: 1) while they assume a simple parameterisation by making iid assumptions about each translation factor, we instead allow for rich correlations by modelling sequences of translation decisions; and 2) we develop our model in the Bayesian framework, using a hierarchical PitmanYor Process prior with rich backoff semantics between high and lower order sequences of translation decisions. Together this results in a model with rich expressiveness but can still generalize well to unseen data. More recently, a number of authors have proposed Markov models for machine translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a"
P13-1033,P06-1121,0,0.0742409,"ecisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the 51st Annual Meeting of the Association for Computational Lingu"
P13-1033,C96-2141,0,0.406643,"grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit only as a preprocessing step for inferring word level alignments from sentence-aligned parallel corpora. They combine a number of factors, including distortion and fertility, which have been shown to improve word-alignment and translation performance over simpler models. Our approach is similar to these works, as we also develop a word-based model, and explicitly consider similar translation decisions, alignment jumps and fertility. We extend these works in two important respects: 1) while they assume a simple parameterisation by making"
P13-1033,N09-1036,0,0.0335699,"each iteration. In this way each process uses slightly “out-of-date” counts, but can process the data independently of the other processes. We found that this approximation improved the runtime significantly with no noticeable effect on accuracy. posterior probability p(ai = j|t−i , o−i ) ∝ the discount parameter, we employ a uniform Beta distribution ax ∼ Beta(1, 1) while for the strength parameter, we employ a vague Gamma distribution bx ∼ Gamma(10, 0.1). All restaurants in the same level share the same hyper-prior and the hyper-parameters for all levels are resampled using slice sampling (Johnson and Goldwater, 2009) every 10 iterations. 5 Experiments In principle our model could be directly used as a MT decoder or as a feature in a decoder. However in this paper we limit our focus to inducing word alignments, i.e., by using the model to infer alignments which are then used in a standard phrasebased translation pipeline. We leave full decoding for later work, which we anticipate would further improve performance by exploiting gapping phrases and other phenomena that implicitly form part of our model but are not represented in the phrase-based decoder. Decoding under our model would be straight-forward in"
P13-1033,N03-1017,0,0.648272,"s word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the"
P13-1033,P07-2045,0,0.0194756,"for the first 1000 iterations, after which we ran a further 500 iterations selecting every 50th sample. A phrase table was constructed using these 10 sets of multiple alignments after combining each pair of directional alignments using the grow-diag-final heuristic. Using multiple samples in this way constitutes Monte Carlo averaging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 Dev 45.78 49.13 49.68 51.32 for the test set we use the IWSLT 2005 test set. The language model is a 3-gram language model trained using the SRILM toolkit (Stolcke, 2002) on the English side of"
P13-1033,N12-1005,0,0.0632862,"ich expressiveness but can still generalize well to unseen data. More recently, a number of authors have proposed Markov models for machine translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a sequence model of translation including reordering. 3 Model Given a source sentence, our model infers a latent derivation which produces a target translation and meanwhile gives a word alignment between the source and the target. We consider a process in which the target string is generated using a left-to-right order, similar to the decoding strategy used by phrase-based machine translation systems (Koehn et al., 2003). During this proc"
P13-1033,P06-1077,0,0.0277194,"decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 333–3"
P13-1033,J03-1002,0,0.0444765,"Missing"
P13-1077,N07-1018,0,0.55118,"provements over competitive baseline systems. 1 This paper develops a phrase-based translation model which aims to address the above shortcomings of the phrase-based translation pipeline. Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora. The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a ‘cache’ learning each production and its complete yield, which in turn is recursively composed of its child constituents. This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting. Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, whi"
P13-1077,N10-1028,1,0.911056,"Missing"
P13-1077,N03-1017,0,0.289824,"parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central"
P13-1077,P09-1088,1,0.944279,"ey are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. We are not the first to consider this idea; Neubig et al. (2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this"
P13-1077,P07-2045,0,0.00926249,"the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Associa"
P13-1077,J93-2003,0,0.0720143,"ased approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader"
P13-1077,D12-1021,0,0.489339,"Missing"
P13-1077,P06-2014,0,0.020981,"ages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-w"
P13-1077,W07-0403,0,0.0200011,"synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual"
P13-1077,W02-1018,0,0.0608507,"Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose As mentioned above, ours is not the first work attempting to generalise adaptor grammars for machine translation; (Neubig et al., 2011) also developed a similar approach based around ITG using a Pitman-Yor Process prior. Our approach improves upon theirs in terms of the model and inference, and critically, this is borne o"
P13-1077,J07-2003,0,0.135124,"ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Me"
P13-1077,W06-1606,0,0.0289279,"en the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Association for Computationa"
P13-1077,P10-1147,0,0.0127123,"te their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to th"
P13-1077,P11-1064,0,0.755949,"780–790, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. We are not the first to consider this idea; Neubig et al. (2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this paper provides a more rigorous and theoretically sound method. Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Ne"
P13-1077,P09-1104,0,0.0157684,"ate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM mo"
P13-1077,P03-1021,0,0.072027,"Section 2.14 In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in all experiments. We train 3-gram language models using modified Kneser-Ney smoothing. For AR-EN experiments the language model is trained on English data as (Blunsom et al., 2009a), and for FA-EN and UREN the English data are the target sides of the bilingual training data. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. 500 iteration Figure 1: Training progress on the UR-EN corpus, showing the posterior probability improving with each full sampling iteration. Different colours denote independent sampling runs. 1e−03 1e−05 time (s) 1e−01 ● ●● ●● ●●●● ● ●● ●● ●● ●● ●● ● ●● ● ● ● ● ●● ● ● ●● ●● ●● ● ● ●● ●● ●● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ●● ●●●● ●● ● ●● ● ● ●● ● ● ●● ● ● ● ●● ● ● ● ●●●● ●● ●● ● ● ● ● ● ● ● ●● ●● ● ● ● ●● ●● ●● ● ● ●● ●●●● ●● ● ● ● ● ● ●● ●● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ●●● ● ●●● ●● ● ● ● ● ● ●● ●"
P13-1077,N10-1014,0,0.0165898,"roductions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002"
P13-1077,D08-1012,0,0.0211363,"vement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from"
P13-1077,P06-1124,0,0.109497,"rminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word"
P13-1077,J97-3002,0,0.533801,"daptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together. Related Work Inversion transduction grammar (or ITG) (Wu, 1997) is a well studied synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang"
P13-1077,P05-1059,0,0.0265561,"1997) is a well studied synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based"
P13-1077,P08-1012,0,0.0209672,"ormalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with"
P13-1098,W11-3702,0,0.0245721,"tweets, it may be useful to know that ‘Johannes Voggenhuber’ (who receives a positive comment for his book) and ‘Peter Pilz’ (whose comment is ques¨ ‘Krone’ (or Krotioned) are members of GRU, nen Zeitung) is the major newspaper in Austria10 ¨ is labelled as a far right party, someand that FPO thing that may cause various reactions from ‘Human Rights’ organisations. 5 Related Work The topic of political opinion mining from Social Media has been the focus of various recent research works. Several papers have presented methods that aim to predict the result of an election (Tumasjan et al., 2010; Bermingham and Smeaton, 2011) or to model voting intention and other kinds of socio-political polls (O’Connor et al., 2010; Lampos, 2012). Their common feature is a methodology based on a meta-analysis of word frequencies using off-the-shelf sentiment tools such as LIWC (Pennebaker et al., 2007) or Senti-WordNet (Esuli and Sebastiani, 2006). Moreover, the proposed techniques tend to incorporate posting volume figures as well as handcrafted lists of words relevant to the task (e.g., names of politicians or parties) in order to filter the content successfully. Such papers have been criticised as their methods do not general"
P13-1098,esuli-sebastiani-2006-sentiwordnet,0,0.00436368,"various reactions from ‘Human Rights’ organisations. 5 Related Work The topic of political opinion mining from Social Media has been the focus of various recent research works. Several papers have presented methods that aim to predict the result of an election (Tumasjan et al., 2010; Bermingham and Smeaton, 2011) or to model voting intention and other kinds of socio-political polls (O’Connor et al., 2010; Lampos, 2012). Their common feature is a methodology based on a meta-analysis of word frequencies using off-the-shelf sentiment tools such as LIWC (Pennebaker et al., 2007) or Senti-WordNet (Esuli and Sebastiani, 2006). Moreover, the proposed techniques tend to incorporate posting volume figures as well as handcrafted lists of words relevant to the task (e.g., names of politicians or parties) in order to filter the content successfully. Such papers have been criticised as their methods do not generalise when applied on different data sets. According to the work in (Gayo-Avello et al., 2011), the methods presented in (Tumasjan et al., 2010) and (O’Connor et al., 2010) failed to predict the result of US congressional elections in 2009. We disagree with the arguments supporting the statement “you cannot predic"
P13-2097,2010.jec-1.5,1,0.743001,"ation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3.2 post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-b"
P13-2097,2009.mtsummit-papers.16,1,0.933478,"Missing"
P13-2097,2011.mtsummit-papers.58,1,0.843038,"Missing"
P13-2097,2011.eamt-1.12,1,0.81515,"e study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3.2 post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previ"
P13-2097,W06-2209,0,0.034761,"g time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine learning algorithms, we refer the reader to a recent shared task on the topic (Callison-Burch et al., 2012). Previous work use supervised learning methods (“passive learning” following the AL terminology) to train QE models. On the other hand, AL has been successfully used in a number of natural language applications such as text classification (Lewis and Gale, 1994), named entity recognition (Vlachos, 2006) and parsing (Baldridge and Osborne, 2004). See Olsson (2009) for an overview on AL for natural language processing as well as a comprehensive list of previous work. 3 3.1 Query Methods The core of an AL setting is how the learner will gather new instances to add to its training data. In our setting, we use a pool-based strategy, where the learner queries an instance pool and selects the best instance according to an informativeness measure. The learner then asks an “oracle” (in this case, the human expert) for the true label of the instance and adds it to the training data. Query methods use"
P13-2097,quirk-2004-training,0,0.0356732,"nd subject to inconsistencies due to the subjectivity of the task. To avoid inconsistencies because of disagreements among annotators, it is often recommended that a QE model is trained 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543"
P13-2097,D08-1112,0,0.0661861,"uery Methods The core of an AL setting is how the learner will gather new instances to add to its training data. In our setting, we use a pool-based strategy, where the learner queries an instance pool and selects the best instance according to an informativeness measure. The learner then asks an “oracle” (in this case, the human expert) for the true label of the instance and adds it to the training data. Query methods use different criteria to predict how informative an instance is. We experiment with two of them: Uncertainty Sampling (US) (Lewis and Gale, 1994) and Information Density (ID) (Settles and Craven, 2008). In the following, we denote M (x) the query score with respect to method M . According to the US method, the learner selects the instance that has the highest labelling variance according to its model: Experimental Settings U S(x) = V ar(y|x) Datasets The ID method considers that more dense regions of the query space bring more useful information, leveraging the instance uncertainty and its similarity to all the other instances in the pool: !β U 1 X ID(x) = V ar(y|x) × sim(x, x(u) ) U We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 25"
P13-2097,2006.amta-papers.25,0,0.0422342,"work on devising more reliable and less subjective quality labels. Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic metric (e.g. NIST (Doddington, 2002)) as the quality label. Quirk (2004) showed that small datasets manually annotated by humans for quality can result in models that outperform those trained on much larger, automatically labelled sets. Since quality labels are subjective to the annotators’ judgements, Specia and Farzindar (2010) evaluated the performance of QE models using HTER (Snover et al., 2006) as the quality score, i.e., the edit distance between the MT output and its post-edited version. Specia (2011) compared the performance of models based on labels for 543 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543–548, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3.2 post-editing effort, post-editing time, and HTER. In terms of learning algorithms, by and large most approaches use Support Vector Machines, particularly regression-based approaches. For an overview on various feature sets and machine lea"
P13-2097,W04-3202,0,\N,Missing
P13-2097,W12-3102,1,\N,Missing
P13-2097,P07-2045,0,\N,Missing
P13-2097,C04-1046,0,\N,Missing
P13-4014,P11-1022,0,0.0616038,"ent; • Selecting among alternative translations produced by different MT systems; • Deciding whether the translation can be used for self-training of MT systems. Work in QE for MT started in the early 2000’s, inspired by the confidence scores used in Speech Recognition: mostly the estimation of word posterior probabilities. Back then it was called confi79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engine"
P13-4014,C04-1046,0,0.524049,"Missing"
P13-4014,W12-3102,1,0.870241,"then it was called confi79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labourintensive. Different language pairs or optimisation against specific quality scores (e.g., post-editing time vs translation adequacy) can benefit from very different feature sets. Q U E ST, our framework for quality estimation, provides a wide range of feature extractors fr"
P13-4014,P10-1064,0,0.0219866,"Missing"
P13-4014,P02-1040,0,0.0950837,"Missing"
P13-4014,P10-1063,0,0.045366,"also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benc"
P13-4014,2009.eamt-1.5,1,0.793882,"roblem by using more complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools"
P13-4014,2011.eamt-1.12,1,0.842332,"complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part"
P14-2025,N13-1073,0,0.294569,"bstract A modification of a reparameterisation of IBM Model 2 is presented, which makes the model more flexible, and able to model a preference for aligning to words to either the right or left, and take into account POS tags on the target side of the corpus. We show that this extension has a very small impact on training times, while obtaining better alignments in terms of BLEU scores. 1 man the seen had He Hij Introduction Word alignment is at the basis of most statistical machine translation. The models that are generally used are often slow to train, and have a large number of parameters. Dyer et al. (2013) present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4. While this model is very effective, it also has a very low number of parameters, and as such doesn’t have a large amount of expressive power. For one thing, it forces the model to consider alignments on both sides of the diagonal equally likely. However, it isn’t clear that this is the case, as for some languages an alignment to earlier or later in the sentence (above or below the diagonal) could be common, due to word order differences. For example, when aligning to Dut"
P14-2025,P07-2045,0,0.00557321,"he models here described and Model 4 training times are given for comparison. Note that the times for the models optimizing only λ and γ, and the model only optimizing ω still calculate the derivatives for the other parameters, and so could be made to be faster than here displayed. For both the BLEU and AER results, the alignments are generated in both directions, and symmetrised using the grow-diag-final-and heuristic, which in preliminary tests had shown to do best in terms of AER. For the comparisons of translation quality, the models are trained up using a phrase-based translation system (Koehn et al., 2007) that used the above listed models to align the data. Language models were augmented with data outside of the corpora for Chinese-English (200M words total) and Arabic-English (100M words total). Test sets for Chinese are MT02, MT03, MT06 and MT08, for Arabic they were MT05, MT06 and MT08, and for French they were the newssyscomb2009 data and the newstest 2009-2012 data. The results are listed in Table 31 . BLEU scores for Arabic-English and Chinese-English are computed with multiple references, while those for French-English are against a single reference. Although the different models made l"
P14-2025,J03-1002,0,0.0120567,"like for only one direction to be more likely. had de n ma en i gez Figure 1: Visualization of aligned sentence pair in Dutch and English, darker shaded squares have a higher alignment probability under the model, a circle indicates a correct alignment. The English sentence runs from bottom to top, the Dutch sentence left to Right. In some cases it could be that the prior probability for a word alignment should be off the diagonal. Furthermore, it is common in word alignment to take word classes into account. This is commonly implemented for the HMM alignment model as well as Models 4 and 5. Och and Ney (2003) show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER). This is not implemented for Model 2, as it already has an alignment model that is dependent on both source and target length, and the position in both sentences, and adding a dependency to word classes would make the the Model even more prone to overfitting than it already is. However, using the reparameterization in (Dyer et al., 2013) would leave the model simple enough even with a relatively large amount of word classes. Figure 2 shows an example of how the model extensions could benefit word alignm"
P14-2025,W00-1308,0,0.0826872,"e 1: Token counts and average amount of time to train models (and separately training time for Model 4) on original corpora in one direction in hours, by corpus. Table 2: AER results on Chinese-English and French-English data sets used as in (Dyer et al., 2013), with stepsize for updates to λ and γ during gradient ascent is 1000, and that for ω is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration. Both λ and γ are initialised to 6, and ω is initialised to 0. For these experiments the pos and pos & split use POS tags generated using the Stanford POS tagger (Toutanova and Manning, 2000), using the supplied models for all of the languages used in the experiments. For comparison, Model 4 is trained for 5 iterations using 5 iterations each of Model 1 and Model 3 as initialization, using GIZA++ (Och and Ney, 2003). Model Fr-En Ar-En Zh-En Original Split Pos Offset Pos & Split Pos & Split & Offset Model 4 25.9 25.9 25.9 26.0 26.0 26.0 26.8 43.8 43.2 43.9 43.9 44.1 44.2 43.9 32.8 32.8 32.9 32.8 33.2 33.3 32.4 Table 3: BLEU results on Chinese-English and French-English data sets For the comparisons in AER, the corpora are used as-is, but for the BLEU comparisons, sentences longer t"
P14-2025,J93-2003,0,\N,Missing
P14-6001,P13-1004,1,0.844479,"on Twitter based on a range or profile and word features (Lampos et al., 2014). We exemplify how to identifying which features are best for predicting user impact by optimising the hy2 http://github.com/SheffieldML/GPy 2 perparameters (e.g. RBF kernel length-scales) using Automatic Relevance Determination (ARD). This basically gives a ranking in importance of the features, allowing interpretability of the models. Switching to a multi-task regression setting, we present an application to Machine Translation Quality Estimation. Our method shows large improvements over previous state-of-the-art (Cohn and Specia, 2013). Concepts in automatic kernel selection are exemplified in an extrapolation regression setting, where we model word time series in Social Media using different kernels (Preot¸iucPietro and Cohn, 2013). The Bayesian evidence helps to select the most suitable kernel, thus giving an implicit classification of time series. In the final section of the tutorial we give a brief overview of advanced topics in the field of GPs. First, we look at non-conjugate likelihoods for modelling classification, count and rank data. This is harder than regression, as Bayesian posterior inference can no longer be"
P14-6001,E14-1043,1,0.871332,"Missing"
P14-6001,D13-1100,1,0.776314,"Missing"
P14-6001,2013.mtsummit-papers.21,0,0.0488352,"Missing"
P14-6001,P13-4014,1,\N,Missing
P14-6001,P07-1033,0,\N,Missing
P15-2030,D13-1170,0,0.0325798,"f.ac.uk Trevor Cohn University of Melbourne, Australia t.cohn@unimelb.edu.au Abstract features can be captured using explicit feature engineering, this process is tedious, limited in scope (e.g., to conjunctions) and – as we show here – can be dramatically improved by representational learning as part of a non-linear model. In this paper, we propose an artificial neural network (ANN) for modelling text regression. In language processing, ANNs were first proposed for probabilistic language modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have mainly focused on ‘big data’ problems with plentiful training examples. Given their large numbers o"
P15-2030,N10-1038,0,0.509057,"Missing"
P15-2030,P14-1062,0,0.285139,"University of Sheffield, UK z.bitvai@shef.ac.uk Trevor Cohn University of Melbourne, Australia t.cohn@unimelb.edu.au Abstract features can be captured using explicit feature engineering, this process is tedious, limited in scope (e.g., to conjunctions) and – as we show here – can be dramatically improved by representational learning as part of a non-linear model. In this paper, we propose an artificial neural network (ANN) for modelling text regression. In language processing, ANNs were first proposed for probabilistic language modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have mainly focused on ‘big data’ problems with plentiful training exa"
P15-2030,D14-1181,0,0.101925,"e modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have mainly focused on ‘big data’ problems with plentiful training examples. Given their large numbers of parameters, often in the millions, one would expect that such models can only be effectively learned on very large datasets. However we show here that a complex deep convolution network can be trained on about a thousand training examples, although careful model design and regularisation is paramount. We consider the problem of predicting the future box-office takings of movies based on reviews by movie critics and movie attributes. Our approach is based on the method and dat"
P15-2030,N09-1031,0,0.141259,"Missing"
P15-2030,P13-1098,1,0.467425,"Missing"
P15-2030,E14-1043,1,0.890273,"Missing"
P15-2085,D14-1190,1,0.88248,"Missing"
P15-2085,P13-1004,1,0.818371,"= exp (fi (t)) p fi (t)|EiO dfi . The predictive distribution over counts at a particular time interval of length w with a mid-point t∗ for rumour Ei is Poisson distributed with rate wλi (t∗ |EiO ). Multi-task learning and incorporating text In order to exploit similarities across rumours we propose a multi-task approach where each rumour represents a task. We consider two approaches. First, we employ a multiple output GP based on the Intrinsic Coregionalization Model (ICM) ´ (Alvarez et al., 2012). It is a method which has been successfully applied to a range of NLP tasks (Beck et al., 2014; Cohn and Specia, 2013). ICM parametrizes the kernel by a matrix representing similarities between pairs of tasks. We expect it to find correlations between rumours exhibiting similar temporal patterns. The kernel takes the form trained on the training set of the rumour. We select its intensity λ using maximum likelihood estimate, which equals to the mean frequency of posts in the training intervals. The second baseline is Gaussian Process (GP) used for predicting hashtag frequencies in Twitter by Preotiuc-Pietro and Cohn (2013). Authors considered various kernels in their experiments, most notably periodic kernels."
P15-2085,N13-1039,0,0.0351162,"mour lifespan, which we split into 20 evenly spaced intervals. This way, our dataset consists in total of 2280 intervals. We iterate over rumours using a form of folded cross-validation, where in each iteration we exclude some (but not all) time intervals for a single target rumour. The excluded time intervals form the test set: either by selecting half at random (interpolation); or by taking only the second half for testing (extrapolation). To ameliorate the problems of data sparsity, we replace words with their Brown cluster ids, using 1000 clusters acquired on a large scale Twitter corpus (Owoputi et al., 2013). The mean function for the underlying GP in LGCP methods is assumed to be 0, which results in intensity function to be around 1 in the absence of nearby observations. This prevents our method from predicting 0 counts in these regions. We add 1 to the counts in the intervals to deal with this problem as a preprocessing step. The original counts can be obtained by decrementing 1 from the predicted counts. Instead, one could use a GP with a non-zero mean function and learn the mean function, a more elegant way of approaching this problem, which we leave for future work. kTXT ((t, i), (t0 , i0 ))"
P15-2085,D13-1100,1,0.935962,"ng rumour popularity. There have been several descriptive studies of rumours in social media, e.g. Procter et al. (2013) analyzed rumours in tweets about the 2011 London riots and showed that they follow similar lifecycles. Friggeri et al. (2014) showed how Facebook constitutes a rich source of rumours and conversation threads on the topic. However, none of these studies tried to model rumour dynamics. The problem of modelling the temporal nature of social media explicitly has received little attention. The work most closely related modelled hash tag frequency time-series in Twitter using GP (Preotiuc-Pietro and Cohn, 2013). It made several simplifications, including discretising time and treating the problem of modelling counts as regression, which are both inappropriate. In contrast we take a more principled approach, using a point process. We use the proposed GP-based method as a baseline to demonstrate the benefit of using our approaches. The log-Gaussian Cox process has been applied for disease and conflict mapping, e.g. ZammitMangion et al. (2012) developed a spatio-temporal model of conflict events in Afghanistan. In contrast here we deal with temporal text data, and model several correlated outputs rathe"
P15-2104,D10-1124,0,0.503589,"Missing"
P15-2104,W06-3808,0,0.029268,"Missing"
P15-2104,C12-1064,1,0.842564,", and temporal data. Text-based methods model the geographical bias of language use in social media, and use it to geolocate non-geotagged users. Gazetted expressions (Leidner and Lieberman, 2011) and geographical names (Quercini et 630 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 630–636, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics stein et al., 2010), (2) T WITTER -US (Roller et al., 2012), and (3) T WITTER -W ORLD (Han et al., 2012). In each dataset, users are represented by a single meta-document, generated by concatenating their tweets. The datasets are pre-partitioned into training, development and test sets, and rebuilt from the original version to include mention information. The first two datasets were constructed to contain mostly English messages. G EOT EXT consists of tweets from 9.5K users: 1895 users are held out for each of development and test data. The primary location of each user is set to the coordinates of their first tweet. T WITTER -US consists of 449K users, of which 10K users are held out for each o"
P15-2104,N15-1153,1,0.615367,"Information Systems The University of Melbourne arahimi@student.unimelb.edu.au {t.cohn,tbaldwin}@unimelb.edu.au Abstract methods. Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015). Methods which combine the two, however, are rare. In this paper, we present our work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline network-based geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013; Rahimi et al., 2015); (2) we demonstrate that removing “celebrity” nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we in"
P15-2104,D12-1137,0,0.837304,"eolocation Using a Unified Text and Network Prediction Model Afshin Rahimi, Trevor Cohn, and Timothy Baldwin Department of Computing and Information Systems The University of Melbourne arahimi@student.unimelb.edu.au {t.cohn,tbaldwin}@unimelb.edu.au Abstract methods. Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015). Methods which combine the two, however, are rare. In this paper, we present our work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline network-based geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013; Rahimi et al., 2015); (2) we demonstrate that removing “celebrity” node"
P15-2104,P11-1096,0,0.623803,"Missing"
P15-2104,D14-1039,0,0.550208,"development and test data. The primary location of each user is, once again, set to the coordinates of their first tweet. T WITTER -W ORLD consists of 1.3M users, of which 10000 each are held out for development and test. Unlike the other two datasets, the primary location of users is mapped to the geographic centre of the city where the majority of their tweets were posted. al., 2010) were used as feature in early work, but were shown to be sparse in coverage. Han et al. (2014) used information-theoretic methods to automatically extract location-indicative words for location classification. Wing and Baldridge (2014) reported that discriminative approaches (based on hierarchical classification over adaptive grids), when optimised properly, are superior to explicit feature selection. Cha et al. (2015) showed that sparse coding can be used to effectively learn a latent representation of tweet text to use in user geolocation. Eisenstein et al. (2010) and Ahmed et al. (2013) proposed topic modelbased approaches to geolocation, based on the assumption that words are generated from hidden topics and geographical regions. Similarly, Yuan et al. (2013) used graphical models to jointly learn spatio-temporal topics"
P15-2139,D14-1096,1,0.401152,"Epos , Earc , W1 and W2 can be shared as indicated with dashed lines. In particular we expect this to be the case when languages use the same POS tagset and arc label sets, as we presume herein. This assumption is motivated by the development of unified annotation for many languages (Nivre et al., 2015; Petrov et al., 2012; McDonald et al., 2013). To allow parameter sharing between languages we could jointly train the parser on the source and target language simultaneously. However, we leave this for future work. Here we take an alternative approach, namely regularization in a similar vein to Duong et al. (2014). First we train a lexicalized neural network parser on the source resource-rich language (English), as described in Section 2. The learned parameters are en , E en , E en , W en , W en . Second, we incorEword pos arc 1 2 porate English parameters as a prior for the target language training. This is straightforward when we use the same architecture, such as a neural network parser, for the target language. All we need to do is modify the learning objective function so that it includes the regularization part. However, we don’t want to regularize the en part related to Eword since it will be ve"
P15-2139,2005.mtsummit-papers.11,0,0.0269162,"h language in the collection. Some languages have over 400k tokens such as cs, fr and es, meanwhile, hu and ga have only around 25k tokens. 4.2 Table 1: Number of tokens (× 1,000) for each language in the Universal Dependency Treebank collection. We initialize the target language word embeddings Eword of our neural network cross-lingual model with pre-trained embeddings. This is an advantage since we can incorporate monolingual data which is usually available for resource-poor languages. We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,5 Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadin¸sˇ et al., 2014). The size of monolingual data also varies significantly. There are languages such as English and German with more than 400 million words, whereas, Irish only has 4 million. We use the skip-gram model from word2vec to induce 50-dimension word embeddings (Mikolov et al., 2013). tagset and arc label annotation for the source and target language. The same POS tagset is required so that the source language parser has similar structure with the target language parser. The requirement of same arc label annotation is mainly needed for evaluation using t"
P15-2139,P14-1126,0,0.0386164,"Missing"
P15-2139,P05-1012,0,0.018181,"uide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method. 1 Introduction Dependency parsing is a crucial component of many natural language processing systems, for ¨ ur and tasks such as text classification (Ozg¨ G¨ung¨or, 2010), statistical machine translation (Xu et al., 2009), relation extraction (Bunescu and Mooney, 2005), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been successful for languages where relatively large treebanks are available (McDonald et al., 2005). However, for many languages, annotated treebanks are not available. They are costly to create, requiring careful design, testing and subsequent refinement of annotation guidelines, along with assessment and management of annotator quality (B¨ohmov´a et al., 2001). The Universal Treebank Annotation Guidelines aim at providing unified annotation for many languages enabling cross-lingual comparison (Nivre et al., 2015). This project provides a starting point for developing a treebank for resource-poor languages. However, a mature parser requires a large treebank for training, and this is still"
P15-2139,N09-1028,0,0.0172191,"costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method. 1 Introduction Dependency parsing is a crucial component of many natural language processing systems, for ¨ ur and tasks such as text classification (Ozg¨ G¨ung¨or, 2010), statistical machine translation (Xu et al., 2009), relation extraction (Bunescu and Mooney, 2005), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been successful for languages where relatively large treebanks are available (McDonald et al., 2005). However, for many languages, annotated treebanks are not available. They are costly to create, requiring careful design, testing and subsequent refinement of annotation guidelines, along with assessment and management of annotator quality (B¨ohmov´a et al., 2001). The Universal Treebank Annotation Guidelines aim at providing unified annotation for many l"
P15-2139,D11-1006,0,0.0235477,"Missing"
P15-2139,P13-2017,0,0.0373468,"Missing"
P15-2139,I08-3008,0,0.0233446,"Missing"
P15-2139,petrov-etal-2012-universal,0,0.0150699,". 3 Cross-lingual parser Our model takes advantage of underlying structure shared between languages. Given the source language parsing structure as in Figure 1 (left), the set of parameters Eword will be different for the target language parser shown in Figure 1 (right) but we hypothesize that Epos , Earc , W1 and W2 can be shared as indicated with dashed lines. In particular we expect this to be the case when languages use the same POS tagset and arc label sets, as we presume herein. This assumption is motivated by the development of unified annotation for many languages (Nivre et al., 2015; Petrov et al., 2012; McDonald et al., 2013). To allow parameter sharing between languages we could jointly train the parser on the source and target language simultaneously. However, we leave this for future work. Here we take an alternative approach, namely regularization in a similar vein to Duong et al. (2014). First we train a lexicalized neural network parser on the source resource-rich language (English), as described in Section 2. The learned parameters are en , E en , E en , W en , W en . Second, we incorEword pos arc 1 2 porate English parameters as a prior for the target language training. This is stra"
P15-2139,skadins-etal-2014-billions,0,0.024592,"Missing"
P15-2139,N13-1126,0,0.0257058,"Missing"
P15-2139,D14-1082,0,\N,Missing
P15-2139,H05-1091,0,\N,Missing
P16-1158,N07-4013,0,0.01262,"Missing"
P16-1158,S07-1003,0,0.010422,"Missing"
P16-1158,W11-2501,0,0.0573442,"ccurring in all embedding sets. There is some overlap between our relations and those included in the analogy task of Mikolov et al. (2013c), but we include a much wider range of lexical semantic relations, especially those standardly evaluated in the relation classification literature. We manually filtered the data to remove duplicates (e.g., as part of merging the two sources of L EX S EMHyper intances), and normalise directionality. The final dataset consists of 12,458 triples hrelation, word1 , word2 i, comprising 15 relation types, extracted from SemEval’12 (Jurgens et al., 2012), BLESS (Baroni and Lenci, 2011), the MSR analogy dataset (Mikolov et al., 2013c), the light verb dataset of Tan et al. (2006a), Princeton WordNet (Fellbaum, 1998), Wiktionary,5 and a web lexicon of collective nouns,6 as listed in Table 2.7 4 Clustering Assuming D IFF V ECs are capable of capturing all lexical relations equally, we would expect clustering to be able to identify sets of word pairs with high relational similarity, or equivalently clusters of similar offset vectors. Under the additional assumption that a given word pair corresponds to a unique lexical relation (in line with our definition of the lexical relatio"
P16-1158,S10-1006,0,0.0326996,"Missing"
P16-1158,E12-1004,0,0.0620553,"map each word pair (wi , wj ) as follows: (a) (wi , wj ) 7→ rk ∈ R, i.e. the “closed-world” setting, where we assume that all word pairs can be uniquely classified according to a relation in R; or (b) (wi , wj ) 7→ rk ∈ R ∪ {φ} where φ signifies the fact that none of the relations in R apply to the word pair in question, i.e. the “open-world” setting. Our starting point for lexical relation learning is the assumption that important information about various types of relations is implicitly embedded in the offset vectors. While a range of methods have been proposed for composing word vectors (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014), in this research we focus exclusively on D IFF V EC (i.e. w2 − w1 ). A second assumption is that there exist dimensions, or directions, in the embedding vector spaces responsible for a particular lexical relation. Such dimensions could be identified and exploited as part of a clustering or classification method, in the context of identifying relations between word pairs or classes of D IFF V ECs. In order to test the generalisability of the D IFF V EC method, we require: (1) word embeddings, and (2) a set of lexical relations to evaluate against. As"
P16-1158,N15-1184,0,0.00539531,"a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such as quality, time, gen"
P16-1158,S12-1047,0,0.193923,"set. In the Open Information Extraction paradigm (Banko et al., 2007; Weikum and Theobald, 2010), also known as unsupervised relation extraction, the relations themselves are also learned from the text (e.g. in the form of text labels). On the other hand, relational similarity prediction involves assessing the degree to which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This"
P16-1158,D13-1169,0,0.0654934,"Missing"
P16-1158,W15-0105,0,0.0442251,"Missing"
P16-1158,S12-1012,0,0.0133615,"es assessing the degree to which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based clas"
P16-1158,W14-1618,0,0.0602254,"chine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010). Recently, attention has been focused on identifying lexical relations using word embeddings, which are dense, low-dimensional vectors obtained either from a “predict-based” neural network trained to predict word contexts, or a “countbased” traditional distributional similarity method combined with dimensionality reduction. The skipgram model of Mikolov et al. (2013a) and other similar language models have been shown to perform well on an analogy completion task (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014a), in the space of relational simThe key operation in these models is vector difference, or vector offset. For example, the paris − france vector appears to encode CAPITAL - OF , presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (Levy and Goldberg, 2014a). The success of the simple offset method on analogy completion suggests that the difference vectors (“D IFF V EC” hereafter) must themselves be meaningful: their direction and/or magnitude encodes a lexical relation. Previous analogy completion tasks used w"
P16-1158,P14-1113,0,0.0554104,"d the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help exp"
P16-1158,P05-1014,0,0.0294569,"er hand, relational similarity prediction involves assessing the degree to which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine"
P16-1158,Q15-1016,0,0.185434,"move to an open-world setting including random word pairs — many of which do not correspond to any lexical relation in the training data — the results are poor. We then investigate methods for better attuning the learned class representation to the lexical relations, focusing on methods for automatically synthesising negative instances. We find that this improves the model performance substantially. We also find that hyper-parameter optimised count-based methods are competitive with predictbased methods under both clustering and supervised relation classification, in line with the findings of Levy et al. (2015a). 2 Background and Related Work A lexical relation is a binary relation r holding between a word pair (wi , wj ); for example, the pair (cart, wheel) stands in the WHOLE - PART relation. Relation learning in NLP includes relation extraction, relation classification, and relational similarity prediction. In relation extraction, related word pairs in a corpus and the relevant relation are identified. Given a word pair, the relation classification task involves assigning a word pair to the correct relation from a pre-defined set. In the Open Information Extraction paradigm (Banko et al., 2007;"
P16-1158,N15-1098,0,0.135987,"move to an open-world setting including random word pairs — many of which do not correspond to any lexical relation in the training data — the results are poor. We then investigate methods for better attuning the learned class representation to the lexical relations, focusing on methods for automatically synthesising negative instances. We find that this improves the model performance substantially. We also find that hyper-parameter optimised count-based methods are competitive with predictbased methods under both clustering and supervised relation classification, in line with the findings of Levy et al. (2015a). 2 Background and Related Work A lexical relation is a binary relation r holding between a word pair (wi , wj ); for example, the pair (cart, wheel) stands in the WHOLE - PART relation. Relation learning in NLP includes relation extraction, relation classification, and relational similarity prediction. In relation extraction, related word pairs in a corpus and the relevant relation are identified. Given a word pair, the relation classification task involves assigning a word pair to the correct relation from a pre-defined set. In the Open Information Extraction paradigm (Banko et al., 2007;"
P16-1158,W13-3207,0,0.0166618,"al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such as quality, time, gender, and distance, finding that for about two-thirds of antonym classes, D IFF V ECs are significantly more correlated than random. Necs¸ulescu et al. (2015) train a classifier on word pairs, using word embeddings to predict coordinates, hypernyms, and meronyms. Roller and Erk (2016) analyse the performance of vector concatenation and difference on the task of predicting lexical entailment and show that vector concatenation overwhelmingly learns to detect Hearst patterns (e.g., including, such as). K¨oper et al. (2015) under"
P16-1158,S10-1011,0,0.0280231,"Missing"
P16-1158,N13-1090,0,0.722758,"lexical relations is a fundamental task in natural language processing (“NLP”), and can contribute to many NLP applications including paraphrasing and generation, machine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010). Recently, attention has been focused on identifying lexical relations using word embeddings, which are dense, low-dimensional vectors obtained either from a “predict-based” neural network trained to predict word contexts, or a “countbased” traditional distributional similarity method combined with dimensionality reduction. The skipgram model of Mikolov et al. (2013a) and other similar language models have been shown to perform well on an analogy completion task (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014a), in the space of relational simThe key operation in these models is vector difference, or vector offset. For example, the paris − france vector appears to encode CAPITAL - OF , presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (Levy and Goldberg, 2014a). The success of the simple offset method on analogy completion suggests that the differe"
P16-1158,S15-1021,0,0.444375,"Missing"
P16-1158,C04-1111,0,0.00884956,"f the frequency profile of our corpus. We train 9 binary RBF-kernel SVM classifiers on the training partition, and evaluate on our randomly augmented test set. Fully annotating our random word pairs is prohibitively expensive, so instead, we manually annotated only the word pairs which were positively classified by one of our models. The results of our experiments are presented in the left half of Table 5, in which we report on results over the combination of the original test data from §5.1 and the random word pairs, noting that recall (R) for O PEN -W ORLD takes the form of relative recall (Pantel et al., 2004) over the positively-classified word pairs. The results are much lower than for the closed-word setting (Table 4), most notably in terms of precision (P). For instance, the random pairs (have, works), (turn, took), and (works, started) were incorrectly classified as V ERB3 , V ERBPast and V ERB3Past , respectively. That is, the model captures syntax, but lacks the ability to capture lexical paradigms, and tends to overgenerate. 5.3 O PEN -W ORLD Training with Negative Sampling To address the problem of incorrectly classifying random word pairs as valid relations, we retrain the classifier on a"
P16-1158,D14-1162,0,0.0773416,"Missing"
P16-1158,E14-1054,1,0.235198,"(A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013)"
P16-1158,D16-1234,0,0.0367941,"berg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such as quality, time, gender, and distance, finding that for about two-thirds of antonym classes, D IFF V ECs are significantly more correlated than random. Necs¸ulescu et al. (2015) train a classifier on word pairs, using word embeddings to predict coordinates, hypernyms, and meronyms. Roller and Erk (2016) analyse the performance of vector concatenation and difference on the task of predicting lexical entailment and show that vector concatenation overwhelmingly learns to detect Hearst patterns (e.g., including, such as). K¨oper et al. (2015) undertake a systematic study of morphosyntactic and semantic relations on word embeddings produced with word2vec (“w2v” hereafter; see §3.1) for English and German. They test a variety of relations including word similarity, antonyms, synonyms, hypernyms, and meronyms, in a novel analogy task. Although the set of relations tested by 1672 Name w2v GloVe SENN"
P16-1158,C14-1097,0,0.128351,"Missing"
P16-1158,D07-1043,0,0.0108822,"Missing"
P16-1158,E14-4008,0,0.0133383,"in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings t"
P16-1158,W06-2407,0,0.0232598,"EFIX N OUNColl Description hypernym meronym characteristic quality, action cause, purpose, or goal location or time association expression or representation object’s action plural form of a noun first to third person verb present-tense form present-tense to past-tense verb form third person present-tense to past-tense verb form light verb construction nominalisation of a verb prefixing with re morpheme collective noun Pairs 1173 2825 71 249 235 187 3583 100 99 100 100 58 3303 118 257 Source SemEval’12 + BLESS SemEval’12 + BLESS SemEval’12 SemEval’12 SemEval’12 SemEval’12 BLESS MSR MSR MSR MSR Tan et al. (2006b) WordNet Wiktionary Web source Example (animal, dog) (airplane, cockpit) (cloud, rain) (cook, eat) (aquarium, fish) (song, emotion) (zip, coat) (year, years) (accept, accepts) (know, knew) (creates, created) (give, approval) (approve, approval) (vote, revote) (army, ants) Table 2: Description of the 15 lexical relations. to train the model. We use the focus word vectors, W = {wk }Vk=1 , normalised such that each kwk k = 1. The GloVe model (Pennington et al., 2014) is based on a similar bilinear formulation, framed as a low-rank decomposition of the matrix of corpus co-occurrence frequencies:"
P16-1158,P10-1040,0,0.00287074,"caling matrix, and b∗ is a bias term. The final model, SENNA (Collobert and Weston, 2008), was initially proposed for multi-task training of several language processing tasks, from language modelling through to semantic role labelling. Here we focus on the statistical language modelling component, which has a pairwise ranking objective to maximise the relative score of each word in its local context: J= where the last c − 1 words are used as context, and f (x) is a non-linear function of the input, defined as a multi-layer perceptron. For HLBL and SENNA, we use the pre-trained embeddings from Turian et al. (2010), trained on the Reuters English newswire corpus. In both cases, the embeddings were scaled by the global standard deviation over the word-embedding matrix, W Wscaled = 0.1 × σ(W ). For w2vwiki , GloVewiki and SVDwiki we used English Wikipedia. We followed the same preprocessing procedure described in Levy et al. (2015a),3 i.e., lower-cased all words and removed non-textual elements. During the training phase, for each model we set a word frequency threshold of 5. For the SVD model, we followed the recommendations of Levy et al. (2015a) in setting the context window size to 2, negative samplin"
P16-1158,J06-3003,0,0.0353682,"Missing"
P16-1158,C14-1212,0,0.467066,"o which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de M"
P16-1158,D09-1097,0,0.0273496,"Missing"
P16-1158,P14-2089,0,0.00548246,"(2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such"
P16-1158,N13-1120,0,0.0317494,"hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and"
P16-2064,D15-1311,1,0.849351,"in society and in some cases even leading to riots. For instance, during an earthquake in Chile in 393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 393–398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics intensity 5 4 3 2 1 0 We define the stance classification task as that in which each tweet dj needs to be classified into one of the four categories, yj ∈ Y , which represents the stance of the tweet dj with respect to the rumour Ri it belongs to. We consider the Leave One Out (LOO) setting, introduced by Lukasik et al. (2015a), where for each rumour Ri ∈ D we construct the test set equal to Ri and the training set equal to D  Ri . The final performance scores we report in the paper are averaged across all rumours. This represents a realistic scenario where a classifier has to deal with a new, unseen rumour. support deny question comment 0 1 2 3 4 time (in hours) 5 6 Figure 1: Intensities of the Hawkes Process for an example Ferguson rumour. Tweet occurrences over time are denoted at the bottom of the figure by different symbols. Intensity for comments is high throughout the rumour lifespan. 3 nered from rumour d"
P16-2064,P15-2085,1,0.865299,"in society and in some cases even leading to riots. For instance, during an earthquake in Chile in 393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 393–398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics intensity 5 4 3 2 1 0 We define the stance classification task as that in which each tweet dj needs to be classified into one of the four categories, yj ∈ Y , which represents the stance of the tweet dj with respect to the rumour Ri it belongs to. We consider the Leave One Out (LOO) setting, introduced by Lukasik et al. (2015a), where for each rumour Ri ∈ D we construct the test set equal to Ri and the training set equal to D  Ri . The final performance scores we report in the paper are averaged across all rumours. This represents a realistic scenario where a classifier has to deal with a new, unseen rumour. support deny question comment 0 1 2 3 4 time (in hours) 5 6 Figure 1: Intensities of the Hawkes Process for an example Ferguson rumour. Tweet occurrences over time are denoted at the bottom of the figure by different symbols. Intensity for comments is high throughout the rumour lifespan. 3 nered from rumour d"
P16-2064,D11-1147,0,0.197497,"Missing"
P16-4022,N16-1122,0,0.184973,"Missing"
P16-4022,D10-1124,0,0.170675,"Missing"
P16-4022,C12-1064,1,0.957211,"n language use is most evident for countries with different languages (e.g. Germany versus China), but also exists for countries which share the same languages (e.g. in the spelling of centre vs. center in British vs. American English). The linguistic geographical bias is not limited to these obvious cases, however, and includes the use of toponyms, names of people, sport teams, and dialectal terms. These differences in use of language can be captured in text-based geolocation models. Previous work have used topic models (Eisenstein et al., 2010) and supervised flat (Wing and Baldridge, 2011; Han et al., 2012; Han et al., 2013; Han et al., 2014; Rahimi et al., 2015b) and hierarchical (Wing and Baldridge, 2014) classification models. The main idea is to learn the geographical distribution of a given word across different locations from training data, and use it to predict a location for a new user. Social ties have also been used for social media user geolocation. Backstrom et al. (2010) showed that Facebook users tend to interact more with nearby people (“location homophily”), and used this property to geolocate users based on the location of their friends, hence popularising networkbased geolocat"
P16-4022,P15-2104,1,0.807258,"e the geographical barrier for users to communicate, the majority of user interactions are still local (Backstrom et al., 2010). This geographical bias can be utilised to geolocate a user by analysing their social interactions. Based on the assumption that social interactions are more likely to be local, a user should be geographically close to their connections. The simplest approach to geolocation is to use the median location of a user’s friends. Recent studies have shown that using both network and text information can improve the coverage and keep the predictions accurate simultaneously (Rahimi et al., 2015b). Despite the widespread use of geolocation, most services are proprietary, overly-simplistic, or complicated to use. Supervised classification models often require huge amounts of geotagged data and large amounts of computing power to be trained. The performance is also heavily dependent on hyperparameter tuning, making the training procedure more challenging for end-users. In this paper we introduce pigeo, a Python geolocation tool that has the following characteristics: (1) it comes with a pre-trained textbased model; (2) it is easy to use; (3) it has been tuned, benchmarked and proven to"
P16-4022,P13-4002,1,0.766633,"Missing"
P16-4022,N15-1153,1,0.664296,"e the geographical barrier for users to communicate, the majority of user interactions are still local (Backstrom et al., 2010). This geographical bias can be utilised to geolocate a user by analysing their social interactions. Based on the assumption that social interactions are more likely to be local, a user should be geographically close to their connections. The simplest approach to geolocation is to use the median location of a user’s friends. Recent studies have shown that using both network and text information can improve the coverage and keep the predictions accurate simultaneously (Rahimi et al., 2015b). Despite the widespread use of geolocation, most services are proprietary, overly-simplistic, or complicated to use. Supervised classification models often require huge amounts of geotagged data and large amounts of computing power to be trained. The performance is also heavily dependent on hyperparameter tuning, making the training procedure more challenging for end-users. In this paper we introduce pigeo, a Python geolocation tool that has the following characteristics: (1) it comes with a pre-trained textbased model; (2) it is easy to use; (3) it has been tuned, benchmarked and proven to"
P16-4022,P11-1096,0,0.109648,"model. Geographical bias in language use is most evident for countries with different languages (e.g. Germany versus China), but also exists for countries which share the same languages (e.g. in the spelling of centre vs. center in British vs. American English). The linguistic geographical bias is not limited to these obvious cases, however, and includes the use of toponyms, names of people, sport teams, and dialectal terms. These differences in use of language can be captured in text-based geolocation models. Previous work have used topic models (Eisenstein et al., 2010) and supervised flat (Wing and Baldridge, 2011; Han et al., 2012; Han et al., 2013; Han et al., 2014; Rahimi et al., 2015b) and hierarchical (Wing and Baldridge, 2014) classification models. The main idea is to learn the geographical distribution of a given word across different locations from training data, and use it to predict a location for a new user. Social ties have also been used for social media user geolocation. Backstrom et al. (2010) showed that Facebook users tend to interact more with nearby people (“location homophily”), and used this property to geolocate users based on the location of their friends, hence popularising net"
P16-4022,D14-1039,0,0.676885,"Missing"
P16-4022,W14-4204,0,0.0597922,"Missing"
P17-1033,W13-0102,0,0.018532,"topic, we replace s with Bt before computing the softmax over the vocabulary. wise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13 Topic models are traditionally evaluated using model perplexity. There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4). We use the same hyper-parameter settings for tdlm and do not tune them. Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words. To aggregate topic coherence scores for a model,"
P17-1033,P03-1054,0,0.0199484,"g, we have additional minibatches for the documents. We start the document classification training after the topic and language models have completed training in each epoch. We use 20 NEWS in this experiment, which is a popular dataset for text classification. 20 NEWS is a collection of forum-like messages from 20 newsgroups categories. We use the “bydate” version of the dataset, where the train and test partition is separated by a specific date. We sample 2K documents from the training set to create the development set. For preprocessing we tokenise words and sentence using Stanford CoreNLP (Klein and Manning, 2003), and lowercase all words. As with previous experiments (Section 4) we additionally filter low/high frequency word types and stopwords. Preprocessed dataset statistics are presented in Table 5. For comparison, we use the same two topic ntm: ntm is a neural topic model proposed by Cao et al. (2015). The document-topic and topicword multinomials are expressed from a neural network perspective using differentiable functions. Model hyper-parameters are tuned using development loss. Topic model performance is presented in Table 4. There are two models of tdlm (tdlm-small and tdlm-large), which spec"
P17-1033,W14-4012,0,0.190012,"Missing"
P17-1033,N16-1057,1,0.799751,"does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4). We use the same hyper-parameter settings for tdlm and do not tune them. Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words. To aggregate topic coherence scores for a model, we calculate the mean coherence over topics. For comparison, we use the following topic models: lda: We use a LDA model as a baseline topic model. We use the same LDA models as were used to learn topic distributions for lstm+lda (Section 4). Following Lau et al. (2014), we compute topic coherence using normalised PMI (“NPMI”) scores. Given the top-n words of a topic, coherence is computed based on the sum of pair13 We use this toolkit to compute topic coherence: https://github.com/"
P17-1033,E14-1056,1,0.793704,"distribution for a particular topic is therefore trivial: we can do so by treating s as having maximum weight (1.0) for the topic of interest, and no weight (0.0) for all other topics. Let Bt denote the topic output vector for the t-th topic. To generate the multinomial distribution over word types for the t-th topic, we replace s with Bt before computing the softmax over the vocabulary. wise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13 Topic models are traditionally evaluated using model perplexity. There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same"
P17-1033,P11-1015,0,0.226548,"Missing"
P17-1033,D08-1038,0,0.0526451,"datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics. 1 Introduction Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009). Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a 2 Related Work Griffiths et al."
P17-1033,D11-1024,0,0.126281,"Missing"
P17-1033,N10-1012,1,0.90555,"e that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics. 1 Introduction Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009). Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a 2 Related Work Griffiths et al. (2004) propose a mod"
P17-1033,N16-1036,0,0.0174909,"ci i In the case where we use a filters, we have d ∈ Ra , and this constitutes the vector representation of the document generated by the convolutional and max-over-time pooling network. The topic vectors are stored in two lookup tables A ∈ Rk×a (input vector) and B ∈ Rk×b (output vector), where k is the number of topics, and a and b are the dimensions of the topic vectors. To align the document vector d with the topics, we compute an attention vector which is used to 2 The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016). We explored various attention styles (including traditional schemes which use one vector for a topic), but found this approach to work best. 1 A non-linear function is typically used here, but preliminary experiments suggest that the identity function works best for tdlm. 357 We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model: ument collections from 3 sources: APNEWS, IMDB and BNC. APNEWS is a collection of Associated Press5 news articles from 2009 to 2016. IMDB is a set of m"
P17-1033,P16-1125,0,0.0617449,"Missing"
P17-2033,C12-1064,1,0.8646,"from geotagged documents (Eisenstein et al., 2010; Ahmed et al., 2013; Cook et al., 2014; Eisenstein, 2015). The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (Monroe et al., 2008). There is a research gap in evaluating the geolocation models in terms of their usability in retrieving dialect terms given a geographic region. 3 Data We use three existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), (2) T WITTER -US (Roller et al., 2012), and (3) T WITTER -W ORLD (Han et al., 2012). These datasets have been used widely for training and evaluation of geolocation models. They are all prepartitioned into training, development and test sets. Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of G EOT EXT and T WITTER -US, and the centre of the closest city in the case of T WITTER -W ORLD.1 G EOT EXT and T WITTER -US cover the continental US, and T WITTER -W ORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1.2 DAREDS is a dialect-te"
P17-2033,E14-1011,0,0.148399,"Missing"
P17-2033,D10-1124,0,0.789721,"Missing"
P17-2033,W15-1527,0,0.421041,"inates into equalsized grids (Serdyukov et al., 2009), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014). Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions (Rahimi et al., 2015a) as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015), utilising both text and network information (Rahimi et al., 2015a). Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015). Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas. The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (Labov et al., 2005; Nerbonne et al., 2008; Gonc¸alves and S´anchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, 2016)), the shortcoming of which is that the"
P17-2033,P15-2104,1,0.426469,"term detection methods. 1 Introduction Many services such as web search (Leung et al., 2010), recommender systems (Ho et al., 2012), targeted advertising (Lim and Datta, 2013), and rapid disaster response (Ashktorab et al., 2014) rely on the location of users to personalise information and extract actionable knowledge. Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, a"
P17-2033,N15-1153,1,0.731334,"term detection methods. 1 Introduction Many services such as web search (Leung et al., 2010), recommender systems (Ho et al., 2012), targeted advertising (Lim and Datta, 2013), and rapid disaster response (Ashktorab et al., 2014) rely on the location of users to personalise information and extract actionable knowledge. Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, a"
P17-2033,D12-1137,0,0.547879,"ress) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015). Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015): given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regio"
P17-2033,P11-1096,0,0.720681,"Missing"
P17-2033,D15-1256,0,0.0516024,"Missing"
P17-2033,D14-1039,0,0.661441,"iving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015). Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015): given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions. The complexity of the task is two-fold: (1) localised named"
P17-2093,P15-1119,0,0.103969,"Missing"
P17-2093,kamholz-etal-2014-panlex,0,0.0260809,"(labelled D ISTANT) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. B I LSTM-D EBIAS (Fang and Cohn, 2016) performs worse than our proposed method, indicating that a linear transformation is insufficient for modelling distant supervision. The accuracies are higher overall for the European cf. Turkic languages, presumably because these languages are 3 Although the use of a translation system conveys a dependence on parallel text, high quality word embeddings can be learned directly from bilingual dictionaries such as Panlex (Kamholz et al., 2014). 4 Code available at https://github.com/mengf1/trpos 1 We could support more than one class label, by marginalising over the set of valid labels for all tokens in the training objective. 2 http://www.cnts.ua.ac.be/conll2003/ner/ 590 da 23.2 61.8 46.3 77.0 73.5 70.4 73.2 72.5 81.1 81.9 Random B I LSTM B I LSTM-C RF M INI TAGGER D ISTANT +CCA D ISTANT +Cluster B I LSTM-D EBIAS +CCA B I LSTM-D EBIAS +Cluster J OINT +CCA J OINT +Cluster nl 30.5 62.1 47.7 72.5 64.5 61.7 72.8 70.1 82.3 81.5 de 27.1 60.5 53.2 75.9 57.7 65.9 72.5 71.2 76.1 78.9 el 23.2 70.1 35.1 75.7 53.1 65.5 71.2 68.7 77.5 80.1 it"
P17-2093,Q16-1022,0,0.0500607,"Missing"
P17-2093,W06-2920,0,0.0768247,"ata. annotated with the most frequent label for the type in the training corpus;1 S UM T YPE Select a word type, z, for annotation with the highest aggregate uncertainty token occurrences, P over P H(z) = i∈D xi,t =z H(xi , t), which effectively combines uncertainty sampling with a bias towards high frequency types; and R ANDOM Select word types randomly. 4 Experiments We evaluate the effectiveness of the proposed model for several different languages, including both simulated low-resource and true low-resource settings. The first evaluation set uses the CoNLLX datasets of European languages (Buchholz and Marsi, 2006), comprising Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv). We use the standard corpus splits. The first 20 sentences of training set are used for training as the tiny labelled (gold) data and the last 20 sentences are used for development (early stopping). We report accuracy on the held-out test set. The second evaluation set includes two highly challenging languages, Turkish (tk) and Malagasy (mg), both having high morphological complexity and the latter has truly scant resources. Turkish data was drawn from CoNLL 20032 and Mal"
P17-2093,P16-1184,0,0.0484002,"Missing"
P17-2093,petrov-etal-2012-universal,0,0.107059,"Missing"
P17-2093,P11-1061,0,0.259955,"models have been developed for POS tagging and achieved good performance, such as RNN and bidirectional long short-term memory (BiLSTM) and CRF-BiLSTM models (Mikolov et al., 2010; Huang et al., 2015). For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (Huang et al., 2015). However, in low-resource languages, these models are seldom used because of limited labelled data. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language. Das and Petrov (2011) used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens. 3 Model We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources. Our approach extends the work of Fang and Cohn (2016), who present a model based on distant supervision in the form of cross-lingual"
P17-2093,W15-1511,0,0.0323969,"using the embedding lookup table for the corresponding language embeddings. We implement our learning procedure with the DyNet toolkit (Neubig et al., 2017).4 The BiLSTM layer uses 128 hidden units, and 32 hidden units for the transformation step. We used SGD with momentum to train models, with early stopping based on development performance. For benchmarks, we compare the proposed model against various state-of-the-art supervised learning methods, namely: a B I LSTM tagger, B I LSTM-C RF tagger (Huang et al., 2015), and a state-of-the-art semi-supervised POS tagging algorithm, M INI TAGGER (Stratos and Collins, 2015), which is also focusing on minimising the amount of labelled data. Note these methods do not use cross-lingual supervision. For a more direct comparison, we include B I LSTMD EBIAS (Fang and Cohn, 2016), applied using our proposed cross-lingual supervision based on dictionaries, instead of parallel corpora; accordingly the key difference is their linear transformation for the distant data, versus our non-linear transformation to the gold data. annotated with the most frequent label for the type in the training corpus;1 S UM T YPE Select a word type, z, for annotation with the highest aggregat"
P17-2093,K16-1018,1,0.851329,"cs https://doi.org/10.18653/v1/P17-2093 Conj distant label, y Noun Verb Det label, y Noun Augmented layer Noun shared h layers h cross-lingual word embedding, e text, x ... tsara fa misaotra raha Distantly)supervised)data) ny marina Manually)Labeled)data Figure 1: Illustration of the architecture of the joint model, which performs joint inference over both distant supervision (left) and manually labelled data (right). T¨ackstr¨om et al. (2013) constructed tag dictionaries by projecting tag information from a highresource language to a low-resource language via alignments in the parallel text. Fang and Cohn (2016) used parallel data to obtain projected tags as distant labels and proposed a joint BiLSTM model trained on both the distant data and 1, 000 tagged tokens. Zhang et al. (2016) used a few word translations pairs to find a linear transformation between two language embeddings. Then they used unsupervised learning to refine embedding transformations and model parameters. Instead we use minimal supervision to refine ‘distant’ labels through modelling the tag transformation, based on a small set of annotations. type bias leads to substantial gains over benchmark methods such as token or sentence le"
P17-2093,Q13-1001,0,0.163219,"Missing"
P17-2093,N01-1026,0,0.21179,"Recently, neural network models have been developed for POS tagging and achieved good performance, such as RNN and bidirectional long short-term memory (BiLSTM) and CRF-BiLSTM models (Mikolov et al., 2010; Huang et al., 2015). For example, the CRFBiLSTM POS tagger obtained the state-of-theart performance on Penn Treebank WSJ corpus (Huang et al., 2015). However, in low-resource languages, these models are seldom used because of limited labelled data. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems in low-resource languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016; Zhang et al., 2016). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting POS tag information from one language to another language. Das and Petrov (2011) used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens. 3 Model We now describe the modelling framework for POS tagging in a low-resource language, based on very limited linguistic resources. Our approach extends the work of Fang and Cohn (2016), who present a model based on distant supervision in the"
P17-2093,N13-1014,0,0.133007,"f ground truth labels and the high cost of annotation, a natural question is whether we can optimise which text to be annotated in order achieve the high accuracy for the lowest cost. We now outline a range of active learning approaches based on the following heuristics, which are used to select the instances for annotation from a pool of candidates: T OKEN Select the token xt with theP highest uncertainty, H(x, t) = − y P (y|x, t) log P (y|x, t); S ENT Select the sentence x with P the highest aggregate uncertainty, H(x) = t H(x, t); F REQ T YPE Select the most frequent unannotated word type (Garrette and Baldridge, 2013), in which case all token instances are 589 glish monolingual corpus with Google Translate.3 The monolingual corpora were constructed from a combination of text from the Leipzig Corpora Collection and Europarl. We trained the languageuniversal POS tagger based on the cross-lingual word embeddings with the universal POS tagset (Petrov et al., 2011), and then applied to the target language using the embedding lookup table for the corresponding language embeddings. We implement our learning procedure with the DyNet toolkit (Neubig et al., 2017).4 The BiLSTM layer uses 128 hidden units, and 32 hid"
P17-2093,N16-1156,0,0.127202,"Missing"
P18-1026,P17-2021,0,0.0266792,"n our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Basti"
P18-1026,N04-1035,0,0.328773,"Missing"
P18-1026,W13-2322,0,0.267685,"Missing"
P18-1026,D17-1209,0,0.130802,"Missing"
P18-1026,W17-4755,0,0.0130573,"available in gold-standard form. Table 2: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05), including the negative CHRF++ result for En-Cs. Interestingly, we found different trends when analysing the CHRF++ numbers. In particular, this metric favours the PB-SMT models for both language pairs, while also showing improved performance for s2s in En-Cs. CHRF++ has been shown to better correlate with human judgments compared to BLEU, both at system and sentence level for both language pairs (Bojar et al., 2017), which motivated our choice as an additional metric. We leave further investigation of this phenomena for future work. Neural networks for graphs Recurrent networks on general graphs were first proposed un280 sation and parameter explosion. In particular, we showed how graph transformations can solve issues with graph-based networks without changing the underlying architecture. This is the case of the proposed Levi graph transformation, which ensures the decoder can attend to edges as well as nodes, but also to the sequential connections added to the dependency trees in the case of NMT. Overa"
P18-1026,W14-3333,0,0.0148702,"emble of the 5 models. Finally, we also report the number of parameters used in each model. Since our encoder architectures are quite different, we try to match the number of parameters between them by changing the dimensionality of the hidden layers (as explained above). We do this to minimise the effects of model capacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001) and perform bootstrap resampling to check statistical significance. However, since recent work has questioned the effectiveness of BLEU with bootstrap resampling (Graham et al., 2014), we also report results using sentence-level CHR F++ (Popovi´c, 2017), using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics. Recent work has shown that evaluation in neural models can lead to wrong conclusions by just changing the random seed (Reimers and Gurevych, 2017). In an effort to make our conclusions more robust, we run each model 5 times using different seeds. From each pool, we report 4.2 Results and analysis Table 1 shows the results on the test set. For the s2s models, we also report results without the scope marking procedure"
P18-1026,D17-1012,0,0.0183755,"incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Bastings for sharing the data from his paper’s experiments. Discussio"
P18-1026,E17-3017,0,0.0312247,", 2005; Scarselli et al., 2009) assume a fixed point representation of the parameters and learn using contraction maps. Li et al. (2016) argues that this restricts the capacity of the model and makes it harder to learn long distance relations between nodes. To tackle these issues, they propose Gated Graph Neural Networks, which extend these architectures with gating mechanisms =σ crv X W`re h(t−1) u + br`e u∈Nv ! ztv = σ czv X W`ze h(t−1) + bz`e u u∈Nv et h v = ρ cv X W`e  rtu h(t−1) u  ! + b`e u∈Nv htv 1 Our implementation uses MXNet (Chen et al., 2015) and is based on the Sockeye toolkit (Hieber et al., 2017). Code is available at github.com/beckdaniel/acl2018_ graph2seq. et = (1 − ztv ) h(i−1) + ztv h v v where e = (u, v, `e ) is the edge between nodes u and v, N (v) is the set of neighbour nodes for v, ρ is a non-linear function, σ is the sigmoid function 274 and cv = czv = crv = |Nv |−1 are normalisation constants. Our formulation differs from the original GGNNs from Li et al. (2016) in some aspects: 1) we add bias vectors for the hidden state, reset gate and update gate computations; 2) labelspecific matrices do not share any components; 3) reset gates are applied to all hidden states before a"
P18-1026,P13-1091,0,0.0244612,"– English-Czech – – – BLEU CHR F++ #params Single models PB-SMT s2s g2s g2s+ 8.6 8.9 8.7 9.8 36.4 33.8 32.3 33.3 – 39.1M 38.4M 38.8M Ensembles s2s g2s g2s+ 11.3 10.4 11.7 36.4 34.7 35.9 195M 192M 194M Results from (Bastings et al., 2017) BoW+GCN 7.5 – BiRNN 8.9 – BiRNN+GCN 9.6 – – – – 6 Related work Graph-to-sequence modelling Early NLP approaches for this problem were based on Hyperedge Replacement Grammars (Drewes et al., 1997, HRGs). These grammars assume the transduction problem can be split into rules that map portions of a graph to a set of tokens in the output sequence. In particular, Chiang et al. (2013) defines a parsing algorithm, followed by a complexity analysis, while Jones et al. (2012) report experiments on semantic-based machine translation using HRGs. HRGs were also used in previous work on AMR parsing (Peng et al., 2015). The main drawback of these grammar-based approaches though is the need for alignments between graph nodes and surface tokens, which are usually not available in gold-standard form. Table 2: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05), including the negat"
P18-1026,C12-1083,0,0.111997,"Missing"
P18-1026,P16-1078,0,0.0208166,"rdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon Universi"
P18-1026,P17-2012,0,0.0159373,"d to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Bastings for sharing the data"
P18-1026,P07-2045,0,0.00451381,"g2s models are almost the same as in the AMR generation experiments (§4.1). The only exception is the GGNN encoder dimensionality, where we use 512 for the experiments with dependency trees only and 448 when the inputs have additional sequential connections. As in the AMR generation setting, we do this to ensure model capacity are comparable in the number of parameters. Another key difference is that the s2s baselines do not use dependency trees: they are trained on the sentences only. In addition to neural models, we also report results for Phrase-Based Statistical MT (PB-SMT), using Moses (Koehn et al., 2007). The PB-SMT models are trained using the same data conditions as s2s (no dependency trees) and use the standard setup in Moses, except for the language model, where we use a 5-gram LM trained on the target side of the respective parallel corpus.8 There ROOT is expl a nsubj deeper punct issue det at amod stake prep . pobj Figure 4: Top: a sentence with its corresponding dependency tree. Bottom: the transformed tree into a Levi graph with additional sequential connections between words (dashed lines). The full graph also contains reverse and self edges, which are omitted in the figure. 5.2 Resu"
P18-1026,N16-1087,0,0.498343,"al language. In particular, many wholesentence semantic frameworks employ directed acyclic graphs as the underlying formalism, while most tree-based syntactic representations can also be seen as graphs. A range of NLP applications can be framed as the process of transducing a graph structure into a sequence. For instance, language generation may involve realising a semantic graph into a surface form and syntactic machine translation involves transforming a tree-annotated source sentence to its translation. Previous work in this setting rely on grammarbased approaches such as tree transducers (Flanigan et al., 2016) and hyperedge replacement gram273 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 273–283 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics want-01 ARG0 G1 AR believe-01 0 boy ARG G1 AR girl Figure 1: Left: the AMR graph representing the sentence “The boy wants the girl to believe him.”. Right: Our proposed architecture using the same AMR graph as input and the surface form as output. The first layer is a concatenation of node and positional embeddings, using distance from the root node as th"
P18-1026,P17-1014,0,0.269851,"d.beck,t.cohn}@unimelb.edu.au ‡ Faculty of Information Technology Monash University, Australia gholamreza.haffari@monash.edu Abstract mars (Jones et al., 2012). A key limitation of these approaches is that alignments between graph nodes and surface tokens are required. These alignments are usually automatically generated so they can propagate errors when building the grammar. More recent approaches transform the graph into a linearised form and use off-the-shelf methods such as phrase-based machine translation (Pourdamghani et al., 2016) or neural sequenceto-sequence (henceforth, s2s) models (Konstas et al., 2017). Such approaches ignore the full graph structure, discarding key information. In this work we propose a model for graph-tosequence (henceforth, g2s) learning that leverages recent advances in neural encoder-decoder architectures. Specifically, we employ an encoder based on Gated Graph Neural Networks (Li et al., 2016, GGNNs), which can incorporate the full graph structure without loss of information. Such networks represent edge information as label-wise parameters, which can be problematic even for small sized label vocabularies (in the order of hundreds). To address this limitation, we also"
P18-1026,P16-1162,0,0.097942,"anistan to block drug supplies. Figure 3: Example showing overgeneration due to reentrancies. Top: original AMR graph with key reentrancies highlighted. Bottom: reference and outputs generated by the s2s and g2s models, highlighting the overgeneration phenomena. 5.1 Experimental setup Data and preprocessing We employ the same data and settings from Bastings et al. (2017),5 which use the News Commentary V11 corpora from the WMT16 translation task.6 English text is tokenised and parsed using SyntaxNet7 while German and Czech texts are tokenised and split into subwords using byte-pair encodings (Sennrich et al., 2016, BPE) (8000 merge operations). ence previously defined concepts in the graph. In the s2s models including Konstas et al. (2017), reentrant nodes are copied in the linearised form, while this is not necessary for our g2s models. We can see that the s2s prediction overgenerates the “India and China” phrase. The g2s prediction avoids overgeneration, and almost perfectly matches the reference. While this is only a single example, it provides evidence that retaining the full graphical structure is beneficial for this task, which is corroborated by our quantitative results. 5 We obtained the data f"
P18-1026,P17-2002,0,0.267833,".3 50.4 28.4M 28.4M 28.3M Ensembles s2s s2s (-s) g2s 52.5 48.9 53.5 142M 142M 141M 26.6 22.0 27.5 Previous work (early AMR treebank versions) KIYCZ17 22.0 – – Previous work (as above + unlabelled data) KIYCZ17 33.8 – – PKH16 26.9 – – SPZWG17 25.6 – – FDSC16 22.0 – – Table 1: Results for AMR generation on the test set. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05). “(-s)” means input without scope marking. KIYCZ17, PKH16, SPZWG17 and FDSC16 are respectively the results reported in Konstas et al. (2017), Pourdamghani et al. (2016), Song et al. (2017) and Flanigan et al. (2016). results using the median model according to performance on the dev set (simulating what is expected from a single run) and using an ensemble of the 5 models. Finally, we also report the number of parameters used in each model. Since our encoder architectures are quite different, we try to match the number of parameters between them by changing the dimensionality of the hidden layers (as explained above). We do this to minimise the effects of model capacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001)"
P18-1026,P06-1077,0,0.0773311,"ensor factorisation to reduce the number of parameters. Applications Early work on AMR generation employs grammars and transducers (Flanigan et al., 2016; Song et al., 2017). Linearisation approaches include (Pourdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australi"
P18-1026,D15-1166,0,0.187779,"Missing"
P18-1026,D17-1159,0,0.510024,"es for v, ρ is a non-linear function, σ is the sigmoid function 274 and cv = czv = crv = |Nv |−1 are normalisation constants. Our formulation differs from the original GGNNs from Li et al. (2016) in some aspects: 1) we add bias vectors for the hidden state, reset gate and update gate computations; 2) labelspecific matrices do not share any components; 3) reset gates are applied to all hidden states before any computation and 4) we add normalisation constants. These modifications were applied based on preliminary experiments and ease of implementation. An alternative to GGNNs is the model from Marcheggiani and Titov (2017), which add edge label information to Graph Convolutional Networks (GCNs). According to Li et al. (2016), the main difference between GCNs and GGNNs is analogous to the difference between convolutional and recurrent networks. More specifically, GGNNs can be seen as multi-layered GCNs where layer-wise parameters are tied and gating mechanisms are added. A large number of layers can propagate node information between longer distances in the graph and, unlike GCNs, GGNNs can have an arbitrary number of layers without increasing the number of parameters. Nevertheless, our architecture borrows idea"
P18-1026,J97-3002,0,0.124119,"is proposed by Schlichtkrull et al. (2017), which uses tensor factorisation to reduce the number of parameters. Applications Early work on AMR generation employs grammars and transducers (Flanigan et al., 2016; Song et al., 2017). Linearisation approaches include (Pourdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated"
P18-1026,P01-1067,0,0.342365,"d by Schlichtkrull et al. (2017), which uses tensor factorisation to reduce the number of parameters. Applications Early work on AMR generation employs grammars and transducers (Flanigan et al., 2016; Song et al., 2017). Linearisation approaches include (Pourdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements"
P18-1026,P02-1038,0,0.0590062,"best results. However, the g2s+ models outperform the baselines in terms of BLEU scores under the same parameter budget, in both single model and ensemble scenarios. This result show that it is possible to incorporate sequential biases in our model without relying on RNNs or any other modification in the architecture. Evaluation We report results in terms of BLEU and CHRF++, using case-sensitive versions of both metrics. Other settings are kept the same as in the AMR generation experiments (§4.1). For PBSMT, we also report the median result of 5 runs, obtained by tuning the model using MERT (Och and Ney, 2002) 5 times. 8 Note that target data is segmented using BPE, which is not the usual setting for PB-SMT. We decided to keep the segmentation to ensure data conditions are the same. 279 We also show some of the results reported by Bastings et al. (2017) in Table 2. Note that their results were based on a different implementation, which may explain some variation in performance. Their BoW+GCN model is the most similar to ours, as it uses only an embedding layer and a GCN encoder. We can see that even our simpler g2s model outperforms their results. A key difference between their approach and ours is"
P18-1026,2001.mtsummit-papers.68,0,0.00892287,"6), Song et al. (2017) and Flanigan et al. (2016). results using the median model according to performance on the dev set (simulating what is expected from a single run) and using an ensemble of the 5 models. Finally, we also report the number of parameters used in each model. Since our encoder architectures are quite different, we try to match the number of parameters between them by changing the dimensionality of the hidden layers (as explained above). We do this to minimise the effects of model capacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001) and perform bootstrap resampling to check statistical significance. However, since recent work has questioned the effectiveness of BLEU with bootstrap resampling (Graham et al., 2014), we also report results using sentence-level CHR F++ (Popovi´c, 2017), using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics. Recent work has shown that evaluation in neural models can lead to wrong conclusions by just changing the random seed (Reimers and Gurevych, 2017). In an effort to make our conclusions more robust, we run each model 5 times using differ"
P18-1026,K15-1004,0,0.019093,"2017) BoW+GCN 7.5 – BiRNN 8.9 – BiRNN+GCN 9.6 – – – – 6 Related work Graph-to-sequence modelling Early NLP approaches for this problem were based on Hyperedge Replacement Grammars (Drewes et al., 1997, HRGs). These grammars assume the transduction problem can be split into rules that map portions of a graph to a set of tokens in the output sequence. In particular, Chiang et al. (2013) defines a parsing algorithm, followed by a complexity analysis, while Jones et al. (2012) report experiments on semantic-based machine translation using HRGs. HRGs were also used in previous work on AMR parsing (Peng et al., 2015). The main drawback of these grammar-based approaches though is the need for alignments between graph nodes and surface tokens, which are usually not available in gold-standard form. Table 2: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05), including the negative CHRF++ result for En-Cs. Interestingly, we found different trends when analysing the CHRF++ numbers. In particular, this metric favours the PB-SMT models for both language pairs, while also showing improved performance for s2s"
P18-1026,W17-4770,0,0.13252,"Missing"
P18-1026,W16-6603,0,0.571969,"† School of Computing and Information Systems University of Melbourne, Australia {d.beck,t.cohn}@unimelb.edu.au ‡ Faculty of Information Technology Monash University, Australia gholamreza.haffari@monash.edu Abstract mars (Jones et al., 2012). A key limitation of these approaches is that alignments between graph nodes and surface tokens are required. These alignments are usually automatically generated so they can propagate errors when building the grammar. More recent approaches transform the graph into a linearised form and use off-the-shelf methods such as phrase-based machine translation (Pourdamghani et al., 2016) or neural sequenceto-sequence (henceforth, s2s) models (Konstas et al., 2017). Such approaches ignore the full graph structure, discarding key information. In this work we propose a model for graph-tosequence (henceforth, g2s) learning that leverages recent advances in neural encoder-decoder architectures. Specifically, we employ an encoder based on Gated Graph Neural Networks (Li et al., 2016, GGNNs), which can incorporate the full graph structure without loss of information. Such networks represent edge information as label-wise parameters, which can be problematic even for small sized labe"
P18-1026,D17-1035,0,0.0255313,"pacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001) and perform bootstrap resampling to check statistical significance. However, since recent work has questioned the effectiveness of BLEU with bootstrap resampling (Graham et al., 2014), we also report results using sentence-level CHR F++ (Popovi´c, 2017), using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics. Recent work has shown that evaluation in neural models can lead to wrong conclusions by just changing the random seed (Reimers and Gurevych, 2017). In an effort to make our conclusions more robust, we run each model 5 times using different seeds. From each pool, we report 4.2 Results and analysis Table 1 shows the results on the test set. For the s2s models, we also report results without the scope marking procedure of Konstas et al. (2017). Our approach significantly outperforms the s2s baselines both with individual models and ensembles, while using a comparable number of parameters. In particular, we obtain these results without relying on scoping heuristics. On Figure 3 we show an example where our model outperforms the baseline. Th"
P18-1115,E17-3017,0,0.0292378,"of TED talks and their respective translations. We trained models to Data Arabic Czech French German Train Dev Test 224,125 6,746 2,762 114,389 5,326 2,762 220,399 5,937 2,762 196,883 6,996 2,762 Table 1: Number of parallel sentence pairs for each language paired with English for IWSLT data. translate from English into Arabic, Czech, French and German. The number of sentences for each language after preprocessing is shown in Table 1. The vocabulary was split into 50,000 subword units using Google’s sentence piece3 software in its standard settings. As our baseline NMT systems we use Sockeye (Hieber et al., 2017)4 . Sockeye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2. We report baselines with and without dropout (Srivastava et al., 2014). For dropout a retention probability of 0.5 was used. As a second baseline we use our own implementation of the model of Zhang et al. (2016) which contains a single sentence-level Gaussian latent variable (SENT). Our implementation differs from theirs in three aspects. First, we feed the last hidden state of the bidirectional encoding into encoding of the source and target sentence into the in"
P18-1115,K16-1002,0,0.185215,"s are available at https://github.com/philschulz/ stochastic-decoder. † Work done prior to joining Amazon. Our proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data. The noise sources are modelled as Gaussian random variables. The contributions of this work are: • The introduction of an NMT system that is capable of capturing word-level variation in translation data. • A thorough discussions of issues encountered when training this model. In particular, we motivate the use of KL scaling as introduced by Bowman et al. (2016) theoretically. 1 Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence. The mapping from the output distribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system. 1243 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1243–1252 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics • An empirical demonstration of the improvements achievable"
P18-1115,D16-1031,0,0.0310306,"fect introduces a long range dependency between the main and auxiliary verb (underlined) that the model handles well. The second example shows variation in the lexical realisation of the verb. The second variant uses a particle verb and we again observe a long range dependency between the main verb and its particle (underlined). els have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016). 7 on several language pairs. As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future. Here, we list the three which we believe to be most promising. Conclusion and Future Work We have presented a recurrent decoder for machine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora. Our experiments confirm our intuition that modelling variation is crucial to the success of machine translation. The proposed model consistently outperforms st"
P18-1115,P02-1040,0,0.103839,"ility vector returned by the softmax. 3 In practice, of course, human translators’ performance varies according to their level of education, their experience on the job, their familiarity with the textual domain and myriads of other factors. Even within a single translator variation may occur due to level of stress, tiredness or status of health. That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing one machinegenerated translation against several human translations (see e.g. Papineni et al., 2002). Prior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable. Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations. A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions. Multimodal modelling of these variations is th"
P18-1115,D17-1066,0,0.0152791,"ur model (SDEC) and the sentent-level latent variable model (SENT). The first SDEC example shows alternation between the German simple past and past perfect. The past perfect introduces a long range dependency between the main and auxiliary verb (underlined) that the model handles well. The second example shows variation in the lexical realisation of the verb. The second variant uses a particle verb and we again observe a long range dependency between the main verb and its particle (underlined). els have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016). 7 on several language pairs. As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future. Here, we list the three which we believe to be most promising. Conclusion and Future Work We have presented a recurrent decoder for machine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translatio"
P18-1115,D16-1050,0,0.45503,"experience on the job, their familiarity with the textual domain and myriads of other factors. Even within a single translator variation may occur due to level of stress, tiredness or status of health. That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing one machinegenerated translation against several human translations (see e.g. Papineni et al., 2002). Prior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable. Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations. A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions. Multimodal modelling of these variations is thus called for—and our results confirm this intuition. An example of variation comes from free word order and agreement phenomena in morphologic"
P18-1115,P17-1029,0,0.0516846,"RNN (ti−1 , yi−1 , zi ) q(z0n ) = q(z0 ) n ∏ q(zi |z<i ) . (9) i=1 (8) The remaining computations stay unchanged. Notice that the latent values are used directly in updating the decoder state. This makes the decoder state a function of a random variable and thus the decoder state is itself random. Applying this argument recursively shows that also the attention mechanism is random, making the decoder entirely stochastic. 4 that use several variables usually employ a mean field approximation under which all latent variables are independent. This turns the ELBO into a sum of expectations (e.g. Zhou and Neubig, 2017). For our stochastic decoder we design a more flexible approximation posterior family which respects the dependencies between the latent variables, Our stochastic decoder can be viewed as a stack of conditional DGMs (Sohn et al., 2015) in which the latent variables depend on one another. The ELBO thus consists of nested positional ELBOs, ELBO0 + Eq(z0 ) [ELBO1 +Eq(z1 ) [ELBO2 + . . .]] , (10) where for a given target position i the ELBO is Inference and Training ELBOi = Eq(zi ) [log p(yi |xm 1 , y<i , z<i , zi )] We use variational inference (see e.g. Blei et al., 2017) to train the model. In"
P18-1181,W15-0705,1,0.794367,"typical rhyming scheme being ABAB CDCD EFEF GG. There are a number of variants, however, mostly seen in the quatrains; e.g. AABB or ABBA are also common. We build our sonnet dataset from the latest image of Project Gutenberg.4 We first create a 3 There are other forms of sonnets, but the Shakespearean sonnet is the dominant one. Hereinafter “sonnet” is used to specifically mean Shakespearean sonnets. 4 https://www.gutenberg.org/. Partition #Sonnets #Words Train Dev Test 2685 335 335 367K 46K 46K Table 1: SONNET dataset statistics. (generic) poetry document collection using the GutenTag tool (Brooke et al., 2015), based on its inbuilt poetry classifier and rule-based structural tagging of individual poems. Given the poems, we use word and character statistics derived from Shakespeare’s 154 sonnets to filter out all non-sonnet poems (to form the “BACKGROUND” dataset), leaving the sonnet corpus (“SONNET”).5 Based on a small-scale manual analysis of SONNET, we find that the approach is sufficient for extracting sonnets with high precision. BACKGROUND serves as a large corpus (34M words) for pre-training word embeddings, and SONNET is further partitioned into training, development and testing sets. Statis"
P18-1181,W14-4012,0,0.155437,"Missing"
P18-1181,D10-1051,0,0.80635,"rains with stress and rhyme patterns that are indistinguishable from human-written poems and rated highly by an expert; • a vanilla language model trained over our sonnet corpus, surprisingly, captures meter implicitly at human-level performance; • while crowd workers rate the poems generated by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; • most work on poetry generation focuses on meter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability. In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Asso"
P18-1181,P17-1016,0,0.423851,"istinguishable from human-written poems and rated highly by an expert; • a vanilla language model trained over our sonnet corpus, surprisingly, captures meter implicitly at human-level performance; • while crowd workers rate the poems generated by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; • most work on poetry generation focuses on meter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability. In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), p"
P18-1181,D14-1181,0,0.00252216,"s and preceding context improves performance substantially, reducing perplexity by almost 10 points from LM to LM∗∗ . The inferior performance of LM∗∗ -C compared to LM∗∗ demonstrates that our approach of processing context with recurrent networks with selective encoding is more effective than convolutional networks. The full model LM∗∗ +PM+RM, which learns stress 20 In Zhang and Lapata (2014), the authors use a series of convolutional networks with a width of 2 words to convert 5/7 poetry lines into a fixed size vector; here we use a standard convolutional network with max-pooling operation (Kim, 2014) to process the context. Pentameter Model To assess the pentameter model, we use the attention weights to predict stress patterns for words in the test data, and compare them against stress patterns in the CMU pronunciation dictionary.21 Words that have no coverage or have nonalternating patterns given by the dictionary are discarded. We use accuracy as the metric, and a predicted stress pattern is judged to be correct if it matches any of the dictionary stress patterns. To extract a stress pattern for a word from the model, we iterate through the pentameter (10 time steps), and append the app"
P18-1181,W09-2005,0,0.105737,"d with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1948–1958 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simplified by incorporating an attention mechanism and training at the character level. For English poetry, Ghazvininejad et al. (2016) introduced a finite-state acceptor to explicitly model rhythm in conjunc"
P18-1181,E17-2025,0,0.0475363,"), and optimise the model with standard categorical cross-entropy loss. We use dropout as regularisation (Srivastava et al., 2014), and apply it to the encoder/decoder LSTM outputs and word embedding lookup. The same regularisation method is used for the pentameter and rhyme models. As our sonnet data is relatively small for training a neural language model (367K words; see Table 1), we pre-train word embeddings and reduce parameters further by introducing weight-sharing between output matrix Wout and embedding matrix Wwrd via a projection matrix Wprj (Inan et al., 2016; Paulus et al., 2017; Press and Wolf, 2017): the pentameter model learns to attend to the appropriate characters to predict the 10 binary stress symbols sequentially.11 As punctuation is not pronounced, we preprocess each sonnet line to remove all punctuation, leaving only spaces and letters. Like the language model, the pentameter model is fashioned as an encoder–decoder network. In the encoder, we embed the characters using the shared embedding matrix Wchr and feed them to the shared bidirectional character-level LSTM (Equation (1)) to produce the character encodings for the sentence: uj = [~uj ; u~j ]. In the decoder, it attends to"
P18-1181,P11-2014,0,0.504514,"ncluded in the dictionary are discarded. Rhyme is determined by extracting the final stressed phoneme for the paired words, and testing if their phoneme patterns match. We predict rhyme for a word pair by feeding them to the rhyme model and computing cosine similarity; if a word pair is assigned a score > 0.8,23 it is considered to rhyme. As a baseline (Rhyme-BL), we first extract for each word the last vowel and all following consonants, and predict a word pair as rhyming if their extracted sequences match. The extracted sequence can be interpreted as a proxy for the last syllable of a word. Reddy and Knight (2011) propose an unsupervised model for learning rhyme schemes in poems via EM. There are two latent variables: φ specifies the distribution of rhyme schemes, and θ defines 23 0.8 is empirically found to be the best threshold based on development data. the pairwise rhyme strength between two words. The model’s objective is to maximise poem likelihood over all possible rhyme scheme assignments under the latent variables φ and θ. We train this model (Rhyme-EM) on our data24 and use the learnt θ to decide whether two words rhyme.25 Table 2 details the rhyming results. The rhyme model performs very str"
P18-1181,P17-1099,0,0.0158761,"that it can simply ignore u∗t to predict the alternating stresses based on gt . For this reason we use only u∗t to compute the stress probability: P (S − ) = σ(We u∗t + be ) P which gives the loss Lent = t − log P (St? ) for the whole sequence, where St? is the target stress at time step t. We find the decoder still has the tendency to attend to the same characters, despite the incorporation of position information. To regularise the model further, we introduce two loss penalties: repeat and coverage loss. The repeat loss penalises the model when it attends to previously attended characters (See et al., 2017), and is computed as follows: Lrep = XX t min(fjt , j t−1 X fjt ) t=1 By keeping a sum of attention weights over all previous time steps, we penalise the model when it focuses on characters that have non-zero history weights. The repeat loss discourages the model from focussing on the same characters, but does not assure that the appropriate characters receive attention. Observing that stresses are aligned with the vowels of a syllable, we therefore penalise the model when vowels are ignored: Lcov = X j∈V ReLU(C − 10 X fjt ) t=1 where V is a set of positions containing vowel characters, and C"
P18-1181,D14-1074,0,0.589919,"Association for Computational Linguistics (Long Papers), pages 1948–1958 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simplified by incorporating an attention mechanism and training at the character level. For English poetry, Ghazvininejad et al. (2016) introduced a finite-state acceptor to explicitly model rhythm in conjunction with a recurrent neural language model for generation. Hopkins and Kiela (2017) improve rhythm modelling with a cascade of weighted state transducers, and demonstrate the use of character-level language model for English poetry. A critical difference over our work is"
P18-1181,P17-1101,0,0.0192447,"a number of variations in addition to the standard pattern (Greene et al., 2010), but our model uses only the standard pattern as it is the dominant one. 1949 (a) Language model (b) Pentameter model (c) Rhyme model Figure 2: Architecture of the language, pentameter and rhyme models. Colours denote shared weights. ter model to sample meter-conforming sentences and the rhyme model to enforce rhyme. The architecture of the joint model is illustrated in Figure 2. We train all the components together by treating each component as a sub-task in a multitask learning setting.8 a selective mechanism (Zhou et al., 2017) to each hi . By defining the representation of the whole context h = [~hC ; h~1 ] (where C is the number of words in the context), the selective mechanism filters the hidden states hi using h as follows: 4.1 where denotes element-wise product. Hereinafter W, U and b are used to refer to model parameters. The intuition behind this procedure is to selectively filter less useful elements from the context words. In the decoder, we embed words xt in the current line using the encoder-shared embedding matrix (Wwrd ) to produce wt . In addition to the word embeddings, we also embed the characters of"
P18-1187,D10-1124,0,0.571092,"Missing"
P18-1187,C12-1064,1,0.905405,"256, and 930 for G EOT EXT, T WITTER -US, and T WITTER -W ORLD, respectively. 3.2 Constructing the Views We build matrix Aˆ as in Equation 1 using the collapsed @-mention graph between users, where two users are connected (Aij = 1) if one mentions the other, or they co-mention another user. The text view is a BoW model of user content with binary term frequency, inverse document frequency, and l2 normalisation of samples. 3.3 Data We use three existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), (2) T WITTER -US (Roller et al., 2012), and (3) T WITTER -W ORLD (Han et al., 2012). These datasets have been used widely for training and evaluation of geolocation models. They are all pre-partitioned into training, development and test Model Selection For GCN, we use highway layers to control the amount of neighbourhood information passed to a node. We use 3 layers in GCN with size 300, 600, 900 for G EOT EXT, T WITTER -US and T WITTER W ORLD respectively. Note that the final softmax layer is also graph convolutional, which sets the radius of the averaging neighbourhood to 4. The 2011 k-d tree bucket size hyperparameter which controls the maximum number of users in each cl"
P18-1187,W16-3929,0,0.129149,"Missing"
P18-1187,P17-1116,0,0.819088,"ge that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (inductive) Rahimi et al. (2017b) Wing and Baldridge (2014) Cha et al. (2015) 38 — — 844 — 581 389 — 425 54 48 — 554 686 — 120 191 — 34 31 — 1456 1669 — 415 509 — Net"
P18-1187,D17-1016,1,0.81063,"adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (induc"
P18-1187,P15-2104,1,0.825044,"f GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (inductive) Rahimi et al. (2017b) Wing and Baldridge"
P18-1187,P17-2033,1,0.801368,"adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (induc"
P18-1187,N15-1153,1,0.864948,"f GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (inductive) Rahimi et al. (2017b) Wing and Baldridge"
P18-1187,W03-0108,0,0.109296,"Missing"
P18-1187,P11-1096,0,0.674526,"Missing"
P18-1187,D14-1039,0,0.592556,"re 5 when no gates are used. We see that by adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Med"
P18-1187,W15-3821,0,0.279426,"Missing"
P18-1187,D12-1137,0,0.669846,"nput to another neural network (right), which is trained supervisedly to predict locations. sets. Each user is represented by the concatenation of their tweets, and labelled with the latitude/longitude of the first collected geotagged tweet in the case of G EOT EXT and T WITTER -US, and the centre of the closest city in the case of T WITTER -W ORLD. G EOT EXT and T WITTER -US cover the continental US, and T WITTER -W ORLD covers the whole world, with 9k, 449k and 1.3m users, respectively. The labels are the discretised geographical coordinates of the training points using a k-d tree following Roller et al. (2012), with the number of labels equal to 129, 256, and 930 for G EOT EXT, T WITTER -US, and T WITTER -W ORLD, respectively. 3.2 Constructing the Views We build matrix Aˆ as in Equation 1 using the collapsed @-mention graph between users, where two users are connected (Aij = 1) if one mentions the other, or they co-mention another user. The text view is a BoW model of user content with binary term frequency, inverse document frequency, and l2 normalisation of samples. 3.3 Data We use three existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), (2) T WITTER -US (Roller"
P18-2005,P07-1056,0,0.0893091,"Cohn School of Computing and Information Systems The University of Melbourne, Australia yitongl4@student.unimelb.edu.au {tbaldwin,tcohn}@unimelb.edu.au Abstract is rarely fundamental to the task of modelling language, and is better considered as a confounding influence. These auxiliary learning signals can mean the models do not adequately capture the core linguistic problem. In such situations, removing these confounds should give better generalisation, especially for out-of-domain evaluation, a similar motivation to research in domain adaptation based on selection biases over text domains (Blitzer et al., 2007; Daum´e III, 2007). Another related problem is privacy: texts convey information about their author, often inadvertently, and many individuals may wish to keep this information private. Consider the case of the AOL search data leak, in which AOL released detailed search logs of many of their users in August 2006 (Pass et al., 2006). Although they deidentified users in the data, the log itself contained sufficient personally identifiable information that allowed many of these individuals to be identifed (Jones et al., 2007). Other sources of user text, such as emails, SMS messages and social m"
P18-2005,P07-1033,0,0.33862,"Missing"
P18-2005,P15-1073,0,0.071204,"heir background, and personal attributes such as gender, age, education and nationality. This variation can have a substantial effect on NLP models learned from text (Hovy et al., 2015), leading to significant variation in inferences across different types of corpora, such as the author’s native language, gender and age. Training corpora are never truly representative, and therefore models fit to these datasets are biased in the sense that they are much more effective for texts from certain groups of user, e.g., middle-aged white men, and considerably poorer for other parts of the population (Hovy, 2015). Moreover, models fit to language corpora often fixate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (Zhao et al., 2017), or translation choices (Rabinovich et al., 2017). This signal, however, 25 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 25–30 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics processing, such as grammar correction or translation. The transfered representations may still contain sensitive information, however,"
P18-2005,E17-1101,0,0.0234177,"on in inferences across different types of corpora, such as the author’s native language, gender and age. Training corpora are never truly representative, and therefore models fit to these datasets are biased in the sense that they are much more effective for texts from certain groups of user, e.g., middle-aged white men, and considerably poorer for other parts of the population (Hovy, 2015). Moreover, models fit to language corpora often fixate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (Zhao et al., 2017), or translation choices (Rabinovich et al., 2017). This signal, however, 25 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 25–30 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics processing, such as grammar correction or translation. The transfered representations may still contain sensitive information, however, especially if an adversary has preliminary knowledge of the training model, in which case they can readily reverse engineer the input, for example, by a GAN attack algorithm (Hitaj et al., 2017). This is true even when differentia"
P18-2005,P15-2079,0,0.129384,"ate, training is based on standard gradient backpropagation for learning the main task, but for the auxiliary task, we start with standard loss backpropagation, however gradients are reversed in sign when they reach h. Consequently the linear output components are trained to be good predictors, but h is trained to be maximally good for the main task and maximally poor for the auxiliary task. Furthermore, Equation 1 can be expanded to scenarios with several (N ) protected attributes, θˆ = min max θM {θ i }N D i=1 X (ˆ y(x; θM ), y) − N  X Data We use the TrustPilot English POS tagged dataset (Hovy and Søgaard, 2015), which consists of 600 sentences, each labelled with both the sex and age of the author, and manually POS tagged based on the Google Universal POS tagset (Petrov et al., 2012). For the purposes of this paper, we follow Hovy and Søgaard’s setup, categorising SEX into female (F) and male (M), and AGE into over-45 (O45) and under-35 (U35). We train the taggers both with and without the adversarial loss, denoted ADV and BASELINE, respectively. For evaluation, we perform a 10-fold cross validation, with a train:dev:test split using ratios of 8:1:1. We also follow the evaluation method in Hovy and"
P18-2005,D17-1323,0,0.0903437,"t al., 2015), leading to significant variation in inferences across different types of corpora, such as the author’s native language, gender and age. Training corpora are never truly representative, and therefore models fit to these datasets are biased in the sense that they are much more effective for texts from certain groups of user, e.g., middle-aged white men, and considerably poorer for other parts of the population (Hovy, 2015). Moreover, models fit to language corpora often fixate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (Zhao et al., 2017), or translation choices (Rabinovich et al., 2017). This signal, however, 25 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 25–30 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics processing, such as grammar correction or translation. The transfered representations may still contain sensitive information, however, especially if an adversary has preliminary knowledge of the training model, in which case they can readily reverse engineer the input, for example, by a GAN attack algorithm (Hitaj"
P18-2005,N16-1130,0,0.045199,"ovy and Søgaard (2015), by reporting the tagging accuracy for sentences over different slices of the data based on SEX and AGE, and the absolute difference between the two settings. Considering the tiny quantity of text in the TrustPilot corpus, we use the Web English Treebank (WebEng: Bies et al. (2012)), as a means of pre-training the tagging model. WebEng was chosen to be as similar as possible in domain to the TrustPilot data, in that the corpus includes unedited user generated internet content. As a second evaluation set, we use a corpus of African-American Vernacular English (AAVE) from Jørgensen et al. (2016), which is used purely for held-out evaluation. AAVE consists of three very heterogeneous domains: LYRICS, SUBTI TLES and TWEETS . Considering the substantial (2)  λi · X (ˆb(x; θDi ), bi ) . i=1 3 Experiments In this section, we report experimental results for our methods with two very different language tasks. 3.1 POS-tagging This first task is part-of-speech (POS) tagging, framed as a sequence tagging problem. Recent demographic studies have found that the author’s gender, age and race can influence tagger performance (Hovy and Søgaard, 2015; Jørgensen et al., 2016). Therefore, we use the"
P18-2005,D14-1181,0,0.0109704,"Missing"
P18-2005,P12-3005,1,0.75166,"s demonstrate that our model can learn relatively gender and age debiased representations, while simultaneously improving the predictive performance, both for indomain and out-of-domain evaluation scenarios. 3.2 LYRICS Data We again use the TrustPilot dataset derived from Hovy et al. (2015), however now we consider the RATING score as the target variable, not POS-tag. Each review is associated with three further attributes: gender (SEX), age (AGE), and location (LOC). To ensure that LOC cannot be trivially predicted based on the script, we discard all non-English reviews based on LANGID . PY (Lui and Baldwin, 2012), by retaining only reviews classified as English with a confidence greater than 0.9. We then subsample 10k reviews for each location to balance the five location classes (US, UK, Germany, Denmark, and France), which were highly skewed in the original dataset. We use the same binary representation of SEX and AGE as the POS task, following the setup in Hovy et al. (2015). To evaluate the different models, we perform 10-fold cross validation and report test performance in terms of the F1 score for the RATING task, and the accuracy of each discriminator. Note that the discriminator can be applied"
P18-2005,petrov-etal-2012-universal,0,0.0253308,"are reversed in sign when they reach h. Consequently the linear output components are trained to be good predictors, but h is trained to be maximally good for the main task and maximally poor for the auxiliary task. Furthermore, Equation 1 can be expanded to scenarios with several (N ) protected attributes, θˆ = min max θM {θ i }N D i=1 X (ˆ y(x; θM ), y) − N  X Data We use the TrustPilot English POS tagged dataset (Hovy and Søgaard, 2015), which consists of 600 sentences, each labelled with both the sex and age of the author, and manually POS tagged based on the Google Universal POS tagset (Petrov et al., 2012). For the purposes of this paper, we follow Hovy and Søgaard’s setup, categorising SEX into female (F) and male (M), and AGE into over-45 (O45) and under-35 (U35). We train the taggers both with and without the adversarial loss, denoted ADV and BASELINE, respectively. For evaluation, we perform a 10-fold cross validation, with a train:dev:test split using ratios of 8:1:1. We also follow the evaluation method in Hovy and Søgaard (2015), by reporting the tagging accuracy for sentences over different slices of the data based on SEX and AGE, and the absolute difference between the two settings. Co"
P18-2030,P15-2030,1,0.933608,"representatives directly. Our objective is to predict the popularity of a petition at the end of its lifetime, solely based on the petition text. Elnoshokaty et al. (2016) is the closest work to this paper, whereby they target Change.org petitions and perform correlation analysis of popularity with the petition’s category, target goal set,2 and the distribution of words in General Inquirer categories (Stone et al., 1962). In our case, we are interested in the task of automatically predicting the number of signatures. We build on the convolutional neural network (CNN) text regression model of Bitvai and Cohn (2015) to infer deep latent features. In addition, we evaluate the effect of an auxiliary ordinal regression objective, which can discriminate petitions that attract different scales of popularOnline petitions are a cost-effective way for citizens to collectively engage with policy-makers in a democracy. Predicting the popularity of a petition — commonly measured by its signature count — based on its textual content has utility for policymakers as well as those posting the petition. In this work, we model this task using CNN regression with an auxiliary ordinal regression objective. We demonstrate t"
P18-2030,D14-1162,0,0.0857336,"signature count. An outline of the model is provided in Figure 1. A petition has three parts: (1) title, (2) main content, and (3) (optionally) additional details.3 We concatenate all three parts to form a single document for each petition. We have n petitions as input training examples of the form {ai , yi }, where ai and yi denote the text and signature count of petition i, respectively. Note that we log-transform the signature count, consistent with previous work (Elnoshokaty et al., 2016; Proskurnia et al., 2017). We represent each token in the document via its pretrained GloVe embedding (Pennington et al., 2014), which we update during learning. We then apply multiple convolution filters with width one, two and three to the dense input document matrix, and apply a ReLU to each. They are then passed through a max-pooling layer with a tanh activation function, and finally a multi-layer perceptron via the exponential linear unit activation, ( x, if x > 0 f (x) = α (exp(x) − 1) otherwise , that achieve different scale of signatures. The intuition behind this is that there are pre-determined thresholds on signatures which trigger different events, with the most important of these being 10k (to guarantee a"
P18-2030,D17-1070,0,0.020889,"General Inquirer lexicon. • Ratio of biased words (B IAS) from the bias lexicon (Recasens et al., 2013). • Syntactic features: number of nouns (NNC), verbs (VBC), adjectives (ADC) and adverbs (RBC). • Number of named entities (NEC), based on the NLTK NER model (Bird et al., 2009). • Freshness (F RE): cosine similarity with all previous petitions, inverse weighted by the difference in start date of petitions (in weeks). • Action score of title (ACT): probability of title conveying the action requested. Predictions are obtained using an one-class SVM model built on the universal representation (Conneau et al., 2017) of titles of rejected petitions,5 as they don’t contain any action request. These rejected petitions are not part of our evaluation dataset. • Policy category popularity score (C SC): commonality of the petition’s policy issue (Subra20 15 10 0 5 6 7 8 9 10 11 12 Log Signature Count 13 Figure 3: US Petitions Signature Distribution manian et al., 2017), based on the recent UK/US election manifesto promises. • Political bias and polarity: relative lean#left+#right ing/polarity based on: (a) #left+#right+#neutral (P BIAS) (b) #left−#right #left+#right (L–R). Sentence-level left, right and neutral"
P18-2030,P13-1162,0,0.0257489,"brief description of the features below: • Additional Information (A DD): binary flag indicating whether the petition has additional details or not. • Ratio of indefinite (I ND) and definite (D EF) articles. • Ratio of first-person singular pronouns (F SP), first-person plural pronouns (F PP), secondperson pronouns (S PP), third-person singular pronouns (T SP), and third-person plural pronouns (T PP). • Ratio of subjective words (S UBJ) and difference between count of positive and negative words (P OL), based on General Inquirer lexicon. • Ratio of biased words (B IAS) from the bias lexicon (Recasens et al., 2013). • Syntactic features: number of nouns (NNC), verbs (VBC), adjectives (ADC) and adverbs (RBC). • Number of named entities (NEC), based on the NLTK NER model (Bird et al., 2009). • Freshness (F RE): cosine similarity with all previous petitions, inverse weighted by the difference in start date of petitions (in weeks). • Action score of title (ACT): probability of title conveying the action requested. Predictions are obtained using an one-class SVM model built on the universal representation (Conneau et al., 2017) of titles of rejected petitions,5 as they don’t contain any action request. These"
P18-2030,U17-1003,1,0.891779,"Missing"
P18-2030,P14-1017,0,0.148678,"1 The code and data from this paper are available from http://github.com/shivashankarrs/ Petitions 2 See http://bit.ly/2BXd0Sl. 182 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 182–188 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Convolution filters with ReLu activation ity (e.g., 10 signatures, the minimum count needed to not be closed vs. 10k signatures, the minimum count to receive a response from UK government). Finally, motivated by text-based message propagation analysis work (Tan et al., 2014; Piotrkowicz et al., 2017), we hand-engineer features which capture wording effects on petition popularity, and measure the ability of the deep model to automatically infer those features. Word embeddings Hold a referendum on Brexit deal ?? . . Max-pooling 2 Fully Connected Layers > ??1 > ??2 > ???? Custom Features Proposed Approach Figure 1: CNN-Regression Model. y denotes signature count. > rk is the auxiliary task that denotes p(petition attracting > rk signatures). Inspired by the successes of CNN for text categorization (Kim, 2014) and text regression (Bitvai and Cohn, 2015), we propose"
P18-2030,D14-1181,0,0.00392836,"d by text-based message propagation analysis work (Tan et al., 2014; Piotrkowicz et al., 2017), we hand-engineer features which capture wording effects on petition popularity, and measure the ability of the deep model to automatically infer those features. Word embeddings Hold a referendum on Brexit deal ?? . . Max-pooling 2 Fully Connected Layers > ??1 > ??2 > ???? Custom Features Proposed Approach Figure 1: CNN-Regression Model. y denotes signature count. > rk is the auxiliary task that denotes p(petition attracting > rk signatures). Inspired by the successes of CNN for text categorization (Kim, 2014) and text regression (Bitvai and Cohn, 2015), we propose a CNN-based model for predicting the signature count. An outline of the model is provided in Figure 1. A petition has three parts: (1) title, (2) main content, and (3) (optionally) additional details.3 We concatenate all three parts to form a single document for each petition. We have n petitions as input training examples of the form {ai , yi }, where ai and yi denote the text and signature count of petition i, respectively. Note that we log-transform the signature count, consistent with previous work (Elnoshokaty et al., 2016; Proskurn"
P18-2045,W17-0908,0,0.0287999,"Missing"
P18-2045,P14-5010,0,0.00454304,"ect: lij = 1; otherwise lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss of the final prediction of right/wrong endings, we also take into account the memory update gate supervision 280 of each chain by adding the second term. More formally, the model is trained to minimise the loss: L = XEntropy(y, yˆ) + α X XEntropy(lij , gij ) i,j where yˆ and gij are defined in Equations 7 and 4 respective"
P18-2045,D17-1168,0,0.413973,"erent Ending: Sam was happy. Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our model demonstrates superior performance to a collection of competitive baselines, setting a new state of the art. 1 1 Figure 1: Story Cloze Test example. The current state-of-the-art approach of Chaturvedi et al. (2017) is based on understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency. Chaturvedi et al. (2017) adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial in"
P18-2045,D14-1082,0,0.00766782,"e lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss of the final prediction of right/wrong endings, we also take into account the memory update gate supervision 280 of each chain by adding the second term. More formally, the model is trained to minimise the loss: L = XEntropy(y, yˆ) + α X XEntropy(lij , gij ) i,j where yˆ and gij are defined in Equations 7 and 4 respectively, y is the gold label f"
P18-2045,W17-0913,0,0.06733,"he necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by Henaff et al. (2017) for reasoning-focused question answering and clozestyle reading comprehensio"
P18-2045,N10-1138,0,0.0149388,"upervise the memory update gates of these chains, we design three sequences of binary labels: lj = {l1j , l2j , . . . , lTj } for j ∈ [1, 3] representing event, sentiment, and topic, and lij ∈ {0, 1}. The label at time i for the j-th aspect is only assigned a value of 1 if the word is a trigger for that particular aspect: lij = 1; otherwise lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss o"
P18-2045,N16-1098,0,0.31157,"he classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial intelligence (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpor"
P18-2045,W17-0906,0,0.06849,"ves, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by Henaff et al. (2017) for reasoning-focused question answering and clozestyle reading comprehension, we introduce a novel multi-task learning objec"
P18-2045,P16-1028,0,0.0303386,"Missing"
P18-2045,K17-1004,0,0.339321,"e (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstr"
P18-2045,H89-1033,0,0.701499,"understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency. Chaturvedi et al. (2017) adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial intelligence (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al.,"
P19-1015,D15-1039,0,0.0285268,"l: 100 NER annotation, 0: no extra resources. the source and target languages through phonemic transcription (Bharadwaj et al., 2016) or Wikification (Tsai et al., 2016). In annotation projection, the annotations of tokens in a source sentence are projected to their aligned tokens in the target language through a parallel corpus. Annotation projection has been applied to POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014; Fang and Cohn, 2016), NER (Zitouni and Florian, 2008; Ehrmann et al., 2011; Agerri et al., 2018), and parsing (Hwa et al., 2005; Ma and Xia, 2014; Rasooli and Collins, 2015a,b). The Bible, Europarl, and recently the Watchtower has been used as parallel corpora, which are limited in genre, size, and language coverage, motivating the use of Wikipedia to create weak annotation for multilingual tasks such as NER (Nothman et al., 2013). Recent advances in (un)supervised bilingual dictionary induction (Gouws and Søgaard, 2015; Duong et al., 2016; Lample et al., 2018; Artetxe et al., 2018; Schuster et al., 2019) have enabled cross-lingual alignment with bilingual dictionaries (Mayhew et al., 2017; Xie et al., 2018). Most annotation projection methods with few exception"
P19-1015,N19-1162,0,0.0367558,"l., 2014; Fang and Cohn, 2016), NER (Zitouni and Florian, 2008; Ehrmann et al., 2011; Agerri et al., 2018), and parsing (Hwa et al., 2005; Ma and Xia, 2014; Rasooli and Collins, 2015a,b). The Bible, Europarl, and recently the Watchtower has been used as parallel corpora, which are limited in genre, size, and language coverage, motivating the use of Wikipedia to create weak annotation for multilingual tasks such as NER (Nothman et al., 2013). Recent advances in (un)supervised bilingual dictionary induction (Gouws and Søgaard, 2015; Duong et al., 2016; Lample et al., 2018; Artetxe et al., 2018; Schuster et al., 2019) have enabled cross-lingual alignment with bilingual dictionaries (Mayhew et al., 2017; Xie et al., 2018). Most annotation projection methods with few exceptions (T¨ackstr¨om, 2012; Plank and Agi´c, 2018) use only one language (often English) as the source language. In 6 Conclusion Cross-lingual transfer does not work out of the box, especially when using large numbers of source languages, and distantly related target languages. In an NER setting using a collection of 41 languages, we showed that simple methods such as uniform ensembling do not work well. We proposed two new multilingual trans"
P19-1015,D18-1034,0,0.419362,"ted text, and therefore much of the progress in NLP has yet to be realised widely. Cross-lingual transfer learning is a technique which can compensate for the dearth of data, by transferring knowledge from high- to lowresource languages, which has typically taken the form of annotation projection over parallel corpora or other multilingual resources (Yarowsky et al., 2001; Hwa et al., 2005), or making use of transferable representations, such as phonetic transcriptions (Bharadwaj et al., 2016), closely related languages (Cotterell and Duh, 2017) or bilingual dictionaries (Mayhew et al., 2017; Xie et al., 2018). Most methods proposed for cross-lingual transfer rely on a single source language, which limits the transferable knowledge to only one source. ∗ Both authors contributed equally to this work. The code and the datasets will be made available at https://github.com/afshinrahimi/mmner. 1 151 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the Bayesian model, and 2) explicit model selection and fine-tuning of a low-resource supervised model, thus allow"
P19-1015,N12-1052,0,\N,Missing
P19-1015,W12-1908,0,\N,Missing
P19-1015,P14-1126,0,\N,Missing
P19-1015,N09-1010,0,\N,Missing
P19-1015,D14-1096,1,\N,Missing
P19-1015,P11-1061,0,\N,Missing
P19-1015,N13-1014,0,\N,Missing
P19-1015,P15-2139,1,\N,Missing
P19-1015,R11-1017,0,\N,Missing
P19-1015,N15-1157,0,\N,Missing
P19-1015,N16-1030,0,\N,Missing
P19-1015,K16-1018,1,\N,Missing
P19-1015,Q17-1010,0,\N,Missing
P19-1015,D16-1153,0,\N,Missing
P19-1015,P17-1028,0,\N,Missing
P19-1015,D17-1269,0,\N,Missing
P19-1015,I17-2016,0,\N,Missing
P19-1015,L18-1557,0,\N,Missing
P19-1015,W18-6125,0,\N,Missing
P19-1015,W02-2024,0,\N,Missing
P19-1186,N10-1027,1,0.726506,"aining, a single epoch of training took about 25min for the CSDA method, using the default settings, and a similar time for DSDA and M - CNN. The runtime increases sub-linearly with increasing latent size k. 3.3 Language Identification To further demonstrate our approaches, we then evaluate our models with the second task, language identification (LangID: Jauhiainen et al. (2018)). For data processing, we use 5 training sets from 5 different domains with 97 language, following the setup of Lui and Baldwin (2011). We evaluate accuracy over 7 holdout benchmarks: E URO G OV, TCL, W IKIPEDIA from Baldwin and Lui (2010), EMEA (Tiedemann, 2009), E URO PARL (Koehn, 2005), TBE (Tromp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks and the averaged scores. We also report the results of GEN, the best model from Li et al. (2018a), and o"
P19-1186,P07-1056,0,0.876772,"a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning. 1 Introduction Text corpora are often collated from several different sources, such as news, literature, microblogs, and web crawls, raising the problem of learning NLP systems from heterogenous data, and how well such models transfer to testing settings. Learning from these corpora requires models which can generalise to different domains, a problem known as transfer learning or domain adaptation (Blitzer et al., 2007; Daum´e III, 2007; Joshi et al., 2012; Kim et al., 2016). In most stateof-the-art frameworks, the model has full knowledge of the domain of instances in the training data, and the domain is treated as a discrete indicator variable. However, in reality, data is often messy, with domain labels not always available, or providing limited information about the style and genre of text. For example, web-crawled corpora are comprised of all manner of text, such as news, marketing, blogs, novels, and recipes, however the type of each document is typically not explicitly specified. Moreover, even corpo"
P19-1186,K16-1002,0,0.0229715,"thods have been employed to learn robust domain-generalised representations (Liu et al., 2016). Li et al. (2018a) considered the case of the model having no access to the target domain, and using adversarial learning to generate domaingeneration representations by cross-comparison between source domains. The other important component of this work is Variational Inference (“VI”), a method from machine learning that approximates probability densities through optimisation (Blei et al., 2017; Kucukelbir et al., 2017). The idea of a variational auto-encoder has been applied to language generation (Bowman et al., 2016; Kim et al., 2018; Miao et al., 2017; Zhou and Neubig, 2017; Zhang et al., 2016) and machine translation (Shah and Barber, 2018; Eikema and Aziz, 2018), but not in the context of semi-supervised domain adaptation. 5 Conclusion In this paper, we have proposed two models— DSDA and CSDA —for multi-domain learning, which use a graphical model with a latent variable to represent the domain. We propose models with a discrete latent variable, and a continuous vectorvalued latent variable, which we model with Beta or Dirichlet priors. For training, we adopt a variational inference technique based on"
P19-1186,P07-1033,0,0.273361,"Missing"
P19-1186,D12-1119,0,0.0520117,"Missing"
P19-1186,D14-1181,0,0.0057556,"Missing"
P19-1186,2005.mtsummit-papers.11,0,0.0532348,"CSDA method, using the default settings, and a similar time for DSDA and M - CNN. The runtime increases sub-linearly with increasing latent size k. 3.3 Language Identification To further demonstrate our approaches, we then evaluate our models with the second task, language identification (LangID: Jauhiainen et al. (2018)). For data processing, we use 5 training sets from 5 different domains with 97 language, following the setup of Lui and Baldwin (2011). We evaluate accuracy over 7 holdout benchmarks: E URO G OV, TCL, W IKIPEDIA from Baldwin and Lui (2010), EMEA (Tiedemann, 2009), E URO PARL (Koehn, 2005), TBE (Tromp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks and the averaged scores. We also report the results of GEN, the best model from Li et al. (2018a), and one state-of-theart off-the-shelf LangID tool: LANG"
P19-1186,N18-2076,1,0.460956,"Missing"
P19-1186,D16-1012,0,0.0330065,"Missing"
P19-1186,I11-1062,1,0.806149,"ntial sentiment, and some domain knowledge, as observed in Figure 3. In terms of the time required for training, a single epoch of training took about 25min for the CSDA method, using the default settings, and a similar time for DSDA and M - CNN. The runtime increases sub-linearly with increasing latent size k. 3.3 Language Identification To further demonstrate our approaches, we then evaluate our models with the second task, language identification (LangID: Jauhiainen et al. (2018)). For data processing, we use 5 training sets from 5 different domains with 97 language, following the setup of Lui and Baldwin (2011). We evaluate accuracy over 7 holdout benchmarks: E URO G OV, TCL, W IKIPEDIA from Baldwin and Lui (2010), EMEA (Tiedemann, 2009), E URO PARL (Koehn, 2005), TBE (Tromp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks"
P19-1186,P12-3005,1,0.786528,"omp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks and the averaged scores. We also report the results of GEN, the best model from Li et al. (2018a), and one state-of-theart off-the-shelf LangID tool: LANGID . PY (Lui and Baldwin, 2012). Note that, both S - CNN and M - CNN are domain unsupervised methods. In terms of results, overall, both of our CSDA models consistently outperform all other baseline models. Comparing the different CSDA variants, Beta vs. Dirichlet, both perform closely across the LangID tasks. Furthermore, CSDA out-performs the state-of-the-art in terms of average scores. Interestingly the two training configurations show that domain knowledge F provides a small performance boost for CSDA, but not does help for DSDA . Above all, the LangID results confirm the effectiveness of our proposed approaches. 4 Rela"
P19-1186,C16-1038,0,0.416007,"substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning. 1 Introduction Text corpora are often collated from several different sources, such as news, literature, microblogs, and web crawls, raising the problem of learning NLP systems from heterogenous data, and how well such models transfer to testing settings. Learning from these corpora requires models which can generalise to different domains, a problem known as transfer learning or domain adaptation (Blitzer et al., 2007; Daum´e III, 2007; Joshi et al., 2012; Kim et al., 2016). In most stateof-the-art frameworks, the model has full knowledge of the domain of instances in the training data, and the domain is treated as a discrete indicator variable. However, in reality, data is often messy, with domain labels not always available, or providing limited information about the style and genre of text. For example, web-crawled corpora are comprised of all manner of text, such as news, marketing, blogs, novels, and recipes, however the type of each document is typically not explicitly specified. Moreover, even corpora that are labelled with a specific domain might themsel"
P19-1186,D16-1050,0,0.029098,"et al., 2016). Li et al. (2018a) considered the case of the model having no access to the target domain, and using adversarial learning to generate domaingeneration representations by cross-comparison between source domains. The other important component of this work is Variational Inference (“VI”), a method from machine learning that approximates probability densities through optimisation (Blei et al., 2017; Kucukelbir et al., 2017). The idea of a variational auto-encoder has been applied to language generation (Bowman et al., 2016; Kim et al., 2018; Miao et al., 2017; Zhou and Neubig, 2017; Zhang et al., 2016) and machine translation (Shah and Barber, 2018; Eikema and Aziz, 2018), but not in the context of semi-supervised domain adaptation. 5 Conclusion In this paper, we have proposed two models— DSDA and CSDA —for multi-domain learning, which use a graphical model with a latent variable to represent the domain. We propose models with a discrete latent variable, and a continuous vectorvalued latent variable, which we model with Beta or Dirichlet priors. For training, we adopt a variational inference technique based on the variational autoencoder. In empirical evaluation over a multi-domain sentimen"
P19-1186,P17-1029,0,0.0180063,"d representations (Liu et al., 2016). Li et al. (2018a) considered the case of the model having no access to the target domain, and using adversarial learning to generate domaingeneration representations by cross-comparison between source domains. The other important component of this work is Variational Inference (“VI”), a method from machine learning that approximates probability densities through optimisation (Blei et al., 2017; Kucukelbir et al., 2017). The idea of a variational auto-encoder has been applied to language generation (Bowman et al., 2016; Kim et al., 2018; Miao et al., 2017; Zhou and Neubig, 2017; Zhang et al., 2016) and machine translation (Shah and Barber, 2018; Eikema and Aziz, 2018), but not in the context of semi-supervised domain adaptation. 5 Conclusion In this paper, we have proposed two models— DSDA and CSDA —for multi-domain learning, which use a graphical model with a latent variable to represent the domain. We propose models with a discrete latent variable, and a continuous vectorvalued latent variable, which we model with Beta or Dirichlet priors. For training, we adopt a variational inference technique based on the variational autoencoder. In empirical evaluation over a"
P19-1269,W05-0909,0,0.51463,"neni et al., 2002) and TER (Snover et al., 2006) use n-gram matching or more explicit word alignment to match the system output with the reference translation. Character-level variants such as BEER, CHR F and C HARAC T ER overcome the problem of harshly penalising morphological variants, and perform surprisingly well despite their simplicity (Stanojevic and Sima’an, 2014; Popovi´c, 2015; Wang et al., 2016). In order to allow for variation in word choice and sentence structure, other metrics use information from shallow linguistic tools such as POStaggers, lemmatizers and synonym dictionaries (Banerjee and Lavie, 2005; Snover et al., 2006; Liu et al., 2010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More r"
P19-1269,W15-3001,0,0.0549887,"f WMT 2016, which is entirely crowdsourced. The sentence-level-metric evaluation data consists of accurate scores for 560 translations each for 6 to-English language pairs and English-to-Russian (we call this the “TrainS” dataset). The dataset also includes mostly singly-annotated2 DA scores for around 125 thousand translations from six source languages into English, and 12.5 thousand translations from English-to-Russian (“TrainL” dataset), that were collected to obtain human scores for MT systems. For the validation set, we use the sentencelevel DA judgements collected for the WMT 2015 data (Bojar et al., 2015): 500 translation-reference pairs each of four to-English language pairs, and English-to-Russian. For more details on implementation and training of our models, see Appendix A. We test our metrics on all language pairs from the WMT 2017(Bojar et al., 2017b) news task in both the sentence and system level setting, and evaluate using Pearson’s correlation between our metrics’ predictions and the Human DA scores. For the sentence level evaluation, insufficient DA annotations were collected for five fromEnglish language pairs, and these were converted to preference judgements. If two MT system tra"
P19-1269,W07-0738,0,0.0858882,"Missing"
P19-1269,D14-1020,1,0.882017,"h ρ Baselines S ENT-BLEU CHR F BEER MEANT 2.0- NOSRL MEANT 2.0 0.274 0.376 0.398 0.395 – 0.269 0.336 0.336 0.324 – 0.446 0.503 0.557 0.565 – 0.259 0.420 0.420 0.425 – 0.468 0.605 0.569 0.636 – 0.377 0.466 0.490 0.482 – 0.642 0.608 0.622 0.705 0.727 T P Table 1: Pearson’s r on the WMT 2017 sentence-level evaluation data. P: Unsupervised metric that relies on pretrained embeddings; TrainS: trained on accurate 3360 instances; TrainL: trained on noisy 125k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold (William’s test; Graham and Baldwin, 2014) BERT R ESIM 0.390 0.338 0.365 0.362 0.564 0.523 0.417 0.350 0.630 0.700 0.457 0.506 0.803 0.699 Table 2: Pearson’s r and Kendall’s τ on the WMT 2017 from-English system-level evaluation data. The first section represents existing metrics, both trained and untrained. We then present results of our unsupervised metric, followed by our supervised metric trained in the TrainL setting: noisy 125k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold (William’s test (Graham and Baldwin, 2014) for Pearson’s r and Bootstrap (Efro"
P19-1269,N15-1124,1,0.879144,"led and max-pooled hidden states of this BiLSTM. To compute the final predicted score, we apply a feedforward regressor over the concatenation of the two sentence representations. p = [vr,avg ; vr,max ; vt,avg ; vt,max ] (12) 0 (13) | | y = w ReLU(W p + b) + b For all models, the predicted score of an MT system is the average predicted score of all its translations in the testset. 4 Experimental Setup We use human evaluation data from the Conference on Machine Translation (WMT) to train and evaluate our models (Bojar et al., 2016, 2017a), which is based on the Direct Assessment (“DA”) method (Graham et al., 2015, 2017). Here, system translations are evaluated by humans in comparison to a human reference translation, using a continuous scale (Graham et al., 2015, 2017). Each annotator assesses a set of 100 items, of which 30 items are for quality control, which is used to filter out annotators who are unskilled or careless. Individual worker scores are first standardised, and then the final score of an MT system is computed as the average score across all translations in the test set. Manual MT evaluation is subjective and difficult, and it is not possible even for a diligent human to be entirely cons"
P19-1269,D15-1124,0,0.248337,"Missing"
P19-1269,P14-1065,0,0.0512274,"Missing"
P19-1269,W17-4755,0,0.274311,"s mostly singly-annotated2 DA scores for around 125 thousand translations from six source languages into English, and 12.5 thousand translations from English-to-Russian (“TrainL” dataset), that were collected to obtain human scores for MT systems. For the validation set, we use the sentencelevel DA judgements collected for the WMT 2015 data (Bojar et al., 2015): 500 translation-reference pairs each of four to-English language pairs, and English-to-Russian. For more details on implementation and training of our models, see Appendix A. We test our metrics on all language pairs from the WMT 2017(Bojar et al., 2017b) news task in both the sentence and system level setting, and evaluate using Pearson’s correlation between our metrics’ predictions and the Human DA scores. For the sentence level evaluation, insufficient DA annotations were collected for five fromEnglish language pairs, and these were converted to preference judgements. If two MT system translations of a source sentence were evaluated by at least two reliable annotators, and the average score for System A is reasonably greater than the average score of System B, then this is interpreted as a Relative Ranking (DA RR) judgement where Sys A is"
P19-1269,W10-1754,0,0.0340102,") use n-gram matching or more explicit word alignment to match the system output with the reference translation. Character-level variants such as BEER, CHR F and C HARAC T ER overcome the problem of harshly penalising morphological variants, and perform surprisingly well despite their simplicity (Stanojevic and Sima’an, 2014; Popovi´c, 2015; Wang et al., 2016). In order to allow for variation in word choice and sentence structure, other metrics use information from shallow linguistic tools such as POStaggers, lemmatizers and synonym dictionaries (Banerjee and Lavie, 2005; Snover et al., 2006; Liu et al., 2010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More recently, metrics such as MEANT 2.0 (Lo,"
P19-1269,W12-3103,0,0.0254921,"ics use information from shallow linguistic tools such as POStaggers, lemmatizers and synonym dictionaries (Banerjee and Lavie, 2005; Snover et al., 2006; Liu et al., 2010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More recently, metrics such as MEANT 2.0 (Lo, 2017) have adopted word embeddings (Mikolov et al., 2013) to capture the semantics of individual words. However, classic word embeddings are independent of word context, and context is captured instead using hand-crafted features or heuristics. Neural metrics such as ReVal and RUSE solve this problem by directly learning embeddings of the entire translation and reference sentences. ReVal (Gupta et al., 2015) learns sentence representations of the MT o"
P19-1269,W17-4767,0,0.130085,"010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More recently, metrics such as MEANT 2.0 (Lo, 2017) have adopted word embeddings (Mikolov et al., 2013) to capture the semantics of individual words. However, classic word embeddings are independent of word context, and context is captured instead using hand-crafted features or heuristics. Neural metrics such as ReVal and RUSE solve this problem by directly learning embeddings of the entire translation and reference sentences. ReVal (Gupta et al., 2015) learns sentence representations of the MT output and reference translation as a Tree-LSTM, and then models their interactions using the element-wise difference and angle between the two. RUSE ("
P19-1269,P17-1152,0,0.223068,"rd embeddings (Peters et al., 2018; Devlin et al., 2019), a technique which captures rich and portable representations of words in context, which have been shown to provide important signal to many other NLP tasks (Rajpurkar et al., 2018). We propose a simple untrained model that uses off-the{tbaldwin,tcohn}@unimelb.edu.au shelf contextual embeddings to compute approximate recall, when comparing a reference to an automatic translation, as well as trained models, including: a recurrent model over reference and translation sequences, incorporating attention; and the adaptation of an NLI method (Chen et al., 2017) to MT evaluation. These approaches, though simple in formulation, are highly effective, and rival or surpass the best approaches from WMT 2017. Moreover, we show further improvements in performance when our trained models are learned using noisy crowd-sourced data, i.e., having single annotations for more instances is better than collecting and aggregating multiple annotations for single instances. The net result is an approach that is more data efficient than existing methods, while producing substantially better human correlations.1 2 Related work MT metrics attempt to automatically predict"
Q15-1033,P11-1022,0,0.0277316,"0.4777 VBP WHNP NN JJ SQ 0.4653 0.4508 0.4274 0.4021 0.4000 Table 3: Top 15 symbols sorted according to their obtained λ values in the SASSTKfull model with fixed α. The numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the correspo"
Q15-1033,D14-1190,1,0.899543,"Missing"
Q15-1033,W14-3338,1,0.917946,"riances into account, grid search would still need around 10 times more computation 4 For specific details on the SVM models used in all experiments performed in this paper we refer the reader to Appendix A. 467 Emotion Analysis The goal of Emotion Analysis is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels for each text (one for each emotion) instead of a single label. Beck et al. (2014a) used a multi-task GP for this task with a bag-of-words feature representation. In theory, it is possible to combine their multi-task kernel with our tree kernels, but to keep the focus of the experiments on testing tree kernel approaches, here we use independently trained models, one per emotion. Dataset We use the dataset provided by the “Affective Text” shared task in SemEval2007 (Strapparava and Mihalcea, 2007), which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Surprise. For each emotion, a score between 0 and 100 is given"
Q15-1033,C04-1046,0,0.231901,"Missing"
Q15-1033,P13-1004,1,0.946875,"of these issues, but have several limitations (see §6 for details). Our proposed approach for model selection relies on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a widely used Bayesian kernel machine. GPs allow efficient and fine-grained model selection by maximizing the evidence on the training data using gradient-based methods, dropping the requirement for development data. As a Bayesian procedure, GPs also naturally balance between model capacity and generalization. GPs have been shown to achieve state of the art performance in various regression tasks (Hensman et al., 2013; Cohn and Specia, 2013). Therefore, we base our approach on this framework. While prediction performance is important to consider (as we show in our experiments), we are 461 Transactions of the Association for Computational Linguistics, vol. 3, pp. 461–473, 2015. Action Editor: Stefan Riezler. Submission batch: 2/2015; Revision batch 7/2015; Published 8/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. mainly interested in two other significant aspects that are enabled by our approach: • Gradient-based methods are more efficient than grid search for high dimensional space"
Q15-1033,W12-3112,1,0.856136,"rresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator. • Eng"
Q15-1033,2011.eamt-1.32,1,0.860626,"he numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited b"
Q15-1033,P10-1064,0,0.0293227,"Missing"
Q15-1033,D14-1219,0,0.0166756,"re space. Also, their method does not take into account the underlying learning algorithm. Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geo"
Q15-1033,P14-5010,0,0.00433599,"on. Grid search is embarassingly parallelizable since each grid point can run in a different core. However, the GP optimization can also benefit from multiple cores by running each kernel computation inside the Gram matrix in parallel. To keep the comparisons simpler, the results shown in this section use a single core but all experiments in §5 employ parallelization in the Gram matrix computation level (for both SVM and GP models). 5 NLP Experiments Our experiments with NLP data address two regression tasks: Emotion Analysis and Quality Estimation. For both tasks, we use the Stanford parser (Manning et al., 2014) to obtain constituency trees for all sentences. Also, rather than using data official splits, we perform 5-fold cross-validation in order to obtain more reliable results. 5.1 Figure 4: Results from performance experiments. The x axis corresponds to wall clock time in seconds and it is in log scale. The y axis shows RMSE on the test set. The blue dashed line corresponds to the RMSE value obtained after L-BFGS converged. Error bars are obtained by measuring one standard deviation over the 20 runs made in each experiment. We can see that optimizing the GP model is consistently much faster than d"
Q15-1033,J08-2003,0,0.0337766,"Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very r"
Q15-1033,E06-1015,0,0.586128,"0 pr(n1 ) 6= pr(n2 )    λ pr(n1 ) = pr(n2 ) ∧ ∆(n1 , n2 ) =  preterm(n1 )    λg(n , n ) otherwise, 1 2 where pr(n) is the grammar production at node n and preterm(n) returns true if n is a pre-terminal node. The function g is defined as follows: g(n1 , n2 ) = |n1 | Y (α + ∆(cin1 , cin2 )) , (3) i=1 where |n |is the number of children of node n and cin is the ith child of node n. This recursive definition is calculated efficiently by employing dynamic programming to cache intermediate ∆ results. Equation 3 also adds another hyperparameter, α. This hyperparameter was introduced by Moschitti (2006b)3 as a way to select between two different tree kernels. If α = 1, we get the original SSTK, if α = 0, then we obtain the Subtree Kernel, which only allows fragments with terminal symbols 3 In his original formulation, this hyperparameter was named σ but here we use α to not confuse it with the GP noise hyperparameter. as leaves. We can also interpret the Subtree Kernel as a “sparse” version of the SSTK, where the “nonsubtree” fragments have their weights equal to zero. Even though fragment weights are affected by both kernel hyperparameters, previous work did not discuss their effects. The"
Q15-1033,W10-2926,0,0.0265089,"t contains complete grammar rules (see Figure 1 for an example). Consider the set of nodes in the two trees as N1 and N2 respectively. We define Ii (n) as an indicator function that returns 1 if fragment fi ∈ F has root n and 0 otherwise. A SSTK can then be defined as: X X k(t1 , t2 ) = ∆(n1 , n2 ) , (2) n1 ∈N1 n2 ∈N2 where ∆(n1 , n2 ) = |F | X λ s(i) 2 Ii (n1 )Ii (n2 ) i=1 and s(i) is the number of fragments in i with at least one child2 . The formulation in Equation 2 is the same as the one shown in Equation 1, except that we are now restricting the weights w(f ) to be a function of a 2 See Pighin and Moschitti (2010) for details and a proof on this derivation. 463 Tree S A B A B a b A B A B A B A B a S b Fragments S S a S b a b Figure 1: An example tree and the respective set of tree fragments defined by a SSTK. hyperparameter λ. The original goal of λ is to act as a decay factor that penalizes contributions from larger fragments cf smaller ones (and therefore, it should be in the [0, 1] interval). Without this factor, the resulting distribution over tree pairs is skewed, giving extremely large values when trees are equal and rapidly decreasing for small differences over fragment counts. The decay factor"
Q15-1033,P13-1147,0,0.0315825,"ion is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very recently they have been successfully employed in a fe"
Q15-1033,D13-1100,1,0.910314,"Missing"
Q15-1033,S14-2138,0,0.0219346,"Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very recently they have been successfully employed in a few NLP tasks such as translation quality estimation (Cohn and Specia, 2013; Beck et al., 2014b), detection of temporal patterns in text (Preot¸iuc-Pietro and Cohn, 2013), semantic similarity (Rios and Specia, 2014) and emotion analysis (Beck et al., 2014a). In terms of feature 471 representations, previous work focused on the vectorial inputs and applied well-known kernels for these inputs, e.g. the RBF kernel. As shown on §5.2, our approach is orthogonal to these previous ones, since kernels can be easily combined in different ways. It is important to note that we are not the first ones to combine GPs with kernels on structured inputs. Driessens et al. (2006) employed a combination of GPs and graph kernels for reinforcement learning. However, unlike our approach, they did not attempt model selection, e"
Q15-1033,2009.eamt-1.5,1,0.873699,"de a quality prediction for new, unseen machine translated texts (Blatz et al., 2004; Bojar et al., 2014). ExamJJR PRP$ WDT RBR VBG 0.8333 0.6933 0.6578 0.5445 0.5163 WHADVP QP JJS NNS . 0.5004 0.5001 0.4996 0.4961 0.4777 VBP WHNP NN JJ SQ 0.4653 0.4508 0.4274 0.4021 0.4000 Table 3: Top 15 symbols sorted according to their obtained λ values in the SASSTKfull model with fixed α. The numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to te"
Q15-1033,P13-4014,1,0.861222,"Missing"
Q15-1033,2011.eamt-1.12,1,0.842829,"ction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator. • English-Spanish (en-es): This dataset was used in the WMT14 Quality Estimation shared task (Bojar et al., 2014), containing 858 sentences translated from English into Spanish and post-edited by an expert translator. For each dataset, post-editing times are first divided by the translation output length (obtaining the post-editing time per word) and then mean normalized. 469 Models Since our data consists of pairs of trees, our models in this task use a pair of tree kernels. We combine these two ke"
Q15-1033,D09-1010,0,0.0764496,"Missing"
Q15-1033,S07-1013,0,\N,Missing
Q16-1034,D07-1090,0,0.0624305,"uirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition. Statistical LMs are probabilistic models that assign a probability to a sequence of words w1N , i"
Q16-1034,buck-etal-2014-n,0,0.0354647,"Missing"
Q16-1034,D07-1021,0,0.0254431,"on time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamen"
Q16-1034,W09-1505,0,0.0196707,"high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine transl"
Q16-1034,D10-1026,0,0.0183067,"te only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. In"
Q16-1034,P13-2121,0,0.0264381,"s much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition. Statistical LMs are probabilistic models that assign a probability to a seq"
Q16-1034,W11-2123,0,0.443499,"model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang a"
Q16-1034,kennington-etal-2012-suffix,0,0.0228694,"ries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition. Statistical LMs are probabilistic models that assign a probability to a sequence of words w1N , indicating how likely the sequence is in the language. m-gram LMs are popular, and prove to be accurate when estimated using large corpora. In these LMs, the probabilities of m-grams are often precomputed and stored explicitly. Although widely suc"
Q16-1034,2005.mtsummit-papers.11,0,0.0203928,"ory and time usage, along with the predictive perplexity score of word-level LMs on a number of different corpora varying in size and domain. For all of our word-level LMs, we use m, ¯ m ˆ ≤ 10. We also demonstrate the positive impact of increasing the set limit on m, ¯ m ˆ from 10 to 50 on improving characterlevel LM perplexity. The SDSL library (Gog et al., 2014) is used to implement our data structures. The benchmarking experiments were run on a single core of a Intel Xeon E5-2687 v3 3.10GHz server with 500GiB of RAM. In our word-level experiments, we use the German subset of the Europarl (Koehn, 2005) as a small corpus, which is 382 MiB in size measuring the raw uncompressed text. We also evaluate on much larger corpora, training on 32GiB subsets of the deduplicated English, Spanish, German, and French Common Crawl corpus (Buck et al., 2014). As test sets, we used newstest-2014 for all languages except Spanish, for which we used newstest-2013.11 In our 9 Although the SA can be very large, we need not store it in memory. The DFS traversal in Algorithm 4 (lines 4–16) means that the calls to SA` occur in increasing order of `. Hence, we use on-disk storage for the SA with a small memory mappe"
Q16-1034,D09-1079,0,0.0224297,"y runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient"
Q16-1034,P11-1027,0,0.0231557,"increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language mod"
Q16-1034,D15-1288,1,0.116456,"ign a probability to a sequence of words w1N , indicating how likely the sequence is in the language. m-gram LMs are popular, and prove to be accurate when estimated using large corpora. In these LMs, the probabilities of m-grams are often precomputed and stored explicitly. Although widely successful, current m-gram LM approaches are impractical for learning high-order LMs on large corpora, due to their poor scaling properties in both training and query phases. Prevailing methods (Heafield, 2011; Stolcke et al., 2011) precompute all m-gram probabilities, and consequently In our previous work (Shareghi et al., 2015), we extended this line of research using a Compressed Suffix Tree (C ST) (Ohlebusch et al., 2010), which provides a considerably more compact searchable means of storing the corpus than an uncompressed suffix array or suffix tree. This approach showed favourable scaling properties with m and had only a modest memory requirement. However, the method only supported Kneser-Ney smoothing, not its modified variant (Chen and Goodman, 1999) which overall performs better and has become the de-facto standard. Additionally, querying was significantly slower than for leading LM toolkits, making the meth"
Q16-1034,P07-1065,0,0.0326882,"sations which improve query runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text"
Q16-1034,P06-1124,0,0.0649117,"ess, with only a modest increase in construction time and memory usage, yet improving query runtimes up to 2500×. In benchmarking against the state-of-the-art KenLM package on large corpora, our method has superior memory usage and highly competitive runtimes for both querying and training. Our approach allows easy experimentation with high order language models, and our results provide evidence that such high orders are most useful when using large training sets. We posit that further perplexity gains can be realised using richer smoothing techniques, such as a non-parametric Bayesian prior (Teh, 2006; Wood et al., 2011). Our ongoing work will explore this avenue, as well as integrating our language model into the Moses machine translation system, and improving the querying time by caching the lower order probabilities (e.g., m < 4) which we believe can improve query time substantially while maintaining a modest memory footprint. 6 Acknowledgements unit time (s) mem (GiB) m = 5 m = 10 m = 20 m = ∞ word 8164 character 17 935 6.29 18.58 73.45 68.66 3.93 2.69 68.76 2.37 68.80 2.33 Table 4: Perplexity results for the 1 billion word benchmark corpus, showing word based and character based MKN m"
Q16-1034,P09-2086,0,0.0213007,"ur method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recogn"
S19-1030,P15-1162,0,0.0499894,"Missing"
S19-1030,J96-2004,0,0.691381,"Missing"
S19-1030,D09-1130,0,0.0313251,"ed to significant advances in the field of computational social science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). With the increasing availability of datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). One task of particular importance is the analysis of the functional 274 pensate for the sparsity of labeled data is semisupervised learning, making use of auxiliary unlabeled data, as done previously for speech act classification in e-mail and forum text (Jeong et al., 2009). Zhang et al. (2012) also used semisupervised methods for speech act classification over Twitter data. They used transductive SVM and graph-based label propagation approaches to annotate unlabeled data using a small seed training set. Joty and Mohiuddin (2018) leveraged outof-domain labeled data based on a domain adversarial learning approach. In this work, we focus on target based speech act analysis (with a custom class-set) for political campaign text and use a deep-learning approach by incorporating contextualized word representations (Peters et al., 2018) and a cross-view training framew"
S19-1030,P16-1165,0,0.0238446,"eases in the lead-up to a federal election, where we expect there to be rich discourse and interplay between political parties. Speech act theory is fundamental to study such discourse and pragmatics (Austin, 1962; Searle, 1976). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Due to its predominantly small-data setting, speech act classification approaches have generally relied on bag-of-words models (Qadir and Riloff, 2011; Vosoughi and Roy, 2016), although recent approaches have used deep-learning models through data augmentation (Joty and Hoque, 2016) and learning word representations for the target domain (Joty and Mohiuddin, 2018), outperforming traditional bag-ofwords approaches. Another technique that has been applied to comical text is relatively new. Most speech act analyses in the political domain have relied exclusively on manual annotation, and no labeled data has been made available for training classifiers. As it is expensive to obtain large-scale annotations, in addition to developing a novel annotated dataset, we also experiment with a semi-supervised approach by utilizing unlabeled text, which is easy to obtain. The contribut"
S19-1030,J18-4012,0,0.0795045,"scourse and interplay between political parties. Speech act theory is fundamental to study such discourse and pragmatics (Austin, 1962; Searle, 1976). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Due to its predominantly small-data setting, speech act classification approaches have generally relied on bag-of-words models (Qadir and Riloff, 2011; Vosoughi and Roy, 2016), although recent approaches have used deep-learning models through data augmentation (Joty and Hoque, 2016) and learning word representations for the target domain (Joty and Mohiuddin, 2018), outperforming traditional bag-ofwords approaches. Another technique that has been applied to comical text is relatively new. Most speech act analyses in the political domain have relied exclusively on manual annotation, and no labeled data has been made available for training classifiers. As it is expensive to obtain large-scale annotations, in addition to developing a novel annotated dataset, we also experiment with a semi-supervised approach by utilizing unlabeled text, which is easy to obtain. The contributions of this paper are as follows: (1) we introduce the novel task of target based"
S19-1030,I11-1068,0,0.484793,"its to a future course of action. Following the work of Artés (2011) and Naurin (2011), we distinguish between action and outcome commissives. Action commissives (commissive-action) are those in which an action is to be taken, while outcome commissives (commissive-outcome) can be defined as a description of reality or goals. Secondly, similar to Naurin (2014) we also classify action commissives into vague (commissive-action-vague) and specific (commissive-action-specific), according to their specificity. This distinction is also related to text specificity analysis work addressed in the news (Louis and Nenkova, 2011) and classroom discussion (Lugini and Litman, 2017) domains. A directive occurs when the speaker expects the listener to take action in response. In an expressive, the speaker expresses their psychological state, while a past-action denotes a retrospective action of the target party, and a verdictive refers to an assessment on prospective or retrospective actions. Examples of the eight speech act classes are Further, the following (from the Labor party) has segments comparing L ABOR and L IBERAL: (3) Our party is united – the Liberals are not united. 4 Election Campaign Dataset We collected me"
S19-1030,W17-5006,0,0.159028,"k of Artés (2011) and Naurin (2011), we distinguish between action and outcome commissives. Action commissives (commissive-action) are those in which an action is to be taken, while outcome commissives (commissive-outcome) can be defined as a description of reality or goals. Secondly, similar to Naurin (2014) we also classify action commissives into vague (commissive-action-vague) and specific (commissive-action-specific), according to their specificity. This distinction is also related to text specificity analysis work addressed in the news (Louis and Nenkova, 2011) and classroom discussion (Lugini and Litman, 2017) domains. A directive occurs when the speaker expects the listener to take action in response. In an expressive, the speaker expresses their psychological state, while a past-action denotes a retrospective action of the target party, and a verdictive refers to an assessment on prospective or retrospective actions. Examples of the eight speech act classes are Further, the following (from the Labor party) has segments comparing L ABOR and L IBERAL: (3) Our party is united – the Liberals are not united. 4 Election Campaign Dataset We collected media releases and speeches from the two major Austra"
S19-1030,W18-5214,0,0.0158292,"et party classes. “Speaker” denotes the party making the utterance. intent of utterances in political text. Though it has received notable attention from many political scientists (see Section 1), the primary focus of almost all work has been to derive insights from manual annotations, and not to study computational approaches to automate the task. Another related task in the political communication domain is reputation defense, in terms of party credibility. Recently, Duthie and Budzynska (2018) proposed an approach to mine ethos support/attack statements from UK parliamentary debates, while Naderi and Hirst (2018) focused on classifying sentences from Question Time in the Canadian parliament as defensive or not. In this work, our source data is speeches and press releases in the lead-up to a federal election, where we expect there to be rich discourse and interplay between political parties. Speech act theory is fundamental to study such discourse and pragmatics (Austin, 1962; Searle, 1976). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Due to its predominantly small-data setting, speech act classification approaches have generally relied on"
S19-1030,W12-0603,0,0.0251888,"ances in the field of computational social science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). With the increasing availability of datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). One task of particular importance is the analysis of the functional 274 pensate for the sparsity of labeled data is semisupervised learning, making use of auxiliary unlabeled data, as done previously for speech act classification in e-mail and forum text (Jeong et al., 2009). Zhang et al. (2012) also used semisupervised methods for speech act classification over Twitter data. They used transductive SVM and graph-based label propagation approaches to annotate unlabeled data using a small seed training set. Joty and Mohiuddin (2018) leveraged outof-domain labeled data based on a domain adversarial learning approach. In this work, we focus on target based speech act analysis (with a custom class-set) for political campaign text and use a deep-learning approach by incorporating contextualized word representations (Peters et al., 2018) and a cross-view training framework (Clark et al., 20"
S19-1030,I17-1071,0,0.150769,"al party referred to in the text — in order to determine the discourse structure. Here, we study the effect of jointly modeling the speech act and target referent of each utterance, in order to exploit the task dependencies. That is, this paper is an application of discourse analysis to the pragmaticsrich domain of political science, to determine the intent of every utterance made by politicians, and in part, automatically extract pledges at varying levels of specificity from campaign speeches and press releases. We assume that each utterance is associated with a unique speech act (similar to Zhao and Kawahara (2017)) and target party,1 meaning that a sentence with multiple speech acts and/or targets must be segmented into component utterances. Take the following example, from the Labor Party: We study pragmatics in political campaign text, through analysis of speech acts and the target of each utterance. We propose a new annotation schema incorporating domainspecific speech acts, such as commissiveaction, and present a novel annotated corpus of media releases and speech transcripts from the 2016 Australian election cycle. We show how speech acts and target referents can be modeled as sequential classific"
S19-1030,D14-1162,0,0.0831537,"Missing"
S19-1030,N18-1202,0,0.188242,"ication in e-mail and forum text (Jeong et al., 2009). Zhang et al. (2012) also used semisupervised methods for speech act classification over Twitter data. They used transductive SVM and graph-based label propagation approaches to annotate unlabeled data using a small seed training set. Joty and Mohiuddin (2018) leveraged outof-domain labeled data based on a domain adversarial learning approach. In this work, we focus on target based speech act analysis (with a custom class-set) for political campaign text and use a deep-learning approach by incorporating contextualized word representations (Peters et al., 2018) and a cross-view training framework (Clark et al., 2018) to leverage in-domain unlabeled text. 3 # Doc # Sent # Utt Avg Utterance Length 258 6609 7641 19.3 Table 2: Dataset Statistics: number of documents, number of sentences, number of utterances, and average utterance length given in Table 1, along with the target party (L ABOR, L IBERAL, or N ONE), indicating which party the speech act is directed towards, and the “speaker” party making the utterance (information which is provided for every utterance). 3.1 Utterance Segmentation Sentences are segmented both in the context of speech act and"
U03-1011,P91-1034,0,0.168079,"Missing"
U03-1011,P94-1020,0,0.0269119,"en problems in the field of natural language processing, and in recent years has attracted considerable research interest (Ide and Veronis, 1998). The increasing availability of large corpora along with electronic sense inventories (such as WordNet; Fellbaum (1998)) has permitted the application of a raft of machine learning techniques to the task and provided an empirical means of performance evaluation. Until recently, most performance evaluation was conducted on disparate data sets, with only the line and interest corpora being used in a significant number of studies (Leacock et al., 1993; Bruce and Wiebe, 1994). SENSEVAL, a global evaluation performed in 1998 (Kilgarriff, 1998) and again in 2001 (Edmonds and Cotton, 2001), provided a common set of disambiguation tasks and performance evaluation criteria, allowing an objective comparison between competing methods. These workshops included the tasks of disambiguating all words in a given text (the all-words task), and disambiguating each occurrence of a given word when it appears with a short context of a few surrounding sentences (the lexical sample task). Performance in the two tasks was measured in terms of precision and recall. Precision was defin"
U03-1011,W98-1507,0,0.148843,"Missing"
U03-1011,J96-2004,0,0.0854093,"Missing"
U03-1011,S01-1001,0,0.0250738,"ch interest (Ide and Veronis, 1998). The increasing availability of large corpora along with electronic sense inventories (such as WordNet; Fellbaum (1998)) has permitted the application of a raft of machine learning techniques to the task and provided an empirical means of performance evaluation. Until recently, most performance evaluation was conducted on disparate data sets, with only the line and interest corpora being used in a significant number of studies (Leacock et al., 1993; Bruce and Wiebe, 1994). SENSEVAL, a global evaluation performed in 1998 (Kilgarriff, 1998) and again in 2001 (Edmonds and Cotton, 2001), provided a common set of disambiguation tasks and performance evaluation criteria, allowing an objective comparison between competing methods. These workshops included the tasks of disambiguating all words in a given text (the all-words task), and disambiguating each occurrence of a given word when it appears with a short context of a few surrounding sentences (the lexical sample task). Performance in the two tasks was measured in terms of precision and recall. Precision was defined as the proportion of classified instances that were correctly classified, and recall as the proportion of inst"
U03-1011,J98-1001,0,0.020426,"disambiguation performance. The current metrics – accuracy, precision and recall – while suitable for two-way classification, are shown to be inadequate when disambiguating between three or more senses. Specifically, these measures do not facilitate comparison with baseline performance nor are they sensitive to non-uniform misclassification costs. Both of these issues can be addressed using ROC analysis. 1 Introduction Word sense disambiguation (WSD) is one of the large open problems in the field of natural language processing, and in recent years has attracted considerable research interest (Ide and Veronis, 1998). The increasing availability of large corpora along with electronic sense inventories (such as WordNet; Fellbaum (1998)) has permitted the application of a raft of machine learning techniques to the task and provided an empirical means of performance evaluation. Until recently, most performance evaluation was conducted on disparate data sets, with only the line and interest corpora being used in a significant number of studies (Leacock et al., 1993; Bruce and Wiebe, 1994). SENSEVAL, a global evaluation performed in 1998 (Kilgarriff, 1998) and again in 2001 (Edmonds and Cotton, 2001), provided"
U03-1011,W93-0102,0,0.0377459,"is one of the large open problems in the field of natural language processing, and in recent years has attracted considerable research interest (Ide and Veronis, 1998). The increasing availability of large corpora along with electronic sense inventories (such as WordNet; Fellbaum (1998)) has permitted the application of a raft of machine learning techniques to the task and provided an empirical means of performance evaluation. Until recently, most performance evaluation was conducted on disparate data sets, with only the line and interest corpora being used in a significant number of studies (Leacock et al., 1993; Bruce and Wiebe, 1994). SENSEVAL, a global evaluation performed in 1998 (Kilgarriff, 1998) and again in 2001 (Edmonds and Cotton, 2001), provided a common set of disambiguation tasks and performance evaluation criteria, allowing an objective comparison between competing methods. These workshops included the tasks of disambiguating all words in a given text (the all-words task), and disambiguating each occurrence of a given word when it appears with a short context of a few surrounding sentences (the lexical sample task). Performance in the two tasks was measured in terms of precision and rec"
U03-1011,W02-0109,0,0.0176581,"ter results than (weighted) random combinations of the trivial classifiers will be considered (3). This method has the added benefit of being robust in the face of changing or imprecise misclassification costs. While it does not provide a readily interpretable measure (4), especially when considering the convex hull in high dimensional space, the AUC can provide such a measure. 4 Empirical results and discussion I have implemented three supervised WSD methods and analysed their performance using the three measures described above. All development was performed in the Natural Language Toolkit (Loper and Bird, 2002) and the source code is available as part of the toolkit. I implemented Yarowsky’s (1994) decision list method, which he used for accent restoration in French and Spanish text (roughly similar to homograph disambiguation). This method uses the single most reliable piece of evidence in predicting the sense. I also implemented Brown et al.’s (1991) method, which was used for MT between French and English using decision trees to resolve the correct translation of each ambiguous word. Training uses the flipflop algorithm (Nadas et al., 1991) to determine which feature will maximise the mutual info"
U03-1011,J00-2011,0,0.0313113,"ectly predicted senses within the set of instances for which the algorithm hazarded a prediction, and recall as the proportion of correctly predicted senses over all instances. This implicitly allows classifiers to opt not to classify every instance. However non-exhaustive classifiers are of limited use, given that they must be combined with other classifiers in order to fully disambiguate a given text. Many tasks in which WSD forms a sub-task, such as machine translation (MT), require the word to be fully disambiguated – an unknown value is unacceptable. Plotting the precision-recall curves (Manning and Schutze, 2000) allows for better performance ranking by optimising precision for a given level of recall. This goes some way in addressing the issues when assessing precision and recall with respect to criterion (1), however the problem ex2 Note also that selecting nothing will not yield a precision value at all, due to a division by zero. ists as to what recall limit is acceptable – there is no theoretical justification for choosing a specific value, and modifying the value may well alter the rankings of the classifiers. The F-measure (a harmonic mean between precision and recall), may be used for simpler"
U03-1011,W96-0208,0,0.0605588,"Missing"
U03-1011,P94-1013,0,0.0248637,"Missing"
U03-1011,P06-4018,0,\N,Missing
U16-1001,P14-1129,0,0.084663,"Missing"
U16-1001,D07-1091,0,0.535235,"). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better translation of OOVs (or low-count words) and resolve ambiguities, hence increase the generalization capabilities of the model. In machine translation with a low-resource setting, resolving data sparseness and semantic ambiguity problems can help improve its performance. In this paper, we investigate utilizi"
U16-1001,N03-1017,0,0.0679502,"able results and improvements over conventional SMT (Luong et al., 2015). The core idea of NMT is the encoder-decoder framework where an encoder encodes the source sequence into a vector representation, and then a decoder generates the target sequence sequentially via a recurrent neural network (RNN). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better tran"
U16-1001,2012.eamt-1.60,0,0.0155488,"linguistic layers independently, and compute layer-specific context vectors {c`i }L `=0 and stack them up:  T ci = c0i , . . . , cL i ; c`i = Tx X n=1 i=1 `=0 (n),` where αi is the attention to the layer ` when generating the target word i, and we define (n) (n),` 1 PL ¯ i := L+1 as the average attenα `=0 αi tion across all layers. Essentially, our regularizer penalizes parameters which induce layer-specific attentions deviating from the average attention. ` ` αij hj j=1 α`i = softmax(e`i ) ; 2   e`ij = MLP gi−1 ; h`j 3 Experiments Data. We conducted our experiments on TED Talks datasets (Cettolo et al., 2012) and translate between English (en) ↔ German (de). For training, we used about 200K parallel sentences, and used tst2010 for tuning model parameters (phrasebased SMT) and early stopping (NMT). We evaluated on the official test sets tst2013 and tst2014, where e`ij denotes the alignment score between the annotation at layer ` and the target word. The MLP for each layer has a different parameterization. Global-Local Attention. Finally, we consider a hybrid global-local attention mechanism which 9 dataset train tune-tst2010 test1-tst2013 test2-tst2014 # tokens (K) 4384.68 35.13 22.86 27.40 # types"
U16-1001,P07-2045,0,0.0478143,"ility to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better translation of OOVs (or low-count words) and resolve ambiguities, hence increase the generalization capabilities of the model. In machine translation with a low-resource setting, resolving data sparseness and semantic ambiguity problems can help improve its performance. In this paper, we investigate utilizing extra syntactic and semantic linguistic factors in the context of the NMT framework. Linguistic factors can inclu"
U16-1001,2014.iwslt-evaluation.1,0,0.0992157,"64 embedding dimensions for each of lemma, word cluster, Part-of-Speech (POS), and labelled dependency sequences, respectively. For training our neural models, the best perplexity scores on tuning sets were used for early stopping of training, which was usually between 5-8 epochs. For decoding, we used a simple greedy algorithm with length normalization. For evaluation of translations, we applied bootstrapping resampling (Koehn, 2004) to measure the statistical significance (p &lt; 0.05) of BLEU score differences between translation outputs of proposed models compared to the baselines. following Cettolo et al. (2014). We chose a word frequency cut-off of ≥ 5 for limiting the vocabulary when training neural models, resulting in 19K and 26K word types for English and German, respectively. All details of data statistics can be found in Table 1. As linguistic factors, we annotated the source sentences with lemmas,2 word clusters,3 and POS tags. We also annotated with the labelled dependency, i.e. by taking the dependency label between each word and its head (together with its direction, i.e. left or right)4 in the dependency parse tree. Also note that the POS tags and dependency parse trees were extracted fro"
U16-1001,D14-1179,0,0.0503059,"Missing"
U16-1001,W04-3250,0,0.0931051,"ces. For the phrasebased SMT baseline, we used the Moses toolkit (Koehn et al., 2007) with its standard configuration. To encode the linguistic factors, we used 128, 64, 64, 64 embedding dimensions for each of lemma, word cluster, Part-of-Speech (POS), and labelled dependency sequences, respectively. For training our neural models, the best perplexity scores on tuning sets were used for early stopping of training, which was usually between 5-8 epochs. For decoding, we used a simple greedy algorithm with length normalization. For evaluation of translations, we applied bootstrapping resampling (Koehn, 2004) to measure the statistical significance (p &lt; 0.05) of BLEU score differences between translation outputs of proposed models compared to the baselines. following Cettolo et al. (2014). We chose a word frequency cut-off of ≥ 5 for limiting the vocabulary when training neural models, resulting in 19K and 26K word types for English and German, respectively. All details of data statistics can be found in Table 1. As linguistic factors, we annotated the source sentences with lemmas,2 word clusters,3 and POS tags. We also annotated with the labelled dependency, i.e. by taking the dependency label be"
U16-1001,J10-4005,0,0.0186301,"r conventional SMT (Luong et al., 2015). The core idea of NMT is the encoder-decoder framework where an encoder encodes the source sequence into a vector representation, and then a decoder generates the target sequence sequentially via a recurrent neural network (RNN). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better translation of OOVs (or low-cou"
U16-1001,N16-1102,1,0.834502,"l in resolving ambiguities of source sentences in translation. We formalize sentence complexity by 4 Related Work Recent advances in deep learning research facilitate innovative ideas in machine translation. The attentional encoder-decoder framework pioneered by Bahdanau et al. (2015) is the core, opening a new trend in neural machine translation. Luong et al. (2015) followed the work of (Bahdanau et al., 2015) by experimenting various options on the generation of soft alignments with global and local attention mechanisms. Inspired by remarkable characteristics of state-of-the-art SMT models, Cohn et al. (2016) incorporated structural alignment biases inspired from conventional statistical alignment models (e.g. IBM models 1, 2) to encourage more linguistic structures in the alignment process. Similar in spirit to this, Feng et al. (2016) made use of additional RNN structure for the attention mechanism, hence likely capturing long range dependencies between the attention vectors. Tu et al. (2016) further proposed a socalled coverage vector to trace the attention history for flexibly adjusting future attentions. Though having been developed for almost 2 years, the NMT models are currently competitive"
U16-1001,D15-1166,0,0.276466,"ors into the NMT framework. Evaluating on translating between English and German in two directions with a low resource setting in the domain of TED talks, we obtain promising results in terms of both perplexity reductions and improved BLEU scores over baseline methods. 1 Introduction Neural Machine Translation (NMT) (Devlin et al., 2014; Bahdanau et al., 2015) is a new paradigm in machine translation (MT) powered by recent advances in sequence to sequence learning frameworks (Graves, 2013; Sutskever et al., 2014). NMT has already made remarkable results and improvements over conventional SMT (Luong et al., 2015). The core idea of NMT is the encoder-decoder framework where an encoder encodes the source sequence into a vector representation, and then a decoder generates the target sequence sequentially via a recurrent neural network (RNN). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SM"
U16-1001,P02-1040,0,0.0946994,"Missing"
U16-1001,W11-2155,0,0.0654445,"Missing"
U16-1001,W16-2209,0,0.047294,"Missing"
U16-1001,J82-2005,0,0.770967,"Missing"
U16-1001,D13-1138,0,0.0357817,"Missing"
U16-1001,P07-2046,0,0.0706373,"Missing"
U16-1007,W08-0601,0,0.0298256,"Missing"
U16-1007,P14-5010,0,0.0334903,"arning algorithms such as SVM. Deep learning based approches (Zeng et al., 2014; Xu et al., 2015) are other alternatives to eliminate the manual feature engineering efforts. However, in this work we are primarily focussed on kernel methods. In NLP, kernel methods have been effectively used for relation extraction and sentence classification. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and not edge labels. As a work around, these kernels require that the original dependency graphs be heuristically altered to translate edge labels into special nodes to create different syntactic representations such as the grammatical relation centered tree (Croce et al., 2011). These limitations were overcome with the Approximate Subgraph Nagesh C Panyam, Karin Verspoor, Trevor Cohn and Rao Kotagiri. 2016. ASM Kernel: Graph Kernel using Approximate Subgraph Matching for Relation Extraction. In Proceedings of Australasian Language Technolog"
U16-1007,W16-3001,0,0.0476605,"Missing"
U16-1007,W11-0216,0,0.0199374,"he graph with at most n nodes. Enumerating the features of this graph involves a single traversal of each such path, which translates to a complexity bound of O(n · m2 ) or simply O(n3 ) (a looser upper bound). Finding the shortest paths across all node pairs can be done in O(n3 ) time using standard graph algorithms (Seidel, 1995). F1 49.0 55.5 53.7 58.1 with ASM kernel. We extend the comparison to two well known tree kernels, namely Subset Tree Kernel (SSTK) and the Partial Tree Kernel (PTK) that have been shown to be effective for relation extraction (Zelenko et al., 2002; Moschitti, 2006; Chowdhury et al., 2011). Note that unlike constituency parse trees, dependency trees have edge labels which cannot be handled by these tree kernels. Therefore, the edge labels are converted into node labels of specially inserted nodes in the original dependency graph, to get a modified structure referred to as the Location Centered Tree (LCT) (Lan et al., 2009). Finally, we compare the ASM kernel with tree kernels in a sentence classification task. This is a straightforward application of kernels in a graph classification problem, over the unmodified dependency graphs of the corpus. We used the Java based Kelp frame"
U16-1007,D14-1050,0,0.0579519,"Missing"
U16-1007,D11-1096,0,0.0234242,"fication. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and not edge labels. As a work around, these kernels require that the original dependency graphs be heuristically altered to translate edge labels into special nodes to create different syntactic representations such as the grammatical relation centered tree (Croce et al., 2011). These limitations were overcome with the Approximate Subgraph Nagesh C Panyam, Karin Verspoor, Trevor Cohn and Rao Kotagiri. 2016. ASM Kernel: Graph Kernel using Approximate Subgraph Matching for Relation Extraction. In Proceedings of Australasian Language Technology Association Workshop, pages 65−73. annotations, which are related entity pairs (metoclopramide, dyskinesia). We assume that the relation (causation) is implied by the training sentence and then to try to infer a similar relation or its absence in the test sentence. Matching (ASM) (Liu et al., 2013), that was designed to be a fle"
U16-1007,P15-4004,0,0.0376669,"Missing"
U16-1007,D15-1206,0,0.0267186,"uch as a graph, by transforming it into a flat array of features is inherently harder. This problem of constructing explicit feature sets for complex objects is generally overcome by kernel methods for classification. Kernel methods allow for an implicit exploration of a vast high dimensional feature space and shift the focus from feature engineering to similarity score design. Importantly, such a kernel must be shown to be symmetric and positive semi-definite (Burges, 1998), to be valid for use with kernelized learning algorithms such as SVM. Deep learning based approches (Zeng et al., 2014; Xu et al., 2015) are other alternatives to eliminate the manual feature engineering efforts. However, in this work we are primarily focussed on kernel methods. In NLP, kernel methods have been effectively used for relation extraction and sentence classification. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and not edge labels. As"
U16-1007,W02-1010,0,0.0184454,"h label pair corresponds to a path in the graph with at most n nodes. Enumerating the features of this graph involves a single traversal of each such path, which translates to a complexity bound of O(n · m2 ) or simply O(n3 ) (a looser upper bound). Finding the shortest paths across all node pairs can be done in O(n3 ) time using standard graph algorithms (Seidel, 1995). F1 49.0 55.5 53.7 58.1 with ASM kernel. We extend the comparison to two well known tree kernels, namely Subset Tree Kernel (SSTK) and the Partial Tree Kernel (PTK) that have been shown to be effective for relation extraction (Zelenko et al., 2002; Moschitti, 2006; Chowdhury et al., 2011). Note that unlike constituency parse trees, dependency trees have edge labels which cannot be handled by these tree kernels. Therefore, the edge labels are converted into node labels of specially inserted nodes in the original dependency graph, to get a modified structure referred to as the Location Centered Tree (LCT) (Lan et al., 2009). Finally, we compare the ASM kernel with tree kernels in a sentence classification task. This is a straightforward application of kernels in a graph classification problem, over the unmodified dependency graphs of the"
U16-1007,W11-1801,0,0.0734385,"Missing"
U16-1007,C14-1220,0,0.0647168,"ressive structure such as a graph, by transforming it into a flat array of features is inherently harder. This problem of constructing explicit feature sets for complex objects is generally overcome by kernel methods for classification. Kernel methods allow for an implicit exploration of a vast high dimensional feature space and shift the focus from feature engineering to similarity score design. Importantly, such a kernel must be shown to be symmetric and positive semi-definite (Burges, 1998), to be valid for use with kernelized learning algorithms such as SVM. Deep learning based approches (Zeng et al., 2014; Xu et al., 2015) are other alternatives to eliminate the manual feature engineering efforts. However, in this work we are primarily focussed on kernel methods. In NLP, kernel methods have been effectively used for relation extraction and sentence classification. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and n"
U16-1007,C02-1150,0,0.236301,"Missing"
U16-1007,H05-1091,0,\N,Missing
U17-1002,W14-4337,0,0.0495726,"Missing"
U17-1002,D14-1181,0,0.00276342,"ch performs consistently well over all tasks. We propose a unified model generalising weight tying and in doing so, make the model more expressive. The proposed model achieves uniformly high performance, improving on the best results for memory network-based models on the bAbI dataset, and competitive results on Dialog bAbI. 1 Introduction Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (He et al., 2016), speech recognition (Graves et al., 2013), and various natural language processing tasks (Bahdanau et al., 2014; Kim, 2014; Xiong et al., 2016). Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014, 2016; Rae et al., 2016). Of particular interest to this work is the work by Sukhbaatar et al. (2015), on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning (Weston et al., 2016) and goal-oriented dialogue tasks (Bordes and Weston, 2016). Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question,"
U17-1002,E17-1001,1,0.652188,"ayer-wise; see Section 2 for a technical description). While N2Ns generally work well with either weight tying approach, as reported in Sukhbaatar et al. (2015), the performance is uneven on some difficult tasks. That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed. In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task. This is realised through the use of a gating vector, inspired by Liu and Perez (2017). Our method achieves the best performance for a memory network-based model on the bAbI dataset, superior to both adjacent and layer-wise weight tying, and competitive results on Dialog bAbI. The paper is organised as follows: after we review N2N and related reasoning models in Section 2, we describe our motivation and detail the elements of our proposed model in Section 3. Section 4 and 5 present the experimental results on the bAbI and Dialog bAbI datasets with analyses in Section 6. Lastly, Section 7 concludes the paper. 2 Related Work End-to-End Memory Networks: Building on top of memory n"
U17-1002,W14-4012,0,0.0143448,"Missing"
U17-1003,W13-1106,0,0.028906,"Missing"
U17-1003,I11-1019,0,0.0318106,"dings averaged with TF-IDF scores as weights. All these approaches model the sentence and documentlevel tasks separately. Related Work The recent adoption of NLP methods has led to significant advances in the field of Computational Social Science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). Some popular tasks addressed with political text include: party position analysis (Biessmann, 2016); political leaning categorization (Akoglu, 2014; Zhou et al., 2011); stance classification (Sridhar et al., 2014); identifying keywords, themes & topics (Karan et al., 2016; Ding et al., 2011); emotion analysis (Rheault, 2016); and sentiment analysis (Bakliwal et al., 2013). The source data includes manifestos, political speeches, news articles, floor debates and social media posts. With the increasing availability of large-scale datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). For example, rather than analyzing the political manifestos of a particular party during an election, mining different manifestos across countries over time can provide deeper comparative insights into"
U17-1003,W16-5612,0,0.0450127,"eights. All these approaches model the sentence and documentlevel tasks separately. Related Work The recent adoption of NLP methods has led to significant advances in the field of Computational Social Science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). Some popular tasks addressed with political text include: party position analysis (Biessmann, 2016); political leaning categorization (Akoglu, 2014; Zhou et al., 2011); stance classification (Sridhar et al., 2014); identifying keywords, themes & topics (Karan et al., 2016; Ding et al., 2011); emotion analysis (Rheault, 2016); and sentiment analysis (Bakliwal et al., 2013). The source data includes manifestos, political speeches, news articles, floor debates and social media posts. With the increasing availability of large-scale datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). For example, rather than analyzing the political manifestos of a particular party during an election, mining different manifestos across countries over time can provide deeper comparative insights into political change. Existing classi"
U17-1003,W17-2906,0,0.447667,"Missing"
U17-1003,E17-2109,0,0.109668,"nguages. In this work, we focus on cross-lingual fine-grained thematic classification (57 categories in total), where we have labeled data for all the languages. For the document-level quantification task, much work has used label count aggregation of manually-annotated sentences as features (Lowe et al., 2011; Benoit and D¨aubler, 2014), while other work has used dictionary- based supervised methods, or unsupervised factor analysis based techniques (Hjorth et al., 2015; Bruinsma and Gemenis, 2017). The latter method uses discrete word representations and deals with mono-lingual text only. In Glavas et al. (2017), the authors lever3 Manifesto Text Analysis In the CMP, trained annotators manually label manifesto sentences according to the 57 finegrained political categories (shown in Table 5), which are grouped into seven policy areas: External Relations, Freedom and Democracy, Political System, Economy, Welfare and Quality of Life, Fabric of Society, and Social Groups. Political parties either write their promises as a bulleted list of individual sentences, or structured as paragraphs (an example is given in Figure 4), providing more information on topic coherence. Also the length of documents, measur"
U17-1003,W14-2715,0,0.178933,"or extreme left and right positions. They represent the documents using word embeddings averaged with TF-IDF scores as weights. All these approaches model the sentence and documentlevel tasks separately. Related Work The recent adoption of NLP methods has led to significant advances in the field of Computational Social Science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). Some popular tasks addressed with political text include: party position analysis (Biessmann, 2016); political leaning categorization (Akoglu, 2014; Zhou et al., 2011); stance classification (Sridhar et al., 2014); identifying keywords, themes & topics (Karan et al., 2016; Ding et al., 2011); emotion analysis (Rheault, 2016); and sentiment analysis (Bakliwal et al., 2013). The source data includes manifestos, political speeches, news articles, floor debates and social media posts. With the increasing availability of large-scale datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). For example, rather than analyzing the political manifestos of a particular party during an election, mining different man"
U17-1003,N16-1149,1,0.88579,"Missing"
U17-1003,W16-2102,0,0.177345,"Missing"
U17-1006,C16-1328,0,0.0671518,"Missing"
U18-1001,P17-1175,0,0.0499344,"Missing"
U18-1001,2012.eamt-1.60,0,0.0122068,"classification. Then we study different methods with minimal efforts for incorporating such side information into existing NMT models. 2 Machine Translation Data with Side Information First, let’s explore some realistic scenarios in which the side information is potentially useful for NMT. TED Talks The TED Talks website2 hosts technical videos from influential speakers around the world on various topics or domains, such as: education, business, science, technology, creativity, etc. Thanks to users’ contributions, most of such videos are subtitled in multiple languages. Based on this website, Cettolo et al. (2012) created a parallel corpus for the MT research community. Inspired by this, Chen et al. (2016) further customised this dataset and included an additional sentence-level topic information.3 We consider such topic information as side information. Fig2 https://www.ted.com/talks https://github.com/wenhuchen/ iwslt-2015-de-en-topics 3 Cong Duy Vu Hoang, Gholamreza Haffari and Trevor Cohn. 2018. Improved Neural Machine Translation using Side Information. In Proceedings of Australasian Language Technology Association Workshop, pages 6−16. ure 1 illustrates some examples of this dataset. As can be see"
U18-1001,2016.amta-researchers.10,0,0.0200853,"information into existing NMT models. 2 Machine Translation Data with Side Information First, let’s explore some realistic scenarios in which the side information is potentially useful for NMT. TED Talks The TED Talks website2 hosts technical videos from influential speakers around the world on various topics or domains, such as: education, business, science, technology, creativity, etc. Thanks to users’ contributions, most of such videos are subtitled in multiple languages. Based on this website, Cettolo et al. (2012) created a parallel corpus for the MT research community. Inspired by this, Chen et al. (2016) further customised this dataset and included an additional sentence-level topic information.3 We consider such topic information as side information. Fig2 https://www.ted.com/talks https://github.com/wenhuchen/ iwslt-2015-de-en-topics 3 Cong Duy Vu Hoang, Gholamreza Haffari and Trevor Cohn. 2018. Improved Neural Machine Translation using Side Information. In Proceedings of Australasian Language Technology Association Workshop, pages 6−16. ure 1 illustrates some examples of this dataset. As can be seen, the keywords (second column, treated as side information) contain additional contextual inf"
U18-1001,N16-1102,1,0.66264,"eural machine translation (NMT). We study various kinds of side information, including topical information and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et a"
U18-1001,P15-2139,1,0.840333,"ared information in the encoder states. Our formulation in Equation 5 gives rise to multi-task learning (MTL). Here, we propose the joint learning of two different but related tasks: NMT and multi-label classification (MLC). Here, the MLC task refers to predicting the labels that possibly represent words of the given side information. This is interesting in the sense that the model is capable of not only generating the translated outputs, but also explicitly predicting what the side information is. Here, we adopt a simple instance of MTL for our case, called soft parameter sharing similar to (Duong et al., 2015; Yang and Hospedales, 2016). In our MTL version, the NMT and MLC tasks share the parameters of the encoders. The difference between the two is at the decoder part. In the NMT task, the decoder is kept unchanged. For the MLC task, we define its objective function (or loss), formulated as: where: λ is the coefficient balancing the two task objectives, whose value is fine-tuned based on the development data to optimise for NMT accuracy measured using BLEU (Papineni et al., 2002). The idea of MTL applied for NLP was firstly explored by (Collobert and Weston, 2008), later attracts increasing atten"
U18-1001,K18-1010,0,0.150475,"ts of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation, seeks to leverage images which can contain cues representing the perception of the image in source text, and potentially can contribute to resolve ambiguity (e.g., lex"
U18-1001,D17-1105,0,0.0227807,"2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation, seeks to leverage images which can contain cues representing the perception of the image in source text, and potentially can contribute to resolve ambiguity (e.g., lexical, gender), 1 http://www.statmt.org/wmt17/ multimodal-task.html vagueness, out-of-vocabulary terms, and topic relevancy. Inspired from the idea of multi-modal translation, in our work, we propose the use of another modality, namely metadata or side information. Previously, Hoang et al. (2016a) have shown the usefulness of side information for neural language models. This work"
U18-1001,E17-2101,0,0.101522,"IPC labels. The full meaning of all IPC labels can be found on the official IPC website,6 however we provide in Figure 3 the glossess for each referenced label. Note that those IPC labels form a WordNet style hierarchy (Fellbaum, 1998), and accordingly may be useful in many other deep models of NLP. Personalised Europarl For the second dataset, we evaluate our proposed idea in the context of personality-aware MT. Mirkin et al. (2015) explored whether translation preserves personality information (e.g., demographic and psychometric traits) in statistical MT (SMT); and further Rabinovich et al. (2017) found that personality information like author’s gender is an obvious signal in source text, but it is less clear in human and machine translated texts. As a result, they created a new dataset for personalised MT4 partially based on the original Europarl. The personality such as author’s gender will be regarded as side information in our setup. An excerpt of this dataset is shown in Figure 2. As can be seen from the figure, there exist many kinds of side information pertaining to authors’ traits, including identification (ID, name), native language, gender, date of birth/age, and plenary sess"
U18-1001,P17-1012,0,0.0613326,"Missing"
U18-1001,D16-1096,0,0.0228671,"dy various kinds of side information, including topical information and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calix"
U18-1001,P18-2050,0,0.0366293,"Missing"
U18-1001,N16-1149,1,0.601696,"on and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation,"
U18-1001,D15-1130,0,0.0221855,"e pair in this corpus is associated with any number of IPC label(s) as well as other metadata, e.g., patent ID, patent family ID, publication date. In this work, we consider only the IPC labels. The full meaning of all IPC labels can be found on the official IPC website,6 however we provide in Figure 3 the glossess for each referenced label. Note that those IPC labels form a WordNet style hierarchy (Fellbaum, 1998), and accordingly may be useful in many other deep models of NLP. Personalised Europarl For the second dataset, we evaluate our proposed idea in the context of personality-aware MT. Mirkin et al. (2015) explored whether translation preserves personality information (e.g., demographic and psychometric traits) in statistical MT (SMT); and further Rabinovich et al. (2017) found that personality information like author’s gender is an obvious signal in source text, but it is less clear in human and machine translated texts. As a result, they created a new dataset for personalised MT4 partially based on the original Europarl. The personality such as author’s gender will be regarded as side information in our setup. An excerpt of this dataset is shown in Figure 2. As can be seen from the figure, th"
U18-1001,U16-1001,1,0.650663,"on and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation,"
U18-1001,P02-1040,0,0.101887,"t the side information is. Here, we adopt a simple instance of MTL for our case, called soft parameter sharing similar to (Duong et al., 2015; Yang and Hospedales, 2016). In our MTL version, the NMT and MLC tasks share the parameters of the encoders. The difference between the two is at the decoder part. In the NMT task, the decoder is kept unchanged. For the MLC task, we define its objective function (or loss), formulated as: where: λ is the coefficient balancing the two task objectives, whose value is fine-tuned based on the development data to optimise for NMT accuracy measured using BLEU (Papineni et al., 2002). The idea of MTL applied for NLP was firstly explored by (Collobert and Weston, 2008), later attracts increasing attentions from the NLP community (Ruder, 2017). Specifically, the idea behind MTL is to leverage related tasks which can be learned jointly — potentially introducing an inductive bias (Feinman and Lake, 2018). An alternative explanation of the benefits of MTL is that joint training with multiple tasks acts as an additional regulariser to the model, reducing the risk of overfitting (Collobert and Weston, 2008; Ruder, 2017, inter alia). 4 4.1 LM LC := − M X 1Twm s log ps ; (8) Exper"
U18-1001,W18-1802,0,0.0860953,"nt set, using the value range of {64, 128, 256, 512}. Similarly, the balancing weight in the mtl method is fine-tuned using the value range of {0.001, 0.01, 0.1, 1.0}. For evaluation, we measured the end translation quality with case-sensitive BLEU (Papineni et al., 2002). We averaged 2 runs for each of the method variants. denote the system variants as follows: base refers to the baseline NMT system using the transformer without using any side information. si-src-prefix and si-src-suffix refer to the NMT system using the side information as respective prefix or suffix of the source sequence (Jehl and Riezler, 2018), applied to both training and decoder/inference. 4.3 Results and Analysis The experimental results can be seen in Table 3. Overall, we obtained limited success for the method of adding side information as prefix or suffix for TED Talks and Personalised Europarl datasets. On the PatTR dataset, small improvements (0.1-0.2 BLEU) are observed. We experimented two sets of side information in the PatTR dataset, including PatTR-1 (651 deep labels) and PatTR (8 shallow labels).13 The possible reason for this phenomenon is that the multi-head attention mechanism in the transformer may have some confus"
U18-1001,E17-1101,0,0.241385,"consider only the IPC labels. The full meaning of all IPC labels can be found on the official IPC website,6 however we provide in Figure 3 the glossess for each referenced label. Note that those IPC labels form a WordNet style hierarchy (Fellbaum, 1998), and accordingly may be useful in many other deep models of NLP. Personalised Europarl For the second dataset, we evaluate our proposed idea in the context of personality-aware MT. Mirkin et al. (2015) explored whether translation preserves personality information (e.g., demographic and psychometric traits) in statistical MT (SMT); and further Rabinovich et al. (2017) found that personality information like author’s gender is an obvious signal in source text, but it is less clear in human and machine translated texts. As a result, they created a new dataset for personalised MT4 partially based on the original Europarl. The personality such as author’s gender will be regarded as side information in our setup. An excerpt of this dataset is shown in Figure 2. As can be seen from the figure, there exist many kinds of side information pertaining to authors’ traits, including identification (ID, name), native language, gender, date of birth/age, and plenary sess"
U18-1001,P17-1139,0,0.0188248,"lation (NMT). We study various kinds of side information, including topical information and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et"
U18-1001,N16-1005,0,0.133644,"itional side information we would like to incorporate into NMT model. 3.3 Conditioning on Side Information Keeping in mind that we would like a generic incorporation method so that only minimal modification of NMT model is required, we propose and evaluate different approaches. Side Information as Source Prefix/Suffix The most simple way to include side information is to add the side information as a string prefix or suffix to the source sequence, and letting the NMT model learn from this modified data. This method requires no modification of the NMT model. This method was firstly proposed by Sennrich et al. (2016a) who added the side constraints (e.g., honMulti-task Learning Consider the case where we would like to use existing side information to 9 mation, formulated as: &quot; improve the main NMT task. We can define a generative model p (y, e|x), formulated as: p (y, e|x) := p (y|x, e) · |{z } p (e|x) |{z } ! # 1 X 0 ps = sigmoid W s g (xi ) + bs ; |x| i (7) where x is the source sequence, comprising of x1 , . . . , xi , . . . , x|x |words. Here, we denote a generic function term g0 (.) which refers to a vectorised representation of a specific word depending on designing the network architecture, e.g.,"
U18-1001,N16-1004,0,0.0333152,"Missing"
U18-1001,P16-1162,0,0.57918,"itional side information we would like to incorporate into NMT model. 3.3 Conditioning on Side Information Keeping in mind that we would like a generic incorporation method so that only minimal modification of NMT model is required, we propose and evaluate different approaches. Side Information as Source Prefix/Suffix The most simple way to include side information is to add the side information as a string prefix or suffix to the source sequence, and letting the NMT model learn from this modified data. This method requires no modification of the NMT model. This method was firstly proposed by Sennrich et al. (2016a) who added the side constraints (e.g., honMulti-task Learning Consider the case where we would like to use existing side information to 9 mation, formulated as: &quot; improve the main NMT task. We can define a generative model p (y, e|x), formulated as: p (y, e|x) := p (y|x, e) · |{z } p (e|x) |{z } ! # 1 X 0 ps = sigmoid W s g (xi ) + bs ; |x| i (7) where x is the source sequence, comprising of x1 , . . . , xi , . . . , x|x |words. Here, we denote a generic function term g0 (.) which refers to a vectorised representation of a specific word depending on designing the network architecture, e.g.,"
U18-1001,W13-2236,0,0.114478,"and Hs the dimensionality of the hidden space. These embedding vectors are used for the input to several different neural architectures, which we now outline. Patent MT Collection Another interesting data is patent translation which includes rich side information. PatTR5 is a sentence-parallel corpus which is a subset of the MAREC Patent Corpus (W¨aschle and Riezler, 2012a). In general, PatTR contains millions of parallel sentences collected from all patent text sections (e.g., title, abstract, claims, description) in multiple languages (English, French, German) (W¨aschle and Riezler, 2012b; Simianer and Riezler, 2013). An appealing feature of this corpus is that it provides a labelling at a sentence level, in the form of IPC (International Patent Classification) codes. The IPC 3.2 NMT Model Formulation Recall the general formulation of NMT (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia) as a conditional language model in which the generation of target sequence is conditioned on the source sequence (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia), formulated as: y t+1 ∼ pΘ (yt+1 |y <t , x) = softmax (f Θ (y <t , x)) ; 4 http://cl.haifa.ac.il/projects/pmt/ index.shtml 5 http://www.cl.u"
U18-1001,E12-1083,0,0.061259,"Missing"
U18-1010,W16-2302,0,0.0389002,"Missing"
U18-1010,N15-1124,1,0.828874,"is the mean standardised score of its translations after discarding scores that do not meet quality control criteria. The noise in worker scores is cancelled out when a large number of translations are averaged. To obtain accurate scores of individual translations, multiple judgments are collected and averaged. As we increase the number of annotators per translation, there is greater consistency and reliability in the mean score. This was empirically tested by showing that there is high correlation between the mean of two independent sets of judgments, when the sample size is greater than 15 (Graham et al., 2015). However, both these tests are based on a sample-size of 10 items, and, as such, the first test has low power; we show that it filters out a large proportion of the total workers. One solution would be to increase the sample size of the degraded-reference-pairs, but this would be at the expense of the number of useful worker annotations. It is better to come up with a model that would use the scores of all workers, and is more robust to low quality scores. Automatic metrics such as BLEU (Papineni et al., 2002) are generally evaluated using the Pear2016a), and has replaced RR since 2017 (Bojar"
U18-1010,P13-1139,0,0.0302029,"ilter out unreliable workers. Background The Conference on Machine Translation (WMT) annually collects human judgements to evaluate the MT systems and metrics submitted to the shared tasks. The evaluation methodology has evolved over the years, from 5 point adequacy and fluency rating, to relative rankings (“RR”), to DA. With RR, annotators are asked to rank translations of 5 different MT systems. In earlier years, the final score of a system was the expected number of times its translations score better than translations by other systems (expected wins). Bayesian models like Hopkins and May (Hopkins and May, 2013) and Trueskill (Sakaguchi et al., 2014) were then proposed to learn the relative ability of the MT systems. Trueskill was adopted by WMT in 2015 as it is more stable and efficient than the expected wins heuristic. DA was trialled at WMT 2016 (Bojar et al., Nitika Mathur, Timothy Baldwin and Trevor Cohn. 2018. Towards Efficient Machine Translation Evaluation by Modelling Annotators. In Proceedings of Australasian Language Technology Association Workshop, pages 77−82. The paired Wilcoxon rank-sum test is used to test whether the worker scored degraded translations worse than the corresponding sy"
U18-1010,P02-1040,0,0.117992,"e mean of two independent sets of judgments, when the sample size is greater than 15 (Graham et al., 2015). However, both these tests are based on a sample-size of 10 items, and, as such, the first test has low power; we show that it filters out a large proportion of the total workers. One solution would be to increase the sample size of the degraded-reference-pairs, but this would be at the expense of the number of useful worker annotations. It is better to come up with a model that would use the scores of all workers, and is more robust to low quality scores. Automatic metrics such as BLEU (Papineni et al., 2002) are generally evaluated using the Pear2016a), and has replaced RR since 2017 (Bojar et al., 2017a). It is more scalable than RR as the number of systems increases (we need to obtain one annotation per system, instead of one annotation per system pair). Each translation is rated independently, minimising the risk of being influenced by the relative quality of other translations. Ideally, it is possible that evaluations can be compared across multiple datasets. For example, we can track the progress of MT systems for a given language pair over the years. Another probabilistic model, EASL (Sakag"
U18-1010,Q14-1025,0,0.0196834,"with Trueskill. Annotators score translations from 5 systems at the same time on a sliding scale, allowing users to explicitly specify the magnitude of difference between system translations. Active learning to select the systems in each comparison to increase efficiency. But it does not model worker reliability, and is, very likely, not compatible with longitudinal evaluation, as the systems are effectively scored relative to each other. In NLP, most other research on learning annotator bias and reliability has been on categorical data (Snow et al., 2008; Carpenter, 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014). 3 Direct Assessment To measure adequacy, in DA, annotators are asked to rate how adequately an MT output expresses the meaning of a reference translation using a continuous slider, which maps to an underlying scale of 0–100. These annotations are crowdsourced using Amazon Mechanical Turk, where “workers” complete “Human Intelligence Tasks” (HITs) in the form of one or more micro-tasks. Each HIT consists of 70 MT system translations, along with an additional 30 control items: 1. degraded versions of 10 of these translations; 2. 10 reference translations by a human expert, corresponding to 10"
U18-1010,W14-3301,0,0.0382128,"Missing"
U18-1010,P18-1020,0,0.0518819,"Missing"
U18-1010,D08-1027,0,0.246619,"Missing"
U19-1002,W14-3346,0,0.0275103,"Missing"
U19-1002,P84-1044,0,0.320586,"Missing"
U19-1002,C08-1048,0,0.0516628,"es users to provide keywords or titles and a rhyming template so the system can expand words and fill them into the user-selected template. This method is used in several successful poetry generation services, for instance, the haiku generation system of Wu et al. (2009) extracts rules from corpus and expand keywords from users to generate haiku sentences based on rules. Another influential approach for Chinese quatrain generation is statistical machine translation (SMT). It is first utilised to generate Chinese couplets, which can be regarded as a special quatrain comprised of two sentences (Jiang and Zhou, 2008). They take the first sentence as input from users and generate an N-best list of second sentence candidates via a phrase-based SMT decoder. This method is extended by He et al. (2012) to generate Chinese quatrains where users are asked to provide the first sentence as input and then the system “translates” three following sentences based on the previous sentences. Recently, neural networks are the predominant technique in the literature. Zhang and Lapata (2014) propose using recurrent neural networks (RNN) to generate the poem sentence by sentence. However, the model they provide is rather co"
U19-1002,P18-1181,1,0.896971,"ZPP 飞流直下三千尺， *P*ZPPZ 疑是银河落九天。 *ZPPZZP Waterfall on Mount Lu Li Bai The Sunlit Censer peak exhales a wreath of cloud. Like an unpended stream the waterfall sounds loud. Its torrent dashes down three thousand feet from high. As if the Silver River fell from azure sky. Table 1: A 7-character Tang Quatrain by Li Bai. tone is indicated in Table 1 where ‘P’ indicates “Ping”, ‘Z’ refers to “Ze” and ‘*’ means this character can be either tone. Following all these constraints, a well-written Chinese quatrain are full of rhythm while expressing abundant emotions. 1.1 Motivation Inspired by Deep-speare (Lau et al., 2018), a joint neural network model that is trained on English sonnets to generate quatrains in iambic pentameter (Halle and Keyser, 1971), we seek to adapt this model to poetry in other languages. The abundance of Chinese quatrain and its important cultural status makes it a natural choice. We analyse the similarity between characteristics of English sonnets and Chinese quatrains and found it is possible to adapt the model to Chinese as the language model can be modified to handle Chinese characters. Lau et al. (2018) found that a vanilla language model can learn meter automatically at human level"
U19-1002,D14-1074,0,0.0254334,"ation (SMT). It is first utilised to generate Chinese couplets, which can be regarded as a special quatrain comprised of two sentences (Jiang and Zhou, 2008). They take the first sentence as input from users and generate an N-best list of second sentence candidates via a phrase-based SMT decoder. This method is extended by He et al. (2012) to generate Chinese quatrains where users are asked to provide the first sentence as input and then the system “translates” three following sentences based on the previous sentences. Recently, neural networks are the predominant technique in the literature. Zhang and Lapata (2014) propose using recurrent neural networks (RNN) to generate the poem sentence by sentence. However, the model they provide is rather complex as it has one CNN and two RNNs, and it still observes theme drift when generating long sequences. To solve these problems, Wang et al. (2016) propose a simpler neural model which treats the whole poem as a character sequence. This approach can be easily extended to generate other genres such as Song Iambics or Haiku and they utilise the attention mechanism (Bahdanau et al., 2014) to avoid theme drift. The closest work to our study is Deep-speare, which is"
U19-1002,P17-1101,0,0.0559112,"Missing"
W05-0622,W05-0620,0,0.0617211,"Missing"
W05-0622,P05-1002,1,0.825914,"ole with respect to the predicate, indicating how the proposition should be semantically interpreted. We apply conditional random fields (CRFs) to the task of SRL proposed by the CoNLL shared task 2005 (Carreras and M`arquez, 2005). CRFs are undirected graphical models which define a conditional distribution over labellings given an observation (Lafferty et al., 2001). These models allow for the use of very large sets of arbitrary, overlapping and non-independent features. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks. While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al., 2004). Most CRF implementations have been specialised to work with chain structures, where the labels and observations form a linear sequence. Framing SRL as a linear tagging task is awkward, as there is no easy model of adjacency between the candidate"
W05-0622,W04-2419,0,0.0241527,"ty et al., 2001). These models allow for the use of very large sets of arbitrary, overlapping and non-independent features. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks. While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al., 2004). Most CRF implementations have been specialised to work with chain structures, where the labels and observations form a linear sequence. Framing SRL as a linear tagging task is awkward, as there is no easy model of adjacency between the candidate constituent phrases. Our approach simultaneously performs both constituent selection and labelling, by defining an undirected random field over the parse tree. This allows the modelling of interactions between parent and child constituents, and the prediction of an optimal argument labelling for all constituents in one pass. The parse tree forms an a"
W05-0622,W03-0430,0,0.0209342,"e labelled with their role with respect to the predicate, indicating how the proposition should be semantically interpreted. We apply conditional random fields (CRFs) to the task of SRL proposed by the CoNLL shared task 2005 (Carreras and M`arquez, 2005). CRFs are undirected graphical models which define a conditional distribution over labellings given an observation (Lafferty et al., 2001). These models allow for the use of very large sets of arbitrary, overlapping and non-independent features. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks. While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al., 2004). Most CRF implementations have been specialised to work with chain structures, where the labels and observations form a linear sequence. Framing SRL as a linear tagging task is awkward, as there is no easy model of adjacency b"
W05-0622,N03-1028,0,0.0131144,"eted. We apply conditional random fields (CRFs) to the task of SRL proposed by the CoNLL shared task 2005 (Carreras and M`arquez, 2005). CRFs are undirected graphical models which define a conditional distribution over labellings given an observation (Lafferty et al., 2001). These models allow for the use of very large sets of arbitrary, overlapping and non-independent features. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks. While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al., 2004). Most CRF implementations have been specialised to work with chain structures, where the labels and observations form a linear sequence. Framing SRL as a linear tagging task is awkward, as there is no easy model of adjacency between the candidate constituent phrases. Our approach simultaneously performs both constituent selection and labelling,"
W05-0622,W04-3212,0,0.511454,"an observation (Lafferty et al., 2001). These models allow for the use of very large sets of arbitrary, overlapping and non-independent features. CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks. While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al., 2004). Most CRF implementations have been specialised to work with chain structures, where the labels and observations form a linear sequence. Framing SRL as a linear tagging task is awkward, as there is no easy model of adjacency between the candidate constituent phrases. Our approach simultaneously performs both constituent selection and labelling, by defining an undirected random field over the parse tree. This allows the modelling of interactions between parent and child constituents, and the prediction of an optimal argument labelling for all constituents in one pass. The pa"
W11-2113,W05-0909,0,0.698253,"when ROSE is trained on human judgements of translations into a different language compared with that use in testing. 1 Introduction Human judgements of translation quality are very expensive. For this reason automatic MT evaluation metrics are used to as an approximation by comparing predicted translations to human authored references. An early MT evaluation metric, BLEU (Papineni et al., 2002), is still the most commonly used metric in automatic machine translation evaluation. However, several drawbacks have been stated by many researchers (Chiang et al., 2008a; CallisonBurch et al., 2006; Banerjee and Lavie, 2005), most notably that it omits recall (substituting this with a penalty for overly short output) and not being easily applied at the sentence level. Later heuristic metrics such as METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements. This allows the combination of different features and can better fit specific tasks, such as evaluation focusing more on fluency/adequacy/relati"
W11-2113,P04-3031,0,0.0308076,"tion 4.1 The testing data on sentence level used in this paper is human ranked sentences from WMT09 (Callison-Burch et al., 2009). Tied rankings were removed, leaving 1702 pairs. We only consider translations into English sentences. On system level, the testing data are the submissions for ’test2008’ test set in WMT08 (Callison-Burch et al., 2008). ROSE, and BLEU were compared with human ranked submitted system in ‘RANK’, ‘CONSTITUENT’ and ‘YES/NO’ tasks. English punctuation and 100 common function words list of four languages in this experiment were generated. English POS was tagged by NLTK (Bird and Loper, 2004). Table 3: ROSE-reg in with SVM kernel functions Metric BLEU-smoothed ROSE-reg ROSE-regpos ROSE-rank ROSE-rankpos are manually ranked from 5 (best) to 1 (worst). In order to test the ROSE’s ability to adapt the language without training data, ROSE was only trained with English data. Data Training data used for ROSE is from WMT10 (Callison-Burch et al., 2010) human judged sentences. A regression model was trained by sentences with human annotation for post editing effort. The three levels used in WMT10 are ‘OK’, ‘EDIT’ and ‘BAD’, which we treat as response values of 3, 2 and 1. In total 2885 se"
W11-2113,E06-1032,0,0.112339,"Missing"
W11-2113,W07-0718,0,0.218899,"George, 2002) extended BLEU by assigning more informative ngrams higher weight. However, BLEU and NIST have several drawbacks, the first being that BLEU uses a geometric mean over all n-grams which makes BLEU almost unusable for sentence level evaluations 1 . Secondly, 1 Note that various approximations exits (Lin and Och, 2004; 124 BLEU and NIST both use the brevity penalty to replace recall, but Banerjee and Lavie (2005) in experiments show that the brevity penalty is a poor substitute for recall. Banerjee and Lavie (2005) proposed a METEOR metric, which that uses recall instead of the BP. Callison-Burch et al. (2007; Callison-Burch et al. (2008) show that METEOR does not perform well in out of English task. This may because that Stemmer or WordNet may not available in some languages, which unable to model synonyms in these cases. In addition, the performance also varies when adjusting weights in precision and recall. Supervised learning approaches have been proposed by many researchers (Corston-Oliver et al., 2001; Duh, 2008; Albercht and Hwa, 2008; Specia and Gimenez, 2010). Corston-Oliver et al. (2001) use a classification method to measure machine translation system quality at the sentence level as be"
W11-2113,W08-0309,0,0.640989,"by assigning more informative ngrams higher weight. However, BLEU and NIST have several drawbacks, the first being that BLEU uses a geometric mean over all n-grams which makes BLEU almost unusable for sentence level evaluations 1 . Secondly, 1 Note that various approximations exits (Lin and Och, 2004; 124 BLEU and NIST both use the brevity penalty to replace recall, but Banerjee and Lavie (2005) in experiments show that the brevity penalty is a poor substitute for recall. Banerjee and Lavie (2005) proposed a METEOR metric, which that uses recall instead of the BP. Callison-Burch et al. (2007; Callison-Burch et al. (2008) show that METEOR does not perform well in out of English task. This may because that Stemmer or WordNet may not available in some languages, which unable to model synonyms in these cases. In addition, the performance also varies when adjusting weights in precision and recall. Supervised learning approaches have been proposed by many researchers (Corston-Oliver et al., 2001; Duh, 2008; Albercht and Hwa, 2008; Specia and Gimenez, 2010). Corston-Oliver et al. (2001) use a classification method to measure machine translation system quality at the sentence level as being human-like translation (go"
W11-2113,W10-1703,0,0.0415855,"SE, and BLEU were compared with human ranked submitted system in ‘RANK’, ‘CONSTITUENT’ and ‘YES/NO’ tasks. English punctuation and 100 common function words list of four languages in this experiment were generated. English POS was tagged by NLTK (Bird and Loper, 2004). Table 3: ROSE-reg in with SVM kernel functions Metric BLEU-smoothed ROSE-reg ROSE-regpos ROSE-rank ROSE-rankpos are manually ranked from 5 (best) to 1 (worst). In order to test the ROSE’s ability to adapt the language without training data, ROSE was only trained with English data. Data Training data used for ROSE is from WMT10 (Callison-Burch et al., 2010) human judged sentences. A regression model was trained by sentences with human annotation for post editing effort. The three levels used in WMT10 are ‘OK’, ‘EDIT’ and ‘BAD’, which we treat as response values of 3, 2 and 1. In total 2885 sentences were used in the regression training. The ranking model was trained by sentences with human annotating sentence ranking, and tied results are allowed in training. In this experiment, 1675 groups of sentences were used for training, and each group contains five sentences, which 127 Table 5 and Table 6 are the Spearman’s rho in system ranking. Table 5"
W11-2113,D08-1064,0,0.0706102,"ts competitive to BLEU. Moreover, this still holds when ROSE is trained on human judgements of translations into a different language compared with that use in testing. 1 Introduction Human judgements of translation quality are very expensive. For this reason automatic MT evaluation metrics are used to as an approximation by comparing predicted translations to human authored references. An early MT evaluation metric, BLEU (Papineni et al., 2002), is still the most commonly used metric in automatic machine translation evaluation. However, several drawbacks have been stated by many researchers (Chiang et al., 2008a; CallisonBurch et al., 2006; Banerjee and Lavie, 2005), most notably that it omits recall (substituting this with a penalty for overly short output) and not being easily applied at the sentence level. Later heuristic metrics such as METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements. This allows the combination of different features and can better fit specific tasks, suc"
W11-2113,D08-1024,0,0.187347,"ts competitive to BLEU. Moreover, this still holds when ROSE is trained on human judgements of translations into a different language compared with that use in testing. 1 Introduction Human judgements of translation quality are very expensive. For this reason automatic MT evaluation metrics are used to as an approximation by comparing predicted translations to human authored references. An early MT evaluation metric, BLEU (Papineni et al., 2002), is still the most commonly used metric in automatic machine translation evaluation. However, several drawbacks have been stated by many researchers (Chiang et al., 2008a; CallisonBurch et al., 2006; Banerjee and Lavie, 2005), most notably that it omits recall (substituting this with a penalty for overly short output) and not being easily applied at the sentence level. Later heuristic metrics such as METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements. This allows the combination of different features and can better fit specific tasks, suc"
W11-2113,P01-1020,0,0.250473,"tput) and not being easily applied at the sentence level. Later heuristic metrics such as METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements. This allows the combination of different features and can better fit specific tasks, such as evaluation focusing more on fluency/adequacy/relative ranks or post editing effort. Previous work includes approaches using classification (Corston-Oliver et al., 2001), regression (Albercht and Hwa, 2008; Specia and Gimenez, 2010), and ranking (Duh, 2008). Most of which achieved good results and better correlations with human judgments than heuristic baseline methods. Overall automatic metrics must find a balance between several key issues: a) applicability to different sized texts (documents vs sentences), b) easy of portability to different languages, c) runtime requirements and d) correlation with human judgement data. Previous work has typically ignored at least one of these issues, e.g., BLEU which applies only to documents (A), trained metrics (Alberc"
W11-2113,W08-0331,0,0.415725,"and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements. This allows the combination of different features and can better fit specific tasks, such as evaluation focusing more on fluency/adequacy/relative ranks or post editing effort. Previous work includes approaches using classification (Corston-Oliver et al., 2001), regression (Albercht and Hwa, 2008; Specia and Gimenez, 2010), and ranking (Duh, 2008). Most of which achieved good results and better correlations with human judgments than heuristic baseline methods. Overall automatic metrics must find a balance between several key issues: a) applicability to different sized texts (documents vs sentences), b) easy of portability to different languages, c) runtime requirements and d) correlation with human judgement data. Previous work has typically ignored at least one of these issues, e.g., BLEU which applies only to documents (A), trained metrics (Albercht and Hwa, 2008; Specia and Gimenez, 2010) which tend to ignore B and C. This paper pre"
W11-2113,C04-1072,0,0.0291854,"clips the counts of each candidate N-gram to the maximum counts of that n-gram that in references, and with a brevity penalty to down-scale the score for output shorter than the reference. In BLEU, each ngram precision is given equal weight in geometric mean, while NIST (Doddington and George, 2002) extended BLEU by assigning more informative ngrams higher weight. However, BLEU and NIST have several drawbacks, the first being that BLEU uses a geometric mean over all n-grams which makes BLEU almost unusable for sentence level evaluations 1 . Secondly, 1 Note that various approximations exits (Lin and Och, 2004; 124 BLEU and NIST both use the brevity penalty to replace recall, but Banerjee and Lavie (2005) in experiments show that the brevity penalty is a poor substitute for recall. Banerjee and Lavie (2005) proposed a METEOR metric, which that uses recall instead of the BP. Callison-Burch et al. (2007; Callison-Burch et al. (2008) show that METEOR does not perform well in out of English task. This may because that Stemmer or WordNet may not available in some languages, which unable to model synonyms in these cases. In addition, the performance also varies when adjusting weights in precision and rec"
W11-2113,P02-1040,0,0.0859832,"based, allowing it to be used in a wider range of settings. Results show that ROSE performs well on many tasks, such as ranking system and syntactic constituents, with results competitive to BLEU. Moreover, this still holds when ROSE is trained on human judgements of translations into a different language compared with that use in testing. 1 Introduction Human judgements of translation quality are very expensive. For this reason automatic MT evaluation metrics are used to as an approximation by comparing predicted translations to human authored references. An early MT evaluation metric, BLEU (Papineni et al., 2002), is still the most commonly used metric in automatic machine translation evaluation. However, several drawbacks have been stated by many researchers (Chiang et al., 2008a; CallisonBurch et al., 2006; Banerjee and Lavie, 2005), most notably that it omits recall (substituting this with a penalty for overly short output) and not being easily applied at the sentence level. Later heuristic metrics such as METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic met"
W11-2113,quirk-2004-training,0,0.0344775,"ch unable to model synonyms in these cases. In addition, the performance also varies when adjusting weights in precision and recall. Supervised learning approaches have been proposed by many researchers (Corston-Oliver et al., 2001; Duh, 2008; Albercht and Hwa, 2008; Specia and Gimenez, 2010). Corston-Oliver et al. (2001) use a classification method to measure machine translation system quality at the sentence level as being human-like translation (good) or machine translated (bad). Features extracted from references and machine translation include heavy linguistic features (requires parser). Quirk (2004) proposed a linear regression model which is trained to match translation quality. Albercht and Hwa (2008) introduced pseudo-references when data driven regression does not have enough training data. Most recently, Specia and Gimenez (2010) combined confidence estimation (without reference, just using the source) and reference-based metrics together in a regression framework to measure sentence-level machine translation quality. Duh (2008) compared the ranking with the regression, with the results that with same feature set, ranking and regression have similar performance, while ranking can to"
W11-2113,2006.amta-papers.25,0,0.0622406,"sed to as an approximation by comparing predicted translations to human authored references. An early MT evaluation metric, BLEU (Papineni et al., 2002), is still the most commonly used metric in automatic machine translation evaluation. However, several drawbacks have been stated by many researchers (Chiang et al., 2008a; CallisonBurch et al., 2006; Banerjee and Lavie, 2005), most notably that it omits recall (substituting this with a penalty for overly short output) and not being easily applied at the sentence level. Later heuristic metrics such as METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements. This allows the combination of different features and can better fit specific tasks, such as evaluation focusing more on fluency/adequacy/relative ranks or post editing effort. Previous work includes approaches using classification (Corston-Oliver et al., 2001), regression (Albercht and Hwa, 2008; Specia and Gimenez, 2010), and ranking (Duh, 2008). Most of which achieved good results"
W11-2113,2010.amta-papers.3,0,0.30262,"uristic metrics such as METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) account for both precision and recall, but their relative weights are difficult to determine manually. In contrast to heuristic metrics, trained metrics use supervised learning to model directly human judgements. This allows the combination of different features and can better fit specific tasks, such as evaluation focusing more on fluency/adequacy/relative ranks or post editing effort. Previous work includes approaches using classification (Corston-Oliver et al., 2001), regression (Albercht and Hwa, 2008; Specia and Gimenez, 2010), and ranking (Duh, 2008). Most of which achieved good results and better correlations with human judgments than heuristic baseline methods. Overall automatic metrics must find a balance between several key issues: a) applicability to different sized texts (documents vs sentences), b) easy of portability to different languages, c) runtime requirements and d) correlation with human judgement data. Previous work has typically ignored at least one of these issues, e.g., BLEU which applies only to documents (A), trained metrics (Albercht and Hwa, 2008; Specia and Gimenez, 2010) which tend to ignor"
W11-2113,W09-0401,0,\N,Missing
W11-2113,P06-4018,0,\N,Missing
W12-1906,P05-1048,0,0.0309982,"t hasn’t been shown to consistently help in related tasks. Machine Translation is no exception, and whether or not WSD systems can improve performance of MT systems is debated. Furthermore, it is unclear how parallel corpuses can be exploited for WSD systems. In this section we will present a brief overview of related work. (Carpuat and Wu, 2007) report an improvement in translation quality by incorporating a WSD system directly in a phrase-based translation system. This is in response to earlier work done, where incorporating the output of a traditional WSD system gave disappointing results (Carpuat and Wu, 2005). The WSD task is redefined, to be similar to choosing the correct phrasal translation for a word, instead of choosing a sense from a sense inventory. This system is trained on the same data as the SMT system is. The output of this model is incorporated into the machine translation system by providing the WSD probabilities for a phrase translation as extra features in a log-linear model (Carpuat and Wu, 2007). This system consistently outperforms the baseline system (the same system, but without WSD component), on multiple metrics, which seems to indicate that WSD can make a useful contributio"
W12-1906,D07-1007,0,0.0283102,"results of the base HMM model. In the second part, the model is qualitatively evaluated by inspecting the senses and associated output distributions of selected words. 2 Previous Work Although most researchers agree that Word Sense Disambiguation (WSD) is a useful field, it hasn’t been shown to consistently help in related tasks. Machine Translation is no exception, and whether or not WSD systems can improve performance of MT systems is debated. Furthermore, it is unclear how parallel corpuses can be exploited for WSD systems. In this section we will present a brief overview of related work. (Carpuat and Wu, 2007) report an improvement in translation quality by incorporating a WSD system directly in a phrase-based translation system. This is in response to earlier work done, where incorporating the output of a traditional WSD system gave disappointing results (Carpuat and Wu, 2005). The WSD task is redefined, to be similar to choosing the correct phrasal translation for a word, instead of choosing a sense from a sense inventory. This system is trained on the same data as the SMT system is. The output of this model is incorporated into the machine translation system by providing the WSD probabilities fo"
W12-1906,P07-1005,0,0.0263771,"T system is. The output of this model is incorporated into the machine translation system by providing the WSD probabilities for a phrase translation as extra features in a log-linear model (Carpuat and Wu, 2007). This system consistently outperforms the baseline system (the same system, but without WSD component), on multiple metrics, which seems to indicate that WSD can make a useful contribution to machine translation. However, the way the system is set up, it could also be viewed as a way of incorporating translation probabilities of other systems into the phrase-based translation model. (Chan and Ng, 2007) introduce a system very similar to that of (Carpuat and Wu, 2007), but as applied to hierarchical phrase-based translation. They demonstrate modest improvements in BLEU score over the unmodified system, as well as some qualita40 tive improvements in the output. Here again, the argument could be made that what is being done is not strictly word sense disambiguation, but augmenting the translation system with extra features for some of the phrase translations. In (Tufis¸ et al., 2004) parallel corpora and aligned WordNets are exploited for WSD. This is done, by word aligning the parallel texts,"
W12-1906,P07-2045,0,0.0039823,"by extending the IBM HMM model with an extra hidden layer which represents the senses a word can take, allowing similar words to share similar output distributions. We test a preliminary version of this model on English-French data. We compare different ways of generating senses and assess the quality of the alignments relative to the IBM HMM model, as well as the generated sense probabilities, in order to gauge the usefulness in Word Sense Disambiguation. 1 Introduction Modern machine translation is dominated by statistical methods, most of which are trained on wordaligned parallel corpora (Koehn et al., 2007; Koehn, 2004), which need to be generated separately. One of the most commonly used methods to generate these word alignments is to use the IBM models 1-5, which generate one-directional alignments. Although the IBM models perform well, they fail to take into account certain situations. For example, if an alignment between two words f1 and e1 is considered, and f1 is an uncommon translation for e1 , the translation probability will be low. It might happen, that an alignment to a different nearby word 39 is preferred by the model. Consider for example the situation where f1 is ‘taal’ (Dutch, m"
W12-1906,koen-2004-pharaoh,0,0.0495766,"M HMM model with an extra hidden layer which represents the senses a word can take, allowing similar words to share similar output distributions. We test a preliminary version of this model on English-French data. We compare different ways of generating senses and assess the quality of the alignments relative to the IBM HMM model, as well as the generated sense probabilities, in order to gauge the usefulness in Word Sense Disambiguation. 1 Introduction Modern machine translation is dominated by statistical methods, most of which are trained on wordaligned parallel corpora (Koehn et al., 2007; Koehn, 2004), which need to be generated separately. One of the most commonly used methods to generate these word alignments is to use the IBM models 1-5, which generate one-directional alignments. Although the IBM models perform well, they fail to take into account certain situations. For example, if an alignment between two words f1 and e1 is considered, and f1 is an uncommon translation for e1 , the translation probability will be low. It might happen, that an alignment to a different nearby word 39 is preferred by the model. Consider for example the situation where f1 is ‘taal’ (Dutch, meaning languag"
W12-1906,2005.mtsummit-papers.11,0,0.0204424,"through the senses. In the other models, the model is increasingly forced to use the shared translation probabilities. 4 Evaluation We will evaluate the early results of this model against the HMM and Model 1 results, and will do a qualitative analysis of the distribution over senses and French words that the model obtains, in order to find out if reasonable predictions for senses are made. The sense HMM model will be evaluated using the three sense inventories suggested in subsection 3.1. The dataset used was a 1 million sentence aligned English-French corpus, taken from the Europarl corpus (Koehn, 2005). The data was tokenised, length limited to a maximum length of 50, and lowercased. The results are evaluated on the test set from the ACL 2005 shared task, using Alignment 43 Error Rate. The models are all trained for 5 iterations, and a pruning threshold is employed that removes probabilities from the translation tables if it is below 1.0 · 10−6 . The results of training models based on senses generated in the 3 ways listed above is shown in Figure 3. The three SHMM models are compared against Model 1, and the standard HMM model, each of which is trained for 5 iterations. The HMM model is in"
W12-1906,P03-1058,0,0.0263419,"dnets for both languages are linked, if the ILI code of a sense is the same, the sense should be sufficiently similar. Thus, the intersection of both sets of ILI is taken to find an ILI code that is common to both pairs. If such a code is found, it represents the sense index of both words. Otherwise, the closest ILI code to the two most similar ILI codes is found, and that is taken as the sense for the word. The current work however only uses a lexical resource for one of the languages, and as such has fewer places to fail, and less demanding requirements. Other similar work includes that in (Ng et al., 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. This is done by word-aligning the parallel corpus, and then finding the senses according to WordNet given a list of nouns. Two senses are lumped together if they are translated into the same chinese word. The selection of correct translations is done manually. Only those occurrences of the chosen nouns that translate to one of the chosen chinese words are considered sense-tagged by the translation. Although similar in approach to what the current system would do, this system uses a much more simple approach to"
W12-1906,C04-1192,0,0.0863225,"Missing"
W12-1906,C96-2141,0,0.261546,"ated into the same chinese word. The selection of correct translations is done manually. Only those occurrences of the chosen nouns that translate to one of the chosen chinese words are considered sense-tagged by the translation. Although similar in approach to what the current system would do, this system uses a much more simple approach to generate sense annotations and it depends on a previously word-aligned corpus, whereas the current approach would integrate alignment and sense-tagging, whis may give a higher accuracy. 3 Senses Model The current model is based on the HMM alignment model (Vogel et al., 1996), as it is a less complex model than IBM models 3 and above, but still finds acceptable alignments. The HMM alignment model is defined as a HMM model, where the observed e a1 a2 am f1 f2 fm e Figure 1: Diagram of HMM model. Arrows indicate dependencies, grey nodes indicate known values, white nodes indicate hidden variables. variables are the words of a sentence in the French language f, and the hidden variables are alignments to words in the English sentence e, or to a null state. See figure 1 for a diagram of the standard HMM model. Under this model, French words can align to at most 1 Engli"
W12-1909,afonso-etal-2002-floresta,0,0.0149427,"d, morphology, part-of-speech and dependency markup, and developed our own conversion into UPOS. Our testing and development sets were drawn from the first 15 Eve files which were manually annotated for dependency structure. The rest of the corpus, which had not been manually annotated for syntax, was merged to form the training set. Phrase-structure treebanks As well as dependency treebanks, we used three different phrasestructure treebanks: The Dutch Alpino treebank (Bouma et al., 2000), the English Penn Treebank V3 (Marcus et al., 1993),9 and the Portuguese Floresta Sint´a(c)tica treebank (Afonso et al., 2002). As these treebanks do not explicitly mark dependencies, we automatically extracted these using head finding heuristics. Thankfully the difficult work of creating such scripts has already been done as part of the CoNLL shared tasks. We have reused their scripts to create dependency representations of these treebanks, before converting into our file format and augmenting with UPOS annotation. In the case of Dutch, we have reused the same CoNLL 2006 data; note that this dataset includes predicted part-of-speech rather than gold standard annotation (Buchholz and Marsi, 2006). For the Portuguese,"
W12-1909,W12-1912,0,0.191162,"ously dominant DMV system. Two forms of light supervision were popular, the first being the inclusion of pre-specified constraints or rules for allowable dependency links, and the second being the tuning of model parameters or selecting between competing models on the labelled development data. Obviously the merits of such supervision would depend on the desired application for the induced parser. The direct comparison of models which include a form of universal prior syntactic information with those that don’t does permit interesting development linguistic questions to be explored in future. Bisk and Hockenmaier (2012) chose to induce a restricted form of Combinatory Categorial Grammar (CCG), the parses of which were then mapped to dependency structures. Restrictions on head-child dependencies were encoded in the allowable categories for each POS tag and the heads of sentences. Key features of their approach were a maximum likelihood objective function and an iterative procedure for generating composite categories from simple ones. Such composite categories allow the parameterisation of larger units than just head-child dependencies, improving over the more limited conditioning of DMV. ˇ Maraˇcek and Zabokr"
W12-1909,D10-1117,1,0.87001,"dency for languages to favour one attachment direction over another. The most frequently cited and extended model for dependency induction is DMV (Klein and Manning, 2004). We provide results for this model trained on each of the coarse (DMVc ), fine (DMVp ), and universal (DMVu ) POS tag sets, all initialised with the original harmonic initialiser. As a further baseline we also evaluated the dependency trees resulting from directly using the harmonic initialiser without any training (H). As a strong benchmark we include the results of the non-parametric Bayesian model previously published in Blunsom and Cohn (2010) (BC). The stated results are for the unlexicalised model described in that paper where the final analysis is formed by choosing the maximum marginal probability dependency links estimated from forty independent Gibbs sampler runs. For part-of-speech tagging we include results from an implementation of the Brown word clustering algorithm (Brown et al., 1992) (Bc,p,u ), and the mkcls tool written by Franz Och (Och, 1999) (MKc,p,u ). Both of these benchmarks were trained with the number of classes matching the number in the gold standard of each of the tagsets in turn: coarse (c), fine (p), and"
W12-1909,P11-1087,1,0.829147,"n et al., 1992) (Bc,p,u ), and the mkcls tool written by Franz Och (Och, 1999) (MKc,p,u ). Both of these benchmarks were trained with the number of classes matching the number in the gold standard of each of the tagsets in turn: coarse (c), fine (p), and universal (u). A notable property of both of these word class models is that they enforce a one-tag-per-type restriction that ensures there is a one-to-one mapping between word types and classes. For POS tagging we also provide benchmark results from two previously published models. The first of these is the Pitman-Yor HMM model described in (Blunsom and Cohn, 2011), which incorporates ta one-tag-per-type restriction (BC). This model was trained with the same number of tags as in the gold standard fine tag set for each corpus. The second benchmark is the HMM with Sparsity Constraints trained using Posterior Regularization (PR) described in (Grac¸a et al., 2011). In this model the HMM emission probabilitiy distribution are estimated using small Maximum Entropy models (features set described in the original paper). The models were trained for 200 iterations of PR using both the same number of hidden states as the coarse Gc and universal Gu gold standard. A"
W12-1909,J92-4003,0,0.0708554,"baseline we also evaluated the dependency trees resulting from directly using the harmonic initialiser without any training (H). As a strong benchmark we include the results of the non-parametric Bayesian model previously published in Blunsom and Cohn (2010) (BC). The stated results are for the unlexicalised model described in that paper where the final analysis is formed by choosing the maximum marginal probability dependency links estimated from forty independent Gibbs sampler runs. For part-of-speech tagging we include results from an implementation of the Brown word clustering algorithm (Brown et al., 1992) (Bc,p,u ), and the mkcls tool written by Franz Och (Och, 1999) (MKc,p,u ). Both of these benchmarks were trained with the number of classes matching the number in the gold standard of each of the tagsets in turn: coarse (c), fine (p), and universal (u). A notable property of both of these word class models is that they enforce a one-tag-per-type restriction that ensures there is a one-to-one mapping between word types and classes. For POS tagging we also provide benchmark results from two previously published models. The first of these is the Pitman-Yor HMM model described in (Blunsom and Coh"
W12-1909,D11-1059,0,0.0301117,"a two stage approach to inducing part-of-speech tags. The first stage used an LDA style probabilistic model to induce a distribution over possible tags for a given word type. These distributions were then hierarchically clustered and the final tags selected using the prefix of the path from the root node to the word type in the cluster tree. The length of the prefixes, and thus the number of tags, was tuned on the labelled development data. The system of Christodoulopoulos et al. (2012) was based upon an LDA type model which included both contexts and other conditionally independent features (Christodoulopoulos et al., 2011). This base system was then iterated with a DMV system and with the resultant dependencies being repeatedly fed back into the POS model as features. This submission is notable for being one of the first to attempt joint POS and dependency induction rather than taking a pipeline approach. 5.2 Dependency Induction The dependency parsing task saw a variety of approaches with only a couple based on the previously dominant DMV system. Two forms of light supervision were popular, the first being the inclusion of pre-specified constraints or rules for allowable dependency links, and the second being"
W12-1909,W12-1913,0,0.15553,"ll of these submissions made significant departures from the benchmark HMM and DMV approaches which have dominated the published literature on these tasks in recent years. The submissions were characterised by varied choices of model structure, parameterisation, regularisation, and the degree to which light supervision was provided through constraints or the use of labelled tuning data. In the following sections we summarise the approaches taken by the systems submitted for each task. 5.1 Part-of-Speech Induction The part-of-speech induction challenge received two submission, (Chrupała, 2012; Christodoulopoulos et al., 2012). Both of these submissions based their induction systems on LDA inspired models for clustering word types by the contexts in which they appear. Notably, the strongest of the provided benchmarks and the two submissions modelled part-of-speech tags at the type level, thus restricting all tokens of a given word type to share the same tag. Though 69 clearly out of step with the gold standard tagging, this one-tag-per-type restriction has previously been shown to be a crude but effective way of regularising models towards a good solution. Below we summarise the approach of each submission, identif"
W12-1909,W12-1914,0,0.0150223,"Encouragingly all of these submissions made significant departures from the benchmark HMM and DMV approaches which have dominated the published literature on these tasks in recent years. The submissions were characterised by varied choices of model structure, parameterisation, regularisation, and the degree to which light supervision was provided through constraints or the use of labelled tuning data. In the following sections we summarise the approaches taken by the systems submitted for each task. 5.1 Part-of-Speech Induction The part-of-speech induction challenge received two submission, (Chrupała, 2012; Christodoulopoulos et al., 2012). Both of these submissions based their induction systems on LDA inspired models for clustering word types by the contexts in which they appear. Notably, the strongest of the provided benchmarks and the two submissions modelled part-of-speech tags at the type level, thus restricting all tokens of a given word type to share the same tag. Though 69 clearly out of step with the gold standard tagging, this one-tag-per-type restriction has previously been shown to be a crude but effective way of regularising models towards a good solution. Below we summarise the ap"
W12-1909,erjavec-etal-2010-jos,0,0.0235483,"Missing"
W12-1909,W12-1915,0,\N,Missing
W12-1909,nivre-etal-2006-talbanken05,0,\N,Missing
W12-1909,J93-2004,0,\N,Missing
W12-1909,D10-1120,0,\N,Missing
W12-1909,W07-0604,0,\N,Missing
W12-1909,W06-2920,0,\N,Missing
W12-1909,N06-1041,0,\N,Missing
W12-1909,E99-1010,0,\N,Missing
W12-1909,P11-1067,0,\N,Missing
W12-1909,D10-1056,0,\N,Missing
W12-1909,D11-1036,0,\N,Missing
W12-1909,W07-2416,0,\N,Missing
W12-1909,P04-1061,0,\N,Missing
W12-1909,P07-1031,0,\N,Missing
W12-1909,W12-1910,0,\N,Missing
W12-1909,D07-1096,0,\N,Missing
W12-1909,D07-1031,0,\N,Missing
W12-1909,D07-1043,0,\N,Missing
W13-2241,P13-1004,1,0.421716,"Missing"
W13-2241,P07-2045,0,0.00303498,"Missing"
W13-2241,D08-1112,0,0.0127566,"a two-step procedure where we first optimise a model with all the SE length scales tied to the same value (which is equivalent to an isotropic model) and we used the resulting values as starting point for the ARD optimisation. 2 339 http://sheffieldml.github.io/GPy/ only simulate AL) and then adds it to the training set. Our procedure started with 50 instances for task 1.1 and 20 instances for task 1.3, given its reduced training set size. We optimised the Gaussian Process hyperparameters every 20 new instances, for both tasks. As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008). This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is: ID(x) = V ar(y|x) × !β U 1 X (u) sim(x, x ) U Figure 1: Test error and test confidence curves for HTER prediction (task 1.1) using the WMT12 training and test sets. u=1 The β parameter controls the relative importance of the density term. In our experiments, we set it to 1, giving equal weights to variance and density. The U term is the number of instances in the query pool. The variance values V ar(y|x) are given by the GP prediction while the similar"
W13-2241,2013.mtsummit-papers.21,1,0.684818,"(x, x0 )) which is parameterized by a mean function (here, 0) and a covariance kernel function k(x, x0 ). Each http://www.quest.dcs.shef.ac.uk 338 All our models were trained using the GPy2 toolkit, an open source implementation of GPs written in Python. response value is then generated from the function evaluated at the corresponding input, yi = f (xi )+ η, where η ∼ N (0, σn2 ) is added white-noise. Prediction is formulated as a Bayesian inference under the posterior: Z p(y∗ |x∗ , D) = p(y∗ |x∗ , f )p(f |D) 3.1 Feature Selection To perform feature selection, we followed the approach used in Shah et al. (2013) and ranked the features according to their learned length scales (from the lowest to the highest). The length scales of a feature can be interpreted as the relevance of such feature for the model. Therefore, the outcome of a GP model using an ARD kernel can be viewed as a list of features ranked by relevance, and this information can be used for feature selection by discarding the lowest ranked (least useful) ones. For task 1.1, we performed this feature selection over all 160 features mentioned in Section 2. For task 1.3, we used a subset of the 80 most general BB features as in (Shah et al."
W13-2241,2006.amta-papers.25,0,0.234895,"Missing"
W13-2241,2009.mtsummit-papers.16,1,0.860974,"of the available instances for training. These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Callison-burch et al., 2012). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT13 QE shared task defined a group of tasks related to QE. In this paper, we present 2 Features In our experiments, we used a set of 160 features which are grouped into black box (BB) and glass box (GB) features. They were extracted using the 337 Proceedings of the Eighth Workshop on Statistic"
W13-2241,P13-4014,1,0.477368,"Missing"
W13-2241,P13-2097,1,0.862588,"Missing"
W13-2241,C04-1046,0,0.0592042,"Missing"
W13-2241,W12-3102,1,\N,Missing
W14-1616,P11-1105,0,0.503106,"Missing"
W14-1616,N13-1001,0,0.492006,"s a 4-SCFG). In contrast, the hierarchical phrase-based model allows only 2-SCFG as each production can rewrite as a maximum of two nonterminals. On the other hand, our approach does not enforce a valid hierarchically nested derivation which is the case for Chiang’s approach. 4 Related Work The method introduced in this paper uses factors defined in the same manner as in Feng and Cohn (2013), but the two methods are quite different. That method (Feng and Cohn, 2013) is wordbased and under the frame of Bayesian model while this method is MP-based and uses a simpler Kneser-Ney smoothing method. Durrani et al. (2013) also present a Markov model based on MPs (they call minimal translation units) and further define operation sequence over MPs which are taken as the events in the Markov model. For the probability estimation, they use Kneser-Ney smoothing with a single backoff path. Different from operation sequence, our method gives a neat definition of factors which uses jump distance directly and avoids the bundle of source words and target words like in their method, and hence mitigates sparsity. Moreover, the use of parallel backoff infers richer structures and provides robust modeling. There are several"
W14-1616,P13-1033,1,0.787915,"oehn et al., 2007) have drastically improved beyond word-based approaches, primarily by using phrase-pairs as translation units, which can memorize local lexical context and reordering patterns. However, this literal memorization mechanism makes it generalize poorly to unseen data. Moreover, phrase-based models make an independent assumption, stating that the application of phrases in a derivation is independent to each other which conflicts with the underlying truth that the translation decisions of phrases should be dependent on context. There are some work aiming to solve the two problems. Feng and Cohn (2013) propose a word-based Markov model to integrate translation and reordering into one model and use the sophisticated hierarchical Pitman-Yor process which backs off from larger to smaller context to provide dynamic adaptive smoothing. This model shows good generalization to unseen data while 151 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 151–159, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Figure 1, the MP sequence is shown in Figure 2. To evaluate the Markov model, we condition each MP on the previous k − 1"
W14-1616,W02-1039,0,0.134573,"Missing"
W14-1616,N10-1140,0,0.0188303,"500; For Moses, search-algorithm=1 and distortion-limit=6; For Moses-chart, search-algorithm=3 and max-charspan8 =20 for Moses-chart. We used both the distortion model and the lexical reordering model for Moses (denoted as Moses-l) except in §5.5 we only used the distortion model (denoted as Moses-d). We implemented the OSM according to Durrani et al. (2013) and used the same configuration with Moses-l. For our method we used the same configuration as Moses-l but adding an additional feature of the Markov model over MPs. do they consider dynamic strategies for estimating k-gram probabilities. Galley and Manning (2010) propose a method to introduce discontinuous phrases into the phrasebased model. It makes use of the decoding mechanism of the phrase-based model which jumps over the source words and hence can hold discontinuous phrases naturally. However, their method doesn’t touch the correlations between phrases and probability modeling which are the key points we focus on. 5 Experiments We design experiments to first compare our method with the phrase-based model (PB), the operation sequence model (OSM) and the hierarchical phrase-based model (HPB), then we present several experiments to test: 1. how each"
W14-1616,N03-1017,0,0.108987,"Missing"
W14-1616,P07-2045,0,0.088292,"= (j1 , j2 , . . . , jI ) is the vector of jump distance between MPi−1 and MPi , or insert for MPs with null source sides.2 To evaluate each of the k-gram models, we use modified Keneser-Ney smoothing to back off from larger context to smaller context recursively. In summary, adding the Markov model into the decoder involves two passes: 1) training a model over the MP sequences extracted from a word aligned parallel corpus; and 2) calculating the probability of the Markov model for each translation hypothesis during decoding. This Markov model is combined with a standard phrase-based model3 (Koehn et al., 2007) and used as an additional feature in the linear model. In what follows, we will describe how to estatimate the k-gram Markov model, focusing on backoff (§2.1) and smoothing (§2.2). Our model is phrase-based and works like a phrase-based decoder by generating target translation left to right using phrase-pairs while jumping around the source sentence. For each derivation, we can easily get its minimal phrase (MPs) sequence where MPs are ordered according to the order of their target side. Then this sequence of events is modeled as a Markov model and the log probability under this Markov model"
W14-1616,N12-1005,0,0.0899217,"e, our method gives a neat definition of factors which uses jump distance directly and avoids the bundle of source words and target words like in their method, and hence mitigates sparsity. Moreover, the use of parallel backoff infers richer structures and provides robust modeling. There are several other work focusing on modeling bilingual information into a Markov model. Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, and use it as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Neither work includes the jump distance, and nor Figure 5: Approximate SCFG patterns for step 0, 3 of Figure 3. X is a non-terminal which can only be rewritten by one MP. · and · · · denote gaps introduced by the left-to-right decoding algorithm and · can only cover one MP while · · · can cover zero or more MPs. In step 1, as the jump factor 1 is dropped, we do not know the orientation between bˇa and t¯a. However several jump distances are known: from X 1 to bˇa is distance -2 and t¯a to k"
W14-1616,N03-2002,0,0.0637057,"troduce gaps in estimating rule probabilities; these backoff patterns often bear close resemblance to SCFG productions in the hierarchical phrase-based model (Chiang, 2007). For example, in step 0 in Figure 3, as all the jump factors are present, this encodes the full ordering of the MPs and gives rise to the aligned MP pairs shown in Figure 5 (a). Note that an X 1 placeholder is included to ensure the jump distance from the previous MP to the MP <bˇa, take> is -2. The approximate SCFG production for the MP pairs is Probability Estimation We adopt the technique used in factor language models (Bilmes and Kirchhoff, 2003; Kirchhoff et al., 2007) to estimate the probability of a k-gram i i p(¯ ei |c) where c = f¯i−k+1 , ji−k+1 , e¯−1 i−k+1 . According to the definition of backoff, only when the count of the k-gram exceeds some given threshold, its maximum-likelihood estimate, pML (¯ ek |c) = N (¯ ek ,c) N (c) is used, where N (·) is the count of an event and/or context. Otherwise, only a portion of pML (¯ ek |c) is used and the remainder is constructed from a lower-level (by dropping a factor). In order to ensure valid probability estimates, i.e. sums <bˇa t¯a X 1 kˇaol`v j`ınq`u, X 1 take it into account>. 4"
W14-1616,J03-1002,0,0.00594419,"ten by one MP. · and · · · denote gaps introduced by the left-to-right decoding algorithm and · can only cover one MP while · · · can cover zero or more MPs. In step 1, as the jump factor 1 is dropped, we do not know the orientation between bˇa and t¯a. However several jump distances are known: from X 1 to bˇa is distance -2 and t¯a to kˇaol`v j`ınq`u is 2. In this case, the source side can be bˇa t¯a X 1 kˇaol`v j`ınq`u, 155 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4grams. We used minimum error rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7 =2"
W14-1616,P96-1041,0,0.21057,"back off to and for the node e¯k |f¯2k , j2k , e¯2k−1 where only the factors of MP1 are dropped, there are k-2 nodes to back off to. 2.2 to unity, probability mass needs to be “stolen” from the higher level and given to the lower level. Hence, the whole definition is ( d pml (¯ ei |c) if N (¯ ei , c) > τk p(¯ ei |c) = N (¯ei ,c) α(c)g(¯ ei , c) otherwise (3) where dN (¯ei ,c) is a discount parameter which reserves probability from the maximum-likelihood estimate for backoff smoothing at the next lowerlevel, and we estimate dN (¯ei ,c) using modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996); τk is the threshold for the count of the k-gram, α(c) is the backoff weight used to make sure the entire distribution still sums to unity, P 1 − e¯:N (¯e,c)>τk dN (¯e,c) pM L (¯ e|c) P α(c) = , e, c) e¯:N (¯ e,c)≤τk g(¯ and g(¯ ei , c) is the backoff probability which we estimate by averaging over the nodes in the next lower level, g(¯ ei , c) = 1X p(¯ ei |c0 ) , φ 0 c where φ is the number of nodes to back off, c0 is the lower-level context after dropping one factor from c. The k-gram for the source and jump factors are estimated in the same way, using the same backoff semantics.4 Note (3)"
W14-1616,P03-1021,0,0.351521,"follows, we will describe how to estatimate the k-gram Markov model, focusing on backoff (§2.1) and smoothing (§2.2). Our model is phrase-based and works like a phrase-based decoder by generating target translation left to right using phrase-pairs while jumping around the source sentence. For each derivation, we can easily get its minimal phrase (MPs) sequence where MPs are ordered according to the order of their target side. Then this sequence of events is modeled as a Markov model and the log probability under this Markov model is included as an additional feature into the linear SMT model (Och, 2003). A MP denotes a phrase which cannot contain other phrases. For example, in the sentence pair in Figure 1, <bˇa t¯a , take it> is a phrase but not a minimal phrase, as it contains smaller phrases of <bˇa , take> and <t¯a , it>. MPs are a complex event representation for sequence modelling, and using these naively would be a poor choice because few bigrams and trigrams will be seen often enough for reliable estimation. In order to reason more effectively from sparse data, we consider more generalized representations by decomposing MPs into their component events: the source phrase (source f¯),"
W14-1616,J07-2003,0,0.0583757,"lower level, g(¯ ei , c) = 1X p(¯ ei |c0 ) , φ 0 c where φ is the number of nodes to back off, c0 is the lower-level context after dropping one factor from c. The k-gram for the source and jump factors are estimated in the same way, using the same backoff semantics.4 Note (3) is applied independently to each of the three models, so the use of backoff may differ in each case. 3 Discussion As a part of the backoff process our method can introduce gaps in estimating rule probabilities; these backoff patterns often bear close resemblance to SCFG productions in the hierarchical phrase-based model (Chiang, 2007). For example, in step 0 in Figure 3, as all the jump factors are present, this encodes the full ordering of the MPs and gives rise to the aligned MP pairs shown in Figure 5 (a). Note that an X 1 placeholder is included to ensure the jump distance from the previous MP to the MP <bˇa, take> is -2. The approximate SCFG production for the MP pairs is Probability Estimation We adopt the technique used in factor language models (Bilmes and Kirchhoff, 2003; Kirchhoff et al., 2007) to estimate the probability of a k-gram i i p(¯ ei |c) where c = f¯i−k+1 , ji−k+1 , e¯−1 i−k+1 . According to the defini"
W14-1616,P11-2031,0,0.069853,"he distortion and lexical reordering models of Moses, and are they complemenatary; 4. whether using MPs as translation units is better in our approach than the simpler tactic of using only word pairs. 5.1 5.2 Data Setup We first give the results of performance comparison. Here we add another system (denoted as Moses-l+trgLM): Moses-l together with the target language model trained on the training data set, using the same configuration with Moses-l. This system is used to test whether our model gains improvement just for using additional information on the training set. We use the open tool of Clark et al. (2011) to control for optimizer stability and test statistical significance. The results are shown in Tables 1 and 2. The two language pairs we used are quite different: Chinese has a much bigger word order difference c.f. English than does Arabic. The results show that our system can outperform the baseline We consider two language pairs: Chinese-English and Arabic-English. The Chinese-English parallel training data is made up of the non-UN portions and non-HK Hansards portions of the NIST training corpora, distributed by the LDC, having 1,658k sentence pairs with 40m and 44m Chinese and English wo"
W14-1616,P02-1040,0,0.0915753,"bˇa is distance -2 and t¯a to kˇaol`v j`ınq`u is 2. In this case, the source side can be bˇa t¯a X 1 kˇaol`v j`ınq`u, 155 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4grams. We used minimum error rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7 =20, stacksize=50 and max-pop-limit=500; For Moses, search-algorithm=1 and distortion-limit=6; For Moses-chart, search-algorithm=3 and max-charspan8 =20 for Moses-chart. We used both the distortion model and the lexical reordering model for Moses (denoted as Moses-l) except in §5.5 we only used the distortion mo"
W14-2507,P13-1098,1,0.882009,"Missing"
W16-3010,H05-1091,0,0.0976369,"oach of combining many subclasses into one negative class reduced precision and hence overall performance. 3. Co-occurrence: A simple approach to relation extraction is to consider all event pairs that occur within a sentence as related. We tried using this cooccurrence strategy for relation types for which SVM or Naive Bayes classifiers did not work effectively. We abandoned this strategy as we observed that the overall F1 score reduced over the development dataset, even as the recall at the relation level improved. 4. Kernel methods: We experimented with the shortest dependency path kernel (Bunescu and Mooney, 2005) and the subset tree kernels (Moschitti, 2006) for classification with SVMs. However their performance was quite low (F1 score &lt; 0.20). It is likely that small training set sizes and multiple entity pairs in most sentences affect the performance of these kernel methods. 5. Dominant class types : In our system we adopted the strategy of only accepting predictions of the dominant class type from each classifier. That is, we filter out predictions of type rj from classifier Ci when j 6= i. This strategy proved very effective when tested over the development dataset. Without this filtering step, w"
W16-3010,W16-3001,0,0.0394085,"Missing"
W16-3010,W09-1401,0,0.049046,"t. Different tissues involving complex genetics and various environmental factors are responsible for the healthy development of a seed. A large body of research literature is available containing this knowledge. The SeeDev binary relation extraction subtask of the BioNLP Shared Task 2016 (Chaix et al., 2016) focuses on extracting relations or events that involve two biological entities as expressed in full-text publication articles. The task represents an important contribution to the broader problem of biomedical relation extraction. Similar to previous BioNLP shared tasks in 2009 and 2011 (Kim et al., 2009; Kim et al., 2011), this task focuses on molecular information extraction. The task organisers provided paragraphs from manually selected full text publications on seed development of Arabidopsis thaliana 2 Approach The seedev task involves extraction of 22 different binary events over 16 entity types. Entity mentions within a sentence and the events between them are provided in the gold standard annotations. In the rest of the article, we refer to an event with two entity arguments as simply a binary relation. We treat relation identification as a supervised 1 Source: https://github.com/unim"
W16-3010,W11-1801,0,0.0753552,"Missing"
W16-3010,W13-2010,1,0.850749,"Missing"
W16-3010,P14-5010,0,0.0079723,"ea and eb are known to be related by the type rc , from the relation annotations, we set label = rc . If they are not related, we set label =NR. NR is a special label to denote no relation. 2. Add the triple t = (ea , eb , label) to the training set of Ci , if (ea , eb ) satisifies the type signature for relation ri , i ∈ [1, 22]. 2.4 Feature Engineering We developed a set of common lexical, syntactic and dependency parse based features. Relation specific features were also developed. For part of speech tagging and dependency parsing of the text, 83 we used the toolset from Stanford CoreNLP (Manning et al., 2014). These features are described in detail below. sists of a total of 7, 082 entities and 3, 575 binary relations and is partitioned into training, development and test datasets. Gold standard entity and relation annotations are provided for training and development data and for test data only entity annotations have been released. The given set of 16 entity types are categorized into 7 different entity groups and 22 different relation types are defined. Pre-defined event signatures constrain the types of entity arguments for each relation. 1. Stop word removal: For some relations (“Has Sequence"
W16-3010,E06-1015,0,0.0370004,"ss reduced precision and hence overall performance. 3. Co-occurrence: A simple approach to relation extraction is to consider all event pairs that occur within a sentence as related. We tried using this cooccurrence strategy for relation types for which SVM or Naive Bayes classifiers did not work effectively. We abandoned this strategy as we observed that the overall F1 score reduced over the development dataset, even as the recall at the relation level improved. 4. Kernel methods: We experimented with the shortest dependency path kernel (Bunescu and Mooney, 2005) and the subset tree kernels (Moschitti, 2006) for classification with SVMs. However their performance was quite low (F1 score &lt; 0.20). It is likely that small training set sizes and multiple entity pairs in most sentences affect the performance of these kernel methods. 5. Dominant class types : In our system we adopted the strategy of only accepting predictions of the dominant class type from each classifier. That is, we filter out predictions of type rj from classifier Ci when j 6= i. This strategy proved very effective when tested over the development dataset. Without this filtering step, we found that our system gets a high recall as"
W17-1002,W04-1013,0,0.0146504,"coder, so ot = Wo xt + Uo ht−1 + bo jt = Wj xt + Uj ht−1 + bj (1) ct = ct−1 ∗ σ(ft ) + tanh(jt ) ∗ σ(it ) ht = tanh(ct ) ∗ σ(ot ) 2 The unnormalised attention weights are computed by combining ht−1 with the convolutional embedding of each source word via dot product. where it , ft and ot are input, forget and output gates, respectively; jt , ct and ht represent the new 8 Combination add-input add-hidden stack-input stack-hidden mlp-input mlp-hidden Equation i0t = it + d h0t = ht + d i0t = [it ; d] h0t = [ht ; d] i0t = tanh(Wi it + Wd d + b) h0t = tanh(Wi ht + Wd d + b) using the ROUGE metric (Lin, 2004), following the same evaluation style of benchmark systems (Rush et al., 2015; Chopra et al., 2016). For outof-domain experiments, we use the same models trained from G IGAWORD, but tune using DUC 03 and test on DUC 04; DUC 03 and DUC 04 each have 500 examples. For the doc2vec encoder, we train using G IGA WORD and infer document vectors for validation and test examples using the trained model. Valid and test examples are excluded from the doc2vec training data. Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time"
W17-1002,D15-1044,0,0.571169,"the document vector to a recurrent neural network decoder. With this decoupled architecture, we decrease the number of parameters in the decoder substantially, and shorten its training time. Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents, but less well for out-of-domain documents. 1 Introduction Abstractive document summarization is a challenging natural language understanding task. Abstractive methods first encode the original document into a high-level representation, and then decode it into the target summary. Rush et al. (2015) proposed the task of headline generation as the first step towards abstractive summarization. Instead of using the full document, the authors experimented with using the first sentence as input, with the aim of generating a coherent headline given the sentence. 2 Attentive Recurrent Neural Network: A Joint Encoder–decoder Architecture The attentive recurrent neural network is composed of an attentive encoder and a recurrent decoder (Chopra et al., 2016), where the encoder is 1 The training time is decreased from 4 days (with full G I GAWORD ) for the coupled model (Rush et al., 2015) to 2 day"
W17-1002,N16-1012,0,0.13036,"nh(jt ) ∗ σ(it ) ht = tanh(ct ) ∗ σ(ot ) 2 The unnormalised attention weights are computed by combining ht−1 with the convolutional embedding of each source word via dot product. where it , ft and ot are input, forget and output gates, respectively; jt , ct and ht represent the new 8 Combination add-input add-hidden stack-input stack-hidden mlp-input mlp-hidden Equation i0t = it + d h0t = ht + d i0t = [it ; d] h0t = [ht ; d] i0t = tanh(Wi it + Wd d + b) h0t = tanh(Wi ht + Wd d + b) using the ROUGE metric (Lin, 2004), following the same evaluation style of benchmark systems (Rush et al., 2015; Chopra et al., 2016). For outof-domain experiments, we use the same models trained from G IGAWORD, but tune using DUC 03 and test on DUC 04; DUC 03 and DUC 04 each have 500 examples. For the doc2vec encoder, we train using G IGA WORD and infer document vectors for validation and test examples using the trained model. Valid and test examples are excluded from the doc2vec training data. Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time t; and “[·; ·]” denotes vector concatenation. 4.2 input, new context and new hidden state, respecti"
W17-1002,N16-1149,1,0.834556,"Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time t; and “[·; ·]” denotes vector concatenation. 4.2 input, new context and new hidden state, respectively; ∗ is the elementwise vector product; and σ is the sigmoid activation function. Given an input word and previous hidden state, the decoder predicts the next word and generates the summary one word at a time. To generate summaries that are related to the document, we incorporate the doc2vec input document signal to the decoder using several methods proposed by Hoang et al. (2016). There are two layers where we can incorporate doc2vec: in the input layer (input), or hidden layer (hidden). There are three methods of incorporation: addition (add), stacking (stack), or via a multilayer perceptron (mlp). Table 1 illustrates the 6 possible approaches to incorporation. Note that add requires doc2vec to have the same vector dimensionality as the layer it is combined with, and stack-hidden doubles the hidden size (assuming they have the same dimensions), resulting in a large output projection matrix and longer training time. 4 4.1 Hyper-parameter tuning For the encoder, we exp"
W17-1002,W16-1609,1,\N,Missing
W17-4115,D15-1041,0,0.059412,"Missing"
W17-4115,P03-1021,0,0.0525884,"Missing"
W17-4115,P16-2058,0,0.0842576,"Missing"
W17-4115,E17-1048,0,0.100246,"Missing"
W17-4115,P17-1184,0,0.0482079,"Missing"
W17-4115,D13-1176,0,0.123419,"Missing"
W17-4115,2005.mtsummit-papers.11,0,0.0131352,"Missing"
W17-4115,N16-1030,0,0.136004,"Missing"
W17-4115,D15-1176,0,0.0715268,"Missing"
W17-4115,W13-3512,0,0.0493206,"Missing"
W17-5404,P06-4018,0,0.00742436,"utputs to fix minor grammar errors, such as adding or removing the determiner a or the. 2.2.2 3 Token Substitution Method Once the algorithm has decided which tokens should be changed, the next move is to find appropriate substitutions. As described above, most systems are based on n-grams, making them very sensitive to unknown tokens. Therefore, we came up with some heuristics. The first approach draws on our earlier work on learning robust text representations (Li et al., 2017), and is based on synonyms of the given token, based on Princeton WordNet (Miller et al., 1990) using the NLTK API (Bird, 2006). Here, we test possible synonyms, considering their part-ofspeech tag, asking the system s whether the loss is reduced after substitution. We also tried to find antonyms that cannot be recognized by the system, causing the predicted sentiment label to not flip. Finally, we add a small amount of human superviResults and Analysis In this section, we detail the results of our methods, and perform error analysis. 3.1 Builder The results for the builder systems over the test set are shown in Table 1. To evaluate the robustness of the builder systems, there are two evaluation criteria: average F-sc"
W17-5404,P15-2030,1,0.839782,"minimal pair, with positive (+1) sentiment: This paper describes our submission to the sentiment analysis sub-task of “Build It, Break It: The Language Edition (BIBI)”, on both the builder and breaker sides. As a builder, we use convolutional neural nets, trained on both phrase and sentence data. As a breaker, we use Q-learning to learn minimal change pairs, and apply a token substitution method automatically. We analyse the results to gauge the robustness of NLP systems. 1 Introduction Recently, deep learning models have made impressive gains over a range of NLP tasks (Bahdanau et al., 2015; Bitvai and Cohn, 2015). However, recent studies have exposed brittleness in the models, e.g. through adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). In these papers, researchers construct cognitively implausible perturbations of raw image inputs to fool state-of-the-art deep learning models. These perturbations are cheap and easy to generate using a “fast-gradient” method, based on analysis of the derivative of the loss with respect to the input. One issue with the generation of adversarial examples for NLP has been the fact that language data is discrete, and hence difficult to map the contin"
W17-5404,P11-1038,1,0.70899,"ples, making our approach limited in application. Therefore, we can’t really conclude that our automatic approach is a success, and we should explore more flexible approaches in the future. However, the approach itself still achieves a break rate higher than the error rate on the origiIt is almost the same situation when the systems encounter out-of-vocabulary words (OOV). Although OOVs are a significant challenge, we believe they can be overcome by training better sentiment-sensitized word embeddings (Mrkˇsi´c et al., 2016), or combining the system with character-level normalization methods (Han and Baldwin, 2011). However, CNNs are not good at dealing with complex grammatical structures or long-distance dependencies. For instance, changing a comparative from more than to less than flips the sentiment and is something that humans are sensitized to, but CNNs tend not to capture this difference. Also, CNNs are not sensitive to tense, such as changing the present tense is to the past tense was to capture pragmatic/connotative effects. For these kinds of examples, we expected to see higher performance among models which better capture syntactic structure, such as recursive neural nets (“RNNs”: Socher et al"
W17-5404,P14-1062,0,0.226354,"Missing"
W17-5404,E17-2004,1,0.797792,"etable as ironic — which remains a challenging problem for us to totally eliminate during generation. Last, we slightly modify the outputs to fix minor grammar errors, such as adding or removing the determiner a or the. 2.2.2 3 Token Substitution Method Once the algorithm has decided which tokens should be changed, the next move is to find appropriate substitutions. As described above, most systems are based on n-grams, making them very sensitive to unknown tokens. Therefore, we came up with some heuristics. The first approach draws on our earlier work on learning robust text representations (Li et al., 2017), and is based on synonyms of the given token, based on Princeton WordNet (Miller et al., 1990) using the NLTK API (Bird, 2006). Here, we test possible synonyms, considering their part-ofspeech tag, asking the system s whether the loss is reduced after substitution. We also tried to find antonyms that cannot be recognized by the system, causing the predicted sentiment label to not flip. Finally, we add a small amount of human superviResults and Analysis In this section, we detail the results of our methods, and perform error analysis. 3.1 Builder The results for the builder systems over the te"
W17-5404,N16-1018,0,0.0232632,"Missing"
W17-5404,P05-1015,0,0.312829,"Timothy Baldwin School of Computing and Information Systems The University of Melbourne, Australia yitongl4@student.unimelb.edu.au, {tcohn,tbaldwin}@unimelb.edu.au Abstract different inputs, and breakers try to construct instances which will cause the builders’ systems to make incorrect predictions. In this paper, we describe our builder and breaker submissions to the sentiment analysis subtask, which is a sentence-level binary classification task, to predict whether a given review sentence is positive or negative with respect to a given movie. The data set is derived from movie review data (Pang and Lee, 2005) and the Stanford Sentiment Treebank (Socher et al., 2013). We participated both as a builder and breaker because we are interested in testing the robustness of state-of-the-art neural models, such as convolutional neural networks (“CNNs”: Kim (2014)). Also, we were interested in the breaker task as an avenue for exploring how well we can automatically construct adversarial test instances. In the sentiment sub-task, the main job of breakers is to construct minimally-changed pairs that are able to fool the builders’ sentiment analysers. For example, the following sentences can be considered to"
W17-5404,D13-1170,0,0.112127,"ems The University of Melbourne, Australia yitongl4@student.unimelb.edu.au, {tcohn,tbaldwin}@unimelb.edu.au Abstract different inputs, and breakers try to construct instances which will cause the builders’ systems to make incorrect predictions. In this paper, we describe our builder and breaker submissions to the sentiment analysis subtask, which is a sentence-level binary classification task, to predict whether a given review sentence is positive or negative with respect to a given movie. The data set is derived from movie review data (Pang and Lee, 2005) and the Stanford Sentiment Treebank (Socher et al., 2013). We participated both as a builder and breaker because we are interested in testing the robustness of state-of-the-art neural models, such as convolutional neural networks (“CNNs”: Kim (2014)). Also, we were interested in the breaker task as an avenue for exploring how well we can automatically construct adversarial test instances. In the sentiment sub-task, the main job of breakers is to construct minimally-changed pairs that are able to fool the builders’ sentiment analysers. For example, the following sentences can be considered to be a minimal pair, with positive (+1) sentiment: This pape"
W18-2703,W11-2138,0,0.144125,"is used to build better translation systems in forward and backward directions, which in turn is used to reback-translate monolingual data. This process can be “iterated” several times. This is a form of co-training (Blum and Mitchell, 1998) where the two models over both translation directions can be used to train one another. We show that iterative back-translation leads to improved results over simple back-translation, under both high and 2 Related Work The idea of back-translation dates back at least to statistical machine translation, where it has been used for semi-supervised learning (Bojar and Tamchyna, 2011), or self-training (Goutte et al., 2009, ch.12, p.237). In modern NMT research, Sennrich et al. (2017) reported significant gains on the WMT and IWSLT shared tasks. They showed that even simply duplicating the monolingual target data into the source was sufficient to realise some benefits. Currey et al. (2017) reported similar findings for low resource conditions, showing that even poor translations can be beneficial. Gwinnup et al. (2017) mention in their system description iteratively applying back-translation, but did not report successful experiments. A more refined idea of back-translatio"
W18-2703,W16-2323,0,0.520255,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,P16-1009,0,0.485876,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,E17-3017,0,0.0244524,"gs) with droption that the back-translation approach still imout of 0.2 for the RNN parameters, and 0.1 otherproves the translation accuracy in all language wise. Training is smoothed with moving average. pairs with a low-resource setting. In the English– It takes about 2–4 days. French experiments, large improvements over the The deep system uses matches the setup of baseline are observed in both directions, with +3.5 Edinburgh’s WMT 2017 system (Sennrich et al., 3 The difference here is on the NMT toolkit used — we 2017). It uses 4 encoder and 4 decoder layers opted to use Amazon’s Sockeye (Hieber et al., 2017). We (Marian setting best-deep) with LSTM cells. used Sockeye’s default configuration with dropout 0.5. 21 Setting NMT baseline back-translation back-translation iterative+1 back-translation iterative+2 back-translation (w/ Moses) French–English 100K 1M English–French 100K 1M 16.7 22.1 22.5 22.6 23.7 18.0 21.5 22.7 22.6 23.5 24.7 27.8 27.9 25.6 27.0 27.3 Farsi–English 100K English-Farsi 100K 21.7 22.1 22.7 22.6 21.8 16.4 16.7 17.1 17.2 16.8 Table 4: Low Resource setting: Impact of the quality of the back-translation systems on the benefit of the synthetic parallel for the final system in a low"
W18-2703,P16-1162,0,0.808379,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,U16-1001,1,0.520715,"nt of parallel data to reach reasonable performance (Koehn and Knowles, 2017). In a lowthe parallel data and the synthetic data generresource setting, only small amount of parallel ated by the base translation system. For better data exist. Previous work has attempted to inperformance, we train a deep model with 8corporate prior or external knowledge to compencheckpoint ensembling; again we use a beam sate for the lack of parallel data, e.g. injecting insize of 2. ductive bias via linguistic constraints (Cohn et al., The final back-translation systems were trained 2016) or linguistic factors (Hoang et al., 2016). using several different systems: a shallow arHowever, it is much cheaper and easier to obtain chitecture, a deep architecture, and an ensemmonolingual data in either the source or target lanble system of 4 independent training runs. guage. An interesting question is whether the (iterAcross the board, the final systems with reative) back-translation can compensate for the lack back-translation outperform the final systems with of parallel data in such low-resource settings. simple back-translation, by a margin of 0.5–1.1 BLEU. To explore this question, we conducted experiments on two datasets"
W18-2703,P18-4020,0,0.0750003,"Missing"
W18-2703,W17-3204,1,0.74613,"ely) in Table 3. Best WMT 2017 28.3 For all experiments, the true-casing model and Table 3: WMT News Translation Task German– the list of BPE operations is left constant. Both English, comparing the quality of different backwere learned from the original parallel training translation systems with different final system arcorpus. chitectures. *Note that the quality for the backtranslation system (Back) is measured in the op4.2 Experiments on Low Resource Scenario posite language direction. NMT is a data-hungry approach, requiring a large amount of parallel data to reach reasonable performance (Koehn and Knowles, 2017). In a lowthe parallel data and the synthetic data generresource setting, only small amount of parallel ated by the base translation system. For better data exist. Previous work has attempted to inperformance, we train a deep model with 8corporate prior or external knowledge to compencheckpoint ensembling; again we use a beam sate for the lack of parallel data, e.g. injecting insize of 2. ductive bias via linguistic constraints (Cohn et al., The final back-translation systems were trained 2016) or linguistic factors (Hoang et al., 2016). using several different systems: a shallow arHowever, it"
W18-2703,W17-4710,0,0.0258173,"ons. Under high-resource conditions, we improve the state of the art with re-back-translation. Under low-resource conditions, we demonstrate Experiments on High Resource Scenario In §3 we demonstrated that the quality of the backtranslation system has significant impact on the effectiveness of the back-translation approach under high-resource data conditions such as WMT 2017 German–English. Here we ask: how much additional benefit can be realised for repeating this process? Also, do the gains for state-of-the-art systems that use deeper models, i.e., more layers in encoder and decoder (Miceli Barone et al., 2017) still apply in this setting? We evaluate on German–English and English– German, under the same data conditions as in Section 3. We experiment with both shallow and deep stacked-layer encoder/decoder architectures. The base translation system is trained on the parallel data only. We train a shallow system using 4-checkpoint ensembling (Chen et al., 2017). The system is used to translate the monolingual data using a beam size of 2. The first back-translation system is trained on 20 German–English Back* Shallow Deep Ensemble back-translation 23.7 32.5 35.0 35.6 re-back-translation 27.9 33.6 36.1"
W18-6102,I13-1041,1,0.733732,"y speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE recognition is low — a common situation with Twitter, because many posts are written colloquially, without capitalization for proper names, and with non-standard syntax (Baldwin et al., 2013, 2015). 2 2.1 Network-based methods Related Work Text-based methods Text-based geolocation methods use text features to estimate geolocation. Unsupervised topic modeling approaches (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al., 2013) are one successful approach in text-based geolocation estimation, although they tend not to scale to larger data sets. It is also possible to use semi-supervised learning over gazetteers (Lieberman et al., 2010; Quercini et al., 2010), whereby gazetted terms are identified and used to construct a distribution over possible locations, and clustering or"
W18-6102,W15-4319,1,0.868415,"Missing"
W18-6102,C12-1064,1,0.851836,"l networks (“R-GCNs”: Schlichtkrull et al. (2017)) are a simple implementation of a graph convolutional network, where a weight matrix is constructed for each channel, and combined via a normalised sum to generate an embedding. Kipf and Welling (2016) adapted graph convolutional networks for text based on a layer-wise propagation rule. Supervised approaches tend to be based on bagof-words modelling of the text, in combination with a machine learning method such as hierarchical logistic regression (Wing and Baldridge, 2014) or a neural network with denoising autoencoder (Liu and Inkpen, 2015). Han et al. (2012) focused on explicitly identifying “location indicative words” using multinomial naive Bayes and logistic regression classifiers combined with feature selection methods, while Rahimi et al. (2015b) extended this work using multi-level regularisation and a multi-layer perceptron architecture (Rahimi et al., 2017b). 3 Methods In this paper, we use the following notation to describe the methods: U is the set of users in the 8 Input entities Input words Semantic relation database e2 e1 R0 R0 e R0 e3 Rk e4 Weight NN weight e Input layer e1, e2 Input layer Weighted sum Channel 2: Relation R0 (out) …"
W18-6102,P17-1116,0,0.709437,"label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE recognition is low — a common situation with Twitter, because many posts are written colloquially, without capitalization"
W18-6102,W16-3928,1,0.822564,"dges in the graph, opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if t"
W18-6102,C16-2055,0,0.0607572,"Missing"
W18-6102,D17-1016,1,0.757551,"opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE"
W18-6102,P15-2104,1,0.958467,"entity (“NE”) tokens, which would only be applicable to NEs attested in the training data. Twitter, as a social media platform, supports a number of different modalities for interacting with other users, such as mentioning another user in the body of a tweet, retweeting the message of another user, or following another user. If we consider the users of the platform as nodes in a graph, these define edges in the graph, opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaki"
W18-6102,P17-2033,1,0.730706,"opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE"
W18-6102,P18-1187,1,0.837067,"pton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE recognition is low — a common situation with Twitter, because many posts are written colloquially, without capitalization for proper names, and with non-standar"
W18-6102,W15-1527,0,0.0300504,"onal graph convolutional networks (“R-GCNs”: Schlichtkrull et al. (2017)) are a simple implementation of a graph convolutional network, where a weight matrix is constructed for each channel, and combined via a normalised sum to generate an embedding. Kipf and Welling (2016) adapted graph convolutional networks for text based on a layer-wise propagation rule. Supervised approaches tend to be based on bagof-words modelling of the text, in combination with a machine learning method such as hierarchical logistic regression (Wing and Baldridge, 2014) or a neural network with denoising autoencoder (Liu and Inkpen, 2015). Han et al. (2012) focused on explicitly identifying “location indicative words” using multinomial naive Bayes and logistic regression classifiers combined with feature selection methods, while Rahimi et al. (2015b) extended this work using multi-level regularisation and a multi-layer perceptron architecture (Rahimi et al., 2017b). 3 Methods In this paper, we use the following notation to describe the methods: U is the set of users in the 8 Input entities Input words Semantic relation database e2 e1 R0 R0 e R0 e3 Rk e4 Weight NN weight e Input layer e1, e2 Input layer Weighted sum Channel 2:"
W18-6102,N15-1153,1,0.95748,"entity (“NE”) tokens, which would only be applicable to NEs attested in the training data. Twitter, as a social media platform, supports a number of different modalities for interacting with other users, such as mentioning another user in the body of a tweet, retweeting the message of another user, or following another user. If we consider the users of the platform as nodes in a graph, these define edges in the graph, opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaki"
W18-6102,W17-4415,0,0.0270451,"Missing"
W18-6102,P13-2005,0,0.0648627,"Missing"
W18-6102,D14-1039,0,0.0886635,"et al., 2015) and quantum structure learning (Gilmer et al., 2017). Relational graph convolutional networks (“R-GCNs”: Schlichtkrull et al. (2017)) are a simple implementation of a graph convolutional network, where a weight matrix is constructed for each channel, and combined via a normalised sum to generate an embedding. Kipf and Welling (2016) adapted graph convolutional networks for text based on a layer-wise propagation rule. Supervised approaches tend to be based on bagof-words modelling of the text, in combination with a machine learning method such as hierarchical logistic regression (Wing and Baldridge, 2014) or a neural network with denoising autoencoder (Liu and Inkpen, 2015). Han et al. (2012) focused on explicitly identifying “location indicative words” using multinomial naive Bayes and logistic regression classifiers combined with feature selection methods, while Rahimi et al. (2015b) extended this work using multi-level regularisation and a multi-layer perceptron architecture (Rahimi et al., 2017b). 3 Methods In this paper, we use the following notation to describe the methods: U is the set of users in the 8 Input entities Input words Semantic relation database e2 e1 R0 R0 e R0 e3 Rk e4 Weig"
W19-5035,D14-1012,0,0.023505,"tion performance of these systems and their ensembles (Habibi et al., 2016). The application of the tmChem model trained on chemical literature corpora of the BioCreative IV CHEMDNER task (Krallinger et al., 2015) and the ChemSpot model trained on a subset of the SCAI corpus (Klinger et al., 2008) resulted in a significant performance drop over chemical patent corpora. Zhang et al. (2016) compared the performance of CRF- and Support Vector Machine (SVM)based models on the CHEMDNER-patents corpus (Krallinger et al., 2015). The features constructed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to enc"
W19-5035,J92-4003,0,0.261664,"their ensembles (Habibi et al., 2016). The application of the tmChem model trained on chemical literature corpora of the BioCreative IV CHEMDNER task (Krallinger et al., 2015) and the ChemSpot model trained on a subset of the SCAI corpus (Klinger et al., 2008) resulted in a significant performance drop over chemical patent corpora. Zhang et al. (2016) compared the performance of CRF- and Support Vector Machine (SVM)based models on the CHEMDNER-patents corpus (Krallinger et al., 2015). The features constructed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et"
W19-5035,N19-1149,0,0.0159323,"extended version as EBC-CRF as illustrated in Figure 1. In particular, for EBC-CRF, we use a concatenation of pretrained word embeddings, CNN-based characterlevel word embeddings and ELMo-based contextualized word embeddings as the input of a BiLSTM encoder. The BiLSTM encoder learns a latent feature vector for each word in the input. Then each latent feature vector is linearly transformed before being fed into a linear-chain CRF layer (Lafferty et al., 2001) for NER tag prediction. We assume binary potential between tags and unary potential between tags and words. Pre-trained word embeddings Dai et al. (2019) showed that NER performance is significantly affected by the overlap between pretrained word embedding vocabulary and the target NER data. Therefore, we explore the effects of different sets of pre-trained word embeddings on the NER performance. We use 200-dimensional pre-trained PubMedPMC and Wiki-PubMed-PMC word embeddings (Pyysalo et al., 2013), which are widely used for NLP tasks in biomedical domain. Both the PubMed-PMC and Wiki-PubMed-PMC embeddings word embeddings were generated by training the Word2Vec skip-gram model (Mikolov et al., 2013) on a collection of PubMed abstracts and PubM"
W19-5035,N19-1423,0,0.0142134,"Hence, we fix the hyper-parameters shown in Table 2 to the suggested values in our experiments, which means that only models with 2stacked LSTM of size 250 are evaluated. In this study, we also consider the choice of tokenizer and word embedding source as hyperparameters. To compare the performance of different tokenizers, we tokenize the same split of datasets with different tokenizers and evaluate the overall F1 score over development set. After the best tokenizer for pre-processing patent corpus is determined, we use datasets tokenized by the best ELMo ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) can be used to generate contextualized word representations by combining internal states of different layers in neural language models. Contextualized word representation can help to improve performance in various NLP tasks by incorporating contextual information, essentially allowing for the same word to have distinct context-dependent meanings. This could be particularly powerful for chemical NER since generic chemical names (e.g. salts, acid) may have different meanings in other domains. We therefore explore the impact of using contextualized word representations for chemical patents. We t"
W19-5035,N16-1030,0,0.775717,"l., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et al. (2016) 329 CRF-based models (Section 3.3) with pre-trained word embeddings (Section 3.4), character-level word embeddings (Section 3.5), contextualized word embeddings (Section 3.6) and implementation details (Section 3.7). developed a LSTM-based approach for encoding character level information. Habibi et al. (2017) presented an empirical study comparing three NER models on a large collection of biomedical corpora including the BioSemantics patent corpus: (1) tmChem–the CRFbased model with hand-crafted features–used as the baseline; (2) a second CRF model based on CRFSuite (Okazaki, 2007) using pre"
W19-5035,D17-1035,0,0.0122379,"sets. We do not update weights for word embeddings if pre-trained word embeddings were used. Character-level representation The BiLSTM-CRF model with character-level word representations (Lample et al., 2016; Ma and Hovy, 2016) has been shown to have state-of-theart performance in NER tasks on chemical patent datasets (Habibi et al., 2017). It has been shown that the choice of using LSTM-based or CNNbased character-level word representation has little effect on final NER performance in both general and biomedical domain while the CNN-based approach has the advantage of reduced training time (Reimers and Gurevych, 2017b; Zhai et al., 2018). Hence, we use the CNN-based approach with the same hyper-parameter settings of Reimers and Gurevych (2017b) for capturing characterlevel information (see Table 2 for details). 3.6 Hyper-para. charEmbedSize filter length # of filters output size (a) BiLSTM-CRF Table 1: Statistics of the unannotated patent corpus used for training ChemPatent embeddings and ELMo. 3.5 Value Adam 0.001 16 1 [0.25, 0.25] 3.7 Implementation details Our NER model implementation is based on the AllenNLP system (Gardner et al., 2017). We learn model parameters using the training set, and we use th"
W19-5035,P16-1101,0,0.294108,"ed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et al. (2016) 329 CRF-based models (Section 3.3) with pre-trained word embeddings (Section 3.4), character-level word embeddings (Section 3.5), contextualized word embeddings (Section 3.6) and implementation details (Section 3.7). developed a LSTM-based approach for encoding character level information. Habibi et al. (2017) presented an empirical study comparing three NER models on a large collection of biomedical corpora including the BioSemantics patent corpus: (1) tmChem–the CRFbased model with hand-crafte"
W19-5035,D14-1162,0,0.0868215,"3) were also explored. The results showed that the BiLSTMCRF model with the combination of domainspecific pre-trained word embedding and LSTMbased character-level word embeddings outperformed the two CRF-based models on chemical NER tasks in both chemical literature and chemical patent corpora. However, this work used only a general tokenizer (i.e. OpenNLP) and word embeddings pre-trained on biomedical corpora. Corbett and Boyle (2018) presented word-level and character-level BiLSTM networks for chemical NER in literature domain. The word-level model employed word embeddings learned by GloVe (Pennington et al., 2014) on a corpus of patent titles and abstracts. The character-level model used two different transfer learning approaches to pre-train its character-level encoder. The first approach attempts to predict neighbor characters at each time step, while the other tries to predict whether a given character sequence is an entry in the chemical database ChEBI (Degtyarenko et al., 2007). Experimental results show that the character-level model can produce better NER performance than word-level model by leveraging transfer learning. In addition, for the wordlevel model, using pre-trained word embeddings lea"
W19-5035,N18-1202,0,0.19692,"ddings are fixed during training of the NER models. For a more concrete comparison, a set of 200-dimensional trainable word embeddings initialized from normal distribution is used as a baseline. The 200-dimensional baseline word embeddings contain all words in the vocabulary of the dataset and are initialized from a normal distribution, the baseline word embeddings are learned during training process. The vocabulary of models Models We use the BiLSTM-CNN-CRF model (Ma and Hovy, 2016) as our baseline. We extend the baseline by adding the contextualized word representations generated from ELMo (Peters et al., 2018). 1 NBIC UMLSGeneChemTokenizer is developed by the Netherlands Bioinformatics Center, available at https:// trac.nbic.nl/data-mining/wiki. 331 Patent Office AU CA EP GB IN US WO Total Document 7,743 1,962 19,274 918 1,913 41,131 11,135 84,076 Sentence 4,662,375 463,123 3,478,258 182,627 261,260 19,800,123 4,830,708 33,687,474 Hyper-para. Optimizer Learning rate Mini-batch size Clip Norm(L2) Dropout Tokens 156,137,670 16,109,776 117,992,191 6,038,837 9,015,238 628,256,609 159,286,325 1,092,836,646 Value 50 3 30 30 (b) CNN-char Table 2: Fixed hyper-parameter configurations. patents (detailed in"
W19-5035,W18-5605,1,0.80271,"ts for word embeddings if pre-trained word embeddings were used. Character-level representation The BiLSTM-CRF model with character-level word representations (Lample et al., 2016; Ma and Hovy, 2016) has been shown to have state-of-theart performance in NER tasks on chemical patent datasets (Habibi et al., 2017). It has been shown that the choice of using LSTM-based or CNNbased character-level word representation has little effect on final NER performance in both general and biomedical domain while the CNN-based approach has the advantage of reduced training time (Reimers and Gurevych, 2017b; Zhai et al., 2018). Hence, we use the CNN-based approach with the same hyper-parameter settings of Reimers and Gurevych (2017b) for capturing characterlevel information (see Table 2 for details). 3.6 Hyper-para. charEmbedSize filter length # of filters output size (a) BiLSTM-CRF Table 1: Statistics of the unannotated patent corpus used for training ChemPatent embeddings and ELMo. 3.5 Value Adam 0.001 16 1 [0.25, 0.25] 3.7 Implementation details Our NER model implementation is based on the AllenNLP system (Gardner et al., 2017). We learn model parameters using the training set, and we use the overall F1 score ov"
W19-5035,W18-2501,0,\N,Missing
