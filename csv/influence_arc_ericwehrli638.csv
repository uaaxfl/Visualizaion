1996.amta-1.29,E93-1070,1,0.835315,"Missing"
1996.amta-1.29,C92-3129,1,0.715578,"TSVox is a prototype system under development at LATL which addresses capabilities (i)(iii) above. Specifically, ITSVox is a rnultilingual/multimodal processing system which combines speech synthesis and interactive translation technologies to produce a multilingual text-to-speech system, that is a system able to ""read"" a document either in its original language or in another language. The translation engine is based on the ITS-2 interactive model (cf. Ramluckun & Wehrli, 1993, Wehrli, 1994). As for the speech output components, they are based on our GBparsers (cf. Laenzlinger & Wehrli, 1991, Wehrli, 1992) for the linguistic aspects, along with a prosodic component developed by the LAIP-University of Lausanne. The actual signal generation is provided by the MBRPsola system for the French synthesizer and by the DecTalk system for English. ITSVox runs under MSWindows and can handle MSWord documents. On-going developments include (i) a WWW version, which will be able to process HTML documents, (ii) the addition of German, first as an additional source language and later as a target language with speech output, and (iii) the addition of a speech input component (to be developed in collaboration wit"
2003.mtsummit-papers.26,J94-4003,0,0.548835,"ations and contextual usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problem. 1 Introduction With progress in natural language processing techniques (NLP), increasingly sophisticated models and methods have been proposed in the machine translation (MT) research. New techniques distinguish the minute differences between similar words (Edmonds and Hirst, 2002) or take into account collocations (Edmonds, 1997), idioms (Wehrli, 1998), or contextually related words (Dagan and Itai, 1994; Lin and Pantel, 2002), etc. This kind of approach depends to varying extents on adequate references. For the fine-grained lexical knowledge model (FLK) (Edmonds and Hirst, 2002), having adequate references is indispensable, or the model will not work in practical applications. However, such detailed references are limited in number, and manual lexicographic coding is too time-consuming to continuously update new information. Other problems with the classical lexicographic organization have been pointed out, such as its inability to represent the semantic distance between defined senses and i"
2003.mtsummit-papers.26,C94-2113,0,0.0390076,"adequate references. For the fine-grained lexical knowledge model (FLK) (Edmonds and Hirst, 2002), having adequate references is indispensable, or the model will not work in practical applications. However, such detailed references are limited in number, and manual lexicographic coding is too time-consuming to continuously update new information. Other problems with the classical lexicographic organization have been pointed out, such as its inability to represent the semantic distance between defined senses and its failure to properly organize the senses, and alternatives have been proposed (Dolan, 1994; Budanitsky and Hirst, 2001; Fellbaum, 1998; Manning, 1993; Ploux, 1997; Pustejovsky and Boguraev, 1994). In addition, subtle lexical knowledge is too vague and too broad to handle. For instance, relations like ∗ Institut des Sciences Cognitives, UMR 5015 CNRS, 67, boulevard Pinel, 69675 BRON cedex France † Institut National Polytechnique de Grenoble ‡ Laboratoire d’Analyse et de Technologie du Langage stagger - drunken, which could be informative for non-English speakers or machines, are too numerous to be processed. Intra-word relations share this problem: while the English word write is co"
2003.mtsummit-papers.26,P99-1005,0,0.0144836,"rench words le final, and match into allumette (wooden lighter), whereas the correct translations are la finale and match, respectively. Trained on five years of the French newspapers Le Monde and L’Humanit´e, our model produces the contexonyms finale and match for the target words Agassi, champions and victoires, and finale only for Rafter and Sampras. This clearly points to the correct target-language words. Yet, two problems remain unsolved: first, unlike the target-language selection phase, no disambiguation is performed for the source language; second, potential data sparseness problems (Lee and Pereira, 1999) are not covered by this direct approach. The first problem involves assigning the meaning of the word match in the concerned paragraph to a proper cluster. But as Figure 2 shows, there are no contexonyms shared with the text in question. We present the contexonym merging method as a remedy to these problems. This consists in inputting more than one target word to get the contexonyms. To compensate for the global frequency effect, a normalizing merging method is used. Table 5 shows the gradual exclusion of less-closely-related contexonyms like cigarette and marriage, and the gradual inclusion"
2003.mtsummit-papers.26,C02-1144,0,0.0145194,"usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problem. 1 Introduction With progress in natural language processing techniques (NLP), increasingly sophisticated models and methods have been proposed in the machine translation (MT) research. New techniques distinguish the minute differences between similar words (Edmonds and Hirst, 2002) or take into account collocations (Edmonds, 1997), idioms (Wehrli, 1998), or contextually related words (Dagan and Itai, 1994; Lin and Pantel, 2002), etc. This kind of approach depends to varying extents on adequate references. For the fine-grained lexical knowledge model (FLK) (Edmonds and Hirst, 2002), having adequate references is indispensable, or the model will not work in practical applications. However, such detailed references are limited in number, and manual lexicographic coding is too time-consuming to continuously update new information. Other problems with the classical lexicographic organization have been pointed out, such as its inability to represent the semantic distance between defined senses and its failure to properly"
2003.mtsummit-papers.26,J03-2001,1,0.877113,", in the sense that it considers co-occurrences of words. The main distinction between the model presented here and other statistical ones is that it generates the minimal senses of words (cliques) in order to organize the related words obtained. Cliques are then represented on the principal plane. This makes it possible to represent several target words in MT tasks. Ploux et al. proposed the prototype of a model that represents synonym senses from a non-senseclassified synonym database (Ploux, 1997; Ploux and Victorri, 1998) and a two-language synonymmatching model based on a mapping method (Ploux and Ji, 2003). The main difference between the present model and the previous one is that the present model is fully automated, insofar as it does not need any kind of hand-coded references, only raw text sources. Furthermore, different sets of cliques can be obtained according to chosen criteria. This will be explained later. 2 Contexonym We define contexonyms as relevant contextually related words for a target word. By context, we mean a certain number of neighboring words of the target word (from a small-sized window to one or more paragraphs). Unlike synonyms or antonyms, contexonyms are not symmetric"
2003.mtsummit-papers.26,J98-1004,0,0.111319,"Missing"
2003.mtsummit-papers.26,J02-2001,0,0.194917,"ntic space. Trained on very large corpora, the model provides relevant, organized contexonyms that reflect the fine-grained connotations and contextual usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problem. 1 Introduction With progress in natural language processing techniques (NLP), increasingly sophisticated models and methods have been proposed in the machine translation (MT) research. New techniques distinguish the minute differences between similar words (Edmonds and Hirst, 2002) or take into account collocations (Edmonds, 1997), idioms (Wehrli, 1998), or contextually related words (Dagan and Itai, 1994; Lin and Pantel, 2002), etc. This kind of approach depends to varying extents on adequate references. For the fine-grained lexical knowledge model (FLK) (Edmonds and Hirst, 2002), having adequate references is indispensable, or the model will not work in practical applications. However, such detailed references are limited in number, and manual lexicographic coding is too time-consuming to continuously update new information. Other problems with the classical lexicogra"
2003.mtsummit-papers.26,P98-2226,1,0.927232,"ntexonyms that reflect the fine-grained connotations and contextual usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problem. 1 Introduction With progress in natural language processing techniques (NLP), increasingly sophisticated models and methods have been proposed in the machine translation (MT) research. New techniques distinguish the minute differences between similar words (Edmonds and Hirst, 2002) or take into account collocations (Edmonds, 1997), idioms (Wehrli, 1998), or contextually related words (Dagan and Itai, 1994; Lin and Pantel, 2002), etc. This kind of approach depends to varying extents on adequate references. For the fine-grained lexical knowledge model (FLK) (Edmonds and Hirst, 2002), having adequate references is indispensable, or the model will not work in practical applications. However, such detailed references are limited in number, and manual lexicographic coding is too time-consuming to continuously update new information. Other problems with the classical lexicographic organization have been pointed out, such as its inability to represe"
2003.mtsummit-papers.26,P97-1067,0,0.0299406,"s relevant, organized contexonyms that reflect the fine-grained connotations and contextual usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problem. 1 Introduction With progress in natural language processing techniques (NLP), increasingly sophisticated models and methods have been proposed in the machine translation (MT) research. New techniques distinguish the minute differences between similar words (Edmonds and Hirst, 2002) or take into account collocations (Edmonds, 1997), idioms (Wehrli, 1998), or contextually related words (Dagan and Itai, 1994; Lin and Pantel, 2002), etc. This kind of approach depends to varying extents on adequate references. For the fine-grained lexical knowledge model (FLK) (Edmonds and Hirst, 2002), having adequate references is indispensable, or the model will not work in practical applications. However, such detailed references are limited in number, and manual lexicographic coding is too time-consuming to continuously update new information. Other problems with the classical lexicographic organization have been pointed out, such as i"
2003.mtsummit-papers.26,P95-1026,0,0.602504,"r a given word, which could serve as a reference. Clearly, contextually related words are meaningful indicators of the target word’s semantic value in a given context. For instance, two sets of words { lit, candle, cigarette } and { tennis, final, win } are trustworthy cue-word sets for disambiguating the word match; stupid is more closely related to blunder than to error (Edmonds and Hirst, 2002), and peace distinguishes treaty from contract (Dagan and Itai, 1994). Such word lists may be obtained for target words by selecting seed words and performing an iterative, decision-list-making task (Yarowsky, 1995), or by latent semantic indexing (LSI) (Landauer et al., 1998). A common limitation of these approaches, however, is that they do not provide a fully automatic method for organizing the related words obtained: identifying seed words needs human intervention and LSI does not provide an automatic classification other than a restricted matching-based one that requires an encyclopedia as a source text (Laham, 1997). A fully automated sense-discrimination method based on a second-order comparison in semantic space has been proposed (Sch¨utze, 1998). Because this approach focuses on comparing vector"
2003.mtsummit-papers.26,P93-1032,0,\N,Missing
2003.mtsummit-papers.26,C98-2221,1,\N,Missing
2004.jeptalnrecital-long.14,P98-2226,1,0.87996,"Missing"
2004.jeptalnrecital-long.14,2003.mtsummit-systems.17,1,0.77716,"Missing"
2007.jeptalnrecital-long.37,J93-1003,0,0.0484621,"Missing"
2007.jeptalnrecital-long.37,E89-1018,0,0.294734,"Missing"
2007.jeptalnrecital-long.37,2005.mtsummit-papers.11,0,0.0209246,"Missing"
2007.jeptalnrecital-long.37,P93-1003,0,0.624199,"Missing"
2007.jeptalnrecital-long.37,E03-1022,1,0.865693,"Missing"
2007.jeptalnrecital-long.37,2003.mtsummit-papers.39,0,0.126817,"Missing"
2007.jeptalnrecital-long.37,W06-1006,1,0.89173,"Missing"
2007.jeptalnrecital-long.37,J93-1007,0,0.39627,"Missing"
2007.jeptalnrecital-long.37,J96-1001,0,0.10566,"Missing"
2007.jeptalnrecital-long.37,E93-1015,0,0.122964,"Missing"
2007.jeptalnrecital-long.37,P06-4016,1,0.88188,"Missing"
2007.jeptalnrecital-long.37,P03-1016,0,0.0425054,"Missing"
2007.jeptalnrecital-long.37,E03-1083,1,\N,Missing
2007.jeptalnrecital-long.37,A94-1006,0,\N,Missing
2007.jeptalnrecital-long.37,P04-1022,0,\N,Missing
2008.jeptalnrecital-court.18,2001.mtsummit-papers.18,0,0.0132226,"’avancée spectaculaire des systèmes de traduction stochastiques (cf. Ney, 2005), de nombreux chercheurs dans le domaine de la traduction automatique considèrent qu’une variante du modèle de transfert basé sur des règles linguistiques constitue encore pour l’heure l’approche la plus satisfaisante au problème de la traduction. Il y a par contre un large désaccord sur la question du choix du modèle linguistique, sur la nature des représentations qui serviront à exprimer les correspondances d’une langue à l’autre, ainsi que sur le niveau d’abstraction auquel le transfert devrait s’effectuer (voir Eberle, 2001). Un des problèmes que nous souhaitons aborder dans ce projet est celui de la traduction multilingue, c’est-à-dire du développement d’un système de traduction capable de traiter non pas quelques paires de langues, mais potentiellement un large nombre de langues. Rappelons, pour illustrer ce point, que dans le contexte européen pas moins d’une quinzaine de langues sont couramment utilisées dans la vie quotidienne mais aussi pour des activités commerciales ou politiques. Et comme le signale Boitet (2001), “Despite considerable investment over the past 50 Eric Wehrli, Luka Nerima years, only a sm"
2008.jeptalnrecital-court.18,2005.mtsummit-papers.11,0,0.0322527,"Missing"
2008.jeptalnrecital-court.18,2005.mtsummit-invited.5,0,0.0239278,"eral languages. The LATL has developed an efficient multilingual parsing technology based on an abstract and generic linguistic model and on object-oriented software design. The proposed project intends to apply a similar approach to the problem of multilingual translation (German, French, Italian and English). Mots-clés : Traduction automatique multilingue, approche par objets, génération de lexiques bilingues. Keywords: Multilingual machine translation, object design, bilingual dictionary derivation. 1 Introduction Malgré l’avancée spectaculaire des systèmes de traduction stochastiques (cf. Ney, 2005), de nombreux chercheurs dans le domaine de la traduction automatique considèrent qu’une variante du modèle de transfert basé sur des règles linguistiques constitue encore pour l’heure l’approche la plus satisfaisante au problème de la traduction. Il y a par contre un large désaccord sur la question du choix du modèle linguistique, sur la nature des représentations qui serviront à exprimer les correspondances d’une langue à l’autre, ainsi que sur le niveau d’abstraction auquel le transfert devrait s’effectuer (voir Eberle, 2001). Un des problèmes que nous souhaitons aborder dans ce projet est"
2008.jeptalnrecital-court.18,W04-2204,0,0.0316832,"Missing"
2008.jeptalnrecital-court.18,P98-2226,1,0.825091,"Missing"
2008.jeptalnrecital-court.18,W07-1216,1,0.89961,"ed by MT (...), and even fewer are capable of quality translation or speech translation.” Le projet MulTra repose sur un niveau de représentation linguistique abstrait, inspiré des travaux récents en linguistique générative (cf. Chomsky, 1995, Culicover et Jackendoff, 2005, Bresnan 2001), suffisamment riche pour exprimer toutes la diversité des langues prises en considération, mais suffisamment abstrait pour permettre de rendre compte de généralisations profondes qui se cachent parfois derrière des différences de surface. Une telle approche a été adoptée avec succès pour l’analyseur Fips (cf. Wehrli, 2007). Au plan informatique, c’est une modélisation “objets"" qui a été retenue, très semblable à celle définie pour l’analyseur multilingue Fips. La rapide prolifération des modules de transfert a souvent été reprochée au modèle de traduction basé sur le transfert syntaxique comme modèle pour la traduction multilingue (voir par exemple Arnold et al. 1995 :81). L’argument repose sur le fait que le nombre de modules de transfert croît de manière quadratique en fonction du nombre n de langues. Cet argument est cependant considérablement affaibli si l’on peut montrer que les modules de transfert sont r"
2009.eamt-1.18,2007.jeptalnrecital-long.26,0,0.045123,"Missing"
2009.eamt-1.18,J90-1003,0,0.212226,"dy Evaluation of Their Translation Adequacy Eric Wehrli, Violeta Seretan, Luka Nerima, Lorenza Russo Language Technology Laboratory University of Geneva Switzerland {firstname.lastname}@unige.ch Abstract [to] break a record) constitute a well-known problem for machine translation. Their identification in the source text and their proper processing by MT systems is the key factor in producing a more acceptable output (Orliac and Dillinger, 2003). Over the past two decades, intensive efforts have been made to devise accurate techniques for collocation extraction from corpora; see, for instance, Church and Hanks (1990), Smadja (1993), Lin (1998), Evert (2004), among many others. Yet, the existing MT systems generally do not integrate collocational resources, or they are not designed to handle collocations in a specific and appropriate manner, as required by their high morpho-syntactic potential. Therefore, such systems often achieve an unsatisfactory literal translation, especially when the collocation items are not found in the canonical order or in close proximity.1 For instance, Example (1b) show the French translation returned by a major MT system, freely available online, for the English sentence in (1"
2009.eamt-1.18,J93-1003,0,0.0321393,"ction of collocations from text corpora is done by using a hybrid extraction method, which combines syntactic information provided by our parser with existing statistical methods for detecting typical lexical associations in corpora. Thus, collocation candidates are first identified from each sentence based on the parse structures returned by the parser, as lexeme combinations in a given syntactic configuration (for instance, verb-object). Then, these candidates are ranked according to their probability to constitute collocations, as computed with the log-likelihood ratio association measure (Dunning, 1993). The tool also implements a wide range of other measures that the user can choose for ranking collocation candidates. The method implemented is similar, in principle, to other hybrid methods that were lately applied for collocation extraction (Lin, 1998; Krenn and Evert, 2001; Orliac and Dillinger, 2003; Kilgarriff et al., 2004; Charest et al., 2007). By selecting candidate collocations as pair of lexemes in a given syntactic relation (such as head-modifier or predicate-argument), these methods are much more appropriate for handling flexible collocations than the standard syntactically-uninfo"
2009.eamt-1.18,2005.mtsummit-papers.11,0,0.00557317,"esentation, to which grammatical transformations (e.g., passivization and other potential extraposition transformations) and morphological generation will apply to create the target sentence. Unless restrictions have been specified in the lexical database, collocations will undergo the exact same grammatical and morphological processes as other lexical items. 5 5.1 comparison. The target language considered was French. The test set contains 200 collocation instances, half in English, half in Italian, that were attested in the English, and, respectively, Italian version of the Europarl corpus (Koehn, 2005). The test set was built as follows. First, a number of 10 collocations of type verb-object has been selected in each source language, from among the results of our previous collocation extraction experiments. Their choice was motivated by the nonliteral translation into French, the (supposed) high morpho-syntactic modification potential, and the sufficient occurrence in the corpus. The selected types are displayed in Table 1 (first column); the second column shows an adequate translation into French. Collocation (English, Italian) bridge gap draw distinction foot bill give support hold presid"
2009.eamt-1.18,2003.mtsummit-papers.39,0,0.268415,"sentence based on the parse structures returned by the parser, as lexeme combinations in a given syntactic configuration (for instance, verb-object). Then, these candidates are ranked according to their probability to constitute collocations, as computed with the log-likelihood ratio association measure (Dunning, 1993). The tool also implements a wide range of other measures that the user can choose for ranking collocation candidates. The method implemented is similar, in principle, to other hybrid methods that were lately applied for collocation extraction (Lin, 1998; Krenn and Evert, 2001; Orliac and Dillinger, 2003; Kilgarriff et al., 2004; Charest et al., 2007). By selecting candidate collocations as pair of lexemes in a given syntactic relation (such as head-modifier or predicate-argument), these methods are much more appropriate for handling flexible collocations than the standard syntactically-uninformed methods, which rely on the linear proximity of words. Unlike the methods cited above, in our system the syntactic relations identified are “deeper” since the underlying parsing mechanism is more 4 We believe that the insertion of new collocations in the lexicon cannot be done in a fully automatic wa"
2009.eamt-1.18,J93-1007,0,0.356989,"anslation Adequacy Eric Wehrli, Violeta Seretan, Luka Nerima, Lorenza Russo Language Technology Laboratory University of Geneva Switzerland {firstname.lastname}@unige.ch Abstract [to] break a record) constitute a well-known problem for machine translation. Their identification in the source text and their proper processing by MT systems is the key factor in producing a more acceptable output (Orliac and Dillinger, 2003). Over the past two decades, intensive efforts have been made to devise accurate techniques for collocation extraction from corpora; see, for instance, Church and Hanks (1990), Smadja (1993), Lin (1998), Evert (2004), among many others. Yet, the existing MT systems generally do not integrate collocational resources, or they are not designed to handle collocations in a specific and appropriate manner, as required by their high morpho-syntactic potential. Therefore, such systems often achieve an unsatisfactory literal translation, especially when the collocation items are not found in the canonical order or in close proximity.1 For instance, Example (1b) show the French translation returned by a major MT system, freely available online, for the English sentence in (1a), where the o"
2009.eamt-1.18,W09-0415,1,0.760945,"n isolation may be polysemous (e.g., break in break - record). If the recognition of a collocation fails, the sense disambiguation information it carries is no longer available. This means that (even though a literal translation of collocations could in principle often result in an understandable if not fully adequate translation) the risk of choosing a wrong target word is rather high, making the literal translation option rather risky. 3 3.1 Our translation system Overview ITS-2 is a large-scale translation system developed in our laboratory, LATL, in the last couple of years (Wehrli, 1998; Wehrli et al., 2009). The language pairs currently supported are: English, German, Italian and Spanish to French, French-German, and French-English. 129 ITS-2 relies on an abstract linguistic level of representation, largely inspired from recent work in generative grammar (Chomsky, 1995; Bresnan, 2001; Culicover and Jackendoff, 2005). This level of representation is both rich enough to express the structural diversity of all the languages taken into account, and abstract enough to capture the generalizations hidden behind obvious surface diversity. At the software level, an object-oriented design has been used, s"
2009.eamt-1.18,P98-2226,1,0.720774,"words, which in isolation may be polysemous (e.g., break in break - record). If the recognition of a collocation fails, the sense disambiguation information it carries is no longer available. This means that (even though a literal translation of collocations could in principle often result in an understandable if not fully adequate translation) the risk of choosing a wrong target word is rather high, making the literal translation option rather risky. 3 3.1 Our translation system Overview ITS-2 is a large-scale translation system developed in our laboratory, LATL, in the last couple of years (Wehrli, 1998; Wehrli et al., 2009). The language pairs currently supported are: English, German, Italian and Spanish to French, French-German, and French-English. 129 ITS-2 relies on an abstract linguistic level of representation, largely inspired from recent work in generative grammar (Chomsky, 1995; Bresnan, 2001; Culicover and Jackendoff, 2005). This level of representation is both rich enough to express the structural diversity of all the languages taken into account, and abstract enough to capture the generalizations hidden behind obvious surface diversity. At the software level, an object-oriented d"
2009.eamt-1.18,W07-1216,1,0.833512,"to French, French-German, and French-English. 129 ITS-2 relies on an abstract linguistic level of representation, largely inspired from recent work in generative grammar (Chomsky, 1995; Bresnan, 2001; Culicover and Jackendoff, 2005). This level of representation is both rich enough to express the structural diversity of all the languages taken into account, and abstract enough to capture the generalizations hidden behind obvious surface diversity. At the software level, an object-oriented design has been used, similar to the design adopted for the Fips multilingual parser on which it relies (Wehrli, 2007). To a large extent, ITS-2 can be viewed as an extension of the parser. It relies heavily on the detailed linguistic analysis provided by the parser for the supported languages, and exploits the lexical information of its monolingual lexicons. Both systems aim to set up a generic module which can be further refined to suit the specific needs of, respectively, a particular language or a particular language-pair. The translation algorithm follows the traditional pattern of a transfer system. First, the input sentence is parsed by the parser, producing an information-rich phrase-structure represe"
2009.eamt-1.18,H93-1052,0,0.162316,"ssue in translation and motivates our particular interest in that matter. In the remainder of the discussion we will restrict our attention to collocations of the verb-object type. This is one of the most common types of collocations, along with the adjective-noun type. At the same time, it is arguably the type that is the hardest to identify, due to the high frequency of extraposition of the object (as will be discussed in Section 4). The non-identification of collocations dramatically affects the quality of the output. Collocations, which are in their vast majority semantically unambiguous (Yarowsky, 1993), are typically made of very common words, which in isolation may be polysemous (e.g., break in break - record). If the recognition of a collocation fails, the sense disambiguation information it carries is no longer available. This means that (even though a literal translation of collocations could in principle often result in an understandable if not fully adequate translation) the risk of choosing a wrong target word is rather high, making the literal translation option rather risky. 3 3.1 Our translation system Overview ITS-2 is a large-scale translation system developed in our laboratory,"
2009.eamt-1.18,C98-2221,1,\N,Missing
2011.jeptalnrecital-court.42,J94-4002,0,0.102597,"Missing"
2011.jeptalnrecital-court.42,2006.jeptalnrecital-poster.18,0,0.0984025,"Missing"
2011.jeptalnrecital-court.43,2010.iwslt-papers.10,0,0.0430639,"Missing"
2011.jeptalnrecital-court.43,J94-4002,0,0.259549,"Missing"
2011.jeptalnrecital-court.43,W07-1216,1,0.779922,"Missing"
2011.jeptalnrecital-court.44,D08-1078,0,0.0589666,"Missing"
2011.jeptalnrecital-court.44,2010.iwslt-papers.10,0,0.0344141,"Missing"
2011.jeptalnrecital-court.44,P86-1040,0,0.266575,"Missing"
2011.jeptalnrecital-court.44,J94-4002,0,0.0968705,"Missing"
2011.jeptalnrecital-court.44,W10-1737,0,0.0535835,"Missing"
2011.jeptalnrecital-court.44,W07-1216,1,0.808368,"Missing"
2011.jeptalnrecital-court.44,W09-0415,1,0.868101,"Missing"
2013.mtsummit-wmwumttt.3,J95-2003,0,0.272516,"Missing"
2013.mtsummit-wmwumttt.3,J86-3001,0,0.0841775,"Missing"
2013.mtsummit-wmwumttt.3,J01-4007,0,0.0360921,"n personal pronouns, the proUn tel argument ne serait pas du tout difficile cedure selects all the noun phrases stored on the a` pr´esenter. stack which agree in person, number and gender In our second example (9), the collocation make with the pronoun. If more than one is selected, preference goes first to the subject arguments and a case occurs twice (making this case, makes it). second non subject arguments, a heuristic inspired Notice that in the second occurrence, the base term in part by the Centering theory (cf. Grosz et al., of the collocation has been pronominalized, with 1986, 1995; Kibble, 2001). Needless to say, the its antecedent in the previous sentence. Thanks to procedure sketched above is merely a first attempt the AR procedure, Its-2 correctly identifies the collocation and translates it appropriately (9c), which at tackling the AR problem. is not the case for Google-translate (9b). 3 Results and final remarks (9)a. Every Democrat is making this case. But Mr Edwards makes it much more stylishly than Mr Kerry. The examples discussed above are all simple sentences constructed for the purpose of the present research. Let us now turn to “real” sentences taken respectively, from th"
2013.mtsummit-wmwumttt.3,J94-4002,0,0.766517,"Missing"
2013.mtsummit-wmwumttt.3,W10-3705,1,0.890755,"Missing"
2013.mtsummit-wmwumttt.3,W07-1216,1,0.768045,", noun-preposition-noun, and the like. With respect to collocations which display a certain amount of syntactic flexibility and in which the two constituents can be arbitrarily far away from each other, commercial MT systems do relatively poorly, as illustrated in the few examples given at the end of the next section. 2.1 Translating collocations with Its-2 In this section, we describe how collocations are handled in the Its-2 translation system (cf. Wehrli (4)a. The record that Paul set is likely to be broken. et al. 2009a, 2009b), which is based on the Fips b. Its-2 multilingual parser (cf. Wehrli, 2007). The proLe record que Paul a e´ tabli est susceptible posed treatment relies on the assumption that cold’ˆetre battu. locations are “pervasive” in NL (cf. Jackendoff, 1997; Mel’cuk, 2003), which calls for a “light” and c. Google translate efficient treatment – perhaps in contrast to true idL’enregistrement qui Paul ensemble est susiomatic expressions, which are far less numerous ceptible d’ˆetre rompu. and may require and justify a much heavier treatment1 . d. Systran Let us first consider again example (2), which Le disque que l’ensemble de Paul est suscepinvolves a verb-object collocation,"
2013.mtsummit-wmwumttt.3,W09-0415,1,0.904506,"Missing"
2013.mtsummit-wmwumttt.3,2009.eamt-1.18,1,0.887649,"Missing"
2016.jeptalnrecital-demo.14,J93-1003,0,0.247723,"Missing"
C16-2019,P16-1231,0,0.0263933,"Missing"
C16-2019,Q13-1034,0,0.0212259,"Missing"
C16-2019,D14-1082,0,0.0124588,"mmar-based constituency parser using both attachment rules (to build phrase-structure representations) and specific procedures This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 The parsing quality is not identical for all languages. The best results are achieved with English and French, then German, Spanish, Italian, then Portuguese and Greek, and finally Romanian. 2 For instance, the Stanford parser (Klein & Manning, 2003; Chen & Manning, 2014), the MaltParser (Nivre et al. 2007), TreeTagger (Schmidt, 1995), Mate Tools (Bohnet et al., 2013), SyntaxNet (Andor et al, 2016), Marmot (Mueller et al, 2013). 3 The Sketch engine (Kilgarriff et al., 2014), mwetoolkit (Ramisch, 2015). 89 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 89–92, Osaka, Japan, December 11-17 2016. to compute properties such as long-distance dependencies, argument-structure building, coordination structures, and so on. It uses an information-rich lexical database containing inflected words, le"
C16-2019,D13-1032,0,0.0532091,"Missing"
C16-2019,petrov-etal-2012-universal,0,0.0224736,"Missing"
C16-2019,W07-1216,1,0.768863,"the scripts described below is quite easy. Furthermore, by default, the parser handles collocations and other MWEs, as well as anaphora resolution (limited to 3rd person personal pronouns). When used in the tagger mode, it can be set to display grammatical functions and collocations (see below for details). The following sections give a short description of the Fips parser, which is at the core of all the tools, some specific details and descriptions of the parser/tagger tool, and finally a description of the collocation extraction tool. 2 The Fips parser/tagger The Fips multilingual parser (Wehrli, 2007; Wehrli & Nerima, 2015) is a grammar-based constituency parser using both attachment rules (to build phrase-structure representations) and specific procedures This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 The parsing quality is not identical for all languages. The best results are achieved with English and French, then German, Spanish, Italian, then Portuguese and Greek, and finally Romanian. 2 For instance, the Stanford parse"
C90-1017,J82-2005,0,0.0342504,"ystems which can consult the user when they are unable to solve a problem 1. However, in order to be a viable alternative to other machine or machineaided translation models, and in addition to the usual requirements of reasonable quality and low cost, an on-line interactive system must also satisfy the requirements of real-time systems and in particular be fast enough not to use the user&apos;s patience. *Part of the work described in this paper has been supported by a grant from the Swiss national science foundation (grant no 11-25362.88). &apos;For a discussion of on-line interactive translation see Kay (1982), Tomita (1986), Johnson and Whitelock (1987), among others. 2For a description of the parser used in the STS project, see Wehrli (1988). 1 76 A r c h i t e c t u r e of t h e s y s t e m and of the general rules and principles of target language grammar. Notice that the projection is done solely on the basis of information internal to the target language. In other words, the interface structure is minimal, as it should be, and is almost entirely a matter of lexical :napping. Both the analysis and generation modules are completely independent of each other. This again, is a desirable feature,"
C90-1017,E85-1021,1,0.799707,"by morphologically related words. At the same time, it also provides the abstract lexical level which is relevant for lexical transfer. 2.1.2 2.1 Lexical database The lexical database is the central piece of the STS system. It contains crucial information used by the three active components of the system. This information is distributed in two monolingual lexicons (SL lexicon and TL lexicon) along with one bilingual lexicon. We shall consider them in turn: 2.1.1 M o n o l i n g u a l lexicons We assume a static - or relational - conception of morphology, along the lines of Jackendoff (1975), Wehrli (1985). According to this view, morphological relations between two or more lexical entries are expressed by a complex network of relations. A monolingual lexicon distinguishes three basic entities: lexeme, word and idiom. A lexeme is an abstract lexical unit, which can be compared, roughly speaking, to a standard dictionary entry. It stands for a whole class of morphological variants. By contrast, a word corresponds to a particular morphological instantiation of a lexeme. In other words, we make a clear distinction among :features which may vary with inflexion and those which are invariant. To give"
C92-3129,C80-1064,0,0.681241,"Missing"
C92-3129,P84-1102,0,0.0338139,"r interaction can supplement modules which have not yet been developed (e.g. semantic and pragmatic components). Other i mport a nt features of the IPS parser include: • Modular architecture, i.e. the parsing mechanism is decomposed into modules, which roughly correspond to some of the components of a standard GB gra mma r (e.g. X, chains, O, etc.). • The parsing strategy is left-to-right, data driven, with parallel t re a t me nt of alterna2For arguments in favor of interactive systems (usually in the context of machine translation, though) see in particular Kay (11180), Melby et al. (1980), Tomita (1984), Wehrli (1990), Zajac (1988). PROC.OFCOL1NG-92, NANTES,AUO. 23-28, 1992 fives. The nonodeterminism of the parser is restricted by a selection mechaafism. a Use of structure-sharing techniques to cut down the number of explicit representations of alternatives. 3 The X module The central module of the IPS system is the module, which acts as the main generator of the system, and determines the syntactic structures of constituents. We assume the X schema in (1): (1) XP --~ Spec X - - + X Compl 2 where X is a lexical or a functional category, Spee and Compl are lists (possibly empty) of maximal pr"
C92-3129,C90-1017,1,0.861608,"Missing"
C92-3129,C88-2160,0,0.0139057,"odules which have not yet been developed (e.g. semantic and pragmatic components). Other i mport a nt features of the IPS parser include: • Modular architecture, i.e. the parsing mechanism is decomposed into modules, which roughly correspond to some of the components of a standard GB gra mma r (e.g. X, chains, O, etc.). • The parsing strategy is left-to-right, data driven, with parallel t re a t me nt of alterna2For arguments in favor of interactive systems (usually in the context of machine translation, though) see in particular Kay (11180), Melby et al. (1980), Tomita (1984), Wehrli (1990), Zajac (1988). PROC.OFCOL1NG-92, NANTES,AUO. 23-28, 1992 fives. The nonodeterminism of the parser is restricted by a selection mechaafism. a Use of structure-sharing techniques to cut down the number of explicit representations of alternatives. 3 The X module The central module of the IPS system is the module, which acts as the main generator of the system, and determines the syntactic structures of constituents. We assume the X schema in (1): (1) XP --~ Spec X - - + X Compl 2 where X is a lexical or a functional category, Spee and Compl are lists (possibly empty) of maximal projections (YP). Architecture"
C96-2115,C67-1009,0,0.634496,"Missing"
C96-2115,C86-1117,0,0.0411027,"in their base position). Thus, the X-schema is parameterized in German as follows: The complement (Compl) precedes the head X ° for the categories V, A, I, whereas it follows the head for the categories C, D, P, N, Adv. 3 As the specifier (Spec) is always on the left, the Xschema has the structure given in (1). (1) XP --, Spec X -~ X ° Compl, if X ° : { C °, D O, po, N O' Adv ° } -~ Compl X °, if X ° = { V °, A °, I °} On the basis of this schema, the clause structure in German has the general representation given in 1There are other proposals to deal with infinitival constructions in German: Netter 1986 discusses an LFG approach, and ~ambow 1994 uses a variation of TAG. 2 C/. Chomsky & Lasnik 1992, Haegeman 1994 for a presentation of Government and Binding Theory (GB), and Berwick et al. 1991, Wehrll 1988 for a possible implementation of the theory. aWe assume Abney's 1987 DP-hypothesis, according to which the head of a noun phrase is the determiner (D O). ment). Figure 1. C In order to keep track of where a constituent can be attached in the structure, a list of active nodes specifics the potential attachment sites; this list is systematically updated. Attachments are further constrained as"
C96-2115,C92-3129,1,0.843705,"lthough German is a partially free word order language, we will assume that it has a fixed base word order, which is modified by a set of movement transformations. In this paper, we will present the argument interpretation strategy *Thanks to Scott Fergusson for comments. This work has been supported in part by a grant from the FNRS, grant no 11-33731.92 681 The DIPS Parser General Properties of DIPS DIPS (Deutsches Interaktives Parsing System ' G e r m a n Interactive Parsing System') is a largescale interactive GB-based 2 parsing system. Its architecture is basically similar to that of IPS (Wehrli 1992) and FIPS (Laenzhnger 8z Wehrli 1991). The parser produces a set of GB Sstructures (trees) from an input sentence. These structures are associated with information concerning traces, argument structure, and case features. The syntactic structure of constituents corresponds to the GB X-schema. We consider German to be an SOV-language (i.e. objects precede their predicates in their base position). Thus, the X-schema is parameterized in German as follows: The complement (Compl) precedes the head X ° for the categories V, A, I, whereas it follows the head for the categories C, D, P, N, Adv. 3 As t"
C98-2221,E89-1001,0,0.0297532,"Missing"
C98-2221,E93-1070,1,0.847261,"Missing"
C98-2221,J89-1001,0,0.0328901,"t of M W E is a f u n d a m e n t a l requirement, as few customers would tolerate a literal translation of such comm o n expressions as e n t r e r en vigueur &apos;to come into effect&apos;, m e t t r e en oeuvre &apos;to i m p l e m e n t &apos; , faire p r e u v e &apos;to s h o w &apos; or faire c o n n a i s s a n c e &apos;to m e e t &apos;. * I am grateful to Anne Vandeventer, Christopher LaenzHnger and Thierry Etchegoyhen for helpful comments. Part of the work described in this paper has been supported by a grant from CTI (grant no 2673.1). 1Cf. Abeill6 & Schabes (1989), Arnold et al. (1995), Laporte (1988), Schenk (1995), Stock (1989), among others. 1388 However, a simple glance at some of the current commercial translation systems shows that none of t h e m can be said to handle M W E s in an appropriate fashion. As a m a t t e r of fact, some of t h e m explicitely warn their users not to use multiword expressions. In this paper, we will first stress some fundamental properties of two classes of MWEs, c o m p o u n d s and i d i o m s , and then present the t r e a t m e n t of idioms developed for our FrenchEnglish ITS-2 translation system (cf. Ramluckun & Wehrli, 1993). 2 Compounds and idioms A two-way partition of M W"
C98-2221,C92-3129,1,0.814124,"ransferconlplltlefl|~ F i g u r e 1. (6) [ WP [ DP Paul] [ ~ a [ vP cass~ [ l)p sa Architecture of ITS-2 For concreteness, we shall first focus on the epinonymous idiom given in (5): (5)a. Paul a cass~ sa pipe. lit. &apos;Paul has broken his pipe&apos; b. Paul kicked the bucket. Translation of (ha) is a three-step process: • Identification of source idiom • Transfer of idiom • Generation of target idiom 3.1 Idiom identification As we argued in the previous section, the task of identifying an idiom is best accomplished at the abstract level of representation (D-structure). ITS-2 uses the IPS parser (cf. Wehrli, 1992, 1997), which produces the structure (6) for the input (ha) 5: 5In example 6, we use the following syntactic labels : T P (Tense Phrase) for sentences, V P for verb phrases, D P for Determiner Phrases, N P for Noun Phrases, and P P for Prepositional Phrases. 1390 Idiom entries specify (a) the canonical form of the idiom (mostly for reference purposes), (b) the syntactic frame with an ordered list of constituents, and (e) the list of constraints associated with each of the constituents. In our (rather simple) example, the lexical constraints associated with the idiom (7) state that the head is"
E03-1022,P91-1022,0,0.0848115,"ize proportion. Once the pivot found, we look in its neighbourhood for the optimal candidate as target paragraph. We perform two kinds of tests on the paragraphs in this span: a test of paragraph content, and a test of paragraphs relative size matching. The first test compares the paragraphs&apos; numbering (if present). The second one determines the paragraph that best matches the rapports of sizes in a context (a sequence of surrounding paragraphs). Concluding, our approach to sentence alignment follows a length correlation strategy, as most of the existing works do, e.g. (Gale and Church, 1991; Brown et al., 1991). Individuating the pivot is a function of the documents sizes, and selecting the most likely target paragraph is a function of the relative sizes of paragraphs in the neighbourhood of the pivot. Similarly to (Simard et al., 1992), we exploit the text content in order to find word anchors (the paragraph numbering in our case). Like in (Romary and Bonhomme, 2000) and (Catizone et al., 1989), first the macro (paragraph-level) structure of documents is examined, possibly using mark-up from text encoding. 133 Annual Meeting of the Association for Computational Linguistics, Berkeley, Canada, pp. 16"
E03-1022,J93-1003,0,0.0561251,"mesures ne soient prises pour s&apos;assurer The two terms of the following collocation are separated by no less than 39 words!: Les amendements qui auront uniquement pour objet l&apos;adaptation a des niveaux plus eleves de protection des droits de propriete intellectuelle etablis et applicables conformement a d&apos;autres accords multilateraux et qui auront ete accept& dans le cadre de ces accords ... 132 3.2 Scoring for Collocation Discovery In order to identify collocations among the cooccurrences, the system achieves an independence hypothesis testing using the Log-Likelihood-ratio (see for instance (Dunning, 1993)). Based on the contingency table below for the two lexical items w 1 and w 2 that co-occur, W2 WI -I -1W2 a WI Table 1. Contingency table for cooccurrences. the system computes the cooccurrence score as follow: logX = 2 (a log a + b log b + c log c + d log d — (a + b) log (a + b) — (a + c) log (a + c) — (b + d) log (b + d) — (c + d) log (c + d) + (a + b + c + d) log (a + b + c + d)). The cooccurrences with a high score are good candidates for collocations. It is however difficult to determine a critical value above which a cooccurrence is a collocation and below which it is not. 3.3 Prelimina"
E03-1022,P91-1023,0,0.0584151,"ased on the documents size proportion. Once the pivot found, we look in its neighbourhood for the optimal candidate as target paragraph. We perform two kinds of tests on the paragraphs in this span: a test of paragraph content, and a test of paragraphs relative size matching. The first test compares the paragraphs&apos; numbering (if present). The second one determines the paragraph that best matches the rapports of sizes in a context (a sequence of surrounding paragraphs). Concluding, our approach to sentence alignment follows a length correlation strategy, as most of the existing works do, e.g. (Gale and Church, 1991; Brown et al., 1991). Individuating the pivot is a function of the documents sizes, and selecting the most likely target paragraph is a function of the relative sizes of paragraphs in the neighbourhood of the pivot. Similarly to (Simard et al., 1992), we exploit the text content in order to find word anchors (the paragraph numbering in our case). Like in (Romary and Bonhomme, 2000) and (Catizone et al., 1989), first the macro (paragraph-level) structure of documents is examined, possibly using mark-up from text encoding. 133 Annual Meeting of the Association for Computational Linguistics, Ber"
E03-1022,1993.tmi-1.17,0,0.0235227,"pressions play a very important role in many NLP applications such as terminology extraction, translation, information retrieval, and multilingual text alignment. This, along with the ever-increasing availability of very large text cor131 pora, has triggered an important need for tools to extract collocations. 3 Collocation Extraction The problem of extracting collocations from texts has been much addressed in the literature, in particular since the work of Church at al. (1991), and several statistical packages have been designed for this purpose (see for instance, the Xtract system of Smadja (1993)). Although very effective, those systems suffer from the fundamental weakness that the measure of relatedness they use is essentially the linear proximity of two or more words. As pointed out above, grammatical dependencies provide a more appropriate criterion of relatedness than simple linear proximity 3.1 Cooccurrence Extraction with Fips Collocations are extracted from syntactically analysed corpora. The analysis is performed by Fips, a large-scale parser based on an adaptation of Chomksy&apos;s ""Principles and Parameters"" theory (Laenzlinger and Wehrli, 1991). Thanks to the syntactic represent"
E03-1022,1992.tmi-1.7,0,0.157726,"relative size matching. The first test compares the paragraphs&apos; numbering (if present). The second one determines the paragraph that best matches the rapports of sizes in a context (a sequence of surrounding paragraphs). Concluding, our approach to sentence alignment follows a length correlation strategy, as most of the existing works do, e.g. (Gale and Church, 1991; Brown et al., 1991). Individuating the pivot is a function of the documents sizes, and selecting the most likely target paragraph is a function of the relative sizes of paragraphs in the neighbourhood of the pivot. Similarly to (Simard et al., 1992), we exploit the text content in order to find word anchors (the paragraph numbering in our case). Like in (Romary and Bonhomme, 2000) and (Catizone et al., 1989), first the macro (paragraph-level) structure of documents is examined, possibly using mark-up from text encoding. 133 Annual Meeting of the Association for Computational Linguistics, Berkeley, Canada, pp. 169-176. 4.2 Method Evaluation The preliminary results we obtained show that the alignment method outlined above is quite reliable. We performed the test on a sample of 800 randomly chosen collocation instances, half of which extrac"
E03-1022,J93-1004,0,\N,Missing
E03-1022,J93-1007,0,\N,Missing
E03-1083,P91-1022,0,0.0723134,"ize proportion. Once the pivot found, we look in its neighbourhood for the optimal candidate as target paragraph. We perform two kinds of tests on the paragraphs in this span: a test of paragraph content, and a test of paragraphs relative size matching. The first test compares the paragraphs&apos; numbering (if present). The second one determines the paragraph that best matches the rapports of sizes in a context (a sequence of surrounding paragraphs). Concluding, our approach to sentence alignment follows a length correlation strategy, as most of the existing works do, e.g. (Gale and Church, 1991; Brown et al., 1991). Individuating the pivot is a function of the documents sizes, and selecting the most likely target paragraph is a function of the relative sizes of paragraphs in the neighbourhood of the pivot. Similarly to (Simard et al., 1992), we exploit the text content in order to find word anchors (the paragraph numbering in our case). Like in (Romary and Bonhomme, 2000) and (Catizone et al., 1989), first the macro (paragraph-level) structure of documents is examined, possibly using mark-up from text encoding. 133 Annual Meeting of the Association for Computational Linguistics, Berkeley, Canada, pp. 16"
E03-1083,J93-1003,0,0.0535645,"mesures ne soient prises pour s&apos;assurer The two terms of the following collocation are separated by no less than 39 words!: Les amendements qui auront uniquement pour objet l&apos;adaptation a des niveaux plus eleves de protection des droits de propriete intellectuelle etablis et applicables conformement a d&apos;autres accords multilateraux et qui auront ete accept& dans le cadre de ces accords ... 132 3.2 Scoring for Collocation Discovery In order to identify collocations among the cooccurrences, the system achieves an independence hypothesis testing using the Log-Likelihood-ratio (see for instance (Dunning, 1993)). Based on the contingency table below for the two lexical items w 1 and w 2 that co-occur, W2 WI -I -1W2 a WI Table 1. Contingency table for cooccurrences. the system computes the cooccurrence score as follow: logX = 2 (a log a + b log b + c log c + d log d — (a + b) log (a + b) — (a + c) log (a + c) — (b + d) log (b + d) — (c + d) log (c + d) + (a + b + c + d) log (a + b + c + d)). The cooccurrences with a high score are good candidates for collocations. It is however difficult to determine a critical value above which a cooccurrence is a collocation and below which it is not. 3.3 Prelimina"
E03-1083,P91-1023,0,0.058356,"ased on the documents size proportion. Once the pivot found, we look in its neighbourhood for the optimal candidate as target paragraph. We perform two kinds of tests on the paragraphs in this span: a test of paragraph content, and a test of paragraphs relative size matching. The first test compares the paragraphs&apos; numbering (if present). The second one determines the paragraph that best matches the rapports of sizes in a context (a sequence of surrounding paragraphs). Concluding, our approach to sentence alignment follows a length correlation strategy, as most of the existing works do, e.g. (Gale and Church, 1991; Brown et al., 1991). Individuating the pivot is a function of the documents sizes, and selecting the most likely target paragraph is a function of the relative sizes of paragraphs in the neighbourhood of the pivot. Similarly to (Simard et al., 1992), we exploit the text content in order to find word anchors (the paragraph numbering in our case). Like in (Romary and Bonhomme, 2000) and (Catizone et al., 1989), first the macro (paragraph-level) structure of documents is examined, possibly using mark-up from text encoding. 133 Annual Meeting of the Association for Computational Linguistics, Ber"
E03-1083,1993.tmi-1.17,0,0.0220381,"pressions play a very important role in many NLP applications such as terminology extraction, translation, information retrieval, and multilingual text alignment. This, along with the ever-increasing availability of very large text cor131 pora, has triggered an important need for tools to extract collocations. 3 Collocation Extraction The problem of extracting collocations from texts has been much addressed in the literature, in particular since the work of Church at al. (1991), and several statistical packages have been designed for this purpose (see for instance, the Xtract system of Smadja (1993)). Although very effective, those systems suffer from the fundamental weakness that the measure of relatedness they use is essentially the linear proximity of two or more words. As pointed out above, grammatical dependencies provide a more appropriate criterion of relatedness than simple linear proximity 3.1 Cooccurrence Extraction with Fips Collocations are extracted from syntactically analysed corpora. The analysis is performed by Fips, a large-scale parser based on an adaptation of Chomksy&apos;s &quot;Principles and Parameters&quot; theory (Laenzlinger and Wehrli, 1991). Thanks to the syntactic represent"
E03-1083,1992.tmi-1.7,0,0.160471,"relative size matching. The first test compares the paragraphs&apos; numbering (if present). The second one determines the paragraph that best matches the rapports of sizes in a context (a sequence of surrounding paragraphs). Concluding, our approach to sentence alignment follows a length correlation strategy, as most of the existing works do, e.g. (Gale and Church, 1991; Brown et al., 1991). Individuating the pivot is a function of the documents sizes, and selecting the most likely target paragraph is a function of the relative sizes of paragraphs in the neighbourhood of the pivot. Similarly to (Simard et al., 1992), we exploit the text content in order to find word anchors (the paragraph numbering in our case). Like in (Romary and Bonhomme, 2000) and (Catizone et al., 1989), first the macro (paragraph-level) structure of documents is examined, possibly using mark-up from text encoding. 133 Annual Meeting of the Association for Computational Linguistics, Berkeley, Canada, pp. 169-176. 4.2 Method Evaluation The preliminary results we obtained show that the alignment method outlined above is quite reliable. We performed the test on a sample of 800 randomly chosen collocation instances, half of which extrac"
E03-1083,J93-1004,0,\N,Missing
E03-1083,J93-1007,0,\N,Missing
E85-1021,P84-1116,0,\N,Missing
E93-1070,C92-3129,1,\N,Missing
nerima-etal-2010-recursive,W07-1216,1,\N,Missing
nerima-etal-2010-recursive,W03-1806,0,\N,Missing
nerima-wehrli-2008-generating,W07-1216,1,\N,Missing
nerima-wehrli-2008-generating,C92-2081,0,\N,Missing
nerima-wehrli-2008-generating,W04-2204,0,\N,Missing
nerima-wehrli-2008-generating,W06-2006,0,\N,Missing
nerima-wehrli-2008-generating,2005.mtsummit-papers.11,0,\N,Missing
nerima-wehrli-2008-generating,W02-1705,0,\N,Missing
nerima-wehrli-2008-generating,2007.mtsummit-papers.73,0,\N,Missing
P06-1120,W93-0309,0,0.224987,"Missing"
P06-1120,2003.mtsummit-papers.39,0,0.0452359,"out by McKeown and Radev (2000), their definition is rather vague, and the distinction from other types of expressions is not clearly drawn); second, because they are prevalent in language. Mel’ˇcuk (1998, 24) claims that “collocations make up the lions share of the phraseme inventory”, and a recent study referred in (Pearce, 2001) showed that each sentence is likely to contain at least one collocation. Collocational information is not only useful, but also indispensable in many applications. In machine translation, for instance, it is considered “the key to producing more acceptable output” (Orliac and Dillinger, 2003, 292). This article presents a system that extracts accurate collocational information from corpora by using a syntactic parser that supports several languages. After describing the underlying methodology (section 2), we report several extraction results for English, French, Spanish and Italian (section 3). Then we present in sections 4 and 5 a comparative evaluation experiment proving that a hybrid approach leads to more accurate results than a classical approach in which syntactic information is not taken into account. Abstract This paper focuses on the use of advanced techniques of text an"
P06-1120,P05-2003,0,0.10104,"ter definitions, hence the importance we lend to using appropriate extraction methodologies, based on syntactic analysis. The hybrid method we developed relies on the parser Fips (Wehrli, 2004), that implements the Government and Binding formalism and supports several languages (besides the ones mentioned in I. in stage one, collocation candidates are identified from the text corpora, based on criteria which are specific to each system; II. in stage two, the candidates are scored and ranked using specific association measures (a review can be found in (Manning and Sch¨utze, 1999; Evert, 2004; Pecina, 2005)). According to this description, in our approach the parser is used in the first stage of extraction, for identifying the collocation candidates. A pair of lexical items is selected as a candidate only if there is a syntactic relation holding between the two items (one being the head of the current parse structure, and the other the lexical head of its specifier/complement). Therefore, the criterion we employ for candidate selection is the syntactic proximity, as opposed to the linear proximity used by traditional, window-based methods. As the parsing goes on, the syntactic word pairs are ext"
P06-1120,W03-1806,0,0.255256,"Missing"
P06-1120,J93-1003,0,0.159603,"candidate selection is the syntactic proximity, as opposed to the linear proximity used by traditional, window-based methods. As the parsing goes on, the syntactic word pairs are extracted from the parse structures created, from each head-specifier or head-complement relation. The pairs obtained are then partitioned according to their syntactic configuration (e.g., noun + adjectival or nominal specifier, noun + argument, noun + adjective in predications, verb + adverbial specifier, verb + argument (subject, object), verb + adjunt, etc). Finally, the loglikelihood ratios test (henceforth LLR) (Dunning, 1993) is applied on each set of pairs. We call this method hybrid, since it combines syntactic and statistical information (about word and cooccurrence frequency). The following examples — which, like all the examples in this paper, are actual extraction results — demonstrate the potential of our system to detect collocation candidates, even if subject to complex syntactic transformations. 2 “Ideally, in order to identify lexical relations in a corpus one would need to first parse it to verify that the words are used in a single phrase structure. However, in practice, freestyle texts contain a grea"
P06-1120,E03-1080,0,0.0527024,"-best list at the different levels considered is therefore annotated with one of the three flags: 1. ungrammatical; 2. trivial combination; 3. multi-word expression (MWE). On the one side, we evaluate the results of our hybrid, parse-based method; on the other, we simulate a window method, by performing the following steps: POS-tag the source texts; filter the lexical items and retain only the open-class POS; consider all their combinations within a collocational window of length 5; and, finally, apply the log-likelihood ratios test on the pairs of each configuration type. In accordance with (Evert and Kermes, 2003), we consider that the comparative evaluation of collocation extraction systems should not be done at the end of the extraction process, but separately for each stage: after the candidate selection stage, for evaluating the quality (in terms of grammaticality) of candidates proposed; and after the application of collocability measures, for evaluating the measures applied. In each of these cases, different evaluation methodologies and resources are required. In our case, since we used the same measure for the second stage (the log-likelihood ratios test), we could still compare the final output"
P06-1120,W06-1006,1,0.827496,"n (En), question adresser a` ministre, programme de aide a` r´enovation r´esidentielle, agent employer force susceptible causer (Fr), bolsa de comercio local, peso en cuota de fondo de inversi´on, permitir uso de papel de deuda esterno (Sp), consiglio federale disporre, creazione di nuovo posto di lavoro, costituire fattore penalizzante per regione (It)5 . Table 1: Extraction statistics In Table 2 we list the top collocations (of length two) extracted for each language. We do not specifically discuss here multilingual issues in collocation extraction; these are dealt with in a separate paper (Seretan and Wehrli, 2006). 3 The low rate of completely parsed sentences for Spanish and Italian are due to the relatively reduced coverage of the parsers of these two languages (under development). However, even if a sentence is not assigned a complete parse tree, some syntactic pairs can still be collected from the partial parses. 4 The log-likelihood ratios score is undefined for those pairs having a cell of the contingency table equal to 0. 5 Note that the output of the procedure contains lemmas rather than inflected forms. 955 4 4.1 Comparative Evaluation Hypotheses the assumption of perfect parsing, this hypothe"
P06-1120,J93-1007,0,\N,Missing
P06-4016,C96-2140,0,0.0881171,"Missing"
P06-4016,2003.mtsummit-systems.17,1,0.904362,"more specific help for an unknown term or an opaque expression. Such typical users are the huge crowd of students and scientists around the world who routinely browse documents in English on the Internet or elsewhere. For on-line documents, a variety of terminological tools are available, some of them commercially, such as the ones provided by Google (word translation services) or Babylon Ltd. More advanced, research-oriented systems based on computational linguistics technologies have also been developed, such as GLOSSERRuG (Nerbonne et al, 1996, 1999), Compass (Breidt et al., 1997) or TWiC (Wehrli, 2003, 2004). Similar needs are less easy to satisfy when it comes to more traditional documents such as books and other printed material. Multilingual scanning devices have been commercialized1 , but they lack the computational linguistic resources to make them truly useful. The shortcomings of such systems are particularly blatant with inflected languages, or with compound-rich languages such as German, while the inadequate treatment of multiword expressions is obvious for all languages. TwicPen has been designed to overcome these shortcomings and intends to provide readers of printed material wi"
P06-4016,2004.jeptalnrecital-long.14,1,0.924426,"Missing"
P98-2226,E89-1001,0,0.0215766,"Missing"
P98-2226,E93-1070,1,0.851087,"Missing"
P98-2226,J89-1001,0,0.0308121,"translation, a proper treatment of M W E is a fundamental requirement, as few customers would tolerate a literal translation of such comm o n expressions as entrer en vigueur &apos;to come into effect&apos;, m e t t r e en oeuvre &apos;to i m p l e m e n t &apos; , faire preuve &apos;to s h o w &apos; or faire connaissance &apos;to m e e t &apos;. &quot; I am grateful to Anne Vandeventer, Christopher Laenzlinger and Thierry Etchegoyhen for helpful comments. Part of the work described in this paper has been supported by a grant from CTI (grant no 2673.1). zCf. Abeill~ & Schabes (1989), Arnold et al. (1995), Laporte (1988), Schenk (1995), Stock (1989), among others. 1388 However, a simple glance at some of the current commercial translation systems shows that none of t h e m can be said to handle MWEs in an appropriate fashion. As a m a t t e r of fact, some of t h e m explicitely warn their users not to use multiword expressions. In this paper, we will first stress some fundamental properties of two classes of MWEs, c o m p o u n d s and i d i o m s , and then present the treatment of idioms developed for our FrenchEnglish ITS-2 translation system (cf. Ramluckun & Wehrli, 1993). 2 Compounds and idioms A two-way partition of MWEs in (i) co"
P98-2226,C92-3129,1,0.866248,"Missing"
scherrer-etal-2014-swissadmin,W07-1216,1,\N,Missing
scherrer-etal-2014-swissadmin,P07-2045,0,\N,Missing
scherrer-etal-2014-swissadmin,petrov-etal-2012-universal,0,\N,Missing
scherrer-etal-2014-swissadmin,2005.mtsummit-papers.11,0,\N,Missing
scherrer-etal-2014-swissadmin,W10-3705,1,\N,Missing
scherrer-etal-2014-swissadmin,scherrer-cartoni-2012-trilingual,1,\N,Missing
seretan-etal-2004-using,W02-0909,0,\N,Missing
seretan-etal-2010-fipsromanian,nerima-etal-2010-recursive,1,\N,Missing
seretan-etal-2010-fipsromanian,W07-1216,1,\N,Missing
W06-1006,P91-1034,0,0.0723439,"Missing"
W06-1006,C90-3010,0,0.362598,"on to the log-likelihood measure (henceforth LL) for distinguishing trivial compounds from lexicalized ones. Finally, Wermter and Hahn (2004) identified PP-V combinations using a POS tagger and a chunker. They based their method on a linguistic criterion (that of limited modifiability) and compared their results with those obtained using the t-score and LL tests. 3.4 Other Languages In addition to English, German and French, other languages for which notable collocation extraction work was performed, are — as we are aware of — the following: • Italian: early extraction work was carried out by Calzolari and Bindi (1990) and employed MI. It was followed by (Basili et al., 1994), that made use of parsing information; • Korean: (Shimohata et al., 1997) used an adjacency n-gram model, and (Kim et al., 1999) relied on POS-tagging; • Chinese: (Huang et al., 2005) used POS information, while (Lu et al., 2004) applied extraction techniques similar to Xtract system (Smadja, 1993); • Japanese: (Ikehara et al., 1995) was based on an improved n-gram method. As for multilingual extraction via alignment (where collocations are first detected in one language and then matched with their translation in another language), mos"
W06-1006,I05-3007,0,0.0798144,"erion (that of limited modifiability) and compared their results with those obtained using the t-score and LL tests. 3.4 Other Languages In addition to English, German and French, other languages for which notable collocation extraction work was performed, are — as we are aware of — the following: • Italian: early extraction work was carried out by Calzolari and Bindi (1990) and employed MI. It was followed by (Basili et al., 1994), that made use of parsing information; • Korean: (Shimohata et al., 1997) used an adjacency n-gram model, and (Kim et al., 1999) relied on POS-tagging; • Chinese: (Huang et al., 2005) used POS information, while (Lu et al., 2004) applied extraction techniques similar to Xtract system (Smadja, 1993); • Japanese: (Ikehara et al., 1995) was based on an improved n-gram method. As for multilingual extraction via alignment (where collocations are first detected in one language and then matched with their translation in another language), most or the existing work concern the English-French language pair, and the Hansard corpus of Canadian Parliament proceedings. Wu (1994) signals a number of problems 2 The following abbreviations are used in this paper: N noun, V - verb, A - adj"
W06-1006,W03-1806,0,0.366742,"Missing"
W06-1006,J93-1003,0,0.0323199,"pairs are identified as the parsing goes on; in other approaches, they are extracted by post-processing the output of syntactic tools. The candidate pairs identified are classified into syntactically homogenous sets, according to the syntactic relations holding between the two items. Only certain predefined syntactic relations are kept, that were judged as collocationally relevant after multiple experiments of extraction and data analysis (e.g., adjective-noun, verb-object, subject-verb, noun-noun, verb-preposition-noun). The sets obtained are then ranked using the loglikelihood ratios test (Dunning, 1993). More details about the system and its performance can be found in (Seretan and Wehrli, 2006). The following examples (taken from the extraction experiment we will describe below) illustrate its potential to detect collocation candidates, even if these are subject to complex syntactic transformations: These are language-specific parameters which need to be set in a successful multilingual extraction procedure. Truly multilingual systems have not been developed yet, but we suggest the following strategy for building such a system: A. parse the source corpus, extract all the syntactic pairs (e."
W06-1006,P01-1025,0,0.0771708,"N, N P Det N. She used a lemmatizer and a POS-tagger before applying a series of AMs, which she then evaluated against a domain-specific terminology dictionary and against a gold-standard manually created from the extraction corpus. Similarly, Bourigault (1992) extracted nounphrases from shallow-parsed text, and Goldman et al. (2001) extracted syntactic collocations by using a full parser and applying the LL test. 3.2 German German is the second most investigated language, thanks to the early work of Breidt (1993) and, more recently, to that of Krenn and Evert, such as (Krenn and Evert, 2001; Evert and Krenn, 2001; Evert, 2004) centered on evaluation. Breidt uses MI and t-score and compares the results accuracy when various parameters vary, such as the window size, presence vs. absence of lemmatization, corpus size, and presence vs. absence of POS and syntactic information. She focuses on N-V pairs2 and, despite the lack of syntactic analysis tools at the time, by simulating parsing she comes to the conclusion that “Very high precision rates, which are an indispensable requirement for lexical acquisition, can only realistically be envisaged for German with parsed corpora” (Breidt, 1993, 82). Later, Kre"
W06-1006,W99-0610,0,0.850324,"chunker. They based their method on a linguistic criterion (that of limited modifiability) and compared their results with those obtained using the t-score and LL tests. 3.4 Other Languages In addition to English, German and French, other languages for which notable collocation extraction work was performed, are — as we are aware of — the following: • Italian: early extraction work was carried out by Calzolari and Bindi (1990) and employed MI. It was followed by (Basili et al., 1994), that made use of parsing information; • Korean: (Shimohata et al., 1997) used an adjacency n-gram model, and (Kim et al., 1999) relied on POS-tagging; • Chinese: (Huang et al., 2005) used POS information, while (Lu et al., 2004) applied extraction techniques similar to Xtract system (Smadja, 1993); • Japanese: (Ikehara et al., 1995) was based on an improved n-gram method. As for multilingual extraction via alignment (where collocations are first detected in one language and then matched with their translation in another language), most or the existing work concern the English-French language pair, and the Hansard corpus of Canadian Parliament proceedings. Wu (1994) signals a number of problems 2 The following abbrevia"
W06-1006,W96-0107,0,0.048865,"Missing"
W06-1006,P93-1003,0,0.0912713,"ns in general (Sag et al., 2002). Collocations are particularly important because of their prevalence in language, regardless of the domain or genre. According to Jackendoff (1997, 156) and Mel’ˇcuk (1998, 24), collocations constitute the bulk of a language’s lexicon. The last decades have witnessed a considerable development of collocation extraction techniques, that concern both monolingual and (parallel) multilingual corpora. We can mention here only part of this work: (Berry-Rogghe, 1973; Church et al., 1989; Smadja, 1993; Lin, 1998; Krenn and Evert, 2001) for monolingual extraction, and (Kupiec, 1993; Wu, 1994; Smadja et al., 1996; Kitamura and MatIntroduction Collocations are understood in this paper as “idiosyncratic syntagmatic combination of lexical items” (Fontenelle, 1992, 222): heavy rain, light breeze, great difficulty, grow steadily, meet requirement, reach consensus, pay attention, ask a question. Unlike idioms (kick the bucket, lend a hand, pull someone’s leg), their meaning is fairly transparent and easy to decode. Yet, differently from the regular productions, (big house, cultural activity, read a book), collocational expressions are highly idiosyncratic, since the lexical it"
W06-1006,J96-1001,0,0.121362,"., 2002). Collocations are particularly important because of their prevalence in language, regardless of the domain or genre. According to Jackendoff (1997, 156) and Mel’ˇcuk (1998, 24), collocations constitute the bulk of a language’s lexicon. The last decades have witnessed a considerable development of collocation extraction techniques, that concern both monolingual and (parallel) multilingual corpora. We can mention here only part of this work: (Berry-Rogghe, 1973; Church et al., 1989; Smadja, 1993; Lin, 1998; Krenn and Evert, 2001) for monolingual extraction, and (Kupiec, 1993; Wu, 1994; Smadja et al., 1996; Kitamura and MatIntroduction Collocations are understood in this paper as “idiosyncratic syntagmatic combination of lexical items” (Fontenelle, 1992, 222): heavy rain, light breeze, great difficulty, grow steadily, meet requirement, reach consensus, pay attention, ask a question. Unlike idioms (kick the bucket, lend a hand, pull someone’s leg), their meaning is fairly transparent and easy to decode. Yet, differently from the regular productions, (big house, cultural activity, read a book), collocational expressions are highly idiosyncratic, since the lexical items a headword combines with in"
W06-1006,2003.mtsummit-systems.17,1,0.696834,"cular AM (such as LL) in a language vs. another. Figure 1 displays the relative proportions of 3 syntactic types — adjectivenoun, subject-verb and verb-object — that can be found at different levels in the significance list returned by LL. The collocation extractor is part of a bigger system (Seretan et al., 2004) that integrates a concordancer and a sentence aligner, and that supports the visualization, the manual validation and the management of a multilingual terminology database. The validated collocations are used for populating the lexicon of the parser and that of a translation system (Wehrli, 2003). 6 A Cross-Lingual Extraction Experiment Figure 1: Cross-lingual proportions of A-N, S-V and V-O pairs at different levels in the significance lists A collocation extraction experiment concerning four different languages (English, Spanish, French, Italian) has been conducted on a parallel subcorpus of 42 files from the European Parliament proceedings. Several statistics and extraction results are reported in Table 1. Statistics tokens sent/file complete parses tokens/sent extr. pairs (tokens) token/type LL is def. English 2526403 2329.1 Spanish 2666764 2513.7 Italian 2575858 2331.6 French 293"
W06-1006,P97-1039,0,0.0237854,"d more taken into account by collocation extraction systems. The rest of the paper is organized as follows. In section 3 we provide a language-oriented review of the existing collocation extraction work. Then we highlight, in section 4, a series of problems that arise in the transfer of methodology to a new language, and we propose a strategy for dealing with them. Section 5 describes an extraction system, and, finally, section 6 presents a case-study on the collocations extracted for four languages, illustrating the cross-lingual variation in the performance of a particular AM. sumoto, 1996; Melamed, 1997) for bilingual extraction via alignment. Traditionally, collocation extraction was considered a language-independent task. Since collocations are recurrent, typical lexical combinations, a wide range of statistical methods based on word co-occurrence frequency have been heavily used for detecting them in text corpora. Among the most often used types of lexical association measures (henceforth AMs) we mention: statistical hypothesis tests (e.g., binomial, Poisson, Fisher, zscore, chi-squared, t-score, and log-likelihood ratio tests), that measure the significance of the association between two"
W06-1006,C04-1141,0,0.0180053,"ble requirement for lexical acquisition, can only realistically be envisaged for German with parsed corpora” (Breidt, 1993, 82). Later, Krenn and Evert (2001) used a German chunker to extract syntactic pairs such as P-N-V. Their work put the basis of formal and systematic methods in collocation extraction evaluation. Zinsmeister and Heid (2003; 2004) focused on N-V and A-N-V combinations identified using a stochastic parser. They applied machine learning techniques in combination to the log-likelihood measure (henceforth LL) for distinguishing trivial compounds from lexicalized ones. Finally, Wermter and Hahn (2004) identified PP-V combinations using a POS tagger and a chunker. They based their method on a linguistic criterion (that of limited modifiability) and compared their results with those obtained using the t-score and LL tests. 3.4 Other Languages In addition to English, German and French, other languages for which notable collocation extraction work was performed, are — as we are aware of — the following: • Italian: early extraction work was carried out by Calzolari and Bindi (1990) and employed MI. It was followed by (Basili et al., 1994), that made use of parsing information; • Korean: (Shimoh"
W06-1006,P94-1012,0,0.336154,"(Sag et al., 2002). Collocations are particularly important because of their prevalence in language, regardless of the domain or genre. According to Jackendoff (1997, 156) and Mel’ˇcuk (1998, 24), collocations constitute the bulk of a language’s lexicon. The last decades have witnessed a considerable development of collocation extraction techniques, that concern both monolingual and (parallel) multilingual corpora. We can mention here only part of this work: (Berry-Rogghe, 1973; Church et al., 1989; Smadja, 1993; Lin, 1998; Krenn and Evert, 2001) for monolingual extraction, and (Kupiec, 1993; Wu, 1994; Smadja et al., 1996; Kitamura and MatIntroduction Collocations are understood in this paper as “idiosyncratic syntagmatic combination of lexical items” (Fontenelle, 1992, 222): heavy rain, light breeze, great difficulty, grow steadily, meet requirement, reach consensus, pay attention, ask a question. Unlike idioms (kick the bucket, lend a hand, pull someone’s leg), their meaning is fairly transparent and easy to decode. Yet, differently from the regular productions, (big house, cultural activity, read a book), collocational expressions are highly idiosyncratic, since the lexical items a head"
W06-1006,P05-2003,0,0.165395,"s) we mention: statistical hypothesis tests (e.g., binomial, Poisson, Fisher, zscore, chi-squared, t-score, and log-likelihood ratio tests), that measure the significance of the association between two words based on a contingency table listing their joint and marginal frequency, and Information-theoretic measures (Mutual Information — henceforth MI — and its variants), that quantity of ‘information’ shared by two random variables. A detailed review of the statistical methods employed in collocation extraction can be found, for instance, in (Evert, 2004). A comprehensive list of AMs is given (Pecina, 2005). Very often, in addition to the information on cooccurrence frequency, language-specific information is also integrated in a collocation extraction system (as it will be seen in section 3): - morphological information, in order to count inflected word forms as instances of the same base form. For instance, ask questions, asks question, asked question are all instances of the same word pair, ask - question; 3 3.1 - syntactic information, in order to recognize a word pair even if subject to (complex) syntactic transformations: ask multiple questions, question asked, questions that one might ask"
W06-1006,2003.mtsummit-papers.57,0,0.0678828,"Missing"
W06-1006,P97-1061,0,0.138059,"(2004) identified PP-V combinations using a POS tagger and a chunker. They based their method on a linguistic criterion (that of limited modifiability) and compared their results with those obtained using the t-score and LL tests. 3.4 Other Languages In addition to English, German and French, other languages for which notable collocation extraction work was performed, are — as we are aware of — the following: • Italian: early extraction work was carried out by Calzolari and Bindi (1990) and employed MI. It was followed by (Basili et al., 1994), that made use of parsing information; • Korean: (Shimohata et al., 1997) used an adjacency n-gram model, and (Kim et al., 1999) relied on POS-tagging; • Chinese: (Huang et al., 2005) used POS information, while (Lu et al., 2004) applied extraction techniques similar to Xtract system (Smadja, 1993); • Japanese: (Ikehara et al., 1995) was based on an improved n-gram method. As for multilingual extraction via alignment (where collocations are first detected in one language and then matched with their translation in another language), most or the existing work concern the English-French language pair, and the Hansard corpus of Canadian Parliament proceedings. Wu (1994"
W06-1006,J94-4005,0,\N,Missing
W06-1006,J90-1003,0,\N,Missing
W06-1006,J93-1007,0,\N,Missing
W06-1006,W89-0240,0,\N,Missing
W06-1006,H89-2012,0,\N,Missing
W06-1006,C94-1074,0,\N,Missing
W06-1006,W02-0909,0,\N,Missing
W06-1006,P84-1058,0,\N,Missing
W06-1006,C92-3150,0,\N,Missing
W06-1006,P06-1120,1,\N,Missing
W07-1216,2005.mtsummit-papers.11,0,0.00546255,"very partial. Fips attempts to produce complete analyzes for input sentences. Since the parsing strategy is (pseudo-)parallel, many analyzes are produced and ranked according to preferences such as local vs. non-local attachments, argument vs. adjunct interpretation, presence vs. absence of a collocation, etc. When a complete analysis fails, the parser outputs a sequence of partial analyzes covering the whole sentence. A comparative evaluation has been conducted to show how the various implementations of Fips compare with respect to a near identical corpus, the European Parliament corpus (cf. Koehn, 2005). We parsed approximately 1 million words in each of the six languages. The table given in figure 5 show the results: The first line in table 5 show the size of each file in terms of symbols (word, punctuation, formatting symbol, etc.), approximately 1 million symbols for each file. The second line gives the number of unknown words, not counting words starting with an uppercase letter which are assumed to be proper nouns (given the fact that in German common nouns are capitalized, we did not leave aside capitalized unknown words for that 126 language). The third line indicates the number of se"
W07-1216,P06-1120,1,0.87019,"Missing"
W07-1216,2004.jeptalnrecital-long.14,1,0.562993,"r Swiss “national” languages (German, French, Italian and English) to which we also added Spanish and (more recently) Greek. (1) The record she broke was very old. 1 Introduction This papers describes the Fips project, which aims at developing a robust, multilingual “deep” linguistic parsing system efficient enough for a wide-range of NLP applications. The system is currently available for six languages (English, French, German, Italian, Spanish and Greek), and has been extensively used for terminology extraction (Seretan & Wehrli, 2006), as well as for terminology assistance and translation (Wehrli, 2004, 2006). This paper is organized as follows. The next section gives an overview of the Fips parser, describing some of its linguistic properties and its main processes. In section 3, we present the [ TP [ DP the [ NP recordi [ CP [ DP e]i [ TP [ DP she ] broke [ e]i ] ] ] ] was [ [ [ very DP FP AP Adv ] old ] ] ] Figure 1: Enriched S-Structure representation for sentence (1) The linguistic assumptions used in this project correspond roughly to a free-adaptation of Chomsky’s generative linguistics, borrowing concepts from the Minimalist model (Chomsky, 1995, 2004), from the Simpler Syntax model"
W07-1216,P06-4016,1,0.808281,"for its implementation, Fips is designed to efficiently parse the four Swiss “national” languages (German, French, Italian and English) to which we also added Spanish and (more recently) Greek. (1) The record she broke was very old. 1 Introduction This papers describes the Fips project, which aims at developing a robust, multilingual “deep” linguistic parsing system efficient enough for a wide-range of NLP applications. The system is currently available for six languages (English, French, German, Italian, Spanish and Greek), and has been extensively used for terminology extraction (Seretan & Wehrli, 2006), as well as for terminology assistance and translation (Wehrli, 2004, 2006). This paper is organized as follows. The next section gives an overview of the Fips parser, describing some of its linguistic properties and its main processes. In section 3, we present the [ TP [ DP the [ NP recordi [ CP [ DP e]i [ TP [ DP she ] broke [ e]i ] ] ] ] was [ [ [ very DP FP AP Adv ] old ] ] ] Figure 1: Enriched S-Structure representation for sentence (1) The linguistic assumptions used in this project correspond roughly to a free-adaptation of Chomsky’s generative linguistics, borrowing concepts from the"
W09-0415,W06-2006,0,0.0132214,"Gamallo, 2007). We plan to use semi-automatic generation to build the 6 remaining dictionaries. For this purpose we will derive a bilingual lexicon by transitivity, using two existing ones. For instance, if we have bilingual correspondences for language pair 1 This attribute takes the form of an integer between 6 (preferred) and 0 (lowest). 92 A ! B and B ! C, we can obtain A ! C. We will see below how the correspondences are validated. The idea of using a pivot language for deriving bilingual lexicons from existing ones is not new. The reader can find related approaches in (Paik & al. 2004, Ahn & Frampton 2006, Zhang & al. 2007) . The specificity of our approach is that the initial resources are manually made, i.e. non noisy, lexicons. The derivation process goes as follows: and German-French lexicons. For the checking of the validity of the correspondences (point 4 of the process) we used the parallel corpus of the debates of the European Parliament during the period 1996 to 2001 (Koehn, 2005). Figure 2 summarizes the results of the four steps of the derivation process: Step 1 2 3 4 1. Take two bilingual tables for language pairs (A, B) and (B, C) and perform a relational equi-join. Perform a filt"
W09-0415,W04-2204,0,0.0228175,"Missing"
W09-0415,2007.mtsummit-papers.4,0,0.0769589,"Missing"
W09-0415,2001.mtsummit-road.1,0,0.0739494,"Missing"
W09-0415,W07-1216,1,\N,Missing
W09-0415,P06-1120,1,\N,Missing
W09-0415,2005.mtsummit-papers.11,0,\N,Missing
W09-0415,W02-1705,0,\N,Missing
W09-0415,2007.mtsummit-papers.73,0,\N,Missing
W10-3705,W04-0407,0,0.175313,"Missing"
W10-3705,P98-1030,0,0.599372,"Missing"
W10-3705,D07-1110,0,0.145015,"Missing"
W10-3705,C02-1004,0,0.0474404,"Missing"
W10-3705,W09-0415,1,0.266799,", as Hindle and Rooth (1993) pointed out, the parsers used by such methods lack precisely the kind of corpus-based information that is required to resolve ambiguity, because many of the existing attachments may be missing or wrong. The current literature provides no indication about the manner in which this circular problem can be circumvented, and on whether flexible MWEs should be processed before, during or after the sentence analysis takes place. der all the possible syntactic realisations of collocations. Alternatively, a post-processing approach (such as the one we pursued previously in Wehrli et al. (2009b)) would identify collocations after the syntactic analysis has been performed, and output a parse tree in which collocational relations are highlighted between the composing items, in order to inform the subsequent processing applications (e.g., a machine translation application). Again, this solution is not fully appropriate, and the reason lies with the important observation that prior collocational knowledge is highly relevant for parsing. Collocational restrictions are, along with other types of information like selectional preferences and subcategorization frames, a major means of struc"
W10-3705,J93-1005,0,0.465074,"Missing"
W10-3705,2009.eamt-1.18,1,0.920514,", as Hindle and Rooth (1993) pointed out, the parsers used by such methods lack precisely the kind of corpus-based information that is required to resolve ambiguity, because many of the existing attachments may be missing or wrong. The current literature provides no indication about the manner in which this circular problem can be circumvented, and on whether flexible MWEs should be processed before, during or after the sentence analysis takes place. der all the possible syntactic realisations of collocations. Alternatively, a post-processing approach (such as the one we pursued previously in Wehrli et al. (2009b)) would identify collocations after the syntactic analysis has been performed, and output a parse tree in which collocational relations are highlighted between the composing items, in order to inform the subsequent processing applications (e.g., a machine translation application). Again, this solution is not fully appropriate, and the reason lies with the important observation that prior collocational knowledge is highly relevant for parsing. Collocational restrictions are, along with other types of information like selectional preferences and subcategorization frames, a major means of struc"
W10-3705,2005.mtsummit-papers.11,0,0.00246178,"ted to a collocation: in example (8a), V1 assigned the words on and business to the lexical entry on business, and in example (8b), it assigned in and country to the entry in the country9 . (8)a. It is not, by any means, specific to the countryside, but it falls especially heavily on small businesses. 4.2 Recall Evaluation To compare the recall of two methods we performed a similar experiment, in which we run the two versions of the parser, V1 and V2, on a small collection of sentences containing annotated collocation instances. These sentences were randomly selected from the Europarl corpus (Koehn, 2005). The collocations they contain are all verb-object collocations. We limit our present investigation to this syntactic type for two reasons: a) annotating a corpus with all instances of collocation entries in the lexicon would be a time-consuming task; and b) verb-object collocations are among the most syntactically flexible and therefore difficult to detect in real texts. Thus, this test set provides realistic information on recall. 9 V1 makes the same error on (8a), but does better on (8b). These expressions are frozen and should not be treated as standard collocations. 10 Coverage refers mo"
W10-3705,2003.mtsummit-papers.39,0,0.0226801,"nglish Italian V1 63 44 V2 76 66 Common 61 42 V1 only 2 2 V2 only 15 24 Table 4. Recall evaluation results: number of correct collocation instances identified. 4.3 Task-based Evaluation In addition to reporting the performance results by using the standard measures of precision and recall, we performed a task-based performance evaluation, in which we quantified the impact that the newly-proposed method has on the quality of the output of a machine translation system. As the examples in table 3 suggest, a literal translation of collocations is rarely the most appropriate. In fact, as stated by Orliac and Dillinger (2003), knowledge of collocations is crucial for machine translation systems. An important purpose in identifying collocations with our parser is to enable their proper treatment in our translation system, a rule-based system that performs syntactic transfer by relying on the structures produced by the parser. In this system, the translation of a collocation takes place as follows. When the parser identifies a collocation in the source sentence, its component words are marked as collocation members, in order to prevent their literal translation. When the transfer module processes the collocation hea"
W10-3705,W07-1216,1,0.0902034,"he parser would otherwise consider. must be considered. Given the high fre3.1 The Method quency of lexical ambiguities, the high level of non-determinism of natural language grammars, To fulfill the goal of interconnecting the parsing grammar-based parsers are faced with a number procedure and the identification of collocations, of alternatives which grows exponentially with the we have incorporated the collocation identificalength of the input sentence. Various methods tion mechanism within the constituent attachment have been proposed to reduce that number, and procedure of our parser Fips (Wehrli, 2007). This in most cases heuristics are added to the parsing parser, like many grammar-based parsers, uses algorithm to limit the number of alternatives. Wi- left attachment and right attachment rules to build thout such heuristics, the performance of a parser respectively left subconstituents and right submight not be satisfactory enough for large scale constituents. Given the fact that Fips’ rules always applications such as machine translation or other involve exactly two constituents – see Wehrli (2007) for details – it is easy to add to the attachtasks involving large corpora. We would like t"
W10-3705,P00-1014,0,0.0241916,"Missing"
W10-3705,zhang-kordoni-2006-automated,0,0.0721706,"Missing"
W10-3705,J94-4005,0,\N,Missing
W10-3705,C98-1030,0,\N,Missing
W10-3705,P98-2177,0,\N,Missing
W10-3705,C98-2172,0,\N,Missing
W11-0819,J93-1003,0,0.123767,"ord entered by the user in the system interface, performs a search in the database that stores the collocation extraction results, and provides a one-page presentation of the collocational information related to the sought word. Users can set visualisation pa126 rameters such as the minimal frequency and association score, which limit the displayed results according to the number of occurrences in the corpus and the “association strength” between the component words, as given by the lexical association measure used to extract collocations. The measure we typically use is log-likelihood ratio (Dunning, 1993); see Pecina (2008) for an inventory of measures. Depending on these parameters, the automatically created collocation entry is more or less exhaustive (the output adapts to the specific user’s purpose). A different sub-entry is created for each part of speech of the sought word (for instance, report can either be a noun or a verb). Under each sub-entry, collocations are organised by syntactic type, e.g., adjectivenoun (comprehensive report), noun-noun (initiative report), subject-verb (report highlights), verb-object (produce a report). To avoid redundancy, only the collocating words are show"
W11-0819,2005.mtsummit-papers.11,0,0.0534882,"Missing"
W11-0819,E03-1022,1,0.702784,"ain system modules are the collocation extraction module, the search & visualisation module, the concordancing and the sentence alignment modules. The processing flow is pipelined. The key module of the system, collocation extraction, relies on a syntax-based methodology that combines lexical statistics with syntactic information provided by Fips, a deep symbolic parser (Wehrli, 2007). This methodology is fully described and evaluated in Seretan (2011). In principle, the extraction takes place only once, but new corpora can be processed later and results are cumulated. The sentence alignment (Nerima et al., 2003) is performed partially, i.e., only for the sentences actually displayed by the concordancing module. It is done on the fly, thus eliminating the need of pre-aligning the corpora. The role of the concordancing module is to present the sentence contexts for a selected collocation (cf. scenario described in §1). The words in this collocation are highlighted for readability. The list of sentences is displayed in the order given by the syntactic variation of collocations, that is, the collocation instances for which the distance between the components is larger are displayed first. This functional"
W11-0819,W09-0415,1,0.844878,"finds direct applicability in our on-going projects of large-scale multilingual syntac2 Europarl includes 11 languages: French, Italian, Spanish, Portuguese, English, Dutch, German, Danish, Swedish, Greek, Finnish. Note that our tool is not tailored to this specific corpus. 125 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 125–127, c Portland, Oregon, USA, 23 June 2011. 2011 Association for Computational Linguistics Figure 1: FipsCoView: System architecture. tic parsing (Wehrli, 2007) and syntax-based machine translation (Wehrli et al., 2009), the on-line version is designed to offer access to the derived collocation resources to a broader community. 2 Architecture and Main Functionalities Figure 1 shows the architecture of FipsCoView. The main system modules are the collocation extraction module, the search & visualisation module, the concordancing and the sentence alignment modules. The processing flow is pipelined. The key module of the system, collocation extraction, relies on a syntax-based methodology that combines lexical statistics with syntactic information provided by Fips, a deep symbolic parser (Wehrli, 2007). This met"
W11-0819,W07-1216,1,0.833967,"ressions (Seretan, 2009). While the off-line system finds direct applicability in our on-going projects of large-scale multilingual syntac2 Europarl includes 11 languages: French, Italian, Spanish, Portuguese, English, Dutch, German, Danish, Swedish, Greek, Finnish. Note that our tool is not tailored to this specific corpus. 125 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 125–127, c Portland, Oregon, USA, 23 June 2011. 2011 Association for Computational Linguistics Figure 1: FipsCoView: System architecture. tic parsing (Wehrli, 2007) and syntax-based machine translation (Wehrli et al., 2009), the on-line version is designed to offer access to the derived collocation resources to a broader community. 2 Architecture and Main Functionalities Figure 1 shows the architecture of FipsCoView. The main system modules are the collocation extraction module, the search & visualisation module, the concordancing and the sentence alignment modules. The processing flow is pipelined. The key module of the system, collocation extraction, relies on a syntax-based methodology that combines lexical statistics with syntactic information provid"
W11-0819,E03-1083,1,\N,Missing
W14-0804,J90-1003,0,0.126439,"Missing"
W14-0804,J82-3004,0,0.578381,"Missing"
W14-0804,D11-1067,0,0.150799,"Missing"
W14-0804,petrov-etal-2012-universal,0,0.0181043,"Missing"
W14-0804,J93-1007,0,0.431415,"Missing"
W14-0804,W10-3705,1,0.804405,"Missing"
W14-0804,W07-1216,1,\N,Missing
W15-2512,W14-3352,0,0.0431238,"cond Workshop on Discourse in Machine Translation (Hardmeier et al., 2015). We present the rulebased Machine Translation (MT) system Its-2 developed at the University of Geneva. A demo can be found here: http://latlapps.unige. ch/Translate? The interest for the pronoun translation task is at the heart of a line of research concerned with discourse phenomena and MT. Now, it is widely acknowledged that many remaining problems within MT can improve only if discourse knowledge, i.e., processing of phenomena beyond the sentence level, is taken into account (Webber and Joshi, 2012; Hardmeier, 2012; Joty et al., 2014). The problem of pronoun translation has its roots in the nature of anaphors. These are words empty of semantic content themselves, such as third person referential pronouns, which refer back to other a. Paul left two bikes in front of the house. When he came back, they were no longer there. The Problem of Pronoun Translation for English-French If sentence (1) is to be translated into French, one has the choice (mainly) between ils and elles for translating the pronoun they. This choice is no longer dependent on the English antecedent bikes, but on its translation in French either as the mascu"
W15-2512,J75-4037,0,0.626073,"Missing"
W15-2512,J94-4002,0,0.429919,"ky’s Binding Theory (1981), which is not an AR method per se, but rather a set of constraints useful to exclude otherwise potential antecedents. These constraints follow two principles: Principle A states that reflexive and reciprocal pronouns find their antecedents within their governing category (the smallest clause that includes them); Principle B states that 3rd person personal pronouns find their antecedents outside of the clause that includes them (Reinhart, 1983; Büring, 2005).4 Our strategy for anaphora resolution recalls in several ways the one used by Hobbs (1978) or Lappin & Leass (Lappin and Leass, 1994), adapted to the specific structures of the Fips parser. The algorithm comprises three steps: 1. impersonal pronouns The impersonal pronoun it in English – il in French – has no antecedent and should be excluded from further consideration by the AR procedure. The identification of impersonal pronouns is achieved on the basis of lexical 89 information (verbs lexically marked as impersonal, for instance meteorological verbs such as to rain or to snow), as well as syntactic information. For instance, adjectives which can take so-called sentential subjects occur with an impersonal subject when the"
W15-2512,E12-3001,0,0.102989,"Missing"
W15-2512,W10-1737,0,0.207943,"Missing"
W15-2512,2010.iwslt-papers.10,0,0.241175,"outside of the clause that contains them. We further restrict possible antecedents to arguments, excluding adjuncts noun phrases. The search for antecedents considers all preceding clauses within the sentence as well as within the previous sentence and makes an ordered list of the noun phrases which agree in number and gender with the pronoun.5 The w/ AR BLEU 22.43 w/o AR 22.44 it they it they Precision 0.1174 0.3631 0.0917 0.2710 Recall 0.1173 0.3481 0.0919 0.2566 Table 2: Contrastive results obtained from the test set. Precision and recall scores were computed using the automatic scorer by Hardmeier and Federico (2010). For the sake of completeness, a manual evaluation of two documents from the testset, amounting to 405 sentences or 203 pronouns, was completed. Two translations with and without the AR component were evaluated. The results are given in Table 3. It can be seen that the reflexive/reciprocal pronouns did not change between the two outputs. Besides, all observed errors were due to incorrect antecedent identification, leading to incorrect pronoun generation. One such a case is (9), where the 5 The n preceding sentences for finding an antecedent is a variable number (Klappholtz and Lockman, 1975)."
W15-2512,D13-1037,0,0.375111,"Missing"
W15-2512,W14-3312,0,0.0541794,"Missing"
W15-2512,P10-1142,0,0.0248302,"part because it agrees in gender or number. For example, in (1a), we are able to link they (pronoun) with bikes (antecedent) because they agree in number. This linking, or resolution, seems trivial for a human, but is not straightforward for a machine, especially if the antecedent and the anaphor are not in the same sentence and the text in question contains several sentences with several potential antecedents. Developing automatic Anaphora Resolution (AR) systems is a research domain on its own and has been active for decades (Mitkov, 2001; Mitkov, 2002; Strube, 2007; Stoyanov et al., 2009; Ng, 2010). In this paper we describe the rule-based MT system Its-2 developed at the University of Geneva and submitted for the shared task on pronoun translation organized within the Second DiscoMT Workshop. For improving pronoun translation, an Anaphora Resolution (AR) step based on Chomsky’s Binding Theory and Hobbs’ algorithm has been implemented. Since this strategy is currently restricted to 3rd person personal pronouns (i.e. they, it translated as elle, elles, il, ils only), absolute performance is affected. However, qualitative differences between the submitted system and a baseline without the"
W15-2512,W13-3307,0,0.207828,"Missing"
W15-2512,W15-2501,0,0.452884,"hora Resolution (AR) step based on Chomsky’s Binding Theory and Hobbs’ algorithm has been implemented. Since this strategy is currently restricted to 3rd person personal pronouns (i.e. they, it translated as elle, elles, il, ils only), absolute performance is affected. However, qualitative differences between the submitted system and a baseline without the AR procedure can be observed. 1 (1) Introduction 1.1 In this paper we describe the system submitted for the shared task on pronoun translation organized in conjunction with the EMNLP 2015 Second Workshop on Discourse in Machine Translation (Hardmeier et al., 2015). We present the rulebased Machine Translation (MT) system Its-2 developed at the University of Geneva. A demo can be found here: http://latlapps.unige. ch/Translate? The interest for the pronoun translation task is at the heart of a line of research concerned with discourse phenomena and MT. Now, it is widely acknowledged that many remaining problems within MT can improve only if discourse knowledge, i.e., processing of phenomena beyond the sentence level, is taken into account (Webber and Joshi, 2012; Hardmeier, 2012; Joty et al., 2014). The problem of pronoun translation has its roots in th"
W15-2512,P02-1040,0,0.105267,"ich this type of pronoun always refers to the subject of the sentence that contains it. In cases of embedded infinitive sentences, we assume the presence of an abstract subject pronoun (PRO, unrealized lexically) whose antecedent is determined by the control theory and ultimately by lexical information. For example, in the sentence Pauli promised Mary [PRO to take care of himselfi ], himself refers to the subject pronoun PRO, which in turn refers to the noun phrase Paul. 5 Results and Discussion The translation of the test set using the AR component does not have an impact on the BLEU scores (Papineni et al., 2002) (as expected). When measuring only the translations of pronouns, however, the AR component shows a positive effect when compared to a baseline without it, as shown in Table 2. Since these results are computed using exact word-level alignment matching between the candidate translation and an unique reference (Hardmeier et al., 2015), they are only indicative. 3. referential non-reflexive/reciprocal pronouns Such pronouns, currently restricted to the non-impersonal it, along with he, him, she, her, they, them, etc., undergo our simplified interpretation of Principle B, which means that they mus"
W15-2512,popescu-belis-etal-2012-discourse,0,0.0277556,"Missing"
W15-2512,J01-4004,0,0.42103,"Missing"
W15-2512,P09-1074,0,0.0266113,"Missing"
W15-2512,W12-3205,0,0.013294,"zed in conjunction with the EMNLP 2015 Second Workshop on Discourse in Machine Translation (Hardmeier et al., 2015). We present the rulebased Machine Translation (MT) system Its-2 developed at the University of Geneva. A demo can be found here: http://latlapps.unige. ch/Translate? The interest for the pronoun translation task is at the heart of a line of research concerned with discourse phenomena and MT. Now, it is widely acknowledged that many remaining problems within MT can improve only if discourse knowledge, i.e., processing of phenomena beyond the sentence level, is taken into account (Webber and Joshi, 2012; Hardmeier, 2012; Joty et al., 2014). The problem of pronoun translation has its roots in the nature of anaphors. These are words empty of semantic content themselves, such as third person referential pronouns, which refer back to other a. Paul left two bikes in front of the house. When he came back, they were no longer there. The Problem of Pronoun Translation for English-French If sentence (1) is to be translated into French, one has the choice (mainly) between ils and elles for translating the pronoun they. This choice is no longer dependent on the English antecedent bikes, but on its tran"
W15-2512,W09-0415,1,0.870571,"Missing"
W15-2512,W07-1216,1,0.660114,"Missing"
W15-2512,M95-1001,0,\N,Missing
W17-1706,W07-1216,1,0.745199,"nd contains fine grained information required by the parser. It is organized as a relational database with four main tables: • words, representing all morphological forms (spellings) of the words of a language, grouped into inflectional paradigms; The Fips parser • lexemes, describing more abstract lexical forms which correspond to the syntactic and semantic readings of a word (a lexeme corresponds roughly to a standard dictionary entry); Our system is a multilingual parser, available for several languages, i.e. French, English, German, Italian, Spanish, Modern Greek, Romanian and Portuguese (Wehrli, 2007; Wehrli and Nerima, 2015). It relies on generative grammar concepts and is basically made up of a generic parsing module which can be refined in order to suit the specific needs of a particular language. It is a constituent parser that functions as follows: it scans an input string from left to right, without any backtracking. The parsing algorithm, iteratively, performs the following three steps: • collocations, which describe multi-word expressions combining two lexical items, not counting function words; • variants, which list all the alternative written forms for a word, e.g. the written"
W17-1706,W14-0804,1,0.803583,"idioms to collocations with specific features, though convenient and appropriate for large classes of id3.3 Parsing and collocations 3.3.1 Collocation identification mechanism The collocation identification mechanism is integrated in the parser. In the present version of the parser, collocations, if present in the lexicon, are identified in the input sentence during the analysis of that sentence, rather than at the end. In this way, priority can be given to parsing alternatives involving collocations. Thus collocational information helps the parser through the maze of alternatives as shown in Wehrli (2014). To fulfil the goal of interconnecting the parsing procedure and the identification of collocations, we have incorporated the collocation identification mechanism within the constituent attachment procedure (see next section). Our parser, like many grammarbased parsers, uses left attachment and right attachment rules to build respectively left subconstituents and right subconstituents. The grammar used for the computational modelling comprises rules and procedures. Attachment rules describe the conditions under which constituents can combine, while procedures compute properties such as long-d"
W17-1706,zhang-kordoni-2006-automated,0,0.0475743,"d idioms. While the first three are expressions of lexical category 2 Multiword expressions: a brief review of related work The standard approach in dealing with MWEs in parsing is to apply a ‘words-with-spaces’ preprocessing step, which marks the MWEs in the input sentence as units which will later be integrated as 54 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 54–59, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics • (syntactically) interpret XP, triggering procedures single blocks in the parse tree built during analysis (Brun, 1998; Zhang and Kordoni, 2006). This method is not really adequate for processing collocations. Unlike other expressions that are fixed or semi-fixed, several collocation types do not allow a ‘words-with-spaces’ treatment because they have a high morphosyntactic flexibility. On the other hand, Alegria et al. (2004) and Villavicencio et al. (2007) adopted a compositional approach to the encoding of MWEs, able to capture more morphosyntactically flexible MWEs. Alegria et al. (2004) showed that by using a MWE processor in the preprocessing stage, a significant improvement in the POS tagging precision is obtained. However, as"
W17-1706,P98-1030,0,0.258795,"locations and idioms. While the first three are expressions of lexical category 2 Multiword expressions: a brief review of related work The standard approach in dealing with MWEs in parsing is to apply a ‘words-with-spaces’ preprocessing step, which marks the MWEs in the input sentence as units which will later be integrated as 54 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 54–59, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics • (syntactically) interpret XP, triggering procedures single blocks in the parse tree built during analysis (Brun, 1998; Zhang and Kordoni, 2006). This method is not really adequate for processing collocations. Unlike other expressions that are fixed or semi-fixed, several collocation types do not allow a ‘words-with-spaces’ treatment because they have a high morphosyntactic flexibility. On the other hand, Alegria et al. (2004) and Villavicencio et al. (2007) adopted a compositional approach to the encoding of MWEs, able to capture more morphosyntactically flexible MWEs. Alegria et al. (2004) showed that by using a MWE processor in the preprocessing stage, a significant improvement in the POS tagging precision"
W17-1706,P16-1016,0,0.0411045,"2004) showed that by using a MWE processor in the preprocessing stage, a significant improvement in the POS tagging precision is obtained. However, as argued by many researchers, e.g. (Heid, 1994; Seretan, 2011; Wehrli and Nerima, 2013), collocation identification is best performed on the basis of parsed material. This is due to the fact that collocations are co-occurrences of lexical items in a specific syntactic configuration. Additionally, Nasr et al. (2015) have developed a joint parsing and MWE identification model for the detection and representation of ambiguous complex function words. Constant and Nivre (2016) developed a transition-based parser which combines two factorized substructures: a standard tree representing the syntactic dependencies between the lexical elements of a sentence and a forest of lexical trees including MWE identified in the sentence. 3 – to build predicate-argument structures – to create chains linking preposed elements to their trace – to find the antecedent of (3rd person) personal pronouns The parsing procedure is a one pass (no preprocessing, no post-processing) scan of the input text, using rules to build up constituent structures and (syntactic) interpretation procedur"
W17-1706,P15-1108,0,0.0580334,"vicencio et al. (2007) adopted a compositional approach to the encoding of MWEs, able to capture more morphosyntactically flexible MWEs. Alegria et al. (2004) showed that by using a MWE processor in the preprocessing stage, a significant improvement in the POS tagging precision is obtained. However, as argued by many researchers, e.g. (Heid, 1994; Seretan, 2011; Wehrli and Nerima, 2013), collocation identification is best performed on the basis of parsed material. This is due to the fact that collocations are co-occurrences of lexical items in a specific syntactic configuration. Additionally, Nasr et al. (2015) have developed a joint parsing and MWE identification model for the detection and representation of ambiguous complex function words. Constant and Nivre (2016) developed a transition-based parser which combines two factorized substructures: a standard tree representing the syntactic dependencies between the lexical elements of a sentence and a forest of lexical trees including MWE identified in the sentence. 3 – to build predicate-argument structures – to create chains linking preposed elements to their trace – to find the antecedent of (3rd person) personal pronouns The parsing procedure is"
W17-1706,scherrer-etal-2014-swissadmin,1,0.861449,"trics. We measured the best F1 score from all possible matches between the set of MWE token ranks in the gold and system sentences by looking at all possible ways of matching MWEs in both sets. In the evaluation per MWE, our system achieved 0.4815 precision with a recall of 0.4680 and Fmeasure of 0.4746. In the evaluation per token, our system achieved 0.5865 precision with a recall of 0.5108 and F-measure of 0.5461. VMWEs annotation The Fips parser can produce several output formats: syntactic tree, tagger, XML/TEI, etc2 . We chose the Fips tagger output developed for the SwissAdmin project (Scherrer et al., 2014) because it gives all the necessary information for the VMWE annotation and, like in pasemetsv, it outputs one token per line. In short, each (Fips) token is displayed on one line, divided in six columns: the token, the Universal POS tag, the richer Fips tag, the lemma, the grammatical function / valency (if any), the collocation (if any)3 . The annotation of VMWEs is processed sentence by sentence and takes place as follows: the Fips output (aligned with the parsemetsv data file) is sequentially traversed line by line. For each verb token, the following tests are performed (in the following p"
W17-1706,D07-1110,0,0.0113404,"as 54 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 54–59, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics • (syntactically) interpret XP, triggering procedures single blocks in the parse tree built during analysis (Brun, 1998; Zhang and Kordoni, 2006). This method is not really adequate for processing collocations. Unlike other expressions that are fixed or semi-fixed, several collocation types do not allow a ‘words-with-spaces’ treatment because they have a high morphosyntactic flexibility. On the other hand, Alegria et al. (2004) and Villavicencio et al. (2007) adopted a compositional approach to the encoding of MWEs, able to capture more morphosyntactically flexible MWEs. Alegria et al. (2004) showed that by using a MWE processor in the preprocessing stage, a significant improvement in the POS tagging precision is obtained. However, as argued by many researchers, e.g. (Heid, 1994; Seretan, 2011; Wehrli and Nerima, 2013), collocation identification is best performed on the basis of parsed material. This is due to the fact that collocations are co-occurrences of lexical items in a specific syntactic configuration. Additionally, Nasr et al. (2015) hav"
W17-1706,W04-0407,0,\N,Missing
W97-0415,1996.amta-1.29,1,0.665613,"eech synthesis component. In the case of English output, most of the speech synthesis work relies on the DeeTalk system, although linguistic structures help to disambiguate non-homophonous homographs (read, lead, record, wind, etc.). The French speech output uses the MBROLA synthesizer developed by T. Dutoit, at the University of Mons. (4) [&apos;r~&quot; [DP I] would like It&gt;,, a room [con iP [vp with shower] and [vl, with view [pp on the garden]]]]] Several of the components used by ITSVox have been described elsewhere. For instance, the translation engine is based on the ITS-2 interactive model (cf. Wehrli, 1996). The GB-parser (French and English) have been discussed in cf. Laenzlinger & Wehrli, 1991, Wehrli, 1992. As for the French speech synthesis system, it is described in Gaudinat and Wehrli (1997). 2.1 The phonetic trie The phonetic lexicon is organized as a t r i e structure (Knuth, 19731, that is a tree structure in which nodes correspond to phonemes and subtrees to possible continuations. Each terminal node specifies one or more lexical entries in the lexical database. For instance, the phonetic sequence [sa] leads to a terminal node in the trie connected to the lexical entries corresponding"
