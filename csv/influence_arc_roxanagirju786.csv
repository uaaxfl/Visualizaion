2009.mtsummit-plenaries.4,2009.mtsummit-plenaries.5,1,0.801989,"Missing"
A00-1037,P98-2180,0,0.066364,"Missing"
A00-1037,C98-2175,0,\N,Missing
al-sabbagh-girju-2010-mining,E06-1047,0,\N,Missing
al-sabbagh-girju-2010-mining,W05-0708,0,\N,Missing
al-sabbagh-girju-2010-mining,P99-1067,0,\N,Missing
al-sabbagh-girju-2010-mining,P91-1048,0,\N,Missing
al-sabbagh-girju-2012-yadac,I11-1036,0,\N,Missing
al-sabbagh-girju-2012-yadac,W05-0708,0,\N,Missing
al-sabbagh-girju-2012-yadac,P11-2007,0,\N,Missing
al-sabbagh-girju-2012-yadac,W09-0807,0,\N,Missing
C14-1144,W11-0413,0,0.0152248,"temic modality2. To create 3arif, we design an interactive crowdsourcing annotation procedure that splits up the annotation process into a series of simplified questions, dispenses with the requirement for expert linguistic knowledge and captures nested modality triggers and their attributes semiautomatically. 1 Introduction Epistemic modality, according to Palmer (2001), defines the speaker's subjective knowledge, beliefs and judgments about the world's states of affairs. Epistemic modality is used as a linguistic feature for multiple NLP tasks and applications, including sentiment analysis (Abdul-Mageed and Diab 2011), opinion mining (Benamara et al. 2012) and scientific discourse evaluation (Waard and Maat 2012), among others. To-date, there are no large-scale modality-annotated Arabic corpora compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Chinese (Cui and Chi 2013), Portuguese (Hendrickx et al. 2012) and Japanese (Matsuyoshi et al. 2010). The creation of modality-annotated corpora is non-trivial because there is no consensus definition of modality and its attributes in theoretical linguistics to be rendered into annotation tasks and guidelines. Furthermore, most current modality a"
C14-1144,I13-1047,1,0.856202,"n 4 describes corpus harvesting and sampling; Section 5 discusses the results and presents a disagreement analysis; Section 6 compares and contrasts our work to related work; and Section 7 highlights the points not covered in this current version of 3arif. 2 Annotation Scheme Our annotation scheme consists of six tasks to label sense, polarity, intensification, tense, holders and scopes for each epistemic modality. Prior to the beginning of the interactive annotation procedure, we highlighted all candidate epistemic modalities in each tweet using a string-match algorithm and the lexicons from Al-Sabbagh et al. (2013, 2014). The algorithm finds all potential epistemic modality triggers (i.e. words and phrases that may convey epistemic modality) within each tweet in our corpus and marks them as annotation units. A total of 9966 candidate epistemic modality triggers are highlighted in 9822 tweets. 2.1 Task 1: Sense Sense annotation is to decide for each highlighted candidate trigger in context whether it actually conveys epistemic modality. The same lexical verb  اشعرA$Er is used as an epistemic modality trigger anticipating a future possibility in example 1; but as a non-modal lexical verb in example 2."
C14-1144,W14-0820,1,0.393957,"Missing"
C14-1144,J08-4004,0,0.102207,"ing method to 4 A total of 304 unique English and Arabic hashtags are found in the sampled corpus. 1527 measure reliability and agreement: for segments to be considered as agreement, they must share both the beginning and end boundaries. We use Krippendorff's alpha α (Krippendorff 2011) as our interannotator reliability measure, following the most recent work on modality annotation for other languages including English (Rubinstein et al. 2013) and Chinese (Cui and Chi 2013). For more details on Krippendorff's alpha and a comparison of inter-annotator agreement measures, we refer the reader to Artstein and Poesio (2008). 5.2 Results We use the surveygizmo services to implement our interactive annotation procedure given that their survey structure is one that allows for using conditional branching and skip logic5. We distributed the survey on Twitter and we had three annotators participating. According to the short qualifying quiz given at the beginning of the survey, all three participants are native Egyptian Arabic (EA) speakers who have at least two-year experience with using Twitter. They are also university graduates who, therefore, master Modern Standard Arabic. None of the participants has a linguistic"
C14-1144,baker-etal-2010-modality,0,0.632009,"edge and captures nested modality triggers and their attributes semiautomatically. 1 Introduction Epistemic modality, according to Palmer (2001), defines the speaker's subjective knowledge, beliefs and judgments about the world's states of affairs. Epistemic modality is used as a linguistic feature for multiple NLP tasks and applications, including sentiment analysis (Abdul-Mageed and Diab 2011), opinion mining (Benamara et al. 2012) and scientific discourse evaluation (Waard and Maat 2012), among others. To-date, there are no large-scale modality-annotated Arabic corpora compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Chinese (Cui and Chi 2013), Portuguese (Hendrickx et al. 2012) and Japanese (Matsuyoshi et al. 2010). The creation of modality-annotated corpora is non-trivial because there is no consensus definition of modality and its attributes in theoretical linguistics to be rendered into annotation tasks and guidelines. Furthermore, most current modality annotation schemes rely on sophisticated theoretically-grounded guidelines that require annotators from linguistics background; hence, annotation is usually restricted to small-scale in-lab settings. In this paper, we pr"
C14-1144,W12-3802,0,0.0148988,"n interactive crowdsourcing annotation procedure that splits up the annotation process into a series of simplified questions, dispenses with the requirement for expert linguistic knowledge and captures nested modality triggers and their attributes semiautomatically. 1 Introduction Epistemic modality, according to Palmer (2001), defines the speaker's subjective knowledge, beliefs and judgments about the world's states of affairs. Epistemic modality is used as a linguistic feature for multiple NLP tasks and applications, including sentiment analysis (Abdul-Mageed and Diab 2011), opinion mining (Benamara et al. 2012) and scientific discourse evaluation (Waard and Maat 2012), among others. To-date, there are no large-scale modality-annotated Arabic corpora compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Chinese (Cui and Chi 2013), Portuguese (Hendrickx et al. 2012) and Japanese (Matsuyoshi et al. 2010). The creation of modality-annotated corpora is non-trivial because there is no consensus definition of modality and its attributes in theoretical linguistics to be rendered into annotation tasks and guidelines. Furthermore, most current modality annotation schemes rely on sophisticated"
C14-1144,W13-0304,0,0.337924,"utes semiautomatically. 1 Introduction Epistemic modality, according to Palmer (2001), defines the speaker's subjective knowledge, beliefs and judgments about the world's states of affairs. Epistemic modality is used as a linguistic feature for multiple NLP tasks and applications, including sentiment analysis (Abdul-Mageed and Diab 2011), opinion mining (Benamara et al. 2012) and scientific discourse evaluation (Waard and Maat 2012), among others. To-date, there are no large-scale modality-annotated Arabic corpora compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Chinese (Cui and Chi 2013), Portuguese (Hendrickx et al. 2012) and Japanese (Matsuyoshi et al. 2010). The creation of modality-annotated corpora is non-trivial because there is no consensus definition of modality and its attributes in theoretical linguistics to be rendered into annotation tasks and guidelines. Furthermore, most current modality annotation schemes rely on sophisticated theoretically-grounded guidelines that require annotators from linguistics background; hence, annotation is usually restricted to small-scale in-lab settings. In this paper, we present 3arif, a large-scale Arabic corpus annotated for epis"
C14-1144,hendrickx-etal-2012-modality,0,0.294708,"duction Epistemic modality, according to Palmer (2001), defines the speaker's subjective knowledge, beliefs and judgments about the world's states of affairs. Epistemic modality is used as a linguistic feature for multiple NLP tasks and applications, including sentiment analysis (Abdul-Mageed and Diab 2011), opinion mining (Benamara et al. 2012) and scientific discourse evaluation (Waard and Maat 2012), among others. To-date, there are no large-scale modality-annotated Arabic corpora compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Chinese (Cui and Chi 2013), Portuguese (Hendrickx et al. 2012) and Japanese (Matsuyoshi et al. 2010). The creation of modality-annotated corpora is non-trivial because there is no consensus definition of modality and its attributes in theoretical linguistics to be rendered into annotation tasks and guidelines. Furthermore, most current modality annotation schemes rely on sophisticated theoretically-grounded guidelines that require annotators from linguistics background; hence, annotation is usually restricted to small-scale in-lab settings. In this paper, we present 3arif, a large-scale Arabic corpus annotated for epistemic modality. 3arif comprises 9822"
C14-1144,matsuyoshi-etal-2010-annotating,0,0.524106,"ositions as TRUE, (2) non-committed belief is when writers hold propositions as FALSE, and (3) not applicable is when propositions are not denoting beliefs at all. Interest is given to writers' beliefs only. Thus, a default value for the modality holder is the writer, and nested holders are not an1530 notated. Their corpus contains 10k words of running text from different domains and genres, including newswire, blog data, email and letter correspondence and transcribed dialogue data. Inter-annotator agreement rate is 0.95 including the NONE category where no belief markers exist. Baker et al. (2010, 2012) simultaneously annotate modality and modality-based negation to build modality taggers to enhance Urdu-English machine translation systems. Their annotation scheme distinguishes eight modality types: requirements, permissions, success, effort, intention, ability, desires and beliefs. Originally, their annotation scheme labels three attributes for each modality type: triggers, holders and targets (i.e. scopes). Yet, holders have not been eventually labeled. A unique feature of their annotation scheme is using a simplified operational procedure to label modality semantic meanings. The procedure"
C14-1144,W13-0306,0,0.222409,"us harvesting and sampling; Section 5 discusses the results and presents a disagreement analysis; Section 6 compares and contrasts our work to related work; and Section 7 highlights the points not covered in this current version of 3arif. 2 Annotation Scheme Our annotation scheme consists of six tasks to label sense, polarity, intensification, tense, holders and scopes for each epistemic modality. Prior to the beginning of the interactive annotation procedure, we highlighted all candidate epistemic modalities in each tweet using a string-match algorithm and the lexicons from Al-Sabbagh et al. (2013, 2014). The algorithm finds all potential epistemic modality triggers (i.e. words and phrases that may convey epistemic modality) within each tweet in our corpus and marks them as annotation units. A total of 9966 candidate epistemic modality triggers are highlighted in 9822 tweets. 2.1 Task 1: Sense Sense annotation is to decide for each highlighted candidate trigger in context whether it actually conveys epistemic modality. The same lexical verb  اشعرA$Er is used as an epistemic modality trigger anticipating a future possibility in example 1; but as a non-modal lexical verb in example 2. 1. 3["
C14-1144,W08-0606,0,0.0398206,"first asked whether the holder is the same as the Twitter user. If not, more questions are displayed to determine: (1) who the real holder is; (2) whether the tweet is a(n) (in)direct quote (e.g. there are direct quotation markers or such words as  قالqAl (he said) and  صرحSrH (he declared), among others), or the tweet conveys the Twitter user's assumptions about others. 1524 When the holder is not the same as the Twitter user, annotators are asked to mark the boundaries of the linguistic unit that corresponds to the holder in the tweet's text, following the maximal length principle from Szarvas et al. (2008), so that they mark the largest possible, meaningful linguistic unit. Hence, in example 8 the holder is the Islamist opponents in #KSA not only the Islamist opponents. 8. [مصر# السعودية موقنون أن]ھا تسعى لقتل الثورة في# اإلسالميون المعارضون في Al&lt;slAmywn AlmEArDwn fy #AlsEwdyp mwqnwn >n[hA tsEY lqtl Alvwrp fy #mSr] Islamist opponents in #KSA know for sure that [it tries to put an end to #Egypt's revolution]. 2.6 Task 6: Scope Scopes are the states of affairs modified by the epistemic modality triggers. Modality scopes in Arabic are most likely realized as clauses, deverbal nouns or to-in"
C14-1144,W12-4306,0,0.0321323,"Missing"
C14-1144,W09-3012,0,\N,Missing
D09-1146,D08-1038,0,0.0262081,"assentation that each topic is in some way shared across all collections. However, it does not explicitly model the similarities and differences between collections as we do in this research. In computational linguistics, topic models have been used in various applications, such as predicting response to political webposts (Yano et al., 2009), analyzing Enron and academic emails (McCallum et al., 2007a), analyzing voting records and corresponding text of resolutions from the U.S. Senate and the U.N. (McCallum et al., 2007b), as well as studying the history of ideas in various research fields (Hall et al., 2008; Paul and Girju, 2009). To our knowledge, the application of topic models to identifying cross-cultural differences is novel. 3 The Model In this section we first review the basic pLSI and LDA models. We then introduce our extension to LDA: cross-collection LDA (ccLDA). 3.1 Basic Topic Modeling The most basic generative model that assumes document topicality is the standard Na¨ıve Bayes model, where each document is assumed to belong to exactly one topic, and each topic is associated with a probability distribution over words (Mitchell, 1997). While this single-topic approach can be sufficien"
D09-1146,R09-1061,1,0.737693,"ch topic is in some way shared across all collections. However, it does not explicitly model the similarities and differences between collections as we do in this research. In computational linguistics, topic models have been used in various applications, such as predicting response to political webposts (Yano et al., 2009), analyzing Enron and academic emails (McCallum et al., 2007a), analyzing voting records and corresponding text of resolutions from the U.S. Senate and the U.N. (McCallum et al., 2007b), as well as studying the history of ideas in various research fields (Hall et al., 2008; Paul and Girju, 2009). To our knowledge, the application of topic models to identifying cross-cultural differences is novel. 3 The Model In this section we first review the basic pLSI and LDA models. We then introduce our extension to LDA: cross-collection LDA (ccLDA). 3.1 Basic Topic Modeling The most basic generative model that assumes document topicality is the standard Na¨ıve Bayes model, where each document is assumed to belong to exactly one topic, and each topic is associated with a probability distribution over words (Mitchell, 1997). While this single-topic approach can be sufficient for classification ta"
D09-1146,N09-1054,0,0.0122354,"n. Wang et al. recently introduced Markov topic models (MTM) (2009), a family of models which can simultaneously learn the topic structure of a single collection while discovering correlated topics in other collections. This is promising in that this type of model makes no assentation that each topic is in some way shared across all collections. However, it does not explicitly model the similarities and differences between collections as we do in this research. In computational linguistics, topic models have been used in various applications, such as predicting response to political webposts (Yano et al., 2009), analyzing Enron and academic emails (McCallum et al., 2007a), analyzing voting records and corresponding text of resolutions from the U.S. Senate and the U.N. (McCallum et al., 2007b), as well as studying the history of ideas in various research fields (Hall et al., 2008; Paul and Girju, 2009). To our knowledge, the application of topic models to identifying cross-cultural differences is novel. 3 The Model In this section we first review the basic pLSI and LDA models. We then introduce our extension to LDA: cross-collection LDA (ccLDA). 3.1 Basic Topic Modeling The most basic generative mode"
D10-1007,N10-1122,0,0.0398014,"LDA-style probabilistic topic models of document content (Blei et al., 2003) have been shown to offer state-of-the-art summarization quality. Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009). In our case, we want to add more structure to a model to incorporate the notion of viewpoint/perspective into our summaries. When it comes to extracting viewpoints, recent research suggests that it may be beneficial to model both topics and perspectives, as sentiment may be expressed differently depending on the issue involved (Brody and Elhadad, 2010; Paul and Girju, 2010). For example, let’s consider a set of product reviews for a home theater system. Content topics in this data might include things like sound quality, usability, etc., while the viewpoints might be the positive and negative sentiments. A word like speakers, for instance depends on the sound topic but not a viewpoint, while good would be an example of a word that depends on a viewpoint but not any particular topic. A word like loud would depend on both (since it would be considered positive sentiment only in the context of the sound quality topic), while a word like think"
D10-1007,N09-1057,0,0.0207838,"sing features returned by a dependency parser. For this, we used the Stanford parser1 , which returns dependency tuples of the form rel(a, b) where rel is some dependency relation and a and b are tokens of a sentence. We can use these specific tuples as features, referred here as the full-tuple representation. One problem with this representation is that we are using very specific information and it is harder for learning algorithms to find patterns due to the lack of redundancy. One solution is to generalize these features and rewrite a tuple rel(a, b) as two tuples: rel(a, ∗) and rel(∗, b) (Greene and Resnik, 2009; Joshi and Ros´e, 2009). We will refer to this as the split-tuple representation. 2.2.3 Negation If a word wi appears in the head of a neg relation, then we would like this to be reflected in other dependency tuples in which wi occurs. For a tuple rel(wi , wj ), if either wi or wj is negated, then we simply rewrite it as ¬rel(wi , wj ). An alternative would be to rewrite the individual word wi as ¬wi . However in our experiments this representation produced worse accuracies, perhaps because this produces less redundancy. 1 http://nlp.stanford.edu/software/ 2.2.4 Polarity We also hypothesize t"
D10-1007,N09-1041,0,0.122569,"model for more complex linguistic features extracted from text. These are 67 more discriminative than single word tokens and can improve the accuracy of extracting multiple viewpoints as we will show in the experimental results’ section. Below we first give a brief introduction to TAM and then present the proposed set of features. 2.1 Topic-Aspect Model (TAM) LDA-style probabilistic topic models of document content (Blei et al., 2003) have been shown to offer state-of-the-art summarization quality. Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009). In our case, we want to add more structure to a model to incorporate the notion of viewpoint/perspective into our summaries. When it comes to extracting viewpoints, recent research suggests that it may be beneficial to model both topics and perspectives, as sentiment may be expressed differently depending on the issue involved (Brody and Elhadad, 2010; Paul and Girju, 2010). For example, let’s consider a set of product reviews for a home theater system. Content topics in this data might include things like sound quality, usability, etc., while the viewpoints might be the positive and negativ"
D10-1007,P09-2079,0,0.0107585,"Missing"
D10-1007,N09-2029,0,0.0310303,"ontrastive macro and micro multi-view summaries in an unsupervised way, which is the goal of our work. For example, Hu and Liu (2006) rank sentences based on their dominant sentiment according to the polarity of adjectives occuring near a product feature in a sentence. A contradiction occurs when two sentences are highly unlikely to be simultaneously true (cf. (Marneffe et al., 2008)). Although little work has been done on contradiction detection, there are a few notable approaches (Harabagiu et al., 2006; Marneffe et al., 2008; Kim and Zhai, 2009). The closest work to ours is perhaps that of Lerman and McDonald (2009) who present an approach to contrastive summarization. They add an objective to their summarization model such that the summary model for one set of text is different from the model for the other set. The idea is to highlight the key differences between the sets, however this is a different type of contrast than the one we study here – our goal is instead to make the summaries similar to each other, to contrast how the same information is conveyed through different viewpoints. In this paper, we propose a two-stage approach to solving this novel summarization problem, which will be explained in"
D10-1007,W06-2915,0,0.0461805,"Missing"
D10-1007,W04-1013,0,0.0669811,"are more (rather than less) similar to each other. This contrastive version of our model-based baseline is formulated as: − k X KL(L(Sm1 )||L(Xm1 )) + m1 =1  1 k−1 P m2 ∈[1,k],m1 6=m2 KL(L(Sm1 )||L(Xm2 ))  Our summary generation algorithm is to iteratively add excerpts to the summary in a greedy fashion, selecting the excerpt with the highest score in each iteration. Note that this approach only generates macro-level summaries, leaving us with the LexRank baseline for micro-level summaries. 4.3.3 Metrics We will evaluate our summaries using a variant of the standard ROUGE evaluation metric (Lin, 2004). Recall that we have two different evaluation sets – one that contains all of the reasons for each view73 point, and one that consists only of aligned pairs of excerpts. Since the same excerpt may appear in multiple pairs, there would be significant redundancy in our reference summary if we were to include every pair. Thus, we will restrict a contrastive reference summary to exclude overlapping pairs, and we will have many reference sets for all possible combinations of pairs. There is only one reference set for the representativeness criterion. Our reference summaries have a unique property"
D10-1007,P08-1118,0,0.070202,"Missing"
D10-1007,H05-1044,0,0.00808263,"iments this representation produced worse accuracies, perhaps because this produces less redundancy. 1 http://nlp.stanford.edu/software/ 2.2.4 Polarity We also hypothesize that lexical polarity information may improve our model. If we are using the full-tuple representation, then a tuple becomes more general by replacing the specific word with a + or −. In the case that both words are polarity words, we use two tuples, replacing only one word at a time rather than replacing both words with their polarity signs. To determine the polarity of a word, we simply use the Subjectivity Clues lexicon (Wilson et al., 2005) and as polarity values, positive (+), negative (-), and neutral (*). Under our split-tuple representation, this becomes more specific by replacing the ∗ with the polarity sign. For example, the tuple amod(idea, good) would be represented as amod(idea, +) and amod(∗, good). We collapse negated features to flip the polarity sign such that ¬rel(a, +) becomes rel(a, −). 2.2.5 Generalized Relations We also experimented with backing off the relations themselves. Since the Stanford dependencies can be organized in a hierarchy2 , we will represent the relations at more generalized levels in the hiera"
I13-1047,matsuyoshi-etal-2010-annotating,0,0.220085,"Missing"
I13-1047,abdul-mageed-diab-2012-awatif,0,0.0725076,"n English, Portuguese, Japanese and Chinese. Baker et al. (2010) used an idiosyncratic categorization of English modality that distinguished 8 semantic meanings: requirement, permissive, success, effort, intension, ability, want and belief. They defined each type as a pattern of the form H (modal) P where H is the sentence’s agent and P is the proposition (e.g. H permits [P to be true/false]). They obtained an average interannotator agreement rate of 0.82. The error analysis of their modality tagger showed that errors resulted primarily from the triggers’ lexical ambiguity. Prabhakaran et al. (2012) focused on 5 semantic meanings of English modality, and used the same HP patterns as Baker et al. (2010) for annotation guidelines. They reported an interannotator agreement rate of 0.95. Their modality tagger yielded a 0.44 F1 score against a goldstandard and 0.79 and 0.91 F1 scores against different testing sets from their crowdsourced data. Rubinstein et al. (2013) used a more standardized typology of English modality that entailed (1) priority modality divided into bouletic, teleological and deontic triggers; and (2) non-priority modality divided into epistemic, circumstantial and ability"
I13-1047,al-sabbagh-girju-2012-yadac,1,0.908051,"ty meanings. Third, implicit scopes are common in Arabic and annotators have to be made aware of them. Fourth, Arabic word order flexibility allows triggers - especially adverbials - to occur in the scope’s initial, medial or final positions, which makes it challenging for annotators to identify scope spans. Finally, modality scopes are not necessarily adjacent to their triggers, which furthermore complicates the detection of their spans. The tweets genre on which we work poses an additional challenge due to language variation. We select a random sample of Arabic tweets from the YADAC corpus (Al-Sabbagh and Girju, 2012) posted in Egypt during the first six months 410 International Joint Conference on Natural Language Processing, pages 410–418, Nagoya, Japan, 14-18 October 2013. of the 2011 revolution. All selected tweets are about the political situation at that time. Tweets are not only in the Egyptian Arabic (EA) dialect, but also in Modern Standard Arabic (MSA), especially tweets from press agencies and celebrities. Therefore, our annotation scheme has to work on both MSA and EA modality. Arabic and the tweets genre are not the only original aspects of this paper. We present a novel linguistically-motivat"
I13-1047,baker-etal-2010-modality,0,0.363384,"dality experiencers) and scopes (i.e. the range of linguistic constituents modified by the modality triggers), and (2) automatically detect power relations among participants in the social network of Twitter by using this modality information. Despite solid work on Arabic modality in theoretical linguistics (Mitchell and al-Hassan, 1994; Brustad, 2000; Moshref, 2012), there are no Arabic corpora annotated for modality, not even the widely used Penn Arabic Treebank. However, there is a plethora of work and annotated corpora for modality in other languages, including English (Saurí et al, 2006; Baker et al., 2010; Prabhakaran et al., 2012; Rubinstein et al., 2013), Portuguese (Hendrickx et al., 2010; Avila and Mello, 2013), Japanese (Matsuyoshi et al. 2010) and Chinese (Cui and Chi, 2013). Arabic modality annotation involves multiple challenges. First, the paradigm of Arabic modality triggers is complex as it includes auxiliaries, lexical verbs, nominals and particles - like many other languages as well. Second, triggers can be lexically and/or semantically ambiguous: a lexically-ambiguous trigger is a word or phrase that may or may not convey a modality meaning based on context. A semantically-ambigu"
I13-1047,W12-3302,0,0.0224361,"Missing"
I13-1047,W12-3807,0,\N,Missing
I13-1047,hendrickx-etal-2012-modality,0,\N,Missing
J06-1005,P99-1008,0,0.938571,"from them. For example, oasis–desert and Guadalupe Mountains National Park–Texas are PLACE – AREA relations. In this paper we use the Winston, Chaffin, and Hermann classification as a criterion for building the training corpus to provide a wide coverage of such subtypes of part– whole relations. 86 Girju, Badulescu, and Moldovan Automatic Discovery of Part–Whole Relations In computational linguistics, although a considerable amount of work has been done on semantic relation detection,7 the work most similar to the task of identifying part–whole semantic relations is that of Hearst (1992) and Berland and Charniak (1999). Hearst developed a method for the automatic acquisition of hypernymy relations by identifying a set of frequently used and mostly unambiguous lexico-syntactic patterns. For example, countries, such as England indicates a hypernymy relation between the words countries and England. In her paper, she mentions that she tried applying the same method to meronymy, but without much success, as the patterns detected also expressed other semantic relations. This is consistent with our study of part–whole lexico-syntactic patterns presented in this paper. In 1999, Berland and Charniak applied statisti"
J06-1005,J95-4004,0,0.0738293,"Missing"
J06-1005,A00-2018,0,0.00966401,"my girl does not). The ambiguity of these patterns explains our rationale for choosing an approach based on a machine learning method to discover discriminating rules automatically. 3. Lexico-Syntactic Patterns that Express Meronymy The automatic discovery of any semantic relation must start with a thorough understanding of the lexical and syntactic forms used to express that relation. Since there are many ways in which something can be part of something else, there is a variety of lexico-syntactic structures that can express a meronymy semantic relation. 7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, and Manning 2003), considerable interest has been shown in the automatic interpretation of various noun phrase-level constructions, such as noun compounds. The focus here is to determine the semantic relations that link the two noun constituents. The best-performing noun compound interpretation systems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques (Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc, domain-specific, hand-coded semantic taxonomies, or on statistical"
J06-1005,J93-1003,0,0.121771,"Missing"
J06-1005,J02-3001,0,0.107462,"t). The ambiguity of these patterns explains our rationale for choosing an approach based on a machine learning method to discover discriminating rules automatically. 3. Lexico-Syntactic Patterns that Express Meronymy The automatic discovery of any semantic relation must start with a thorough understanding of the lexical and syntactic forms used to express that relation. Since there are many ways in which something can be part of something else, there is a variety of lexico-syntactic structures that can express a meronymy semantic relation. 7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, and Manning 2003), considerable interest has been shown in the automatic interpretation of various noun phrase-level constructions, such as noun compounds. The focus here is to determine the semantic relations that link the two noun constituents. The best-performing noun compound interpretation systems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques (Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc, domain-specific, hand-coded semantic taxonomies, or on statistical patterns in a large corpu"
J06-1005,N03-1011,1,0.888805,"Missing"
J06-1005,C92-2082,0,0.741029,"Missing"
J06-1005,J93-2004,0,0.0283044,"Missing"
J06-1005,H05-1112,1,0.735372,"the verb to have has the sense of to possess, and only in some particular contexts refers to meronymy. Table 5 presents a summary of some of the most frequent part–whole lexicosyntactic patterns we observed, classified based on their ambiguity. Below we discuss further the ambiguities encountered in the patterns of the first three clusters. The Semantic Ambiguity of Genitive Constructions In English there are two kinds of genitives: the s-genitive and the of-genitive. A characteristic of the genitives is that they are very ambiguous, as the constructions can be given various interpretations (Moldovan and Badulescu 2005). For instance, genitives can encode relations such as PART– WHOLE (Mary’s hand), POSSESSION (Mary’s car), KINSHIP (Mary’s sister), PROPERTY / ATTRIBUTE HOLDER (Mary’s beauty), DEPICTION – DEPICTED (Mary’s painting — if it depicts her), SOURCE - FROM (Mary’s birth city), or Table 5 Examples of meronymic expressions based on their ambiguity. Types of Part–Whole Expressions Positive Examples (part–whole) Unambiguous The parts of an airplane include the engine, .. The substance consists of three ingredients. One of the air’s constituents is oxygen. The cloud was made of dust. Iceland is a member"
J06-1005,W04-2609,1,0.888751,"Missing"
J06-1005,W04-2607,0,0.00916935,"f word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns. 1. Introduction The identification of semantic relations in text is at the core of Natural Language Processing and many of its applications. Detecting semantic relations between various text segments, such as phrases, sentences, and discourse spans, is important for automatic text understanding (Rosario, Hearst, and Fillmore 2002; Lapata 2002; Morris and Hirst 2004). Furthermore, semantic relations represent the core elements in the organization of lexical semantic knowledge bases intended for inference purposes. Recently, there has been a renewed interest in text semantics as evidenced by the international ∗ Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL 61801, E-mail: girju@uiuc.edu. † Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail: adriana@languagecomputer.com. ‡ Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail: moldovan@lan"
J06-1005,W04-0848,1,0.743602,"Missing"
J06-1005,J93-2005,0,0.0427892,"Missing"
J06-1005,W93-0307,0,0.0402405,"Missing"
J06-1005,W01-0511,0,0.154961,"Missing"
J06-1005,P02-1032,0,0.0130828,"Missing"
J06-1005,H05-1047,1,0.642971,"Missing"
J06-1005,C94-2125,0,0.0109779,"many ways in which something can be part of something else, there is a variety of lexico-syntactic structures that can express a meronymy semantic relation. 7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, and Manning 2003), considerable interest has been shown in the automatic interpretation of various noun phrase-level constructions, such as noun compounds. The focus here is to determine the semantic relations that link the two noun constituents. The best-performing noun compound interpretation systems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques (Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc, domain-specific, hand-coded semantic taxonomies, or on statistical patterns in a large corpus of examples, respectively. 8 The North American News Corpus (NANC) of 1 million words. 87 Computational Linguistics Volume 32, Number 1 There are unambiguous lexical expressions that always convey a part–whole relation. For example: (2) The substance consists of three ingredients. (3) The cloud was made of dust. (4) Iceland is a member of NATO. In these cases the simple detecti"
J06-1005,J02-3004,0,\N,Missing
J09-2003,P98-1013,0,0.218142,"Missing"
J09-2003,W06-2110,0,0.179577,"Missing"
J09-2003,calzolari-etal-2002-towards,0,0.0570837,"s Volume 35, Number 2 in transitive sentential contexts (i.e., those sentences containing a transitive verb) are mapped onto the selected set of constructions based on lexical similarity over the verbs. However, although the Web-based solution might overcome the data sparsity problem, current probabilistic models are limited because they do not take full advantage of the structure and the meaning of language. From a cross-linguistic perspective, there hasn’t been much work on the automatic interpretation of nominal phrases and compounds. Busa and Johnston (1996), Johnston and Busa (1996), and Calzolari et al. (2002), for example, focus on the differences between English and Italian noun–noun compounds. In their work they argue that a computational approach to the cross-linguistic interpretation of these compounds has to rely on a rich lexical representation model, such as those provided by FrameNet frames (Baker, Fillmore, and Lowe 1998) and qualia structure (Pustejovsky 1995). In the qualia structure representation, for example, the meaning of a lexical concept, such as the modiﬁer in a noun–noun compound, is deﬁned in terms of four elements representing concept attributes along with their use and purpo"
J09-2003,A00-2018,0,0.104355,"Missing"
J09-2003,J06-1005,1,0.844652,"Missing"
J09-2003,W96-0309,0,0.123657,". 189 Computational Linguistics Volume 35, Number 2 in transitive sentential contexts (i.e., those sentences containing a transitive verb) are mapped onto the selected set of constructions based on lexical similarity over the verbs. However, although the Web-based solution might overcome the data sparsity problem, current probabilistic models are limited because they do not take full advantage of the structure and the meaning of language. From a cross-linguistic perspective, there hasn’t been much work on the automatic interpretation of nominal phrases and compounds. Busa and Johnston (1996), Johnston and Busa (1996), and Calzolari et al. (2002), for example, focus on the differences between English and Italian noun–noun compounds. In their work they argue that a computational approach to the cross-linguistic interpretation of these compounds has to rely on a rich lexical representation model, such as those provided by FrameNet frames (Baker, Fillmore, and Lowe 1998) and qualia structure (Pustejovsky 1995). In the qualia structure representation, for example, the meaning of a lexical concept, such as the modiﬁer in a noun–noun compound, is deﬁned in terms of four elements representing concept attributes a"
J09-2003,I05-1082,0,0.426937,"Missing"
J09-2003,P06-2064,0,0.0248228,"Missing"
J09-2003,P95-1007,0,0.406891,"of tea translate only as the N P N instance cea¸sc˘a de ceai (‘cup of tea’), while sailor suit translates as costum de marinar (‘suit of sailor’) and the suit of the sailor as the genitive-marked N N costumul marinarului (‘suit-the sailor-GEN’). Thus, we study the distribution of semantic relations across different nominal phrases and compounds in one language and across all six languages, and analyze the resulting similarities and differences. This distribution is evaluated over the two different corpora based on two state-of-the-art classiﬁcation tag sets: Lauer’s set of eight prepositions (Lauer 1995) and our list of 22 semantic relations. A mapping between the two tag sets is also provided. In order to test their contribution to the task of semantic interpretation, prepositions and other linguistic clues are employed as features in a supervised, knowledgeintensive model. Furthermore, given a training set of English nominal phrases and compounds along with their translations in the ﬁve Romance languages, our algorithm automatically learns classiﬁcation rules and applies them to unseen test instances for semantic interpretation. As training and test data we used 3,124 Europarl and 2,023 CLU"
J09-2003,meyers-etal-2004-cross,0,0.0434859,"Missing"
J09-2003,W04-0838,0,0.0285511,"Missing"
J09-2003,H05-1112,0,0.406706,"en a training set of English nominal phrases and compounds along with their translations in the ﬁve Romance languages, our algorithm automatically learns classiﬁcation rules and applies them to unseen test instances for semantic interpretation. As training and test data we used 3,124 Europarl and 2,023 CLUVI token instances. These instances were annotated with semantic relations and analyzed for inter-annotator agreement. The results are compared against two 187 Computational Linguistics Volume 35, Number 2 state-of-the-art approaches: a supervised machine learning model, semantic scattering (Moldovan and Badulescu 2005), and a Web-based unsupervised model (Lapata and Keller 2005). Moreover, we show that the Romanian linguistic features contribute more substantially to the overall performance than the features obtained for the other Romance languages. This is explained by the fact that the choice of the linguistic constructions (either genitive-marked N N or N P N) in Romanian is highly correlated with their meaning. The article is organized as follows. Section 2 presents a summary of related work. In Section 3 we describe the general approach to the interpretation of nominal phrases and compounds and list th"
J09-2003,W04-2609,1,0.919925,"Missing"
J09-2003,W05-0603,0,0.145217,"Missing"
J09-2003,W03-0411,0,0.219192,"Missing"
J09-2003,P06-1015,0,0.0215145,"Missing"
J09-2003,N04-1041,0,0.0483798,"Missing"
J09-2003,P06-1100,0,0.0459832,"Missing"
J09-2003,pustejovsky-etal-2006-towards,0,0.0138896,"FrameNet frames (Baker, Fillmore, and Lowe 1998) and qualia structure (Pustejovsky 1995). In the qualia structure representation, for example, the meaning of a lexical concept, such as the modiﬁer in a noun–noun compound, is deﬁned in terms of four elements representing concept attributes along with their use and purpose. Thus, qualia structure provides a relational structure that enables the compositional interpretation of the modiﬁer in relation to the head noun. Two implementations of such representations are provided by the SIMPLE Project ontology (Lenci et al. 2000) and the OMB ontology (Pustejovsky et al. 2006). The SIMPLE ontology, for example, is developed for 12 European languages and deﬁnes entry words that are mapped onto high-level concepts in EuroWordNet (Vossen 1998), a version of WordNet developed for European languages. In this article, we use a supervised semantic interpretation model employing rich linguistic features generated from corpus evidence coupled with word sense disambiguation and WordNet concept structure information. The results obtained are compared against two state-of-the-art approaches: a supervised machine learning model, semantic scattering (Moldovan and Badulescu 2005)"
J09-2003,P02-1032,0,0.0333642,"Missing"
J09-2003,H05-1047,0,0.0263683,"Missing"
J09-2003,P06-1040,0,0.0247389,"Missing"
J09-2003,W06-2112,0,0.185113,"Missing"
J09-2003,bel-etal-2000-simple,0,\N,Missing
J09-2003,J09-2001,0,\N,Missing
J09-2003,C98-1013,0,\N,Missing
J09-2003,J02-3004,0,\N,Missing
J09-2003,N03-1011,1,\N,Missing
J09-2003,W01-0511,0,\N,Missing
N03-1011,P99-1008,0,0.84155,"Missing"
N03-1011,A00-2018,0,0.00477532,"cept, we manually annotate it with its corresponding sense in WordNet, for example carving knife 1 means sense number 1. 3.3 Building the Training Corpus and the Test Corpus In order to learn the constraints, we used the SemCor 1.7 and TREC 9 text collections. From the first two sets of the SemCor collection, 19,000 sentences were selected. Another 100,000 sentences were extracted from the LA Times articles of TREC 9. A corpus “A” was thus created from the selected sentences of each text collection. Each sentence in this corpus was then parsed using the syntactic parser developed by Charniak (Charniak, 2000). Focusing only on the sentences containing relations indicated by the three patterns considered, we manually annotated all the noun phrases in the 53,944 relationships matched by these patterns with their corresponding senses in WordNet (with the exception of those from SemCor). 6,973 of these relationships were part-whole relations, while 46,971 were not meronymic relations. We used for training a corpus of 34,609 positive examples (6,973 pairs of NPs in a part-whole relation extracted from the corpus “A” and 27,636 extracted from WordNet as selected pairs) and 46,971 negative examples (the"
N16-1047,W11-1902,0,0.0944782,"Missing"
N16-1047,P09-2078,0,0.70317,"linois Urbana, IL 61801, USA girlea2@illinois.edu Roxana Girju University of Illinois Urbana, IL 61801, USA girju@illinois.edu Abstract We tackle the problem of identifying deceptive agents in highly-motivated high-conflict dialogues. We consider the case where we only have textual information. We show the usefulness of psycho-linguistic deception and persuasion features on a small dataset for the game of Werewolf. We analyse the role of syntax and we identify some characteristics of players in deceptive roles. 1 Introduction Deception detection has gained some attention in the NLP community (Mihalcea and Strapparava, 2009; Ott et al., 2011; Jindal and Liu, 2008). The focus has mostly been on detecting insincere reviews or arguments. However, there has been little work (Hung and Chittaranjan, 2010) in detecting deception and manipulation in dialogues. When the agents involved in a dialogue have conflicting goals, they are often motivated to use deception and manipulation in order to reach those goals. Examples include trials and negotiations. The high motivation for using deception and the possiblity of tracking the effects on participants throughout the dialogue sets this problem apart from identifying decepti"
N16-1047,W11-1709,0,0.0260749,"ta. The resulting data set consists of 701 instances, of which 116 are instances of a werewolf role. Given the small size and the skewed distribution of the dataset, we balanced the data with resampling so that we have enough instances to learn from. 3.2 Features 3.2.1 Psycholinguistic Features (Tausczik and Pennebaker, 2010) suggest word count and use of negative emotions, motion, and sense words are indicative of deception. We counted the negative emotion words using the MPQA subjectivity lexicon of (Wilson et al., 2005). We also experimented with the NRC wordemotion association lexicon of (Mohammad and Yang, 2011), but found the MPQA lexicon to perform better. Since we didn’t have access to LIWC (Tausczik and Pennebaker, 2010), we used manually created lists of motion (arrive, run, walk) and sense (see, sense, appearance) words. The lists are up to 50 words long. We also considered the number of verbs, based on our intuition that heavy use of verbs can be associated to motion. However, we don’t expect the number of motion words to be as important in our domain. This is because deception in the Werewolf game does not refer to a fabricated story that other players have to be convinced to believe, but rat"
N16-1047,P11-1032,0,0.48197,"lea2@illinois.edu Roxana Girju University of Illinois Urbana, IL 61801, USA girju@illinois.edu Abstract We tackle the problem of identifying deceptive agents in highly-motivated high-conflict dialogues. We consider the case where we only have textual information. We show the usefulness of psycho-linguistic deception and persuasion features on a small dataset for the game of Werewolf. We analyse the role of syntax and we identify some characteristics of players in deceptive roles. 1 Introduction Deception detection has gained some attention in the NLP community (Mihalcea and Strapparava, 2009; Ott et al., 2011; Jindal and Liu, 2008). The focus has mostly been on detecting insincere reviews or arguments. However, there has been little work (Hung and Chittaranjan, 2010) in detecting deception and manipulation in dialogues. When the agents involved in a dialogue have conflicting goals, they are often motivated to use deception and manipulation in order to reach those goals. Examples include trials and negotiations. The high motivation for using deception and the possiblity of tracking the effects on participants throughout the dialogue sets this problem apart from identifying deception in nonce text f"
N16-1047,H05-1044,0,0.0179977,"or whether the player is or isn’t a werewolf. We do not consider the judge as part of the data. The resulting data set consists of 701 instances, of which 116 are instances of a werewolf role. Given the small size and the skewed distribution of the dataset, we balanced the data with resampling so that we have enough instances to learn from. 3.2 Features 3.2.1 Psycholinguistic Features (Tausczik and Pennebaker, 2010) suggest word count and use of negative emotions, motion, and sense words are indicative of deception. We counted the negative emotion words using the MPQA subjectivity lexicon of (Wilson et al., 2005). We also experimented with the NRC wordemotion association lexicon of (Mohammad and Yang, 2011), but found the MPQA lexicon to perform better. Since we didn’t have access to LIWC (Tausczik and Pennebaker, 2010), we used manually created lists of motion (arrive, run, walk) and sense (see, sense, appearance) words. The lists are up to 50 words long. We also considered the number of verbs, based on our intuition that heavy use of verbs can be associated to motion. However, we don’t expect the number of motion words to be as important in our domain. This is because deception in the Werewolf game"
P01-1037,A00-1023,0,\N,Missing
P01-1037,C00-1043,1,\N,Missing
P01-1037,H94-1052,0,\N,Missing
P01-1037,P00-1071,1,\N,Missing
P01-1037,A00-1025,0,\N,Missing
P01-1037,P96-1025,0,\N,Missing
P01-1037,A00-1021,0,\N,Missing
P01-1037,P95-1037,0,\N,Missing
P01-1037,A00-1041,0,\N,Missing
P07-1072,W07-1527,1,0.797542,"I/ 570 added to the list. The two computational semantics annotators had to tag each English constituent noun with its corresponding WordNet sense and each instance with the corresponding semantic category. If the word was not found in WordNet the instance was not considered. Whenever the annotators found an example encoding a semantic category other than those provided or they didn’t know what interpretation to give, they had to tag it as “OTHER - SR”, and respectively “OTHER - PP”3 . The details of the annotation task and the observations drawn from there are presented in a companion paper (Girju, 2007). The corpus instances used in the corpus analysis phase have the following format: <NPEn ;NPEs ; NP It ; NP F r ; NP P ort ; NP Ro ; target&gt;. The word target is one of the 23 (22 + OTHER - SR) semantic relations and one of the eight prepositions considered or OTHER - PP (with the exception of those N P N instances that already contain a preposition). For example, <development cooperation; cooperaci´on para el desarrollo; cooperazione allo sviluppo; coop´eration au d´eveloppement; cooperare pentru dezvoltare; PURPOSE / FOR&gt;. The annotators’ agreement was measured using r(E) Kappa statistics: K"
P07-1072,P06-2064,0,0.245661,"the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS - A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of deep linguistic information. In this paper we investigate 3.2 The data the role of cross-linguistic information in the task of English NP semantic interpretation and show the The data was collected from two text collections with different distributions and of different genre, importance of a set of novel linguistic features. 569 POSSESSION (famil"
P07-1072,N04-1016,0,0.405823,"-art classification tag sets: Lauer’s set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. We show that various crosslingual cues can help in the NP interpretation task when employed in an SVM model. The results are compared against two state of the art approaches: a suProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics pervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a webbased probabilistic model (Lapata and Keller, 2004). The paper is organized as follows. In Section 2 we present a summary of the previous work. Section 3 lists the syntactic and semantic interpretation categories used along with observations regarding their distribution on the two different cross-lingual corpora. Sections 4 and 5 present a learning model and results for the interpretation of English noun phrases. Finally, in Section 6 we offer some discussion and conclusions. 3 Corpus Analysis For a better understanding of the meaning of the N N and N P N instances, we analyzed the semantic behavior of these constructions on a large crosslingu"
P07-1072,P95-1007,0,0.439252,"ian. The focus on Romance languages is well motivated. It is mostly true that English noun phrases translate into constructions of the form N P N in Romance languages where, as we will show below, the P (preposition) varies in ways that correlate with the semantics. Thus Romance languages will give us another source of evidence for disambiguating the semantic relations in English NPs. We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauer’s set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. We show that various crosslingual cues can help in the NP interpretation task when employed in an SVM model. The results are compared against two state of the art approaches: a suProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics pervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a webbased probabilistic model (Lapata and Keller, 2004). The paper is organized as follows. In Section 2"
P07-1072,meyers-etal-2004-cross,0,0.0272516,"ecifies if the head (F3) and the modifier (F4) nouns are related to a corresponding WordNet verb (e.g. statement derived from to state; cry from to cry). F5. Prepositional cues that link the two nouns in an NP. These can be either simple or complex prepositions such as “of” or “according to”. In case of N N instances, this feature is “–” (e.g., framework law). F6 and F7. Type of nominalized noun indicates the specific class of nouns the head (F6) or modifier (F7) belongs to depending on the verb it derives from. First, we check if the noun is a nominalization. For English we used NomLex-Plus (Meyers et al., 2004) to map nouns to corresponding verbs.5 For example, “destruction of the city”, where destruction is a nominalization. F6 and F7 may overlap with features F3 and F4 which are used in case the noun to be checked does not have an entry in the NomLex-Plus dictionary. These features are of particular importance since they impose some constraints on the possible set of relations the instance can encode. They take the following values (identified based on list of verbs extracted from VerbNet (Kipper et al., 2000)): a. Active form nouns which have an intrinsic active voice predicate-argument structure"
P07-1072,W04-0838,0,0.0149575,"nstances, this feature is “–”. 4.2 Learning Models We have experimented with the support vector machines (SVM) model6 and compared the results against two state-of-the-art models: a supervised model, Semantic Scattering (SS), (Moldovan and Badulescu, 2005), and a web-based unsupervised model (Lapata and Keller, 2004). The SVM and SS models were trained and tested on the Europarl and CLUVI corpora using a 8:2 ratio. The test dataset was randomly selected from each corpus and the test nouns (only for English) were tagged with the corresponding sense in context using a state of the art WSD tool (Mihalcea and Faruque, 2004). After the initial NP instances in the training and test corpora were expanded with the corresponding features, we had to prepare them for SVM and SS. The method consists of a set of automatic iterative procedures of specialization of the English nouns on the WordNet IS - A hierarchy. Thus, after a set of necessary specialization iterations, the method produces specialized examples which through supervised machine learning are transformed into sets of semantic rules. This specialization procedure improves the system’s performance since it efficiently separates the positive and negative noun-n"
P07-1072,H05-1112,0,0.18694,"of noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauer’s set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. We show that various crosslingual cues can help in the NP interpretation task when employed in an SVM model. The results are compared against two state of the art approaches: a suProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics pervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a webbased probabilistic model (Lapata and Keller, 2004). The paper is organized as follows. In Section 2 we present a summary of the previous work. Section 3 lists the syntactic and semantic interpretation categories used along with observations regarding their distribution on the two different cross-lingual corpora. Sections 4 and 5 present a learning model and results for the interpretation of English noun phrases. Finally, in Section 6 we offer some discussion and conclusions. 3 Corpus Analysis For a better understanding of the meaning of the N N and N P N instances, we analyzed the"
P07-1072,P06-1015,0,0.00793301,"coupled with a large lexical hierarchy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst, 2001). On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS - A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of deep linguistic information. In this paper we investigate 3.2 The data the role of cross-linguistic information in the task of English NP semantic interpretation and show the The"
P07-1072,P06-1101,0,0.0246078,"archy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst, 2001). On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS - A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of deep linguistic information. In this paper we investigate 3.2 The data the role of cross-linguistic information in the task of English NP semantic interpretation and show the The data was collected f"
P07-1072,P06-1040,0,0.00710255,"istics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. Other researchers (Pantel and Pennacchiotti, 2006), (Snow et al., 2006) use clustering techniques coupled with syntactic dependency features to identify IS - A relations in large text collections. (Kim and Baldwin, 2006) and (Turney, 2006) focus on the lexical similarity of unseen noun compounds with those found in training. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of deep linguistic information. In this paper we investigate 3.2 The data the role of cross-linguistic information in the task of English NP semantic interpretation and show the The data was collected from two text collections with different distributions and of different genre, importance of a set of novel linguistic features. 569 POSSESSION (family estate); KINSHIP"
P07-1072,W01-0511,0,\N,Missing
R09-1061,D08-1038,0,0.118581,"l editors. Another approach to the analysis of scientific research relies on topic models which uncover structures used to explore text collections. In particular, they divide documents according to their topics and use the hidden structure to determine similarity between documents. Popular unsupervised topic models such as Latent Dirichlet Allocation (LDA) [2] and hierarchical models [7] have been successfully applied to various publications such as The American Political Science Review and Science. In Computational Linguistics, the only work of which we are aware is that of Hall et al. 2008 [4] who study the history of ideas using LDA and topic entropy. In this paper we extend over the work of Hall et al. 2008 [4] by adding two related fields (Linguistics and Education) and by employing various novel topic models for scientific research analysis. 3 Approach In this section we present the data used in this research and the topic models employed. We categorize both by topics 337 International Conference RANLP 2009 - Borovets, Bulgaria, pages 337–342 Field Venue LING LING LING LING CL CL CL CL CL EDU EDU Language Linguistics, Journal of Linguistic Inquiry Ling. & Philosophy ACL EACL NA"
R09-1069,S07-1085,1,0.868695,"s no systematic study of clusters of closely related and overlapping semantic relations. In this paper we provide an analysis of a set of five most frequently occurring semantic relations which are nearmisses (Part-Whole, Origin-Entity, Purpose) and overlaps (Part-Whole, Measure, Content- Container). Moreover, we compare the performance of three state-of-the-art relation identification systems which employ different feature sets: (1) an improved implementation of a supervised model, SemScat2 [2], (2) the SNoW machine learning architecture [23], and (3) a competitive 1007 SemEval-Task 4 system [1]. The systems were trained and tested on a corpus of 1,000 examples. The results show that in order to identify such nearmisses and overlaps accurately, a semantic relation iden381 International Conference RANLP 2009 - Borovets, Bulgaria, pages 381–387 tification system needs to go beyond the ontological information of the two nouns and rely heavily on contextual and pragmatic knowledge. The paper is organized as follows. In the next section we present previous work, followed by an analysis of semantic relations. In particular, we provide a classification of clusters of near-miss and overlappi"
R09-1069,W03-1210,1,0.775957,"employing various feature sets. The results show that in order to identify such near-misses and overlaps accurately, a semantic relation identification system needs to go beyond the ontological information of the two nouns and rely heavily on contextual and pragmatic knowledge. (2) Keywords lexical semantics; semantic relations; machine learning 1 Introduction Although semantic relations have been studied for a long time both in linguistics and natural language processing, they received special attention recently due to research done in various knowledge-rich tasks such as question answering [3, 17], information retrieval [11], and textual entailment [24]. The identification of semantic relations between nominals is the task of recognizing the relationship between two nouns in context. For example, the noun pair (cycling, happiness) encodes a Cause-Effect relation in the sentence He derives great joy and happiness from cycling. This task requires several local and global decisions needed for relation identification. This involves the meaning of the two noun entities along with the meaning of other words in context. The problem, while simple to state is hard to solve. The reason is that t"
R09-1069,P07-1072,1,0.898069,"Missing"
R09-1069,J06-1005,1,0.823224,"ations between Nominals The SemEval 2007 task on semantic relations between nominals is to identify the underlying semantic relation between two nouns in the context of a clause. Since there is no concensus on the number and abstraction level of semantic relations, the SemEval effort focused on seven frequently occurring semantic relations listed by many researchers in their lists of relations [20, 6, 8]: Cause– Effect, Instrument–Agency, Product–Producer, Origin– Entity, Theme–Tool, Part–Whole, and Content–Container. The dataset provided consists of a definition file and 140 1 2 Girju et al. [5] trained the annotators providing explicit annotation schemas based on a well defined classification of 6 subtypes of PartWhole relations [25]. This collection is freely available at: http : //apf el.ai.uiuc.edu/resources.html. training and about 70 test sentences for each of the seven relations considered. The definition file for each relation includes a detailed definition, restrictions and conventions, and prototypical positive and near–miss negative examples. For example, the Part–Whole relation is defined as follows [9]: Definition Part–Whole(X, Y) is true for a sentence S that mentions e"
R09-1069,W04-2610,1,0.778865,"used in this research, an evaluation of the frequently occurring set of such semantic relations, and propose a classification of contingency relations. 3.1 SemEval Task 4: Classification of Semantic Relations between Nominals The SemEval 2007 task on semantic relations between nominals is to identify the underlying semantic relation between two nouns in the context of a clause. Since there is no concensus on the number and abstraction level of semantic relations, the SemEval effort focused on seven frequently occurring semantic relations listed by many researchers in their lists of relations [20, 6, 8]: Cause– Effect, Instrument–Agency, Product–Producer, Origin– Entity, Theme–Tool, Part–Whole, and Content–Container. The dataset provided consists of a definition file and 140 1 2 Girju et al. [5] trained the annotators providing explicit annotation schemas based on a well defined classification of 6 subtypes of PartWhole relations [25]. This collection is freely available at: http : //apf el.ai.uiuc.edu/resources.html. training and about 70 test sentences for each of the seven relations considered. The definition file for each relation includes a detailed definition, restrictions and conventi"
R09-1069,S07-1003,1,0.913479,"ss) encodes a Cause-Effect relation in the sentence He derives great joy and happiness from cycling. This task requires several local and global decisions needed for relation identification. This involves the meaning of the two noun entities along with the meaning of other words in context. The problem, while simple to state is hard to solve. The reason is that the meaning encoded by the two nouns is not always explicitely stated in context. Despite the encouraging results obtained by the participating systems at the SemEval-2007 - Task 4: Classification of Semantic Relations between Nominals [9], the problem needs further analysis. For example, the set of semantic relations considered for this problem needs to be better understood. Thus, a more thorough analysis of semantic relations needs to be done before building systems capable of recognizing them automatically in context. Particular attention should be given to those semantic relations that are difficult to differentiate (near-misses) and those relations that coexist in some particular contexts (overlaps). Consider for example the following sentences: (1) a. I got home and big he1ibranchesh/e1i had fallen off the he2itreeh/e2i i"
R09-1069,C92-2082,0,0.247058,"Missing"
R09-1069,P06-2064,0,0.0361554,"Missing"
R09-1069,P95-1007,0,0.116436,"Missing"
R09-1069,W04-2609,1,0.898848,"Missing"
R09-1069,P06-1100,0,0.0571105,"Missing"
R09-1069,P06-2105,0,0.0318186,"der to identify such near-misses and overlaps accurately, a semantic relation identification system needs to go beyond the ontological information of the two nouns and rely heavily on contextual and pragmatic knowledge. (2) Keywords lexical semantics; semantic relations; machine learning 1 Introduction Although semantic relations have been studied for a long time both in linguistics and natural language processing, they received special attention recently due to research done in various knowledge-rich tasks such as question answering [3, 17], information retrieval [11], and textual entailment [24]. The identification of semantic relations between nominals is the task of recognizing the relationship between two nouns in context. For example, the noun pair (cycling, happiness) encodes a Cause-Effect relation in the sentence He derives great joy and happiness from cycling. This task requires several local and global decisions needed for relation identification. This involves the meaning of the two noun entities along with the meaning of other words in context. The problem, while simple to state is hard to solve. The reason is that the meaning encoded by the two nouns is not always explici"
R09-1069,J02-3004,0,\N,Missing
R09-1069,W01-0511,0,\N,Missing
S07-1003,W04-3205,0,0.0491922,"ierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We hav"
S07-1003,C92-2082,0,0.0345238,"a variety of methods (since we work with relations between nominals, the part of speech is always noun). We have used WordNet 3.0 on the Web and sense index tags. We chose the following semantic relations: Cause-Effect, Content-Container, InstrumentAgency, Origin-Entity, Part-Whole, ProductProducer and Theme-Tool. We wrote seven detailed definitions, including restrictions and conventions, plus prototypical positive and near-miss negative examples. For each relation separately, we based data collection on wild-card search patterns that Google allows. We built the patterns manually, following Hearst (1992) and Nakov and Hearst (2006). Instances of the relation Content-Container, for example, come up in response to queries such as “* contains *”, “* holds *”, “the * in the *”. Following the model of the Senseval-3 English Lexical Sample Task, we set out to collect 140 training and at least 70 test examples per relation, so we had a number of different patterns to ensure variety. We also aimed to collect a balanced number of positive and negative examples. The use of heuristic patterns to search for both positive and negative examples 14 should naturally result in negative examples that are near"
S07-1003,J02-3004,0,0.0275234,"oun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms. We do not presume to propose a single classification scheme, however allurin"
S07-1003,W04-2609,1,0.951787,"cine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work f"
S07-1003,W01-0511,0,0.263052,"cer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2"
S07-1003,P02-1032,0,0.0943661,"traction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and"
S07-1003,H05-1047,0,0.0359726,"their results. There were 14 teams who submitted 15 systems. 1 Task Description and Related Work The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities – honey bee, for example, shows an instance of the ProductProducer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, w"
S07-1003,H91-1061,0,\N,Missing
S07-1085,A00-2018,0,0.00616465,"is used for V-ing X/Theme, neither X nor Y can be the subject of the sentence, and hence Theme-Tool(X, Y) would be false. This restriction is also captured by the nominalization feature in case X or Y is an agential noun. PP attachment (F9) is defined for NP PP constructions, where the prepositional phrase containing the noun e2 attaches or not to the NP (containing e1 ). The rationale is to identify negative instances where the PP attaches to any other word before NP in the sentence. For example, eat <e1 >pizza</e1 > with <e2 >a fork</e2 >, where with a fork attaches to the verb to eat (cf. (Charniak, 2000)). Furthermore, we implemented and used two semantic role features which identify the semantic role of the phrase in a verb–argument structure, phrase containing either e1 (F10) or e2 (F11). In particular, we focus on three semantic roles: Time, Location, Manner. The feature is set to 1 if the target noun is part of a semantic role phrase and to 0 otherwise. The idea is to filter out near-miss examples, expecially for the Instrument-Agency relation. For this, we used ASSERT, a semantic role labeler developed at the University of Colorado at Boulder2 which was queried through a web interface. I"
S07-1085,W04-2610,1,0.918427,"on for Computational Linguistics No. F1 F2 F3, F4 F5, F6 F7, F8 F9 F10, F11 F12, F13, F14 F15, F16 F17 F18 Feature Definition Feature Set #1: Core features Argument position indicates the position of the arguments in the semantic relation (Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1 , e2 ), where e1 is the part and e2 is the whole). Semantic specialization this is the prediction returned by the automatic WordNet IS - A semantic (Girju et al., 2005; Girju et al., 2006) specialization procedure. Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations (Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns, other nominalizations, and neither. Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location. Feature Set #2: Context features Grammatical role describes the grammatical role of e1 (F7) and e2 (F8). There are three possible values: subject, direct object, or neither. PP Attachment applies to NP PP constructions and indicates if the prepositional phrase containing e2 attaches to the NP containing e1 . Semantic Role is concerned with the semantic role of the phrase containing either e1 (F10) or e2 (F11). In"
S07-1085,J06-1005,1,0.863153,"this research. Feature set #1: Core features This set contains six features that were employed in all seven relation classifiers. The features take into consideration only lexico-semantic information 386 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386–389, c Prague, June 2007. 2007 Association for Computational Linguistics No. F1 F2 F3, F4 F5, F6 F7, F8 F9 F10, F11 F12, F13, F14 F15, F16 F17 F18 Feature Definition Feature Set #1: Core features Argument position indicates the position of the arguments in the semantic relation (Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1 , e2 ), where e1 is the part and e2 is the whole). Semantic specialization this is the prediction returned by the automatic WordNet IS - A semantic (Girju et al., 2005; Girju et al., 2006) specialization procedure. Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations (Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns, other nominalizations, and neither. Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location. Feature Set #2: Context features Grammatical role describes the grammatical"
S07-1085,W03-1210,1,0.935424,"Missing"
S07-1085,W04-2609,1,0.881787,"3 Total 80 78 93 81 71 72 74 78.4 Base-F 67.8 65.5 80.0 61.5 58.0 53.1 67.9 Base-Acc 51.2 51.3 66.7 55.6 59.2 63.9 51.4 Best features F1, F2, F5, F6, F12–F14 F7, F8, F10, F11, F15–F18 F1–F4, F12–F14 F1, F2, F5, F6, F12–F14 F1–F6, F15, F16 F1–F4 F1–F6, F12–F14 Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macroaveraged for system’s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the baseline accuracy score (majority). lection of 3,129 sentences from Wall Street Journal (Moldovan et al., 2004; Girju et al., 2004) was considered for Part-Whole (1,003), Origin-Entity (167), Product-Producer (112), and Theme-Tool (91). We also extracted 552 Product-Producer instances from eXtended WordNet4 (noun entries and their gloss definition). Moreover, for Theme-Tool and ContentContainer we used special lists of constraints5 . Besides the selectional restrictions imposed on the nouns by special features such as F15 and F16 (psychological feature), we created lists of containers from various thesauri6 and identified selectional restrictions that differentiate between containers and locations rel"
S18-1057,H05-1073,0,0.0883445,"Missing"
S18-1057,W17-5205,0,0.0592803,"able to detect novel expression of sentiment. Keyword based approaches classify text based on the detection of unambiguous words in language. They depend on large scale lexicons with affective labels for words, such NRC (Mohammad and Turney, 2013). Knowledgebased approaches use web ontologies or semantic networks. A major advantage of such systems is that they enable the system to use conceptual ideas derived from world knowledge (Cambria and Hussain, 2012). Recently, distributed approaches have been proposed that leverage word embeddings and train deep neural networks on the embedding space (Mohammad and Bravo-Marquez, 2017a). Shared evaluations have encouraged the community to create benchmarks over shared tasks, and have been organized frequently. The Affective Text task at SemEval 2007 (Strapparava and Mihalcea, 2007) asked its participants to predict emotion labels for headlines of news articles. More recently, the Shared Task on Emotion Intensity (EmoInt) at WASSA 2017 (Mohammad and Bravo-Marquez, 2017a), had 22 participating teams who were given a corpus of 3,960 English tweets annotated with a continuous intensity score for each of four of Ekman’s basic emotions: anger, fear, joy and sadness. cally determ"
S18-1057,S18-1001,0,0.0524831,"Missing"
S18-1057,baccianella-etal-2010-sentiwordnet,0,0.169516,"Missing"
S18-1057,W16-0404,0,0.0243741,"Missing"
S18-1057,S07-1013,0,0.233253,"ack of learning capabilities to detect them. Current affective computing systems focus mainly on facial expressions, body language, speech (tone of voice, rhythm, etc.), keystroke as well as physiological input (e.g., heart rate, body temperature) to capture and process changes in a user’s emotional state. However, in environments such as social 377 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 377–384 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics tion labels are uncommon in the literature. Affective Text (Strapparava and Mihalcea, 2007), created for SemEval 2007, contains emotion annotations headlines of news articles. Alm et al. annotated about 185 children’s stories with the Ekman labels. Aman and Szpakowicz created annotated 5,000 sentences with additional labels for intensity and emotion bearing phrases. Preotiuc-Pietro et al. annotated 3,000 social media posts for valence and arousal, making this one of the few datasets that contains annotations based on the VAD model. Approaches: Rule-based approaches incorporate domain knowledge. This can include termbased n-gram features, distance between certain terms or pre-specifi"
S18-1057,strapparava-valitutti-2004-wordnet,0,0.367809,"Missing"
S18-1057,P97-1023,0,0.699549,"s. Alm et al. annotated about 185 children’s stories with the Ekman labels. Aman and Szpakowicz created annotated 5,000 sentences with additional labels for intensity and emotion bearing phrases. Preotiuc-Pietro et al. annotated 3,000 social media posts for valence and arousal, making this one of the few datasets that contains annotations based on the VAD model. Approaches: Rule-based approaches incorporate domain knowledge. This can include termbased n-gram features, distance between certain terms or pre-specified POS patterns. Early work in this area focused mainly on linguistic heuristics (Hatzivassiloglou and McKeown, 1997). However, a major drawback of these rule-based approaches is that they are unable to detect novel expression of sentiment. Keyword based approaches classify text based on the detection of unambiguous words in language. They depend on large scale lexicons with affective labels for words, such NRC (Mohammad and Turney, 2013). Knowledgebased approaches use web ontologies or semantic networks. A major advantage of such systems is that they enable the system to use conceptual ideas derived from world knowledge (Cambria and Hussain, 2012). Recently, distributed approaches have been proposed that le"
S18-1057,H05-1044,0,0.140991,"t sets or across the training and test sets are not directly comparable. We therefore devise a model that can predict and eliminate the mismatch between the two sets of labels. As a means to model the mismatch in the distributions of the two label sets, we train a linear model that, for the labels in the development set, learns a function between the predictions made for the development set and Feature Space We have used the AffectiveTweets (Mohammad and Bravo-Marquez, 2017b), a package in Weka (Hall et al., 2009) for extracting certain features from a tweet. The features extracted are: MPQA (Wilson et al., 2005), BingLiu (Bauman et al., 2017), AFINN (Nielsen, 2011), Sentiment-140 Emoticon (Kiritchenko et al., 1 Note: XGB stands for the XGBoost implementation of gradient boosted decision trees. SVM was implmented using sklearn (http://scikit-learn.org/stable/). 379 Figure 1: Ensemble used in the UIUC system. 5 the ground truth. This learner does not affect the training in any way, but is a way to transform the predictions made for the development set so that they are comparable to the ground truth labels. 4.4 Results In this section, we describe the results of our official submission to SemEval 2018 ("
W03-1207,P02-1031,0,0.0240159,"Missing"
W03-1207,A00-2031,0,\N,Missing
W03-1207,P00-1065,0,\N,Missing
W03-1207,H94-1020,0,\N,Missing
W03-1210,A00-2031,0,0.0455871,"th a causation link. According to the philosophy researcher Jaegwon Kim (Kim 1993), any discussion of causation implies an ontological framework of entities among which causal relations are to hold, and also ”an accompanying logical and semantical framework in which these entities can be talked about”. He argues that the entities that represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts, and that coherent causal talk is possible only within a coherent ontological framework of such states of affairs. Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicateargument recognition tasks, such as semantic roles. However, lexical and syntactic information alone is not sufficient for the detection of complex semantic relations, such as CAUSE. Based on these considerents and on our observations of the English texts, we selected a list of 19 features which are divided here into two categories: lexical and semantic features. The lexical feature is represented by the causation verb in the pattern considered. As verb senses in WordNet are fine grained pro"
W03-1210,A00-2018,0,0.0166458,"on link. According to the philosophy researcher Jaegwon Kim (Kim 1993), any discussion of causation implies an ontological framework of entities among which causal relations are to hold, and also ”an accompanying logical and semantical framework in which these entities can be talked about”. He argues that the entities that represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts, and that coherent causal talk is possible only within a coherent ontological framework of such states of affairs. Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicateargument recognition tasks, such as semantic roles. However, lexical and syntactic information alone is not sufficient for the detection of complex semantic relations, such as CAUSE. Based on these considerents and on our observations of the English texts, we selected a list of 19 features which are divided here into two categories: lexical and semantic features. The lexical feature is represented by the causation verb in the pattern considered. As verb senses in WordNet are fine grained pro"
W03-1210,P00-1065,0,0.00389932,"g to the philosophy researcher Jaegwon Kim (Kim 1993), any discussion of causation implies an ontological framework of entities among which causal relations are to hold, and also ”an accompanying logical and semantical framework in which these entities can be talked about”. He argues that the entities that represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts, and that coherent causal talk is possible only within a coherent ontological framework of such states of affairs. Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicateargument recognition tasks, such as semantic roles. However, lexical and syntactic information alone is not sufficient for the detection of complex semantic relations, such as CAUSE. Based on these considerents and on our observations of the English texts, we selected a list of 19 features which are divided here into two categories: lexical and semantic features. The lexical feature is represented by the causation verb in the pattern considered. As verb senses in WordNet are fine grained providing a large list of seman"
W03-1210,P00-1043,0,0.655842,"Missing"
W04-2609,P98-1013,0,0.0729414,"Missing"
W04-2609,A00-2031,0,0.521413,"ous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on sta"
W04-2609,P01-1017,0,0.0737792,"Missing"
W04-2609,J02-3001,0,0.102668,"a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer a"
W04-2609,P02-1031,0,0.0148143,"ns, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsk"
W04-2609,N03-1011,1,0.590975,"Missing"
W04-2609,W04-2610,1,0.739062,"vels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others. These errors could be substantially decreased with more research effort. A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations. The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure. Details about this approach are provided in (Girju et al. 2004)). 3 Applications Semantic relations occur with high frequency in open text, and thus, their discovery is paramount for many applications. One important application is Question Answering. A powerful method of answering more difficult questions is to associate to each question the semantic relation that reflects the meaning of that question and then search for that semantic relation over the candidates of semantically tagged paragraphs. Here is an example. Q. Where have nuclear incidents occurred? From the question stem word where, we know the question asks for a LOCATION which is found in the"
W04-2609,J93-2005,0,0.0217005,"lmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Vanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques. 2 Approach 2.1 Basic Approach We approach the problem top-down, namely identify and"
W04-2609,W01-0511,0,0.811371,"d by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound"
W04-2609,P02-1032,0,0.624644,"Missing"
W04-2609,C94-2125,0,0.825094,"ome references. Most of the time, the semantic relations are encoded by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these com"
W04-2609,C98-1013,0,\N,Missing
W04-2609,J02-3004,0,\N,Missing
W04-2610,P98-1013,0,0.07349,"Missing"
W04-2610,A00-2031,0,0.075496,"and (5) Adjective clauses where the head noun is modified by a relative clause (eg the man who was driving the car - an AGENT relation between man and driving). 1.2 Previous work on the discovery of semantic relations The development of large semantically annotated corpora, such as Penn Treebank2 and, more recently, PropBank (Kingsbury, et al. 2002), as well as semantic knowledge bases, such as FrameNet (Baker, Fillmore, and Lowe 1998), have stimulated a high interest in the automatic acquisition of semantic relations, and especially of semantic roles. In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. These statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in a learning algorithm. While these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominalized noun phrases and present a method for their automatic detection in op"
W04-2610,kingsbury-palmer-2002-treebank,0,0.0614616,"Missing"
W04-2610,P01-1017,0,0.0819073,"Missing"
W04-2610,W04-2609,1,0.796026,"Missing"
W04-2610,J02-3001,0,0.0537888,"re the head noun is modified by a relative clause (eg the man who was driving the car - an AGENT relation between man and driving). 1.2 Previous work on the discovery of semantic relations The development of large semantically annotated corpora, such as Penn Treebank2 and, more recently, PropBank (Kingsbury, et al. 2002), as well as semantic knowledge bases, such as FrameNet (Baker, Fillmore, and Lowe 1998), have stimulated a high interest in the automatic acquisition of semantic relations, and especially of semantic roles. In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. These statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in a learning algorithm. While these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominalized noun phrases and present a method for their automatic detection in open-text. 1.3 Approach We app"
W04-2610,P02-1031,0,0.117825,"by a relative clause (eg the man who was driving the car - an AGENT relation between man and driving). 1.2 Previous work on the discovery of semantic relations The development of large semantically annotated corpora, such as Penn Treebank2 and, more recently, PropBank (Kingsbury, et al. 2002), as well as semantic knowledge bases, such as FrameNet (Baker, Fillmore, and Lowe 1998), have stimulated a high interest in the automatic acquisition of semantic relations, and especially of semantic roles. In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. These statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in a learning algorithm. While these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominalized noun phrases and present a method for their automatic detection in open-text. 1.3 Approach We approach the problem top-down"
W04-2610,C94-1042,0,0.0576763,"Missing"
W04-2610,C98-1013,0,\N,Missing
W07-1527,P98-1015,0,0.0194325,"nin, 1980), for example, uses semantic categories such as “dissolved in” to build interpretations of compounds such as “salt water” and “sugar water”. In this research we experiment with two sets of semantic classification categories defined at different abstraction levels. The first is a core set of 22 semantic relations (22 SRs), set which was identified by us from the linguistics literature and from various experiments after many iterations over a period of time (Moldovan and Girju, 2003)4 . We proved 4 There are also other lists of semantic relations used by the research community (e.g., (Barker and Szpakowicz, 1998)), but 170 empirically that this set is encoded by noun – noun pairs in noun phrases and is a subset of our larger list of 35 semantic relations. This list, presented in Table 1 along with examples and semantic argument frames, is general enough to cover a large majority of text semantics while keeping the semantic relations to a manageable number. A semantic argument frame is defined for each semantic relation and indicates the position of each semantic argument in the underlying relation. For example, “Arg1 is part of (whole) Arg2 ” identifies the part (Arg1 ) and the whole (Arg2 ) entities"
W07-1527,P99-1008,0,0.0120567,"is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annot"
W07-1527,A00-2018,0,0.00877901,"Missing"
W07-1527,J06-1005,1,0.803027,"a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic"
W07-1527,P07-1072,1,0.863905,"as accurate as possible, many of such features are manually identified and annotated. Thus, the annotation process is an important task that requires not only considerable amount of time, but also experience with various annotation schemas and tools, and a good understanding of the research topic. Moreover, the extension of the noun phrase interpretation task to other natural languages brings forward new annotation issues. This paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system (Girju, 2007). The system relies on cross-linguistic evidence from a set of five Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the five Romance languages, our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl2 and CLUVI3 . The training data was annotated with contextual features based on two state-ofthe-art classification tag s"
W07-1527,N04-1016,0,0.0866621,"constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annotation Workshop, pages 168–1"
W07-1527,P95-1007,0,0.0761927,"es. In the following section we present two different state-of-the-art classification sets used in NP interpretation. 3 Lists of semantic classification relations Although researchers (Downing, 1977), (Jespersen, 1954) argued that noun compounds, and NPs in general, encode an infinite set of semantic relations, many agree (Finin, 1980), (Levi, 1978) there is a limited number of relations that occur with high frequency in these constructions. However, the number and the level of abstraction of these frequently used semantic categories are not agreed upon. They can vary from a few prepositions (Lauer, 1995) to hundreds and even thousands more specific semantic relations (Finin, 1980). The more abstract the categories, the more noun phrases are covered, but also the more room for variation as to which category a phrase should be assigned. Lauer (Lauer, 1995), for example, considers a set of eight prepositions as semantic classification categories that can link the head and the modifier nouns in a noun compound: of, for, with, in, on, at, about, and from. However, according to this classification, the noun compound love story, for instance, can be classified both as story of love and story about l"
W07-1527,W04-2609,1,0.849287,". For example, a compound family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING"
W07-1527,W05-0603,0,0.0127259,"e time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annotation Workshop, pages 168–175, c Prague, June 2007. 2"
W07-1527,P06-1015,0,0.0343458,"family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computation"
W07-1527,P06-1100,0,0.0152353,"ed as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at A"
W07-1527,P02-1032,0,0.0153112,"between the two concepts. For example, a compound family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiw"
W07-1527,P06-1101,0,0.0679567,"f silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from"
W07-1527,P06-1040,0,0.0754059,"it. Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances (noun compounds) and are either (weakly) supervised, knowledge-intensive (Rosario and Hearst, 2001), (Rosario et al., 2002), (Moldovan et al., 2004), (Pantel and Pennacchiotti, 2006), (Pennacchiotti and Pantel, 2006), (Kim and Baldwin, 2006), (Snow et al., 2006), (Girju et al., 2005; Girju et al., 2006), or use statistical models on large collections of unlabeled data (Berland and Charniak, 1999), (Lapata and Keller, 2004), (Nakov and Hearst, 2005), (Turney, 2006). Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed (Girju et al., 2006) that, for est from the computational linguistics community: Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Workshop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. 168 Proceedings of the Linguistic Annotation Workshop, pages 168–175, c Prague, June 2007. 2007 Association"
W07-1527,C98-1015,0,\N,Missing
W07-1527,W01-0511,0,\N,Missing
W09-1111,P98-1013,0,0.0186448,"Missing"
W09-1111,W04-3205,0,0.0297468,"tions, in particular on the quantifiers each other and one another (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We a"
W09-1111,P08-1079,0,0.122432,"er (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We also present some interesting observations on the data thus obtaine"
W09-1111,J06-3003,0,0.0412294,"and one another (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We also present some interesting observa"
W09-1111,H05-1044,0,0.00932439,"Missing"
W09-1111,C98-1013,0,\N,Missing
W09-1115,W03-1004,0,0.232793,"r (region) alignment, which can be categorized best as alignment between monolingual comparable corpora, but 1 The GDA tag set is designed to allow machines to automatically infer the underlying structure of documents. More information is available at http://i-content.org/gda. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 111–119, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics could also be easily construed as document passage retrieval, which is a well-researched topic in the Information Retrieval community. Barzilay and Elhadad (2003) incorporate context to facilitate alignment between monolingual comparable corpora by first learning paragraph matching rules in a supervised way, and then refining the alignment at the sentence level within paragraphs. Nelken and Shieber (2008) used TF-IDF term weighting with logistic regression to align sentences from pericopes in the gospels of the new testament. Callan (1994) analyzed various ways to define document passages and identified three main passage types, discourse (based on physical structure of the document), semantic (based on topic boundaries), and window (based on token dis"
W09-1115,I05-1066,0,0.422715,"r method based on the number of matched terms, and query expansion degrades aligner performance. 1 2 Introduction Automatic generation of slide presentations is a task the Computational Linguistics community has not yet pursued in much depth. A robust system capable of generating slide presentations from papers would save the author much tedium when organizing her presentations. In this paper we investigate this task from a novel perspective. While others have developed interesting approaches to slide generation from documents by modeling the problem in a unique way (Utiyama and Hasida, 1999; Shibata and Kurohashi, 2005), the aim of the research this paper initiates is to discover how humans create slide presentations, focusing more specifically on academic papers. Thus we take a corpus-based approach to the problem, and as a first step focus on the task of automatically aligning slide presentations to academic papers. 111 Related Work Automatic slide generation from documents is a thus far under-investigated topic. Utiyama and Hasida (1999) generate slides from GDA1 (global document annotation) tagged documents. They detect topics within the documents by analyzing GDA coreference links, modeled each slide as"
W09-1115,W99-0204,0,0.0101107,"more poorly than a simpler method based on the number of matched terms, and query expansion degrades aligner performance. 1 2 Introduction Automatic generation of slide presentations is a task the Computational Linguistics community has not yet pursued in much depth. A robust system capable of generating slide presentations from papers would save the author much tedium when organizing her presentations. In this paper we investigate this task from a novel perspective. While others have developed interesting approaches to slide generation from documents by modeling the problem in a unique way (Utiyama and Hasida, 1999; Shibata and Kurohashi, 2005), the aim of the research this paper initiates is to discover how humans create slide presentations, focusing more specifically on academic papers. Thus we take a corpus-based approach to the problem, and as a first step focus on the task of automatically aligning slide presentations to academic papers. 111 Related Work Automatic slide generation from documents is a thus far under-investigated topic. Utiyama and Hasida (1999) generate slides from GDA1 (global document annotation) tagged documents. They detect topics within the documents by analyzing GDA coreferenc"
W09-1115,W08-1807,0,0.0654857,"Missing"
W09-1115,E06-1021,0,\N,Missing
W13-4004,P09-1077,0,0.0429352,"cting causality in both explicit and implicit contexts (Sporleder and Lascarides, 2008). The complexity of the task of detecting causality between events stems from the fact that there are many factors involved, such as contextual features of an instance (e.g., lexical items, tenses of verbs, arguments of verbs, etc.), semantic and pragmatic features of events, background knowledge, world knowledge, common sense, etc. Prior approaches have employed contextual features of an instance to identify causality between events or discourse segments (Bethard and Martin, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). Although contextual features provide important knowledge about sentence(s) in which events appear, humans also make use of other information such as background knowledge to comprehend causality. For instance, in example 2 we use knowledge about the causal association between verbal entities rage and collapse to label it with causality. This research is motivated by the need to extract and analyze other type of knowledge necessary for the identification of causal relations between verbal events. We start from the fact that verbs are the Introduction The identification of semantic relations be"
W13-4004,P98-1013,0,0.00922397,"o express events and semantic relations between events. Thus, in order to identify and extract causal relations between events (denoted by (evi , evj )), it is critical for a model to employ knowledge about the tendency of a verb pair (vi , vj ) to encode causation. For example, the pair (kill, arrest) has a high tendency to encode a cause relation irrespective of the context in which it is used, thereby a good indicator of causality. The state-of-the-art resources on verb semantics, such as WordNet, VerbNet, PropBank, FrameNet, etc. (Miller, 1990; Kipper et al., 2000; Kingsbury et al., 2002; Baker et al., 1998), provide information about the semantic classes, thematic roles and selectional restrictions of verbs. Among these, WordNet is the only resource which provides information about the cause relation between verbs, but it has very limited coverage. For VERBOCEAN, a semi-automatically generated resource, Chklovski and Pantel (2004) have used explicit lexical patterns (e.g., “verb * by verb”) as means of mining enablement (cause-effect) relations between verbs. Such approaches help detecting causality with high precision but suffer from limited coverage due to the highly implicit nature of languag"
W13-4004,P09-2004,0,0.0691553,"ntribute most towards detecting causality in both explicit and implicit contexts (Sporleder and Lascarides, 2008). The complexity of the task of detecting causality between events stems from the fact that there are many factors involved, such as contextual features of an instance (e.g., lexical items, tenses of verbs, arguments of verbs, etc.), semantic and pragmatic features of events, background knowledge, world knowledge, common sense, etc. Prior approaches have employed contextual features of an instance to identify causality between events or discourse segments (Bethard and Martin, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). Although contextual features provide important knowledge about sentence(s) in which events appear, humans also make use of other information such as background knowledge to comprehend causality. For instance, in example 2 we use knowledge about the causal association between verbal entities rage and collapse to label it with causality. This research is motivated by the need to extract and analyze other type of knowledge necessary for the identification of causal relations between verbal events. We start from the fact that verbs are the Introduction The identification of"
W13-4004,P08-2045,0,0.70375,"ies three categories of verb pairs: Strongly Causal, Ambiguous, and Strongly Non-causal. The knowledge base is evaluated empirically. The results show that our metrics perform significantly better than the state-of-the-art on the task of detecting causal verbal events. 1 There were early reports of buildings collapsing along the coast. (CAUSE (erage , ecollapse )) In example 1, the two bold events are causally connected by an explicit and unambiguous discourse marker (because). However, in English, not all discourse markers unambiguously identify causality (Prasad et al., 2008) - for example, Bethard and Martin (2008) proposed a corpus of 1000 causal and non-causal event pairs conjoined by the marker and. Even more, causal relations can be encoded by implicit contexts - i.e., those where no discourse marker is present (example 2). Despite the recent achievements obtained in discourse processing, it is still unclear what types of knowledge can contribute most towards detecting causality in both explicit and implicit contexts (Sporleder and Lascarides, 2008). The complexity of the task of detecting causality between events stems from the fact that there are many factors involved, such as contextual features"
W13-4004,P08-1090,0,0.0962623,"Missing"
W13-4004,P09-1068,0,0.0710318,"Missing"
W13-4004,W04-3205,0,0.0186793,"y to encode a cause relation irrespective of the context in which it is used, thereby a good indicator of causality. The state-of-the-art resources on verb semantics, such as WordNet, VerbNet, PropBank, FrameNet, etc. (Miller, 1990; Kipper et al., 2000; Kingsbury et al., 2002; Baker et al., 1998), provide information about the semantic classes, thematic roles and selectional restrictions of verbs. Among these, WordNet is the only resource which provides information about the cause relation between verbs, but it has very limited coverage. For VERBOCEAN, a semi-automatically generated resource, Chklovski and Pantel (2004) have used explicit lexical patterns (e.g., “verb * by verb”) as means of mining enablement (cause-effect) relations between verbs. Such approaches help detecting causality with high precision but suffer from limited coverage due to the highly implicit nature of language. Moreover, such resources do not provide any information about the likelihood of a causal relation in verb pairs - e.g., (kill, arrest) has a high tendency to encode cause relation as compared with the pair (build, maintain). The pair (build, maintain) seems ambiguous because it can encode both cause and non-cause relations de"
W13-4004,D11-1027,0,0.578858,"rs for this purpose. • We introduce an automated procedure to build a training corpus of causal and non-causal event pairs. This prevents us from the trouble of annotating a large number of event pairs for cause and non-cause relations. Our metrics make use of supervision from the training corpus to identify causality in verb pairs. We also provide a mechanism to determine causal verb pairs which remain undiscovered due to the issue of training data sparseness. • We revisit recent approaches employing distributional similarity methods to predict causality between events (Riaz and Girju, 2010; Do et al., 2011). The state-of-the-art metric Cause-Effect Association (CEA) (Do et al., 2011) identifies causality mainly based on probabilities of verb-verb, verb-argument, and argument-argument pairs. In comparison with CEA, our metrics perform significantly better by improving the prior knowledge about the causal associations from CEA’s components. After a brief review of related work in next section, we describe our approach for acquisition of training corpus in section 3. The model for the extraction of causal associations is presented in section 4, followed by the evaluation and discussion in section 5"
W13-4004,C98-1013,0,\N,Missing
W13-4004,W03-1210,1,\N,Missing
W13-4004,P02-1047,0,\N,Missing
W13-4004,prasad-etal-2008-penn,0,\N,Missing
W13-4004,P08-1000,0,\N,Missing
W14-0707,P98-1013,0,0.0414262,"employs a supervised classifier relying on linguistic features to make binary predictions (i.e., does a verb-noun phrase pair encode a cause or non-cause relation?). We then incorporate additional types of knowledge on top of these binary predictions to improve performance. 3.1 Supervised Classifier In this section, we propose a basic supervised classifier to identify causation encoded by v-np pairs. To set up this supervised classifier, we need a training corpus of instances of v-np pairs encoding cause and non-cause relations. For this purpose, we employ the annotations of FrameNet project (Baker et al., 1998) provided for verbs. For example, consider the following annotation from FrameNet for the verb “dying” with argument “solvent abuse” where the pair “dyingsolvent abuse” encodes causality. A campaign has started to try to cut the rising number of children dying [cause from solvent abuse]. To generate a training corpus, we collect annotations of verbs from FrameNet s.t. the annotated element (aka. frame element) is a noun phrase. For example, we get a causal training instance of “dying-solvent abuse” pair from the above annotation. We assume that if a FrameNet’s annotated element contains a verb"
W14-0707,de-marneffe-etal-2006-generating,0,0.0193699,"Missing"
W14-0707,P09-1077,0,0.0200739,"inguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based on unsupervised co-occu"
W14-0707,P09-2004,0,0.0317084,"ed Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based o"
W14-0707,W06-1618,0,0.126504,"ss cv (¬cv ) contains the verbs with high (less) tendency to encode causation. Using above argument we claim that all verbs representing reporting events belong ¬cv class and verbs representing rest of the types of events belong to cv class. We build a supervised classifier which automatically classifies verbs into cv and ¬cv classes. We extract the instances of verbal events (i.e., verbs or verbal phrases) from TimeBank corpus and assign the labels cv and ¬cv to these instances. Using these labeled instances, we build a supervised classifier by adopting the same set features as introduced in Bethard and Martin (2006) to identify semantic classes of verbs. Due to space constraint, we refer the reader to Bethard and Martin (2006) for the details of features. Again we use Naive Bayes to take predictions of cv and ¬cv labels and their corresponding probabilities using equation 4. We incorporate the knowledge of semantics of verbs in our model by making the following additions to the integer linear program. Z3 = Z2 + X X x3 (v, l)P (v, l) X x3 (v, l) = 1 ∀ v : v-np ∈ I (10) l∈L3 x3 (v, l) ∈ {0, 1} ∀ v : v-np ∈ I ∀l ∈ L3 (11) x1 (v-np, ¬c) − x3 (v, ¬cv ) &gt;= 0 ∀ v : v-np, ∀ v-np ∈ I (12) x3 (v, cv ) − x1 (v-np,"
W14-0707,P08-2045,0,0.126106,"l the above types of knowledge for the current task. Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model"
W14-0707,W04-3205,0,0.0284794,"same event of “presenting” and thus encoding non-cause relation with each other. In our model, we determine the verb-noun pairs representing same or distinct events to make predictions accordingly. • We adopt the framework of Integer Linear Programming (ILP) (Roth and Yih, 2004; Do et al., 2011) to combine all the above types of knowledge for the current task. Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do"
W14-0707,W13-4004,1,0.89406,"erb-Noun Pairs via Noun and Verb Semantics Mehwish Riaz and Roxana Girju Department of Computer Science and Beckman Institute University of Illinois at Urbana-Champaign Urbana, IL 61801, USA {mriaz2,girju}@illinois.edu Abstract 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a supervised classifier, our model identifies causation by employing shallow linguistic fe"
W14-0707,D11-1027,0,0.762865,"ng Causality in Verb-Noun Pairs via Noun and Verb Semantics Mehwish Riaz and Roxana Girju Department of Computer Science and Beckman Institute University of Illinois at Urbana-Champaign Urbana, IL 61801, USA {mriaz2,girju}@illinois.edu Abstract 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a supervised classifier, our model identifies causation by employin"
W14-0707,P05-1045,0,0.182105,"se represents a named entity then it can have least tendency to encode causal relations unless there is a metonymic reading associated with it. For example, consider the following cause and non-cause examples where noun phrase is a named entity. 4. Sandy hit Cuba as a Category 3 hurricane. 5. Almost all the weapon sites in Iraq were destroyed by the United States. In example 4, Cuba is location and does not encode causality. However, in example 5 the pair “destroyed-the United States” encode causality where a metonymic reading is associated with the location. We apply Named Entity Recognizer (Finkel et al., 2005) and assume if a noun phrase is identified as a named entity then its corresponding verb-noun phrase pair encodes noncause relation. This constraint can lead to a false negative prediction when the metonymic reading is associated with a noun phrase. In order to avoid as much false negatives as possible, we imply the following simple rule i.e., if one of the following cue words appear between a verb and a noun phrase then do not apply the constraint stated above. (1) v-np∈I l∈L1 X x1 (v-np, l) = 1 ∀ v-np ∈ I (2) l∈L1 x1 (v-np, l) ∈ {0, 1} ∀ v-np ∈ I ∀l ∈ L1 (3) Here L1 = {c, ¬c}, I is the set o"
W14-0707,W04-2401,0,0.0417293,"ts effect. In example 3 above, the noun “hurricane” is cause and the verb “died” is its effect. However, a verb-noun pair may not encode causality when a verb and a noun represent same event. For example, in instance “Colin Powell presented further evidence in his presentation.”, the verb “presented” and the noun “presentation” represent same event of “presenting” and thus encoding non-cause relation with each other. In our model, we determine the verb-noun pairs representing same or distinct events to make predictions accordingly. • We adopt the framework of Integer Linear Programming (ILP) (Roth and Yih, 2004; Do et al., 2011) to combine all the above types of knowledge for the current task. Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to i"
W14-0707,N03-1033,0,0.167066,"Missing"
W14-0707,C98-1013,0,\N,Missing
W14-0707,W03-1210,1,\N,Missing
W14-0820,al-sabbagh-girju-2012-yadac,1,0.820705,"of MWEs. 3 Related Work There is a plethora of research on generalpurpose Arabic MWEs. Yet, no prior work has focused on AM-MWEs. Hawwari et al. (2012) describe the manual construction of a repository for Arabic MWEs that classifies them based on their morpho-syntactic structures. Corpus Ajdir LDC ISI YADAC Tashkeel Total Token # 113774517 28880558 6328248 6149726 41472307 Types # 2217557 532443 457361 358950 3566311 Description a monolingual newswire corpus of Modern Standard Arabic an LDC parallel Arabic-English corpus (Munteanu & Marcu 2007) a dialectal Arabic corpus of Weblogs and tweets (Al-Sabbagh & Girju 2012) a vowelized corpus of Classical and Modern Standard Arabic books Table 2: Statistics for the extraction corpora Attia et al. (2010) describe the construction of a lexicon of Arabic MWEs based on (1) correspondence asymmetries between Arabic Wikipedia titles and titles in 21 different languages, (2) English MWEs extracted from Princeton WordNet 3.0 and automatically translated into Arabic, and (3) lexical association measures. Bounhas and Slimani (2009) use syntactic patterns and Log-Likelihood Ratio to extract environmental Arabic MWEs. They achieve precision rates of 0.93, 0.66 and 0.67 for"
W14-0820,W10-3704,0,0.0653192,"Missing"
W14-0820,W12-3403,0,0.0473783,"Missing"
W14-0820,D13-1101,0,0.0244072,"Missing"
W14-0820,W09-3012,0,\N,Missing
W14-0820,I13-1025,0,\N,Missing
W14-0820,pasha-etal-2014-madamira,0,\N,Missing
W14-0820,I13-1047,1,\N,Missing
W14-0820,W13-1015,0,\N,Missing
W14-0820,N10-1142,0,\N,Missing
W14-4322,P09-1077,0,0.0443743,"Missing"
W14-4322,P98-1013,0,0.175518,"Missing"
W14-4322,P09-2004,0,0.161092,"em of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, part-of-speech tags of words, etc.) in the framework of supervised learning (Girju, 2003; Sporleder and Lascarides, 2008; Bethard and Martin, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009) and do not involve deeper semantics of language. Analysis of such approaches by Sporleder and Lascarides (2008) have revealed that the linguistic features are not always sufficient to achieve a good performance on the task of identifying semantic relations including causality. In this work, we propose a model that deeply processes and acquires the specific semantic information about the participants of a verb-noun phrase (v-np) pair (i.e., noun and verb semantics) to identify causality with a better performance over the baseline model depending merely on shallow linguist"
W14-4322,D12-1091,0,0.0415431,"Missing"
W14-4322,W06-1618,0,0.22339,"vents – i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting. According to the definitions of these classes, we claim that the reporting events (e.g., say, tell, etc.) just describe and narrate other events instead of encoding causality with them. Using this claim, we consider that all instances of reporting verbal events of TimeBank belong to the class ¬Cev and the rest of instances of verbal events lie in the class Cev . After acquiring instances of the classes Cev and ¬Cev , we build a supervised classifier for these two classes. We use the features introduced by Bethard and Martin (2006) to build this classifier (see Bethard and Martin (2006) for the details). Employing predictions and probabilities of assignments of the labels Cev and ¬Cev we add the following two constraints to ILP: (1) if the event represented by v belongs to ¬Cev then the corresponding v-np pair must be labeled with ¬C and (2) if a v-np pair is a causal pair then the event represented by v must be labeled with Cev . 4 frames, semantic classes of verbal events via a data intensive approach and association of metonymic readings with noun phrases to identify causality with a better performance. 4.1 Verb Fram"
W14-4322,P08-2045,0,0.487299,"2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, part-of-speech tags of words, etc.) in the framework of supervised learning (Girju, 2003; Sporleder and Lascarides, 2008; Bethard and Martin, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009) and do not involve deeper semantics of language. Analysis of such approaches by Sporleder and Lascarides (2008) have revealed that the linguistic features are not always sufficient to achieve a good performance on the task of identifying semantic relations including causality. In this work, we propose a model that deeply processes and acquires the specific semantic information about the participants of a verb-noun phrase (v-np) pair (i.e., noun and verb semantics) to identify causality with a better performance over the baseline model depending"
W14-4322,W04-3205,0,0.0571946,"sources of knowledge other than linguistic features for the current task, we have recently proposed a model that incorporates semantic classes of nouns and verbs with a high and low tendency to encode causation (Riaz and Girju, 2014). In this work, we exploit information about verb frames, data-driven verb semantics and metonymies to achieve more progress on our recent work. 2 3 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse Model for Recognizing Causality In this section we provide an overview of our previous model (Riaz and Girju, 2014) for identifying causality in v-np pairs where v (np) stands for verb (noun phrase). This model works in the following two stages: (1) A supervised classifier is used to make binary predictions (i.e., the label cause (C) or non-cause (¬C)) em"
W14-4322,W13-4004,1,0.928631,"ing various linguistic constructions e.g., between two verbs, a verb and a noun, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; 161 Proceedings of the SIGDIAL 2014 Conference, pages 161–170, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics just describe and narrate other events instead of encoding causality with them. We proposed an Integer Linear Programming (ILP) model (Roth and Yih, 2004; Do et al., 2011) to combine noun and verb semantics with the decisions of a supervised classifier which only relies on linguistic features. In this paper, we extend our previous model by acquiring and exploiting the following three novel"
W14-4322,D11-1027,0,0.669467,"ncoded in text using various linguistic constructions e.g., between two verbs, a verb and a noun, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; 161 Proceedings of the SIGDIAL 2014 Conference, pages 161–170, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics just describe and narrate other events instead of encoding causality with them. We proposed an Integer Linear Programming (ILP) model (Roth and Yih, 2004; Do et al., 2011) to combine noun and verb semantics with the decisions of a supervised classifier which only relies on linguistic features. In this paper, we extend our previous model by acquiring and exploiting th"
W14-4322,W14-0707,1,0.25076,"alysis of such approaches by Sporleder and Lascarides (2008) have revealed that the linguistic features are not always sufficient to achieve a good performance on the task of identifying semantic relations including causality. In this work, we propose a model that deeply processes and acquires the specific semantic information about the participants of a verb-noun phrase (v-np) pair (i.e., noun and verb semantics) to identify causality with a better performance over the baseline model depending merely on shallow linguistic features. The work in this paper builds on our recent work reported in Riaz and Girju (2014). In that previous model, we identified the semantic classes of nouns and verbs with a high and low tendency to encode causation. For example, a named entity such as LOCATION may have the least tendency to encode causation. We leveraged such information about nouns to filter false positives. Similarly, we utilized the TimeBank’s (Pustejovsky et al., 2006) classification of verbal events (i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting) and their definitions to claim that the reporting events (e.g., say, tell, etc.) Recognition of causality is important to achiev"
W14-4322,P05-1045,0,0.0102857,"oices can be assigned to np (see constraints 5 and 6). Constraint 7 enforces that if an np belongs to the class ¬Cnp then its corresponding v-np pair is assigned the label ¬C. In particular, we maximize the objective function Z2 (4) subject to the constraints introduced till now. For each v-np pair, we predict the semantic class of np using our supervised classifier for the labels l ∈ L2 and set the probabilities – i.e., P (fnp (v-np), l) = 1, P (fnp (v-np), {L2 } − {l}) = 0 if the label l ∈ L2 is assigned to np. Also before running our supervised classifier, we run a named entity recognizer (Finkel et al., 2005) and assign the label ¬Cnp to all noun phrases identified as named entities. We also determine association of metonymies with the noun phrases identified as named entities. For the current task we also acquire two semantic classes of verbs i.e., Cev and ¬Cev where the class Cev (¬Cev ) contains the verbal events with a high (low) tendency to encode causation. In order to derive these two classes we exploit the TimeBank corpus (Pustejovsky et al., 2003) which provides seven semantic classes of verbal events – i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting. Accor"
W14-4322,W04-2401,0,0.0201018,"ricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; 161 Proceedings of the SIGDIAL 2014 Conference, pages 161–170, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics just describe and narrate other events instead of encoding causality with them. We proposed an Integer Linear Programming (ILP) model (Roth and Yih, 2004; Do et al., 2011) to combine noun and verb semantics with the decisions of a supervised classifier which only relies on linguistic features. In this paper, we extend our previous model by acquiring and exploiting the following three novel types of knowledge: 1. We learn the information about tendencies of various verb frames to encode causation. For example, our model identifies if the subject of verb “destroy” (“occur”) has a high (low) tendency to encode causation. Such information helps gain performance by exploiting causal semantics of each verb frame separately. We also learn and incorpo"
W14-4322,N03-1033,0,0.0121544,"Missing"
W14-4322,de-marneffe-etal-2006-generating,0,0.0102682,"Missing"
W14-4322,C98-1013,0,\N,Missing
W14-4322,W03-1210,1,\N,Missing
W14-4322,L08-1000,0,\N,Missing
W14-4322,P08-1000,0,\N,Missing
W14-4920,I13-1047,1,0.899999,"active procedure; Section 3 gives examples for the final output representations; Section 4 describes corpus harvesting and sampling; Section 5 provides the annotation results and disagreement analysis; and Section 6 compares and contrasts our work with related work. 2 Annotation Scheme: Tasks and Guidelines Our annotation scheme comprises six tasks to label sense, polarity, intensification, tense, holders, and scopes for each event modality. Prior to the beginning of the interactive procedure, we highlight all event modalities in each tweet using a string-match algorithm and the lexicons from Al-Sabbagh et al. (2013, 2014a). The algorithm finds all potential event modality triggers (i.e. words/phrases that convey event modality) within each tweet in our corpus and marks them as annotation units. A total of 12134 candidate triggers are highlighted in 9949 tweets. 2.1 Task 1: Sense Sense annotation is to decide for each candidate trigger in context whether it actually conveys event modality given the tweet's context. The same present participle  حاببHAbb in example 1 is a volition trigger meaning I want/desire; whereas in example 2 it is a non-modal present participle meaning like/prefer/respect. 1. 2. ["
W14-4920,W14-0820,1,0.84866,"Missing"
W14-4920,C14-1144,1,0.746495,"Missing"
W14-4920,J08-4004,0,0.0286648,"5-6 are segmentation-based where the output of the annotation is a text segment. For the segmentation-based tasks, we use an all-ornothing method to measure inter-annotator reliability: for segments to be considered as agreement, they must share both the beginning and end boundaries. We use Krippendorff's alpha α (Krippendorff 2011) as our inter-annotator reliability measure, following the most recent work on modality annotation for other languages including English (Rubinstein et al. 2013) and Chinese (Cui and Chi 2013). For more details on Krippendorff's alpha and a, we refer the reader to Artstein and Poesio (2008). 5.2 Results We use the surveygizmo survey services2 to implement our interactive annotation procedure given that their survey structure is one that uses conditional branching and skip logic. We distribute the survey on Twitter and we have three annotators participating. According to the short qualifying quiz given at the beginning of the survey, all three participants are native Egyptian Arabic (EA) speakers who have at least two-year experience with Twitter. They are also university graduates who, therefore, master MSA. None of the participants has a linguistics background. Table 1 shows al"
W14-4920,baker-etal-2010-modality,0,0.413991,"rs on themselves as in promises. Ability is the (in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic background; hence, annotation typically takes place in in-lab settings at small scales. In this paper, we present an interactive annotation procedure to annota"
W14-4920,N12-2002,0,0.0240288,"and permission emanate from an external authority such as the law; whereas commitments are the obligations placed by speakers on themselves as in promises. Ability is the (in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic background; hence, annotation typically t"
W14-4920,C12-3005,0,0.0140814,"1 Introduction Event modality, according to Palmer (2001), describes events that are not actualized but are merely potential. It comprises obligation, permission, commitment, ability, and volition. Both obligation and permission emanate from an external authority such as the law; whereas commitments are the obligations placed by speakers on themselves as in promises. Ability is the (in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be ren"
W14-4920,W13-0105,0,0.0622474,"e Twitter user, annotators are asked to mark the boundaries of the linguistic unit that corresponds to the holder in the tweet's text. Annotators are instructed to use the maximal length principle from Szarvas et al. (2008) so that they mark the largest possible meaningful linguistic unit. Thus, in example 4 the holder is  الدكتور كمال الجنزوريAldktwr kmAl Aljnzwry (Dr. Kamal Alganzoury) not only Kamal Alganzoury. 2.6 Task 6: Scopes Scopes are the events modified by the trigger, syntactically realized as clauses, verb phrases, deverbal nouns or to-infinitives, according to Al-Sabbagh et al. (2013). We use the same maximal length principle from Task 5 so that the marked scope segment corresponds to the largest meaningful linguistic unit that describes the event. Typically, scope segments are delimited by: (1) punctuation markers and (2) subordinate conjunctions. Annotators are instructed that: (1) a single trigger may have one or more scopes; (2) two or more triggers - especially conjoined by coordinating particles - can share the same scope; and (3) scopes are not necessarily adjacent to their triggers. Examples 7, 8 and 9 illustrate each of these guidelines, repecetively. 7. 8. 9. 3 ["
W14-4920,W13-0304,0,0.178761,"es, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic background; hence, annotation typically takes place in in-lab settings at small scales. In this paper, we present an interactive annotation procedure to annotate event modality and its attributes of sense, polarity, intensification, tense, holders, and scopes in Modern Standard and Egyptian A"
W14-4920,hendrickx-etal-2012-modality,0,0.05016,"defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic background; hence, annotation typically takes place in in-lab settings at small scales. In this paper, we present an interactive annotation procedure to annotate event modality and its attributes of sense, polarity, intensification, tense, holders, and scopes i"
W14-4920,N10-1142,0,0.0130359,"potential. It comprises obligation, permission, commitment, ability, and volition. Both obligation and permission emanate from an external authority such as the law; whereas commitments are the obligations placed by speakers on themselves as in promises. Ability is the (in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated"
W14-4920,Y09-1034,0,0.0272805,"ion. Both obligation and permission emanate from an external authority such as the law; whereas commitments are the obligations placed by speakers on themselves as in promises. Ability is the (in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic background; henc"
W14-4920,matsuyoshi-etal-2010-annotating,0,0.0959464,"to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic background; hence, annotation typically takes place in in-lab settings at small scales. In this paper, we present an interactive annotation procedure to annotate event modality and its attributes of sense, polarity, intensifi"
W14-4920,W10-0207,0,0.015202,"s semi-automatically. 1 Introduction Event modality, according to Palmer (2001), describes events that are not actualized but are merely potential. It comprises obligation, permission, commitment, ability, and volition. Both obligation and permission emanate from an external authority such as the law; whereas commitments are the obligations placed by speakers on themselves as in promises. Ability is the (in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality"
W14-4920,W13-0306,0,0.167426,"omises. Ability is the (in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and preferences. Event modality is used for several NLP tasks, including sales and marketing analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). To-date, there are no large-scale Arabic corpora annotated for event modality compared to English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the creation of modality-annotated corpora is the lack of consensus definitions of modality and its attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic background; hence, annotation typically takes place in in-lab settings at small scales. In this paper, we present an interactive annotation procedure to annotate event modality and its attri"
W15-0806,C08-1001,0,0.0775703,"Missing"
