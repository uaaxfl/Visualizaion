2020.acl-main.199,P14-1098,0,0.0536108,"Missing"
2020.acl-main.199,E12-1004,0,0.0199823,"atternbased or distributional methods. Then, a taxonomy is constructed from these is-a pairs. The pattern-based methods, pioneered by Hearst (1992), detect is-a relation of a term pair (x, y) using the appearance of x and y in the same sentence through some lexical patterns or linguistic rules (Ritter et al., 2009; Luu et al., 2014). Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, which is also regarded as a path-based method. An alternative approach for detecting is-a relation is the distributional methods (Baroni et al., 2012; Roller et al., 2014), using the distributional representation of terms to directly predict relations. As for the step of taxonomy construction using the extracted is-a pairs, most of the approaches do it by incrementally attaching new terms (Snow et al., 2006; Shen et al., 2012; Alfarone and Davis, 2015; Wu et al., 2012). Mao et al. (2018) is the first to present a reinforcement learning based approach, named TaxoRL, for this task. For each term pair, its representation in TaxoRL is obtained by the path LSTM encoder, the word embeddings of both terms, and the embeddings of features. Recently"
2020.acl-main.199,C92-2082,0,0.358296,"the acyclicity constraint-based DAG structure learning model (Zheng et al., 2018; Yu et al., 2019) for taxonomy generation task. The input of Graph2Taxo is a cross-domain 1 By unseen domain, we refer to a domain for which taxonomy is not available to the system. 2198 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2198–2208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics noisy graph constructed by connecting noisy candidate is-a pairs, which are extracted from a large corpus using standard linguistic pattern-based approaches (Hearst, 1992). It is noisy because pattern-based approaches are prone to poor coverage as well as wrong extractions. In addition, it is cross-domain because the noisy is-a pairs are extracted from a large-scale corpus which contains a collection of text from multiple domains. Our proposed neural model directly encodes the structural information from a noisy graph into the embedding space. Since the links between domains are also used in our model, it has not only structural information of multiple domains but also crossdomain information. We demonstrate effectiveness of our proposed method on science and e"
2020.acl-main.199,D10-1108,0,0.0886465,"Missing"
2020.acl-main.199,D14-1088,0,0.0363785,"Missing"
2020.acl-main.199,P18-1229,0,0.170039,"l. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, which is also regarded as a path-based method. An alternative approach for detecting is-a relation is the distributional methods (Baroni et al., 2012; Roller et al., 2014), using the distributional representation of terms to directly predict relations. As for the step of taxonomy construction using the extracted is-a pairs, most of the approaches do it by incrementally attaching new terms (Snow et al., 2006; Shen et al., 2012; Alfarone and Davis, 2015; Wu et al., 2012). Mao et al. (2018) is the first to present a reinforcement learning based approach, named TaxoRL, for this task. For each term pair, its representation in TaxoRL is obtained by the path LSTM encoder, the word embeddings of both terms, and the embeddings of features. Recently, Dash et al. (2020) argued that strict partial orders2 correspond more directly to DAGs. They proposed a neural network architecture, called Strict Partial Order Network (SPON), that enforces asymmetry and transitive properties as soft constraints. Empirically, they showed that such a network produces better results for detecting hyponym-hy"
2020.acl-main.199,P06-1101,0,0.0672895,"al patterns or linguistic rules (Ritter et al., 2009; Luu et al., 2014). Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, which is also regarded as a path-based method. An alternative approach for detecting is-a relation is the distributional methods (Baroni et al., 2012; Roller et al., 2014), using the distributional representation of terms to directly predict relations. As for the step of taxonomy construction using the extracted is-a pairs, most of the approaches do it by incrementally attaching new terms (Snow et al., 2006; Shen et al., 2012; Alfarone and Davis, 2015; Wu et al., 2012). Mao et al. (2018) is the first to present a reinforcement learning based approach, named TaxoRL, for this task. For each term pair, its representation in TaxoRL is obtained by the path LSTM encoder, the word embeddings of both terms, and the embeddings of features. Recently, Dash et al. (2020) argued that strict partial orders2 correspond more directly to DAGs. They proposed a neural network architecture, called Strict Partial Order Network (SPON), that enforces asymmetry and transitive properties as soft constraints. Empirically"
2020.acl-main.199,S16-1206,0,0.0399209,"Missing"
2020.acl-main.199,C14-1097,0,0.0418993,"Missing"
2020.acl-main.199,J13-3007,0,0.022682,"ial Order Network (SPON), that enforces asymmetry and transitive properties as soft constraints. Empirically, they showed that such a network produces better results for detecting hyponym-hypernym pairs on a number of datasets for different languages and domains in both supervised and unsupervised settings. Many graph-based methods such as Kozareva and Hovy (2010) and Luu et al. (2014) regard the task of hypernymy organization as a hypernymy detection problem followed by a graph pruning problem. For the graph pruning task, various graphtheoretic approaches such as optimal branching algorithm (Velardi et al., 2013), Edmond’s algorithm (Karp, 1971) and Tarjan’s algorithm (Tarjan, 1972) have been used over the years. In addition to these, Wang et al. (2017) mentions several other graphbased taxonomy induction approaches. In contrast, our approach formulates the taxonomy construction task as a DAG generation problem instead of an incremental taxonomy learning (Mao et al., 2018), which differentiates it when compared with the existing methods. In addition, our approach uses the knowledge from existing domains (Bansal et al., 2014; Gan et al., 2016) to build the taxonomies of missing domains. 3 The Graph2Tax"
2020.acl-main.199,D17-1123,0,0.0304855,"Missing"
2020.acl-main.247,P19-1620,0,0.0294827,"Missing"
2020.acl-main.247,N18-2092,0,0.237051,"on answering systems, rather than alleviating the pressure to encode specific facts in the pre-training of a language model. of BERT (390k batches of 256). Hermann et al. (2015) introduces a reading comprehension task constructed automatically from news articles with summaries. In this view the constructed dataset is used both for training and test. Also, entities were replaced with anonymized markers to limit the influence of world knowledge. Unlike our span selection pre-training task, this requires summaries paired with articles and focuses only on entities. A similar approach was taken in Dhingra et al. (2018) to augment training data for question answering. Wikipedia articles were divided into introduction and body with sentences from the introduction used to construct queries for the body passage. Phrases and entities are used as possible answer terms. 3.1 Onishi et al. (2016) constructed a question answering dataset where answers are always people. Unlike other work, this did not use document structure but instead used a search index to retrieve a related passage for a given question. Because the answers are always people, and there are only a few different people in each passage, the task is mu"
2020.acl-main.247,P18-1031,0,0.0209814,"fact prediction F1 by 1 point and outperforming the previous best system. Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount. 1 • Masked Language Model (MLM): predicting masked word pieces from the surrounding context • Next Sentence Prediction (NSP): predicting if the two provided sequences follow sequentially in text or not Introduction State-of-the-art approaches for NLP tasks are based on language models that are pre-trained on tasks which do not require labeled data (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Sun et al., 2019). Fine tuning language models to downstream tasks, such as question answering or other natural language understanding tasks, has been shown to be a general and effective strategy. BERT is a recently The masked LM or “cloze” task (Taylor, 1953) and next sentence prediction are auxiliary tasks (Ando and Zhang, 2005) requiring language understanding, and therefore train the model to acquire effective representations of language. However, the cloze pre-training task often poses instances that require only shallow predicti"
2020.acl-main.247,Q19-1026,0,0.0477647,"task is similar to the cloze task, but is designed to have a fewer simple instances requiring only syntactic or collocation understanding. For cloze instances that require specific knowledge, rather than training the model to encode this knowledge in its parameterization, we provide a relevant and answer-bearing passage paired with the cloze instance. We provide an extensive evaluation of the span selection pre-training method across four reading comprehension tasks: the Stanford Question Answering Dataset (SQuAD) in both version 1.1 and 2.0; followed by the Google Natural Questions dataset (Kwiatkowski et al., 2019) and a multihop Question Answering dataset, HotpotQA (Yang et al., 2018). We report consistent improvements over both BERTBASE and BERTLARGE models in all reading comprehension benchmarks. The rest of the paper is structured as follows. In section 2 We describe earlier work on similar tasks and relate our extended pre-training to the broader research efforts on pre-training transformers. To provide context for our contribution, we review the most relevant parts of BERT in Section 3. Next, we describe and formalize our pre-training task and the architectural adjustments to BERT in Section 4. Fi"
2020.acl-main.247,2021.ccl-1.108,0,0.0958925,"Missing"
2020.acl-main.247,D16-1241,0,0.0242639,"es. In this view the constructed dataset is used both for training and test. Also, entities were replaced with anonymized markers to limit the influence of world knowledge. Unlike our span selection pre-training task, this requires summaries paired with articles and focuses only on entities. A similar approach was taken in Dhingra et al. (2018) to augment training data for question answering. Wikipedia articles were divided into introduction and body with sentences from the introduction used to construct queries for the body passage. Phrases and entities are used as possible answer terms. 3.1 Onishi et al. (2016) constructed a question answering dataset where answers are always people. Unlike other work, this did not use document structure but instead used a search index to retrieve a related passage for a given question. Because the answers are always people, and there are only a few different people in each passage, the task is multiple choice rather than span selection. Self training (Sachan and Xing, 2018) has also been used to jointly train to construct questions and generate self-supervised training data. BERT was trained for one million batches, with 256 token sequences in each. Although this i"
2020.acl-main.247,N18-1202,0,0.0496324,"oints and supporting fact prediction F1 by 1 point and outperforming the previous best system. Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount. 1 • Masked Language Model (MLM): predicting masked word pieces from the surrounding context • Next Sentence Prediction (NSP): predicting if the two provided sequences follow sequentially in text or not Introduction State-of-the-art approaches for NLP tasks are based on language models that are pre-trained on tasks which do not require labeled data (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Sun et al., 2019). Fine tuning language models to downstream tasks, such as question answering or other natural language understanding tasks, has been shown to be a general and effective strategy. BERT is a recently The masked LM or “cloze” task (Taylor, 1953) and next sentence prediction are auxiliary tasks (Ando and Zhang, 2005) requiring language understanding, and therefore train the model to acquire effective representations of language. However, the cloze pre-training task often poses instances that requi"
2020.acl-main.247,P18-2124,0,0.0908963,"Missing"
2020.acl-main.247,D16-1264,0,0.163956,"Missing"
2020.acl-main.247,N18-1058,0,0.027004,"ia articles were divided into introduction and body with sentences from the introduction used to construct queries for the body passage. Phrases and entities are used as possible answer terms. 3.1 Onishi et al. (2016) constructed a question answering dataset where answers are always people. Unlike other work, this did not use document structure but instead used a search index to retrieve a related passage for a given question. Because the answers are always people, and there are only a few different people in each passage, the task is multiple choice rather than span selection. Self training (Sachan and Xing, 2018) has also been used to jointly train to construct questions and generate self-supervised training data. BERT was trained for one million batches, with 256 token sequences in each. Although this is already a considerable amount of pre-training, recent research has shown continued improvement from additional pre-training data. XLNet (Yang et al., 2019) used four times as much text, augmenting the Wikipedia and BooksCorpus (Zhu et al., 2015) with text from web crawls, the number of instances trained over was also increased by a factor of four. RoBERTa (Liu et al., 2019) enlarged the text corpus b"
2020.acl-main.247,D18-1259,0,0.0238746,"ances requiring only syntactic or collocation understanding. For cloze instances that require specific knowledge, rather than training the model to encode this knowledge in its parameterization, we provide a relevant and answer-bearing passage paired with the cloze instance. We provide an extensive evaluation of the span selection pre-training method across four reading comprehension tasks: the Stanford Question Answering Dataset (SQuAD) in both version 1.1 and 2.0; followed by the Google Natural Questions dataset (Kwiatkowski et al., 2019) and a multihop Question Answering dataset, HotpotQA (Yang et al., 2018). We report consistent improvements over both BERTBASE and BERTLARGE models in all reading comprehension benchmarks. The rest of the paper is structured as follows. In section 2 We describe earlier work on similar tasks and relate our extended pre-training to the broader research efforts on pre-training transformers. To provide context for our contribution, we review the most relevant parts of BERT in Section 3. Next, we describe and formalize our pre-training task and the architectural adjustments to BERT in Section 4. Finally we provide an extensive empirical evaluation in MRC tasks, describ"
2021.acl-demo.24,2020.acl-main.398,0,0.069817,"Missing"
2021.acl-demo.24,Q19-1026,0,0.0317746,"forementioned issues and create an end-to-end table QA benchmark with NLQs, we introduce two new benchmarks, E2E WTQ and E2E GNQ, inspired by WikiTableQuestions and GNQtables. The WikiTableQuestions (Pasupat and Liang, 2015) benchmark is originally designed for finding answer to questions from given tables. It consists of complex NLQs and tables extracted from Wikipedia. We filter the benchmark following Glass et al. (2020) to generate a subset of 1,216 questions with 2,108 tables. The GNQtables dataset, introduced in Shraga et al. (2020c), extends the Google Natural Questions (NQ) benchmark (Kwiatkowski et al., 2019). It contains 789 NLQs and a large table corpus of 74,224 tables. For each question, the ground truth 205 # of tables 24,241 68 2,108 1.6M 74,224 2,108 74,224 WikiSQL TabMCQ WikiTableQuestions WikiTables GNQtables E2E WTQ E2E GNQ # of queries 80,654 9,092 22,033 100 789 1,216 789 Retrieval task 7 7 7 3 3 3 3 QA task 3 3 3 7 7 3 3 Reference (Zhong et al., 2017) (Jauhar et al., 2016) (Pasupat and Liang, 2015) (Bhagavatula et al., 2015) (Shraga et al., 2020c) Table 1: Comparison of table QA and retrieval benchmarks only points to the most relevant table (with a binary grade 1 indicates relevant),"
2021.acl-demo.24,P15-1142,0,0.154423,"um cell-level scores. Once the re-ranking is done, the top-k tables out of T are returned to the users. The correct cells on the top-k tables are later identified by locating the intersection of the most relevant columns and rows discovered by the RCI model. 4 4.1 Experiments Data Proposed Benchmarks: Existing table retrieval and QA benchmarks focus on either answering NLQs on a single table or the retrieval of multiple tables for a keyword query. A comprehensive comparison of existing benchmarks with their limitations is listed in Table 1. WikiSQL (Zhong et al., 2017) and WikiTableQuestions (Pasupat and Liang, 2015) are widely used to evaluate table QA systems. More recently, they have been used by TA PAS (Herzig et al., 2020) and TA B ERT (Yin et al., 2020) where transformer-based models for QA over tables have been introduced. However, these benchmarks are not created to be used as part of an end-to-end table retrieval and QA pipeline. On the other hand, WikiTables was created based on the corpus introduced by Bhagavatula et al. (2015) and used in many recent table retrieval studies (Zhang and Balog, 2018a; Deng et al., 2019; Shraga et al., 2020b,c). Despite its popularity, the WikiTables benchmark has"
2021.acl-demo.24,2020.acl-main.745,0,0.392745,"queries and table contents. Recently, there is a growing demand to support natural language questions (NLQs) over tables and answer the NLQs directly, rather than simply retrieving top-k relevant tables for keyword-based queries. Shraga et al. (2020c) introduce the first NLQ-based table retrieval system, which leverages an advanced deep learning model. Although it is a practical approach to better understand the structure of NLQs and table content, it only focuses on table retrieval rather than answering NLQs. Lately, transformer-based pre-training approaches have been introduced in TA B ERT (Yin et al., 2020), TA PAS (Herzig et al., 2020), and the Row-Column Intersection model (RCI) (Glass et al., 2020). These algorithms are very powerful at answering questions on given tables; however, one cannot apply them over all tables in a corpus due to the computationally expensive nature of transformers. An end-to-end table QA system that accomplishes both tasks is in need as it has the following advantages over separated systems: (1) It reduces error accumulations caused by inconsistent, separated models; (2) It is easier to fine-tune, optimize, and perform error analysis and reasoning on an end-to-end sy"
2021.acl-demo.24,D18-1425,0,0.0261966,", Zhang and Balog (2018b) introduces an ad-hoc table retrieval method, retrieving tables with features such as #query term, #columns, #null values, etc. Similar work includes 207 Sun et al. (2019), Bhagavatula et al. (2013), and Shraga et al. (2020a). The current state-of-the-art model is introduced in Shraga et al. (2020c), where tables are treated as multi-modal objects and retrieved with a neural ranking model. We compare CLTR with this approach in Section 4. Table QA Models Early table QA systems typically convert natural language questions into SQL format to answer questions over tables (Yu et al., 2018; Guo and Gao, 2020; Lin et al., 2019; Xu et al., 2018). In Jim´enez-Ruiz et al. (2020), the authors promote the idea of matching tabular data to knowledge graphs and create the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab), which provide a new solution for table understanding and QA related tasks. Recently, TA PAS (Herzig et al., 2020) and TA B ERT (Yin et al., 2020) introduce the transformer-based approaches for this task. The RCI (Glass et al., 2020) model is the state-of-theart model for QA over tables. It utilizes a transfer learning based framework to indepe"
2021.acl-short.34,2020.findings-emnlp.89,1,0.890895,"nowledge base entities using BLINK entity linker. The entity nodes in graph are also used to predict the number and locations of relation slots. A slot is defined as a pair of nodes in the AMR graph, where the corresponding entities have a relation in knowledge base in the context of the question. For instance, in Figure 3, nodes city and person are involved in a KB relation death place relevant for this question. Slot prediction is done using a deterministic rule-based transformation described in (Kapanipathi et al., 2021). In particular, we use their 1 We use the stack transformer parser of Astudillo et al. (2020); Lee et al. (2020) for generating AMR graphs and the BLINK system of Wu et al. (2019) for entity linking. 2.2 Neural Relation Linking Model SemReL employs a Siamese network, where the input question and target relations are embedded in the same vector space. The most likely relation is the one whose representation is closest to that of the input question. Figure 3 shows the overall architecture of our model. We use a Transformer model (Vaswani et al., 2017) as a shared encoder for both the input questions and candidate relations. In particular, we use the pre-trained BERT model (Devlin et al."
2021.acl-short.34,2020.findings-emnlp.288,1,0.722696,"ing BLINK entity linker. The entity nodes in graph are also used to predict the number and locations of relation slots. A slot is defined as a pair of nodes in the AMR graph, where the corresponding entities have a relation in knowledge base in the context of the question. For instance, in Figure 3, nodes city and person are involved in a KB relation death place relevant for this question. Slot prediction is done using a deterministic rule-based transformation described in (Kapanipathi et al., 2021). In particular, we use their 1 We use the stack transformer parser of Astudillo et al. (2020); Lee et al. (2020) for generating AMR graphs and the BLINK system of Wu et al. (2019) for entity linking. 2.2 Neural Relation Linking Model SemReL employs a Siamese network, where the input question and target relations are embedded in the same vector space. The most likely relation is the one whose representation is closest to that of the input question. Figure 3 shows the overall architecture of our model. We use a Transformer model (Vaswani et al., 2017) as a shared encoder for both the input questions and candidate relations. In particular, we use the pre-trained BERT model (Devlin et al., 2018) to initiali"
2021.acl-short.34,2020.coling-main.222,0,0.0372914,"ved significant interest due to its real-world applications. KBQA is a task where a natural language question is transformed into a precise structured query, using Entity Linking and Relation Linking as necessary sub-tasks to retrieve an answer. For example, the question “Who founded the city where Pat Vincent died?” requires mapping (a) founded and died to relations dbo:founder and dbo:deathPlace, and (b) entity Pat Vincent to dbr:Pat Vincent, given DBpedia as the knowledge base. Semantic parses such as Abstract Meaning Representation (AMR) have recently shown to be useful for the KBQA task (Lim et al., 2020). However, critical tasks for KBQA such as Relation Linking continue to be addressed primarily using the question text (Mulang’ et al., 2020; Sakor et al., 2019b; Lin et al., 2020), ignoring the AMR parses of the question which can introduce additional semantics. In the literature, some systems such as SLING (Mihindukulasooriya et al., 2020) have used AMR for relation linking. However, similar to other rulebased approaches (Sakor et al., 2019b), SLING depends heavily on the specific target KG (DBpedia) and it is based on a complex ensemble of different approaches, making portability to new kno"
2021.acl-short.34,2020.starsem-1.13,0,0.0352846,"Missing"
2021.acl-short.34,N19-1243,0,0.374389,"sing Entity Linking and Relation Linking as necessary sub-tasks to retrieve an answer. For example, the question “Who founded the city where Pat Vincent died?” requires mapping (a) founded and died to relations dbo:founder and dbo:deathPlace, and (b) entity Pat Vincent to dbr:Pat Vincent, given DBpedia as the knowledge base. Semantic parses such as Abstract Meaning Representation (AMR) have recently shown to be useful for the KBQA task (Lim et al., 2020). However, critical tasks for KBQA such as Relation Linking continue to be addressed primarily using the question text (Mulang’ et al., 2020; Sakor et al., 2019b; Lin et al., 2020), ignoring the AMR parses of the question which can introduce additional semantics. In the literature, some systems such as SLING (Mihindukulasooriya et al., 2020) have used AMR for relation linking. However, similar to other rulebased approaches (Sakor et al., 2019b), SLING depends heavily on the specific target KG (DBpedia) and it is based on a complex ensemble of different approaches, making portability to new knowledge bases a non-trivial task. In this work, we propose SemReL; a single Semantics-aware neural model for Relation linking. SemReL takes as input the question"
2021.emnlp-main.148,2020.acl-main.142,0,0.0813999,"aining the context encoder. In addition, we explore the idea of adapting KGI to a new domain. The domain adaptation process consists of indexing the new corpus using our pretrained DPR and substituting it in place of the original Wikipedia index. This enables zero-shot slot filling on the new dataset with respect to a new schema, avoiding the additional effort needed to rebuild NLP pipelines. We provide a few additional examples for each new relation, showing that zeroshot performance quickly improves with a few-shot learning setup. We explore this approach on a variant of the TACRED dataset (Alt et al., 2020) that we specifically introduce to evaluate the zero/fewshot slot filling task for domain adaption. The contributions of this work are as follows: 2 Related Work The use of language models as sources of knowledge (Petroni et al., 2019; Roberts et al., 2020; Wang et al., 2020; Petroni et al., 2020), has opened tasks such as zero-shot slot filling to pre-trained transformers. Furthermore, the introduction of retrieval augmented language models such as RAG (Lewis et al., 2020b) and REALM (Guu et al., 2020) also permit providing textual provenance for the generated slot fillers. KILT (Petroni et a"
2021.emnlp-main.148,P15-1034,0,0.0364136,"Filling task it fills out an infobox for the entity. Some slot filling systems provide evidence text to explain the predictions. Figure 1 illustrates the slot filling task. Many KBP systems described in the literature commonly involve complex pipelines for named entity recognition, entity co-reference resolution and relation extraction (Ellis et al., 2015). In particular, the task of extracting relations between entities from text has been shown to be the weakest component of the chain. The community proposed different solutions to improve relation extraction performance, such as rule-based (Angeli et al., 2015), supervised (Zhang et al., 2017), or distantly supervised (Glass et al., 2018). However, all these approaches require a considerable human effort in creating hand-crafted rules, annotating training data, or building well-curated datasets for boot1 Introduction strapping relation classifiers. Slot filling is a sub-task of Knowledge Base PopRecently, pre-trained language models have been ulation (KBP), where the goal is to recognize a used for slot filling (Petroni et al., 2020), opening pre-determined set of relations for a given entity a new research direction that might provide an efand use"
2021.emnlp-main.148,2020.tacl-1.5,0,0.0134949,"e of the KGI system. answer. Rather than index passages which are then consumed by a reader or generator component, it Section 2 present an overview of the state of indexes the phrases in the corpus that can be potenthe art in slot filling. Section 3 describes our KGI tial answers to questions, or fillers for slots. Each system, providing details on the DPR and RAG phrase is represented by the pair of its start and end models and describing our novel approach to hard token vectors from the final layer of a transformer negatives. Our system is evaluated in Sections 4 initialized from SpanBERT (Joshi et al., 2020). and 5 which include a detailed analysis. Section 6 concludes the paper and highlights some interesting GENRE (Cao et al., 2021) addresses the retrieval direction for future work. task in KILT slot filling by using a sequence-to1940 sequence transformer to generate the title of the Wikipedia page where the answer can be found. This method can produce excellent scores for retrieval but it does not address the problem of producing the slot filler. It is trained on BLINK (Wu et al., 2020) and all KILT tasks jointly. Open Retrieval Question Answering (ORQA) (Lee et al., 2019) introduced neural in"
2021.emnlp-main.148,2020.emnlp-main.550,0,0.0607895,"elated Work The use of language models as sources of knowledge (Petroni et al., 2019; Roberts et al., 2020; Wang et al., 2020; Petroni et al., 2020), has opened tasks such as zero-shot slot filling to pre-trained transformers. Furthermore, the introduction of retrieval augmented language models such as RAG (Lewis et al., 2020b) and REALM (Guu et al., 2020) also permit providing textual provenance for the generated slot fillers. KILT (Petroni et al., 2021) was introduced with a number of baseline approaches. The best performing of these is RAG (Lewis et al., 2020b). The model incorporates DPR (Karpukhin et al., 2020) to first gather evidence passages for the query, then uses a model initialized from BART (Lewis et al., 2020a) to do sequence-to-sequence generation from each evidence passage concatenated with the query in order to generate the answer. In the baseline RAG approach only the query encoder and generation component are fine-tuned on the task. The passage encoder, trained on Natural Questions (Kwiatkowski et al., 2019) is held fixed. Interestingly, while it gives the best performance of the baselines tested on the task of producing slot fillers, its performance on the retrieval metrics is worse t"
2021.emnlp-main.148,Q19-1026,0,0.100425,"slot fillers. KILT (Petroni et al., 2021) was introduced with a number of baseline approaches. The best performing of these is RAG (Lewis et al., 2020b). The model incorporates DPR (Karpukhin et al., 2020) to first gather evidence passages for the query, then uses a model initialized from BART (Lewis et al., 2020a) to do sequence-to-sequence generation from each evidence passage concatenated with the query in order to generate the answer. In the baseline RAG approach only the query encoder and generation component are fine-tuned on the task. The passage encoder, trained on Natural Questions (Kwiatkowski et al., 2019) is held fixed. Interestingly, while it gives the best performance of the baselines tested on the task of producing slot fillers, its performance on the retrieval metrics is worse than BM25 (Petroni et al., 2021). This suggests that fine-tuning the entire retrieval component 1. We describe an end-to-end solution for slot could be beneficial. Another baseline in KILT is filling, called KGI, that improves the state-of- BARTLARGE fine-tuned on the slot filling tasks the-art in the KILT slot filling benchmarks by but without the usage of the retrieval model. a large margin. In an effort to improve"
2021.emnlp-main.148,N19-1423,0,0.153788,"21) addresses the retrieval direction for future work. task in KILT slot filling by using a sequence-to1940 sequence transformer to generate the title of the Wikipedia page where the answer can be found. This method can produce excellent scores for retrieval but it does not address the problem of producing the slot filler. It is trained on BLINK (Wu et al., 2020) and all KILT tasks jointly. Open Retrieval Question Answering (ORQA) (Lee et al., 2019) introduced neural information retrieval for the related task of factoid question answering. Like DPR, the retrieval is based on a biencoder BERT (Devlin et al., 2019) model. Unlike DPR, ORQA projects the BERT [CLS] vector to a lower dimensional (128) space. It also uses the inverse cloze pre-training task for retrieval, while DPR does not use retrieval specific pre-training. original DPR work (Karpukhin et al., 2020). We first index the passages using a traditional keyword search engine, Anserini3 . The head entity and the relation are used as a keyword query to find the topk passages by BM25. Passages with overlapping paragraphs to the ground truth are excluded as well as passages that contain a correct answer. The remaining top ranked result is used as a"
2021.emnlp-main.148,2021.acl-long.518,0,0.0147242,"veness of hard negmulti-task training of the KILT suite of benchmarks ative mining for DPR when combined with to train the DPR passage and query encoder. The end-to-end training for slot filling tasks. top-3 passages returned by the resulting passage 3. We evaluate the domain adaptation of KGI index were then combined into a single sequence using zero/few-shot slot filling, demonstrat- with the query and a BART model was used to ing its robustness on zero-shot TACRED, a produce the answer. This resulted in large gains in benchmark released with this paper. retrieval performance. DensePhrases (Lee et al., 2021) is a different 4. We publicly release the pre-trained models approach to knowledge intensive tasks with a short and source code of the KGI system. answer. Rather than index passages which are then consumed by a reader or generator component, it Section 2 present an overview of the state of indexes the phrases in the corpus that can be potenthe art in slot filling. Section 3 describes our KGI tial answers to questions, or fillers for slots. Each system, providing details on the DPR and RAG phrase is represented by the pair of its start and end models and describing our novel approach to hard t"
2021.emnlp-main.148,P19-1612,0,0.0238178,"zed from SpanBERT (Joshi et al., 2020). and 5 which include a detailed analysis. Section 6 concludes the paper and highlights some interesting GENRE (Cao et al., 2021) addresses the retrieval direction for future work. task in KILT slot filling by using a sequence-to1940 sequence transformer to generate the title of the Wikipedia page where the answer can be found. This method can produce excellent scores for retrieval but it does not address the problem of producing the slot filler. It is trained on BLINK (Wu et al., 2020) and all KILT tasks jointly. Open Retrieval Question Answering (ORQA) (Lee et al., 2019) introduced neural information retrieval for the related task of factoid question answering. Like DPR, the retrieval is based on a biencoder BERT (Devlin et al., 2019) model. Unlike DPR, ORQA projects the BERT [CLS] vector to a lower dimensional (128) space. It also uses the inverse cloze pre-training task for retrieval, while DPR does not use retrieval specific pre-training. original DPR work (Karpukhin et al., 2020). We first index the passages using a traditional keyword search engine, Anserini3 . The head entity and the relation are used as a keyword query to find the topk passages by BM25"
2021.emnlp-main.148,L18-1544,0,0.0462313,"Missing"
2021.emnlp-main.148,K17-1034,0,0.0286162,"re-trained language models have been ulation (KBP), where the goal is to recognize a used for slot filling (Petroni et al., 2020), opening pre-determined set of relations for a given entity a new research direction that might provide an efand use them to populate infobox like structures. fective solution to the aforementioned problems. This can be done by exploring the occurrences of In particular, the KILT benchmark (Petroni et al., the input entity in the corpus and gathering infor- 2021), standardizes two zero-shot slot filling tasks, mation about its slot fillers from the context in zsRE (Levy et al., 2017) and T-REx (Elsahar et al., which it is located. A slot filling system processes 2018), providing a competitive evaluation frameand indexes a corpus of documents. Then, when work to drive advancements in slot filling. Howprompted with an entity and a number of relations, ever, the best performance achieved by the current 1 Our source code is available at: https://github. retrieval-based models on the two slot filling tasks com/IBM/kgi-slot-filling in KILT are still not satisfactory. This is mainly 1939 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, page"
2021.emnlp-main.148,2020.acl-main.703,0,0.0874313,"t performance quickly improves with a few-shot learning setup. We explore this approach on a variant of the TACRED dataset (Alt et al., 2020) that we specifically introduce to evaluate the zero/fewshot slot filling task for domain adaption. The contributions of this work are as follows: 2 Related Work The use of language models as sources of knowledge (Petroni et al., 2019; Roberts et al., 2020; Wang et al., 2020; Petroni et al., 2020), has opened tasks such as zero-shot slot filling to pre-trained transformers. Furthermore, the introduction of retrieval augmented language models such as RAG (Lewis et al., 2020b) and REALM (Guu et al., 2020) also permit providing textual provenance for the generated slot fillers. KILT (Petroni et al., 2021) was introduced with a number of baseline approaches. The best performing of these is RAG (Lewis et al., 2020b). The model incorporates DPR (Karpukhin et al., 2020) to first gather evidence passages for the query, then uses a model initialized from BART (Lewis et al., 2020a) to do sequence-to-sequence generation from each evidence passage concatenated with the query in order to generate the answer. In the baseline RAG approach only the query encoder and generation"
2021.emnlp-main.148,2021.acl-long.89,0,0.0316257,"Missing"
2021.emnlp-main.148,2021.naacl-main.200,0,0.075611,"Missing"
2021.emnlp-main.148,D19-1250,0,0.0184822,"ikipedia index. This enables zero-shot slot filling on the new dataset with respect to a new schema, avoiding the additional effort needed to rebuild NLP pipelines. We provide a few additional examples for each new relation, showing that zeroshot performance quickly improves with a few-shot learning setup. We explore this approach on a variant of the TACRED dataset (Alt et al., 2020) that we specifically introduce to evaluate the zero/fewshot slot filling task for domain adaption. The contributions of this work are as follows: 2 Related Work The use of language models as sources of knowledge (Petroni et al., 2019; Roberts et al., 2020; Wang et al., 2020; Petroni et al., 2020), has opened tasks such as zero-shot slot filling to pre-trained transformers. Furthermore, the introduction of retrieval augmented language models such as RAG (Lewis et al., 2020b) and REALM (Guu et al., 2020) also permit providing textual provenance for the generated slot fillers. KILT (Petroni et al., 2021) was introduced with a number of baseline approaches. The best performing of these is RAG (Lewis et al., 2020b). The model incorporates DPR (Karpukhin et al., 2020) to first gather evidence passages for the query, then uses a"
2021.emnlp-main.148,2020.emnlp-main.437,0,0.0228458,"Missing"
2021.emnlp-main.148,P16-1158,0,0.0216352,"Missing"
2021.emnlp-main.148,2020.emnlp-main.519,0,0.0358151,"final layer of a transformer negatives. Our system is evaluated in Sections 4 initialized from SpanBERT (Joshi et al., 2020). and 5 which include a detailed analysis. Section 6 concludes the paper and highlights some interesting GENRE (Cao et al., 2021) addresses the retrieval direction for future work. task in KILT slot filling by using a sequence-to1940 sequence transformer to generate the title of the Wikipedia page where the answer can be found. This method can produce excellent scores for retrieval but it does not address the problem of producing the slot filler. It is trained on BLINK (Wu et al., 2020) and all KILT tasks jointly. Open Retrieval Question Answering (ORQA) (Lee et al., 2019) introduced neural information retrieval for the related task of factoid question answering. Like DPR, the retrieval is based on a biencoder BERT (Devlin et al., 2019) model. Unlike DPR, ORQA projects the BERT [CLS] vector to a lower dimensional (128) space. It also uses the inverse cloze pre-training task for retrieval, while DPR does not use retrieval specific pre-training. original DPR work (Karpukhin et al., 2020). We first index the passages using a traditional keyword search engine, Anserini3 . The he"
2021.emnlp-main.148,D17-1004,0,0.177797,"box for the entity. Some slot filling systems provide evidence text to explain the predictions. Figure 1 illustrates the slot filling task. Many KBP systems described in the literature commonly involve complex pipelines for named entity recognition, entity co-reference resolution and relation extraction (Ellis et al., 2015). In particular, the task of extracting relations between entities from text has been shown to be the weakest component of the chain. The community proposed different solutions to improve relation extraction performance, such as rule-based (Angeli et al., 2015), supervised (Zhang et al., 2017), or distantly supervised (Glass et al., 2018). However, all these approaches require a considerable human effort in creating hand-crafted rules, annotating training data, or building well-curated datasets for boot1 Introduction strapping relation classifiers. Slot filling is a sub-task of Knowledge Base PopRecently, pre-trained language models have been ulation (KBP), where the goal is to recognize a used for slot filling (Petroni et al., 2020), opening pre-determined set of relations for a given entity a new research direction that might provide an efand use them to populate infobox like str"
2021.emnlp-main.148,N19-1327,1,0.902267,"Missing"
2021.emnlp-main.342,P15-1142,0,0.21153,"linked to the “Candidate” column in the table. ers compared to the more widely-studied passagebased reading comprehension (RC) problem. Further, TableQA may involve complex questions with multi-cell or aggregate answers. Most of the TableQA systems use semantic parsing approaches that utilizes language encoders to produce an intermediate logical form from the natural language question which is executed that over the tabular data to get the answer. While some systems (Zhong et al., 2017) were fully supervised, needing pairs of questions and logical forms as training data, more recent systems (Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Dasigi et al., 2019) rely only on the answer as weak supervision and search for a correct logical form. The current best TableQA systems (Herzig et al., 2020; Yin et al., 2020a) capitalize on advances in language modeling, such as BERT, and extend it to encode table representations as well. They are shown to produce excellent results on popular benchmarks such as WikiSQL (Zhong et al., 2017) and WikiTableQuestions(WikiTQ) (Pasupat and Liang, 2015). With increasing prevalence of text analytics as a centrally-trained service that serves diverse customers, practical"
2021.emnlp-main.342,2020.emnlp-main.468,0,0.0285174,"TQ (Paparsing approach (MAPO) (Liang et al., 2018). supat and Liang, 2015). The benchmark creation Question generation (QG) (Liu et al., 2020; Sul- process is described in Section 3.1. Then, we intan et al., 2020; Shakeri et al., 2020) has been troduce the proposed framework (illustrated in Figwidely explored in reading comprehension (RC) ure 2) to help TableQA system cope with topic shift. task to reduce the burden of annotating large vol- Section 3.2 describes the topic specific vocabulary umes of Q-A pairs given a context paragraph. Re- extension for BERT, followed by Question Genercently, Puri et al. (2020) used GPT-2 (Radford et al., ation in target topic in Section 3.3 and reranking 2019) to generate synthetic data for RC, showing logical forms in Section 3.4. that synthetic data alone is sufficient to obtain state3.1 TableQA topic-shift benchmark of-art on the SQUAD1.1 dataset. For the QG task in TableQA, systems proposed by Benmalek et al. To create a topic-shift TableQA benchmark out (2019); Guo et al. (2018); Serban et al. (2016) uti- of existing datasets, topics have to be assigned to lize the structure of intermediate logical forms (e.g., every instance. Once topics are assigned, we crea"
2021.emnlp-main.342,P16-1056,0,0.0290525,"arge vol- Section 3.2 describes the topic specific vocabulary umes of Q-A pairs given a context paragraph. Re- extension for BERT, followed by Question Genercently, Puri et al. (2020) used GPT-2 (Radford et al., ation in target topic in Section 3.3 and reranking 2019) to generate synthetic data for RC, showing logical forms in Section 3.4. that synthetic data alone is sufficient to obtain state3.1 TableQA topic-shift benchmark of-art on the SQUAD1.1 dataset. For the QG task in TableQA, systems proposed by Benmalek et al. To create a topic-shift TableQA benchmark out (2019); Guo et al. (2018); Serban et al. (2016) uti- of existing datasets, topics have to be assigned to lize the structure of intermediate logical forms (e.g., every instance. Once topics are assigned, we create SQL) to generate natural language questions. How- train-test splits with topic shift. I.e., train instances ever, none of these QG methods utilize the addi- and test instances come from non-overlapping sets tional context like table headers, structure and se- of topics. TableQA instances are triplets of the 4161 form {table, question, answer}. For the datasets text data for a target topic is inexpensive. We name WikiSQL and WikiTQ"
2021.emnlp-main.342,2020.emnlp-main.439,0,0.0429358,"ly, content, and (ii) relationship between table cells, no public topic-sliced TableQA dataset is available. derived from its structure. To generate the struc- We introduce a topic-shift benchmark by creating tured query, the encoding obtained from TaBERT new splits in existing popular TableQA datasets: is coupled with a memory augmented semantic WikiSQL (Zhong et al., 2017) and WikiTQ (Paparsing approach (MAPO) (Liang et al., 2018). supat and Liang, 2015). The benchmark creation Question generation (QG) (Liu et al., 2020; Sul- process is described in Section 3.1. Then, we intan et al., 2020; Shakeri et al., 2020) has been troduce the proposed framework (illustrated in Figwidely explored in reading comprehension (RC) ure 2) to help TableQA system cope with topic shift. task to reduce the burden of annotating large vol- Section 3.2 describes the topic specific vocabulary umes of Q-A pairs given a context paragraph. Re- extension for BERT, followed by Question Genercently, Puri et al. (2020) used GPT-2 (Radford et al., ation in target topic in Section 3.3 and reranking 2019) to generate synthetic data for RC, showing logical forms in Section 3.4. that synthetic data alone is sufficient to obtain state3.1"
2021.emnlp-main.342,2020.acl-main.500,0,0.0967867,"Missing"
2021.emnlp-main.342,D19-1254,0,0.023933,"fically for TableQA Recent TableQA systems (Herzig et al., 2020; with the assistance of a logical query and large pre-trained multitask transformers. Yin et al., 2020a; Glass et al., 2021) extend BERT Domain adaptation approaches in QA (Lee et al., to encode the entire table including headers, rows 2019; Ganin et al., 2016) have so far mostly used and columns. They aim to learn a table-embedding representation that can capture correlations be- adversarial learning with an aim to identify dotween question keywords and target cell of the ta- main agnostic features, including in RC applications (Wang et al., 2019; Cao et al., 2020). Howble. TAPAS (Herzig et al., 2020) and RCI (Glass ever, for the TableQA systems using BERT-style et al., 2021) are designed to answer a question by language models with vast pre-training, topic shifts predicting the correct cells in the table in a truly remain an unexplored problem. end-to-end manner. TaBERT (Yin et al., 2020a) is a powerful encoder developed specifically for the 3 T3QA framework TableQA task. TaBERT jointly encodes a natural language question and the table, implicitly creating To our knowledge, this is the first work to explore (i) entity links between q"
2021.emnlp-main.342,2020.acl-main.745,0,0.278996,"answers. Most of the TableQA systems use semantic parsing approaches that utilizes language encoders to produce an intermediate logical form from the natural language question which is executed that over the tabular data to get the answer. While some systems (Zhong et al., 2017) were fully supervised, needing pairs of questions and logical forms as training data, more recent systems (Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Dasigi et al., 2019) rely only on the answer as weak supervision and search for a correct logical form. The current best TableQA systems (Herzig et al., 2020; Yin et al., 2020a) capitalize on advances in language modeling, such as BERT, and extend it to encode table representations as well. They are shown to produce excellent results on popular benchmarks such as WikiSQL (Zhong et al., 2017) and WikiTableQuestions(WikiTQ) (Pasupat and Liang, 2015). With increasing prevalence of text analytics as a centrally-trained service that serves diverse customers, practical QA systems will encounter tables ∗ Equal contribution by first two authors. 1 and questions from topics which they may not have The source code and new dataset splits are available at https://github.com/IB"
2021.emnlp-main.342,W07-0201,0,0.0156841,"Missing"
2021.emnlp-main.811,P15-1034,0,0.073423,"Missing"
2021.emnlp-main.811,K16-1002,0,0.0515527,"Missing"
2021.emnlp-main.811,D15-1084,0,0.0122043,"e demonstrate empirically that CUVA improves state of the art (SOTA) on the Entity C ANONICALIZATION task, across four academic benchmarks. 2 Related Work performs relation phrase clustering by using AMIE algorithm (Galárraga et al., 2013). (Wu et al., 2018) propose a modification to the previous approach by using pruning and bounding techniques. Concept Resolver (Krishnamurthy and Mitchell, 2011), which makes “one sense per category” assumption, is used for clustering NP mentions in NELL. This approach requires additional information in the form of a schema of relation types. KB-Unify (Delli Bovi et al., 2015) addresses the problem of unifying multiple canonical and open KGs into one KG, but requires additional sense inventory, which may not be available. The CESI architecture (Vashishth et al., 2018a) models the C ANONICALIZATION task in a two-step pipeline approach, i.e., in the first step, it uses a HolE algorithm (Nickel et al., 2016) to learn embeddings for NPs (and RPs), and then in an independent second step, it “plugs"" these learned embeddings into a Hierarchical Agglomerative Clustering (HAC) algorithm to generate clusters. Currently, CESI is the state of the art on this task. Unlike CESI,"
2021.emnlp-main.811,D11-1142,0,0.521894,"arks shows that CUVA outperare similar relations. forms the existing state-of-the-art approaches. Thus, the task of canonicalizing NPs and RPs Moreover, we introduce C ANONIC N ELL, a within an Open KG is significant. Otherwise, Open novel dataset to evaluate entity canonicalizaKGs will have an explosion of redundant facts, tion systems. which is highly undesirable, for the following reasons. Firstly, redundant facts use a higher mem1 Introduction ory footprint. Secondly, querying an Open KG is Open Information Extraction (OpenIE) methods likely to yield sub-optimal results, for e.g. it will (Fader et al., 2011a; Stanovsky et al., 2018) can not return all facts associated with NYC when usbe used to extract triples in the form (noun phrase, ing New York City as the query. Finally, allowing relation phrase, noun phrase) from given text cor- downstream applications such as Link Prediction pora in an unsupervised way without requiring a (Bordes et al., 2013) to know that NYC and New pre-defined ontology schema. This makes them York City refers to the same entity, will improve suitable to build large Open Knowledge Graphs their performance while operating on large Open (OpenKGs) from huge collections of"
2021.emnlp-main.811,P11-1058,0,0.0153467,"butions, • We introduce CUVA, a novel neural architecture for the C ANONICALIZATION task, based on joint learning of mention representations and cluster assignments for entity and relation clusters using variational autoencoders. • We demonstrate empirically that CUVA improves state of the art (SOTA) on the Entity C ANONICALIZATION task, across four academic benchmarks. 2 Related Work performs relation phrase clustering by using AMIE algorithm (Galárraga et al., 2013). (Wu et al., 2018) propose a modification to the previous approach by using pruning and bounding techniques. Concept Resolver (Krishnamurthy and Mitchell, 2011), which makes “one sense per category” assumption, is used for clustering NP mentions in NELL. This approach requires additional information in the form of a schema of relation types. KB-Unify (Delli Bovi et al., 2015) addresses the problem of unifying multiple canonical and open KGs into one KG, but requires additional sense inventory, which may not be available. The CESI architecture (Vashishth et al., 2018a) models the C ANONICALIZATION task in a two-step pipeline approach, i.e., in the first step, it uses a HolE algorithm (Nickel et al., 2016) to learn embeddings for NPs (and RPs), and the"
2021.emnlp-main.811,W12-3016,0,0.0290774,"ependency trees to entity (or relation) are in the same cluster. split the sentences into simpler and independent We assume that each cluster corresponds to eisegments. ther a latent entity or a latent relation; the label There have been several previous works to group of such a latent entity/relation is unknown to the NPs and RPs into coherent clusters. A traditional learner. approach to canonicalize NPs is to map them to CUVA uses two variational autoencoders i.e. Ean existing KB such as Wikidata, also referred to VAE and R-VAE, one each for entities and relaas the Entity Linking (EL) task (Lin et al., 2012; tions. Both E-VAE and R-VAE use a mixture of Ceccarelli et al., 2014). A major problem with Gaussians for modeling latent entities and relathese EL approaches is that many NPs may refer tions. Also, we use a Knowledge Graph Embedto entities that are not present in the KB, in which ding (KGE) module to encode the structural inforcase they are not clustered. mation present within the Open KG. CUVA works The RESOLVER system (Yates and Etzioni, as follows: 2009) uses string similarity features to cluster phrases in TextRunner (Banko et al., 2007) triples. 1. A latent entity (or relation) as defi"
2021.emnlp-main.811,N19-1112,0,0.0189041,"able 5 illustrates the output of our system for canonicalizing NPs and RPs on ReVerb45K. The top block corresponds to six NP clusters, one per 6.1 Comparison with Pretrained LMs line. The algorithm is able to correctly group kodagu and coorg (different name of the same dis- In this section, we investigate how CUVA fares trict in India), despite having completely different against pretrained language models. Table 6 illussurface forms. However, a common mistake that trates the results when evaluated on ReVerb45K. our proposed system makes is depicted in row five, Following the observations of (Liu et al., 2019; i.e., four different people each having the same Tenney et al., 2019), we use the lower layers (layers name bill are clustered together. This error can be one through six) of a pretrained BERT, RoBERTa mitigated by keeping track of the type information and a knowledge graph enhanced ERNIE (Zhang (Dash et al., 2020) of each NP for disambiguation. et al., 2019) base model via the HuggingFace TransThe bottom four rows in Table 5 correspond to formers library (Wolf et al., 2020). For building four RP clusters. While the equivalence of RPs is static representation for each entity mentions, we cap"
2021.emnlp-main.811,P15-2070,0,0.042802,"Missing"
2021.emnlp-main.811,D14-1162,0,0.0877683,"ing NPs and RPs for the Base, Ambiguous, and ReVerb45K datasets. For canonicalizing NPs on C ANONIC N ELL, we use IDF Token Overlap as the only strategy to generate Side Information. This strategy is an inherent property of the dataset and needs no external resources (Section A). Moreover, for C ANONIC N ELL we do not canonicalize the RPs, since they are already unique. Finally, a detailed description of the range of values tried per hyperparameter, and the final values used within CUVA is provided in Section C. 4.2 Results second row in Table 2, i.e., GloVe+HAC uses a pretrained GloVe model (Pennington et al., 2014a) to first build embeddings for entity mentions and then uses a HAC algorithm for clustering. For multi token phrases, GloVe embeddings for tokens are averaged together. GloVe captures the semantics of NPs and does not rely on its surface form, thus performing well across all the datasets. The third row augments GloVe+HAC by first initializing with pretrained GloVe vectors, followed by an optimization step wherein the Side Information (Section 3.3) loss objective is minimized, and finally clustering via the HAC algorithm. The fourth row, i.e. HolE (GloVe) uses the HolE Knowledge Graph Embeddi"
2021.emnlp-main.811,D12-1048,0,0.137813,"Missing"
2021.emnlp-main.811,spitkovsky-chang-2012-cross,0,0.0569841,"Missing"
2021.emnlp-main.811,N18-1081,0,0.0873144,"outperare similar relations. forms the existing state-of-the-art approaches. Thus, the task of canonicalizing NPs and RPs Moreover, we introduce C ANONIC N ELL, a within an Open KG is significant. Otherwise, Open novel dataset to evaluate entity canonicalizaKGs will have an explosion of redundant facts, tion systems. which is highly undesirable, for the following reasons. Firstly, redundant facts use a higher mem1 Introduction ory footprint. Secondly, querying an Open KG is Open Information Extraction (OpenIE) methods likely to yield sub-optimal results, for e.g. it will (Fader et al., 2011a; Stanovsky et al., 2018) can not return all facts associated with NYC when usbe used to extract triples in the form (noun phrase, ing New York City as the query. Finally, allowing relation phrase, noun phrase) from given text cor- downstream applications such as Link Prediction pora in an unsupervised way without requiring a (Bordes et al., 2013) to know that NYC and New pre-defined ontology schema. This makes them York City refers to the same entity, will improve suitable to build large Open Knowledge Graphs their performance while operating on large Open (OpenKGs) from huge collections of unstructured KGs. Hence, i"
2021.emnlp-main.811,L18-1008,0,0.030216,"hyperparameters used in these experiments. SI indicates whether an approach uses Side Information or not. metric, with the Micro and Pair F1 metrics achieving identical performance as CESI. Finally, on the Base dataset, CUVA achieves identical performance as CESI. Moreover, the results in Table 3 also show a similar trend when evaluated on all entity mentions, i.e., both head and tail NPs belonging to Reverb45K. Table 4 shows the results for the Entity Canonicalization task when evaluated on the C ANONIC N ELL dataset. The first two rows correspond to approaches that use pretrained FastText (Mikolov et al., 2018) and GloVe models to build unique embeddings for NPs and then use HAC to generate clusters. Moreover, in the absence of contextual information for the C ANONIC N ELL triples, both CESI and CUVA use IDF Token Overlap as the only source of Side Information. Moreover, from Table 4, it is clear that CUVA achieves the new state of the art result on this benchmark as well. 5 Qualitative Analysis Predicted Clusters for ReVerb45K {utc, coordinate universal time, universal coordinate time} 4 {justice alito, samuel alito, sam alito, alito} 4 {kodagu, coorg} 4 {johnny storm, human torch} 4 {bill cosby, b"
2021.emnlp-main.811,P19-1452,0,0.0128181,"and RPs on ReVerb45K. The top block corresponds to six NP clusters, one per 6.1 Comparison with Pretrained LMs line. The algorithm is able to correctly group kodagu and coorg (different name of the same dis- In this section, we investigate how CUVA fares trict in India), despite having completely different against pretrained language models. Table 6 illussurface forms. However, a common mistake that trates the results when evaluated on ReVerb45K. our proposed system makes is depicted in row five, Following the observations of (Liu et al., 2019; i.e., four different people each having the same Tenney et al., 2019), we use the lower layers (layers name bill are clustered together. This error can be one through six) of a pretrained BERT, RoBERTa mitigated by keeping track of the type information and a knowledge graph enhanced ERNIE (Zhang (Dash et al., 2020) of each NP for disambiguation. et al., 2019) base model via the HuggingFace TransThe bottom four rows in Table 5 correspond to formers library (Wolf et al., 2020). For building four RP clusters. While the equivalence of RPs is static representation for each entity mentions, we captured in the first two rows of the bottom block, use a mean pooling str"
2021.emnlp-main.811,D16-1177,0,0.0731498,"Missing"
2021.findings-acl.339,2020.findings-emnlp.89,1,0.729463,"d OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i. The edge set EG consists"
2021.findings-acl.339,W13-2322,0,0.0612546,"Missing"
2021.findings-acl.339,D13-1160,0,0.0607099,"f the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts"
2021.findings-acl.339,P13-2131,0,0.0801353,"Missing"
2021.findings-acl.339,dorr-etal-1998-thematic,0,0.326306,"Missing"
2021.findings-acl.339,kingsbury-palmer-2002-treebank,0,0.0480491,"(AMR) graph; (ii) transforms the AMR graph to a set of candidate KB-aligned logical queries, via a novel but simple graph transformation approach; (iii) uses a Logical Neural Network (LNN) (Riegel et al., 2020) to reason over KB facts and produce answers to KB-aligned logical queries. We describe each of these modules in the following sections. 2.1 AMR Parsing NSQA utilizes AMR parsing to reduce the complexity and noise of natural language questions. An AMR parse is a rooted, directed, acyclic graph. AMR nodes represent concepts, which may include normalized surface symbols, Propbank frames (Kingsbury and Palmer, 2002) as well as other AMR-specific constructs to handle named entities, quantities, dates and other phenomena. Edges in an AMR graph represent the relations between concepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate vari"
2021.findings-acl.339,2020.findings-emnlp.288,1,0.824748,"Missing"
2021.findings-acl.339,P19-1451,1,0.842549,"cepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i."
2021.findings-acl.339,N19-1243,0,0.0615569,"Missing"
2021.findings-acl.339,D07-1071,0,0.057132,"l start [sic] the final match of the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to re"
2021.findings-acl.339,N19-1301,0,0.0169546,"proaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgrap"
2021.findings-acl.339,D18-1110,1,0.837164,"ually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgraph via filling the entity and relationship slots of 12 predefined question template. Their performance on th"
2021.findings-acl.339,P14-1090,0,0.0858204,"Missing"
2021.findings-acl.339,P17-1053,1,0.839108,"rk in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions"
2021.internlp-1.5,2021.ccl-1.108,0,0.032091,"Missing"
2021.naacl-main.96,2020.acl-main.740,0,0.0463084,"Missing"
2021.naacl-main.96,2020.acl-main.398,0,0.0654805,"large corpora, rather than finding relevant rows and columns within tables. 3 MRC Model We provide a brief description of our underlying Machine Reading Comprehension (MRC) model architecture, which we use as a strong baseline. The architecture is inspired by (Alberti et al., 2019b; Pan et al., 2019; Glass et al., 2020) and direct interested readers to their papers for more deMore recently, transformer based pre-training tails. Our MRC model follows the approach introapproaches have been introduced in TA B ERT (Yin duced by (Devlin et al., 2019) of starting with a preet al., 2020) and TA PAS (Herzig et al., 2020) to trained transformer based language model (LM) improve accuracy for table QA. TA B ERT has been and then fine-tuning MRC specific feed-forward pre-trained on 26 million tables and NL sentences layers on both general question answering datasets extracted from Wikipedia and WDC WebTable Cor- (SQuAD 2.0 and NQ) as well as the table specific pus (Yin et al., 2020). The model can be plugged question answers associated with the datasets in into a neural semantic parser as an encoder to pro- Section 5. 1214 We use ALBERT (Lan et al., 2020) as the underlying LM similar to models which achieve SOTA"
2021.naacl-main.96,Q19-1026,0,0.0115083,"h from large corpora in Section 2. We then describe a weakly supervised machine reading comprehension (MRC) system as a baseline that is capable of answering questions over tables in Section 3. In Section 4, we introduce two models that decompose TableQA as the intersection between rows and columns of a table using a transformer architecture. Experimental results are reported and discussed in Section 5 and finally Section 6 concludes the paper and discusses the future work. 2 Related Work QA from text: There is plenty of work on QA from plain text (Brill et al., 2002; Lin, 2007; Pa¸sca, 2003; Kwiatkowski et al., 2019; Pan et al., 2019). • An MRC based strong baseline for table Typical strategies rely on token overlap between QA task: We investigate a transfer learning the question and passage text either based on a bag approach by utilizing a fully supervised read- of word statistics or contextualized language model ing comprehension system built on top of a representations. In either case, tabular structure is large pre-trained language model. Specifi- not leveraged to capture semantic relationships becally, it is first fine-tuned on SQuAD then tween rows and columns. As we show in Section 5, on Natural"
2021.naacl-main.96,N19-1423,0,0.189395,"f work focuses on retrieval of top-k tables with high precision from large corpora, rather than finding relevant rows and columns within tables. 3 MRC Model We provide a brief description of our underlying Machine Reading Comprehension (MRC) model architecture, which we use as a strong baseline. The architecture is inspired by (Alberti et al., 2019b; Pan et al., 2019; Glass et al., 2020) and direct interested readers to their papers for more deMore recently, transformer based pre-training tails. Our MRC model follows the approach introapproaches have been introduced in TA B ERT (Yin duced by (Devlin et al., 2019) of starting with a preet al., 2020) and TA PAS (Herzig et al., 2020) to trained transformer based language model (LM) improve accuracy for table QA. TA B ERT has been and then fine-tuning MRC specific feed-forward pre-trained on 26 million tables and NL sentences layers on both general question answering datasets extracted from Wikipedia and WDC WebTable Cor- (SQuAD 2.0 and NQ) as well as the table specific pus (Yin et al., 2020). The model can be plugged question answers associated with the datasets in into a neural semantic parser as an encoder to pro- Section 5. 1214 We use ALBERT (Lan et"
2021.naacl-main.96,D19-1284,0,0.0228308,"Missing"
2021.naacl-main.96,2020.acl-main.247,1,0.797487,"rieve similar tables (Das Sarma et al., 2012), retrieve tables based on column names (Pimplikar and Sarawagi, 2012) and adding new columns to existing entity lists (Yakout et al., 2012; Zhang and Chakrabarti, 2013). This thread of work focuses on retrieval of top-k tables with high precision from large corpora, rather than finding relevant rows and columns within tables. 3 MRC Model We provide a brief description of our underlying Machine Reading Comprehension (MRC) model architecture, which we use as a strong baseline. The architecture is inspired by (Alberti et al., 2019b; Pan et al., 2019; Glass et al., 2020) and direct interested readers to their papers for more deMore recently, transformer based pre-training tails. Our MRC model follows the approach introapproaches have been introduced in TA B ERT (Yin duced by (Devlin et al., 2019) of starting with a preet al., 2020) and TA PAS (Herzig et al., 2020) to trained transformer based language model (LM) improve accuracy for table QA. TA B ERT has been and then fine-tuning MRC specific feed-forward pre-trained on 26 million tables and NL sentences layers on both general question answering datasets extracted from Wikipedia and WDC WebTable Cor- (SQuAD"
2021.naacl-main.96,P15-1142,0,0.431582,"in this paper is on Lookup questions since the answers are verifiable by users although our proposed techniques outperform the state-of-the-art (SOTA) approaches on both question types. In this paper, we propose a new approach to table QA that independently predicts the probability Tabular data format is a commonly used layout in of containing the answer to a question in each row domain specific enterprise documents as well as and column of a table. By taking the Row and open domain webpages to store structured informa- Column Intersection (RCI) of these probabilistic tion in a compact form (Pasupat and Liang, 2015; predictions, RCI gives a probability for each cell Canim et al., 2019). In order to make use of these of the table. These probabilities are either used to resources, many techniques have been proposed for answer questions directly or highlight the relevant the retrieval of tables (Cafarella et al., 2008; Zhang regions of tables as a heatmap, helping users to easand Balog, 2018; Venetis et al., 2011; Shraga et al., ily locate the answers over tables (See Figure 1 for 2020; Sun et al., 2016). Given a large corpus of a question answered with the help of a heatmap). 1 We developed two models for"
2021.naacl-main.96,P18-2124,0,0.069166,"Missing"
2021.naacl-main.96,2020.acl-main.745,0,0.0723504,"for more deMore recently, transformer based pre-training tails. Our MRC model follows the approach introapproaches have been introduced in TA B ERT (Yin duced by (Devlin et al., 2019) of starting with a preet al., 2020) and TA PAS (Herzig et al., 2020) to trained transformer based language model (LM) improve accuracy for table QA. TA B ERT has been and then fine-tuning MRC specific feed-forward pre-trained on 26 million tables and NL sentences layers on both general question answering datasets extracted from Wikipedia and WDC WebTable Cor- (SQuAD 2.0 and NQ) as well as the table specific pus (Yin et al., 2020). The model can be plugged question answers associated with the datasets in into a neural semantic parser as an encoder to pro- Section 5. 1214 We use ALBERT (Lan et al., 2020) as the underlying LM similar to models which achieve SOTA on the SQuAD 2.0 leaderboard (Zhang et al., 2020b,a) at the time of writing. More specifically, we show results starting from the weights and dimensions of the base v2 version (25M parameters) of the LM shared by (Lan et al., 2020). We also experiment with the xxlarge v2 version (235M parameters) as well. The input to the model is a token sequence (X) consisting"
2021.naacl-main.96,D16-1264,0,0.11699,"Missing"
2021.naacl-main.96,W18-5446,0,0.0388105,"Missing"
2021.naacl-main.96,D19-1391,0,0.0232759,"Missing"
C08-1034,C02-1130,0,0.603902,"from seeds, and further seeds from patterns. Despite the fact that the evaluation was on relation extraction the method is general and might be applied to entity extraction and categorization. The approach was further extended by Agichtein and Gravano (2000). Our approach differs from theirs in that we do not learn patterns. Thus, we do not require ad hoc strategies for generating patterns and estimating their reliability, a crucial issue in these approaches as “bad” patterns may extract wrong seeds instances that in turn may generate even more inaccurate patterns in the following iteration. Fleischman and Hovy (2002) approached the ontology population problem as a supervised classification task. They compare different machine learning algorithms, providing instances in their context as training examples as well as more global semantic information derived from topic signature and WordNet. Alfonseca and Manandhar (2002) and Cimiano and V¨olker (2005) present similar approaches relying on the Harris’ distributional hypothesis and the vector-space model. They assign a particular instance represented by a certain context vector to the concept corresponding to the most similar vector. Contexts are represented u"
C08-1034,D07-1026,1,0.746589,"presentative Business man Health professional micro macro 0.4 10 20 30 40 50 Number of examples Figure 2: Learning curve on the People Ontology. gross misclassification errors (e.g., Biologist vs. Poet), while minor errors (e.g., Poet vs. Dramatist) are less relevant. This approach is similar to the one proposed by Melamed and Resnik (2000) for a similar hierarchical categorization task. 5.2 Accuracy Table 1 shows micro- and macro-averaged results of the proposed method obtained on the Tanev and Magnini (2006) benchmark and compares them with the class-example (Tanev and Magnini, 2006), IBLE (Giuliano and Gliozzo, 2007), and class-word (Cimiano and V¨olker, 2005) methods, respectively. Table 2 shows micro- and macroaveraged results of the proposed method obtained on the People Ontology and compares them with the random and most frequent baseline methods.7 In both experiments, the IBOP algorithm was trained on 20 examples per category and setting the parameter θ = 0 in Equation 2. For the People Ontology, we performed a disaggregated evaluation, whose results are shown in Table 3, while Figure 2 shows the learning curve. The experiment was conducted setting the parameter θ = 0. System IBOP Class-Example IBLE"
C08-1034,S07-1029,1,0.861423,"irst collect snippets containing it from the Web. Then, for each snippet, we substitute the new instance with each of the training instances. The snippets play a crucial role in our approach because we expect that they provide the features that characterize the category to which the entity belongs. Thus, it is important to collect a sufficiently large number of snippets to capture the features that allow a fine-grained classification. To estimate the correctness of each substitution, we calculate a plausibility score using a modified version of the lexical substitution algorithm introduced in Giuliano et al. (2007), that assigns higher scores to the substitutions that generate highly frequent sentences on the Web. In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus,2 which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). Finally, on the basis of these plausibility scores, the algorithm assigns the new instance to the category whose individuals show a closer linguisti"
C08-1034,C92-2082,0,0.0197211,"iding instances in their context as training examples as well as more global semantic information derived from topic signature and WordNet. Alfonseca and Manandhar (2002) and Cimiano and V¨olker (2005) present similar approaches relying on the Harris’ distributional hypothesis and the vector-space model. They assign a particular instance represented by a certain context vector to the concept corresponding to the most similar vector. Contexts are represented using lexicalsyntactic features. KnowItAll (Etzioni et al., 2005) uses a search engine and semantic patterns (similar to those defined by Hearst (1992)) to classify named entities on the Web. The approach uses simple techniques from the ontology learning field to perform extraction and then annotation. It also is able to perform very simple pattern induction, consisting of looking at n words before and n words after the occurrence of an example in the document. With pattern learning, KnowItAll becomes a bootstrapped learning system, where rules are used to learn new seeds, which in turn are used to learn new rules. A similar approach is used in C-PANKOW (Cimiano et al., 2005). Compared to KnowItAll and C-PANKOW, our approach does not need ha"
C08-1034,S07-1009,0,0.012627,"imate the correctness of each substitution, we calculate a plausibility score using a modified version of the lexical substitution algorithm introduced in Giuliano et al. (2007), that assigns higher scores to the substitutions that generate highly frequent sentences on the Web. In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus,2 which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). Finally, on the basis of these plausibility scores, the algorithm assigns the new instance to the category whose individuals show a closer linguistic behavior (i.e., they can be substituted generating plausible statements). 3 The IBOP algorithm In this section, we describe the algorithmic and mathematical details of our approach. The instance-based ontology population (IBOP) algorithm is an instance-based supervised machine learning approach.3 The proposed algorithm is summarized as follows: Step 1 For each candidate instance i, we collect the first N snippets containing i from the Web. For"
C08-1034,W02-0816,0,0.0176921,"gy. In most of the cases, ontologies are partially populated during the development phase and after that the annotation cost is practically negligible, making this method highly attractive in many applicative domains. This allows us to define an instance-based learning approach for fine-grained c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Lexical substitution consists in identifying the most likely alternatives (substitutes) of a target word given its context (McCarthy, 2002). We present an approach to ontology population based on a lexical substitution technique. It consists in estimating the plausibility of sentences where the named entity to be classified is substituted with the ones contained in the training data, in our case, a partially populated ontology. Plausibility is estimated by using Web data, while the classification algorithm is instance-based. We evaluated our method on two different ontology population tasks. Experiments show that our solution is effective, outperforming existing methods, and it can be applied to practical ontology population prob"
C08-1034,E06-1003,0,0.152776,"ical ontology population problems. 1 Introduction 265 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 265–272 Manchester, August 2008 entity categorization that exploits the Web to collect evidence of the new entities and does not require any labeled text for supervision, only a partially populated ontology. Therefore, it can be used in different domains and languages to enrich an existing ontology with new entities extracted from texts by a named-entity recognition system and/or databases. We evaluated our method on the benchmark proposed by Tanev and Magnini (2006) to provide a fair comparison with other approaches, and on a general purpose ontology of people derived from WordNet (Fellbaum, 1998) to perform a more extensive evaluation. Specifically, the experiments were designed to investigate the effectiveness of our approach at different levels of generality and with different amounts of training data. The results show that it significantly outperforms the baseline methods and, where a comparison is possible, other approaches and achieves a good performance with a small number of examples per category. Error analysis shows that most of the misclassifi"
C12-1058,A97-1029,0,0.0589246,"of identifying mentions of rigid designators, especially people, places and organizations (Coates-Stephens, 1992). Recently, biological types such as protein, DNA and cell line have gained attention (Settles, 2004). Unlike automatic term recognition, where the goal is to build a dictionary of terms, NER typically proceeds from a corpus annotated with mentions and their types and learns a model for detecting future mentions. One common model is the linear chain Conditional Random Field (CRF) (McCallum and Li, 2003). This is the discriminatively trained variant of the Hidden Markov Model (HMM) (Bikel et al., 1997). In the simplest version of such models, each word belonging to a mention for some type is tagged with that type’s identifier. One weakness of this simple version is that in the case where two mentions of a certain type are contiguous, it is not possible to tell where one begins and the other ends - or even that there are two mentions rather than one. To alleviate this problem, and enrich the model, more complex tag sets are used. A model might have a tag for the beginning, inside and end of each type of entity. This enables the model to learn words that are more likely to begin and end menti"
C12-1058,E06-3004,0,0.0197645,"s entities of the selected type and patterns for identifying the entities. From a small set of seed entities, lexico-syntactic patterns are learned that suggest an entity of the appropriate type. For example, the noun phrase following “headquarted in” is likely to be a location. Extraction patterns are scored by their frequency and (estimated) reliability and entities are scored by the weighted sum of patterns that identify them. This scoring helps to alleviate the core problem of bootstrapping: semantic drift and also allows for a precision/recall trade-off. A related bootstrapping approach (Kozareva, 2006) trains an NER classifier based on the current gazetteer then runs the classifier over text and adds the new terms recognized by the classifier to the gazetteer. This method requires a recognition system that goes beyond dictionary matching and uses context to a significant degree. The features used by the classifier include capitalization, trigger words specific to locations, organizations and people, and whether words in the noun phrase belong to a gazetteer for the type of interest. Unlike traditional NER bootstrapping, STR assumes an existing large dictionary with hundreds if not thousands"
C12-1058,P98-2127,0,0.106239,"erns. (b) Patterns are the left hand side of PCFG rules. 2.1 Word Clustering The goal of this component is to cluster the words in W into clusters C1 , C2 , . . . , Cn . Clusters are partly determined by distributional similarity, meaning that words in the same cluster 945 have similar meanings because they are substitutable in context. This follows a large body of work in unsupervised clustering of words. Early work used distributional similarity to perform part-of-speech induction (Schütze, 1993). Later work has developed automatic methods of thesaurus construction using similar techniques (Lin, 1998). Using a large, domain specific, unannotated corpus U, STR gathers vectors describing the context in which each word is found. Since parsing is generally lower quality in non-newswire domains, we do not use dependency based contexts. Instead the corpus is simply tokenized. Contexts are built from a small window of surrounding words: two words to the left and two words to the right. The information about the direction and distance from the word is retained. For example, in the sentence “Early detection of cancer increases the odds of successful treatment.” the contexts for the word “cancer” ar"
C12-1058,C00-1077,0,0.0162824,"on ATR and term clustering. In STR we identify that a sequence of words is a term and that it has a particular type in a single process. ATR operates by first gathering strings that are terms and then clustering them, or extending existing clusters where each cluster of terms are terms of a common type. After terms are identified, some measure of term similarity is applied to cluster the terms or extend existing clusters of terms. Some methods of term similarity are contextual, lexical and pattern-based. Contextual similarity uses information about what words appear before and after the term (Maynard and Ananiadou, 2000), or what words are nearby in a dependency parse (Grefenstette, 1994). The frequency of each of these contexts is a dimension in a vector describing the distribution of the term. These vectors can then be compared for similarity using some vector similarity function. Lexical similarity examines what words are in common between two terms. For example, two terms with the same head word are likely to have the same type. Pattern-based similarity (Nenadi´c et al., 2002) uses lexico-syntactic patterns like the such-as pattern “X such as Y and Z”. These patterns can be used as evidence that the terms"
C12-1058,W03-0430,0,0.0141298,"med Entity Recognition Our methods are also similar to Named Entity Recognition (NER). NER is the task of identifying mentions of rigid designators, especially people, places and organizations (Coates-Stephens, 1992). Recently, biological types such as protein, DNA and cell line have gained attention (Settles, 2004). Unlike automatic term recognition, where the goal is to build a dictionary of terms, NER typically proceeds from a corpus annotated with mentions and their types and learns a model for detecting future mentions. One common model is the linear chain Conditional Random Field (CRF) (McCallum and Li, 2003). This is the discriminatively trained variant of the Hidden Markov Model (HMM) (Bikel et al., 1997). In the simplest version of such models, each word belonging to a mention for some type is tagged with that type’s identifier. One weakness of this simple version is that in the case where two mentions of a certain type are contiguous, it is not possible to tell where one begins and the other ends - or even that there are two mentions rather than one. To alleviate this problem, and enrich the model, more complex tag sets are used. A model might have a tag for the beginning, inside and end of ea"
C12-1058,W02-1408,0,0.0860323,"Missing"
C12-1058,P93-1034,0,0.320656,"ype similarity. 2. Constructing patterns describing terms in the dictionary. (a) Sequences of Clusters are patterns. (b) Patterns are the left hand side of PCFG rules. 2.1 Word Clustering The goal of this component is to cluster the words in W into clusters C1 , C2 , . . . , Cn . Clusters are partly determined by distributional similarity, meaning that words in the same cluster 945 have similar meanings because they are substitutable in context. This follows a large body of work in unsupervised clustering of words. Early work used distributional similarity to perform part-of-speech induction (Schütze, 1993). Later work has developed automatic methods of thesaurus construction using similar techniques (Lin, 1998). Using a large, domain specific, unannotated corpus U, STR gathers vectors describing the context in which each word is found. Since parsing is generally lower quality in non-newswire domains, we do not use dependency based contexts. Instead the corpus is simply tokenized. Contexts are built from a small window of surrounding words: two words to the left and two words to the right. The information about the direction and distance from the word is retained. For example, in the sentence “E"
C12-1058,W04-1221,0,0.0254627,"mines the words inside the term and this metric only considers lexical matches between the words of the term, not the types of the words or the structure of the words in the term. The main advantage of this type of work is that it can proceed without an existing term dictionary. 4.2 Supervised Named Entity Recognition Our methods are also similar to Named Entity Recognition (NER). NER is the task of identifying mentions of rigid designators, especially people, places and organizations (Coates-Stephens, 1992). Recently, biological types such as protein, DNA and cell line have gained attention (Settles, 2004). Unlike automatic term recognition, where the goal is to build a dictionary of terms, NER typically proceeds from a corpus annotated with mentions and their types and learns a model for detecting future mentions. One common model is the linear chain Conditional Random Field (CRF) (McCallum and Li, 2003). This is the discriminatively trained variant of the Hidden Markov Model (HMM) (Bikel et al., 1997). In the simplest version of such models, each word belonging to a mention for some type is tagged with that type’s identifier. One weakness of this simple version is that in the case where two m"
C12-1058,E95-1004,0,0.256533,"Missing"
C12-1058,Y10-1067,0,0.0603165,"Missing"
C12-1058,C98-2122,0,\N,Missing
C16-1218,P14-2013,0,0.567176,"pical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2310 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2310–2320, Osaka, Japan, December 11-17 2016. 2011; Hoffart et al., 2011; Ratinov et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015; Francis-Landau et al., 2016). The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obtain a coherent set of target enti"
C16-1218,W10-3503,0,0.437759,"Missing"
C16-1218,W06-1615,0,0.0661441,"els (except for the ACE dataset on Wikipedia 2014 and the WIKI dataset on Wikipedia 2016), thereby demonstrating the benefits of the joint modeling for local and global features via neural networks for EL in this work. 3.5 Domain Adaptation Experiments The purpose of this section is to further evaluate the models in the domain adaptation setting to investigate their cross-domain robustness for EL. It is often observed in many natural language processing tasks that the performance of a model trained on a source domain would degrade significantly when it is applied to a different target domain (Blitzer et al., 2006; Daume, 2007; McClosky et al., 2010; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Ji"
C16-1218,E06-1002,0,0.619818,"g the available context information in both the documents and the knowledge bases. The early approach for the ranking problem in EL has resolved the entity mentions in documents independently (the local approach), utilizing various discrete and hand-designed features/heuristics to measure the local mention-to-entity relatedness for ranking. These features are often specific to each entity mention and candidate entity, covering a wide range of linguistic and/or structured representations such as lexical and part-of-speech tags of context words, dependency paths, topical features, KB infoboxes (Bunescu and Pasca, 2006; Mendes et al., 2011; Cassidy et al., 2011; Ji and Grishman, 2011; Shen et al., 2014) etc. Although the local approach can exploit a rich set of discrete structures for EL, its limitation is twofold: (i) The independent ranking mechanism in the local approach overlooks the topical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/"
C16-1218,D13-1184,0,0.0479497,"(Bunescu and Pasca, 2006; Milne and Witten, 2008; Zheng et al., 2010; Ji and Grishman, 2011; Mendes et al., 2011; Cassidy et al., 2011; Shen et al., 2014). In contrast, the global approach jointly maps all the entity mentions within documents to model the topical coherence. Various techniques have been exploited for capturing such semantic consistency, including Wikipedia category agreement (Cucerzan, 2007), Wikipedia link-based measures (Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012), Point-wise Mutual Information measures (Ratinov et al., 2011), integer linear programming (Cheng and Roth, 2013), PageRank (Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015), stacked generalization (He et al., 2013a), to name a few. The entity linking techniques and systems have been actively evaluated at the NIST-organized Text Analysis Conference (Ji et al., 2014). Neural networks are applied to entity linking very recently. He et al. (2013b) learn enttiy representation via Stacked Denoising Auto-encoders. Sun et al. (2015) employ convolutional neural networks and neural tensor networks to model mentions, entities and contexts while Francis-Landau et al. (2016) combine CNN-based representations w"
C16-1218,D07-1074,0,0.612406,"internal structures of each separate mention-entity pair, covering the name string comparisons between the surfaces of the entity mentions and target candidates, entity popularity or entity type and so on (Bunescu and Pasca, 2006; Milne and Witten, 2008; Zheng et al., 2010; Ji and Grishman, 2011; Mendes et al., 2011; Cassidy et al., 2011; Shen et al., 2014). In contrast, the global approach jointly maps all the entity mentions within documents to model the topical coherence. Various techniques have been exploited for capturing such semantic consistency, including Wikipedia category agreement (Cucerzan, 2007), Wikipedia link-based measures (Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012), Point-wise Mutual Information measures (Ratinov et al., 2011), integer linear programming (Cheng and Roth, 2013), PageRank (Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015), stacked generalization (He et al., 2013a), to name a few. The entity linking techniques and systems have been actively evaluated at the NIST-organized Text Analysis Conference (Ji et al., 2014). Neural networks are applied to entity linking very recently. He et al. (2013b) learn enttiy representation via Stacked Denoisin"
C16-1218,P07-1033,0,0.069154,"E dataset on Wikipedia 2014 and the WIKI dataset on Wikipedia 2016), thereby demonstrating the benefits of the joint modeling for local and global features via neural networks for EL in this work. 3.5 Domain Adaptation Experiments The purpose of this section is to further evaluate the models in the domain adaptation setting to investigate their cross-domain robustness for EL. It is often observed in many natural language processing tasks that the performance of a model trained on a source domain would degrade significantly when it is applied to a different target domain (Blitzer et al., 2006; Daume, 2007; McClosky et al., 2010; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Jiang, 2009; Pl"
C16-1218,Q14-1037,0,0.475289,"n to link other mentions in that document due to the semantic relatedness among them. For example, the appearances of “Manchester” and “Chelsea” as the football clubs in a document would make it more likely that the entity mention “Liverpool” in the same document is also a football club. Unfortunately, the coherence assumption of the global approach does not hold in some situations, necessitating the discrete/coarse features in the local approach as a mechanism to compensate for the potential exceptions of the coherence assumption (Ratinov et al., 2011; Hoffart et al., 2011; Sil et al., 2012; Durrett and Klein, 2014; Pershina et al., 2015). Consequently, the global approach is still subject to the second limitation of data sparseness of the local approach due to their use of discrete features. Recently, the surge of neural network (NN) models has presented an effective mechanism to mitigate the second limitation of the local approach. In such models, words are represented by continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013) and features for the entity mentions and candidate entities are automatically learnt from data. This essentially alleviates the data sparsene"
C16-1218,E14-1052,0,0.0329515,"Missing"
C16-1218,N16-1150,0,0.0631352,"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2310–2320, Osaka, Japan, December 11-17 2016. 2011; Hoffart et al., 2011; Ratinov et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015; Francis-Landau et al., 2016). The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obtain a coherent set of target entities. The central idea is that the referent entities of some mentions in a document might in turn introduce useful information to link other mentions in that document due to the semantic relatedness among them. For example, the appearances of “Manchester” and “Chelsea” as the football clubs in a document would make it more likely that the entity mention “Liverpool”"
C16-1218,D15-1205,0,0.0184998,"ains should be related to each other. Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach (Francis-Landau et al., 2016). 3.5.1 Dataset We use the ACE dataset to evaluate the cross-domain performance of the models. ACE involves documents in 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen et al., 2015c; Gormley et al., 2015), we use news (the union of bn and nw) as the source domain and bc, cts, wl, un as four different target domains. We take half of bc as the development set and use the remaining data for testing. We note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles (Plank and Moschitti, 2013). 3.5.2 Evaluation Table 3 compares Global-RNN with the neural network EL model in (Francis-Landau et al., 2016), the best reported model on the ACE dataset in the literature8 ."
C16-1218,D13-1041,0,0.0781351,"h overlooks the topical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2310 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2310–2320, Osaka, Japan, December 11-17 2016. 2011; Hoffart et al., 2011; Ratinov et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015; Francis-Landau et al., 2016). The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obt"
C16-1218,P13-2006,0,0.497707,"h overlooks the topical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2310 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2310–2320, Osaka, Japan, December 11-17 2016. 2011; Hoffart et al., 2011; Ratinov et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015; Francis-Landau et al., 2016). The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obt"
C16-1218,D11-1072,0,0.559865,"Missing"
C16-1218,P11-1115,0,0.243312,"owledge bases. The early approach for the ranking problem in EL has resolved the entity mentions in documents independently (the local approach), utilizing various discrete and hand-designed features/heuristics to measure the local mention-to-entity relatedness for ranking. These features are often specific to each entity mention and candidate entity, covering a wide range of linguistic and/or structured representations such as lexical and part-of-speech tags of context words, dependency paths, topical features, KB infoboxes (Bunescu and Pasca, 2006; Mendes et al., 2011; Cassidy et al., 2011; Ji and Grishman, 2011; Shen et al., 2014) etc. Although the local approach can exploit a rich set of discrete structures for EL, its limitation is twofold: (i) The independent ranking mechanism in the local approach overlooks the topical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2310 Proceedings of COLI"
C16-1218,P09-1114,0,0.0281284,"06; Daume, 2007; McClosky et al., 2010; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Jiang, 2009; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). These invariants serve as the connectors between different domains and help to transfer the knowledge from one domain to the others. For EL, we hypothesize that the global coherence is an effective domain-independent feature that would help to improve the crossdomain performance of the models. The intuition is that the entities mentioned in a document of any domains should be related to each other. Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach ("
C16-1218,P14-1062,0,0.0134622,"a sparseness of the local approach due to their use of discrete features. Recently, the surge of neural network (NN) models has presented an effective mechanism to mitigate the second limitation of the local approach. In such models, words are represented by continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013) and features for the entity mentions and candidate entities are automatically learnt from data. This essentially alleviates the data sparseness problem of unseen words/features and helps to extract more effective features for EL in a given dataset (Kalchbrenner et al., 2014; Nguyen et al., 2016a). In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015; Francis-Landau et al., 2016). However, as the previous NN models for EL are local, they cannot capture the global interdependence among the target entities in the same document (the first limitation of the local approach). Guided by these analyses, in this paper, we propose to use neural networks to model both the local mention-to-entity similarities and the global relatedness among target entities in an"
C16-1218,N10-1004,0,0.0111721,"Wikipedia 2014 and the WIKI dataset on Wikipedia 2016), thereby demonstrating the benefits of the joint modeling for local and global features via neural networks for EL in this work. 3.5 Domain Adaptation Experiments The purpose of this section is to further evaluate the models in the domain adaptation setting to investigate their cross-domain robustness for EL. It is often observed in many natural language processing tasks that the performance of a model trained on a source domain would degrade significantly when it is applied to a different target domain (Blitzer et al., 2006; Daume, 2007; McClosky et al., 2010; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Jiang, 2009; Plank and Moschitti, 2013"
C16-1218,P14-2012,1,0.853076,"016), thereby demonstrating the benefits of the joint modeling for local and global features via neural networks for EL in this work. 3.5 Domain Adaptation Experiments The purpose of this section is to further evaluate the models in the domain adaptation setting to investigate their cross-domain robustness for EL. It is often observed in many natural language processing tasks that the performance of a model trained on a source domain would degrade significantly when it is applied to a different target domain (Blitzer et al., 2006; Daume, 2007; McClosky et al., 2010; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Jiang, 2009; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). These invariants serve"
C16-1218,W15-1506,1,0.837489,"on for x, we first transform each word xi ∈ x into a real-valued, h-dimensional vector wi using the word embedding table E (Mikolov et al., 2013): wi = E[xi ]. This essentially converts the word sequence x into a sequence of vectors that is padded with zero vectors to form a fixed-length sequence of vectors w = (w1 , w2 , . . . , wn ) of length n. In the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non-linear function G and pooled by the sum function (Francis-Landau et al., 2016). Following the previous work on CNN (Nguyen and Grishman, (2015a; 2015b)), we utilize the set L of multiple window sizes to parameterize the convolution operation. Each window size l ∈ L corresponds to a convolution matrix Ml ∈ Rv×lh of dimensionality v. Eventually, the concatenation vector x ¯ of the resulting vectors for each window size in L would be used as the distributed representation for x: x¯ = L M n−l+1 X l∈L G(Ml wi:(i+l−1) ) i=1 where is the concatenation operation over the window set L and wi:(i+l−1) is the concatenation vector of the given word vectors. For convenience, let s¯i , c¯i , d¯i , t¯ij , ¯bij , t¯∗i and ¯b∗i be the distributed rep"
C16-1218,P15-2060,1,0.862736,"on for x, we first transform each word xi ∈ x into a real-valued, h-dimensional vector wi using the word embedding table E (Mikolov et al., 2013): wi = E[xi ]. This essentially converts the word sequence x into a sequence of vectors that is padded with zero vectors to form a fixed-length sequence of vectors w = (w1 , w2 , . . . , wn ) of length n. In the next step, we apply the convolution operation over w to generate the hidden vector sequence, that is then transformed by a non-linear function G and pooled by the sum function (Francis-Landau et al., 2016). Following the previous work on CNN (Nguyen and Grishman, (2015a; 2015b)), we utilize the set L of multiple window sizes to parameterize the convolution operation. Each window size l ∈ L corresponds to a convolution matrix Ml ∈ Rv×lh of dimensionality v. Eventually, the concatenation vector x ¯ of the resulting vectors for each window size in L would be used as the distributed representation for x: x¯ = L M n−l+1 X l∈L G(Ml wi:(i+l−1) ) i=1 where is the concatenation operation over the window set L and wi:(i+l−1) is the concatenation vector of the given word vectors. For convenience, let s¯i , c¯i , d¯i , t¯ij , ¯bij , t¯∗i and ¯b∗i be the distributed rep"
C16-1218,P15-1062,1,0.848157,"a document of any domains should be related to each other. Eventually, we expect that the proposed model with global coherence features would be more robust to domain shifts than the local approach (Francis-Landau et al., 2016). 3.5.1 Dataset We use the ACE dataset to evaluate the cross-domain performance of the models. ACE involves documents in 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following the common practice of domain adaptation research on this dataset (Plank and Moschitti, 2013; Nguyen et al., 2015c; Gormley et al., 2015), we use news (the union of bn and nw) as the source domain and bc, cts, wl, un as four different target domains. We take half of bc as the development set and use the remaining data for testing. We note that news consists of formally written documents while a majority of the other domains is informal text, making the source and target domains very divergent in terms of vocabulary and styles (Plank and Moschitti, 2013). 3.5.2 Evaluation Table 3 compares Global-RNN with the neural network EL model in (Francis-Landau et al., 2016), the best reported model on the ACE datas"
C16-1218,N16-1034,1,0.779583,"pproach due to their use of discrete features. Recently, the surge of neural network (NN) models has presented an effective mechanism to mitigate the second limitation of the local approach. In such models, words are represented by continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013) and features for the entity mentions and candidate entities are automatically learnt from data. This essentially alleviates the data sparseness problem of unseen words/features and helps to extract more effective features for EL in a given dataset (Kalchbrenner et al., 2014; Nguyen et al., 2016a). In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015; Francis-Landau et al., 2016). However, as the previous NN models for EL are local, they cannot capture the global interdependence among the target entities in the same document (the first limitation of the local approach). Guided by these analyses, in this paper, we propose to use neural networks to model both the local mention-to-entity similarities and the global relatedness among target entities in an unified architecture."
C16-1218,N15-1026,0,0.538127,"entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2310 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2310–2320, Osaka, Japan, December 11-17 2016. 2011; Hoffart et al., 2011; Ratinov et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015; Francis-Landau et al., 2016). The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simultaneously to obtain a coherent set of target entities. The central idea i"
C16-1218,P13-1147,0,0.136387,"WIKI dataset on Wikipedia 2016), thereby demonstrating the benefits of the joint modeling for local and global features via neural networks for EL in this work. 3.5 Domain Adaptation Experiments The purpose of this section is to further evaluate the models in the domain adaptation setting to investigate their cross-domain robustness for EL. It is often observed in many natural language processing tasks that the performance of a model trained on a source domain would degrade significantly when it is applied to a different target domain (Blitzer et al., 2006; Daume, 2007; McClosky et al., 2010; Plank and Moschitti, 2013; Nguyen and Grishman, 2014a). Such a performance loss originates from a variety of mismatches between the source and the target domains, including the differences in vocabulary, data distributions, styles etc. This has motivated the domain adaptation research that aims to improve the cross-domain performance of the models by adaptation techniques. One of the key strategies of the domain adaptation techniques is the search for the domain-independent features that are discriminative across different domains (Blitzer et al., 2006; Jiang, 2009; Plank and Moschitti, 2013; Nguyen and Grishman, 2014"
C16-1218,P11-1157,0,0.0255394,"Missing"
C16-1218,P11-1138,0,0.566842,"m in the local approach overlooks the topical coherence among the target entities referred by the entity mentions within the same document. This is undesirable as the topical coherence has been shown to be effective for EL in the previous work (Han et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2310 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2310–2320, Osaka, Japan, December 11-17 2016. 2011; Hoffart et al., 2011; Ratinov et al., 2011; He et al., 2013b; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). (ii) The local approach might suffer from the data sparseness issue of unseen words/features, the difficulty of calibrating, and the failure to induce the underlying similarity structures at high levels of abstraction for EL (due to the extensive reliance on the hand-designed coarse features) (Sun et al., 2015; Francis-Landau et al., 2016). The first drawback of the local approach has been overcome by the global models in which all entity mentions (or a group of entity mentions) within a document are disambiguated simu"
C16-1218,D12-1011,0,0.0143572,"useful information to link other mentions in that document due to the semantic relatedness among them. For example, the appearances of “Manchester” and “Chelsea” as the football clubs in a document would make it more likely that the entity mention “Liverpool” in the same document is also a football club. Unfortunately, the coherence assumption of the global approach does not hold in some situations, necessitating the discrete/coarse features in the local approach as a mechanism to compensate for the potential exceptions of the coherence assumption (Ratinov et al., 2011; Hoffart et al., 2011; Sil et al., 2012; Durrett and Klein, 2014; Pershina et al., 2015). Consequently, the global approach is still subject to the second limitation of data sparseness of the local approach due to their use of discrete features. Recently, the surge of neural network (NN) models has presented an effective mechanism to mitigate the second limitation of the local approach. In such models, words are represented by continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013) and features for the entity mentions and candidate entities are automatically learnt from data. This essentially all"
C16-1218,P10-1040,0,0.0292972,"al approach as a mechanism to compensate for the potential exceptions of the coherence assumption (Ratinov et al., 2011; Hoffart et al., 2011; Sil et al., 2012; Durrett and Klein, 2014; Pershina et al., 2015). Consequently, the global approach is still subject to the second limitation of data sparseness of the local approach due to their use of discrete features. Recently, the surge of neural network (NN) models has presented an effective mechanism to mitigate the second limitation of the local approach. In such models, words are represented by continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013) and features for the entity mentions and candidate entities are automatically learnt from data. This essentially alleviates the data sparseness problem of unseen words/features and helps to extract more effective features for EL in a given dataset (Kalchbrenner et al., 2014; Nguyen et al., 2016a). In practice, the features automatically induced by NN are combined with the discrete features in the local approach to extend their coverage for EL (Sun et al., 2015; Francis-Landau et al., 2016). However, as the previous NN models for EL are local, they cannot capture the glo"
C16-1218,N10-1072,0,0.0165096,"ews. 4 Related Work Entity linking or disambiguation has been studied extensively in NLP research, falling broadly into two major approaches: local and global disambiguation. Both approaches share the goal of measuring the similarities between the entity mentions and the target candidates in the reference KB. The local paradigm focuses on the internal structures of each separate mention-entity pair, covering the name string comparisons between the surfaces of the entity mentions and target candidates, entity popularity or entity type and so on (Bunescu and Pasca, 2006; Milne and Witten, 2008; Zheng et al., 2010; Ji and Grishman, 2011; Mendes et al., 2011; Cassidy et al., 2011; Shen et al., 2014). In contrast, the global approach jointly maps all the entity mentions within documents to model the topical coherence. Various techniques have been exploited for capturing such semantic consistency, including Wikipedia category agreement (Cucerzan, 2007), Wikipedia link-based measures (Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012), Point-wise Mutual Information measures (Ratinov et al., 2011), integer linear programming (Cheng and Roth, 2013), PageRank (Alhelbawy and Gaizauskas, 2014; Pers"
D07-1026,P06-1057,1,0.916979,"hether the substitution of a particular word w with the word e in a coherent text Hw = H l wH r generates a sentence He = H l eH r such that Hw → He , where H l and H r denote the left and the right context of w, respectively. For example, given the word ‘weapon’ a system may substitute it with the synonym ‘arm’, in order to identify relevant texts that denote the sought concept using the latter term. A particular case of lexical entailment is recognizing synonymy, where both Hw → He and He → Hw hold. In the literature, slight variations of this problem are also referred to as sense matching (Dagan et al., 2006), lexical reference (Glickman et al., 2006a) and lexical substitution (Glickman et al., 2006b). They have been applied to a wide variety of tasks, such as semantic matching, subtitle generation and Word Sense Disambiguation (WSD). Modeling lexical entailment is also a prerequisite to approach the SemEval-2007 lexical substitution task1 , consisting of finding alternative words that can occur in given context. In this paper, we propose to apply an approach for lexical entailment to the ontology population task. The basic idea is that if a word entails another one in a given context then the for"
D07-1026,P04-1036,0,0.0199237,"00 country 15037 Table 2: Number of training examples for each class. 5.2 Results Table 4 shows our results compared with two baselines (i.e., random and most frequent, estimated from the test data) and the two alternative approaches for ontology population described in the previous section. Our system outperforms both baselines and largely surpasses the Class-Word unsupervised method. It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al., 2004). In addition, our system is also competitive when compared to supervised approaches, being it only 5 points lower than the Class-Example method, while it does not require seed examples and syntactic parsing. This characteristic makes our system flexible and adaptable to different languages and domains. System RND Baseline Class-Word MF baseline IBLE Class-Example Micro F1 0.20 0.42 0.52 0.57 0.62 Macro F1 0.20 0.33 NA 0.47 0.68 Table 3: Comparison of different ontology population techniques. 7 6 person statesman 119 writer 3436 athlete 642 actor 2356 inventor 105 http://www.csie.ntu.edu.tw/∼c"
D07-1026,E06-1003,0,0.570859,"ained entity (e.g., location), the candidate entailed words are the terms corresponding to the fine-grained classes (e.g., lake or mountain) and the entailing words are mentions of entities (e.g., New York, Ontario) belonging to the coarse-grained class, recognized by an entity tagger. Experiments show that our method for recognizing lexical entailment is effective for the ontology population task, reporting improvements over a state-of-the-art unsupervised technique based on contextual similarity measures (Cimiano and V¨olker, 2005). In addition, we also compared it to a supervised approach (Tanev and Magnini, 2006), that we regarded as an upper bound, obtaining comparable results. 2 The Ontology Population Task Populating concepts of a predefined ontology with instances found in a corpus is a primary goal of knowledge management systems. As concepts in the ontology are generally structured into hierarchies belonging to a common ontological type (e.g., people or locations), the problem of populating ontologies can be solved hierarchically, firstly identifying instances in texts as belonging to the topmost concepts, and then assigning them to a fine-grained class. Supervised named entity recognition (NER)"
D07-1026,C02-1130,0,0.23978,"ion (NER) systems can be used for accomplishing the first step. State-of-the-art NER systems are characterized by high accuracy, but they require a large amount of training data. However, domain specific ontologies generally contains many “fine-grained” categories (e.g., particular categories of people, such as writers, scientists, and so on) and, as a consequence, supervised methods cannot be used because the annotation costs would become prohibitive. Therefore, in the literature, the fine-grained classification task has been approached by adopting weakly supervised (Tanev and Magnini, 2006; Fleischman and Hovy, 2002) or unsupervised methods (Cimiano and V¨olker, 2005). Tanev and Magnini (2006) proposed a weakly supervised method that requires as training data a list of terms without context for each class under consideration. Such list can be automatically acquired from existing ontologies or other sources (i.e., database fields, web sites like Wikipedia, etc.) since the approach imposes virtually no restrictions on them. Given a generic syntactically parsed corpus containing at least each training entity twice, the algorithm learns, for each class, a feature vector describing the contexts where those ent"
D07-1026,W00-0726,0,0.015622,"e English CLEF corpus6 . Even the evaluation task is rather small and can be perceived as an artificial experimental setting, it is the best available benchmark we can use to compare our system to existing approaches in the literature, as we are not aware of other available resources. To perform NER we used CRFs (Lafferty et al., 2001). We trained a first-order CRF on the MUC data set to annotate locations and people. In our experiments, we used the implementation provided in MALLET (McCallum, 2002). We used a standard feature set inspired by the literature on text chunking and NER (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) to train a first-order CRFs. Each instance is represented by encoding all the following families of features, all time-shifted by 2,-1,0,1,2: (a) the word itself, (b) the PoS tag of the token, (c) orthographic predicates, such as capitalization, upper-case, numeric, single character, and punctuation, (d) gazetteers of locations, people names and organizations, (e) character-n-gram predicates for 2 6 n 6 3. As an (unsupervised) training set for the finegrained categories, we exploited all occurrences in context of their corresponding"
D07-1026,W06-2608,1,0.846888,"d subsequences kernel (Shawe-Taylor and Cristianini, 2004). In the spirit of kernel methods, this kernel is able to compare sequences directly in the input space, avoiding any explicit feature mapping. To perform this operation, it counts how many times a (non-contiguous) subsequence of symbols u of length n occurs in the input string s, and penalizes non-contiguous occurrences according to the number of the contained gaps. To define our syntagmatic kernel, we adapted the generic definition of the sequence kernels to the problem of recognizing collocations in local word contexts. We refer to (Giuliano et al., 2006) for a detailed description of the syntagmatic kernel. 4 Lexical Entailment for Ontology Population following formula: R(e, c) = In this section, we apply the IBLE technique, described in Section 3, to recognize lexical entailment for ontology population. To this aim, we cast ontology population as a lexical entailment task, where the fine-grained categories are the candidate entailed words, and the named entities to be subcategorized are the entailing words. Below, we present the main steps of our algorithm in details. Step 1 By using a state-of-the-art supervised NER system, we recognize the"
D07-1026,W06-1621,0,0.100485,"word w with the word e in a coherent text Hw = H l wH r generates a sentence He = H l eH r such that Hw → He , where H l and H r denote the left and the right context of w, respectively. For example, given the word ‘weapon’ a system may substitute it with the synonym ‘arm’, in order to identify relevant texts that denote the sought concept using the latter term. A particular case of lexical entailment is recognizing synonymy, where both Hw → He and He → Hw hold. In the literature, slight variations of this problem are also referred to as sense matching (Dagan et al., 2006), lexical reference (Glickman et al., 2006a) and lexical substitution (Glickman et al., 2006b). They have been applied to a wide variety of tasks, such as semantic matching, subtitle generation and Word Sense Disambiguation (WSD). Modeling lexical entailment is also a prerequisite to approach the SemEval-2007 lexical substitution task1 , consisting of finding alternative words that can occur in given context. In this paper, we propose to apply an approach for lexical entailment to the ontology population task. The basic idea is that if a word entails another one in a given context then the former is an instance or a subclass of the la"
D07-1026,W06-2907,0,0.0141914,"word w with the word e in a coherent text Hw = H l wH r generates a sentence He = H l eH r such that Hw → He , where H l and H r denote the left and the right context of w, respectively. For example, given the word ‘weapon’ a system may substitute it with the synonym ‘arm’, in order to identify relevant texts that denote the sought concept using the latter term. A particular case of lexical entailment is recognizing synonymy, where both Hw → He and He → Hw hold. In the literature, slight variations of this problem are also referred to as sense matching (Dagan et al., 2006), lexical reference (Glickman et al., 2006a) and lexical substitution (Glickman et al., 2006b). They have been applied to a wide variety of tasks, such as semantic matching, subtitle generation and Word Sense Disambiguation (WSD). Modeling lexical entailment is also a prerequisite to approach the SemEval-2007 lexical substitution task1 , consisting of finding alternative words that can occur in given context. In this paper, we propose to apply an approach for lexical entailment to the ontology population task. The basic idea is that if a word entails another one in a given context then the former is an instance or a subclass of the la"
D07-1026,P05-1050,1,0.91587,"bally” by context vectors, are doomed to fail. We will provide empirical evidence of this in the evaluation section. Choosing an appropriate similarity function for the contexts of the words to be substituted is a primary issue. In this work, we exploited similarity functions already defined in the WSD literature, relying on the analogy between the lexical entail251 ment and the WSD task. The state-of-the-art supervised WSD methodology, reporting the best results in most of the Senseval-3 lexical sample tasks in different languages, is based on a combination of syntagmatic and domain kernels (Gliozzo et al., 2005) in a SVM classification framework. Therefore, we adopted exactly the same strategy for our purposes. A great advantage of this methodology is that it is totally corpus based, as it does not require neither the availability of lexical databases, nor the use of complex preprocessing steps such as parsing or anaphora resolution, allowing us to apply it on different languages and domains once large corpora are available for training. Therefore, we exploited exactly the same strategy to implement the IBLE classifier required for our purposes, defining a kernel composed by n simple kernels, each re"
D07-1026,P98-2127,0,0.829398,"cales up. Therefore, most of the present research in ontology population is focusing on either unsupervised approaches (Cimiano and V¨olker, 2005) or weakly supervised approaches (Tanev and Magnini, 2006). Unsupervised approaches are mostly based on term similarity metrics. Cimiano and V¨olker (2005) assign a particular entity to the fine-grained class such that the contextual similarity is maximal among the set of fine-grained subclasses of a coarse-grained category. Contextual similarity has been measured by adopting lexico-syntactic features provided by a dependency parser, as proposed in (Lin, 1998). 250 3 Instance Based Lexical Entailment Dagan et al. (2006) adapted the classical supervised WSD setting to approach the sense matching problem (i.e., the binary lexical entailment problem of deciding whether a word, such as position, entails a different word, such as job, in a given context) by defining a one-class learning algorithm based on support vector machines (SVM). They train a oneclass model for each entailed word (e.g., all the occurrences of the word job in the corpus) and, then, apply it to classify all the occurrences of the entailing words (e.g., the word position), providing"
D07-1026,W02-2024,0,0.0214841,"n be perceived as an artificial experimental setting, it is the best available benchmark we can use to compare our system to existing approaches in the literature, as we are not aware of other available resources. To perform NER we used CRFs (Lafferty et al., 2001). We trained a first-order CRF on the MUC data set to annotate locations and people. In our experiments, we used the implementation provided in MALLET (McCallum, 2002). We used a standard feature set inspired by the literature on text chunking and NER (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) to train a first-order CRFs. Each instance is represented by encoding all the following families of features, all time-shifted by 2,-1,0,1,2: (a) the word itself, (b) the PoS tag of the token, (c) orthographic predicates, such as capitalization, upper-case, numeric, single character, and punctuation, (d) gazetteers of locations, people names and organizations, (e) character-n-gram predicates for 2 6 n 6 3. As an (unsupervised) training set for the finegrained categories, we exploited all occurrences in context of their corresponding terms we found in the CLEF corpus (e.g., for the category ac"
D07-1026,P94-1013,0,0.00869058,"dition to the standard VSM, a domain kernel, exploiting external information acquired from unlabeled data, has been also used to reduce the amount of (labeled) training data. Here, given that our approach is fully unsupervised, i.e., we can obtain as many examples as we need, we do not use the domain kernel. 252 The Syntagmatic Kernel Syntagmatic aspects are probably the most important evidence for recognizing lexical entailment. In general, the strategy adopted to model syntagmatic relations in WSD is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994). The main drawback of this approach is that non contiguous or shifted collocations cannot be identified, decreasing the generalization power of the learning algorithm. For example, suppose that the word job has to be disambiguated into the sentence “. . . permanent academic job in. . . ”, and that the occurrence “We offer permanent positions. . . ” is provided for training. A traditional feature mapping would extract the context words w−1 :academic, w−2 :permanent to represent the former, and w−1 :permanent, w−2 :offer to index the latter. Evidently such features will not match, leading the a"
D07-1026,P05-1052,0,0.0134698,"uring similarity between the objects xi and xj from different perspectives4 . One means to satisfy both the WSD and the lexical entailment requirements is to consider two different aspects of similarity: domain aspects, mainly related to the topic (i.e., the global context) of the texts in which the word occurs, and syntagmatic aspects, concerning the lexico-syntactic pattern in the local context. Domain aspects are captured by the domain kernel, described in Section 3.1, while syntagmatic aspects are taken into account by the syntagmatic kernel, presented in Section 3.2. 3 Some recent works (Zhao and Grishman, 2005; Gliozzo et al., 2005) empirically demostrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones. In addition, this formulation allows evaluating the individual contribution of each information source. 4 An exhaustive discussion about kernel methods for NLP can be found in (Shawe-Taylor and Cristianini, 2004). Entailed position task job ... looking for permanent academic job in ... The job of repairing 1 2 3 4 5 6 Training ... from entry-level through permanent positions. My academic position ... ... put"
D07-1026,W03-0419,0,\N,Missing
D07-1026,C98-2122,0,\N,Missing
D14-1066,biemann-2012-turk,0,0.0891066,"revious works (Szarvas et al., 2013), but use only substitutes from a DT. To train a single classifier, features that distinguishing the meaning of words in different context need to be considered. Such features could be e.g. n-grams, features from distributional semantics or features which are extracted • Ranking the substitution candidates according to their context Such a substitution system can help for semantic text similarity (B¨ar et al., 2012), textual entailment (Dagan et al., 2013) or plagiarism detection (Chong and Specia, 2011). Datasets provided by McCarthy and Navigli (2009) and Biemann (2012) offer manually annotated substitutes for a given set of target words within a context (sentence). Contrary to these two datasets in Kremer et al. (2014) a dataset is offered where all words have are annotated with substitutes. All the datasets are suited for the open domain. But a system performing lexical substitution is not only of interest for the open domain, but also for the medical domain. Such a system could then be applied to medical word sense disambiguation, entailment or question answering tasks. Here we introduce a new dataset and adapt the lexical substitution system, provided by"
D14-1066,R11-1102,0,0.0164729,"or the generation of substitute candidates we do not use WordNet, as done in previous works (Szarvas et al., 2013), but use only substitutes from a DT. To train a single classifier, features that distinguishing the meaning of words in different context need to be considered. Such features could be e.g. n-grams, features from distributional semantics or features which are extracted • Ranking the substitution candidates according to their context Such a substitution system can help for semantic text similarity (B¨ar et al., 2012), textual entailment (Dagan et al., 2013) or plagiarism detection (Chong and Specia, 2011). Datasets provided by McCarthy and Navigli (2009) and Biemann (2012) offer manually annotated substitutes for a given set of target words within a context (sentence). Contrary to these two datasets in Kremer et al. (2014) a dataset is offered where all words have are annotated with substitutes. All the datasets are suited for the open domain. But a system performing lexical substitution is not only of interest for the open domain, but also for the medical domain. Such a system could then be applied to medical word sense disambiguation, entailment or question answering tasks. Here we introduce"
D14-1066,S07-1029,1,0.808024,"omain, which trains a single classifier for all instances using delexicalized features. We show significant improvements over a strong baseline coming from a distributional thesaurus (DT). Whereas in the open domain system, features derived from WordNet show only slight improvements, we show that its counterpart for the medical domain (UMLS) shows a significant additional benefit when used for feature generation. 1 2 Related Work For the general domain, the lexical substitution task was initiated by a Semeval-2007 Task (McCarthy and Navigli, 2009). This task was won by an unsupervised method (Giuliano et al., 2007), which uses WordNet for the substitution candidate generation and then relies on the Google Web1T n-grams (Brants and Franz, 2006)1 to rank the substitutes. The currently best system, to our knowledge, is proposed by Szarvas et al. (2013). This is a supervised approach, where a single classifier is trained using delexicalized features for all substitutes and can thus be applied even to previously unseen substitutes. Although there have been many approaches for solving the task for the general domain, only slight effort has been done in adapting it to different domains. Introduction The task o"
D14-1066,E14-1057,0,0.0959426,"Missing"
D14-1066,N13-1133,0,0.358861,"from WordNet show only slight improvements, we show that its counterpart for the medical domain (UMLS) shows a significant additional benefit when used for feature generation. 1 2 Related Work For the general domain, the lexical substitution task was initiated by a Semeval-2007 Task (McCarthy and Navigli, 2009). This task was won by an unsupervised method (Giuliano et al., 2007), which uses WordNet for the substitution candidate generation and then relies on the Google Web1T n-grams (Brants and Franz, 2006)1 to rank the substitutes. The currently best system, to our knowledge, is proposed by Szarvas et al. (2013). This is a supervised approach, where a single classifier is trained using delexicalized features for all substitutes and can thus be applied even to previously unseen substitutes. Although there have been many approaches for solving the task for the general domain, only slight effort has been done in adapting it to different domains. Introduction The task of lexical substitution (McCarthy and Navigli, 2009) deals with the substitution of a target term within a sentence with words having the same meaning. Thus, the task divides into two subtasks: 3 • Identification of substitution candidates,"
D14-1066,S12-1059,0,\N,Missing
D14-1161,W13-3512,0,0.0398232,"chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat each entry of a thesaurus as a document giving synonyms positive term counts, and antonyms negative term counts, and preform LSA on the signed TF-IDF matrix In this way, synonyms will have cosine similarities close to one and antonyms close to minus one. Chang et al. (2013)"
D14-1161,N13-1090,0,0.0127329,"(LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat each entry of a thesau"
D14-1161,D08-1103,0,0.123968,"e) It is able to bring the advantages from both word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section"
D14-1161,D13-1167,0,0.451124,"calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section 3 presents our BPTF model and the sampling method. Section 4 s"
D14-1161,S13-1035,0,0.0222398,"Missing"
D14-1161,I13-1056,0,0.0614149,"Missing"
D14-1161,C08-1114,0,0.0189629,"n and YAGO type) It is able to bring the advantages from both word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and anton"
D14-1161,D12-1111,0,0.425162,"h word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section 3 presents our BPTF model and the samplin"
D14-1161,J13-3004,0,\N,Missing
E06-2016,W99-0707,0,0.030664,"he evaluation we did for the query Karl Marx. Karl TRIM TRUM FRIM TRIM ???E TRIM TRIM ?RIM TRIM TRIM TRIM 1 Marx: economic_organisation determines superstructure capitalism needs capitalists proletariat overthrow bourgeoisie marx understood capitalism marx later marxists labour_power be production societies are class_societies private_property equals exploitation primitive_societies were classless social_relationships form economic_basis max_weber criticised marxist_view For the experiments reported in this paper we used a memory-based shallow parser developed at CNTS Antwerp and ILK Tilburg (Daelemans et al., 1999) together with a set of scripts to extract SVO patterns (Reinberger et al., 2004) kindly put at our disposal by the authors. 149 TRIM ?R?E ?R?E ?RUE TRIM contradictions legitimizes class_structure societies is political_level class_society where false_consciousness social_system containing such_contradictions human_societies organizing production Several aspects are addressed: truthfulness (i.e. True vs. False in the first column), relevance for the query (i.e. Relevant vs. Not-relevant in the second column), information content (i.e. Informative vs. Uninformative, third column) and meaningful"
E06-2016,P05-1050,1,0.829848,"n be disambiguated by simply identifying the domain of the text. As a consequence, concepts belonging to different domains are basically unrelated. This observation is crucial from a methodological point of view, allowing us to perform a large scale structural analysis of the whole lexicon of a language, otherwise computationally infeasible. In fact, restricting the attention to a particular domain is a way to reduce the complexity of the overall relation extraction task, that is evidently quadratic in the number of terms. Domain information can be expressed by exploiting Domain Models (DMs) (Gliozzo et al., 2005). A DM is represented by a k × k 0 rectangular matrix D, containing the domain relevance for each term with respect to each domain, where k is the cardinality of the vocabulary, and k 0 is the size of the Domain Set. DMs can be acquired from texts in a totally unsupervised way by exploiting a lexical coherence assumption (Gliozzo, 2005). To this aim, term clustering algorithms can be adopted: each cluster represents a Semantic Domain. The degree of association among terms and clusters, estimated by the learning algorithm, provides a domain relevance function. For our experiments we adopted a c"
E12-1019,P08-1090,0,0.0466414,"icant gains by exploiting context. 3. Empirical evidence illustrates that our framework for temporal linking is very effective for the task, achieving an F1-score of 0.76 on events and 0.72 on fluents/relations, as well as 0.65 for TempEval2, approaching state-of-the-art. 2 Related Work Most of the previous work on relation extraction focuses on entity-entity relations, such as in the ACE (Doddington et al., 2004) tasks. Temporal relations are part of this, but to a lesser extent. The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al. (2006)). Recently, TempEval workshops have focused on the temporal related issues in NLP. Some of 186 the TempEval tasks overlap with ours in many ways. Our task is similar to task A and C of TempEval-1 (Verhagen et al., 2007) in the sense that we attempt to identify temporal relation between events and time expressions or document dates. However, we do not use a restricted set of events, but focus primarily on a single temporal relation tlink instead of named relations like BEFORE, AFTER or OVERLAP (although we show that we can incorp"
E12-1019,W10-0901,0,0.0217002,"best systems achieve F1-scores of 0.76 on events and 0.72 on fluents. 1 Before his death in October, Steve Jobs led Apple for 15 years. Introduction It is a long-standing goal of NLP to process natural language content in such a way that machines can effectively reason over the entities, relations, and events discussed within that content. The applications of such technology are numerous, including intelligence gathering, business analytics, healthcare, education, etc. Indeed, the promise of machine reading is actively driving research in this area (Etzioni et al., 2007; Barker et al., 2007; Clark and Harrison, 2010; Strassel et al., 2010). Temporal information is a crucial aspect of this task. For a machine to successfully understand natural language text, it must be able to associate time points and temporal durations with relations and events it discovers in text. ∗ The first author conducted this research during an internship at IBM Research. For a machine reading system processing this sentence, we would expect it to link the fluent CEO of (Steve Jobs, Apple) to time duration “15 years”. Similarly we expect it to link the event “death” to the time expression “October”. We do not take a strong “ontol"
E12-1019,doddington-etal-2004-automatic,0,0.077992,"paper can be summarized as follows: 1. We define a common methodology to link events and fluents to timestamps. 2. We use tree kernels in combination with classical feature-based approaches to obtain significant gains by exploiting context. 3. Empirical evidence illustrates that our framework for temporal linking is very effective for the task, achieving an F1-score of 0.76 on events and 0.72 on fluents/relations, as well as 0.65 for TempEval2, approaching state-of-the-art. 2 Related Work Most of the previous work on relation extraction focuses on entity-entity relations, such as in the ACE (Doddington et al., 2004) tasks. Temporal relations are part of this, but to a lesser extent. The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al. (2006)). Recently, TempEval workshops have focused on the temporal related issues in NLP. Some of 186 the TempEval tasks overlap with ours in many ways. Our task is similar to task A and C of TempEval-1 (Verhagen et al., 2007) in the sense that we attempt to identify temporal relation between events and time expressions or docum"
E12-1019,W01-1313,0,0.161495,"Missing"
E12-1019,J09-4007,1,0.83103,"atures include the test for certain words indicating the event is a noun, a verb, and if so which tense it has and whether it is a reporting verb. 4.2 Tree Kernel Engineering We expect that there exist certain patterns between the entities of a temporal link, which manifest on several levels: some on the lexical level, others expressed by certain sequences of POS tags, NE labels, or other representations. Kernels provide a principled way of expanding the number of dimensions in which we search for a decision boundary, and allow us to easily model local sequences and patterns in a natural way (Giuliano et al., 2009). While it is possible to define a space in which we find a decision boundary that separates positive and negative instances with manually engineered features, these features can hardly capture the notion of context as well as those explored by a tree kernel. Tree Kernels are a family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common. This generates a highdimensional feature space that can be handled efficiently using dynamic programming techniques (Shawe-Taylor and Christianini, 2004). For our purposes we use"
E12-1019,P04-1043,0,0.0358114,"notion of context as well as those explored by a tree kernel. Tree Kernels are a family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common. This generates a highdimensional feature space that can be handled efficiently using dynamic programming techniques (Shawe-Taylor and Christianini, 2004). For our purposes we used an implementation of the Subtree and Subset Tree (SST) (Moschitti, 2006). The advantages of using tree kernels are two-fold: thanks to an existing implementation 188 (SVMlight with tree kernels, Moschitti (2004)), it is faster and easier than traditional feature engineering. The tree structure also allows us to use different levels of representations (POS, lemma, etc.) and combine their contributions, while at the same time taking into account the ordering of labels. We use POS, lemma, semantic type, and a representation that replaces each word with a concatenation of its features (capitalization, countable, abstract/concrete noun, etc.). We developed a shallow tree representation that captures the context of the target terms, without encoding too much structure (which may prevent generalization). In"
E12-1019,E06-1015,0,0.0195403,"a decision boundary that separates positive and negative instances with manually engineered features, these features can hardly capture the notion of context as well as those explored by a tree kernel. Tree Kernels are a family of kernel functions developed to compute the similarity between tree structures by counting the number of subtrees they have in common. This generates a highdimensional feature space that can be handled efficiently using dynamic programming techniques (Shawe-Taylor and Christianini, 2004). For our purposes we used an implementation of the Subtree and Subset Tree (SST) (Moschitti, 2006). The advantages of using tree kernels are two-fold: thanks to an existing implementation 188 (SVMlight with tree kernels, Moschitti (2004)), it is faster and easier than traditional feature engineering. The tree structure also allows us to use different levels of representations (POS, lemma, etc.) and combine their contributions, while at the same time taking into account the ordering of labels. We use POS, lemma, semantic type, and a representation that replaces each word with a concatenation of its features (capitalization, countable, abstract/concrete noun, etc.). We developed a shallow tr"
E12-1019,P06-1050,0,0.109155,"r framework for temporal linking is very effective for the task, achieving an F1-score of 0.76 on events and 0.72 on fluents/relations, as well as 0.65 for TempEval2, approaching state-of-the-art. 2 Related Work Most of the previous work on relation extraction focuses on entity-entity relations, such as in the ACE (Doddington et al., 2004) tasks. Temporal relations are part of this, but to a lesser extent. The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al. (2006)). Recently, TempEval workshops have focused on the temporal related issues in NLP. Some of 186 the TempEval tasks overlap with ours in many ways. Our task is similar to task A and C of TempEval-1 (Verhagen et al., 2007) in the sense that we attempt to identify temporal relation between events and time expressions or document dates. However, we do not use a restricted set of events, but focus primarily on a single temporal relation tlink instead of named relations like BEFORE, AFTER or OVERLAP (although we show that we can incorporate these as well). Part of our task is similar to task C of Te"
E12-1019,strassel-etal-2010-darpa,0,0.296009,"cores of 0.76 on events and 0.72 on fluents. 1 Before his death in October, Steve Jobs led Apple for 15 years. Introduction It is a long-standing goal of NLP to process natural language content in such a way that machines can effectively reason over the entities, relations, and events discussed within that content. The applications of such technology are numerous, including intelligence gathering, business analytics, healthcare, education, etc. Indeed, the promise of machine reading is actively driving research in this area (Etzioni et al., 2007; Barker et al., 2007; Clark and Harrison, 2010; Strassel et al., 2010). Temporal information is a crucial aspect of this task. For a machine to successfully understand natural language text, it must be able to associate time points and temporal durations with relations and events it discovers in text. ∗ The first author conducted this research during an internship at IBM Research. For a machine reading system processing this sentence, we would expect it to link the fluent CEO of (Steve Jobs, Apple) to time duration “15 years”. Similarly we expect it to link the event “death” to the time expression “October”. We do not take a strong “ontological” position on what"
E12-1019,S07-1014,0,0.281389,"Missing"
E12-1019,S10-1010,0,\N,Missing
H05-1017,W99-0613,0,0.250205,"ct the required amounts of hand labeled data. Unlabeled text collections, on the other hand, are in general easily available. An alternative approach is to provide the necessary supervision by means of sets of “seeds” of intuitively relevant features. Adopting terminology The IL approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space. Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in (Yarowsky, 1995; Collins and Singer, 1999; Liu et al., 2004). This way the user does not need to specify exact classification rules (and feature weights), but rather perform a somewhat simpler task of specifying few typical seed features for the category. Given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (extensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an Expectation-Maximization schem"
H05-1017,C00-1066,0,0.0228791,"ng for Text Categorization The TC task is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be determined as a b"
H05-1017,P04-1033,0,0.365294,"sk is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be determined as a binary criterion, considering each see"
H05-1017,W99-0908,0,0.708007,"re research. 2 Bootstrapping for Text Categorization The TC task is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be"
H05-1017,P95-1026,0,0.231154,"fficult to collect the required amounts of hand labeled data. Unlabeled text collections, on the other hand, are in general easily available. An alternative approach is to provide the necessary supervision by means of sets of “seeds” of intuitively relevant features. Adopting terminology The IL approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space. Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in (Yarowsky, 1995; Collins and Singer, 1999; Liu et al., 2004). This way the user does not need to specify exact classification rules (and feature weights), but rather perform a somewhat simpler task of specifying few typical seed features for the category. Given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (extensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an Expe"
H05-1017,W05-0608,1,\N,Missing
J09-4007,W04-0827,0,0.0446019,"Missing"
J09-4007,W06-2608,1,0.867902,"Missing"
J09-4007,E06-1051,1,0.830508,"Missing"
J09-4007,P05-1050,1,0.785313,"Missing"
J09-4007,magnini-cavaglia-2000-integrating,0,0.0627202,"Missing"
J09-4007,P04-1043,0,0.0343892,"typically used as features in WSD. 3.3 Composite Kernel Having deﬁned all the individual kernels representing syntagmatic and domain aspects of sense distinction, we can deﬁne the composite kernel to combine and extend the individual kernels. The closure properties of the kernel functions allows us to deﬁne the composite kernel as n  KC (xi , xj ) = l =1 Kl (xi , xj )  Kl (xj , xj )Kl (xi , xi ) (19) where Kl is a valid individual kernel. The individual kernels are normalized—this plays an important role in allowing us to integrate information from heterogeneous feature spaces. Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effectiveness of combining kernels in this way: The composite kernel consistently improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. In order to show the effectiveness of the proposed domain model in supervised  . They are completely speciﬁed by learning, we deﬁned two WSD kernels, Kwsd and Kwsd the n individual kernels that compose them in Equation (19). Kwsd i"
J09-4007,S07-1016,0,0.0252856,"Missing"
J09-4007,W04-0811,0,0.0503622,"Missing"
J09-4007,W04-0856,1,0.901318,"Missing"
J09-4007,P94-1013,0,0.0994997,"t can be applied to the strictly supervised settings, in which external knowledge is not available. To summarize, the domain kernel allows us to plug external knowledge into the supervised learning process; it will be compared and combined with the standard bagof-words approach in Section 4. In the following section, we shall see that domain models are also useful for deﬁning soft-matching collocation kernels. 3.2 Syntagmatic Kernels Collocations (such as bigrams and trigrams) extracted from the local context of the word to be disambiguated are typically used to capture syntagmatic relations (Yarowsky 1994). However, traditional approaches to WSD fail to represent non-contiguous or shifted collocations, and fail to consider lexical variability. For example, suppose we have to disambiguate the verb to score in the sentence Ronaldo scored the ﬁrst goal, given the labeled example The football player scored two goals in the second half as training. A traditional approach has no clues to return the right answer because the two sentences have no features in common. The use of kernels on strings allows us to overcome the aforementioned problems by representing (non-contiguous) collocations and exploiti"
J09-4007,W04-0864,0,0.0268411,"Missing"
J09-4007,P05-1052,0,0.0237351,"nto account second order relations among terms. For example, the similarity of the two sentences He is affected by AIDS and HIV is a virus is very high, because the terms AIDS, HIV, and virus are strongly associated with the medicine domain. A DM can be estimated from manually constructed lexical resources, such as WordNet Domains (Magnini and Cavagli`a 2000), or by performing a term-clustering process on a (large) corpus. However, the second approach is more attractive because it allows us to automatically acquire DMs for different languages and domains. In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposition (SVD) to acquire DMs from a corpus represented by its term-by-document matrix T, in a unsupervised way.4 SVD decomposes the term-by-document matrix T into three matrixes T  VΣk UT , where V and U are orthogonal matrices (i.e., VT V = I and UT U = I) whose columns are the eigenvectors of TTT and TT T, respectively, and Σk is the diagonal k × k matrix containing the highest k  k eigenvalues of T, and all the remaining elements set to 0. The parameter k is the dimensionality of the domain VSM and can be ﬁxed in advance. Under this setting, we deﬁne the"
N07-1017,C92-2082,0,0.184991,"elations are ubiquitous, and affect ontology reliability, if used to populate it, as the relation drives the wrong type of ontological knowledge. Erroneous or false relations. These are particularly harmful, since they directly affect algorithm precision. A pattern-based relation extraction algorithm is particularly likely to extract erroneous relations if it uses generic patterns, which are defined in (Pantel and Pennacchiotti, 2006) as broad coverage, noisy patterns with high recall and low precision (e.g. “X of Y” for part-of relation). Harvesting algorithms either ignore generic patterns (Hearst, 1992) (affecting system recall) or use manually supervised filtering approaches (Girju et al., 2006) or use completely unsupervised Web-filtering methods (Pantel and Pennacchiotti, 2006). Yet, these methods still do not sufficiently mitigate the problem of erroneous relations. Background knowledge. Another aspect that makes relation harvesting difficult is related to the 131 Proceedings of NAACL HLT 2007, pages 131–138, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics nature of semantic relations: relations among entities are mostly paradigmatic (de Saussure, 1922), and a"
N07-1017,magnini-cavaglia-2000-integrating,0,0.10389,"Missing"
N07-1017,P06-1015,1,0.878152,"ng that very high precision can be reached. 1 Introduction Relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts (Buitelaar et al., 2005) and Question Answering (Pasca and Harabagiu, 2001). The most common approach for acquiring concepts, instances and relations is to harvest semantic knowledge from texts. These techniques have been largely explored and today they achieve reasonable accuracy. Harvested lexical resources, such as concept lists (Pantel and Lin, 2002), facts (Etzioni et al., 2002) and semantic relations (Pantel and Pennacchiotti, 2006) could be then successfully used in different frameworks and applications. The state of the art technology for relation extraction primarily relies on pattern-based approaches Irrelevant relations. These are valid relations that are not of interest in the domain at hand. For example, in a political domain, “Condoleezza Rice is a football fan” is not as relevant as “Condoleezza Rice is the Secretary of State of the United States”. Irrelevant relations are ubiquitous, and affect ontology reliability, if used to populate it, as the relation drives the wrong type of ontological knowledge. Erroneou"
N07-1017,P02-1006,0,0.00953961,"OStagged corpus, Espresso takes as input few seed instances (e.g. nitrogen is-a element) or seed surface patterns (e.g. X/NN such/JJ as/IN Y/NN). It then incrementally learns new patterns and instances by iterating on the following three phases, until a specific stop condition is met (i.e., new patterns are below a pre-defined threshold of reliability). Pattern Induction. Given an input set of seed instances I, Espresso infers new patterns connecting as many instances as possible in the given corpus. To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). For each instance in input, the sentences containing it are first retrieved and then 134 generalized, by replacing term expressions with a terminological label using regular expressions on the POS-tags. This generalization allows to ease the problem of data sparseness in small corpora. Unfortunately, as patterns become more generic, they are more prone to low precision. Pattern Ranking and Selection. Espresso ranks all extracted patterns using a reliability measure rπ and discards all but the top-k P patterns, where k is set to the number of patterns from the previous iteration plus one. rπ"
N07-1017,P06-1101,0,0.0450548,"Missing"
N07-1017,E06-2016,1,\N,Missing
N07-1017,N03-1011,0,\N,Missing
N19-1327,D17-1070,0,0.0277877,"atasets used in our experiments. Then, we evaluate it on the long-tailed relations of each dataset. During the test phase, only a single example for each unseen relation is provided. This example is not used to update the parameters of the model as in a classification task, but rather to produce the vector representation of the relation itself. Entity pairs having mention sets close to this representation are more likely to be analogous. The experiments show promising results of our approach on this task, compared with the recent deep models commonly used for encoding textual representations (Conneau et al., 2017). However, when the number of the unseen relation types increases, the performance of our model become far from the results obtained in the one-shot image classification (Koch et al., 2015), opening an interesting challenge for future work. Finally, our model shows a transfer capability in other tasks through the use of its pre-trained vectors. Indeed, a branch of the hierarchical siamese network can be used to generate entity-entity representations given sets of mentions as input, that we call analogy embeddings. In our experiments, we integrate those representations into an existing end-to-e"
N19-1327,L18-1544,0,0.344433,"stions: (RQ1) How to collect and organize a dataset for training? (RQ2) What kinds of models are effective for this task? (RQ3) How should the model be evaluated? Knowledge bases, such as Wikidata or DBpedia, consist of large relational data sources organized in the form of triples, predicate(S UBJECT, O BJECT). We exploit this information to build a reliable set of analogous facts used as ground truth. Then, we adopt distant supervision to retrieve relation mentions in web-scale textual corpora by matching the subject-object entities which co-occur in the same sentences (Riedel et al., 2010; ElSahar et al., 2018; Glass and Gliozzo, 2018a). Through this technique we can train our model on millions of analogy examples without human supervision. Since our goal is to train a model able to compute the relational similarity given two sets of textual mentions, we use siamese networks to learn discriminative features between those two instances (Hadsell et al., 2006). This kind of neural network has been used in both computer vision (Koch et al., 2015) and natural language processing (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) in order to map two similar instances close in a feature space. However"
N19-1327,N16-2002,0,0.0202526,"of the word vector space models, is useful for synonym detection, word sense disambiguation and so on. Instead, relational similarity is suitable for understanding analogies between two pairs of words. Our neural-based analogy approach is inspired by this finding. Recently, word analogies, namely the proportional analogy between two word pairs such as a : b = c : d, have been used by (Mikolov et al., 2013; Pennington et al., 2014) to show the capability of word embeddings to discover linguistic regularities in word contexts using vector offsets (e.g. king − man + woman = queen). The works in (Gladkova et al., 2016; Vylomova et al., 2016) explore the use of word vectors to model the semantic relations. The proportional analogy is also adopted by (Liu et al., 2017) as analogical inference in order to learn multi-relational embeddings which are evaluated on knowledge base completion benchmarks. However, in order to apply word embedding models to proportional analogy, the model must have seen the words during training. This approach is unsuitable for computing the analogy between out-of-vocabulary words. Our approach overcomes this limitation, since it works by considering the contexts where the entities o"
N19-1327,P18-1147,1,0.710346,"ollect and organize a dataset for training? (RQ2) What kinds of models are effective for this task? (RQ3) How should the model be evaluated? Knowledge bases, such as Wikidata or DBpedia, consist of large relational data sources organized in the form of triples, predicate(S UBJECT, O BJECT). We exploit this information to build a reliable set of analogous facts used as ground truth. Then, we adopt distant supervision to retrieve relation mentions in web-scale textual corpora by matching the subject-object entities which co-occur in the same sentences (Riedel et al., 2010; ElSahar et al., 2018; Glass and Gliozzo, 2018a). Through this technique we can train our model on millions of analogy examples without human supervision. Since our goal is to train a model able to compute the relational similarity given two sets of textual mentions, we use siamese networks to learn discriminative features between those two instances (Hadsell et al., 2006). This kind of neural network has been used in both computer vision (Koch et al., 2015) and natural language processing (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) in order to map two similar instances close in a feature space. However, in our setting each ins"
N19-1327,K17-1034,0,0.0641942,"ew training examples. This model has two main limitations. Firstly, it works only by pairing two single mentions and is not able to handle a whole set of mentions referring to a relation instance. Our hierarchical siamese network overcomes this issue by using an attention mechanism at both word and mention level. Moreover, their one-shot evaluation mainly focuses on extracting the same relation types seen during training. Instead, the goal of our one-shot task is to evaluate the transfer capability in extracting unseen relation types across domains using a single pre-trained model. Recently, (Levy et al., 2017) propose to reduce RE slot-filling to a question answering problem. The main idea is to build a set of question-answer pairs for the relations in knowledge bases and train a reading comprehension model using this dataset. This approach shows promising zero-shot capability in extracting unseen relation types. However, the schema querification phase requires a crowdsourcing effort. Our method uses distant supervision, so it does not need any kind of manual annotations. Word Analogy The analogy problem, from a computational linguistic perspective, was originally addressed by (Turney, 2006) who in"
N19-1327,P16-1200,0,0.0783589,", L ED Z EP - PELIN ), where memberOf is a relation label expressed by the linguistic context “is the singer of the band”. Since a given relation can be expressed using different textual patterns surrounding entities, the state-of-the-art RE models which follow this approach need a considerable amount of examples for each relation to reach satisfactory performance. Distant supervision (Mintz et al., 2009) instead uses training examples from a knowledge base, guaranteeing a large amount of (popular) relation examples without human intervention, which can be used effectively by neural networks (Lin et al., 2016; Glass et al., 2018). However, even with this technique, approaching RE as a classification task presents several limitations: (1) distant supervision models are not accurate in extracting relations with a long-tailed distribution, because they typically have a small set of instances in knowledge bases; (2) in most domains, relation types are very specific and only a few examples of each relation are available; (3) these models cannot be applied to recognize new relation types not observed during training. In this paper, we address RE from a different perspective by reducing it to an analogy"
N19-1327,D12-1048,0,0.0201936,"y used for distantly supervised relation extraction. 2 Related Work Relation Extraction Several approaches have been proposed in the literature to address the problem of extracting relations from text with minimal supervision. The bootstrapping method (Agichtein and Grais labeled, but each individual mention in the set is unlabeled. 3236 vano, 2000) collects the textual patterns between a few example pairs of entities iteratively, and uses them to retrieve other pairs of entities from a corpus. This method is limited by the semantic drift issue since wrong patterns might be collected. OpenIE (Mausam et al., 2012) is an unsupervised method for extracting triples from text, where the relations are linguistic phrases. The lack of a canonical form for the extracted relations makes this approach not suitable to populate knowledge bases with a fixed schema. Universal schema (Riedel et al., 2013) addresses RE by combining the OpenIE and knowledge base relations through a matrix factorization technique typically adopted in the collaborative filtering approach of recommendation systems. The column-less (Toutanova et al., 2015) and row-less (Verga and McCallum, 2016) extensions of this method can handle unseen"
N19-1327,P09-1113,0,0.507731,"ns in text among a predefined set of relation types. For instance, given the sentence “Robert Plant is the singer of the band Led Zeppelin”, an effective RE system might extract the triple memberOf(ROBERT P LANT, L ED Z EP - PELIN ), where memberOf is a relation label expressed by the linguistic context “is the singer of the band”. Since a given relation can be expressed using different textual patterns surrounding entities, the state-of-the-art RE models which follow this approach need a considerable amount of examples for each relation to reach satisfactory performance. Distant supervision (Mintz et al., 2009) instead uses training examples from a knowledge base, guaranteeing a large amount of (popular) relation examples without human intervention, which can be used effectively by neural networks (Lin et al., 2016; Glass et al., 2018). However, even with this technique, approaching RE as a classification task presents several limitations: (1) distant supervision models are not accurate in extracting relations with a long-tailed distribution, because they typically have a small set of instances in knowledge bases; (2) in most domains, relation types are very specific and only a few examples of each"
N19-1327,W16-1617,0,0.0258557,"bject entities which co-occur in the same sentences (Riedel et al., 2010; ElSahar et al., 2018; Glass and Gliozzo, 2018a). Through this technique we can train our model on millions of analogy examples without human supervision. Since our goal is to train a model able to compute the relational similarity given two sets of textual mentions, we use siamese networks to learn discriminative features between those two instances (Hadsell et al., 2006). This kind of neural network has been used in both computer vision (Koch et al., 2015) and natural language processing (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) in order to map two similar instances close in a feature space. However, in our setting each instance consists of a set of mentions, therefore it is inherently a multi-instance learning task1 . We propose a hierarchical siamese network 1 Due to the weak supervision, the whole set of mentions with an attention mechanism at both word level (Yang et al., 2016) and at the set level (Ilse et al., 2018) in order to select the textual mention which better describes the relation. To the best of our knowledge, this is the first application of a siamese network by pairing sets of instances, so it can b"
N19-1327,D14-1162,0,0.102967,"thors provide an interesting argument regarding the different types of similarities, attributional and relational, and their use in solving word analogies. Attributional similarity, typical of the word vector space models, is useful for synonym detection, word sense disambiguation and so on. Instead, relational similarity is suitable for understanding analogies between two pairs of words. Our neural-based analogy approach is inspired by this finding. Recently, word analogies, namely the proportional analogy between two word pairs such as a : b = c : d, have been used by (Mikolov et al., 2013; Pennington et al., 2014) to show the capability of word embeddings to discover linguistic regularities in word contexts using vector offsets (e.g. king − man + woman = queen). The works in (Gladkova et al., 2016; Vylomova et al., 2016) explore the use of word vectors to model the semantic relations. The proportional analogy is also adopted by (Liu et al., 2017) as analogical inference in order to learn multi-relational embeddings which are evaluated on knowledge base completion benchmarks. However, in order to apply word embedding models to proportional analogy, the model must have seen the words during training. Thi"
N19-1327,P11-1055,0,0.054935,"nd we used it as a feature extractor for entity pairs in both train-test standard splits of NYT-FB, as used in (Zeng et al., 2015; Lin et al., 2016). Figure 4 reports the results of our evaluation. The model which uses the features generated by HSN largely improve the performances of PCNN-KI, despite the HSN is trained on a different corpus and using a different KB. In the same chart, we also report a compared evaluation for other approaches proposed in the literature for the NYTFB benchmark: PCNN+ATT (Lin et al., 2016), CNN+ATT (Zeng et al., 2015), MIML-RE (Surdeanu et al., 2012), H OFFMANN (Hoffmann et al., 2011), M INTZ (Mintz et al., 2009). We run the evaluation also on CC-DBP, a larger dataset for distantly supervised RE, using the same train-test setting adopted in (Glass and Gliozzo, 2018b). As done for the NYT-FB dataset, we incorporate the analogy embeddings generated by the same HSN trained on the T-REX. The results confirm the improvements obtained by PCNN-KI model if it integrates our pre-trained embeddings (Table 3). PCNN-KI PCNN-KI+A NALOGY AUC 0.437 0.500 F1 0.468 0.504 Table 3: AUC and F1 results on CC-DBP. 6 Conclusion and Future Work In this paper, we proposed a novel approach to learn"
N19-1327,N13-1008,0,0.0247492,"but each individual mention in the set is unlabeled. 3236 vano, 2000) collects the textual patterns between a few example pairs of entities iteratively, and uses them to retrieve other pairs of entities from a corpus. This method is limited by the semantic drift issue since wrong patterns might be collected. OpenIE (Mausam et al., 2012) is an unsupervised method for extracting triples from text, where the relations are linguistic phrases. The lack of a canonical form for the extracted relations makes this approach not suitable to populate knowledge bases with a fixed schema. Universal schema (Riedel et al., 2013) addresses RE by combining the OpenIE and knowledge base relations through a matrix factorization technique typically adopted in the collaborative filtering approach of recommendation systems. The column-less (Toutanova et al., 2015) and row-less (Verga and McCallum, 2016) extensions of this method can handle unseen entity pairs and textual relations when combined (Verga et al., 2017). The one-shot RE has been addressed by (Yuan et al., 2017), who adopt a siamese network to extract fine-grained relations which typically have few training examples. This model has two main limitations. Firstly,"
N19-1327,D12-1042,0,0.0383075,"pre-trained the HSN on the T-REX and we used it as a feature extractor for entity pairs in both train-test standard splits of NYT-FB, as used in (Zeng et al., 2015; Lin et al., 2016). Figure 4 reports the results of our evaluation. The model which uses the features generated by HSN largely improve the performances of PCNN-KI, despite the HSN is trained on a different corpus and using a different KB. In the same chart, we also report a compared evaluation for other approaches proposed in the literature for the NYTFB benchmark: PCNN+ATT (Lin et al., 2016), CNN+ATT (Zeng et al., 2015), MIML-RE (Surdeanu et al., 2012), H OFFMANN (Hoffmann et al., 2011), M INTZ (Mintz et al., 2009). We run the evaluation also on CC-DBP, a larger dataset for distantly supervised RE, using the same train-test setting adopted in (Glass and Gliozzo, 2018b). As done for the NYT-FB dataset, we incorporate the analogy embeddings generated by the same HSN trained on the T-REX. The results confirm the improvements obtained by PCNN-KI model if it integrates our pre-trained embeddings (Table 3). PCNN-KI PCNN-KI+A NALOGY AUC 0.437 0.500 F1 0.468 0.504 Table 3: AUC and F1 results on CC-DBP. 6 Conclusion and Future Work In this paper, we"
N19-1327,D15-1174,0,0.0240206,"is limited by the semantic drift issue since wrong patterns might be collected. OpenIE (Mausam et al., 2012) is an unsupervised method for extracting triples from text, where the relations are linguistic phrases. The lack of a canonical form for the extracted relations makes this approach not suitable to populate knowledge bases with a fixed schema. Universal schema (Riedel et al., 2013) addresses RE by combining the OpenIE and knowledge base relations through a matrix factorization technique typically adopted in the collaborative filtering approach of recommendation systems. The column-less (Toutanova et al., 2015) and row-less (Verga and McCallum, 2016) extensions of this method can handle unseen entity pairs and textual relations when combined (Verga et al., 2017). The one-shot RE has been addressed by (Yuan et al., 2017), who adopt a siamese network to extract fine-grained relations which typically have few training examples. This model has two main limitations. Firstly, it works only by pairing two single mentions and is not able to handle a whole set of mentions referring to a relation instance. Our hierarchical siamese network overcomes this issue by using an attention mechanism at both word and m"
N19-1327,J06-3003,0,0.100093,"(Levy et al., 2017) propose to reduce RE slot-filling to a question answering problem. The main idea is to build a set of question-answer pairs for the relations in knowledge bases and train a reading comprehension model using this dataset. This approach shows promising zero-shot capability in extracting unseen relation types. However, the schema querification phase requires a crowdsourcing effort. Our method uses distant supervision, so it does not need any kind of manual annotations. Word Analogy The analogy problem, from a computational linguistic perspective, was originally addressed by (Turney, 2006) who investigate several similarity measures for solving word analogy questions in the Scholastic Aptitude Test dataset. The authors provide an interesting argument regarding the different types of similarities, attributional and relational, and their use in solving word analogies. Attributional similarity, typical of the word vector space models, is useful for synonym detection, word sense disambiguation and so on. Instead, relational similarity is suitable for understanding analogies between two pairs of words. Our neural-based analogy approach is inspired by this finding. Recently, word ana"
N19-1327,W16-1312,0,0.018235,"since wrong patterns might be collected. OpenIE (Mausam et al., 2012) is an unsupervised method for extracting triples from text, where the relations are linguistic phrases. The lack of a canonical form for the extracted relations makes this approach not suitable to populate knowledge bases with a fixed schema. Universal schema (Riedel et al., 2013) addresses RE by combining the OpenIE and knowledge base relations through a matrix factorization technique typically adopted in the collaborative filtering approach of recommendation systems. The column-less (Toutanova et al., 2015) and row-less (Verga and McCallum, 2016) extensions of this method can handle unseen entity pairs and textual relations when combined (Verga et al., 2017). The one-shot RE has been addressed by (Yuan et al., 2017), who adopt a siamese network to extract fine-grained relations which typically have few training examples. This model has two main limitations. Firstly, it works only by pairing two single mentions and is not able to handle a whole set of mentions referring to a relation instance. Our hierarchical siamese network overcomes this issue by using an attention mechanism at both word and mention level. Moreover, their one-shot e"
N19-1327,E17-1058,0,0.0166833,"from text, where the relations are linguistic phrases. The lack of a canonical form for the extracted relations makes this approach not suitable to populate knowledge bases with a fixed schema. Universal schema (Riedel et al., 2013) addresses RE by combining the OpenIE and knowledge base relations through a matrix factorization technique typically adopted in the collaborative filtering approach of recommendation systems. The column-less (Toutanova et al., 2015) and row-less (Verga and McCallum, 2016) extensions of this method can handle unseen entity pairs and textual relations when combined (Verga et al., 2017). The one-shot RE has been addressed by (Yuan et al., 2017), who adopt a siamese network to extract fine-grained relations which typically have few training examples. This model has two main limitations. Firstly, it works only by pairing two single mentions and is not able to handle a whole set of mentions referring to a relation instance. Our hierarchical siamese network overcomes this issue by using an attention mechanism at both word and mention level. Moreover, their one-shot evaluation mainly focuses on extracting the same relation types seen during training. Instead, the goal of our one-"
N19-1327,P16-1158,0,0.142751,"e models, is useful for synonym detection, word sense disambiguation and so on. Instead, relational similarity is suitable for understanding analogies between two pairs of words. Our neural-based analogy approach is inspired by this finding. Recently, word analogies, namely the proportional analogy between two word pairs such as a : b = c : d, have been used by (Mikolov et al., 2013; Pennington et al., 2014) to show the capability of word embeddings to discover linguistic regularities in word contexts using vector offsets (e.g. king − man + woman = queen). The works in (Gladkova et al., 2016; Vylomova et al., 2016) explore the use of word vectors to model the semantic relations. The proportional analogy is also adopted by (Liu et al., 2017) as analogical inference in order to learn multi-relational embeddings which are evaluated on knowledge base completion benchmarks. However, in order to apply word embedding models to proportional analogy, the model must have seen the words during training. This approach is unsuitable for computing the analogy between out-of-vocabulary words. Our approach overcomes this limitation, since it works by considering the contexts where the entities occur. 3 Learning Relatio"
N19-1327,N16-1174,0,0.110989,"to learn discriminative features between those two instances (Hadsell et al., 2006). This kind of neural network has been used in both computer vision (Koch et al., 2015) and natural language processing (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) in order to map two similar instances close in a feature space. However, in our setting each instance consists of a set of mentions, therefore it is inherently a multi-instance learning task1 . We propose a hierarchical siamese network 1 Due to the weak supervision, the whole set of mentions with an attention mechanism at both word level (Yang et al., 2016) and at the set level (Ilse et al., 2018) in order to select the textual mention which better describes the relation. To the best of our knowledge, this is the first application of a siamese network by pairing sets of instances, so it can be considered a novelty of this work. We evaluate the generalization capability of our model in recognizing unseen relation types through an one-shot relational classification task introduced in this paper. We train the parameters of the model on a subset of most frequent relations of one of three different distantly supervised datasets used in our experiment"
N19-1327,D15-1203,0,0.0212492,"which is the state-of-the-art for this benchmark. The final fully-connected layer uses the representation from HSN in combination with its own learned multi-instanced vector representation to predict a confidence score for each relation. During the training of this joint model, PCNNKI+A NALOGY, we freeze our analogy embeddings in order to avoid the loose the knowledge transfer capability. As for the one-shot setting described before, we use the same pre-trained the HSN on the T-REX and we used it as a feature extractor for entity pairs in both train-test standard splits of NYT-FB, as used in (Zeng et al., 2015; Lin et al., 2016). Figure 4 reports the results of our evaluation. The model which uses the features generated by HSN largely improve the performances of PCNN-KI, despite the HSN is trained on a different corpus and using a different KB. In the same chart, we also report a compared evaluation for other approaches proposed in the literature for the NYTFB benchmark: PCNN+ATT (Lin et al., 2016), CNN+ATT (Zeng et al., 2015), MIML-RE (Surdeanu et al., 2012), H OFFMANN (Hoffmann et al., 2011), M INTZ (Mintz et al., 2009). We run the evaluation also on CC-DBP, a larger dataset for distantly supervi"
P05-1050,W04-0827,0,0.0156161,"Missing"
P05-1050,magnini-cavaglia-2000-integrating,0,0.0207238,"Missing"
P05-1050,W04-0856,1,0.684687,"a, with a large set of unlabeled data. However, at our knowledge, none of the participants exploited this unlabeled material. Exploring this direction is the main focus of this paper. In particular we acquire a Domain Model (DM) for the lexicon (i.e. a lexical resource representing domain associations among terms), and we exploit this information inside our supervised WSD algorithm. DMs can be automatically induced from unlabeled corpora, allowing the portability of the methodology among languages. We identified kernel methods as a viable framework in which to implement the assumptions above (Strapparava et al., 2004). 404 Exploiting the properties of kernels, we have defined independently a set of domain and syntagmatic kernels and we combined them in order to define a complete kernel for WSD. The domain kernels estimate the (domain) similarity (Magnini et al., 2002) among contexts, while the syntagmatic kernels evaluate the similarity among collocations. We will demonstrate that using DMs induced from unlabeled corpora is a feasible strategy to increase the generalization capability of the WSD algorithm. Our system far outperforms the state-ofthe-art systems in all the tasks in which it has been tested."
P06-1057,C04-1177,0,0.0111673,"ubstitution lexicons in practical applications, since they would mostly introduce noise to the system. To avoid this problem the list of WordNet synonyms for each target word was filtered by a lexicographer, who excluded manually obscure synonyms that seemed worthless in practice. The source synonym for each target word was then picked randomly from the filtered list. Table 1 shows the 25 source-target pairs created for our experiments. In future work it may be possible to apply automatic methods for filtering infrequent sense correspondences in the dataset, by adopting algorithms such as in (McCarthy et al., 2004). Problem Setting and Dataset To investigate the direct sense matching problem it is necessary to obtain an appropriate dataset of examples for this binary classification task, along with gold standard annotation. While there is no such standard (application independent) dataset available it is possible to derive it automatically from existing WSD evaluation datasets, as described below. This methodology also allows comparing direct approaches for sense matching with classical indirect approaches, which apply an intermediate step of identifying the most likely WordNet sense. We derived our dat"
P06-1057,W02-0816,0,0.492519,"Missing"
P06-1057,J98-1004,0,0.0536338,"Missing"
P06-1057,W98-0705,0,0.0286631,"Missing"
P06-1057,P94-1013,0,0.0448618,"Missing"
P06-1057,P98-2127,0,0.0820922,"htein1 , Carlo Strapparava2 1 Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel 2 ITC-Irst, via Sommarive, I-38050, Trento, Italy Abstract ference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most commonly used resource for lexical substitution is the manually constructed WordNet (Fellbaum, 1998). Another option is to use statistical word similarities, such as in the database constructed by Dekang Lin (Lin, 1998). We generically refer to such resources as substitution lexicons. When using a substitution lexicon it is assumed that there are some contexts in which the given synonymous words share the same meaning. Yet, due to polysemy, it is needed to verify that the senses of the two words do indeed match in a given context. For example, there are contexts in which the source word ‘weapon’ may be substituted by the target word ‘arm’; however one should recognize that ‘arm’ has a different sense than ‘weapon’ in sentences such as “repetitive movements could cause injuries to hands, wrists and arms.” A c"
P06-1057,W97-0322,0,\N,Missing
P06-1057,W07-1401,1,\N,Missing
P06-1057,C98-2122,0,\N,Missing
P06-1070,W02-0902,0,0.0107383,"even if in principle it could exist and return 1 for a strict subset of document pairs. The texts inside comparable corpora, being about the same topics, should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing us to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that comparable corpora contain a large amount of such words, just because texts, referring to the same topics in different languages, will often adopt the same terms to denote the same entities1 . 1 554 According to our assumption, a possible additional crirelies on the availability of a multilingual lexical resource. For languages with scarce resources a bilingual dictionary could be not easily available. Secondly, an important requirement of such a resource is its coverage (i.e. the amount of possible translation pairs that are actually contained in it). Finally, another"
P06-1070,W04-0856,1,0.233652,"Section 5, this model is rather poor because of its sparseness. In the next section, we will show how to use such words as seeds to induce a Multilingual Domain VSM, in which second order relations among terms and documents in different languages are considered to improve the similarity estimation. 3.1 Multilingual Domain Model A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such as Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. A Domain Model is composed of soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identify groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among terms in all the languages and domains, as illustrated in Table 1. For example the term virus is associated to both 3 E"
P06-1070,P04-1023,0,0.0138086,"Such limitation is mainly due to the problem of tuning large scale multilingual lexical resources (e.g. MultiWordNet, EuroWordNet) for the specific application task (e.g. discarding irrelevant senses, extending the lexicon with domain specific terms and their translations). On the other hand, empirical approaches are in general more accurate, because they can be trained from domain specific collections of parallel text to represent the application needs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation (Callison-Burch et al., 2004), Cross Lingual Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language (e.g. English) while the system is trained using labeled documents in a source language (e.g. Italian). In this work we present many solutions according to the availability of bilingual resources, and we show that it is possible to deal with the problem even when no such resources are accessible. The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora. Experiments show the effectiveness of our approach, provi"
P06-1070,P04-1067,0,0.0121436,"ing labeled examples in a source language (e.g. English), and it classifies documents in a different target language (e.g. Italian). 553 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553–560, c Sydney, July 2006. 2006 Association for Computational Linguistics Information Retrieval (Littman et al., 1998), and so on. However it is not always easy to find or build parallel corpora. This is the main reason why the “weaker” notion of comparable corpora is a matter of recent interest in the field of Computational Linguistics (Gaussier et al., 2004). In fact, comparable corpora are easier to collect for most languages (e.g. collections of international news agencies), providing a low cost knowledge source for multilingual applications. The main problem of adopting comparable corpora for multilingual knowledge acquisition is that only weaker statistical evidence can be captured. In fact, while parallel corpora provide stronger (text-based) statistical evidence to detect translation pairs by analyzing term co-occurrences in translated documents, comparable corpora provides weaker (term-based) evidence, because text alignments are not avail"
P06-1070,W05-0802,1,0.614459,"Abstract The applicative interest for the CLTC is immediately clear in the globalized Web scenario. For example, in the community based trade (e.g. eBay) it is often necessary to archive texts in different languages by adopting common merceological categories, very often defined by collections of documents in a source language (e.g. English). Another application along this direction is Cross Lingual Question Answering, in which it would be very useful to filter out the candidate answers according to their topics. In the literature, this task has been proposed quite recently (Bel et al., 2003; Gliozzo and Strapparava, 2005). In those works, authors exploited comparable corpora showing promising results. A more recent work (Rigutini et al., 2005) proposed the use of Machine Translation techniques to approach the same task. Classical approaches for multilingual problems have been conceived by following two main directions: (i) knowledge based approaches, mostly implemented by rule based systems and (ii) empirical approaches, in general relying on statistical learning from parallel corpora. Knowledge based approaches are often affected by low accuracy. Such limitation is mainly due to the problem of tuning large sc"
P16-2040,D10-1049,0,0.166285,"years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of target"
P16-2040,W11-2810,0,0.0313148,"he generation pipeline paradigm (Reiter and Dale, 1997), with content selection determined by the relation in the company’s DBpedia entry while microplanning and realization are carried out through template generation. While some generation systems, particularly in early years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on s"
P16-2040,H05-1115,0,0.0322632,"Missing"
P16-2040,P06-2020,0,0.1083,"Missing"
P16-2040,P09-1024,0,0.0333998,"company and combines 243 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 243–248, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tence selection system (Blair-Goldensohn et al., 2003; Weischedel et al., 2004; Schiffman et al., 2001). In our approach, we target both relevance and variety of expression, driving content by selecting sentences that match company/entity pairs and inducing multiple patterns of expression. Sentence selection has also been used in prior work on generating Wikipedia overall articles (Sauper and Barzilay, 2009). Their focus is more on learning domain-specific templates that control the topic structure of an overview, a much longer text than we generate. ically used to express the relations on the web. In the process, it uses bootstrapping (Agichtein and Gravano, 2000) to learn new ways of expressing the relations corresponding to each company/entity pair, alternating with learning new pairs that match the learned expression patterns. Since the bootstrapping process is driven only by company/entity pairs and lexical patterns, it has the potential to learn a wider variety of expressions for each pair"
P16-2040,P01-1059,0,0.200363,"Missing"
P16-2040,I11-1032,1,0.806686,"bstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of targeted and data-driven methods for a pure sen244 pattern.1 The entities are therefore assumed to be related since they are expressed in the same way as the seed pair. Unlike the TD approach, the actual relationship between the entities is unknown (since the only data we use is the web text, not the structured RDF data); all we need to know here is that a relationship exists. We alternate learning the patterns and generating entity pairs over our development set of 100 companies. We then tak"
P16-2040,H05-1015,0,0.040684,"eration has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of targeted and data-driven methods for a pure sen244 pattern.1 The entities are therefore assumed to be related since they are expressed in the same way as the see"
P16-2040,E14-4041,0,0.0206945,"digm (Reiter and Dale, 1997), with content selection determined by the relation in the company’s DBpedia entry while microplanning and realization are carried out through template generation. While some generation systems, particularly in early years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant s"
P16-2040,W14-4424,0,0.0271504,", relevant facts were discarded due to phrases that mentioned lesser facts we did not have the data to replace. We therefore added a postprocessing step to remove, if possible, any phrases Related Work The TD approach falls into the generation pipeline paradigm (Reiter and Dale, 1997), with content selection determined by the relation in the company’s DBpedia entry while microplanning and realization are carried out through template generation. While some generation systems, particularly in early years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occup"
P16-2040,P13-1138,0,0.14605,"cated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of targeted and data-driven method"
P16-2040,W07-0734,0,0.0595219,"ompare the three versions of our output - generated by the TD, DD, and hybrid approach - against multi-document summaries generated (from the same search results used by our DD approach) by TextRank (Mihalcea and Tarau, 2004). For each one of the approaches as well as the baseline, we generated descriptions for all companies that were part of the S&P500 as of January 2016. We used our development set of 100 companies for tuning, and the evaluation results are based on the remaining 400. We conducted two types of experiments. The first is an automated evaluation, where we use the METEOR score (Lavie and Agarwal, 2007) between the description generated by one of our approaches or by the baseline and the first section of the Wikipedia article for the company. In Wikipedia articles, the first section typically serves as an introduction or overview of the most important information about the company. METEOR scores capture the content overlap between the generated description and the Wikipedia text. To avoid bias from different text sizes, we set the same size limit for all descriptions when comparing them. We experimented with three settings: 150 words, 500 words, and no size limit. In addition, we conducted a"
P16-2040,W04-3252,0,\N,Missing
P16-2040,D15-1230,1,\N,Missing
P18-1147,C16-1332,0,0.0195486,"100000 10000 1000 100 10 Number of Unary Relations Schema is an approach based on combining the matrix factorization approach of universal schema (Riedel et al., 2013), with repesentations of textual relations produced by an LSTM (Chang et al., 2016). The rows of the universal schema matrix are entity pairs, and will only be supported by a textual relation if they occur in a sentence together. Other approaches to relational knowledge induction have used distributed representations for words or entities and used a model to predict the relation between two terms based on their semantic vectors (Drozd et al., 2016). This enables the discovery of relations between terms that do not co-occur in the same sentence. However, the distributed representation of the entities is developed from the corpus without any ability to focus on the relations of interest. One example of such work is LexNET, which developed a model using the distributional word vectors of two terms to predict lexical relations between them (DSh ). The term vectors are concatenated and used as input to a single hidden layer neural network. Unlike our approach to unary relations the term vectors are produced by a standard relation-independent"
P18-1147,D15-1205,0,0.0409089,"Missing"
P18-1147,P16-1145,0,0.0270581,"Missing"
P18-1147,P16-1200,0,0.324582,"this network represent the probability for the input entity to belong to each unary relation. To demonstrate the effectiveness of our approach we developed a new KBP benchmark, consisting of extracting unseen DBPedia triples from the text of a web crawl, using a portion of DBpedia to train the model. As part of the contributions for this paper, we release the benchmark to the research community providing the software needed to generate it from Common Crawl and DBpedia as an open source project2 . As a baseline, we adapt a state of the art deep learning based approach for relation extraction (Lin et al., 2016). Our experiments clearly show that using unary relations to generate new triples greatly complements traditional binary approaches. An analysis of the data shows that our approach is able to capture implicit information from textual mentions and to highlight the reasons why the assignments have been made. The paper is structured as follows. In section 2 we describe the state of the art in distantly super2 https://github.com/IBM/cc-dbp vised KBP methodologies, with a focus on knowledge induction applications. Section 3 introduces the use of Unary Relations for KBP and section 4 outlines the pr"
P18-1147,P09-1113,0,0.185109,"duces the use of Unary Relations for KBP and section 4 outlines the process for producing and training them. Section 5 describes a deep learning architecture able to recognize unary relations from textual evidence. In section 6 we describe the benchmark for evaluation. Section 7 provides an extensive evaluation of unary relations, and a saliency map exploration of what the deep learning model has learned. Section 8 concludes the paper highlighting research directions for future work. 2 Related Work Binary relation extraction using distant supervision has a long history (Surdeanu et al., 2012; Mintz et al., 2009). Mentions of entities from the knowledge base are located in text. When two entities are mentioned in the same sentence that sentence becomes part of the evidence for the relation (if any) between those entities. The set of sentences mentioning an entity pair is used in a machine learning model to predict how the entities are related, if at all. Deep learning has been applied to binary relation extraction. CNN-based (Zeng et al., 2014), LSTM-based (Xu et al., 2015), attention based (Wang et al., 2016) and compositional embedding based (Gormley et al., 2015) models have been trained successful"
P18-1147,Q17-1008,0,0.0139081,"f any) between those entities. The set of sentences mentioning an entity pair is used in a machine learning model to predict how the entities are related, if at all. Deep learning has been applied to binary relation extraction. CNN-based (Zeng et al., 2014), LSTM-based (Xu et al., 2015), attention based (Wang et al., 2016) and compositional embedding based (Gormley et al., 2015) models have been trained successfully using a sentence as the unit of context. Recently, cross sentence approaches have been explored by building paths connecting the two identified arguments through related entities (Peng et al., 2017; Zeng et al., 2016). These approaches are limited by requiring both entities to be mentioned in a textual context. The context aggregation approaches of state-of-the-art neural models, max-pooling (Zeng et al., 2015) and attention (Lin et al., 2016), do not consider that different contexts may contribute to the prediction in different ways. Instead, the context pooling only determines the degree of a sentence’s contribution to the relation prediction. TAC-KBP is a long running challenge for knowledge base population. Effective systems in these competitions combine many approaches such as rule"
P18-1147,D15-1203,0,0.592913,"extraction. CNN-based (Zeng et al., 2014), LSTM-based (Xu et al., 2015), attention based (Wang et al., 2016) and compositional embedding based (Gormley et al., 2015) models have been trained successfully using a sentence as the unit of context. Recently, cross sentence approaches have been explored by building paths connecting the two identified arguments through related entities (Peng et al., 2017; Zeng et al., 2016). These approaches are limited by requiring both entities to be mentioned in a textual context. The context aggregation approaches of state-of-the-art neural models, max-pooling (Zeng et al., 2015) and attention (Lin et al., 2016), do not consider that different contexts may contribute to the prediction in different ways. Instead, the context pooling only determines the degree of a sentence’s contribution to the relation prediction. TAC-KBP is a long running challenge for knowledge base population. Effective systems in these competitions combine many approaches such as rule-based relation extraction, directly supervised linear and neural network extractors, distantly supervised neural network models (Zhang et al., 2016) and tensor factorization approaches to relation prediction. Composi"
P18-1147,C14-1220,0,0.0410348,"ghlighting research directions for future work. 2 Related Work Binary relation extraction using distant supervision has a long history (Surdeanu et al., 2012; Mintz et al., 2009). Mentions of entities from the knowledge base are located in text. When two entities are mentioned in the same sentence that sentence becomes part of the evidence for the relation (if any) between those entities. The set of sentences mentioning an entity pair is used in a machine learning model to predict how the entities are related, if at all. Deep learning has been applied to binary relation extraction. CNN-based (Zeng et al., 2014), LSTM-based (Xu et al., 2015), attention based (Wang et al., 2016) and compositional embedding based (Gormley et al., 2015) models have been trained successfully using a sentence as the unit of context. Recently, cross sentence approaches have been explored by building paths connecting the two identified arguments through related entities (Peng et al., 2017; Zeng et al., 2016). These approaches are limited by requiring both entities to be mentioned in a textual context. The context aggregation approaches of state-of-the-art neural models, max-pooling (Zeng et al., 2015) and attention (Lin et"
P18-1147,N13-1008,0,0.0779516,"ity as one argument, thus providing enough training for each unary classifier. Therefore, in the example above, we will not likely be able to generate predicates for all the 195 countries, because some of them will either not occur at all in the training data or they will be very infrequent. However, even in cases where arguments tend to follow a long tail distribution, it makes sense to generate unary predicates for the most frequent ones. 1000000 100000 10000 1000 100 10 Number of Unary Relations Schema is an approach based on combining the matrix factorization approach of universal schema (Riedel et al., 2013), with repesentations of textual relations produced by an LSTM (Chang et al., 2016). The rows of the universal schema matrix are entity pairs, and will only be supported by a textual relation if they occur in a sentence together. Other approaches to relational knowledge induction have used distributed representations for words or entities and used a model to predict the relation between two terms based on their semantic vectors (Drozd et al., 2016). This enables the discovery of relations between terms that do not co-occur in the same sentence. However, the distributed representation of the en"
P18-1147,D12-1042,0,0.0722393,"ations. Section 3 introduces the use of Unary Relations for KBP and section 4 outlines the process for producing and training them. Section 5 describes a deep learning architecture able to recognize unary relations from textual evidence. In section 6 we describe the benchmark for evaluation. Section 7 provides an extensive evaluation of unary relations, and a saliency map exploration of what the deep learning model has learned. Section 8 concludes the paper highlighting research directions for future work. 2 Related Work Binary relation extraction using distant supervision has a long history (Surdeanu et al., 2012; Mintz et al., 2009). Mentions of entities from the knowledge base are located in text. When two entities are mentioned in the same sentence that sentence becomes part of the evidence for the relation (if any) between those entities. The set of sentences mentioning an entity pair is used in a machine learning model to predict how the entities are related, if at all. Deep learning has been applied to binary relation extraction. CNN-based (Zeng et al., 2014), LSTM-based (Xu et al., 2015), attention based (Wang et al., 2016) and compositional embedding based (Gormley et al., 2015) models have be"
P18-1147,P16-1123,0,0.0475621,"Missing"
P18-1147,D15-1206,0,0.0273401,"or future work. 2 Related Work Binary relation extraction using distant supervision has a long history (Surdeanu et al., 2012; Mintz et al., 2009). Mentions of entities from the knowledge base are located in text. When two entities are mentioned in the same sentence that sentence becomes part of the evidence for the relation (if any) between those entities. The set of sentences mentioning an entity pair is used in a machine learning model to predict how the entities are related, if at all. Deep learning has been applied to binary relation extraction. CNN-based (Zeng et al., 2014), LSTM-based (Xu et al., 2015), attention based (Wang et al., 2016) and compositional embedding based (Gormley et al., 2015) models have been trained successfully using a sentence as the unit of context. Recently, cross sentence approaches have been explored by building paths connecting the two identified arguments through related entities (Peng et al., 2017; Zeng et al., 2016). These approaches are limited by requiring both entities to be mentioned in a textual context. The context aggregation approaches of state-of-the-art neural models, max-pooling (Zeng et al., 2015) and attention (Lin et al., 2016), do not consider th"
picca-etal-2008-lmm,P98-1013,0,\N,Missing
picca-etal-2008-lmm,C98-1013,0,\N,Missing
picca-etal-2008-supersense,C04-1053,0,\N,Missing
picca-etal-2008-supersense,W02-1001,0,\N,Missing
picca-etal-2008-supersense,W03-1022,1,\N,Missing
picca-etal-2008-supersense,W06-1670,1,\N,Missing
picca-etal-2008-supersense,H05-1064,0,\N,Missing
picca-etal-2008-supersense,C96-1079,0,\N,Missing
S01-1027,magnini-cavaglia-2000-integrating,1,0.739707,"stical techniques, for the three tasks we participated in, i.e. English &apos;all words&apos;, English &apos;lexical sample&apos; and Italian &apos;lexical sample&apos;. The main lexical resource for domains is &quot;WordNet Domains&quot;, an extension of English Wordnet 1.6 (Fellbaum, 1998) developed at ITC-irst, where synsets have been annotated with domain information. 2 WordN et Domains The basic lexical resource we used in SENSEVAL2 is &quot;WordNet Domains&quot;, an extension of WoRDNET 1.6 where each synset has been annotated with at least one domain label, selected from a set of about two hundred labels hierarchically organized (see (Magnini and Cavaglia, 2000) for the annotation methodology and for the evaluation of the resource). The information from the domains that we added is complementary to what is already in WoRDNET. First of all a domain may include synsets of different syntactic categories: for instance MEDICINE groups together senses from Nouns, such as doctor#i and hospi tal#i, and from Verbs such as operate#7. Second, a domain may include senses from different WoRDNET sub-hierarchies (i.e. deriving from different &quot;unique beginners&quot; or from different &quot;lexicographer files&quot;). For example, SPORT contains senses such as athlete#i, deriving f"
S01-1027,W00-0804,1,0.924982,"focuses on the role of domain information. The hypothesis is that domain labels (such as MEDICINE, ARCHITECTURE and SPORT) provide a natural and powerful way to establish semantic relations among word senses, which can be profitably used during the disambiguation process. In particular, domains constitute a fundamental feature of text coherence, such that word senses occurring in a coherent portion of text tend to maximize domain similarity. The importance of domain information in WSD has been remarked in several works, including (Gonzalo et al., 1998) and (Buitelaar and Sacaleanu, 2001). In (Magnini and Strapparava, 2000) we introduced &quot;Word Domain Disambiguation&quot; (WDD) as a variant of WSD where for each word in a text a domain label (among those allowed by the word) has to be chosen instead of a sense label. We also argued that WDD can be applied to disambiguation tasks that do not require fine grained sense distinctions, such as information retrieval and content-based user modeling. For SENSEVAL111 2 the goal was to evaluate the role of domain information in WSD: no other syntactic or semantic information has been used (e.g. semantic relations in WoRDNET) except domain labels. Three systems have been impleme"
S07-1029,P06-1057,1,0.540703,"which is formally defined as a relationship between a coherent text T and a language expression, the hypothesis H. T is said to entail H, denoted by T → H, if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005; Dagan and Glickman., 2004). Even though this notion has been only recently proposed in the computational linguistics literature, it attracts more and more attention due to the high generality of its settings and to the usefulness of its (potential) applications. 1 In the literature, slight variations of this problem have been also referred to as sense matching (Dagan et al., 2006). With respect to lexical entailment, the lexical substitution task has a more restrictive criterion. In fact, two words can be substituted when meaning is preserved, while the criterion for lexical entailment is that the meaning of the thesis is implied by the meaning of the hypothesis. The latter condition is in general ensured by substituting either hyperonyms or synonyms, while the former is more rigid because only synonyms are in principle accepted. Formally, in a lexical entailment task a system is asked to decide whether the substitution of a particular term w with the term e in a coher"
S07-1029,W06-1621,0,0.0200022,"Abstract This paper summarizes FBK-irst participation at the lexical substitution task of the S EMEVAL competition. We submitted two different systems, both exploiting synonym lists extracted from dictionaries. For each word to be substituted, the systems rank the associated synonym list according to a similarity metric based on Latent Semantic Analysis and to the occurrences in the Web 1T 5-gram corpus, respectively. In particular, the latter system achieves the state-of-the-art performance, largely surpassing the baseline proposed by the organizers. 1 Introduction The lexical substitution (Glickman et al., 2006a) can be regarded as a subtask of the lexical entailment, in which for a given word in context the system is asked to select an alternative word that can be replaced in that context preserving the meaning. Lexical Entailment, and in particular lexical reference (Glickman et al., 2006b)1 , is in turn a subtask of textual entailment, which is formally defined as a relationship between a coherent text T and a language expression, the hypothesis H. T is said to entail H, denoted by T → H, if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005; Dagan and Glickman., 2004). Ev"
S07-1029,W06-2907,0,\N,Missing
W04-0856,W04-0800,0,0.235941,"Missing"
W04-0856,magnini-cavaglia-2000-integrating,0,0.334557,"Missing"
W04-0856,H93-1052,0,0.298182,"Missing"
W04-3249,C00-1066,0,0.0320323,", domain detection allows a number of useful simplifications in text processing applications, such as, for instance, in Word Sense Disambiguation (WSD). In this paper we introduce Domain Relevance Estimation (DRE) a fully unsupervised technique for domain detection. Roughly speaking, DRE can be viewed as a text categorization (TC) problem (Sebastiani, 2002), even if we do not approach the problem in the standard supervised setting requiring category labeled training data. In fact, recently, unsupervised approaches to TC have received more and more attention in the literature (see for example (Ko and Seo, 2000). We assume a pre-defined set of categories, each defined by means of a list of related terms. We call such categories domains and we consider them as a set of general topics (e.g. S PORT, M EDICINE, P OLITICS) that cover the main disciplines and areas of human activity. For each domain, the list of related words is extracted from W ORD N ET D O MAINS (Magnini and Cavagli`a, 2000), an extension of W ORD N ET in which synsets are annotated with domain labels. We have identified about 40 domains (out of 200 present in W ORD N ET D OMAINS) and we will use them for experiments throughout the paper"
W04-3249,magnini-cavaglia-2000-integrating,1,0.883007,"Missing"
W04-3249,S01-1005,0,0.012662,"- Table 2: W ORD N ET senses and domains for the word “bank”. from act#2, and playing field#1 from location#1. Domains may group senses of the same word into thematic clusters, which has the important sideeffect of reducing the level of ambiguity when we are disambiguating to a domain. Table 2 shows an example. The word “bank” has ten different senses in W ORD N ET 1.6: three of them (i.e. bank#1, bank#3 and bank#6) can be grouped under the E CONOMY domain, while bank#2 and bank#7 both belong to G EOGRAPHY and G EOL OGY. Grouping related senses is an emerging topic in WSD (see, for instance (Palmer et al., 2001)). Finally, there are W ORD N ET synsets that do not belong to a specific domain, but rather appear in texts associated with any domain. For this reason, a FACTOTUM label has been created that basically includes generic synsets, which appear frequently in different contexts. Thus the FACTOTUM domain can be thought of as a “placeholder” for all other domains. 3 Domain Relevance Estimation for Texts The basic idea of domain relevance estimation for texts is to exploit lexical coherence inside texts. From the domain point of view lexical coherence is equivalent to domain coherence, i.e. the fact"
W05-0608,W04-0856,1,0.479824,"ers of terms. Each cluster represents a semantic domain (Gliozzo et al., 2004), i.e. a set of terms that often co-occur in texts having similar topics. A Domain Model is represented by a k × k 0 rectangular matrix D, containing the degree of association among terms and domains, as illustrated in Table 1. HIV AIDS virus laptop M EDICINE 1 1 0.5 0 C OMPUTER S CIENCE 0 0 0.5 1 Table 1: Example of Domain Matrix 1 The idea of exploiting a Domain Kernel to help a supervised classification framework, has been profitably used also in other NLP tasks such as word sense disambiguation (see for example (Strapparava et al., 2004)). 57 Domain Models can be used to describe lexical ambiguity and variability. Lexical ambiguity is represented by associating one term to more than one domain, while variability is represented by associating different terms to the same domain. For example the term virus is associated to both the domain C OMPUTER S CIENCE and the domain M EDICINE (ambiguity) while the domain M EDICINE is associated to both the terms AIDS and HIV (variability). More formally, let D = {D1 , D2 , ..., Dk0 } be a set of domains, such that k 0  k. A Domain Model is fully defined by a k × k 0 domain matrix D repres"
W05-0802,P04-1067,0,0.0222367,"fferent news agencies), and it is not known if a func10 tion ψ exists, even if in principle it could exist and return 1 for a strict subset of document pairs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation, Cross language Information Retrieval (Littman et al., 1998), lexical acquisition, and so on. However it is not always easy to find or build parallel corpora. This is the main reason because the weaker notion of comparable corpora is a matter recent interest in the field of Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that co"
W05-0802,W02-0902,0,0.011052,"Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that comparable corpora contain a large amount of such words, just because texts, referring to the same topics in different languages, will often adopt the same terms to denote the same entities1 . However, the simple presence of these shared words is not enough to get significant results in TC tasks. As we will see, we need to exploit these common words to induce a second-order similarity for the other words in the lexicons. 3 The Multilingual Vector Space Model Let T = {t1 , t2 , . . . , tn } be a corpus, and V = {w1 , w2 , . . . , wk } be its vocabulary. In the monolingu"
W05-0802,magnini-cavaglia-2000-integrating,0,0.0339774,"Missing"
W05-0802,W04-0856,1,0.625109,"Section 6, this model is rather poor because of its sparseness. In the next section, we will show how to use such words as seeds to induce a Multilingual Domain VSM, in which second order relations among terms and documents in different languages are considered to improve the similarity estimation. 4 Multilingual Domain Models A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such us Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. 11 A Domain Model is composed by soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identifies groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among terms in all the languages and domains, as illustrated in Table 1. e/i HIV AIDS e/i viruse/i hospitale laptope M i"
W06-2608,P05-1050,1,0.535293,"k from the Vector Space Model (Salton and McGill, 1983) into the Domain Space (see Section 4), defined by the following mapping: C OMPUTER S CIENCE 0 0 0.5 1 Table 1: Example of Domain Model. ing a lexical coherence assumption (Gliozzo, 2005). To this aim, Term Clustering algorithms can be used: a different domain is defined for each cluster, and the degree of association between terms and clusters, estimated by the unsupervised learning algorithm, provides a domain relevance function. As a clustering technique we exploit Latent Semantic Analysis (LSA), following the methodology described in (Gliozzo et al., 2005b). This operation is done offline, and can be efficiently performed on large corpora. LSA is performed by means of SVD of the termby-document matrix T representing the corpus. The SVD algorithm can be exploited to acquire a domain matrix D from a large corpus in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrices T = VΣ k UT where Σk is the diagonal k × k matrix containing the k singular values of T. D = VΣk0 where k 0  k. Once a DM has been defined by the matrix D, the Domain Space is a k 0 dimensional space, in which both texts and terms are represe"
W06-2608,P94-1013,0,\N,Missing
W09-3531,quasthoff-etal-2006-corpus,0,0.0380059,"Missing"
W09-3531,C04-1053,0,0.0760618,"Missing"
W09-3531,W03-1022,0,0.721639,"Recognition (NER) is a basic task in NLP that has the intent of automatically recognizing Named Entities. Incidentally, NER systems can be a useful step for broad-coverage ontology engineering but they have two main limitations: • Traditional categories (e.g., person, location, and organization) are too few and generic. It is quite evident that taxonomies require more categories than the three mentioned above. • Even though NER systems are supposed to recognize individuals, very often they also returns common names and no clear distinction with concepts is made. 2 A Super Sense Tagger (SST) (Ciaramita and Johnson, 2003) is an extended NER system that uses the wider set of categories composed by the 41 most general concepts defined by WordNet. WordNet has been organized according to psycholinguistic theories on the principles governing lexical memory (Beckwith et al., 1991). Thus the broadest WordNet’s categories can serve as basis for a set of categories which exhaustively covers, at least as a first approximation, all possible concepts occurring in a sentence. The aim of this paper is to develop and explore the property of instances being lexicalized identically in different languages in order to produce a"
W09-3531,W02-1001,0,0.0514683,"Missing"
W09-3531,picca-etal-2008-supersense,1,0.845809,"Missing"
W09-3531,J06-1001,0,\N,Missing
W13-3413,C12-1109,0,0.0128479,"t practices and metrics. • Natural Language Processing in Watson Murdock et al. (2012a), McCord et al. (2012). • Structured Knowledge in Watson Murdock et al. (2012b), Kalyanpur et al. (2012), Fan et al. (2012). • Semantic Web OWL, RDF, Semantic Web resources. • Domain Adaptation Ferrucci et al. (2012). • UIMA The UIMA framework, Annotators, Types, Descriptors, tools. Hands-on exercise with the class project architecture (Epstein et al., 2012). • Midterm Workshop Presentations of each team’s project idea and their research into related work and the state of the art. • Distributional Semantics Miller et al. (2012), Gliozzo and Isabella (2005). • Machine Learning and Strategy in Watson 2 87 http://uima.apache.org Figure 2: Overview of the class project framework 1. Query Analysis their project by extending this framework. Even though we built the framework to perform semantic search over a text corpus, many of the teams in this course had projects that went far beyond just semantic search. Our hope was that each team would be able to able independently develop interesting new components for the processing stages of the pipeline, and at the end of the course we would be able to merge the most interesting"
W13-5002,J10-4006,0,0.099525,"ext, and splits these observations into a pair of two parts, which we call the “Jo” and the “Bim”2 . All JoBim pairs are maintained in the bipartite First-Order JoBim graph T C(T, C, E) with T set of terms (Jos), C set of contexts (Bims), and e(t, c, f ) ∈ E edges between t ∈ T , c ∈ C with frequency f . While these parts can be thought of as language elements referred to as terms, and their respective context features, splits over arbitrary structures are possible (including pairs of terms for Jos), which makes this formulation more general than similar formulations found e.g. in (Lin, 1998; Baroni and Lenci, 2010). These splits form the basis for the computation of global similarities and for their contextualization. A Holing System based on dependency parses is illustrated in Figure 1: for each dependency relation, two JoBim pairs are generated. 2.2 Distributed Distributional Thesaurus Computation We employ the Apache Hadoop MapReduce Framwork3 , and Apache Pig4 , for parallelizing and distributing the computation of the DT. We describe this computation in terms of graph transformations. 2 arbitrary names to emphasize the generality, should be thought of as ”term” and ”context” 3 http://hadoop.apache."
W13-5002,D10-1115,0,0.0529973,"Missing"
W13-5002,W10-2309,1,0.861363,"corpus and the general out-of-context similarity of each expanded term to the corresponding term in the original sentence. Therefore the probability associated with any expansion t for any position xi is given by Equation 1. Where Z is the partition function, a normalization constant. P (xi = t) = 1 Z X ew(x) (1) {x |xi =t} The balance between the plausibility of an expanded sentence according to the language model, and its per-term similarity to the original sentence is an application specific tuning parameter. 2.4 We cluster this graph using the Chinese Whispers graph clustering algorithm (Biemann, 2010), which finds the number of clusters automatically, to obtain induced word senses. Running shallow, partof-speech-based IS-A patterns (Hearst, 1992) over the text collection, we obtain a list of extracted ISA relationships between terms, and their frequency. For each of the word clusters, consisting of similar terms for the same target term sense, we aggregate the IS-A information by summing the frequency of hypernyms, and multiplying this sum by the number of words in the cluster that elicited this hypernym. This results in taxonomic information for labeling the clusters, which provides an ab"
W13-5002,de-marneffe-etal-2006-generating,0,0.0289943,"Missing"
W13-5002,J93-1003,0,0.142765,"tions in (Biemann and Riedl, 2013), but could be replaced by any other measure without interfering with the remainder of the system. 2.3 Contextualization with CRF While the distributional thesaurus provides the similarity between pairs of terms, the fidelity of a particular expansion depends on the context. From the term-context associations gathered in the construction of the distributional thesaurus we effectively have a language model, factorized according to the holing operation. As with any language model, smoothing is critical to performance. There may be 5 we use log-likelihood ratio (Dunning, 1993) or LMI (Evert, 2004) many JoBim (term-context) pairs that are valid and yet under represented in the corpus. Yet, there may be some similar term-context pair that is attested in the corpus. We can find similar contexts by expanding the term arguments with similar terms. However, again we are confronted with the fact that the similarity of these terms depends on the context. This suggests some technique of joint inference to expand terms in context. We use marginal inference in a conditional random field (CRF) (Lafferty et al., 2001). A particular world, x is defined as single, definite sequen"
W13-5002,D08-1094,0,0.113522,"Missing"
W13-5002,C92-2082,0,0.630311,"associated with any expansion t for any position xi is given by Equation 1. Where Z is the partition function, a normalization constant. P (xi = t) = 1 Z X ew(x) (1) {x |xi =t} The balance between the plausibility of an expanded sentence according to the language model, and its per-term similarity to the original sentence is an application specific tuning parameter. 2.4 We cluster this graph using the Chinese Whispers graph clustering algorithm (Biemann, 2010), which finds the number of clusters automatically, to obtain induced word senses. Running shallow, partof-speech-based IS-A patterns (Hearst, 1992) over the text collection, we obtain a list of extracted ISA relationships between terms, and their frequency. For each of the word clusters, consisting of similar terms for the same target term sense, we aggregate the IS-A information by summing the frequency of hypernyms, and multiplying this sum by the number of words in the cluster that elicited this hypernym. This results in taxonomic information for labeling the clusters, which provides an abstraction layer for terms in context6 . Table 1 shows an example of this labeling from the model described below. The most similar 200 terms for ”ja"
W13-5002,A83-1012,0,0.184331,"Missing"
W13-5002,N10-4001,0,0.0231793,"employed in other frameworks for distributional semantics such as LSA (Deerwester et al., 1990) or LDA (Blei et al., 2003). The main contribution of this paper is to describe how we operationalize semantic similarity in a graph-based framework and explore this semantic graph using an interactive visualization. We describe a scalable and flexible computation of a distributional thesaurus (DT), and the contextualization of distributional similarity for specific occurrences of language elements (i.e. terms). For related works on the computation of distributional similarity, see e.g. (Lin, 1998; Lin and Dyer, 2010). 2.1 Holing System To keep the framework flexible and abstract with respect to the pre-processing that identifies structure in language material, we introduce the holing operation, cf. (Biemann and Riedl, 2013). It is applied to observations over the structure of text, and splits these observations into a pair of two parts, which we call the “Jo” and the “Bim”2 . All JoBim pairs are maintained in the bipartite First-Order JoBim graph T C(T, C, E) with T set of terms (Jos), C set of contexts (Bims), and e(t, c, f ) ∈ E edges between t ∈ T , c ∈ C with frequency f . While these parts can be tho"
W13-5002,P98-2127,0,0.946274,"t. We construct a semantic analyzer able to self-adapt to new domains and languages by unsupervised learning of semantics from large corpora of raw text. At the moment, this analyzer encompasses contextualized similarity, sense clustering, and a mapping of senses to existing knowledge bases. While its primary target application is functional domain adaptation of Question Answering (QA) systems (Fer1 http://sf.net/projects/jobimtext/ Underlying Technologies While distributional semantics (de Saussure, 1959; Harris, 1951; Miller and Charles, 1991) and the computation of distributional thesauri (Lin, 1998) has been around for decades, its full potential has yet to be utilized in Natural Language Processing (NLP) tasks and applications. Structural semantics claims that meaning can be fully defined by semantic oppositions and relations between words. In order to perform a reliable knowledge acquisition process in this framework, we gather statistical information about word co-occurrences with syntactic contexts from very large corpora. To avoid the intrinsic quadratic complexity of the similarity computation, we have developed an optimized process based on MapReduce (Dean and Ghemawat, 2004) that"
W13-5002,C12-1109,1,0.898356,"Missing"
W13-5002,N13-1133,1,0.897977,"Missing"
W13-5002,C02-1114,0,0.0375749,"ilar terms higher. To model this more explicitly, and to give rise to linking senses to taxonomies and domain ontologies, we apply a word sense induction (WSI) technique and use information extracted by IS-A-patterns (Hearst, 1992) to label the clusters. Using the aggregated context features of the clusters, the word cluster senses are assigned in context. The DT entry for each term j as given in SOJO (J, E) induces an open neighborhood graph Nj (Vj , Ej ) with Vj = {j 0 |e(j, j 0 , s) ∈ E) and Ej the projection of E regarding Vj , consisting of similar terms to j and their similarities, cf. (Widdows and Dorow, 2002). 8 jaguar N.1 IS-A labels car, brand, company, automaker, manufacturer, vehicle animal, species, wildlife, team, wild animal, cat similar terms geely, lincoln-mercury, tesla, peugeot, ..., mitsubishi, cadillac, jag, benz, mclaren, skoda, infiniti, sable, thunderbird panther, cougar, alligator, tiger, elephant, bull, hippo, dragon, leopard, shark, bear, otter, lynx, lion Table 1: Word sense induction and cluster labeling example for “jaguar”. The shortened cluster for the car sense has 186 members. 3 Interactive Visualization 3.1 Open Domain Model The open domain model used in the current visu"
W13-5002,C98-2122,0,\N,Missing
