2011.mtsummit-papers.33,P02-1040,0,0.0842457,"Missing"
2011.mtsummit-papers.33,P03-1021,0,0.0258257,"btain the N unique translation candidates. It should be noticed that two translation candidates are identical only if their translation string and the corresponding feature vector values are identical at the same time 3) For each of the N translation candidates we calculate the total voting score of M systems by simply adding the voting score of each system. The score of each system for each candidate is calculated using the linear combination of the system’s weight vector and the candidate’s feature vector. The weight vector of each member system is gained in the training process using MERT (Och, 2003). While the feature vector of each translation candidate is gained in step 1). The formula for calculating the total voting score of candidate c is listed below:  ሬሬሬሬሬሬሬሬሬԦୡ  ሬሬሬԦ θ  ሺ ሻ ൌ    ୀଵ 4) Finally we re-rank these N translation candidates by the total voting scores and output the one with the highest score In order to be concise and to prove the effectiveness of bagging, we do not add any extra features in our implements and experiments. 4 Experiments 4.1 Experimental Setup We performed the experiments on Chinese-English translation using an in-house implementation of t"
2011.mtsummit-papers.33,P05-1033,0,0.088627,"candidate is gained in step 1). The formula for calculating the total voting score of candidate c is listed below:  ሬሬሬሬሬሬሬሬሬԦୡ  ሬሬሬԦ θ  ሺ ሻ ൌ    ୀଵ 4) Finally we re-rank these N translation candidates by the total voting scores and output the one with the highest score In order to be concise and to prove the effectiveness of bagging, we do not add any extra features in our implements and experiments. 4 Experiments 4.1 Experimental Setup We performed the experiments on Chinese-English translation using an in-house implementation of the hierarchical phrase based SMT model (David Chiang, 2005). The model is tuned using standard MERT (Och, 2003). We use the corpus of NTCIR9 Patent translation task3 Chinese-English part which contains one million sentence pairs. We obtain one thousand sentence pairs for tuning and testing respectively 3 http://ntcir.nii.ac.jp/PatentMT/ 295 without overlap. We use GIZA++4 to perform the bi-directional word alignment between source and target side of each sentence pair. The final word alignment is generated using the grow-diag-final method. And at last all sentence pairs with alignment information is used to extract rules and phrases. A 5-gram language"
2011.mtsummit-papers.33,A00-2005,0,0.0700437,"Missing"
2011.mtsummit-papers.33,N09-3001,0,0.0166854,"t from that of training set. Bagging uses the voting result of m classifiers each with a unique distribution of the same model, so generally it is stable in statistics. Secondly bagging can avoid the over-fitting problem which a plenty of classifiers suffer. Finally bagging can be seen as an unsupervised method which doesn’t need the labeled corpus used to train the recognizer in domain recognizing methods. Bagging has been used successfully in many NLP applications such as Syntactic Parsing (Hen294 derson and Brill, 2000), Semantic Parsing (Nielsen and Pradhan, 2004), Coreference Resolution (Vemulapalli et al., 2009; Vemulapalli et al., 2010), Word Sense Disambiguation (Nielsen and Pradhan, 2004) and so on. 3 Bagging-based domain adaptation Suppose that there are M available statistics machine systems {ɊሺɅଵ ሻǡ ɊሺɅଶ ሻǡ ǥ ǡ ɊሺɅ ሻ}, the task of system combination is to build a new translation system ɋሺɊሺɅଵ ሻǡ ɊሺɅଶ ሻǡ ǥ ǡ ɊሺɅ ሻሻ which denotes the combination system. It combines the translation outputs from each of its cell system ɊሺɅ୧ ሻ which we call here a member system of it. As discussed in section 1, hardly any single system can achieve a good performance on multidomain translation problem. Besides, th"
2011.mtsummit-papers.33,W02-1405,0,0.0201586,"data. We test the results with the cluster number from 2 to 5, and the results are listed below: clusters 2 3 4 5 BLEU 31.09 31.24 31.05 30.61 Table 4 results of the unsupervised domain recognizing based method From the above results we can see a similar situation: when there are too many clusters the translation performance drops due to data sparsity; and as the cluster number decreases, the performance ascends at first and reaches the highest record of 31.24 BLEU score when the cluster number is three; and finally drops as the discrimination of class recognizer becomes weak. 5 Related Work Langlais (2002) first mention Domain Adaptation problem in SMT area by mention the problem of how to use a SMT to translate a corpus far different from the one it has been trained on. Then he makes notable achievement by integrating specific lexicon tables. Eck et al. (2004) proposed a language model adaptation technique in SMT using information retrieval techniques. Firstly, each test document is translated with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-transla"
2011.mtsummit-papers.33,eck-etal-2004-language,0,0.0275279,"tuation: when there are too many clusters the translation performance drops due to data sparsity; and as the cluster number decreases, the performance ascends at first and reaches the highest record of 31.24 BLEU score when the cluster number is three; and finally drops as the discrimination of class recognizer becomes weak. 5 Related Work Langlais (2002) first mention Domain Adaptation problem in SMT area by mention the problem of how to use a SMT to translate a corpus far different from the one it has been trained on. Then he makes notable achievement by integrating specific lexicon tables. Eck et al. (2004) proposed a language model adaptation technique in SMT using information retrieval techniques. Firstly, each test document is translated with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-translated using the adapted language model. Hasan and Ney (2005) proposed a method for building class-based language models. He applies regular expressions based method to cluster the sentences into specific classes. And then he interpolates them with the main langu"
2011.mtsummit-papers.33,2005.eamt-1.17,0,0.0955281,"antly over a hierarchical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since an SMT system trained on a corpus with heterogeneous topics may fail to achieve a good performance on domain-specific translation, while an SMT system trained on a domain-specific corpus may achieve a deteriorative performance for outof-domain translation (Haque et al., 2009). Besides more and more evaluation tasks begin to focus on mul"
2011.mtsummit-papers.33,W07-0733,0,0.0286338,"with general language model; and then the translation is used to select the most similar documents; then the adapted language model is built using these documents; finally the test document is re-translated using the adapted language model. Hasan and Ney (2005) proposed a method for building class-based language models. He applies regular expressions based method to cluster the sentences into specific classes. And then he interpolates them with the main language models to elude the data sparseness. And finally this method achieves improvements in terms of perplexity reduction and error rates. Koehn and Schroeder (2007) carried out a scheme of integrating in-domain and out-ofdomain language models using log-linear features of an SMT model, and used multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder6. Xu et al. (2007) proposed a method which uses the information retrieval approaches to classify the input sentences. This method is based on domains along with domain-dependent language models and feature weights which are gained in the training process of SMT models. This method resulted in a significant improvement in domain-dependent translation."
2011.mtsummit-papers.33,2007.mtsummit-papers.68,0,0.343632,"sually resort to statistical classifiers, but they require annotated monolingual data in different domains, which may not be available in some cases. We instead propose a simple but effective bagging-based approach without using any annotated data. Large-scale experiments show that our new method improves translation quality significantly over a hierarchical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since"
2011.mtsummit-papers.33,Y09-2027,0,0.0315654,"Missing"
2011.mtsummit-papers.33,2010.amta-papers.16,0,0.234962,"Missing"
2011.mtsummit-papers.33,D07-1054,0,0.0139736,"ical phrase-based baseline by 0.82 BLEU points and it's even higher than some conventional classifier-based methods. 1 Introduction Conventional approaches usually resort to statistical classifiers and there are a plenty of notable jobs on it (Xu et al., 2007; Bertoldi and Federico, 2009; Banerjee et al., 2010). They all achieve significant improvement over baseline when a large amount of annotated monolingual data in multidomains is available for training the classifier. Yet they shrivel when the annotated monolingual data is deficient. Others proposed unsupervised ways (Hasan and Ney, 2005; Yamamoto and Sumita, 2007) and reported improvement over baseline. Yet their results show that they do not outperform the conventional approaches obviously. In recent years domain adaptation problem in SMT becomes more important (Banerjee et al., 2010). Since an SMT system trained on a corpus with heterogeneous topics may fail to achieve a good performance on domain-specific translation, while an SMT system trained on a domain-specific corpus may achieve a deteriorative performance for outof-domain translation (Haque et al., 2009). Besides more and more evaluation tasks begin to focus on multi-domain translation. For e"
2020.acl-main.482,D13-1135,0,0.318673,"Missing"
2020.acl-main.482,P16-1074,0,0.187763,"Missing"
2020.acl-main.482,K17-1023,0,0.0195992,"us stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their task is closer to zero pronoun recovery. Most similar to our work, Liu et al. (2017) converted zero pronoun resolution as a machine reading comprehension task (Rajpurkar et al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the su"
2020.acl-main.482,W16-3612,0,0.0368814,"Missing"
2020.acl-main.482,D10-1062,0,0.0193741,"o pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recov"
2020.acl-main.482,N19-1423,0,0.547208,"as a dropped pronoun and what type the pronoun is. ZP resolution is solved as extractive reading comprehension (Rajpurkar et al., 2016), where each word space is taken as a query and its anaphoric mentions are treated as the answers. For non-ZP spaces where there is no corresponding anaphoric mentions, we assign the sentence beginning (span [0,0]) as the answer. Experiments on two benchmarks, OntoNotes 5.01 (ZP resolution) and BaiduZhdiao (Zhang et al., 2016) (ZP recovery), show that joint modeling gives us 1.5+ absolute F1-score gains for both tasks over our very strong baselines using BERT (Devlin et al., 2019). Our overall system gives an dramatic improvement of 3.5 F1 points over previous stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their"
2020.acl-main.482,P11-1081,0,0.551431,"Missing"
2020.acl-main.482,D10-1086,0,0.525685,"Missing"
2020.acl-main.482,P17-1010,0,0.395405,"over previous stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their task is closer to zero pronoun recovery. Most similar to our work, Liu et al. (2017) converted zero pronoun resolution as a machine reading comprehension task (Rajpurkar et al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the su"
2020.acl-main.482,P19-1441,0,0.0226198,"t al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the supervised data of several related tasks to achieve further improvements. In particular, Liu et al. (2019) utilize this framework to jointly solve GLUE tasks (Wang et al., 2019). But their experiments show that multitask learning does not help across all tasks. Our work takes a similar spirit, and our contribution is mainly on the zero pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery,"
2020.acl-main.482,C96-2137,0,0.0827163,"Missing"
2020.acl-main.482,D16-1264,0,0.655517,"rom the supervised data of both tasks. As the result, we enjoy the benefit of more supervised training data. To improve the robustness of heterogeneous training and introduce more supervision, we introduce zero pronoun detection, a common sub-task for both ZP resolution and recovery. Zero pronoun detection is a binaryclassification task aiming to detect whether a word space has a dropped pronoun. We consider ZP recovery as a sequence labeling task, regarding whether each word space has a dropped pronoun and what type the pronoun is. ZP resolution is solved as extractive reading comprehension (Rajpurkar et al., 2016), where each word space is taken as a query and its anaphoric mentions are treated as the answers. For non-ZP spaces where there is no corresponding anaphoric mentions, we assign the sentence beginning (span [0,0]) as the answer. Experiments on two benchmarks, OntoNotes 5.01 (ZP resolution) and BaiduZhdiao (Zhang et al., 2016) (ZP recovery), show that joint modeling gives us 1.5+ absolute F1-score gains for both tasks over our very strong baselines using BERT (Devlin et al., 2019). Our overall system gives an dramatic improvement of 3.5 F1 points over previous stateof-the-art results on both t"
2020.acl-main.482,N15-1052,0,0.211097,"Missing"
2020.acl-main.482,Q19-1016,0,0.0292498,"Missing"
2020.acl-main.482,C08-1097,0,0.354952,"Missing"
2020.acl-main.482,Q19-1014,1,0.87841,"Missing"
2020.acl-main.482,W16-4615,0,0.0407466,"Missing"
2020.acl-main.482,C18-1002,0,0.192729,"Missing"
2020.acl-main.482,N16-1113,0,0.219648,"t helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recovery is to restore any dropped pronouns f"
2020.acl-main.482,P13-1081,0,0.361222,"tion, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recovery is to restore an"
2020.acl-main.482,N19-1095,0,0.0207607,"Missing"
2020.acl-main.482,P15-2051,0,0.249019,"at multitask learning does not help across all tasks. Our work takes a similar spirit, and our contribution is mainly on the zero pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al.,"
2020.acl-main.482,D17-1135,0,0.16016,"Missing"
2020.acl-main.712,P17-2021,0,0.0210526,"lignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist tra"
2020.acl-main.712,P19-1080,0,0.0229762,"e girl wants the boy to go”, which conveys an opposite meaning to the AMR graph. In particular, this can be very likely if “the girl wants” appears much more frequent than “the boy wants” in the training corpus. This is a very important issue, because of its wide existence across many neural graph-to-text 7987 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may no"
2020.acl-main.712,W13-2322,0,0.0245618,"veness of our approach over a state-of-the-art baseline. Our code is available at http://github.com/ Soistesimmer/AMR-multiview. 1 boy ARG2 ARG1 eat-01 Above the Veil ARG0 followedBy lunch precededBy girl mod (a) beautiful Into Battle Aenir (b) Figure 1: (a) An AMR graph meaning “The boy wants the beautiful girl to eat lunch with him.”, and (b) A knowledge graph carrying the meaning “Above the Veil is an Australian novel and the sequel to Aenir. It was followed by Into the Battle.” Many text generation tasks take graph structures as their inputs, such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Knowledge Graph (KG) and database tables. For example, as shown in Figure 1(a), AMR-to-text generation is to generate a sentence that preserves the meaning of an input AMR graph, which is composed by a set of concepts (such as “boy” and “want-01”) and their relations (such as “:ARG0” and “:ARG1”). Similarly, as shown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks main"
2020.acl-main.712,P18-1026,0,0.0217616,"hown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to prod"
2020.acl-main.712,N19-1223,0,0.0333895,"ormation of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) exte"
2020.acl-main.712,N19-1366,0,0.0480077,"le relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction l"
2020.acl-main.712,W14-3348,0,0.0263306,"One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pourdamghani et al., 2014). We use this da"
2020.acl-main.712,W19-8652,0,0.0315152,"Missing"
2020.acl-main.712,W18-6539,0,0.033398,"Missing"
2020.acl-main.712,S16-1186,0,0.0222153,"to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN"
2020.acl-main.712,W17-3518,0,0.413529,"y” in Figure 1(a)) contains a pair of entities and their relation. As the next step, the alignments between graph nodes and target words are generated to ground this view into the target sentence for reconstruction. Our second view is the linearization of each input graph produced by depth-first graph traversal, and this view is reconstructed token-by-token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effective"
2020.acl-main.712,Q19-1019,0,0.023917,"tion is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucia"
2020.acl-main.712,N19-1235,0,0.0208537,"iew focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (201"
2020.acl-main.712,D19-6310,0,0.299588,"res, only the most frequent 20K are kept, while the rest are mapped into a special UNK feature.1 X i2[1..N ] p(yi |si ; ✓), (5) where ✓ represents all model parameters. R2 | (3) where W Q , W K and W R2 are model parameters, and dh denotes the encoder-state dimension. The encoder adopts L self-attention layers and H L = L (hL 1 . . . h|V |) represents the concatenated top-layer hidden states of the encoder, which will be used in attention-based decoding. 4 Multi-View Autoencoding Losses Figure 2 visualizes the training framework using our multi-view autoencoding losses, where the 1 Zhu et al. (2019) also mentions other (such as CNN-based or self-attention-based) alternatives to calculate ij . While the GPU memory consumption of these alternatives is a few times more than our baseline, ours actually shows a comparable performance. 7989 View 1: triple relations ARG2 ARG1 want-01 ARG1 ARG0 boy ARG2 ARG1 lunch ARG0 mod ARG0 ARG1 The boy wants the beautiful girl to eat lunch with him eat-01 ARG0 girl Encoder Attention Language modeling loss Decoder mod View 2: linearized graph beautiful want :ARG0 boy :ARG1 eat ( :ARG0 (girl :mod beautiful) :ARG1 lunch :ARG2 boy) Figure 2: The training framew"
2020.acl-main.712,P17-1089,0,0.0189482,". . . sN ) denotes the concatenated states for the target sentence (Equation 4), and the loss for reconstructing this view is defined as the negative log-likelihood for the linearized graph: X lauto2 = log p(xi |ti ; ✓), (15) i2[1..M ] (12) where [x] in the subscript represents choosing the x-th item from the corresponding vector. As the final step, the loss for reconstructing this view is defined as the negative log-likelihood of all target arcs E 0 (the grounded triples from E): X lauto1 = log p(yj , l|yi ) (13) (yj ,l,yi )2E 0 4.2 infer the original graph structure. Besides, previous work (Iyer et al., 2017; Konstas et al., 2017) has shown the effectiveness of generating linearized graphs as sequences for graph parsing, which also confirms our observation. Given a linearized graph represented as a sequence of tokens x1 , . . . , xM , where each token xi can be a graph node, a edge label or a inserted bracket, we adopt another standard Transformer decoder (SADecoderg ) to produce the sequence: Loss 2: Reconstructing Linearized Graphs with a Transformer Decoder As a supplement to our first loss for reconstructing the local information of each grounded triple, we introduce the second loss for predi"
2020.acl-main.712,P19-1236,1,0.815011,"or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for traini"
2020.acl-main.712,N19-1238,0,0.067091,"Missing"
2020.acl-main.712,P17-1014,0,0.415658,"and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “t"
2020.acl-main.712,D18-1183,0,0.0602147,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,D18-1264,0,0.0957235,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,W18-6501,0,0.143887,"oder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (recon"
2020.acl-main.712,W03-3017,0,0.220221,"al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003) to convert tree parsing into the prediction of transition actions, while we study reconstructing graphs, where there is no common parsing algorithm for all graph types. 7988 (Liu et al., 2018b) and sentiment analysis (Rei and Søgaard, 2019). Since input reconstruction is not intuitively related to these tasks, the autoencoding loss only serves as more training signals. Different from these efforts, we leverage autoencoding loss as a means to preserve input knowledge. Besides, we study reconstructing complex graphs, proposing a general multi-view approach for this goal. 3 Base: Structure-Aware"
2020.acl-main.712,P02-1040,0,0.106933,"ighly sensitive to the input order. One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pou"
2020.acl-main.712,D14-1048,0,0.178545,"rds of the entity in order to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder"
2020.acl-main.712,P17-1194,0,0.169993,"nal Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may not even exist. To make our approach general across different types of graphs, we propose to reconstruct different views of each input graph (rather than the original graph), where each view highlights one aspect of the graph and is easy to produce. Then through multi-task learning, the autoencoding losses of all views are back-propagated to the whole model so that th"
2020.acl-main.712,D19-1314,0,0.0725088,"a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and re"
2020.acl-main.712,P16-1162,0,0.042332,"than LDC2015E86, we may conclude that the problem of dropping input information may not be effectively reduced by simply adding more supervised data, and as a result, our approach can still be effective on a larger dataset. This conclusion can also be confirmed by comparing the gains of our approach on both AMR datasets regarding BLEU score (2.3 vs 2.5). 5.9 Main Results on WebNLG Table 6 shows the comparison of our results with previous results on the WebNLG testset. ADAPT (Gardent et al., 2017) is based on the standard encoder-decoder architecture (Cho et al., 2014) with byte pair encoding (Sennrich et al., 2016), and it was the best system of the challenge. GCNEC (Marcheggiani and Perez-Beltrachini, 2018) is a recent model using a graph convolution network (Kipf and Welling, 2017) for encoding KGs. Our baseline shows a comparable performance with the previous state of the art. Based on this baseline, applying either loss leads to a significant improvement, and their combination brings a gain of more than 2 BLEU points. Although the baseline already achieves a very high BLEU score, yet the gains on this task are still comparable with those on AMR-to-text generation. This observation may imply that the"
2020.acl-main.712,Q19-1002,1,0.855603,"riments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple record"
2020.acl-main.712,P18-1150,1,0.905082,", KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentence"
2020.acl-main.712,N18-1106,0,0.0225874,"Missing"
2020.acl-main.712,P18-1151,0,0.0378542,"token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target"
2020.acl-main.712,D17-1129,0,0.0198313,"entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN , the representation"
2020.acl-main.712,2020.tacl-1.2,0,0.0576841,"orts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, which conveys an opposite meaning to the AM"
2020.acl-main.712,D18-1509,0,0.0228232,"struct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003"
2020.acl-main.712,D17-1239,0,0.0254359,"2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple records that fit into several fields.We study a more challenging topic on how to reconstruct a complex graph structure rather than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word."
2020.acl-main.712,P17-1065,0,0.0213542,"ur model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based alg"
2020.acl-main.712,D18-1112,1,0.768306,"hts the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wisem"
2020.acl-main.712,D19-1548,0,0.667496,"ns (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, whic"
2020.emnlp-main.537,2021.ccl-1.108,0,0.186468,"Missing"
2020.emnlp-main.537,P14-5010,0,0.00441807,"Missing"
2020.emnlp-main.537,D18-1191,0,0.090127,"Missing"
2020.emnlp-main.537,D19-1605,0,0.0631344,"ng. One important factor that contributes to this difficulty is coreference and information omission, where mention is dropped or replaced by a pronoun for simplicity. These phenomena dramatically introduce the requirements for long-distance reasoning, as they frequently occurred in our daily conversations, especially in pro-drop languages like Chinese and Japanese. To tackle these problems, sentence rewriting was introduced to ease the burden of dialogue models by simplifying the multi-turn dialogue modeling into a single-turn problem. Several approaches (Su et al., 2019; Zhang et al., 2019; Elgohary et al., 2019) have been proposed to address the rewriting task. Conceptually, these models follow the conventional encoder-decoder architecture that first encodes the dialogue context into a distributional representation and then decodes it to the rewritten utterance. Their decoders mainly use global attention methods that attends to all words in the dialogue context without prior focus, which may result in inaccurate concentration on some dispensable words. We also observe that the accuracy of their models significantly decreases when working on long dialogue contexts. This observation is expected since i"
2020.emnlp-main.537,P19-1369,0,0.140627,"Missing"
2020.emnlp-main.537,P17-1061,0,0.0246771,"ho did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous stateof-the-art systems. Utterance 2 Utterance 3 Utterance 30 需要粤语 (I may need Cantonese.) 粤语ARG0 是普通话ARG1 吗 (Is Cantonese Mandarin ?) 不算predicate 吧 (Maybe Not.) 粤语不算普通话吧 (Cantonese may be not Mandarin.) Table 1: One example of multi-turn dialogue. The goal of dialogue rewriting is to rewrite utterance 3 into 30 . Introduction Recent research (Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2017; Zhao et al., 2017; Shao et al., 2017) on dialogue generation has been achieving impressive progress for making singleturn responses, while producing coherent multiturn replies still remains extremely challenging. One important factor that contributes to this difficulty is coreference and information omission, where mention is dropped or replaced by a pronoun for simplicity. These phenomena dramatically introduce the requirements for long-distance reasoning, as they frequently occurred in our daily conversations, especially in pro-drop languages like Chinese and Japanese. To tackle these problems, sentence rewr"
2020.emnlp-main.92,2020.acl-main.640,0,0.352246,"raph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decoders is that they tend to produce fluent sentences that may not retain the meaning of input AMRs. We investigate enhancing AMR-to-text decoding by integrating online back-parsing, simultaneously"
2020.emnlp-main.92,N19-1223,0,0.0333348,"Missing"
2020.emnlp-main.92,N19-1366,0,0.0197127,"any applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words"
2020.emnlp-main.92,W14-3348,0,0.0128671,". We tune these hyperparameters on the LDC2015E86 development 4 https://nlp.stanford.edu/software/tokenizer.shtml https://github.com/rsennrich/subword-nmt 6 We do not choose their best model (G-Trans-SA) due to its large GPU memory consumption, and its performance is actually comparable with G-Trans-F in our experiments. 1210 5 Model Figure 3: BLEU scores on the LDC2015E86 devset against different hyperparameter values. set and use the selected values for testing7 . Model Evaluation. We set the decoding beam size as 5 and take BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) as automatic evaluation metrics. We also employ human evaluation to assess the semantic faithfulness and generation fluency of compared methods by randomly selecting 50 AMR graphs for comparison. Three people familiar with AMR are asked to score the generation quality with regard to three aspects — concept preservation rate, relation preservation rate and fluency (on a scale of [0, 5]). Details about the criteria are: • Concept preservation rate assesses to what extent the concepts in input AMR graphs are involved in generated sentences. • Relation preservation rate measures to what extent th"
2020.emnlp-main.92,N16-1087,0,0.0665669,"es little attention to the AMR node “local” and “problem” during text generation. In contrast, our system gives a more accurate alignment to the relevant AMR nodes in decoding. In the second example, the baseline model incorrectly positions the terms “doctor”, “see” and “worse cases” while our approach generates a more natural sentence. This can be attributed to the edge prediction task, which can inform the decoder to preserve the relation that “doctor” is the subject of “see” and “worse cases” is the object. 5 Related Work Early studies on AMR-to-text generation rely on statistical methods. Flanigan et al. (2016) convert input AMR graphs to trees by splitting re-entrances, before translating these trees into target sentences with a tree-to-string transducer; Pourdamghani et al. (2016) apply a phrase-based MT system on linearized AMRs; Song et al. (2017) design a synchronous node replacement grammar to parse input AMRs while generating target sentences. These approaches show comparable or better results than early neural models (Konstas et al., 2017). However, recent neural approaches (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020; Mager et al., 2020) have demonstrated the s"
2020.emnlp-main.92,W17-3518,0,0.0865946,"he task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performance"
2020.emnlp-main.92,Q19-1019,0,0.143331,". AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model,"
2020.emnlp-main.92,N19-1235,0,0.0198829,"machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one li"
2020.emnlp-main.92,D18-1086,0,0.016656,"possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on lineariz"
2020.emnlp-main.92,N19-1238,0,0.0324426,"Missing"
2020.emnlp-main.92,P17-1014,0,0.505826,"riments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer. 1 Figure 1: An example AMR graph meaning “The police could help the victim.” Introduction Abstract meaning representation (AMR) (Banarescu et al., 2013) is a semantic graph representation that abstracts meaning away from a sentence. Figure 1 shows an AMR graph, where the nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attra"
2020.emnlp-main.92,C18-1101,0,0.0607051,"he nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas"
2020.emnlp-main.92,C16-1291,0,0.0221542,"At each decoding time step, the proposed decoder takes the encoder states as inputs and generates a new word (as in Section 2.2), together with its corresponding AMR node (Section 3.1) and its outgoing edges (Section 3.2), These predictions are then used inputs to calculate the next state (Section 3.3). 3.1 Node Prediction We first equip a standard decoder with the ability to make word-to-node alignments while generating target words. Making alignments can be formalized as a matching problem, which aims to find the most relevant AMR graph node for each target word. Inspired by previous work (Liu et al., 2016; Mi et al., 2016), we solve the matching problem by supervising the word-to-node attention scores given by the Transformer decoder. In order to deal with words without alignments, we introduce a NULL node v∅ into the input AMR graph (as shown in 1208 Figure 2) and align such words to it.3 More specifically, at each decoding step t, our Transformer decoder first calculates the top decoder layer word-to-node attention distribution βt0 = 0 , β 0 , ..., β 0 ] (Eq 3 and Eq 4) after taking [βt0 t1 tN L L L the encoder states H L = [hL 0 , h1 , h2 , . . . , hN ] together with the previously generate"
2020.emnlp-main.92,2020.acl-main.167,0,0.0321795,"Missing"
2020.emnlp-main.92,D16-1249,0,0.167105,"time step, the proposed decoder takes the encoder states as inputs and generates a new word (as in Section 2.2), together with its corresponding AMR node (Section 3.1) and its outgoing edges (Section 3.2), These predictions are then used inputs to calculate the next state (Section 3.3). 3.1 Node Prediction We first equip a standard decoder with the ability to make word-to-node alignments while generating target words. Making alignments can be formalized as a matching problem, which aims to find the most relevant AMR graph node for each target word. Inspired by previous work (Liu et al., 2016; Mi et al., 2016), we solve the matching problem by supervising the word-to-node attention scores given by the Transformer decoder. In order to deal with words without alignments, we introduce a NULL node v∅ into the input AMR graph (as shown in 1208 Figure 2) and align such words to it.3 More specifically, at each decoding step t, our Transformer decoder first calculates the top decoder layer word-to-node attention distribution βt0 = 0 , β 0 , ..., β 0 ] (Eq 3 and Eq 4) after taking [βt0 t1 tN L L L the encoder states H L = [hL 0 , h1 , h2 , . . . , hN ] together with the previously generated sequence 0 and h"
2020.emnlp-main.92,P02-1040,0,0.107302,"Our models are trained for 500K steps on a single 2080Ti GPU. We tune these hyperparameters on the LDC2015E86 development 4 https://nlp.stanford.edu/software/tokenizer.shtml https://github.com/rsennrich/subword-nmt 6 We do not choose their best model (G-Trans-SA) due to its large GPU memory consumption, and its performance is actually comparable with G-Trans-F in our experiments. 1210 5 Model Figure 3: BLEU scores on the LDC2015E86 devset against different hyperparameter values. set and use the selected values for testing7 . Model Evaluation. We set the decoding beam size as 5 and take BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) as automatic evaluation metrics. We also employ human evaluation to assess the semantic faithfulness and generation fluency of compared methods by randomly selecting 50 AMR graphs for comparison. Three people familiar with AMR are asked to score the generation quality with regard to three aspects — concept preservation rate, relation preservation rate and fluency (on a scale of [0, 5]). Details about the criteria are: • Concept preservation rate assesses to what extent the concepts in input AMR graphs are involved in generated s"
2020.emnlp-main.92,W17-4770,0,0.0385168,"Missing"
2020.emnlp-main.92,Q19-1002,1,0.855378,"at abstracts meaning away from a sentence. Figure 1 shows an AMR graph, where the nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 20"
2020.emnlp-main.92,P17-2002,1,0.826348,"s “doctor”, “see” and “worse cases” while our approach generates a more natural sentence. This can be attributed to the edge prediction task, which can inform the decoder to preserve the relation that “doctor” is the subject of “see” and “worse cases” is the object. 5 Related Work Early studies on AMR-to-text generation rely on statistical methods. Flanigan et al. (2016) convert input AMR graphs to trees by splitting re-entrances, before translating these trees into target sentences with a tree-to-string transducer; Pourdamghani et al. (2016) apply a phrase-based MT system on linearized AMRs; Song et al. (2017) design a synchronous node replacement grammar to parse input AMRs while generating target sentences. These approaches show comparable or better results than early neural models (Konstas et al., 2017). However, recent neural approaches (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020; Mager et al., 2020) have demonstrated the state-of-the-art performances thanks to the use of contextualized embeddings. Related work on NMT studies back-translation loss (Sennrich et al., 2016; Tu et al., 2017) by translating the target reference back into the source text (reconstruction"
2020.emnlp-main.92,P18-1150,1,0.920296,"ld help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as"
2020.emnlp-main.92,D14-1048,0,0.0282584,"periments We conduct experiments on two benchmark AMRto-text generation datasets, including LDC2015E86 and LDC2017T10. These two datasets contain 16,833 and 36,521 training examples, respectively, and share a common set of 1,368 development and 1,371 test instances. 4.1 Experimental Settings Data preprocessing. Following previous work (Song et al., 2018; Zhu et al., 2019), we take a standard simplifier (Konstas et al., 2017) to preprocess AMR graphs, adopting the Stanford tokenizer4 and Subword Tool5 to segment text into subword units. The node-to-word alignments are generated by ISI aligner (Pourdamghani et al., 2014). We then project the source AMR graph onto the target sentence according to such alignments. For node prediction, the attention distributions are normalized, but the alignment scores generated by the ISI aligner are unnormalized hard 0/1 values. To enable cross entropy loss, we follow previous work (Mi et al., 2016) to normalize the goldstandard alignment scores. Hyperparameters. We choose the feature-based model6 of Zhu et al. (2019) as our baseline (GTrans-F-Ours). Also following their settings, both the encoder and decoder have 6 layers, with each layer having 8 attention heads. The sizes"
2020.emnlp-main.92,2020.tacl-1.2,0,0.529734,"oblems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decoders is that they tend to produce fluent sentences that may not retain the meaning of input AMRs. We investigate enhancing AMR-to-text decoding by integrating online back-parsing, simultaneously predicting a projec"
2020.emnlp-main.92,W16-6603,0,0.188465,"ding. In the second example, the baseline model incorrectly positions the terms “doctor”, “see” and “worse cases” while our approach generates a more natural sentence. This can be attributed to the edge prediction task, which can inform the decoder to preserve the relation that “doctor” is the subject of “see” and “worse cases” is the object. 5 Related Work Early studies on AMR-to-text generation rely on statistical methods. Flanigan et al. (2016) convert input AMR graphs to trees by splitting re-entrances, before translating these trees into target sentences with a tree-to-string transducer; Pourdamghani et al. (2016) apply a phrase-based MT system on linearized AMRs; Song et al. (2017) design a synchronous node replacement grammar to parse input AMRs while generating target sentences. These approaches show comparable or better results than early neural models (Konstas et al., 2017). However, recent neural approaches (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020; Mager et al., 2020) have demonstrated the state-of-the-art performances thanks to the use of contextualized embeddings. Related work on NMT studies back-translation loss (Sennrich et al., 2016; Tu et al., 2017) by tran"
2020.emnlp-main.92,D18-1509,0,0.15256,"of previous decoder hidden states, respectively. In contrast to the baseline in Eq 3, at time t+1, the hidden state of the first decoder layer is calculated as: sˆ1t+1 = SAN(s01 , ..., s0t , ~yt , ~vt , ~et ), c1t+1 = AN(ˆ s1t+1 , H L ), s1t+1 = (16) FF(c1t+1 , sˆ1t+1 ), where the definition of H L , SAN, AN, FF and [s01 , . . . , s0t ] are the same as Eq 3. ~v0 and ~e0 (as shown in Figure 2) are defined as zero vectors. The hidden states of upper decoder layers ([s2t+1 , ..., sL t+1 ]) are updated in the same way as Eq 3. Following previous work on syntactic text generation (Wu et al., 2017; Wang et al., 2018), we use gold AMR nodes and outgoing edges as inputs for training, while we take automatic predictions for decoding. 3.4 Training Objective The overall training objective is: `total = `std + λ1 `node + λ2 `label , (17) where λ1 and λ2 are weighting hyper-parameters for `node and `label , respectively. Model BLEU Meteor G-Trans-F-Ours 30.20 35.23 Node Prediction MSE Node Prediction CE 30.66 30.85 35.60 35.71 Edge Prediction share Edge Prediction independent 31.19 31.13 35.75 35.69 Table 1: BLEU and Meteor scores on the LDC2015E86 devset under different model settings. 4 Experiments We conduct e"
2020.emnlp-main.92,prasad-etal-2008-penn,0,0.00704984,"the back-parsing mechanism. With regard to the generation fluency, our model also gives better results than baseline. The main reason is that the relations between concepts such as subjectpredicate relation and modified relation are helpful Model BLEU Meteor Baseline 30.15 35.36 + Node Prediction + Node Prediction (Int.) 30.49 30.72 35.66 35.94 + Edge Prediction + Edge Prediction (Int.) 30.80 31.07 35.71 35.87 + Both Prediction + Both Prediction (Int.) 30.96 31.48 35.92 36.15 Table 5: Ablation study on LDC2015E86 test set. for generating fluency sentences. Apart from that, we study discourse (Prasad et al., 2008) relations, which are essential for generating a good sentence with correct meaning. Specifically, we consider 4 common discourse relations (“Cause”, “Contrast”, “Condition”, “Coordinating”). For each type of discourse, we randomly select 50 examples from the test set and ask 3 linguistic experts to calculate the discourse preservation accuracy by checking if the generated sentence preserves such information. Table 4 gives discourse preservation accuracy results of the baseline and our model, respectively. The baseline already performs well, which is likely because discourse information can so"
2020.emnlp-main.92,D17-1239,0,0.0603656,"Missing"
2020.emnlp-main.92,D19-1314,0,0.457003,"Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decode"
2020.emnlp-main.92,K17-1045,0,0.0260422,"s an AMR graph, where the nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-ba"
2020.emnlp-main.92,D19-1548,0,0.399423,"bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decoders is that they tend to produce fluent sentences that may not retain the meaning of input AMRs. We investigate enhancing AMR-to-text decoding by integrating online back-pars"
2020.inlg-1.17,P18-1026,0,0.0125692,"ncy tree captures finer-grained word-level relations. The relations “poss” and “amod” (possessive pronoun and adjectival modifier respectively) in this example reveal the syntactic structure within the phrase “my biggest complaint”. As a different way of encoding the input, we consider a sentence, along with its dependency and semantic role annotations, as a graph. We then use a Graph Neural Network (GNN) (Marcheggiani and Titov, 2017) to encode the sentence, noting the reported success of GNNs in representing syntactic and semantic structures (Marcheggiani and Titov, 2017; Song et al., 2018; Beck et al., 2018; Xu et al., 2018b). For the overall architecture, the proposed GNN layers can be stacked onto (or replace) the encoder of an existing system. Previous advances on style transfer were achieved by new designs of the decoder or the learning framework (Shen et al., 2017; Fu et al., 2018). Our approach can be considered to be orthogonal to these previous designs. Preliminary experiments with available text style transfer models on benchmark datasets validate the utility of our input-encoding approach for preserving input semantic information when compared with a strong baseline (Gong et al., 2019)"
2020.inlg-1.17,N19-1320,1,0.841652,"aspect (e.g., sentiment) and retain other contentrelated words (Xu et al., 2018a; Li et al., 2018; Wu et al., 2019; Madaan et al., 2020). However, their success was limited to specific situations where the style words are explicit. When the negative sentiment is expressed implicitly, as in “The only thing I was offered was a free dessert!!!” this approach cannot have the desired effect. A second direction to address the semantic loss has been the use of back-translation (Prabhumoye et al., 2018; Lample et al., 2019; He et al., 2020) and/or reinforcement learning to preserve the input content (Gong et al., 2019; Luo et al., 2019). Generally, these techniques involve a more complex model training, adding another layer of difficulty to obtain a strong style transfer model with robust performance. Introduction poss amod { My nsubj advmod ccomp nsubj nmod case poss biggest complaint , {however , is {{what happened {with our meal . { Text style transfer aims at rephrasing an input sentence as an output sentence in a target style (e.g. sentiment change from negative to positive), while preserving the original content. The utility of text style transfer has been shown in applications such as personalized r"
2020.inlg-1.17,2020.acl-main.603,1,0.790783,"rage of the three evaluation metrics to account for the fact that they may not be in the same scale. r = ↵rsem + rstyle + ⌘rlm , Baseline The baseline we consider for this study is a recent model (Gong et al., 2019) based on a generatordiscriminator framework. The generator transfers sentences from the source style to the target style and is in a tight feedback loop with a set of dis(1) where ↵, and are weighting coefficients. Reinforcement learning (RL). RL trains the generator with the feedback received from the discriminators (scores given by the discriminators to its generated sentences) (Gong et al., 2020). Under the RL framework, generating a target sentence 114 Semantic 0.919 0.831 0.864 0.896 CA MD RL GT Negative-to-Positive Style Overall Perplexity 0.842 0.440 119 0.972 0.448 104 0.964 0.456 126 0.943 0.459 126 Semantic 0.914 0.824 0.842 0.863 Positive-to-Negative Style Overall Perplexity 0.821 0.433 148 0.864 0.422 92 0.952 0.447 101 0.974 0.458 99 Table 1: Model performance of style transfer on Yelp dataset (GT is RL with the proposed enhancement). was formulated as making a sequence of actions, where an action is a token produced at a decoding step. Taking the decoding step t as an examp"
2020.inlg-1.17,P19-1041,0,0.0178706,"aint , {however , is {{what happened {with our meal . { Text style transfer aims at rephrasing an input sentence as an output sentence in a target style (e.g. sentiment change from negative to positive), while preserving the original content. The utility of text style transfer has been shown in applications such as personalized response generation (Zhou et al., 2017; Niu and Bansal, 2018) and poetry generation (Yang et al., 2018a). In particular, unsupervised style transfer has been extensively explored due to a lack of parallel corpora (Hu et al., 2017; Shen et al., 2017; Yang et al., 2018b; John et al., 2019). Most previous efforts on unsupervised text style transfer have relied on separating the content from the style of input texts. This was achieved via a transfer model with multiple decoders (Fu et al., 2018) or extra auxiliary losses (John et al., 2019) to learn the disentangled representation vectors for content and style respectively. The content vector was later combined with the vector of the desired style to produce the output. While these prior studies have successfully demonstrated the capability to adapt input texts { 1 DISC { Text style transfer aims to change an input sentence to an"
2020.inlg-1.17,N18-1169,0,0.0567718,"Missing"
2020.inlg-1.17,D15-1166,0,0.0465181,"the details of our implementation in the supplementary material. 2 criminators that evaluate the quality of the transferred sentences. The reported results revealed its competitive style transfer performance on available benchmark datasets. Generator. The generator is a typical seq2seq model with a sequence encoder and a sequence decoder (Bahdanau et al., 2015). The encoder adopts a Gated Recurrent Unit (GRU) to take in an input sentence with words {x1 , . . . , xN } and produce the encoder states {h1 , ..., hN } sequentially. The sequence decoder is another GRU with the attention mechanism (Luong et al., 2015). At each time step t, the decoder updates its hidden state st with the target token generated at time t 1. It then predicts the current token yt using the current decoder state and the weighted sum of the encoder states, where the weights are produced by the attention mechanism. Discriminators. Three discriminators are included, each serving to judge one aspect of the quality of the generated target sentences from among meaning preservation, transfer strength, and fluency. The meaning preservation is evaluated using the word mover’s distance (Kusner et al., 2015), which calculates the similar"
2020.inlg-1.17,2020.acl-main.169,0,0.0264755,"Missing"
2020.inlg-1.17,P14-5010,0,0.0024368,"a GNN encoder allows a direct interaction between distant words that are semantically or syntactically related (Zhang et al., 2018), thereby serving the style transfer process. 4 Experiments Dataset. We focus on the task of sentiment transfer, retaining the setting of the Yelp dataset as (Shen et al., 2017). The dataset contains 176, 878 negative and 267, 314 positive sentences for training, 25, 278 negative and 38, 205 positive sentences for development, and 50, 278 negative and 76, 392 positive sentences for testing. We construct syntacticsemantic graphs with the Stanford dependency parser (Manning et al., 2014) and the semantic role labeler of the AllenNLP (Gardner et al., 2018). Baselines. We compared our model (GT) with three state-of-the-art models for text style transfer: (1) Reinforcement learning based model (RL). RL is the baseline summarized in Section 2. (2) Cross alignment model (CA). CA transfers the text style by combining content representation with style information (Shen et al., 2017). (3) Multi-decoder model (MD). MD disentangles content from style, and adopts multiple decoders to produce outputs of various styles (Fu et al., 2018). Implementation. We include the implementation detai"
2020.inlg-1.17,D17-1159,0,0.0330487,"ectly capture the key information in a sentence, such as its subject and object. In the example of Figure 1, “my biggest complaint” is identified as the subject of “is”. Meanwhile, a dependency tree captures finer-grained word-level relations. The relations “poss” and “amod” (possessive pronoun and adjectival modifier respectively) in this example reveal the syntactic structure within the phrase “my biggest complaint”. As a different way of encoding the input, we consider a sentence, along with its dependency and semantic role annotations, as a graph. We then use a Graph Neural Network (GNN) (Marcheggiani and Titov, 2017) to encode the sentence, noting the reported success of GNNs in representing syntactic and semantic structures (Marcheggiani and Titov, 2017; Song et al., 2018; Beck et al., 2018; Xu et al., 2018b). For the overall architecture, the proposed GNN layers can be stacked onto (or replace) the encoder of an existing system. Previous advances on style transfer were achieved by new designs of the decoder or the learning framework (Shen et al., 2017; Fu et al., 2018). Our approach can be considered to be orthogonal to these previous designs. Preliminary experiments with available text style transfer m"
2020.inlg-1.17,Q18-1027,0,0.0147969,"echniques involve a more complex model training, adding another layer of difficulty to obtain a strong style transfer model with robust performance. Introduction poss amod { My nsubj advmod ccomp nsubj nmod case poss biggest complaint , {however , is {{what happened {with our meal . { Text style transfer aims at rephrasing an input sentence as an output sentence in a target style (e.g. sentiment change from negative to positive), while preserving the original content. The utility of text style transfer has been shown in applications such as personalized response generation (Zhou et al., 2017; Niu and Bansal, 2018) and poetry generation (Yang et al., 2018a). In particular, unsupervised style transfer has been extensively explored due to a lack of parallel corpora (Hu et al., 2017; Shen et al., 2017; Yang et al., 2018b; John et al., 2019). Most previous efforts on unsupervised text style transfer have relied on separating the content from the style of input texts. This was achieved via a transfer model with multiple decoders (Fu et al., 2018) or extra auxiliary losses (John et al., 2019) to learn the disentangled representation vectors for content and style respectively. The content vector was later comb"
2020.inlg-1.17,P18-1080,0,0.0214481,"becomes irrelevant. To alleviate this problem, some studies sought to explicitly replace the words related to the stylistic aspect (e.g., sentiment) and retain other contentrelated words (Xu et al., 2018a; Li et al., 2018; Wu et al., 2019; Madaan et al., 2020). However, their success was limited to specific situations where the style words are explicit. When the negative sentiment is expressed implicitly, as in “The only thing I was offered was a free dessert!!!” this approach cannot have the desired effect. A second direction to address the semantic loss has been the use of back-translation (Prabhumoye et al., 2018; Lample et al., 2019; He et al., 2020) and/or reinforcement learning to preserve the input content (Gong et al., 2019; Luo et al., 2019). Generally, these techniques involve a more complex model training, adding another layer of difficulty to obtain a strong style transfer model with robust performance. Introduction poss amod { My nsubj advmod ccomp nsubj nmod case poss biggest complaint , {however , is {{what happened {with our meal . { Text style transfer aims at rephrasing an input sentence as an output sentence in a target style (e.g. sentiment change from negative to positive), while pre"
2020.inlg-1.17,P18-1150,1,0.835976,"eanwhile, a dependency tree captures finer-grained word-level relations. The relations “poss” and “amod” (possessive pronoun and adjectival modifier respectively) in this example reveal the syntactic structure within the phrase “my biggest complaint”. As a different way of encoding the input, we consider a sentence, along with its dependency and semantic role annotations, as a graph. We then use a Graph Neural Network (GNN) (Marcheggiani and Titov, 2017) to encode the sentence, noting the reported success of GNNs in representing syntactic and semantic structures (Marcheggiani and Titov, 2017; Song et al., 2018; Beck et al., 2018; Xu et al., 2018b). For the overall architecture, the proposed GNN layers can be stacked onto (or replace) the encoder of an existing system. Previous advances on style transfer were achieved by new designs of the decoder or the learning framework (Shen et al., 2017; Fu et al., 2018). Our approach can be considered to be orthogonal to these previous designs. Preliminary experiments with available text style transfer models on benchmark datasets validate the utility of our input-encoding approach for preserving input semantic information when compared with a strong baseline"
2020.inlg-1.17,P19-1482,0,0.0137487,"illinois.edu lfsong@tencent.com spbhat2@illinois.edu Abstract to the desired style, the proposed approaches suffer from a significant loss of semantic content. For instance, when a model takes as input “The lounge is very outdated” to generate “The food is delicious”, where the key information of the input (The lounge) is missing in the output, the rendering of the transfer becomes irrelevant. To alleviate this problem, some studies sought to explicitly replace the words related to the stylistic aspect (e.g., sentiment) and retain other contentrelated words (Xu et al., 2018a; Li et al., 2018; Wu et al., 2019; Madaan et al., 2020). However, their success was limited to specific situations where the style words are explicit. When the negative sentiment is expressed implicitly, as in “The only thing I was offered was a free dessert!!!” this approach cannot have the desired effect. A second direction to address the semantic loss has been the use of back-translation (Prabhumoye et al., 2018; Lample et al., 2019; He et al., 2020) and/or reinforcement learning to preserve the input content (Gong et al., 2019; Luo et al., 2019). Generally, these techniques involve a more complex model training, adding an"
2020.inlg-1.17,P18-1090,0,0.064997,"Missing"
2020.inlg-1.17,D18-1110,0,0.0990536,"t AI Lab, Bellevue, WA, USA hgong6@illinois.edu lfsong@tencent.com spbhat2@illinois.edu Abstract to the desired style, the proposed approaches suffer from a significant loss of semantic content. For instance, when a model takes as input “The lounge is very outdated” to generate “The food is delicious”, where the key information of the input (The lounge) is missing in the output, the rendering of the transfer becomes irrelevant. To alleviate this problem, some studies sought to explicitly replace the words related to the stylistic aspect (e.g., sentiment) and retain other contentrelated words (Xu et al., 2018a; Li et al., 2018; Wu et al., 2019; Madaan et al., 2020). However, their success was limited to specific situations where the style words are explicit. When the negative sentiment is expressed implicitly, as in “The only thing I was offered was a free dessert!!!” this approach cannot have the desired effect. A second direction to address the semantic loss has been the use of back-translation (Prabhumoye et al., 2018; Lample et al., 2019; He et al., 2020) and/or reinforcement learning to preserve the input content (Gong et al., 2019; Luo et al., 2019). Generally, these techniques involve a mor"
2020.inlg-1.17,D18-1430,0,0.0227023,"ng, adding another layer of difficulty to obtain a strong style transfer model with robust performance. Introduction poss amod { My nsubj advmod ccomp nsubj nmod case poss biggest complaint , {however , is {{what happened {with our meal . { Text style transfer aims at rephrasing an input sentence as an output sentence in a target style (e.g. sentiment change from negative to positive), while preserving the original content. The utility of text style transfer has been shown in applications such as personalized response generation (Zhou et al., 2017; Niu and Bansal, 2018) and poetry generation (Yang et al., 2018a). In particular, unsupervised style transfer has been extensively explored due to a lack of parallel corpora (Hu et al., 2017; Shen et al., 2017; Yang et al., 2018b; John et al., 2019). Most previous efforts on unsupervised text style transfer have relied on separating the content from the style of input texts. This was achieved via a transfer model with multiple decoders (Fu et al., 2018) or extra auxiliary losses (John et al., 2019) to learn the disentangled representation vectors for content and style respectively. The content vector was later combined with the vector of the desired style"
2020.inlg-1.17,P18-1030,1,0.840276,"aseline for negative-to-positive transfer, while still outperforming the other models. In particular, GT shows a largely improved semantic score compared to RL on both tasks. In terms of perplexity, GT is comparable to RL. In the encoding stage, a sequence encoder such as a GRU network only allows information to be propagated sequentially within a sentence. Because of this, the encoding process could result in an information loss when long-range dependencies are present. Conversely, a GNN encoder allows a direct interaction between distant words that are semantically or syntactically related (Zhang et al., 2018), thereby serving the style transfer process. 4 Experiments Dataset. We focus on the task of sentiment transfer, retaining the setting of the Yelp dataset as (Shen et al., 2017). The dataset contains 176, 878 negative and 267, 314 positive sentences for training, 25, 278 negative and 38, 205 positive sentences for development, and 50, 278 negative and 76, 392 positive sentences for testing. We construct syntacticsemantic graphs with the Stanford dependency parser (Manning et al., 2014) and the semantic role labeler of the AllenNLP (Gardner et al., 2018). Baselines. We compared our model (GT) w"
2021.acl-demo.1,C18-1139,0,0.0140491,"d the URL is available on the web page.4 The text matching API is used to calculate the similarity between a pair of sentences. Similar to the text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsin"
2021.acl-demo.1,P81-1022,0,0.278718,"lar expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level. For phraselevel segmentation, some phrases (especially noun phrases) may contained as a unit. An unsupervised algorithm is implemented in TexSmart for both English and Chinese word segmentation. We ch"
2021.acl-demo.1,U15-1010,0,0.0276516,"TTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the"
2021.acl-demo.1,W18-2501,0,0.0662142,"Missing"
2021.acl-demo.1,W09-3302,0,0.0687657,"Missing"
2021.acl-demo.1,C92-2082,0,0.277702,"de web search (e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster"
2021.acl-demo.1,C10-3004,0,0.154401,"Missing"
2021.acl-demo.1,P08-1067,0,0.029343,"ven by TexSmart for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en"
2021.acl-demo.1,D14-1082,0,0.0308477,"rt for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offl"
2021.acl-demo.1,D18-1536,0,0.0331353,"Missing"
2021.acl-demo.1,P18-1249,0,0.0167912,"he text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsing) tries to assign role labels to words or phrases in a sentence. TexSmart takes a sequence labeling model with BERT as the text encoder for sema"
2021.acl-demo.1,2021.findings-emnlp.18,1,0.731087,"Missing"
2021.acl-demo.1,P17-1152,0,0.0965523,"Missing"
2021.acl-demo.1,C16-1111,0,0.0255628,"HTTP API and the SDK may be slightly different, because the HTTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms fo"
2021.acl-demo.1,2021.naacl-main.116,1,0.743751,"entation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will be added in v0.3.0. Programming langu"
2021.acl-demo.1,W96-0213,0,0.664796,"ime was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will"
2021.acl-demo.1,C18-1166,0,0.0468114,"s a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK)"
2021.acl-demo.1,W03-0419,0,0.678216,"Missing"
2021.acl-demo.1,D15-1135,1,0.740722,"ient. In this sense, both methods are general in practice. ture. As a result, applications using these tools have to implement deep semantic representation by themselves. Some NLP toolkits make use of regular expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level."
2021.acl-demo.1,W02-0109,0,0.557609,"Missing"
2021.acl-demo.1,C10-1112,1,0.519815,"rocedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-grained NER, semantic"
2021.acl-demo.1,P14-5010,0,0.00644413,"Missing"
2021.acl-demo.1,N18-2028,1,0.809382,"uring the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-gra"
2021.acl-demo.1,J93-2004,0,0.0740651,"ng a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the RoBERTa-CRF and Flair models. As we found RoBERTa-CRF performs better on the Chinese dataset while Flair is better on the English dataset, we report results of RoBERTa-CRF for Chinese and Flair for English in our experiments. Constituency Parsing We conduct parsing experiments on both English and Chinese datasets. For English task, we use WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for th"
2021.acl-demo.1,W13-3516,0,0.0334885,"e WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for the training, development and test sets. The network parameters of our model are initialized using RoBERTa. The batch size is set to 32 and the learning rate is 5×10−5 . Text Matching Two text matching algorithms are evaluated: ESIM and Linkage. The datasets used in evaluating English text matching are MRPC6 and QUORA7 . For Chinese text matching, four datasets are involved: LCQMC (Liu et al., 2018b), AFQMC (Xu et al., 2020), BQ_CORPUS (Chen et al., 2018), and PAWSzh (Zhang et al., 2019). We evaluate the quality FGNER Base Hybrid 45.9 53.8 Table 1: Semantic"
2021.acl-demo.1,P11-1116,1,0.692266,"(e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally"
2021.acl-demo.1,P13-4009,0,0.0710732,"Missing"
2021.acl-demo.1,N19-1131,0,0.045871,"Missing"
2021.acl-long.324,2020.crac-1.4,0,0.157332,"nly named entities and date entities are considered, and they do not consider merging non-identical nodes (e.g., “Bill” and “he” in Figure 1) that are also frequent in reallife situation. Subsequent work considers more 4204 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4204–4214 August 1–6, 2021. ©2021 Association for Computational Linguistics co-reference cases by either manually annotating AMR coreference information (O’Gorman et al., 2018) or taking a pipeline system (Anikina et al., 2020) consisting of a textual coreference resolution model (Lee et al., 2018) and an AMR-to-text aligner (Flanigan et al., 2014). Yet there is little research on automatically resolving coreference ambiguities directly on AMR, making use of AMR graph-structural features. In this work, we formulate AMR coreference resolution as a missing-link prediction problem over AMR graphs, where the input consists of multiple sentence-level AMRs, and the goal is to recover the missing coreference links connecting the AMR nodes that represent to the same entity. There are two types of links. The first type corre"
2021.acl-long.324,W13-2322,0,0.119856,"Missing"
2021.acl-long.324,D17-1209,0,0.0606619,"Missing"
2021.acl-long.324,P18-1026,0,0.49436,"racter-level CNN. We concatenate both ek and echar embeddings for each k concept before using a linear projection to form the initial representation: xk = W node ([ek ; echar ]) + bnode , k (1) where W node and bnode are model parameters. To enable global information exchange across different sentence-level AMRs, we construct a draft document-level graph by connecting the root nodes of each AMR subgraph as shown in Figure 2. This is important because AMR coreference resolution involves cross-sentence reasoning. We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations. GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009). Compared with alternatives such as Graph Convolutional Network (GCN, Kipf and Welling 2017; 4205 :?????????? leave-11 ??1??? :name :???? : ???? :?????? :?? :op1 … … :CONNECT :arg1 ??2??2 ??(dummy ??, he) ??(leave-11, he) ??(Bill, he) he ??(arrive-01, he) :time ℒ???????? + ℒ???????????????????? = ?? … … date-entity ??3??? Input Representation GRN Encoder ∶dropped SOFTMAX ??3??? arrive-01 ??1??2"
2021.acl-long.324,2020.lrec-1.86,0,0.032401,"nts a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to"
2021.acl-long.324,2020.acl-main.119,0,0.240537,"tions. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components"
2021.acl-long.324,2020.acl-main.640,0,0.264686,"tions. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components"
2021.acl-long.324,N19-1366,0,0.0123276,"mising results over the years. Recent work (Lee et al., 2017, 2018; Kantor and Globerson, 2019) 4211 tackled the problem end-to-end by jointly detecting mentions and predicting coreference. Lee et al. (2018) build a complete end-to-end system with the span-ranking architecture and higher-order inference technique. While previous work considers only text-level coreference, we investigate AMR co-reference resolution. AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced. We choose a classic GRN model following Song et al. (2018) to represent our document-level AMR graph and leave the exploiting on a more efficient GNN structure for future work. 5 Conclusion We investigated a novel end-to-end multi-sentence AMR coreference resolution model using a graph neural network. Compared with previous rulebased and pipeline methods, our model better captures multi-sentence semantic information. Results on MS-AMR (in-domain) and LP (out-of-domain) datasets show the superiority and robustness of our model. In a"
2021.acl-long.324,J12-4003,0,0.023857,"by our end-toend model is solid and can transfer to a downstream application. D2S-AMRcoref-bert achieves the best performance, which is consistent with the above experiments. 4 Related Work Multi-sentence AMR Although some previous work (Szubert et al., 2020; Van Noord and Bos, 2017) explore the coreference phenomena of AMR, they mainly focus on the situation within a sentence. On the other hand, previous work on multi-sentence AMR primarily focuses on data annotation. Song et al. (2019a) annotate dropped pronouns over Chinese AMR but only deals with implicit roles in specific constructions. Gerber and Chai (2012) provide implicit role annotations, but the resources were limited to a small inventory of 5-10 predicate types rather than all implicit arguments. O’Gorman et al. (2018) annotated the MS-AMR dataset by simultaneously considering coreference, implicit role coreference and bridging relations. We consider coreference resolution as the prerequisite for creating multi-sentence AMRs, proposing the first end-to-end model for this task. Coreference Resolution Coreference resolution is a fundamental problem in natural language processing. Neural network models have shown promising results over the yea"
2021.acl-long.324,D18-1086,0,0.121583,"dotted ellipse represents an implicit role coreference. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have re"
2021.acl-long.324,S18-1150,0,0.0605553,"Missing"
2021.acl-long.324,2020.tacl-1.5,0,0.0135645,"hree measures include: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005). Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores. Baselines To study the effectiveness of end-toend AMR coreference resolution, we compare our model with the following baselines: • Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph by linking identical entities. • Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-text aligner (Flanigan et al., 2014). The former generates coreference from text, and the later projects this information from text to AMRs. logQ(y) k=1 y∈Yk ∩GOLD(k) (16) where GOLD(k) =  if mention vk does not belong to any gold cluster. Q(y) is calculated using Eq. 10. 3 resolution information over the development and test data of the Little Prince (LP) AMR corpus4 and use it as an out-of-domain test set. For this dataset, we consider each chapter as a document. The data statistics are shown in Table 1. Experiments We conduct experiments on the MS-AMR dataset3 (O"
2021.acl-long.324,P19-1066,0,0.0118251,"he resources were limited to a small inventory of 5-10 predicate types rather than all implicit arguments. O’Gorman et al. (2018) annotated the MS-AMR dataset by simultaneously considering coreference, implicit role coreference and bridging relations. We consider coreference resolution as the prerequisite for creating multi-sentence AMRs, proposing the first end-to-end model for this task. Coreference Resolution Coreference resolution is a fundamental problem in natural language processing. Neural network models have shown promising results over the years. Recent work (Lee et al., 2017, 2018; Kantor and Globerson, 2019) 4211 tackled the problem end-to-end by jointly detecting mentions and predicting coreference. Lee et al. (2018) build a complete end-to-end system with the span-ranking architecture and higher-order inference technique. While previous work considers only text-level coreference, we investigate AMR co-reference resolution. AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced. We choose"
2021.acl-long.324,P19-1340,0,0.0483421,"Missing"
2021.acl-long.324,D17-1018,0,0.197666,"level AMRs, and the goal is to recover the missing coreference links connecting the AMR nodes that represent to the same entity. There are two types of links. The first type corresponds to the standard situation, where the edge connects two entity nodes (e.g., “Bill” and “he” in Figure 1) that refer to the same entity. The second type is the implicit role coreference, where one node (e.g., “Paris” in Figure 1) is a dropped argument (“:arg3”) of other predicate node (“arrive-01”). We propose an AMR coreference resolution model by extending an end-to-end text-based coreference resolution model (Lee et al., 2017). In particular, we use a graph neural network to represent input AMRs for inducing expressive features. To enable cross-sentence information exchange, we make connections between sentence-level AMRs by linking their root nodes. Besides, we introduce a concept identification module to distinguish functional graph nodes (non-concept nodes, e.g., “person” in Figure 1), entity nodes (e.g., “Bill”), verbal nodes with implicit role (e.g., “arrive-01”) and other regular nodes (e.g., “leave-11”) to help improve the performance. The final antecedent prediction is conducted between the selected nodes a"
2021.acl-long.324,N18-2108,0,0.0505525,"Missing"
2021.acl-long.324,W15-4502,0,0.0166655,"cu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015"
2021.acl-long.324,C18-1101,0,0.0542114,"s an implicit role coreference. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing re"
2021.acl-long.324,N15-1114,0,0.343236,"sentation (AMR) has become very popular and AMR has been shown effective on many sentence-level tasks, little work has studied how to generate AMRs that can represent multi-sentence information. We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs. Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations. Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization. 1 :arg0 person :name name :op1 Bill Sentence2: He arrived at noon. arrive-01 leave-11 :arg1 :arg2 city he :name name Paris :arg3 date-entity :dayperiod :op1 noon Paris Figure 1: Multi-sentence AMR example, where nodes with the same non-black color are coreferential and the dotted ellipse represents an implicit role coreference. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) repre"
2021.acl-long.324,H05-1004,0,0.0657307,"eference clusters GOLD(k)|N k=1 and antecedent candidates Yk = {, yπ , ..., yk−1 } for mention vk , Lantecedent measures whether mentions are linked to their correct antecedent. Since the antecedents are latent, the antecedent loss is a marginal log-likelihood of all correct antecedents implied by gold clustering: Lantecedent (θ) = N Y X 3.1 Setup Evaluation Metrics We use the standard evaluation metrics for coreference resolution evaluation, computed using the official CoNLL-2012 evaluation toolkit. Three measures include: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005). Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores. Baselines To study the effectiveness of end-toend AMR coreference resolution, we compare our model with the following baselines: • Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph by linking identical entities. • Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-text aligner (Flanigan et al., 2014). The former generat"
2021.acl-long.324,P18-1037,0,0.0192462,"n Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequis"
2021.acl-long.324,P19-1451,0,0.021632,"s concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coref"
2021.acl-long.324,C18-1313,0,0.0377273,"Missing"
2021.acl-long.324,N15-1119,0,0.0284858,"Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components referring to the same entity. Figure 1 shows the AMR graphs of two consecutive sentences in a document. An AMR coreference resolution model need to identify two coreference cases: “he” refers to “Bill” in the first graph, and “arrive-01” omits an argument “:arg3” that refers to “Paris”. Relatively little research has been done on AMR coreference r"
2021.acl-long.324,2020.findings-emnlp.163,0,0.0579032,"Missing"
2021.acl-long.324,W17-2315,0,0.0588787,"Missing"
2021.acl-long.324,Q19-1002,1,0.955433,"m for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et"
2021.acl-long.324,2020.findings-emnlp.199,0,0.483163,"e with D2S-Rule-based on the downstream summerization task. This shows that the error propagation issue of Pipeline can introduce further negative effects to a downstream application. On the other hand, both D2S-AMRcoref-base and D2SAMRcoref-bert show much better results than the baselines across all Rouge metrics. This demonstrates that the improvements made by our end-toend model is solid and can transfer to a downstream application. D2S-AMRcoref-bert achieves the best performance, which is consistent with the above experiments. 4 Related Work Multi-sentence AMR Although some previous work (Szubert et al., 2020; Van Noord and Bos, 2017) explore the coreference phenomena of AMR, they mainly focus on the situation within a sentence. On the other hand, previous work on multi-sentence AMR primarily focuses on data annotation. Song et al. (2019a) annotate dropped pronouns over Chinese AMR but only deals with implicit roles in specific constructions. Gerber and Chai (2012) provide implicit role annotations, but the resources were limited to a small inventory of 5-10 predicate types rather than all implicit arguments. O’Gorman et al. (2018) annotated the MS-AMR dataset by simultaneously considering corefer"
2021.acl-long.324,W17-7306,0,0.0248713,"Missing"
2021.acl-long.324,M95-1005,0,0.167102,"ent Prediction Loss. Given a training AMR document with gold coreference clusters GOLD(k)|N k=1 and antecedent candidates Yk = {, yπ , ..., yk−1 } for mention vk , Lantecedent measures whether mentions are linked to their correct antecedent. Since the antecedents are latent, the antecedent loss is a marginal log-likelihood of all correct antecedents implied by gold clustering: Lantecedent (θ) = N Y X 3.1 Setup Evaluation Metrics We use the standard evaluation metrics for coreference resolution evaluation, computed using the official CoNLL-2012 evaluation toolkit. Three measures include: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005). Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores. Baselines To study the effectiveness of end-toend AMR coreference resolution, we compare our model with the following baselines: • Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph by linking identical entities. • Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-te"
2021.acl-long.324,2020.tacl-1.2,0,0.100941,"ee et al., 2017, 2018; Kantor and Globerson, 2019) 4211 tackled the problem end-to-end by jointly detecting mentions and predicting coreference. Lee et al. (2018) build a complete end-to-end system with the span-ranking architecture and higher-order inference technique. While previous work considers only text-level coreference, we investigate AMR co-reference resolution. AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced. We choose a classic GRN model following Song et al. (2018) to represent our document-level AMR graph and leave the exploiting on a more efficient GNN structure for future work. 5 Conclusion We investigated a novel end-to-end multi-sentence AMR coreference resolution model using a graph neural network. Compared with previous rulebased and pipeline methods, our model better captures multi-sentence semantic information. Results on MS-AMR (in-domain) and LP (out-of-domain) datasets show the superiority and robustness of our model. In addition, experiments on the downstream t"
2021.acl-long.324,P19-1009,0,0.0479197,"Missing"
2021.acl-long.324,2020.emnlp-main.169,0,0.562041,"Missing"
2021.acl-long.324,P18-1030,1,0.902422,"puted by using a character-level CNN. We concatenate both ek and echar embeddings for each k concept before using a linear projection to form the initial representation: xk = W node ([ek ; echar ]) + bnode , k (1) where W node and bnode are model parameters. To enable global information exchange across different sentence-level AMRs, we construct a draft document-level graph by connecting the root nodes of each AMR subgraph as shown in Figure 2. This is important because AMR coreference resolution involves cross-sentence reasoning. We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations. GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009). Compared with alternatives such as Graph Convolutional Network (GCN, Kipf and Welling 2017; 4205 :?????????? leave-11 ??1??? :name :???? : ???? :?????? :?? :op1 … … :CONNECT :arg1 ??2??2 ??(dummy ??, he) ??(leave-11, he) ??(Bill, he) he ??(arrive-01, he) :time ℒ???????? + ℒ???????????????????? = ?? … … date-entity ??3??? Input Representation GRN Encoder ∶dropped SOFTMAX ??3"
2021.acl-long.324,2020.acl-main.397,1,0.901984,"knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components referring to the sam"
2021.acl-long.324,P18-1150,1,0.944722,"ngs echar k are computed by using a character-level CNN. We concatenate both ek and echar embeddings for each k concept before using a linear projection to form the initial representation: xk = W node ([ek ; echar ]) + bnode , k (1) where W node and bnode are model parameters. To enable global information exchange across different sentence-level AMRs, we construct a draft document-level graph by connecting the root nodes of each AMR subgraph as shown in Figure 2. This is important because AMR coreference resolution involves cross-sentence reasoning. We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations. GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009). Compared with alternatives such as Graph Convolutional Network (GCN, Kipf and Welling 2017; 4205 :?????????? leave-11 ??1??? :name :???? : ???? :?????? :?? :op1 … … :CONNECT :arg1 ??2??2 ??(dummy ??, he) ??(leave-11, he) ??(Bill, he) he ??(arrive-01, he) :time ℒ???????? + ℒ???????????????????? = ?? … … date-entity ??3??? Input Representation GRN Encoder"
2021.acl-long.342,W13-2322,0,0.0831056,"ared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be useful for reducing the negative influence of variances in surf"
2021.acl-long.342,2020.acl-main.9,0,0.313415,"se generation tasks show the superiority of our model. To our knowledge, we are the first to leverage a formal semantic representation into neural dialogue modeling. 1 Ground-Truth: Maybe, but I would rather watch romance, science fiction, crime or even disaster movie instead of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-"
2021.acl-long.342,2020.lrec-1.86,0,0.115932,"antic representation for the modeling of everyday conversations. Constructing AMRs beyond Sentence Level There are a few attempts to construct AMRs beyond the sentence level. Liu et al. (2015) construct document-level AMRs by merging identical concepts of sentence-level AMRs for abstractive summerization, and Liao et al. (2018) further extend this approach to multi-document summerization. O’Gorman et al. (2018) manually annotate co-reference information across sentence AMRs. We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al"
2021.acl-long.342,D18-1547,0,0.04815,"Missing"
2021.acl-long.342,2020.acl-main.119,0,0.238764,"dialogue generation. One more advantage is that AMR is helpful to enhance the robustness and has a potential to improve the interpretability of neural models. To our knowledge, this is the first attempt to leverage the AMR semantic representation into neural networks for dialogue understanding and generation. Our code is available at https://github.com/muyeby/AMR-Dialogue. 2 Constructing Dialogue AMRs Figure 2 illustrates our method for constructing a dialogue-level AMR graph from multiple utterancelevel AMRs. Given a dialogue consisting multiple utterances, we adopt a pretrained AMR parser (Cai and Lam, 2020) to obtain an AMR graph for each utterance. For utterances containing multiple sentences, we parse them into multiple AMR graphs, and mark them belonging to the same utterance. We construct each dialogue AMR graph by making connections between utterance AMRs. In particular, we take three strategies according to speaker, identical concept and co-reference information. Speaker We add a dummy node and connect it to all root nodes of utterance AMRs. We add speaker tags (e.g., SPEAKER 1 and SPEAKER 2) to the edges to distinguish different speakers. The dummy node ensures that all utterance AMRs are"
2021.acl-long.342,P17-1175,0,0.0645532,"Missing"
2021.acl-long.342,2020.emnlp-main.651,0,0.0706939,"Missing"
2021.acl-long.342,D18-1241,0,0.0169852,"aster movie instead of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 201"
2021.acl-long.342,N19-1423,0,0.0185884,"ttps://github.com/huggingface/neuralcoref For simplicity, we omit the coreference links between the second and third utterance for display. 4 (5) (1) L L {hL 1 , h2 , ..., hn }, αij = Attn(hi , hj ). (4) where W1 , W2 , b1 , b2 are model parameters. Prel = softmax(W3 [ha1 ; ha2 ] + b3 ), H = SeqEncoder(emb(S)), (3) where W3 and b3 are model parameters. The k-th value of Prel is the conditional probability of k-th relation in R. Given a training instance hS, a1 , a2 , ri, the local loss is: ` = −logP (r|S, a1 , a2 ; θ), (6) where θ denotes the set of model parameters. In practice, we use BERT (Devlin et al., 2019) for calculating ha1 and ha2 , which can be regarded as pre-trained initialization of the Transformer encoder. 4432 ℎ2?? ℎ1?? ℎ3?? ℎ4?? ℎ5?? Graph Transformer Projected AMR edges ℎ1?? Text ??1 ??1 ℎ2?? ℎ3?? ??2 Transformer ??2 (a) ??3 ℎ4?? ??3 ??4 ??1 ??1 ??2 ??2 ??3 ??3 Graph Encoder ??1 Sequence Encoder ??2 ℎ�1 ℎ� 3 ℎ� 2 ??3 ??3 ??1 ??2 ??3 ??4 ??5 ??4 Sequence Encoder Dual Attention ℎ� 4 (b) ??2 Graph Encoder Feature Fusion ℎ5?? ??5 ??1 ??2 ??3 ??4 ??5 ??4 ??1 ℎ� 5 … ???? ???? ???? ????+1 ????+1 … (c) Figure 3: AMR for dialogue modeling. (a) Using AMR to enrich text representation. (b,"
2021.acl-long.342,D19-1407,0,0.0519174,"Missing"
2021.acl-long.342,P14-1134,0,0.0788866,"(W Q hl−1 i ) (W hj √ d + W Rr ij ) , where W R is a transformation matrix, r ij is the embedding of relation rij , d is hidden state size, and {h01 , h02 , ..., h0M } = {n1 , n2 , ..., nM }. The hidden state of ni is then updated as: XM hli = αij (W V hl−1 + W R r ij ), (11) j j=1 where W V is a parameter matrix. Overall, given an input AMR graph G = hV, Ei, the graph Transformer encoder can be written as H = GraphEncoder(emb(V), emb(E)), (12) L , ..., hL } denotes top-layer where H = {hL , h 1 2 M graph encoder hidden states. 4.2 Enriching Text Representation We first use the JAMR aligner (Flanigan et al., 2014) to obtain a node-to-word alignment, then adopt the alignment to project the AMR edges onto text with following rules:    ri0 j 0 , if A(ni0 ) = wi , A(nj 0 ) = wj , if i = j, rˆij = Self,   None, otherwise, (13) where A is a one-to-K alignment (K ∈ [0, . . . , N ]). In this way, we obtain a projected graph G 0 = hV 0 , E 0 i, where V 0 represents the set of input words {w1 , w2 , ..., wN } and E 0 denotes a set of word-to-word semantic relations. Inspired by previous work on AMR graph modeling (Guo et al., 2019; Song et al., 2019b; Sun et al., 2019), we adopt a hierarchical encoder that"
2021.acl-long.342,kingsbury-palmer-2002-treebank,0,0.576566,"Missing"
2021.acl-long.342,P17-1014,0,0.272741,"rmation across sentence AMRs. We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an"
2021.acl-long.342,N16-1014,0,0.0367911,"uation. Following Bao et al. (2020), we ask annotators who study linguistics to evaluate model outputs from four aspects, which are fluency, coherence, informativeness and overall performance. The scores are in a scale of {0, 1, 2}. The higher, the better. 6.1 6.2 6 Response Generation Experiments Settings We take Transformer as a baseline. Our hyperparameters are selected by word prediction accuracy on validation dataset. The detailed hyperparameters are given in Appendix (See Table 6). Metric We set the decoding beam size as 5 and adopt BLEU-1/2/3/4 (Papineni et al., 2002) and Distinct-1/2 (Li et al., 2016) as automatic evaluation metrics. The former measures the ngram overlap between generated response and Automatic Evaluation Results Table 2 reports the performances of the previous state-of-the-art methods and proposed models on the DailyDialog testset. For the previous methods, PLATO and PLATO w/o L are both Transformer models pre-trained on large-scale conversational data (8.3 million samples) and finetuned on DailyDialog. For completeness, we also report other systems including Seq2Seq (Vinyals and Le, 2015) and iVAEMI (Fang et al., 2019). 4436 Model Transformer Hier Dual Fluency Coherence"
2021.acl-long.342,I17-1099,0,0.199859,"t to leverage a formal semantic representation into neural dialogue modeling. 1 Ground-Truth: Maybe, but I would rather watch romance, science fiction, crime or even disaster movie instead of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For"
2021.acl-long.342,C18-1101,0,0.0681589,"nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main"
2021.acl-long.342,N15-1114,0,0.174097,"es and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explici"
2021.acl-long.342,P18-1037,0,0.017178,"AMRs. We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construc"
2021.acl-long.342,N18-2078,0,0.0234461,"nt context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be u"
2021.acl-long.342,2020.acl-main.173,0,0.022716,"al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving strong empirical results, neural models can suffer from spurious feature associations in their neural semantic representation (Poliak et al., 2018; Kaushik et al., 2020), which can lead to weak robustness, inducing irrelevant dialogue states (Xu and Sarikaya, 2014; Sharma et al., 2019; Rastogi et al., 2019) and generating unfaithful or irrelevant text (Maynez et al., 2020; Niu and Bansal, 2020). As shown in Figure 1, the baseline Transformer model pays attention to the word “lamb” but ignores its surrounding context, which has important contents (marked with squares) that indicate its true meaning, thereby giving an irrelevant response that is related to food. Intuitively, such issues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceed"
2021.acl-long.342,2020.acl-main.141,0,0.062879,"ious work on DialogRE, we report macro F1 score on relations in both the standard (F1) and conversational settings (F1c ; Yu et al., 2020). F1c is computed over the first few turns of a dialogue where two arguments are first mentioned. 7 Main Results Table 1 shows the results of different systems on DialogRE. We compare the proposed model with two BERT-based approches, BERT and BERTs . Based on BERT, BERTs (Yu et al., 2020) highlights speaker information by replacing speaker arguments with special tokens. For completeness, we also include recent methods, such as AGGCN (Guo et al., 2019), LSR (Nan et al., 2020) and DHGAT (Chen et al., 2020). BERTc and Hier, Dual represent our baseline and the proposed models, respectively. By incorporating speaker information, BERTs gives the best performance among the previous system. Our BERTc baseline outperforms BERTs by a large margin, as BERTc additionally considers argument representations for classification. Hier significantly (p < 0.01)8 outperforms BERTc in all settings, with 1.4 points of improvement in terms of F1 score on average. A similar trend is observed under F1c . This shows that semantic information in AMR is beneficial to dialogue relation extra"
2021.acl-long.342,C18-1313,0,0.0344113,"Missing"
2021.acl-long.342,J05-1004,0,0.303733,"Missing"
2021.acl-long.342,N19-2013,0,0.0204127,"etworks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving strong empirical results, neural models can suffer from spurious feature associations in their neural semantic representation (Poliak et al., 2018; Kaushik et al., 2020), which can lead to weak robustness, inducing irrelevant dialogue states (Xu and Sarikaya, 2014; Sharma et al., 2019; Rastogi et al., 2019) and generating unfaithful or irrelevant text (Maynez et al., 2020; Niu and Bansal, 2020). As shown in Figure 1, the baseline Transformer model pays attention to the word “lamb” but ignores its surrounding context, which has important contents (marked with squares) that indicate its true meaning, thereby giving an irrelevant response that is related to food. Intuitively, such issues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explic"
2021.acl-long.342,Q19-1016,0,0.0267408,"of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020)"
2021.acl-long.342,D11-1054,0,0.138267,"Missing"
2021.acl-long.342,2021.eacl-main.228,0,0.0155269,"rmation, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentiall"
2021.acl-long.342,P02-1040,0,0.114126,". In addition, we also conduct human evaluation. Following Bao et al. (2020), we ask annotators who study linguistics to evaluate model outputs from four aspects, which are fluency, coherence, informativeness and overall performance. The scores are in a scale of {0, 1, 2}. The higher, the better. 6.1 6.2 6 Response Generation Experiments Settings We take Transformer as a baseline. Our hyperparameters are selected by word prediction accuracy on validation dataset. The detailed hyperparameters are given in Appendix (See Table 6). Metric We set the decoding beam size as 5 and adopt BLEU-1/2/3/4 (Papineni et al., 2002) and Distinct-1/2 (Li et al., 2016) as automatic evaluation metrics. The former measures the ngram overlap between generated response and Automatic Evaluation Results Table 2 reports the performances of the previous state-of-the-art methods and proposed models on the DailyDialog testset. For the previous methods, PLATO and PLATO w/o L are both Transformer models pre-trained on large-scale conversational data (8.3 million samples) and finetuned on DailyDialog. For completeness, we also report other systems including Seq2Seq (Vinyals and Le, 2015) and iVAEMI (Fang et al., 2019). 4436 Model Trans"
2021.acl-long.342,S18-2023,0,0.0282355,"Missing"
2021.acl-long.342,N19-1057,0,0.0200161,"thods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving strong empirical results, neural models can suffer from spurious feature associations in their neural semantic representation (Poliak et al., 2018; Kaushik et al., 2020), which can lead to weak robustness, inducing irrelevant dialogue states (Xu and Sarikaya, 2014; Sharma et al., 2019; Rastogi et al., 2019) and generating unfaithful or irrelevant text (Maynez et al., 2020; Niu and Bansal, 2020). As shown in Figure 1, the baseline Transformer model pays attention to the word “lamb” but ignores its surrounding context, which has important contents (marked with squares) that indicate its true meaning, thereby giving an irrelevant response that is related to food. Intuitively, such issues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most"
2021.acl-long.342,D19-1462,0,0.0203939,":poss :speaker1 I :mode bill :ARG1 I :ARG1 :ARG1 :ARG1 I :ARG2 say Dummy mistake I possible Graph Merge :ARG1 have :ARG0 :ARG0 sir I fear S1 :speaker2 certain :ARG1 :ARG0 AMR Parsing :mode interrogative I :ARG2 say S2 :speaker1 :poss bill I interrogative :ARG1 possible :ARG1 have :ARG0 it (b) Utterance AMR Graphs unknown (c) Dialogue AMR Graph Figure 2: Dialogue AMR graph construction process. Step 1: parse raw-text utterance into utterance AMR graphs; Step 2: connect utterance AMR graphs into a dialogue AMR graph. which are frequent in conversations (Grosz et al., 1995; Newman et al., 2008; Quan et al., 2019). We conduct co-reference resolution on dialogue text using an off-to-shelf model3 in order to identify concept nodes in utterance AMRs that refer to the same entity. For example, in Figure 2, “I” in the first utterance, and “sir” in the second utterance refer to the same entity, SPEAKR 1. We add edges labeled with COREF between them, starting from later nodes to earlier nodes (later and earlier here refer to the temporal order of ongoing conversation), to indicate their relation4 . 3 Baseline System We adopt a standard Transformer (Vaswani et al., 2017) for dialogue history encoding. Typicall"
2021.acl-long.342,Q19-1002,1,0.933316,"ures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be useful for reducing"
2021.acl-long.342,2020.acl-main.712,1,0.763791,"Rs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems. Experiments on two benchmarks show advantage"
2021.acl-long.342,D19-1020,1,0.939365,"ures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be useful for reducing"
2021.acl-long.342,P18-1150,1,0.854873,"deling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems. Experim"
2021.acl-long.342,D18-1548,0,0.0189623,"ues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g"
2021.acl-long.342,D19-1569,0,0.150277,"y having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), whil"
2021.acl-long.342,2020.acl-main.444,0,0.304169,"… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving s"
2021.acl-long.342,P19-1009,0,0.0481586,"Missing"
2021.acl-long.342,2020.acl-main.67,0,0.0181622,"enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems. Experiments on two benchmarks show advantages of using AMR sema"
2021.acl-long.342,D19-1548,0,0.278152,"uares) and multiple speaker interactions. To this end, we propose an algorithm to automatically derive dialogue-level AMRs from utterance-level AMRs, by adding cross-utterance links that indicate speakers, identical mentions and co-reference links. One example is shown in the right block of Figure 2, where newly added edges are in color. We consider two main approaches of making use of such dialogue-level AMR structures. For the first method, we merge an AMR with tokens in its corresponding sentence via AMR-to-text alignments, before encoding the resulting structure using a graph Transformer (Zhu et al., 2019). For the second method, we separately encode an AMR and its corresponding sentence, before leveraging both representations via feature fusion (Mangai et al., 2010) or dual attention (Calixto et al., 2017). We verify the effectiveness of the proposed framework on a dialogue relation extraction task (Yu et al., 2020) and a response generation task (Li et al., 2017). Experimental results show that the proposed framework outperforms previous methods (Vaswani et al., 2017; Bao et al., 2020; Yu et al., 2020), achieving the new state-of-the-art results on both benchmarks. Deep analysis and human eva"
2021.acl-short.84,D19-1539,0,0.0217174,"9) have achieved promising results on various natural language processing (NLP) tasks, including natural language understanding, text generation and question anwsering (Liu et al., 2019; Song et al., 2019; Reddy et al., 2019). In order to acquire general linguistic and semantic knowledge, these pretraining methods are usually performed on open-domain corpus, like Wikipedia and BooksCorpus. In light of the success from open-domain pretraining, a further question is naturally raised: whether downstream tasks can also benefit from domain-adaptive pretraining? To answer this question, later work (Baevski et al., 2019; Gururangan et al., 2020) has demonstrated that continued pretraining on the unlabeled data in the target domain can further contribute to the corresponding downstream task. However, these studies are dependent on additional data that can be unavailable in certain scenarios, and they only evaluated on easy downstream tasks. For instance, Gururangan et al. (2020) perform continued pretraining with masked language modeling loss on several relevant domains, and they obtain improvements on eight well-studied classification tasks, which are too simple to exhibit the strength of continued domain-ad"
2021.acl-short.84,N19-1423,0,0.533055,"e obtained impressive gains on various NLP tasks. In this paper, we probe the effectiveness of domainadaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domainadaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances. 1 Introduction Recent advances in pretraining methods (Devlin et al., 2019; Joshi et al., 2020; Yang et al., 2019) have achieved promising results on various natural language processing (NLP) tasks, including natural language understanding, text generation and question anwsering (Liu et al., 2019; Song et al., 2019; Reddy et al., 2019). In order to acquire general linguistic and semantic knowledge, these pretraining methods are usually performed on open-domain corpus, like Wikipedia and BooksCorpus. In light of the success from open-domain pretraining, a further question is naturally raised: whether downstream tasks can also benefit from domain-adaptive pretraining?"
2021.acl-short.84,2020.acl-main.740,0,0.0526358,"Missing"
2021.acl-short.84,2020.aacl-main.42,1,0.774911,"Missing"
2021.acl-short.84,2020.tacl-1.5,0,0.251461,"gains on various NLP tasks. In this paper, we probe the effectiveness of domainadaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domainadaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances. 1 Introduction Recent advances in pretraining methods (Devlin et al., 2019; Joshi et al., 2020; Yang et al., 2019) have achieved promising results on various natural language processing (NLP) tasks, including natural language understanding, text generation and question anwsering (Liu et al., 2019; Song et al., 2019; Reddy et al., 2019). In order to acquire general linguistic and semantic knowledge, these pretraining methods are usually performed on open-domain corpus, like Wikipedia and BooksCorpus. In light of the success from open-domain pretraining, a further question is naturally raised: whether downstream tasks can also benefit from domain-adaptive pretraining? To answer this ques"
2021.acl-short.84,Q19-1016,0,0.0277302,"s, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domainadaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances. 1 Introduction Recent advances in pretraining methods (Devlin et al., 2019; Joshi et al., 2020; Yang et al., 2019) have achieved promising results on various natural language processing (NLP) tasks, including natural language understanding, text generation and question anwsering (Liu et al., 2019; Song et al., 2019; Reddy et al., 2019). In order to acquire general linguistic and semantic knowledge, these pretraining methods are usually performed on open-domain corpus, like Wikipedia and BooksCorpus. In light of the success from open-domain pretraining, a further question is naturally raised: whether downstream tasks can also benefit from domain-adaptive pretraining? To answer this question, later work (Baevski et al., 2019; Gururangan et al., 2020) has demonstrated that continued pretraining on the unlabeled data in the target domain can further contribute to the corresponding downstream task. However, these studies are dep"
2021.acl-short.84,P19-1369,0,0.028399,"d dataset, including 6K dialogue sessions and 102K utterances on five domains. Since the state-of-the-art models on these tasks are all developed based on BERT, we use the same model architectures but just replace the BERT base with our domain-adaptive pretrained BERT. Notice that, we also experiment with other pretrained language models such as RoBERTa and XLNet. We observed similar results but here we only report the results based on BERT due to the space limitation. In particular, we perform the domain-adaptive pretraining on CSRL task using all dialogue sessions of training set in DuConv (Wu et al., 2019) and NewsDialog (Wang et al., 2021), which includes 26K and 20K sessions, respectively; on the SLU task, we use the whole CrossWOZ training dataset. The hyper-parameters used in our model are listed as follows. The network parameters of our model are initialized using the pretrained language model. The batch size is set to 128. We use Adam (Kingma and Ba, 2015) with learning rate 5e-5 to update parameters. |Y | LP M O Experiments 1 X =− −f (xt , {xp0 , ..., xpm−1 })i |Y | t=1 where p0 ,... pm−1 are m predicates that occur in the sentence. In practice, we first follow the SpanBERT to sample a s"
2021.acl-short.84,2020.acl-main.383,0,0.0283286,"rom X as predicates and perform perturbation masking on those predicates. 4 |Y | LSBO 1 X log p(xt |yt ; θ) =− |Y | t=1 3.3 Perturbation Masking Objective In dialogue understanding tasks like CSRL, the major goal is to capture the semantic information such as the correlation between arguments and predicate. However, for the sake of generalization, existing pretraining models do not consider the semantic information of a word and also not assess the impact of predicate has on the prediction of arguments in their objectives. To address this, we propose to use the perturbation masking technique (Wu et al., 2020) to explicitly measure the correlation between arguments and predicate and further introduce that into our objective. The perturbation masking is originally proposed to assess the impact one word has on the prediction of another in MLM. In particular, given a list of tokens X, we first use a pretrained language model M to map each xi into a contextualized representation H(X)i . Then, we use a two-stage approach to capture the impact word xj has on the prediction of another word xi . First, we replace xi with the [MASK] token and feed the new sequence X{xi } into M. We use H(X{xi })i to denot"
2021.acl-short.84,2020.emnlp-main.537,1,0.846877,"Missing"
2021.acl-short.84,2020.tacl-1.19,0,0.0223526,"(xi , xj ) = d(H(X{xi })i , H(X{xi , xj })i ), where d is the distance metric that captures the difference between two vectors. In experiments, we use the Euclidean distance as the distance metric. Since our goal is to better learn the correlation between arguments and predicate, we introduce a perturbation masking objective that maximizes the impact of predicate on the prediction of argument span: We evaluate pretraining objectives on three datasets, DuConv, NewsDialog2 and CrossWOZ. The former two datasets are annotated by Xu et al. (2021) for the CSRL task and the last one is provided by Zhu et al. (2020) for the SLU task. Duconv is a Chinese knowledge-driven dialogue dataset, focusing on the domain of movies and stars. NewsDialog is a dataset collected in a way that follows the setting for constructing general open-domain dialogues: two participants engage in chitchat, and during the conversation, the topic is allowed to change naturally. Xu et al. (2021) annotates 3K dialogue sessions of DuConv to train their CSRL parser, and directly test on 200 annotated dialogue sessions of NewsDialog. CrossWOZ is a Chinese Wizard-of-Oz task-oriented dataset, including 6K dialogue sessions and 102K uttera"
2021.emnlp-main.402,2020.tacl-1.36,0,0.0661528,"Missing"
2021.emnlp-main.402,W14-3346,0,0.0961204,"ulti-task sequence tagging. In particular, for each input word, we decide whether to delete it or not, and at the same time, we choose what span from the dialogue context need to be inserted to the front of the current word. In this way, our solution enjoys a far smaller search space than the generation based approaches. Since our model does not directly take features from the word-to-word interactions of its output utterances, this may cause the lack of fluency. To encourage more fluent outputs, we propose to inject additional supervisions from two popular metrics, i.e., sentence-level BLEU (Chen and Cherry, 2014) and the perplexity of a pretrained GPT-2 (Radford et al., 2019) model, using the framework of “REINFORCE with a baseline” (Williams, 1992). Sentence-level BLEU is computationally efficient, but it requires references and thus may only provide domain-specific knowledge. Conversely, the perplexity by GPT-2 is reference-free, giving more guidance on open-domain scenarios benefiting from the large-scale pretraining. Experiments on two dialogue rewriting benchmarks show that our model can give huge improvements (14.6 in BLEU4 score and 18.9 percent of exact match) over the current state-of-the-art"
2021.emnlp-main.402,2020.emnlp-main.651,0,0.074226,"Missing"
2021.emnlp-main.402,N19-1423,0,0.0777142,"able has limited coverage. Though we also convert our original problem into a multi-task tagging problem, we predict what span to be inserted, avoiding the issues caused by using a phrase table. Besides, we study injecting richer supervision signals to improve the fluency of outputs, which is a common issue for tagging based approaches on text generation, as they do not directly model wordto-word dependencies. Finally, we are the first to apply sequence tagging on dialogue rewriting, showing much better performances than those of BERT-based strong baselines. 3 Our baseline consists of a BERT (Devlin et al., 2019) encoder and a Transformer (Vaswani et al., 2017) decoder with a copy mechanism. Given input tokens X = (x1 , . . . , xN ) that is the concatenation of the current dialogue context c = (u1 , . . . , ui−1 ) and the latest utterance ui , the BERT encoder is firstly adopted to represent the input with contextualized embeddings: (1) Next, the Transformer decoder with copy mechanism is adopted to generate a rewriting output u0 = (y1 , . . . , yM ) one token at a time: p(yt |y<t , X) = θt pvocab + (1 − θt )pattn t t pattn , st t vocab pt (2) = TransDecoder(y<t , E) (3) = Softmax(Linear(st )) (4) whe"
2021.emnlp-main.402,D19-1605,0,0.0392318,"Missing"
2021.emnlp-main.402,P16-1154,0,0.0773972,"Missing"
2021.emnlp-main.402,P16-1014,0,0.0473789,"Missing"
2021.emnlp-main.402,I17-1099,0,0.0609614,"Missing"
2021.emnlp-main.402,P19-1003,0,0.339307,"r the current state-of-the-art systems when transferring to another dataset. u1 上海最近天气怎么样？ (How is the recent weather in Shanghai?) u2 最近经常阴天下雨。 (It is always raining recently.) u3 冬天就是这样。 (Winter is like this.) u03 上海冬天就是经常阴天下雨。 (It is always raining in winter Shanghai.) Table 1: An example dialogue including the context utterances (u1 and u2 ), the latest utterance (u3 ) and the rewritten utterance (u03 ). on these tasks are still far from satisfactory, not to mention their uncovered situations, such as when a whole verb phrase is omitted. Recently, the task of dialogue utterance rewriting (Su et al., 2019; Pan et al., 2019; Elgohary et al., 1 Introduction 2019) was proposed as for explicitly representing multi-turn dialogues. The task aims to reconstruct Recent years have witnessed increasing attention the latest dialogue utterance into a new utterance in conversation-based tasks, such as conversational question answering (Choi et al., 2018; Reddy et al., that is semantically equivalent to the original one 2019; Sun et al., 2019), dialogue response genera- and can be understood without referring to the contion (Li et al., 2017; Zhang et al., 2018; Wu et al., text. In another point of view, it"
2021.emnlp-main.402,Q19-1014,1,0.890552,"Missing"
2021.emnlp-main.402,2020.emnlp-main.227,0,0.0155517,"method, where all context words that need to be inserted during rewriting are identified in the first step. The second step adopts a pointer generator that takes the outputs of the first step as additional features to produce the output. Xu et al. (2020) train a model of semantic role labeling (SRL) to highlight the core meaning (e.g., who did what to whom) of each input dialogue to prevent their rewriter from violating this information. To obtain an accurate SRL model on dialogues, they manually annotate SRL information for more than 27,000 dialogue turns, which is timeconsuming and costly. Liu et al. (2020) casts this task into a semantic segmentation problem, a major task in computer vision. In particular, their model generates a word-level matrix, which contains the operations of substitution and insertion, for each original utterance. They adopt a heavy model that takes 10 convolution layers in addition to the BERT encoder. None of the existing efforts mention the robustness issue, a critical aspect for the usability of this task. Besides, they only compare performances under automatic metrics (e.g., BLEU). We take the first step to address this severe robustness issue, and we adopt multiple"
2021.emnlp-main.402,D19-1510,0,0.0209559,"to address this severe robustness issue, and we adopt multiple measures for comprehensive evaluation. Besides, we propose a novel model based on sequence tagging for solving this task, and our model takes a much smaller search space than previous models. Sequence tagging for text generation Given the intrinsic nature of typical text-generation problems (e.g., machine translation), i.e. (1) the number of predictions cannot be determined by inputs, and (2) the candidate space for each prediction is usually very large, sequence tagging is not commonly adopted on text-generation tasks. Recently, Malmi et al. (2019) proposed a model based on sequence tagging for sentence fusion and sentence splitting, and they show that their model outperforms a vanilla sequence-to-sequence baseline. In particular, their model can decide whether to keep or delete each input word and what phrase needs 2 Related Work to be inserted in front of it. As a result, they have Initial efforts (Su et al., 2019; Elgohary et al., to extract a large phrase table from the training 2019) treat dialogue utterance rewriting as a stan- data, causing inevitable computation for choosing 4914 phrases from the table. Their approach also faces"
2021.emnlp-main.402,D19-1191,0,0.228972,"te-of-the-art systems when transferring to another dataset. u1 上海最近天气怎么样？ (How is the recent weather in Shanghai?) u2 最近经常阴天下雨。 (It is always raining recently.) u3 冬天就是这样。 (Winter is like this.) u03 上海冬天就是经常阴天下雨。 (It is always raining in winter Shanghai.) Table 1: An example dialogue including the context utterances (u1 and u2 ), the latest utterance (u3 ) and the rewritten utterance (u03 ). on these tasks are still far from satisfactory, not to mention their uncovered situations, such as when a whole verb phrase is omitted. Recently, the task of dialogue utterance rewriting (Su et al., 2019; Pan et al., 2019; Elgohary et al., 1 Introduction 2019) was proposed as for explicitly representing multi-turn dialogues. The task aims to reconstruct Recent years have witnessed increasing attention the latest dialogue utterance into a new utterance in conversation-based tasks, such as conversational question answering (Choi et al., 2018; Reddy et al., that is semantically equivalent to the original one 2019; Sun et al., 2019), dialogue response genera- and can be understood without referring to the contion (Li et al., 2017; Zhang et al., 2018; Wu et al., text. In another point of view, it integrates the rec"
2021.emnlp-main.402,P02-1040,0,0.1101,"2016; See et al., 2017). They have demonstrated recovery. But, the state-of-the-art performances almost ready-to-use performances on the test set ∗ Work done while J. Hao was interning and L. Wang was from the same data source as the training set. Howworking at Tencent AI Lab. † Corresponding author. ever, they are not robust, as our experiments show 4913 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4913–4924 c November 7–11, 2021. 2021 Association for Computational Linguistics that their performances can drop dramatically (by roughly 33 BLEU4 (Papineni et al., 2002) points and 44 percent of exact match) on another test set created from a different data source (not necessarily from a totally different domain). We argue that it may not be the best practice to model utterance rewriting as standard text generation. One main reason is that text generation introduces an overly large search space, while a rewriting output (e.g., u03 in Table 1) always keeps the core semantic meaning of its input (e.g., u3 ). Besides, exposure bias (Wiseman and Rush, 2016) can further exacerbate the problem for test cases that are not similar to the training set, resulting in ou"
2021.emnlp-main.402,D16-1264,0,0.0452181,"of our model. For a fair comparison, it takes the same BERT-based encoder (Equation 1) as the baseline to represent each input. For simplicity, we directly apply classifiers to predict the corresponding tags for each input word. In particular, to determine whether each word xn in the current utterance ui should be kept or deleted, we use a binary classifier: p(dn |X, n) = Softmax(Wd en + bd ) (7) where Wd and bd are learnable parameters, dn is the binary classification result, and en is the BERT embedding for xn . 4916 Moreover, we cast span prediction as machine reading comprehension (MRC) (Rajpurkar et al., 2016), where a predicted span corresponds to an MRC target answer. For each input token xn ∈ ui , we follow the previous work on MRC to predict ed the start position sst n and end position sn for the target span sn , performing separate self-attention mechanisms for them: p(sst n |X, n) = Attnstart (E, en ) (8) p(sed n |X, n) = Attnend (E, en ) (9) where Attnstart and Attnend are the self-attention layers for predicting the start and end positions of a span. We use the standard additive attention mechanism (Bahdanau et al., 2014) to perform the attention function. The probability for the whole span"
2021.emnlp-main.402,Q19-1016,0,0.0404538,"Missing"
2021.emnlp-main.402,P17-1099,0,0.0428704,", . . . , ui−1 ) and the latest utterance ui , the BERT encoder is firstly adopted to represent the input with contextualized embeddings: (1) Next, the Transformer decoder with copy mechanism is adopted to generate a rewriting output u0 = (y1 , . . . , yM ) one token at a time: p(yt |y<t , X) = θt pvocab + (1 − θt )pattn t t pattn , st t vocab pt (2) = TransDecoder(y<t , E) (3) = Softmax(Linear(st )) (4) where TransDecoder is the Transformer decoder that returns the attention probability distribution pattn over the encoder states E and the latest det coder state st for each step t. Following See et al. (2017), the generation probability θt for timestep t is calculated from the weighted sum for the encoder-decoder cross attention distribution and the encoder hidden states. X θt = σ(w| (pattn [n] · en )) (5) t n∈[1..N ] where w represents the model parameter. In this way, the copy mechanism encourages copying words from the input tokens. The T RANS -PG baseline is trained with standard cross-entropy loss: X Lgen = − log p(yt |y<t , X; θ) (6) t∈[1..M ] where θ represents all model parameters. 5 06 06 27 Shanghai recently recently Reference <start> Input <start> Deletion Insertion often 5 Shanghai 9 w"
2021.emnlp-main.402,D16-1137,0,0.0229302,"Missing"
2021.emnlp-main.402,P19-1369,0,0.0548787,"Missing"
2021.emnlp-main.402,2020.emnlp-main.537,1,0.851999,"Missing"
2021.emnlp-main.402,2020.acl-main.444,1,0.888547,"Missing"
2021.emnlp-main.402,P18-1205,0,0.0608904,"Missing"
2021.emnlp-main.402,2020.acl-demos.30,0,0.0330435,".7 76.0 69.8 67.6 76.5 78.8 79.3 80.5 7.4 8.6 12.9 24.5 26.8 31.8 Table 3: Test results of all comparing models trained on the R EWRITE dataset. Model settings We implement the baseline and our model on top of a BERT-base model (Devlin et al., 2019), and we use Adam (Kingma and Ba, 2015) as the optimizer, setting the learning rate to 3e−5 as determined by a development experiment. For the reinforcement learning stage, we respectively use the sentence-level BLEU score with “Smoothing 3” (Chen and Cherry, 2014) or the perplexity score based on a Chinese GPT-2 model trained on massive dialogues (Zhang et al., 2020)4 as the reward function. It is worth noting that the GPT-2 model is not fine-tuned during the reinforcement learning stage. 6.2 Main Results Training on R EWRITE Table 3 shows the results when all comparing models are trained on the R EWRITE dataset, before evaluating on the indomain R EWRITE and the R ESTORATION test data for robustness examination. On the R EWRITE test set, our tagging-based models (Rows 4-6) are much better than the T RANS -PG+BERT baseline, and they can get comparable performances with RUN, the previous state-of-the-art model. RUN usually gets high numbers on BLEU1 withou"
2021.emnlp-main.402,2020.acl-main.635,0,0.0367031,"Missing"
2021.emnlp-main.402,D19-1192,0,0.0176334,"n open-domain scenarios benefiting from the large-scale pretraining. Experiments on two dialogue rewriting benchmarks show that our model can give huge improvements (14.6 in BLEU4 score and 18.9 percent of exact match) over the current state-of-the-art model for cross-dataset evaluation. More analysis shows that the outputs of our model keep more semantic information from the inputs. Our code is available at https://github.com/ freesunshine0316/RaST-plus. dard text generation problem, adopting sequenceto-sequence models with copy mechanism to tackle this problem. Later work (Pan et al., 2019; Zhou et al., 2019; Huang et al., 2021) explores taskspecific features for additional gains in performance. For instance, Pan et al. (2019) adopts a pipeline-based method, where all context words that need to be inserted during rewriting are identified in the first step. The second step adopts a pointer generator that takes the outputs of the first step as additional features to produce the output. Xu et al. (2020) train a model of semantic role labeling (SRL) to highlight the core meaning (e.g., who did what to whom) of each input dialogue to prevent their rewriter from violating this information. To obtain an"
2021.emnlp-main.457,P17-1147,0,0.0229705,"Missing"
2021.emnlp-main.457,D17-1126,0,0.0473201,"Missing"
2021.emnlp-main.457,D19-1362,0,0.0271465,"s show that: The wide availability of neural network models has allowed development of novel and complex natural language processing tasks, many of which are in low-resource settings. With new definitions of tasks comes challenges of constructing new datasets, which is still an expensive and timeintensive endeavor. Many researchers have resorted 1. Instance-adaptive noise-robust training proto constructing datasets by using completely autoposed in this work enhances the noisemated pipelines (e.g. Lan et al., 2017; Joshi et al., robustness of the losses on noisy and cor2017; Paul et al., 2019; Lange et al., 2019; Sousa rupted datasets, which results in large peret al., 2019; Wu et al., 2020). However, silver labels formance gains when instance-specific noisecollected this way are still quite noisy compared resistance hyperparameters are used. to expert annotation. Because such methods have been gaining popularity and practicality, it is impor2. Noise-robust losses are an effective way to 5647 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5647–5663 c November 7–11, 2021. 2021 Association for Computational Linguistics combat noise in silver-standard NLP d"
2021.emnlp-main.457,D14-1162,0,0.0861334,"Missing"
2021.emnlp-main.457,P17-1099,0,0.0412692,"Missing"
2021.emnlp-main.457,D13-1170,0,0.00892486,"Missing"
2021.emnlp-main.457,N19-1152,0,0.0224825,"Missing"
2021.findings-acl.223,2020.webnlg-1.8,0,0.151112,"on (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fi"
2021.findings-acl.223,2020.emnlp-main.54,1,0.690434,"o-text (KG-to-text) generation aims to generate high-quality texts which are consistent with input graphs (Gardent et al., 2017). This task requires to simultaneously encode the graph structure and the content, and effectively leverage the input graphs in the decoding process (Zhao et al., 2020). As a major natural language generation (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-t"
2021.findings-acl.223,2020.coling-main.217,0,0.0283643,"ers with the input of linearized graphs (Gardent et al., 2017; Trisedya et al., 2018; Moryossef et al., 2019), researchers focus on more complex encoder structures for better graph representations, such as graph neural networks (Marcheggiani and Perez-Beltrachini, 2018; Ribeiro et al., 2020b) and graph Transformers (Koncel-Kedziorski et al., 2019; Schmitt et al., 2020a). 2) Unsupervised training: researchers devise unsupervised training objectives to jointly learn the tasks of graph-to-text and textto-graph conversion with non-parallel graph-text data (Schmitt et al., 2020b; Guo et al., 2020; Jin et al., 2020). 3) Building pre-trained models: With the development of pre-trained NLG models such as GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), recent work directly fine-tunes these models on graph-totext datasets and reports impressive performance (Ribeiro et al., 2020a; Kale and Rastogi, 2020; Chen et al., 2020b; Mager et al., 2020). Compared with the existing work on pre-trained models for KG-to-text generation, our model utilizes pre-training methods to explicitly learn graphtext alignments instead of directly fine-tuning textto-text pre-trained models on"
2021.findings-acl.223,2020.inlg-1.14,0,0.196457,"arameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fine-tuning, and outperform other models with sophisticated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained models capture contextual information via bidirectional Transformers (Devlin et al., 2019), which i"
2021.findings-acl.223,N19-1238,0,0.0375841,"Missing"
2021.findings-acl.223,D18-2012,0,0.017748,"Lewis et al., 2020) and T5 (Raffel et al., 2020) as the base model in this paper, which are denoted by JointGT (BART) and JointGT (T5), respectively. The hyper-parameters of the Transformer blocks were the same as BARTbase and T5-base because of the limited computational resources. We initialized our model parameters with the pre-trained checkpoint of BARTbase / T5-base except for the structure-aware semantic aggregation module, which was randomly initialized. We followed BART / T5 to use BytePair Encoding (BPE) vocabulary (Radford et al., 2019) with the size of 50,265 / WordPiece vocabulary (Kudo and Richardson, 2018) with the size of 32,000. The batch size was 42 / 32 for JointGT (BART) / JointGT (T5). The maximum length of linearized input graphs was 600, while the maximum length of text sequences was 64. We adopted Adam (Kingma and Ba, 2015) as the optimizer and set the learning rate to be 3e-5. The warmup ratio was 0.1. JointGT was pre-trained on KGTEXT for 1 epoch with the proposed pre-training tasks. It took 44 / 69 hours for JointGT (BART) / JointGT (T5) on 3 NVIDIA Quadro RTX 6000 GPUs. 4.2 Fine-Tuning Settings We adopted WebNLG, WebQuestions and Path Questions as the benchmark datasets during fine"
2021.findings-acl.223,2020.acl-main.703,0,0.127151,"’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fine-tuning, and outperform other models with sophisticated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained m"
2021.findings-acl.223,2020.acl-main.167,0,0.0456604,"Missing"
2021.findings-acl.223,W18-6501,0,0.0153687,"of KG-to-text generation including WebNLG, WebQuestions and PathQuestions. Results show that JointGT achieves new state-of-theart performance on KG-to-text generation. 2 Related Work KG-to-Text Generation Recent studies on KG-to-text generation tasks mainly fall into three aspects: 1) Encoder modification: To alleviate the structural information loss of sequence encoders with the input of linearized graphs (Gardent et al., 2017; Trisedya et al., 2018; Moryossef et al., 2019), researchers focus on more complex encoder structures for better graph representations, such as graph neural networks (Marcheggiani and Perez-Beltrachini, 2018; Ribeiro et al., 2020b) and graph Transformers (Koncel-Kedziorski et al., 2019; Schmitt et al., 2020a). 2) Unsupervised training: researchers devise unsupervised training objectives to jointly learn the tasks of graph-to-text and textto-graph conversion with non-parallel graph-text data (Schmitt et al., 2020b; Guo et al., 2020; Jin et al., 2020). 3) Building pre-trained models: With the development of pre-trained NLG models such as GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), recent work directly fine-tunes these models on graph-totext datasets and"
2021.findings-acl.223,N19-1236,0,0.0570043,"Missing"
2021.findings-acl.223,P02-1040,0,0.109804,"each dataset as our baselines, including Seq2Seq with copying or delexicalisation (Shimorina and Gardent, 2018) for WebNLG v2.0, and G2S (Chen et al., 2020d) for WebQuestions and PathQuestions. We directly re-printed the results of baselines if they use the same datasets as ours. Otherwise, we implemented the baselines based on the codes and model parameters released by the original papers. We reported all the results of our implemented models with the mean values over 5 runs. 4.4 Automatic Evaluation We followed the existing work (Shimorina and Gardent, 2018; Chen et al., 2020d) to use BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004) as our automatic metrics. The main results on WebNLG, WebQuestions and PathQuestions are shown in Table 1. We can observe that JointGT based on BART / T5 can outperform vanilla BART / T5 on most of the metrics, respectively, and obtain the state-of-the-art performance on all the datasets. This indicates that our method can promote graph-text alignments and further enhance the performance of the state-of-theart pre-trained models on KG-to-text datasets. 4.5 Human Evaluation To further evaluate the quality of generated results, we condu"
2021.findings-acl.223,D19-1005,0,0.0242072,"re-trained models for KG-to-text generation, our model utilizes pre-training methods to explicitly learn graphtext alignments instead of directly fine-tuning textto-text pre-trained models on KG-to-text datasets. KG-Enhanced Pre-Trained Models Another line of related studies is pre-trained models enhanced by knowledge graphs for natural language understanding (NLU). The motivation of these models is to incorporate knowledge graphs into pre-trained models to facilitate the understanding of entities and relations in natural language. Early work including ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) directly uses fixed entity embeddings based on TransE (Bordes et al., 2013) or word vectors (Mikolov et al., 2013) during pre-training. Recent work like KEPLER (Wang et al., 2021) and JAKET (Yu et al., 2020) resorts to jointly pre-training graph-text representations. Specifically, they encode the textual descriptions of entities with pre-trained language models as entity embeddings and jointly optimize the knowledge embedding objective and the masked language modeling objective. In comparison, our model focuses on joint pretraining methods on knowledge graph encoding and sequence decoding in"
2021.findings-acl.223,2020.tacl-1.38,0,0.336959,"ata, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences, and directly fine-tune textto-text Transformer-based pre-trained models like GPT (Radford et al., 2018, 2019), BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) on KG-totext datasets (Ribeiro et al., 2020a; Kale and Rastogi, 2020). Benefiting from self-supervised pretraining on large-scale unlabelled text corpora, pretrained language models can generate high-quality texts via simply fine-tuning, and outperform other models with sophisticated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained models capture contextual information via bidirectional Transformers (Dev"
2021.findings-acl.223,2020.emnlp-main.577,0,0.0368561,"Missing"
2021.findings-acl.223,P16-1056,0,0.0453243,"Missing"
2021.findings-acl.223,N18-2074,0,0.183227,"Graph-Text Embedding Alignment ? Figure 3: Overview of our proposed pre-training tasks: (a) Graph enhanced text reconstruction: reconstructing the text sequence given the complete graph. (b) Text enhanced graph reconstruction: predicting the masked entities and relations of the corrupted graph conditioned on the complete text. (c) Graph-text embedding alignment: matching the embedding vectors of the knowledge graph and the text via Optimal Transport. The special token &lt;SEP&gt; is to separate the linearized graph and the text, while &lt;M&gt; denotes the placeholder for masked tokens. attention layer (Shaw et al., 2018): z˜il = |V| X l l βij (zjl W V S + qij W V R) j=1 exp(ulij ) l βij = P|V| l p=1 exp(uip ) ulij = zil W QS  l W KR zjl W KS + qij √ dk (3) 3.3 &gt; i = 1, 2, · · · , |V| where W QS , W KS , W V S , W KR , W V R are the weight matrices in the structure-aware selfattention. This layer integrates the contextual semantic representation of entities and relations based on the graph structure, thereby injecting the structural information into the vanilla Transformer layer. Finally, we use a residual layer to fuse semantic and structural representations of entities, and obtain the hidden states for th"
2021.findings-acl.223,W18-6543,0,0.295351,"OUGE BLEU METEOR ROUGE † † ‡ 36.00 65.00 29.45 30.96‡ 55.45‡ 44.51 70.94 29.61 31.48 55.42 46.04 73.06 28.78 30.55 55.12 76.10** 58.55 75.91 61.01** 72.31 30.02* 73.57** 28.95 45.01 46.32** 32.05** 31.29 55.60 54.47 BLEU 61.48‡ 63.74 58.95 65.89** 60.45 PathQuestions METEOR ROUGE 44.57‡ 77.72‡ 47.23 77.76 44.72 76.58 48.25** 45.38 78.87** 77.59 Table 1: Results on WebNLG, WebQuestions and PathQuestions. SOTA-NPT indicates the state-of-the-art performance from the baselines without pre-training. #Param means the number of model parameters. The results marked with †, ‡ and ] are re-printed from Shimorina and Gardent (2018), Chen et al. (2020d) and Chen et al. (2020b), respectively. - means that the results are not reported in the corresponding references. * indicates that our model significantly outperforms BART and T5 on the corresponding datasets (t-test, p &lt; 0.05), while ** means p &lt; 0.01. Wikipedia hyperlinks of entities in the sentences. The detailed statistics of KGTEXT are shown in Table 2. Dataset KGTEXT WebNLG(U) WebNLG(C) WebQuestions PathQuestions #Ent #Rel 1.8M 3,114 3,129 25,703 7,250 1,210 373 373 672 378 #Instances #Triples Length (Train / Valid / Test) 6.98M / 10K / 10K 27.2 20.2 34,352 / 4,316"
2021.findings-acl.223,2020.acl-main.712,1,0.718322,"our proposed structureaware semantic aggregation module, we fixed the pre-training tasks and compared our encoder with two Transformer-based encoders commonly used in the existing work: SeqEnc: This sequence encoder takes linearized graphs as input and ignores structural information (Ribeiro et al., 2020a; Kale and Rastogi, 2020). RelEnc: This relation-aware encoder regards the entity sequence as input and leverages the relation embedding into the self-attention layer. Both the entity and relation embedding vectors are directly learned as model parameters (Shaw et al., 2018; Zhu et al., 2019; Song et al., 2020). 4.6.2 Pre-Training Task Model JointGT (BART) w/o TextRecon w/o GraphRecon w/o OT w/ BARTPretrain w/ KGPTPretrain BLEU METEOR ROUGE 65.92 47.15 76.10 64.22 46.56 74.96 65.37 47.09 75.97 65.03 47.09 75.83 64.60 46.78 75.74 65.14 46.94 75.72 Table 6: Ablation test of three pre-training tasks on WebNLG(U), including text / graph reconstruction and graph-text alignments via OT. BARTPretrain / KGPTPretrain means using the pre-training tasks of BART / KGPT instead of our tasks on KGTEXT. Model #Param BLEU METEOR ROUGE JointGT (BART) 160M 65.92 47.15 76.10 w/ SeqEnc 140M 64.82 46.87 75.37 160M 65.17"
2021.findings-acl.223,N18-1059,0,0.0186059,"lits: the traditional split (Unconstrained) which guarantees that there is no overlap of input graphs among train / validation / test sets, and a more challenging split (Constrained) where the non-overlap constraint is applied to the triples of input graphs. We denoted these two data splits as WebNLG(U) and WebNLG(C) in our paper. We followed the preprocessing steps of the existing work (Chen et al., 2020b) to replace the underlines in the entities and relations with spaces, and split the entities and relations in a camel case into multiple words. WebQuestions: This dataset (Yih et al., 2016; Talmor and Berant, 2018) is the benchmark for question generation over knowledge bases (KBQG), whose purpose is to generate natural language questions about the corresponding knowledge graphs (Serban et al., 2016). It is constructed from two question answering datasets, i.e., WebQuestionsSP (Yih et al., 2016) and ComplexWebQuestions (Talmor and Berant, 2018). These two datasets contain natural language questions, SPARQL queries and answer entities. We converted the SPARQL query to return a subgraph, and used the same preprocessing steps and data splits as the existing work (Kumar et al., 2019; Chen et al., 2020d). Pa"
2021.findings-acl.223,P18-1151,0,0.0238869,"layer, and utilizes three pre-training tasks to explicitly learn graph-text alignments in the discrete and continuous spaces. • We conduct experiments on the datasets of KG-to-text generation including WebNLG, WebQuestions and PathQuestions. Results show that JointGT achieves new state-of-theart performance on KG-to-text generation. 2 Related Work KG-to-Text Generation Recent studies on KG-to-text generation tasks mainly fall into three aspects: 1) Encoder modification: To alleviate the structural information loss of sequence encoders with the input of linearized graphs (Gardent et al., 2017; Trisedya et al., 2018; Moryossef et al., 2019), researchers focus on more complex encoder structures for better graph representations, such as graph neural networks (Marcheggiani and Perez-Beltrachini, 2018; Ribeiro et al., 2020b) and graph Transformers (Koncel-Kedziorski et al., 2019; Schmitt et al., 2020a). 2) Unsupervised training: researchers devise unsupervised training objectives to jointly learn the tasks of graph-to-text and textto-graph conversion with non-parallel graph-text data (Schmitt et al., 2020b; Guo et al., 2020; Jin et al., 2020). 3) Building pre-trained models: With the development of pre-train"
2021.findings-acl.223,P16-2033,0,0.0165938,"o official data splits: the traditional split (Unconstrained) which guarantees that there is no overlap of input graphs among train / validation / test sets, and a more challenging split (Constrained) where the non-overlap constraint is applied to the triples of input graphs. We denoted these two data splits as WebNLG(U) and WebNLG(C) in our paper. We followed the preprocessing steps of the existing work (Chen et al., 2020b) to replace the underlines in the entities and relations with spaces, and split the entities and relations in a camel case into multiple words. WebQuestions: This dataset (Yih et al., 2016; Talmor and Berant, 2018) is the benchmark for question generation over knowledge bases (KBQG), whose purpose is to generate natural language questions about the corresponding knowledge graphs (Serban et al., 2016). It is constructed from two question answering datasets, i.e., WebQuestionsSP (Yih et al., 2016) and ComplexWebQuestions (Talmor and Berant, 2018). These two datasets contain natural language questions, SPARQL queries and answer entities. We converted the SPARQL query to return a subgraph, and used the same preprocessing steps and data splits as the existing work (Kumar et al., 201"
2021.findings-acl.223,P19-1139,0,0.0260899,"mpared with the existing work on pre-trained models for KG-to-text generation, our model utilizes pre-training methods to explicitly learn graphtext alignments instead of directly fine-tuning textto-text pre-trained models on KG-to-text datasets. KG-Enhanced Pre-Trained Models Another line of related studies is pre-trained models enhanced by knowledge graphs for natural language understanding (NLU). The motivation of these models is to incorporate knowledge graphs into pre-trained models to facilitate the understanding of entities and relations in natural language. Early work including ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019) directly uses fixed entity embeddings based on TransE (Bordes et al., 2013) or word vectors (Mikolov et al., 2013) during pre-training. Recent work like KEPLER (Wang et al., 2021) and JAKET (Yu et al., 2020) resorts to jointly pre-training graph-text representations. Specifically, they encode the textual descriptions of entities with pre-trained language models as entity embeddings and jointly optimize the knowledge embedding objective and the masked language modeling objective. In comparison, our model focuses on joint pretraining methods on knowledge graph"
2021.findings-acl.223,2020.acl-main.224,0,0.0321197,"re-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new stateof-the-art performance on various KG-to-text datasets1 . 1 Introduction Knowledge-graph-to-text (KG-to-text) generation aims to generate high-quality texts which are consistent with input graphs (Gardent et al., 2017). This task requires to simultaneously encode the graph structure and the content, and effectively leverage the input graphs in the decoding process (Zhao et al., 2020). As a major natural language generation (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / rela"
2021.findings-acl.223,C18-1171,1,0.845721,"rious KG-to-text datasets1 . 1 Introduction Knowledge-graph-to-text (KG-to-text) generation aims to generate high-quality texts which are consistent with input graphs (Gardent et al., 2017). This task requires to simultaneously encode the graph structure and the content, and effectively leverage the input graphs in the decoding process (Zhao et al., 2020). As a major natural language generation (NLG) task that connects knowledge graphs and texts, this task can further promote the applicability of knowledge graphs in more realistic NLG scenarios, such as knowledge-grounded dialogue generation (Zhou et al., 2018a) and story generation (Guan et al., 2019; Ji et al., 2020). Due to the limited amount of graph-text parallel data, it’s hard for typical neural text generation ∗ Corresponding author The data, codes, and model parameters are available at https://github.com/thu-coai/JointGT. 1 models to learn the alignments between source entities / relations and target tokens from scratch (Guo et al., 2020; Fu et al., 2020). Recent work resorts to constructing general-purpose pre-trained language models for KG-to-text generation. The most common and simple way is to linearize input graphs into text sequences"
2021.findings-acl.223,D19-1548,0,0.0634937,"ated structures. Despite the superior performance of fine-tuning pre-trained models on KG-to-text datasets, we argue that building pre-trained models for KG-totext generation still faces two major challenges: 1) Structural information loss during encoding. Most of the existing pre-trained models capture contextual information via bidirectional Transformers (Devlin et al., 2019), which include full attention connections. This model structure may neglect the structural information when encoding knowledge graphs since the relation between each pair of input entities is not explicitly considered (Zhu et al., 2019). 2) Absence of explicit graph-text alignments. Existing work on pre-trained models for text generation commonly adopts auto-encoding or auto-regressive text reconstruction to learn texttext alignments, which encodes the corrupted text sequence and decodes the original sequence (Lewis et al., 2020; Raffel et al., 2020). Since knowledge graphs may possess more complex structures than text sequences, it’s hard to explicitly learn graphtext alignments by directly using the pre-training tasks based on text reconstruction. Thus, we propose a graph-text joint represen2526 Findings of the Association"
2021.naacl-main.119,N19-1116,0,0.38966,"odality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction. Sentence: A squirrel jumps on stump. Bird sound (a) Sentence: Man starts to play the guitar fast. Guitar sound (b) Figure 1: Examples of video aided unsupervised grammar induction. We aim to improve the constituency parser by leveraging aligned video-sentence pairs. when applying to other domains (Fried et al., 2019). To address these issues, recent approaches (Shen et al., 2018b; Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019) design unsupervised constituency parsers and grammar inducers, since they can be 1 Introduction trained on large-scale unlabeled data. In particConstituency parsing is an important task in nat- ular, there has been growing interests in exploitural language processing, which aims to capture ing visual information for unsupervised grammar syntactic information in sentences in the form of induction because visual information can capture constituency parsing trees. Many conventional ap- important knowledge required for language learnproaches learn constituency parser from human"
2021.naacl-main.119,P19-1031,0,0.0143096,"rom different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction. Sentence: A squirrel jumps on stump. Bird sound (a) Sentence: Man starts to play the guitar fast. Guitar sound (b) Figure 1: Examples of video aided unsupervised grammar induction. We aim to improve the constituency parser by leveraging aligned video-sentence pairs. when applying to other domains (Fried et al., 2019). To address these issues, recent approaches (Shen et al., 2018b; Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019) design unsupervised constituency parsers and grammar inducers, since they can be 1 Introduction trained on large-scale unlabeled data. In particConstituency parsing is an important task in nat- ular, there has been growing interests in exploitural language processing, which aims to capture ing visual information for unsupervised grammar syntactic information in sentences in the form of induction because visual information can capture constituency parsing trees. Many conve"
2021.naacl-main.119,Q18-1016,1,0.473556,"each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction. Sentence: A squirrel jumps on stump. Bird sound (a) Sentence: Man starts to play the guitar fast. Guitar sound (b) Figure 1: Examples of video aided unsupervised grammar induction. We aim to improve the constituency parser by leveraging aligned video-sentence pairs. when applying to other domains (Fried et al., 2019). To address these issues, recent approaches (Shen et al., 2018b; Jin et al., 2018; Drozdov et al., 2019; Kim et al., 2019) design unsupervised constituency parsers and grammar inducers, since they can be 1 Introduction trained on large-scale unlabeled data. In particConstituency parsing is an important task in nat- ular, there has been growing interests in exploitural language processing, which aims to capture ing visual information for unsupervised grammar syntactic information in sentences in the form of induction because visual information can capture constituency parsing trees. Many conventional ap- important knowledge required for language learnproaches learn constitu"
2021.naacl-main.119,P19-1234,1,0.359842,"run of SENet154, I3D and MMC-PCFG. We can observe that SENet identifies all NPs but fails at the VP. I3D correctly predicts the VP but fails at recognizing a NP, “the man”. Our MMCPCFG can take advantages of all experts and produce the correct prediction. 5 Related Work Grammar Induction Grammar induction and unsupervised parsing has been a long-standing problem in computational linguistics (Carroll and Charniak, 1992). Recent work utilized neural networks in predicting constituency structures with no supervision (Shen et al., 2018a; Drozdov et al., 2019; Shen et al., 2018b; Kim et al., 2019; Jin et al., 2019a) and showed promising results. In addition to learning purely from text, there is a growing interest to use image information to improve accuracy of induced constituency trees (Shi et al., 2019; Kojima et al., 2020; Zhao and Titov, 2020; Jin and Schuler, 2020). Different from previous work, our work improves the constituency parser by using videos containing richer information than images. Video-Text Matching Video-text matching has been widely studied in various tasks, such as video 4 retrieval (Liu et al., 2019; Gabeur et al., 2020), moDifferent runs represent models trained with different"
2021.naacl-main.119,2020.aacl-main.42,1,0.595113,"knowledge required for language learnproaches learn constituency parser from human- ing that is ignored by text (Gleitman, 1990; Pinker annotated datasets such as Penn Treebank (Marcus and MacWhinney, 1987; Tomasello, 2003). This et al., 1993). However, annotating syntactic trees task aims to learn a constituency parser from raw by human language experts is expensive and time- unlabeled text aided by its visual context. consuming, while the supervised approaches are Previous methods (Shi et al., 2019; Kojima et al., limited to several major languages. In addition, 2020; Zhao and Titov, 2020; Jin and Schuler, 2020) the treebanks for training these supervised parsers learn to parse sentences by exploiting object inforare small in size and restricted to the newswire mation from images. However, images are static domain, thus their performances tend to be worse and cannot present the dynamic interactions among ∗ visual objects, which usually correspond to verb This work was done when Songyang Zhang was an intern at Tencent AI Lab. phrases that carry important information. There1513 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Langu"
2021.naacl-main.119,Q18-1019,0,0.0606338,"Missing"
D13-1108,P08-1009,0,0.0222484,"els (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrase"
D13-1108,P05-1033,0,0.864996,"P2 , VP3 indicate the phrases which can not be captured by dependency syntactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing the nodes with same phrasal categories. approach yields encouraging results by exploiting two types of trees. Large-scale experiments (Section 5) on Chinese-English translation show that our model significantly outperforms the state-ofthe-art single constituency-to-string model by averaged +2.45 BLEU points, dependency-to-string model by averaged +0.91 BLEU points, and hierarchical phrase-based model (Chiang, 2005) by averaged +1.12 BLEU points, on three Chinese-English NIST test sets. 2 Grammar We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. A headdependents relation consists of a head and all its dependents in dependency trees, and it can represent long distance dependencies. Incorporating phrasal nodes of constituency trees into head-dependents relations further enhances the compatibility with phrases of our rules. Figure 1 shows an example of phrases whic"
D13-1108,J07-2003,0,0.179871,"rt the input phrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the 1072 number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than β times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node,"
D13-1108,P05-1066,0,0.290508,"Missing"
D13-1108,P05-1067,0,0.529981,"ndencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which ar"
D13-1108,P12-1100,1,0.661115,"1). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constituency and dependency trees, while their work applied two types of trees on two sides. Instead, our model simultaneously utilizes constituency and dependency trees on the source side to direct the translation, which is concerned w"
D13-1108,W02-1039,0,0.349783,"l phrasal nodes from the constituency tree C to the dependency tree T , which can be easily accomplished by phrases mapping according to the common covered source sequences. As dependency trees can capture some phrasal information by dependency syntactic 1069 phrases, in order to complement the information that dependency trees can not capture, we only label the phrasal nodes that cover dependency non-syntactic phrases. Then, we annotate alignment information to the phrasal nodes labeled dependency tree T , as shown in Figure 4. For description convenience, we make use of the notion of spans (Fox, 2002; Lin, 2004). Given a node n in the source phrasal nodes labeled T with word alignment information, the spans of n induced by the word alignment are consecutive sequences of words in the target sentence. As shown in Figure 4, we annotate each node n of phrasal nodes labeled T with two attributes: node span and subtree span; besides, we annotate phrasal span to the parts covered by phrasal nodes in each subtree rooted at n. The three types of spans are defined as follows: Definition 1 Given a node n, its node span nsp(n) is the consecutive target word sequence aligned with the node n. æ Take t"
D13-1108,N04-1035,0,0.61655,"4: An annotated dependency tree. Each node is annotated with two spans, the former is node span and the latter subtree span. The fragments covered by phrasal nodes are annotated with phrasal spans. The nodes denoted by the solid line box are not nsp consistent. 3 Rule Extraction In this section, we describe how to extract rules from a set of 4-tuples hC, T, S, Ai, where C is a source constituency tree, T is a source dependency tree, S is a target side sentence, and A is an word alignment relation between T /C and S. We extract CHDR rules from each 4-tuple hC, T, S, Ai based on GHKM algorithm (Galley et al., 2004) with three steps: 1. Label the dependency tree with phrasal nodes from the constituency tree, and annotate alignment information to the phrasal nodes labeled dependency tree (Section 3.1). 2. Identify acceptable CHDR fragments from the annotated dependency tree for rule induction (Section 3.2). 3. Induce a set of lexicalized and generalized CHDR rules from the acceptable fragments (Section 3.3). 3.1 Annotation Given a 4-tuple hC, T, S, Ai, we first label phrasal nodes from the constituency tree C to the dependency tree T , which can be easily accomplished by phrases mapping according to the c"
D13-1108,N04-1014,0,0.441996,"del achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents"
D13-1108,P07-1019,0,0.118738,"hrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the 1072 number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than β times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we keep a topscoring subs"
D13-1108,2006.amta-papers.8,0,0.492272,"r the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that"
D13-1108,N03-1017,0,0.262119,"ording to the notion in the phrase-based model (Koehn et /NR), tsp( al., 2003). For example, nsp( /NN) and psp(NP1 ) are consistent while nsp( /JJ) and nsp( /NN) are not consistent. The annotation can be achieved by a single postorder transversal of the phrasal nodes labeled dependency tree. For simplicity, we call the annotated phrasal nodes labeled dependency tree annotated dependency tree. The extraction of bilingual phrases (including the translation of head node, dependency syntactic phrases and the fragment covered by a phrasal node) can be readily achieved by the algorithm described in Koehn et al., (2003). In the following, we focus on CHDR rules extraction.  ? 3.2 )P æ Intel 1 Before present the method of acceptable fragments identification, we give a brief description of CHDR fragments. A CHDR fragment is an annotated fragment that consists of a source head-dependents relation with/without constituency phrasal nodes, a target string and the word alignment information between the source and target side. We identify the acceptable CHDR fragments that are suitable for rule induction from the annotated dependency tree. We divide the acceptable CHDR fragments into two categories depending on w"
D13-1108,C04-1090,0,0.866617,"e) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-depend"
D13-1108,P06-1077,1,0.835734,"y improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our mode"
D13-1108,P09-1063,1,0.87343,"Missing"
D13-1108,P11-1128,1,0.890153,"Missing"
D13-1108,P08-1114,0,0.0207519,"2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and"
D13-1108,P10-1145,1,0.777695,"al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constit"
D13-1108,P08-1023,1,0.872168,"which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu"
D13-1108,P02-1038,0,0.524086,"le the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: Y P (d) ∝ φi (d)λi (1) i where φi are features defined on derivations and λi are feature weights. In our experiments of this paper, the features are used as follows: • CHDR rules translation probabilities P (t|s) and P (s|t), and CHDR rules lexical translation probabilities Plex (t|s) and Plex (s|t); • bilingual phrases translation probabilities Pbp (t|s) and Pbp (s|t), and bilingual phrases lexical translation proba"
D13-1108,J03-1002,0,0.00904566,"). Finally, we give detail analysis (Section 5.4). 5.1 Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric 2 . We parse the source sentences to constituency trees (without binarization) and projective dependency trees with Stanford Parser (Klein and Manning, 2002). The word alignments are obtained by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We get bilingual phrases from word-aligned data with algorithm described in Koehn et al. (2003) by running Moses Toolkit 3 . We apply SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram 1 Including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 2 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 3 http://www.statmt.org/moses/ System Moses-chart cons2str dep2str consdep2str Rule # 116.4M 25.4M+32.5M 19.6M+32.5M 23"
D13-1108,J04-4002,0,0.206297,"rules in Xie et al., (2011). CHDR-normal rules are equivalent with the head-dependents relation rules and the CHDRphrasal rules are the extension of these rules. For convenience of description, we use the subscript to distinguish the phrasal nodes with the same category, such as VP2 and VP3 . In actual operation, we use VP instead of VP2 and VP3 . We handle the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P (t|s) and P (s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: Y P (d) ∝ φi (d)λi (1) i where φi are features defined on deriv"
D13-1108,P03-1021,0,0.347804,"Missing"
D13-1108,P05-1034,0,0.638161,"BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with"
D13-1108,P08-1066,0,0.730547,"oy single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal nodes, and the tar"
D13-1108,D11-1020,1,0.723184,"Technology, Chinese Academy of Sciences §University of Chinese Academy of Sciences {mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn ‡Centre for Next Generation Localisation Faculty of Engineering and Computing, Dublin City University qliu@computing.dcu.ie Abstract quences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability of long distance reorderings. We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly imp"
D13-1108,W07-0706,1,0.921503,"els, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and incorporate constituents to them. Our model employs rules that represent the source side as head-dependents relations which are incorporated with constituency phrasal"
D13-1108,P01-1067,0,0.200551,"results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and seIn this paper, we propose to combine the advantages of source side constituency and dependency trees. Since the dependency tree is structurally simpler and directly represents long distance dependencies, we take dependency trees as the backbone and"
D13-1108,C12-1186,0,0.0116478,"translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) 1074 proposed a constituency-to-dependency tran"
D13-1108,2007.mtsummit-papers.71,0,0.0319027,"(means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 2g Ñy 2g Ñy 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based"
D13-1108,J08-3004,0,\N,Missing
D14-1021,W09-0437,0,0.0218632,"2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphology and paraphrasing all add to the difficulty of rule extraction. In addition, restricting target word orders by hard translation rules can also hurt output fluency. ∗ * Work done while visiting Singapore University of Technology and Design (SUTD) 177 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177–182, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics below λ · Pmax are filtered out, where Pmax is the probability of t"
D14-1021,P07-1020,0,0.0211782,"algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis comp"
D14-1021,P08-1024,0,0.0633776,"Missing"
D14-1021,J93-2003,0,0.0506297,"minimizing engineering efforts. Shown in Figure 1, the end-to-end system consists of two main components: lexical transfer and synthesis. The former provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the"
D14-1021,P05-1033,0,0.0949535,"et equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli"
D14-1021,E14-1028,0,0.178381,"Missing"
D14-1021,W11-2107,0,0.0896313,"Missing"
D14-1021,C12-1121,0,0.0359306,"Missing"
D14-1021,W99-0604,0,0.134086,"r provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target transl"
D14-1021,P05-1034,0,0.0509849,"thesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translat"
D14-1021,N13-1025,0,0.0341822,"Missing"
D14-1021,D08-1052,0,0.0180782,"ependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word"
D14-1021,W02-1039,0,0.0634627,"s for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target translation options for overlap"
D14-1021,N04-1035,0,0.0948608,"phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage p"
D14-1021,P06-1139,0,0.0331291,"urrently rather separated research fields. The system is not strongly dependent on the specific generation algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reorderin"
D14-1021,P12-2061,0,0.0155088,"actic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming th"
D14-1021,P09-1091,0,0.221722,"ng on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et a"
D14-1021,E09-1097,0,0.299711,"et word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SM"
D14-1021,D09-1043,0,0.0746518,"U comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis s"
D14-1021,J99-4005,0,0.137947,"ined consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing t"
D14-1021,J10-4005,0,0.0465794,"Missing"
D14-1021,P12-3004,1,0.847822,"Chinese-English dataset, which consists of training sentence pairs from the dialog task (dialog) and Basic Travel and Expression Corpus (BTEC). The union of dialog and BTEC are taken as our training set, which contains 30,033 sentence pairs. For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences. For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico"
D14-1021,D11-1020,1,0.858858,"both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphol"
D14-1021,P06-1096,0,0.0607434,"Missing"
D14-1021,P06-1077,1,0.766363,"s, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), whi"
D14-1021,W06-1606,0,0.034268,"output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibi"
D14-1021,J93-2004,0,0.0459525,"Missing"
D14-1021,D13-1112,0,0.0442095,"Missing"
D14-1021,D11-1106,1,0.885569,"of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), w"
D14-1021,E12-1075,1,0.701608,") translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. Acknowledgement The work has been supported by the Singapore Ministration of Education Tier 2 project T2"
D14-1021,P07-1002,0,\N,Missing
D14-1021,C14-1104,1,\N,Missing
D16-1224,D15-1198,0,0.142364,"ations, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence"
D16-1224,W13-2322,0,0.526909,"ing for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset. 1 ARG0 go-01 ARG0 boy Figure 1: AMR graph for “The boy wants to go”. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of"
D16-1224,C10-1012,0,0.0287772,"input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, L"
D16-1224,P13-2131,0,0.0614093,"tence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, each consisting of a rooted, connected AMR fragment and a corresponding translation. These rules serve in a similar way to rules in SMT models. We learn the rules by a modified version of the sampling algorithm of Peng 2084 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2084–2089, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al. (2015), and use the rule matching algorithm of Cai and Knight (2013). For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered. To generate a sentence for the whole AMR, we search for an ordered cut, and concatenate translations of all rules in the cut. TSP is used to traverse different cuts and determine the best order. Intuitively, our method is similar to phrase-based SMT, which first cuts the input sentence into phrases, then obtains the translation for each source phrase, before finally generating the target sente"
D16-1224,P14-1134,0,0.339027,"machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragment"
D16-1224,N16-1087,0,0.350788,"ammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, eac"
D16-1224,C12-1083,0,0.362052,"on (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been do"
D16-1224,N03-1017,0,0.0278261,"hose AMR fragments share common concepts. Otherwise the traveling cost is evaluated by a maximum entropy model, which will be discussed in detail in Section 2.4. 2.3 Rule Acquisition We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given 2086 an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f ), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discarding function words, while our task is AMR-to-text generation, and we need to be able to fill in these unaligned"
D16-1224,P02-1040,0,0.0987956,"have lower path length than others. 3 3.1 Dev 13.13 13.15 17.68 17.19 21.12 23.00 Test 16.94 14.93 18.09 17.75 22.44 23.00 Table 1: Main results. training instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same meaning. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. To solve the AGTSP, we use Or-tool3 . Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses4 to translate the linearized AMR into a sentence. To traverse the children of an AMR concept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMRgen5 (Flanigan et al., 2016), which is trai"
D16-1224,K15-1004,1,0.941008,"intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the tra"
D16-1224,D15-1136,0,0.160292,"-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally ge"
D16-1224,N15-3006,0,0.10334,"sentation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment"
D16-1224,E09-1097,0,0.0297715,"(2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and th"
D16-1224,N15-1040,0,0.0534278,"Missing"
D16-1224,D09-1043,0,0.0313619,"apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, Lin Zhao, and Yifan He. This work was funded by NSF IIS-144"
D16-1224,P09-1038,0,0.143191,"4 System PBMT OnlyConceptRule OnlyInducedRule OnlyBigramLM All JAMR-gen Traveling cost Considering an AGTSP graph whose nodes are clustered into m groups, we define the traveling cost for a tour T in Equation 1: cost(ns , ne ) = − m X log p(“yes”|nTi , nTi+1 ) (1) i=0 where nT0 = ns , nTm+1 = ne and each nTi (i ∈ [1 . . . m]) belongs to a group that is different from all others. Here p(“yes”|nj , ni ) represents a learned score for a move from nj to ni . The choices before nTi are independent from choosing nTi+1 given nTi because of the Markovian property of the TSP problem. Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi ) by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model. However, as a result, the number of nodes in the AGTSP graph grows exponentially. To tackle the problem, we treat it as a local binary (“yes” or “no”) classification problem whether we should move to nj from ni . We train a maximum entropy model, where p(“yes”|ni , nj ) is defined as: p(“yes”|ni , nj ) = k hX i 1 exp λi fi (“"
D16-1224,J15-3005,1,0.852028,"k here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds"
D18-1246,D17-1209,0,0.168044,"ia the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particul"
D18-1246,P10-1160,0,0.346512,"to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation extraction. As shown in Figure 1 (a), graphs are constructed from input sentences with dependency edges,"
D18-1246,N18-1082,0,0.0254715,"Missing"
D18-1246,D15-1205,0,0.0337129,"a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed"
D18-1246,W09-2415,0,0.154489,"Missing"
D18-1246,N07-1015,0,0.0610435,"tion in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary rela"
D18-1246,P14-1038,0,0.0304269,"Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng"
D18-1246,P14-5010,0,0.00328524,", or a multi-class classification problem of detecting which relation holds for the entity mentions. Take Table 1 as an example. The binary classification task is to determine whether gefitinib would have an effect on this type of cancer, given a cancer patient with 858E mutation on gene EGFR. The multi-class classification task is to detect the exact drug effect: response, resistance, sensitivity, etc. 3 Baseline: Bi-directional DAG LSTM Peng et al. (2017) formulate the task as a graphstructured problem in order to adopt rich dependency and discourse features. In particular, Stanford parser (Manning et al., 2014) is used to assign syntactic structure to input sentences, and heads of two consecutive sentences are connected to represent discourse information, resulting in a graph structure. For each input graph G = (V, E), the nodes V are words within input sentences, and each edge e 2 E connects two words that either have a relation or are adjacent to each other. Each edge is denoted as a triple (i, j, l), where i and j are the indices of the source and target words, respectively, and the edge label l indicates either a dependency or discourse relation (such as “nsubj”) or a relative position (such as"
D18-1246,D17-1159,0,0.24825,"between “exon-19” and “EGFR” via the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper paramet"
D18-1246,P05-1061,0,0.240902,"comparisons with the binary relation extraction results. However, the performance gaps between GS GLSTM and Bidir DAG LSTM dramatically increase, showing the superiority of GS GLSTM over Bidir DAG LSTM in utilizing context information. 7 B INARY 50.7 71.7* Table 6: Average test accuracies for multi-class relation extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relyin"
D18-1246,P16-1105,0,0.546468,"ral language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LST"
D18-1246,P15-1150,0,0.174145,"Missing"
D18-1246,W06-1671,0,0.0844934,"extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semanticall"
D18-1246,J05-1004,1,0.140836,"An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine"
D18-1246,D18-1110,1,0.848544,"ng node states sequentially: for each input graph, a start node and a node sequence are chosen, which determines the order of recurrent state updates. In contrast, our graph LSTM do not need ordering of graph nodes, and is highly parallelizable. 2233 Graph convolutional networks (GCNs) and very recently graph recurrent networks (GRNs) have been used to model graph structures in NLP tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017), text generation (Song et al., 2018), text representation (Zhang et al., 2018) and semantic parsing (Xu et al., 2018b,a). In particular, Zhang et al. (2018) use GRN to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs, showing that the representation is superior compared to BiLSTM on serialized AMR. Our work is in line with their work in the investigation of GRN on NLP. To our knowledge, we are the first to use GRN for representing dependency and discourse structures. Under"
D18-1246,Q17-1008,0,0.11233,"parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which co"
D18-1246,D14-1162,0,0.0965546,"or nonresponse”, “sensitivity”, “response”, “resistance” and “None”. We follow Peng et al. (2017) and binarize multi-class labels by grouping all relation classes as “Yes” and treat “None” as “No”. 6.2 Settings Following Peng et al. (2017), five-fold crossvalidation is used for evaluating the models,3 and the final test accuracy is calculated by averaging the test accuracies over all five folds. For each fold, we randomly separate 200 instances from the training set for development. The batch size is set as 8 for all experiments. Word embeddings are initialized with the 100-dimensional GloVe (Pennington et al., 2014) vectors, pretrained on 6 billion words from Wikipedia and web text. The edge label embeddings are 3-dimensional and randomly 2 The dataset is available at http://hanover.azurewebsites.net. 3 The released data has been separated into 5 portions, and we follow the exact split. 2230 Model Quirk and Poon (2017) Peng et al. (2017) - EMBED Peng et al. (2017) - FULL + multi-task Bidir DAG LSTM GS GLSTM Figure 3: Dev accuracies against transition steps for the graph state LSTM model. initialized. Pretrained word embeddings are not updated during training. The dimension of hidden vectors in LSTM units"
D18-1246,P13-1147,0,0.0141763,"ond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct"
D18-1246,E17-1110,0,0.142755,"ation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows"
D18-1246,P18-1150,1,0.937565,"s lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state,"
D18-1246,D17-1182,1,0.86292,", relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation"
D18-1246,P18-1030,1,0.910776,"ginal subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state, with words in the gr"
D18-1246,P05-1052,0,0.0482608,"at tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentio"
D18-1246,R11-1004,0,0.0888552,"instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semantically meaningful parts. The nodes of"
D19-1020,D17-1209,0,0.142049,"endency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary informati"
D19-1020,P18-1026,0,0.0260887,"nce. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative i"
D19-1020,H05-1091,0,0.867277,"the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (a) nmod comp comp nmod comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (b) Figure 1: (a) 1-best dependency tree and (b) dependency forest for a medical-domain sentence, where edge label “comp” represents “compound”. Associated mentions are in different colors. Some irrelevant words and edges are omitted for simplicity. Previous work has shown that dependency syntax is important for guiding relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Liu et al., 2015; Gormley et al., 2015; Xu et al., 2015a,b; Miwa and Bansal, 2016; Zhang et al., 2018b), especially in biological and medical domains (Quirk and Poon, 2017; Peng et al., 2017; Song et al., 2018b). Compared with sequential surface-level structures, such as POS tags, dependency trees help to model word-toword relations more easily by drawing direct connections between distant words that are syntactically correlated. Take the phrase “effect on the medicine” for example; “effect” and “medicine” are directly connected in a dependency tree, regardless of how many modifiers are adde"
D19-1020,W11-2905,0,0.0727527,"Missing"
D19-1020,W11-0216,0,0.0751446,"Missing"
D19-1020,C96-1058,0,0.0517257,"w recall given an imperfect parser. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the fi"
D19-1020,D15-1205,1,0.779495,"Missing"
D19-1020,P19-1024,0,0.154261,"Missing"
D19-1020,W09-2415,0,0.0207045,"Missing"
D19-1020,W05-1506,0,0.102938,"r. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the final K-bests by combining identical dependen"
D19-1020,D15-1137,0,0.031279,"2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semant"
D19-1020,I05-1006,0,0.184,"Missing"
D19-1020,Q17-1029,1,0.895317,"Missing"
D19-1020,P15-2047,0,0.103796,"Missing"
D19-1020,D11-1149,0,0.0191059,"ms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine"
D19-1020,P18-1249,0,0.0333378,"Missing"
D19-1020,D17-1159,0,0.500712,"iment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of"
D19-1020,J93-2004,0,0.064101,"able 1: Statistics on forests generated with various (upper half) and K (lower half) on the development set. • D EP T REE: Our baseline using 1-best dependency trees, as shown in Section 4. 7.4 • E DGEWISE PS and E DGEWISE: Our models using the forests generated by our E DGEWISE algorithm with or without parser scores. • KB EST E ISNER PS and KB EST E ISNER: Our model using the forests generated by our KB EST E ISNER algorithm with or without parser scores, respectively. 7.3 Settings We take a state-of-the-art deep biaffine parser (Dozat and Manning, 2017), trained on the Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) converted to Universal Dependency, to obtain 1-best trees and full search spaces for generating forests. Using standard PTB data split (02–21 for training, 22 for development and 23 for testing), it gives UAS and LAS scores of 95.7 and 94.6, respectively. For the other hyper-parameters, word embeddings are initialized with the 200-dimensional BioASQ vectors5 , pretrained on 10M abstracts of biomedical articles, and are fixed during training. The dimension of hidden vectors in Bi-LSTM is set to 200, and the number of message passing steps T is set to 2 based on Zhang et al. (2018b). We use Ada"
D19-1020,P08-2026,0,0.0861827,"Missing"
D19-1020,W16-3009,0,0.0314234,"Missing"
D19-1020,C10-1123,0,0.0261185,"ature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering"
D19-1020,P08-1023,0,0.0537829,"t al., 2017) and a recent dataset focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees f"
D19-1020,N18-1080,0,0.246962,"present an edge for simplicity, and p✏ is the parser probability for edge ✏. The edge probabilities are not adjusted during end-task training. 6 Training Relation loss Given a set of training instances, each containing a sentence s with two target mentions ⇠ and ⇣, and a dependency structure D (tree or forest), we train our models with a crossentropy loss between the gold-standard relations r and model distribution: lR = log p(r|s, ⇠, ⇣, D; ✓), (13) where ✓ represents the model parameters. Using additional NER loss For training on BioCreative VI CPR, we follow previous work (Liu et al., 2017; Verga et al., 2018) to take NER loss as additional supervision, though the mention boundaries are known during testing. lN ER = N 1 X log p(tn |s, D; ✓), N (14) n=1 where tn is the gold NE tag of wn with the “BIO” scheme. Both losses are conditionally independent given the deep features produced by our Experiments We conduct experiments on two medical benchmarks to test the usefulness of dependency forest. 7.1 Data BioCreative VI CPR (Krallinger et al., 2017) This task2 focuses on the relations between chemical compounds (such as drugs) and proteins (such as genes). The full corpus contains 1020, 612 and 800 ext"
D19-1020,P16-1105,0,0.0839479,"Missing"
D19-1020,Q17-1008,0,0.357248,"on. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two."
D19-1020,D15-1062,0,0.231787,"Missing"
D19-1020,E17-1110,0,0.0869635,"wed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two. The output is a relation from a predefined relation set R = (r1 , . . . , rM , None), where “None” means that no relation holds for the entities. Two steps are taken for predicting the correct relation given an input sentence. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode bot"
D19-1020,W08-0504,0,0.0520198,"Missing"
D19-1020,C10-2133,0,0.0320316,"from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature. 1 Introduction The sheer amount of medical articles and their rapid growth prevent researchers from receiving comprehensive literature knowledge by direct reading. This can hamper both medical research and clinical diagnosis. NLP techniques have been used for automating the knowledge extraction process from the medical literature (Friedman et al., 2001; Yu and Agichtein, 2003; Hirschman et al., 2005; Xu et al., 2010; Sondhi et al., 2010; Abacha and Zweigenbaum, 2011). Along this line of work, a long-standing task is relation extraction, which mines factual knowledge from free text by labeling relations between entity mentions. As shown in Figure 1, the sub-clause “previously observed cytochrome P450 3A4 ( CYP3A4 ) interaction of the dual orexin receptor antagonist almorexant” contains two entities, namely “orexin receptor” and “almorexant”. There is an “adversary” relation between these two entities, denoted as“CPR:6”. ⇤ Yue Zhang is the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin rec"
D19-1020,Q19-1002,1,0.928194,"e usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated"
D19-1020,P18-1150,1,0.94662,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,D18-1110,1,0.810915,"neration (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi repres"
D19-1020,D15-1206,0,0.201939,"Missing"
D19-1020,C18-1120,0,0.0200441,"focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labe"
D19-1020,D18-1246,1,0.942065,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,P18-1030,1,0.867698,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1020,N19-1152,0,0.0439742,"Missing"
D19-1020,D18-1244,0,0.376206,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
K15-1004,W13-2322,0,0.191247,"f starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information within limited context. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997). Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. HRG has great potential for applications in natural language unIntroduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of the edge-labeled representation of an AMR graph where the edges are labeled while the nodes are not. The label of the leaf edge going out of a node represents the concept of the node, and the label of a non-leaf edge shows the relation between the concepts of the two nodes it connects to. This formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jo"
K15-1004,P13-2131,0,0.330005,"elow it. And there is no explicit order of composition with each branch. Another constraint we have tried is to attach all unaligned edges to the head node concept. The problem with this constraint is that it is very hard to generalize and introduces a lot of additional redundant relation edges. As for sampling, we initialize all cut variables in the forest as 1 (except for nodes that are marked as nosample cut, which indicates we initialize it with 0 and keep it fixed) and uniformly sample an incoming edge for each node. We evaluate the performance of our SHRG-based parser using Smatch v1.0 (Cai and Knight, 2013), which evaluates the precision, recall and F 1 of the concepts and relations all together. Table 1 shows the dev results of our sampled grammar using different lexical rules that maps substrings to graph fragments. Concept id only is the result of using the concepts identified by Flanigan et al. (2014). From second line, we replace the concept identification result with the lexical rules we have extracted from the training data (except for named entities and time expressions). +MCMC shows the result using additional alignments identified using our sampling approach. We can see that using the"
K15-1004,P13-1091,0,0.30562,"tations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jointly encodes a set of selected semantic phenomena which renders it useful in applications like question answering and semantics-based machine translation. 32 Proceedings of the 19th Conference on Computational Language Learning, pages 32–41, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics derstanding and generation, and also semanticsbased machine translation. Given a graph as input, finding its derivation of HRG rules is NP-complete (Drewes et al., 1997). Chiang et al. (2013) describe in detail a graph recognition algorithm and present an optimization scheme which enables the parsing algorithm to run in polynomial time when the treewidth and degree of the graph are bounded. However, there is still no real system available for parsing large graphs. An SHRG can be used for AMR graph parsing where each SHRG rule consists of a pair of a CFG rule and an HRG rule, which can generate strings and AMR graphs in parallel. Jones et al. (2012) present a Syntactic Semantic Algorithm that learns SHRG by matching minimal parse constituents to aligned graph fragments and incremen"
K15-1004,D14-1048,0,0.0444466,"based parser and present preliminary results of a graph-grammar-based approach. 1 ARG0 ARG0 girl ARG1 boy Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second step, a transition-based algorithm is used to greedily modify the dependency tree into an AMR graph. The benefit of starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information w"
K15-1004,J14-1007,1,0.906562,"Missing"
K15-1004,N15-1040,0,0.558357,"raph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second step, a transition-based algorithm is used to greedily modify the dependency tree into an AMR graph. The benefit of starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information within limited context. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes"
K15-1004,N09-1062,0,0.0250222,"nce, AMR graph) pair for “The boy wants the girl to believe him” sents the current node is internal to an SHRG rule, while 1 represents the current node is the boundary of two SHRG rules. Let all the edge variables form the random vector Y and all the cut variables form the random vector Z. Given an assignment y to the edge variables and assignment z to the cut variables, our desired distribution is proportional to the product of weights of the rules specified by the assignment: MCMC sampling Sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of vari"
K15-1004,P14-1134,0,0.241304,"phrase-based machine translation and come up with an efficient algorithm to learn graph grammars from string-graph pairs. We propose an effective approximation strategy to resolve the complexity issue of graph compositions. We also show some useful strategies to overcome existing problems in an SHRG-based parser and present preliminary results of a graph-grammar-based approach. 1 ARG0 ARG0 girl ARG1 boy Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second s"
K15-1004,C12-1083,0,0.0439477,"Missing"
K15-1004,N03-1017,0,0.0118933,"d word. After this alignment, there are also left-over edges that are not aligned from any substrings, which are called unaligned edges. Given an aligned string, AMR graph pair, a phrase-fragment pair n is a pair ([i, j], f ) which defines a pair of a phrase [i, j] and a fragment f such that words in positions [i, j] are only aligned to concepts in the fragment f and vice versa (with unaligned words and edges omitted). A fragment forest H = hV, Ei is a hypergraph made of a set of hypernodes V and hyperedges E. Each node n = ([i, j], f ) is tight on the string side similar to the definition by Koehn et al. (2003), i.e., n contains no unaligned words at its boundaries. Note here we do not have the constraint that f should be connected or single rooted, but we will deal with these constraints separately in the sampling procedure. We define two phrases [i1 , j1 ], [i2 , j2 ] to be adjacent if word indices {j1 , j1 + 1, . . . , i2 − 1} are all unaligned. We also define two fragments f1 = hV1 , E1 i, f2 = hV2 , E2 i to be disjoint if E1 ∩ E2 = ∅. And f1 and f2 are adjacent if they are disjoint and f = hV1 ∪ V2 , E1 ∪ E2 i is connected. We also define the compose operation of two nodes: it takes two nodes n"
K15-1004,P03-1021,0,0.0290647,"[X1-1, n] | (. :a/and :op1 [X1-1, 1] :op2 [X1-1, 2] · · · :opn [X1-1, n]) where the HRG side is a :a/and coordinate structure of X1-1s connected with relation :ops. 5 Precision 0.37 0.57 0.60 0.59 Experiments We use the same newswire section of LDC2013E117 as Flanigan et al. (2014), which 39 JAMR Wang et al. Our approach Precision 0.67 0.64 0.59 Recall 0.58 0.62 0.57 F-score 0.62 0.63 0.58 a graph language model into our CFG decoder, which should also help improve the performance. All the weights of the local features mentioned in Section 4.2 are tuned by hand. We have tried tuning with MERT (Och, 2003), but the computation of smatch score for the k-best list has become a major overhead. This issue might come from the NP-Completeness of the problem smatch tries to evaluate, unlike the simple counting of N-grams in BLEU (Papineni et al., 2001). Parallelization might be a consideration for tuning smatch score with MERT. Table 2: Comparisons of smatch score results alignments of length 6 on the string side from our constructed forest. We can see that using this alignment table further improves the smatch score. This is because the larger phrase-fragment pairs can make better use of the dependen"
K15-1004,D14-1180,1,0.858979,"to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of variables: an edge variable en representing which incoming hyperedge is chosen at a given node n in the forest (allowing us to sample one tree from a forest) and a cut variable zn representing whether node n in forest is a boundary between two SHRG rules or is internal to an SHRG rule (allowing us to sample rules from a tree). Figure 5 shows one sampled derivation from the forest. We have sampled one tree from the forest using the edge variables. We also have a 0-1 variable at each node in this tree where 0 reprePt (Y = y, Z = z) ∝ Y"
K15-1004,P09-2012,1,0.751774,"r for “The boy wants the girl to believe him” sents the current node is internal to an SHRG rule, while 1 represents the current node is the boundary of two SHRG rules. Let all the edge variables form the random vector Y and all the cut variables form the random vector Z. Given an assignment y to the edge variables and assignment z to the cut variables, our desired distribution is proportional to the product of weights of the rules specified by the assignment: MCMC sampling Sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of variables: an edge variable"
K15-1004,J07-2003,0,\N,Missing
K15-1004,D08-1076,0,\N,Missing
N18-2090,P16-1014,0,0.046067,"wer, which encodes all words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function f"
N18-2090,D16-1264,0,0.134063,"ng et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives better BLEU scores than the state of the art. Furthermore, the questions generated by our model help to improve a strong extractive QA system. Our code is available at https://github.com/freesunshine0316/MPQG. Work done during an internship at IBM. 569 Proceedings of NAACL-HLT 2018, pages 569–574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Baseline: sequence-to-sequence Our baseline is a sequence-to-sequence model (Bahdanau et al., 2015) with the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016). It uses an LSTM"
N18-2090,N10-1086,0,0.479507,"born. This can be easily determined by leveraging the contextual information of “10 july 1856 – 7 january 1943”, while it is relatively hard when only the answer position information is adopted. Introduction The task of natural question generation (NQG) is to generate a fluent and relevant question given a passage and a target answer. Recently NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target ans"
N18-2090,D17-1090,0,0.106253,"Missing"
N18-2090,P17-1123,0,0.132216,"tly NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input be"
N18-2090,P16-1154,0,0.116662,"words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function fm to match two vec"
N18-2090,P17-1096,0,0.0263172,"ic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different pe"
N18-2090,W17-2603,0,0.0998808,"asing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives bette"
P17-2002,2006.amta-papers.8,0,0.0358799,"istinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ,"
P17-2002,D15-1198,0,0.0124762,"s. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3#"
P17-2002,C12-1083,0,0.0608432,"Missing"
P17-2002,W13-2322,0,0.227385,"nto a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syn"
P17-2002,N03-1017,0,0.0322761,"rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set. 2.3 t where g denotes the input AMR, fi (·, ·) and wi represent a feature and the corresponding weight, respectively. The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3). The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005). We perform bottom-up search to transduce input AMRs to surface strings. Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score. Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam. Concept Rules and Glue Rules In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations. For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of th"
P17-2002,P05-1033,0,0.0721117,"ubscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node"
P17-2002,W15-4502,0,0.0386307,"to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the"
P17-2002,P06-1077,0,0.016854,"non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a correspondi"
P17-2002,D13-1108,1,0.889096,"Missing"
P17-2002,N16-1087,0,0.26532,"versity of Technology and Design Abstract This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result. 1 #X3# #X3# ARG1 #X2# go-01 #X2# ARG0 ARG0 #X1# ARG0 boy want-01 ARG0 want-01 ARG0 ARG1 #X1# go-01 ARG1 go-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the"
P17-2002,P14-1134,0,0.0267127,"oy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2#"
P17-2002,P03-1021,0,0.0915657,"want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0 (b / boy)) E the boy 1 #X# wants 2 3 #X# to go 4 5 6 the boy wants 7 8 Table 1: Example rule set 9 10 11 AMR graphs and generate output strings according to the learned grammar. Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding. It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset. 2 2.1 12 13 Data: training corpus C Result: rule instances R R ← []; for (Sent, AM R, ∼) in C do Rcur ← F RAGMENT E XTRACT(Sent,AM R,∼); for ri in Rcur do R.APPEND(ri ) ; for rj in Rcur /{ri } do if ri .C ONTAINS(rj ) then rij ← ri .COLLAPSE(rj ); R.APPEND(rij ) ; end end end end Algorithm 1: Rule extraction (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process"
P17-2002,P03-1011,1,0.658507,"o S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels o"
P17-2002,P02-1040,0,0.101213,"connected by edge l. 3.3 Dev 21.12 23.00 25.24 16.75 23.99 23.48 25.09 Experiments Setup We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances. Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner. Rules are extracted from the training data, and model parameters are tuned on the dev set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50. We investigate the effectiveness of rules and features by ablation tests: “NoInducedRule” does not adopt induced rules, “NoConceptRule” does not adopt concept rules, “NoMovingDistance” does not adopt the moving distance feature, and “NoReorderModel” disables the reordering model. Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate 4.3 Grammar analysis We have shown the effectiveness of our synchronous node replacement grammar (SNRG)"
P17-2002,P16-1001,0,0.0154203,"aph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy want"
P17-2002,K15-1004,1,0.839012,"ges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3#"
P17-2002,P15-1143,0,0.0161628,"antic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go"
P17-2002,E17-1035,1,0.853477,"s such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction pr"
P17-2002,W16-6603,0,0.200621,"), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0"
P17-2002,D16-1065,0,0.0438864,"useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2:"
P17-2002,D15-1136,0,0.0142351,"ons between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1"
P17-2002,P08-1066,0,0.00990465,"ances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string ov"
P17-2002,D16-1224,1,0.913448,"o-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (N"
P17-2002,D16-1112,0,0.0090019,"At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedi"
P17-2002,W15-3504,0,0.363273,"Missing"
P17-2002,N15-3006,0,0.0208597,", “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0"
P17-2002,N15-1040,0,0.0145351,"nt concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String:"
P17-2002,J97-3002,0,0.0460913,"nly one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, co"
P17-2002,D11-1020,0,0.0167829,"ws an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪ ∆ and ∼ den"
P17-2002,P02-1039,0,\N,Missing
P17-2002,P13-2131,0,\N,Missing
P18-1030,N16-1030,0,0.121858,"Missing"
P18-1030,D17-1209,0,0.0649709,"ention and stacked CNN in this respect, incrementally refining sentence representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have been used for computer program verification (Li et al., 2016) and image object detection (Liang et al., 2016). The closest previous work in NLP includes the use of convolutional neural networks (Bastings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahda"
P18-1030,P15-1033,0,0.0235531,"on and sequence labelling show that S-LSTM gives better accuracies compared to BiLSTM using the same number of parameters, while being faster. We release our code and models at https://github.com/ leuchine/S-LSTM, which include all baselines and the final model. 2 Related Work LSTM (Graves and Schmidhuber, 2005) showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models (Bahdanau et al., 2015). LSTM encoders have since been explored for other tasks, including syntactic parsing (Dyer et al., 2015), text classification (Yang et al., 2016) and machine reading (Hermann et al., 2015). Bidirectional extensions have become a standard configuration for achieving state-of-the-art accuracies among various tasks (Wen et al., 2015; Ma and Hovy, 2016; Dozat and Manning, 2017). SLSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words, but different in the design of state transition. CNNs (Krizhevsky et al., 2012) also allow better parallelisation compared to LSTMs for sentence encoding (Kim, 2014), thanks to parallelism among convolution filters. On the other hand,"
P18-1030,P17-1001,0,0.0290537,"Missing"
P18-1030,D15-1104,0,0.087169,"Missing"
P18-1030,P16-1101,0,0.19955,"he final model. 2 Related Work LSTM (Graves and Schmidhuber, 2005) showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models (Bahdanau et al., 2015). LSTM encoders have since been explored for other tasks, including syntactic parsing (Dyer et al., 2015), text classification (Yang et al., 2016) and machine reading (Hermann et al., 2015). Bidirectional extensions have become a standard configuration for achieving state-of-the-art accuracies among various tasks (Wen et al., 2015; Ma and Hovy, 2016; Dozat and Manning, 2017). SLSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words, but different in the design of state transition. CNNs (Krizhevsky et al., 2012) also allow better parallelisation compared to LSTMs for sentence encoding (Kim, 2014), thanks to parallelism among convolution filters. On the other hand, convolution features embody only fix-sized local ngram information, whereas sentence-level feature aggregation via pooling can lead to loss of information (Sabour et al., 2017). In contrast, S-LSTM uses a global sentence-level node to assemble a"
P18-1030,D17-1159,0,0.0554216,"in this respect, incrementally refining sentence representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have been used for computer program verification (Li et al., 2016) and image object detection (Liang et al., 2016). The closest previous work in NLP includes the use of convolutional neural networks (Bastings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahdanau et al. (2015) in introducin"
P18-1030,J93-2004,0,0.0610428,"Missing"
P18-1030,P14-1062,0,0.0717103,"Association for Computational Linguistics Attention (Bahdanau et al., 2015) has recently been explored as a standalone method for sentence encoding, giving competitive results compared to Bi-LSTM encoders for neural machine translation (Vaswani et al., 2017). The attention mechanism allows parallelisation, and can play a similar role to the sentence-level state in S-LSTMs, which uses neural gates to integrate word-level information compared to hierarchical attention. S-LSTM further allows local communication between neighbouring words. Hierarchical stacking of CNN layers (LeCun et al., 1995; Kalchbrenner et al., 2014; Papandreou et al., 2015; Dauphin et al., 2017) allows better interaction between non-local components in a sentence via incremental levels of abstraction. S-LSTM is similar to hierarchical attention and stacked CNN in this respect, incrementally refining sentence representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have b"
P18-1030,D14-1181,0,0.0498126,"astings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahdanau et al. (2015) in introducing a neural representation to the NLP literature. At each recurrent step, information exchange is conducted between consecutive words in the sentence, and between the sentence-level state and each word. In particular, each word receives information from its predecessor and successor simultaneously. From an initial state without information exchange, each word-level state can obtain 3-gram, 5-gram and 7-gram information after 1, 2 and 3 recurrent steps, respectively. Being connected with every word, the sentence-level state vector serves to exchange non-l"
P18-1030,W17-3204,0,0.0216347,"g benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers. 1 0 1 ... tim e ... ... ... ... ... t-1 ... ... ... t Figure 1: Sentence-State LSTM dustry. In addition, local ngrams, which have been shown a highly useful source of contextual information for NLP, are not explicitly modelled (Wang et al., 2016). Finally, sequential information flow leads to relatively weaker power in capturing longrange dependencies, which results in lower performance in encoding longer sentences (Koehn and Knowles, 2017). Introduction Neural models have become the dominant approach in the NLP literature. Compared to handcrafted indicator features, neural sentence representations are less sparse, and more flexible in encoding intricate syntactic and semantic information. Among various neural networks for encoding sentences, bi-directional LSTMs (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been a dominant method, giving state-of-the-art results in language modelling (Sundermeyer et al., 2012), machine translation (Bahdanau et al., 2015), syntactic parsing (Dozat and Manning, 2017) and question answering (Ta"
P18-1030,W14-1609,0,0.0386609,"Missing"
P18-1030,Q17-1008,0,0.0295337,"nce representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have been used for computer program verification (Li et al., 2016) and image object detection (Liang et al., 2016). The closest previous work in NLP includes the use of convolutional neural networks (Bastings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahdanau et al. (2015) in introducing a neural representation to the N"
P18-1030,D14-1162,0,0.0860755,"Missing"
P18-1030,P15-1150,0,0.348017,"Missing"
P18-1030,P17-1161,0,0.0318105,"Missing"
P18-1030,W09-1119,0,0.150791,"Missing"
P18-1030,C16-1229,0,0.0546673,"Missing"
P18-1030,P17-1194,0,0.0315892,"Missing"
P18-1030,D15-1199,0,0.0609081,"Missing"
P18-1030,W03-0419,0,0.191843,"Missing"
P18-1030,N16-1174,0,0.475386,"M gives better accuracies compared to BiLSTM using the same number of parameters, while being faster. We release our code and models at https://github.com/ leuchine/S-LSTM, which include all baselines and the final model. 2 Related Work LSTM (Graves and Schmidhuber, 2005) showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models (Bahdanau et al., 2015). LSTM encoders have since been explored for other tasks, including syntactic parsing (Dyer et al., 2015), text classification (Yang et al., 2016) and machine reading (Hermann et al., 2015). Bidirectional extensions have become a standard configuration for achieving state-of-the-art accuracies among various tasks (Wen et al., 2015; Ma and Hovy, 2016; Dozat and Manning, 2017). SLSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words, but different in the design of state transition. CNNs (Krizhevsky et al., 2012) also allow better parallelisation compared to LSTMs for sentence encoding (Kim, 2014), thanks to parallelism among convolution filters. On the other hand, convolution features embody only fix-siz"
P18-1030,D12-1110,0,0.173199,"Missing"
P18-1030,D11-1014,0,0.115743,"Missing"
P18-1030,P11-2009,0,0.0376289,"Missing"
P18-1150,W13-2322,0,0.343839,"le to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature. 1 :ARG0 :ARG1 :ARG2 person :name name genius :op1 ""Ryan"" Figure 1: An example of AMR graph meaning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the"
P18-1150,D17-1209,0,0.0587751,"to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to"
P18-1150,S16-1186,0,0.439676,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,N16-1087,0,0.285942,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,S17-2159,0,0.0529915,"n is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a l"
P18-1150,P16-1154,0,0.593108,"tional Linguistics (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our mode"
P18-1150,P16-1014,0,0.452163,"s (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our model is consistently better"
P18-1150,C12-1083,0,0.109094,"Missing"
P18-1150,N03-1017,0,0.00861135,"We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, SNRG (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results. Graph2seq+charLSTM+copy achieves a BLEU score of 23.3, which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus. In addition, our model without character LSTM is still 0.7 BLEU points higher than MSeq2seq+Anon. Note that MSeq2seq+Anon relies on anonymization, which"
P18-1150,P17-1014,0,0.218578,"s can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a linear recurrent neural network to automatically induce their original connections from bracketed string forms. To address this issue, we introduce a novel graph-to-sequence model, where a graph-state LSTM is used to encode AMR structure"
P18-1150,S17-2096,0,0.214613,"Missing"
P18-1150,W15-4502,0,0.0367213,"nius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and"
P18-1150,D17-1159,0,0.0610451,"rovide staff and funding for the research center . S2S: agreed to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN."
P18-1150,P05-1012,0,0.0220157,"y incoming or outgoing edges are used. From the results, we can see that there is a huge drop when state transition is performed only with incoming or outgoing edges. Using edges of one direction, the node states only contain information of ancestors or descendants. On the other hand, node states contain information of ancestors, descendants, and siblings if edges of both directions are used. From the results, we can conclude that not only the ancestors and descendants, but also the siblings are important for modeling the AMR graphs. This is similar to observations on syntactic parsing tasks (McDonald et al., 2005), where sibling features are adopted. We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-"
P18-1150,S17-2158,0,0.0361727,"MR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It"
P18-1150,P02-1040,0,0.103679,"onstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or linearized token. Data We use a standard AMR corpus (LDC2015E86) as our experimental dataset, which contains 16,833 instances for training, 1368 for dev"
P18-1150,Q17-1008,0,0.0361108,"te LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to future work. In this work our main goal is to show that graph LSTM encoding of AMR is superior compared with sequence LSTM. Closest to our work, Peng et al. (2017) modeled syntactic and discourse structures using DAG LSTM, which can be viewed as extensions to tree LSTMs (Tai et al., 2015). The state update follows the sentence order for each node, and has sequential nature. Our state update is in parallel. In addition, Peng et al. (2017) split input graphs into separate DAGs before their method can be used. To our knowledge, we are the first to apply an LSTM structure to encode AMR graphs. The recurrent information exchange mechanism in our state transition process is remotely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999)."
P18-1150,D14-1162,0,0.0936009,"19.9 20.6 20.4 22.2 22.1 22.8 Time 35.4s 37.4s 39.7s 11.2s 11.1s 9.2s 16.3s Table 1: D EV BLEU scores and decoding times. AMRs, as the AMR parser of Konstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or line"
P18-1150,W16-6603,0,0.403474,"(Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as par"
P18-1150,P17-1099,0,0.0409756,"tely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymize"
P18-1150,P17-2002,1,0.902838,"event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and"
P18-1150,N18-2090,1,0.825928,"opagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymized reviewers for the insightful comments, and"
P18-1150,P15-1150,0,0.208715,"Missing"
P18-1150,D16-1112,0,0.0316661,"aning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2"
P18-1150,W15-3504,0,0.0515809,"Missing"
P18-1150,P16-1008,0,0.0232013,". ; aN ] (4) where N is the number of input tokens. The decoder yields an output sequence w1 , w2 , . . . , wM by calculating a sequence of hidden states s1 , s2 . . . , sM recurrently. While generating the t-th word, the decoder considers five factors: (1) the attention memory A; (2) the previous hidden state of the LSTM model st 1 ; (3) the embedding of the current input (previously generated word) et ; (4) the previous context vector µt 1 , which is calculated with attention from A; and (5) the previous coverage vector γt 1 , which is the accumulation of all attention distributions so far (Tu et al., 2016). When t = 1, we initialize µ0 and γ0 as zero vectors, set e1 to the embedding of the start token “<s>”, and s0 as the average of all encoder states. For each time-step t, the decoder feeds the concatenation of the embedding of the current input et and the previous context vector µt 1 into the 1617 In order to capture non-local interaction between nodes, we allow information exchange between nodes through a sequence of state transitions, leading to a sequence of states g0 , g1 , . . . , gt , . . . , where gt = {hjt }|vj ∈V . The initial state g0 consists of a set of initial node states hj0 = h"
P18-1171,P17-1183,0,0.0238709,"ossible output. The transition system can also provide better local context information than the linearized graph representation, which is important for neural AMR parsing given the limited amount of data. More specifically, we use bi-LSTM to encode two levels of input information for AMR parsing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories. We also want to make better use of the complex transition system to address the data sparsity issue for neural AMR parsing. We extend the hard attention model of Aharoni and Goldberg (2017), which deals with the nearly-monotonic alignment in the morphological inflection task, to the more general scenario of transition systems where the input buffer is processed from left-to-right. When we process the buffer in this ordered manner, the sequence of target transition actions are also strictly aligned left-to-right according to the input order. On the decoder side, we augment the prediction of output action with embedding features from the current transition state. Our experiments show that encoding information from the transition state significantly improves sequenceto-sequence mod"
P18-1171,D15-1198,0,0.11179,"which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final pe"
P18-1171,D17-1130,0,0.232718,"step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tok"
P18-1171,W13-2322,0,0.138186,"ch decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semanti"
P18-1171,P17-1112,0,0.681502,"currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer. Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs. They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack"
P18-1171,P13-2131,0,0.0629132,"subgraph each category is collapsed from, and map each category to its original subgraph representation. We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens. 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template. We further categorize the dataset with the categories we have defined. After categorization, we use Stanford CoreNLP (Mann"
P18-1171,P17-1193,0,0.0167447,", which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that the monotonic hard attention model can"
P18-1171,E17-1051,1,0.937526,"hard attention only attends to one position of the input, it performs slightly better than the soft attention model, while the time complexity is lower. Impact of Different Cache Sizes The cache size of the transition system can be optimized as a trade-off between coverage of AMR graphs and the prediction accuracy. While larger cache size increases the coverage of AMR graphs, it complicates the prediction procedure with more cache decisions to make. From Table 3 we can see that 1848 System Soft Soft+feats Hard+feats P 0.55 0.69 0.70 R 0.51 0.63 0.64 F 0.53 0.66 0.67 System Peng et al. (2018) Damonte et al. (2017) JAMR Ours Table 2: Impact of various components for the sequence-to-sequence model (dev). Cache Size 4 5 6 P 0.69 0.70 0.69 R 0.63 0.64 0.64 F 0.66 0.67 0.66 Table 3: Impact of cache size for the sequenceto-sequence model, hard attention (dev). the hard attention model performs best with cache size 5. The soft attention model also achieves best performance with the same cache size. Comparison with other Parsers Table 4 shows the comparison with other AMR parsers. The first three systems are some competitive neural models. We can see that our parser significantly outperforms the sequence-to-ac"
P18-1171,S15-2154,0,0.0200108,"t if they are distant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that"
P18-1171,P15-1033,0,0.0292887,"parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the traini"
P18-1171,S16-1186,0,0.307711,"numbers (NUMBER) and phrases (PHRASE). The phrases are extracted based on the multiple-to-one alignment in the training data. One example phrase is more than which aligns to a single concept more-than. We first collapse spans and subgraphs into these categories based on the alignment from the JAMR aligner (Flanigan et al., 2014), which greedily aligns a span of words to AMR subgraphs using a set of heuristics. This categorization procedure enables the parser to capture mappings from continuous spans on the sentence side to connected subgraphs on the AMR side. We use the semi-Markov model from Flanigan et al. (2016) as the concept identifier, which jointly segments the sentence into a sequence of spans and maps each span to a subgraph. During decoding, our output has categories, and we need to map 4 For example, verbalization of “teacher” as “(person :ARG0-of teach-01)”, or “minister” as “(person :ARG0-of (have-org-role-91 :ARG2 minister))”. 1847 Peng et al. (2018) Soft+feats Hard+feats ShiftOrPop 0.87 0.93 0.94 PushIndex 0.87 0.84 0.85 ArcBinary 0.83 0.91 0.93 ArcLabel 0.81 0.75 0.77 Table 1: Performance breakdown of each transition phase. each category to the corresponding AMR concept or subgraph. We s"
P18-1171,P14-1134,0,0.551749,"rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled te"
P18-1171,J18-1004,1,0.920933,"o transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer. Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs. They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack and buffer. Peng et al. (2018) apply the cache transition system to AMR parsing and design refined action phases, each modeled with a separate feedforward neural network, to deal with some practical implementation issues. In this paper, we propose a sequence-to-actionsequence approach for AMR parsing with cache transition systems. We want to take advantage of t"
P18-1171,P16-1025,0,0.0429727,"ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization"
P18-1171,P17-1014,0,0.290237,"arsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), wh"
P18-1171,N15-1114,0,0.0368503,"s and achieves competitive results in comparison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model"
P18-1171,P14-5010,0,0.00734709,"Missing"
P18-1171,S16-1166,0,0.0868203,"0.84 0.85 ArcBinary 0.83 0.91 0.93 ArcLabel 0.81 0.75 0.77 Table 1: Performance breakdown of each transition phase. each category to the corresponding AMR concept or subgraph. We save a table Q which shows the original subgraph each category is collapsed from, and map each category to its original subgraph representation. We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens. 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named"
P18-1171,J08-4003,0,0.0317682,"rent from Peng et al. (2018) in two ways: the PushIndex phase is initiated before making all the arc decisions; the newly introduced concept is placed at the last cache position instead of the leftmost buffer position, which essentially increases the cache size by 1. Given the sentence “John wants to go” and the recognized concept sequence “Per want-01 go-01” (person name category Per for “John”), our cache transition parser can construct the AMR graph shown in Figure 1 using the run shown in Figure 2 with cache size of 3. 2.1 Oracle Extraction Algorithm We use the following oracle algorithm (Nivre, 2008) to derive the sequence of actions that leads to the gold AMR graph for a cache transition parser with cache size m. The correctness of the oracle is shown by Gildea et al. (2018). Let EG be the set of edges of the gold graph G. We maintain the set of vertices that is not yet shifted into the cache as S, which is initialized with all vertices in G. The vertices are ordered according to their aligned position in the word sequence and the unaligned vertices are listed according to their order in the depth-first traversal of the graph. The oracle algorithm can look into 1844 Figure 3: Sequence-to"
P18-1171,S15-2153,0,0.127115,"hat are close and not if they are distant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models."
P18-1171,K15-1004,1,0.889524,"ample of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. H"
P18-1171,N15-1040,0,0.330941,". Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out"
P18-1171,J16-3001,0,0.0600652,"tant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5. When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired. 6 we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Acknowledgments We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions. Conclusion References In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models. We also show that the monotonic hard"
P18-1171,E17-1035,1,0.839076,"and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by"
P18-1171,D14-1162,0,0.0817312,", . . . , a∗t−1 , X; θ), (6) t=1 where X represents the input word and concept sequences, and θ is the model parameters. Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best performance on the dev set is selected to evaluate on the test set. Dropout with rate 0.3 is used during training. Beam search with a beam size of 10 is used for decoding. Both training and decoding use a Tesla K20X GPU. Hidden state sizes for both encoder and decoder are set to 100. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. The embeddings for POS tags and features are randomly initialized, with the sizes of 20 and 50, respectively. 4.2 Preprocessing and Postprocessing As the AMR data is very sparse, we collapse some subgraphs or spans into categories based on the alignment. We define some special categories such as named entities (NE), dates (DATE), single rooted subgraphs involving multiple concepts (MULT)4 , numbers (NUMBER) and phrases (PHRASE). The phrases are extracted based on the multiple-to-one alignment in the training data. One example phrase is mor"
P18-1171,D15-1136,0,0.229429,"esent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still fal"
P18-1171,W09-1119,0,0.0693426,"ntains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template. We further categorize the dataset with the categories we have defined. After categorization, we use Stanford CoreNLP (Manning et al., 2014) to get the POS tags and dependencies of the categorized dataset. We run the oracle algorithm separately for training and dev data (with alignment) to get the statistics of individual phases. We use a cache size of 5 in our experiments. 5.2 Results Individual Phase Accuracy We first evaluate the prediction accuracy of individual phases on the dev oracle data assuming gold prediction histor"
P18-1171,D16-1112,0,0.0456677,"parison with the best-performing models.1 1 ARG1 ARG0 go-01 person ARG0 name name op1 “John” Figure 1: An example of AMR graph representing the meaning of: “John wants to go” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing."
P18-1171,D17-1129,0,0.670836,". Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models. The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system 1842 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics of Wang and Xue (2017), which is currently the top performing system. This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system. Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions. Instead of linearizing the target AMR graph to a sequence s"
P18-1171,P15-2141,0,0.0742167,". Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts. AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). 1 The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017). On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models. Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out"
P19-1053,P14-2009,0,0.255066,"Missing"
P19-1053,D14-1181,0,0.0111454,"Missing"
P19-1053,S14-2076,0,0.121985,"o Lu1∗ , Jinsong Su1† , Yubin Ge4 , Linfeng Song5 , Le Sun2 , Jiebo Luo5 1 Xiamen University, Xiamen, China 2 Institute of Software, Chinese Academy of Sciences, Beijing, China 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mec"
P19-1053,W04-3250,0,0.0188998,"69 71.70 80.35 71.36 80.59 71.15 80.32 71.01 80.50 72.04 80.96 72.67 81.33 72.90∗∗ 81.53∗ TWITTER Macro-F1 Accuracy — — 66.17 67.71 66.18 67.78 67.20 68.90 67.47 69.17 67.88∗∗ 69.64∗∗ 73.60 74.97 76.82 77.60 76.78 77.54 76.53 77.46 76.58 77.46 77.42 78.08 77.63 78.47 77.72∗∗ 78.61∗ Table 4: Experimental results on various datasets. We directly cited the best experimental results of MN and TNet reported in (Wang et al., 2018; Li et al., 2018). ∗∗ and ∗ means significant at p <0.01 and p <0.05 over the baselines (MN, TNet) on each test set, respectively. Here we conducted 1,000 bootstrap tests (Koehn, 2004) to measure the significance in metric score differences. First, both of our reimplemented MN and TNet are comparable to their original models reported in (Wang et al., 2018; Li et al., 2018). These results show that our reimplemented baselines are competitive. When we replace the CNN of TNet with an attention mechanism, TNet-ATT is slightly inferior to TNet. Moreover, when we perform additional K+1-iteration of training on these models, their performance has not changed significantly, suggesting simply increasing training time is unable to enhance the performance of the neural ASC models. Sec"
P19-1053,P18-1087,0,0.532047,"a-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-freq"
P19-1053,C16-1291,0,0.0132555,"on weights of “cute” in training instances with neural or negative polarity are significantly decreased. Specifically, in these instances, the average weight of “cute” is reduced to 0.07 times of the original. Hence, TNet-ATT(+AS) assigns a smaller weight (0.1090→0.0062) to “cute” and achieves the correct sentiment prediction. 5 Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection (Liu et al., 2017), machine translation (Liu et al., 2016), and police killing detection (Nguyen and Nguyen, 2018). However, such supervised attention acquisition is labor-intense. Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models. Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work. Our work is inspired by two recent models: one is (Wei et al., 2017) proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation probl"
P19-1053,P17-1164,0,0.139556,"sses both TNet and TNet-ATT. These results strongly demonstrate the effectiveness and generality of our approach. 4.3 Case Study Following this trend, researchers have resorted to more sophisticated attention mechanisms to refine neural ASC models. Chen et al., (2017) proposed a multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. An interactive attention network has been designed by Ma et al., (2017) for ASC, where two attention networks were introduced to model the target and context interactively. Liu et al., (2017) proposed to leverage multiple attentions for ASC: one obtained from the left context and the other one acquired from the right context of a given aspect. Very recently, transformation-based model has also been explored for ASC (Li et al., 2018), and the attention mechanism is replaced by CNN. In order to know how our method improves neural ASC models, we deeply analyze attention results of TNet-ATT and TNet-ATT(+AS). It has been found that our proposed approach can solve the above-mentioned two issues well. Table 5 provides two test cases. TNet-ATT incorrectly predicts the sentiment of the fi"
P19-1053,C18-1193,0,0.0198957,"eural or negative polarity are significantly decreased. Specifically, in these instances, the average weight of “cute” is reduced to 0.07 times of the original. Hence, TNet-ATT(+AS) assigns a smaller weight (0.1090→0.0062) to “cute” and achieves the correct sentiment prediction. 5 Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection (Liu et al., 2017), machine translation (Liu et al., 2016), and police killing detection (Nguyen and Nguyen, 2018). However, such supervised attention acquisition is labor-intense. Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models. Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work. Our work is inspired by two recent models: one is (Wei et al., 2017) proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems, and the other one is (Xu et al., 2018) where a drop"
P19-1053,D14-1162,0,0.0823334,"Missing"
P19-1053,D17-1047,0,0.480008,"f Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is l"
P19-1053,C16-1311,0,0.569845,"a 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus"
P19-1053,D16-1021,0,0.460212,"a 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus"
P19-1053,S14-2036,0,0.0370387,"bin Ge4 , Linfeng Song5 , Le Sun2 , Jiebo Luo5 1 Xiamen University, Xiamen, China 2 Institute of Software, Chinese Academy of Sciences, Beijing, China 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital rol"
P19-1053,P18-1088,0,0.616588,"na, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-frequency ones. As a res"
P19-1053,D16-1058,0,0.335356,"inese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent w"
P19-1053,K18-1055,0,0.0203569,"hanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-frequency ones. As a result, the performance of attentional neural ASC models is still far from satisfaction. We speculate that this is because there exist widely “apparent patterns” and “inapparent patterns”. Here, “apparent patterns” are interpreted as high-frequency words with strong sentiment polarities and “inapparent patterns” are referred to as low-frequency ones in training data. As mentioned in (Li et al., 2018; Xu et al., 2018; Lin et al., 2017), NNs are easily affected by these two modes: “apparent patterns” tend to be overly learned while “inapparent patterns” often can not be fully learned. Here we use sentences in Table 1 to explain this defect. In the first three training sentences, given the fact that the context word “small” occurs frequently with negative sentiment, the attenIn aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tend"
P19-1053,N16-1174,0,0.159322,"Missing"
P19-1053,E17-2091,0,0.113118,"Missing"
P19-1053,S14-2004,0,\N,Missing
P19-1446,W13-2322,0,0.389057,"Missing"
P19-1446,P17-1112,0,0.0342968,"ing Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suff"
P19-1446,P13-2131,0,0.16234,"demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between two AMRs (finding the exact best mapping is NP-complete). The search errors weaken its robustness as a metric. To enhance robustness, the hill-climbing search is executed multiple times with random restarts. This decreases efficiency and, more importantly, does not eliminate search errors. Figure 2 show"
P19-1446,W10-1703,0,0.0808578,"Missing"
P19-1446,W14-3346,0,0.130265,"Missing"
P19-1446,J18-1005,1,0.742255,"ing the properties of being rooted and acyclic. Both the original and inverse relations carry the same semantic meaning. Following S MATCH, we unify both types of relations by reverting all inverse relations to their original ones, before calculating S EM B LEU scores. Efficiency As an important factor, the efficiency of S EM B LEU largely depends on the number of extracted n-grams. One potential problem is that there can be a large number of extracted ngrams for very dense graphs. For a fully connected graph with N nodes, there are O(N n ) possible ngrams. Luckily, AMRs are tree-like graphs (Chiang et al., 2018) that are very sparse. For a tree with N nodes, the number of n-grams is bounded by O(n · N ), which is linear in the tree scale. As tree-like graphs, we expect the number of n-grams Comparison with S MATCH In general, S MATCH breaks down the problem of comparing two AMRs into comparing the smallest units: nodes and edges. It treats each AMR as a bag of nodes and edges, and then calculates an F1 score regarding the correctly mapped nodes and edges. Given two AMRs, S MATCH searches for one-to-one mappings between the graph nodes by maximizing the overall F1 score, and the edgeto-edge mappings a"
P19-1446,E17-1051,0,0.0503911,"large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between two AMRs (finding the exact best mapping is NP-complete). The search errors weaken its robustness as a metric. To enhance robustness, the hill-climbing search is executed multiple times with random restarts. This decreases efficiency and, more importantly, does not eliminate search errors. Figure 2 shows the means and error bounds of S MATCH scores as a function o"
P19-1446,P14-1134,0,0.488908,"Missing"
P19-1446,P18-1170,0,0.0322099,"Missing"
P19-1446,W04-3250,0,0.496815,"Missing"
P19-1446,P17-1014,0,0.0570761,"(Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major dra"
P19-1446,P18-1037,0,0.19416,"tence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one nod"
P19-1446,P02-1040,0,0.106943,"Missing"
P19-1446,K15-1004,1,0.853345,"ction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH ("
P19-1446,P18-1171,1,0.775362,"rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between t"
P19-1446,D15-1136,0,0.108417,"Missing"
P19-1446,D17-1129,0,0.0389543,"13) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows two AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is"
P19-1446,D18-1198,0,0.0146831,"AMR graphs in which the nodes (such as “girl” and “leave-11”) represent AMR concepts and the edges (such as “ARG0” and “ARG1”) represent relations between the concepts. The task of parsing sentences into AMRs has received increasing attention, due to the demand for better natural language understanding. Despite the large amount of work on AMR parsing (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Ballesteros and Al-Onaizan, 2017; Lyu and Titov, 2018; Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), little attention has been paid to evaluating the parsing results, leaving S MATCH Figure 2: Average, minimal and maximal S MATCH scores over 100 runs on 100 sentences. The running time increases from 6.6 seconds (r=1) to 21.0 (r = 4). (Cai and Knight, 2013) as the only overall performance metric. Damonte et al. (2017) developed a suite of fine-grained performance measures based on the node mappings of S MATCH (see below). S MATCH suffers from two major drawbacks: first, it is based on greedy hill-climbing to find a one-to-one node mapping between two AMRs (finding the exact best mapping is N"
P19-1446,D15-1198,0,\N,Missing
P19-1446,D17-1130,0,\N,Missing
Q19-1002,P17-2021,0,0.0274704,"ledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association"
Q19-1002,D15-1198,0,0.0207633,"MRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experim"
Q19-1002,J12-2006,0,0.0211864,"en training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling,"
Q19-1002,W13-2322,0,0.484662,"ow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the concepts and edges (such as :ARG0 and :ARG1) represent the relations between concepts they connect. Comparing with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they"
Q19-1002,S16-1186,0,0.244245,"ohn and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorpo"
Q19-1002,D17-1209,0,0.0409234,"eto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4."
Q19-1002,P14-1134,0,0.0836165,"with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representat"
Q19-1002,P18-1026,0,0.190312,"se layer: − ← − → s 0 = W 1 [ h 0 ; h N ] + b1 , and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs for text generation, showing that the representation is superior compared to BiLSTM on serialized AMR. We extend Song et al. (2018) by investigating the usefulness of AMR for neural machine translation. To our knowledge, we are the first to use GRN for machine translation. In addition to GRNs and GCNs, there have been other graph neural networks, such as graph gated neural network (GGNN) (Li et al., 2015b; Beck et al., 2018). Because our main concern is to empirically investigate the effectiveness of AMR for NMT, we leave it to future work to compare GCN, GGNN, and GRN for our task. 3 where W 1 and b1 are model parameters. For each decoding step m, the decoder feeds the concatenation of the embedding of the current input eym and the previous context vector ζ m−1 into the LSTM model to update its hidden state: Baseline: Attention-Based BiLSTM sm = LSTM(sm−1 , [eym ; ζ m−1 ]). We take the attention-based sequence-to-sequence model of Bahdanau et al. (2015) as the baseline, but use LSTM cells (Hochreiter and Schmidh"
Q19-1002,P18-1170,0,0.0223019,"2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. I"
Q19-1002,P17-1112,0,0.0195771,"ed by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional"
Q19-1002,D18-1198,0,0.0365651,") for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a ful"
Q19-1002,P17-1177,0,0.0237591,"ntion-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distr"
Q19-1002,P82-1020,0,0.739452,"Missing"
Q19-1002,D17-1263,0,0.0258882,"feng Song,1 Daniel Gildea,1 Yue Zhang,2 Zhiguo Wang,3 and Jinsong Su4 1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 School of Engineering, Westlake University, China 3 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 4 Xiamen University, Xiamen, China 1 {lsong10,gildea}@cs.rochester.edu 2 yue.zhang@wias.org.cn 3 zgw.tomorrow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the co"
Q19-1002,D14-1179,0,0.0368049,"Missing"
Q19-1002,C12-1083,0,0.0622695,"not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid"
Q19-1002,W14-3348,0,0.0650882,"Missing"
Q19-1002,P14-5010,0,0.00592251,"Missing"
Q19-1002,P17-4012,0,0.114376,"Missing"
Q19-1002,N18-2078,0,0.264809,"s to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. cantly improves a strong attention-based sequenceto-sequence baseline (25.5 vs 23.7 BLEU). When trained with small-scale (226K) data, the improvement increases"
Q19-1002,W04-3250,0,0.44934,"Missing"
Q19-1002,P17-1014,0,0.345035,"n dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can signifi"
Q19-1002,P02-1040,0,0.103866,"Missing"
Q19-1002,P17-1064,0,0.0168608,"ove a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational"
Q19-1002,K15-1004,1,0.859168,"relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman da"
Q19-1002,W15-4502,0,0.34139,", which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a full AMR graph is considered as a single state, with nodes in the graph being its substates. State transitions are performed on the graph recurrently, allowing substates to exchange information through edges. At each r"
Q19-1002,P18-1171,1,0.850218,"a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 20"
Q19-1002,C10-1081,1,0.785947,"viate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layer"
Q19-1002,D15-1136,0,0.0408605,"ations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard"
Q19-1002,P18-1037,0,0.181496,"y capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based s"
Q19-1002,P16-1162,0,0.0975416,"ns around 4.5 million sentence pairs for training. In addition, we use a subset of the full dataset (News Commentary v11 [NC-v11], containing around 243,000 sentence pairs) for development and additional experiments. For all experiments, we use newstest2013 and newstest2016 as the development and test sets, respectively. To preprocess the data, the tokenizer from Moses4 is used to tokenize both the English and German sides. The training sentence pairs where either side is longer than 50 words are filtered out after tokenization. To deal with rare and compound words, byte-pair encoding (BPE)5 (Sennrich et al., 2016) is applied to both sides. In particular, 8,000 and 16,000 BPE merges are used on the News Commentary v11 subset and the full training set, respectively. On the other hand, JAMR6 (Flanigan et al., 2016) is adopted to parse the English sentences into AMRs before BPE is applied. The statistics of the training data and vocabularies after preprocessing are shown in Tables 1 and 2, respectively. For the experiments with the full training set, we used the top 40K where el and ei are the embeddings of edge label l and source node vi , and W 4 and b4 are model parameters. 4.2 Training Incorporating AM"
Q19-1002,2006.amta-papers.25,0,0.0803469,"Missing"
Q19-1002,D17-1129,0,0.0841465,"dition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a str"
Q19-1002,N06-1056,0,0.151113,"Missing"
Q19-1002,P18-1150,1,0.928432,"ir graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Grosc"
Q19-1002,N09-2004,0,0.0443451,"n from AMR can alleviate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolution"
Q19-1002,P16-2049,0,0.0266319,"g AMR as additional knowledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published"
Q19-1002,D16-1112,0,0.0991011,"Missing"
Q19-1002,P18-1030,1,0.718086,"g representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; G"
S16-2009,N09-1004,0,0.0251889,"ense Embedding Learning for Word Sense Induction 1 Linfeng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Ti"
S16-2009,D14-1110,0,0.0217553,"06; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its"
S16-2009,C14-1123,0,0.0578541,"w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (cluster"
S16-2009,N15-1070,0,0.0201635,"with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each ins"
S16-2009,P14-1023,0,0.236732,"rd sense disambiguation (WSD) assumes there exists an already-known sense inventory, and the sense of a word type is disambiguated according to the sense inventory. Therefore, clustering methods are generally applied in WSI tasks, while classification methods 85 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 85–90, Berlin, Germany, August 11-12, 2016. ods separately train a specific VSM for each word. No methods have shown distributional vectors can keep knowledge for multiple words while showing competitive performance. tributional models (Baroni et al., 2014), and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding represent"
S16-2009,S10-1079,0,0.0264613,"raditional methods for WSI tasks, the advantages of our method include: 1) WSI models for all the polysemous words are trained jointly under the multi-task learning framework; 2) distributed sense embeddings are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models (Baroni et al., 2014). To verify the two statements, we carefully designed comparative experiments described in the next section. 3 3.1 3.2 Comparing on SemEval-2010 We compare our methods with the following systems: (1) UoY (Korkontzelos and Manandhar, 2010) which is the best system in the SemEval2010 WSI competition; (2) NMFlib (Van de Cruys and Apidianaki, 2011) which adopts non-negative matrix factorization to factor a matrix and then conducts word sense clustering on the test set; (3) NB (Choe and Charniak, 2013) which adopts naive Bayes with the generative story that a context is generated by picking a sense and then all context words given the sense; and (4) Spectral (Goyal and Hovy, 2014) which applies spectral clustering on a set of distributional context vectors. Experimental results are shown in Table 1. Let us see the results on superv"
S16-2009,E06-1018,0,0.0790095,"Missing"
S16-2009,W15-1504,0,0.0378227,"Missing"
S16-2009,E09-1013,0,0.0267862,"where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have"
S16-2009,E12-1060,0,0.0146655,"centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group simi"
S16-2009,N10-1013,0,0.196952,"-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,D15-1200,0,0.170573,"lity distribution among all the senses for each instance, which can be seen as soft clustering algorithms. As for knowledge representation, existing WSI methods use the vector space model (VSM) to represent each context. In feature-based models, each instance is represented as a vector of values, where a value can be the count of a feature or the co-occurrence between two words. In Bayesian methods, the vectors are represented as co-occurrences between documents and senses or between senses and words. Overall existing methst = arg max sim(µ(wt , k), vc ) k=1,..,K (1) Another group of methods (Li and Jurafsky, 2015) employs non-parametric algorithms to dynamically decide the number of senses for each word, and each instance is assigned to a sense following a probability distribution in Equation 2, where St is the set of already generated senses for wt , and γ is a constant probability for generating a new sense for wt . ( p(k|µ(wt , k), vc ) ∀ k ∈ St st ∼ γ for new sense (2) From the above discussions, we can obviously notice that WSI task and sense embedding task are inter-related. The two factors in sense embedding learning can be aligned to the two factors of WSI task. Concretely, deciding the number"
S16-2009,P15-1173,0,0.0663194,"Missing"
S16-2009,S10-1011,0,0.223826,"Missing"
S16-2009,D10-1012,0,0.0843825,"Missing"
S16-2009,C14-1016,0,0.0236297,"09; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for each polysemous word in training stage. In testing stage, our method represents each instance as a contextual vector, and induces its sense by finding th"
S16-2009,D14-1113,0,0.547899,"itask learning framework (Caruana, 1997). Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. 2 2.1 2.2 Sense Embedding for WSI As mentioned in Section 1, sense embedding methods learn a distributed representation for each sense of a polysemous word. There are two key factors for sense embedding learning: (1) how to decide the number of senses for each polysemous word and (2) how to learn an embedding representation for each sense. To decide the number of senses in factor (1), one group of methods (Huang et al., 2012; Neelakantan et al., 2014) set a fixed number K of senses for each word, and each instance is assigned to the most probable sense according to Equation 1, where µ(wt , k) is the vector for the k-th sense centroid of word w, and vc is the representation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each insta"
S16-2009,P11-1148,0,0.044133,"Missing"
S16-2009,S10-1081,0,0.0243244,"than CRP-PPMI on both SR and VM. CRP-PPMI has higher PF mainly because it induces fewer number of senses. The above results prove that using sense embeddings have better performance than using count-based distributional models. Besides, SE-WSI-fix is significantly better than WE-Kmeans on every metric. As WEKmeans and SE-WSI-fix learn sense centroids in the same vectors space, while the latter performs joint learning. Therefore, the joint learning is better than learning separately. As for the unsupervised evaluations, SE-WSIfix achieves a good V-Measure score (VM) with a few induced senses. Pedersen (2010) points out that bad models can increase VM by increasing the number of clusters, but doing this will harm performance on both Paired F-score (PF) and SR. Even though UoY, NMFlib and NB show better VM, they (especially UoY) induced more senses than SE-WSI-fix. SE-WSI-fix has higher PF than all others, and higher SR than UoY and NMFlib . 4 Related Work K˚ageb¨ack et al. (2015) proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted M"
S16-2009,Q15-1005,0,0.0201342,"sentation vector of the instance. Methodology Word Sense Induction WSI is generally considered as an unsupervised clustering task under the distributional hypothesis (Harris, 1954) that the word meaning is reflected by the set of contexts in which it appears. Existing WSI methods can be roughly divided into featurebased or Bayesian. Feature-based methods first represent each instance as a context vector, then utilize a clustering algorithm on the context vectors to induce all the senses. Bayesian methods (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012; Goyal and Hovy, 2014; Wang et al., 2015), on the other hand, discover senses based on topic models. They adopt either the LDA (Blei et al., 2003) or HDP (Teh et al., 2006) model by viewing each target word as a corpus and the contexts as pseudo-documents, where a context includes all words within a window centred by the target word. For sense induction, they first extract pseudo-documents for the target word, then train topic model, finally pick the most probable topic for each test pseudo-document as the sense. All of the existing WSI methods have two important factors: 1) how to group similar instances (clustering algorithm) and 2"
S16-2009,D14-1162,0,0.0881563,"sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding methods learn distributed representations for senses of a polysemous word, which is similar to the sense centroid of WSI tasks. In this work, we point out that the WSI task and the sense embedding task are highly interrelated, and propose to jointly learn sense centroids (embeddings) of all polysemous words for the WSI task. Concretely, our method induces several sense centroids (embedding) for"
S16-2009,P14-1137,0,0.0424369,"Missing"
S16-2009,W04-2406,0,0.0636251,"feng Song1 , Zhiguo Wang2 , Haitao Mi2 and Daniel Gildea1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Abstract are utilized in WSD tasks. WSI has been successfully applied to many NLP tasks such as machine translation (Xiong and Zhang, 2014), information retrieval (Navigli and Crisafulli, 2010) and novel sense detection (Lau et al., 2012). However, existing methods usually represent each instance with discrete hand-crafted features (Bordag, 2006; Chen et al., 2009; Van de Cruys and Apidianaki, 2011; Purandare and Pedersen, 2004), which are designed manually and require linguistic knowledge. Most previous methods require learning a specific model for each polysemous word, which limits their usability for downstream applications and loses the chance to jointly learn senses for multiple words. There is a great advance in recent distributed semantics, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014) and sense embedding (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Chen et al., 2014; Tian et al., 2014). Comparing with word embedding, sense embedding m"
S16-2009,W11-1102,0,0.0490206,"Missing"
S16-2009,P12-1092,0,\N,Missing
S16-2009,D13-1148,0,\N,Missing
W11-1911,P05-1022,0,0.0603318,"Missing"
W11-1911,P08-1067,0,0.0477932,"Missing"
W11-1911,P04-1018,0,0.370236,"andidates among all constituents from both given parse tree and packed forest. The packed forest is a compact representation of all parse trees for a given sentence. Readers can refer to (Mi et al., 2008) for detailed definitions. Once the mentions are identified, the left step is to group mentions referring to same object into similar entity. This problem can be viewed as binary classification problem of determining whether each mention pairs corefer. We use a Maximum Entropy classifier to predict the possibility that two mentions refer to the similar entity. And mainly following the work of Luo et al. (2004), we use a beam search algorithm based on Bell Tree to obtain the global optimal classification. As this is the first time we participate competition of coreference resolution, we mainly concentrate on developing fault tolerant capability of our system while omitting feature engineering and other helpful technologies. 2 Mention Detection The first step of the coreference resolution tries to recognize occurrences of mentions in documents. Note that we recognize mention boundaries only on development and test set while generating training 76 Proceedings of the 15th Conference on Computational Na"
W11-1911,P08-1023,1,0.901686,"Missing"
W11-1911,P10-1142,0,0.0187597,"we use the L-BFGS parameter estimation algorithm with gaussian prior smoothing (Chen and Rosenfeld, 1999). We set the gaussian prior to 2 and train the model in 100 iterations. • [A][B][C][D], [A][B][CD] 3.1 Creation of Entities This stage aims to create the mentions detected in the first stage into entities, according to the prediction of classifier. One simple method is to use a greedy algorithm, by comparing each mention to its previous mentions and refer to the one that has the highest probability. In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). To address this problem, we follow the literature (Luo et al., 2004) and propose to use beam search to find global optimal partition. Intuitively, creation of entities can be casted as partition problem. And the number of partitions equals the Bell Number (Bell, which has a ∑ 1934), kn “closed” formula B(n) = 1e ∞ . k=0 k! Clearly, this number is very huge when n is large, enumeration of all partitions is impossible, so we instead designing a beam search algorithm to find the best partition. Formally, the task is to optimize the following objective, yˆ = arg max ϕ∈P ∑ P rob(e) (1) e∈ϕ ∑ pos("
W11-1911,W11-1901,0,0.0939802,"Missing"
W11-1911,J01-4004,0,0.0947592,"ence, intuitively, we 77 3 Determining Coreference This stage is to determine which mentions belong to the same entity. We train a Maximum Entropy classifier (Le, 2004) to decide whether two mentions are coreferent. We use the method proposed by Soon, et al.’s to generate the training instances, where a positive instance is formed between current mention Mj and its closest preceding antecedent Mi , and a negative instance is created by paring Mj with each of the intervening mentions, Mi+1 , Mi+2 ,...,Mj−1 . We use the following features to train our classifier. Features in Soon et al.’s work (Soon et al., 2001) Lexical features IS PREFIX: whether the string of one mention is prefix of the other; IS SUFFIX: whether the string of one mention is suffix of the other; ACRONYM: whether one mention is the acronym of the other; Distance features SENT DIST: distance between the sentences containing the two mentions; MEN DIST: number of mentions between two mentions; Grammatical features IJ PRONOUN: whether both mentions are pronoun; I NESTED: whether mention i is nested in another mention; J NESTED: whether mention j is nested in another mention; Syntax features HEAD: whether the heads of two mentions have t"
W11-1911,D08-1022,0,\N,Missing
W18-6553,P09-1091,0,0.489263,"al network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Nethe"
W18-6553,D16-1032,0,0.0287327,"yntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang an"
W18-6553,D15-1043,1,0.952914,"ial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work"
W18-6553,N15-1012,1,0.926688,"ently, Schmaltz et al. (2016) report new state-of-the-art results by leveraging a neural language model without using syntactic information. In their experiments, the neural language model, which is less sparse and captures long-range dependencies, outperforms previous discrete syntactic systems. A research question that naturally arises from this result is whether syntactic information is helpful for a neural linearization system. We empirically answer this question by comparing a neural transition-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integr"
W18-6553,P14-2128,1,0.86206,"latility . the debate between the stock and futures markets is prepared for wall street will cause another situation about whether de-linkage crash undoubtedly properly renewed friday . the wall street futures markets undoubtedly will cause renewed debate about whether the stock situation is properly prepared for an other crash between friday and de-linkage . the de-linkage between the stock and futures markets friday will undoubtedly cause renewed debate about whether wall street is prope rly prepared for another crash situation . Table 5: Output samples. 0.8 The results are consistent with (Ma et al., 2014) in that both increasing beam size and using richer features are solutions for error propagation. Synl ×LSTM-512 Synl ×LSTM-1 LSTM-512 LSTM-1 0.7 0.6 S YN×LSTM is better than S YN+LSTM. In fact, S YN×LSTM can be considered as interpolation with α being automatically calculated under different states. Finally, S YNl ×LSTM is better than S YN×LSTM except under greedy search, showing that word-to-word dependency features may be sufficient for this task. BLEU 0.5 0.4 0.3 0.2 0.1 10 As for the decoding times, S YNl ×LSTM shows a moderate time growth along increasing beam size, which is roughly 1.5"
W18-6553,P18-1026,0,0.0565936,"Missing"
W18-6553,C10-1012,0,0.143374,"ving significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8,"
W18-6553,J08-4003,0,0.0315241,"terpolated probability distribution is chosen, before both systems advancing to a new state using the action. The interpolated conditional probability is: 6 Following Chen and Manning (2014), we set the training objective as maximizing the loglikelihood of each successive action conditioned on the dependency tree, which can be gold or automatically parsed. To train our linearizer, we first generate training examples {(si , ti )}m i=1 from the training sentences and their gold parse trees, where si is a state, and ti ∈ T is the corresponding oracle transition. We use the “arc standard” oracle (Nivre, 2008), which always prefers S HIFT over LEFTA RC. The final training objective is to minimize the cross-entropy loss, plus an L2regularization term: p(a|si , hi ; θ1 , θ2 ) = log p(a|si ; θ1 ) + α log p(a|hi ; θ2 ), (8) where si and θ1 are the state and parameters of the linearizer, hi and θ2 are the state and parameters of the LSTM language model, and α is the interpolation hyper parameter. The action spaces of the two systems are different because the actions of the LSTM language model correspond only to the shift actions of the linearizer. To match the probability distributions, we expand the di"
W18-6553,P02-1040,0,0.10215,"orward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integrating neural language models: interpolating the log probabilities of both models and integrating the neural language model as a feature. On standard benchmarks, our syntactic linearizer gives results that are higher than the LSTM language model of Schmaltz et al. (2016) by 7 BLEU points (Papineni et al., 2002) using greedy search, The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multilayer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better"
W18-6553,D14-1082,0,0.635996,"-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integrating neural language models: interpolating the log probabilities of both models and integrating the neural language model as a feature. On standard benchmarks, our syntactic linearizer gives results that are higher than the LSTM language model of Schmaltz et al. (2016) by 7 BLEU points (Papineni et al., 2002) using greedy search, The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sent"
W18-6553,D16-1255,0,0.596497,"with discriminative features. Recently, Schmaltz et al. (2016) report new state-of-the-art results by leveraging a neural language model without using syntactic information. In their experiments, the neural language model, which is less sparse and captures long-range dependencies, outperforms previous discrete syntactic systems. A research question that naturally arises from this result is whether syntactic information is helpful for a neural linearization system. We empirically answer this question by comparing a neural transition-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we inves"
W18-6553,E14-1028,0,0.0515525,"Missing"
W18-6553,E12-1075,1,0.923719,"this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et"
W18-6553,P16-1056,0,0.0600418,"Missing"
W18-6553,D11-1106,1,0.961405,"., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the highest-scored hypothesis"
W18-6553,P18-1030,1,0.876063,"Missing"
W18-6553,P11-2033,1,0.776233,"on-like models. Second, we investigate a light version of the system, which only uses word features, while previous works all rely on POS tags and arc labels, limiting their usability on low-resource domains and languages. Schmaltz et al. (2016) are the first to adopt neural networks on this task, while only using surface features. To our knowledge, we are the first to leverage both neural networks and syntactic features. The contrast between our method and the method of Chen and Manning (2014) is reminiscent of the contrast between the method of Liu et al. (2015) and the dependency parser of Zhang and Nivre (2011). Comparing with the dependency parsing task, which assumes that POS tags are available as input, the search space of syntactic linearization is much larger. Recent work (Zhang, 2013; Song et al., 2014; Task Given an input bag-of-words x = {x1 , x2 , ..., xn }, the goal is to output the correct permutation y, which recovers the original sentence, from the set of all possible permutations Y. A linearizer can be seen as a scoring function f over Y, which is trained to output its highest scoring permutation yˆ = argmaxy0 ∈Y f (x, y 0 ) as close as possible to the correct permutation y. 3.1 Baseli"
W18-6553,P18-1150,1,0.887521,"Missing"
W18-6553,D14-1021,1,0.83641,"ir syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as contex"
W18-6553,P13-1043,1,0.818051,"on the training set. we use ten-fold jackknifing to construct WSJ training data with different accuracies. More specifically, the data is first randomly split into ten equalsize subsets, and then each subset is automatically parsed with a constituent parser trained on the other subsets, before the results are finally converted to dependency trees using Penn2Malt. In order to obtain datasets with different parsing accuracies, we randomly sample a small number of sentences from each training subset and choose different training iterations, as shown in Table 4. In our experiments, we use ZPar3 (Zhu et al., 2013) for automatic constituent parsing. Our syntactic linearizer is implemented with Keras.4 We randomly initialize Ew , Et , El , W1 and W2 within (−0.01, 0.01), and use default setting for other parameters. The hyper-parameters and parameters which achieve the best performance on the development set are chosen for final evaluation. Our vocabulary comes from SENNA5 , which has 130,000 words. The activation functions tanh and softmax are added on top of the hidden and output layers, respectively. We use Adagrad (Duchi et al., 2011) with an initial learning rate of 0.01, regularization parameter λ"
W18-6553,E09-1097,0,0.732492,"using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question ge"
W18-6553,D15-1199,0,0.0691019,"Missing"
W18-6553,W05-1104,0,0.0491195,"earization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the highest-scored hypothesis from the priority queue, expanding it by combination with the words in the chart, before finally putting all new hypotheses back into the priority queue. As the search space is huge, a timeout threshold is set, beyond which the search terminates and the current best hypothesis is taken as the result. Liu et al. (2015) adapt the transition-"
W18-6553,D09-1043,0,0.03745,"e generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the h"
W19-4805,D16-1011,0,0.0782085,"Missing"
W19-4805,N18-1100,0,0.0839713,"their attention weights. However, previous work has several limitations. Lin et al. (2017), for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1, such as “bleeding after nasogastric tube insertion”, are larger than a single word. Another issue of Lin et al. (2017) is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear. Mullenbach et al. (2018) defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting. Thus the explainability of the model is limited. Lei et al. (2016) introduce a regularizer over the selected (single-word) positions to encourage the model to extract larger phrases. However, their method can not tell how much a selected unit contributes to the model’s decision through a weight value. In this paper, we study what the meaningful units to highlight are. We define multi-granular ngrams as basic units, so that all"
W19-4805,D14-1162,0,0.0826462,"coder, all ngrams with the same order can be computed in parallel, and the model needs at most 7 iterative steps along the depth dimension for representing a given text of arbitrary length. 4 ACC #Param. 2.6 4.6 1.4 64.8 64.5 66.2 848,228 147,928 168,228 tokens, and one label out of five categories indicating which disease this document is about. We randomly split the dataset into train/dev/test sets by 8:1:1 for each category, and end up with 11,216/1,442/1,444 instances for each set. Hyperparameters We use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (Pennington et al., 2014), and set the hidden size as 100 for node embeddings. We apply dropout to every layer with a dropout ratio 0.2, and set the batch size as 50. We minimize the cross-entropy of the training set with the ADAM optimizer (Kingma and Ba, 2014), and set the learning rate is to 0.001. During training, the pre-trained word embeddings are not updated. (5) (6) Eval Time 57.0 92.1 30.3 Table 2: Efficiency evaluation. (4) c=i u+f h +f h Train Time 4.1 Properties of the multi-granular encoder Influence of the n-gram order: For CNN and our LeftForest encoder, we vary the order of ngrams from 1 to 9, and plot"
W19-4805,P15-1150,0,0.13561,"Missing"
