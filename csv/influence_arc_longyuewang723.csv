2020.acl-main.153,P19-1174,0,0.0290448,"Missing"
2020.acl-main.153,P17-4012,0,0.0347528,"ces. For Chinese⇔English, we follow Hassan et al. (2018) to get 20 million Transformer Big 28.8 Head XL SANs 28.6 28.3 0 2 4 6 8 10 12 14 τ (#heads with XL PE) Figure 3: BLEU score on newstest2014 for different τ . sentence pairs. We develop on devtest2017 and test on newstest2017. We use SacreBLEU (Post, 2018) as the evaluation metric with statistical significance test (Collins et al., 2005). We evaluate the proposed XL PE strategies on Transformer. The baseline systems include Relative PE (Shaw et al., 2018) and directional SAN (DiSAN, Shen et al. 2018). We implement them on top of OpenNMT (Klein et al., 2017). In addition, we report the results of previous studies (Hao et al., 2019; Wang et al., 2019; Chen et al., 2019b,a; Du and Way, 2017; Hassan et al., 2018). The reordered source sentences are generated by BTG-based preordering model (Neubig et al., 2012) trained with above sub-word level1 parallel corpus. At training phase, we first obtain word alignments from parallel data using GIZA++ or FastAlign, and then the training process is to find the optimal BTG tree for source sentence consistent with the order of the target sentence based on the word alignments and parallel data. At decoding phase"
2020.acl-main.153,D19-1139,0,0.0409462,"Missing"
2020.acl-main.153,P05-1066,0,0.595062,"Missing"
2020.acl-main.153,W17-5706,0,0.0669987,". . , OH )WO (11) In particular, τ = 0 refers to the original Transformer (Vaswani et al., 2017) and τ = H means that XL PE will propagate over all attention heads. 4 Experiments We conduct experiments on word order-diverse language pairs: WMT’14 English⇒German (En-De), WAT’17 Japanese⇒English (Ja-En), and WMT’17 Chinese⇔English (Zh-En & En-Zh). For English⇒German, the training set consists of 4.5 million sentence pairs and newstest2013 & 2014 are used as the dev. and test sets, respectively. BPE with 32K merge operations is used to handle low-frequency words. For Japanese⇒English, we follow Morishita et al. (2017) to use the first two sections as training data, which consists of 2.0 million sentence pairs. The dev. and test sets contain 1790 and 1812 sentences. For Chinese⇔English, we follow Hassan et al. (2018) to get 20 million Transformer Big 28.8 Head XL SANs 28.6 28.3 0 2 4 6 8 10 12 14 τ (#heads with XL PE) Figure 3: BLEU score on newstest2014 for different τ . sentence pairs. We develop on devtest2017 and test on newstest2017. We use SacreBLEU (Post, 2018) as the evaluation metric with statistical significance test (Collins et al., 2005). We evaluate the proposed XL PE strategies on Transformer."
2020.acl-main.153,N19-1423,0,0.0212178,"0 1 2 3 4 5 [XL POS] 0 3 4 5 1 2 with Sharon (b) Absolute(abs) Position vs. Cross-Lingual(XL) Position Figure 1: Illustration of cross-lingual position for English⇒Chinese translation task. (a) BTG tree shows the cross-lingual preordering. The top-left corner is the transduction grammar. (b) the difference between absolute position encoding (APE) and our proposed crosslingual position encoding (XL PE) . Introduction Although self-attention networks (SANs) (Lin et al., 2017) have achieved the state-of-the-art performance on several natural language processing (NLP) tasks (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2018), they possess the innate disadvantage of sequential modeling due to the lack of positional information. Therefore, absolute position encoding (APE) (Vaswani et al., 2017) and relative position encoding (RPE) (Shaw et al., 2018) were introduced to better capture the sequential dependencies. However, either absolute or relative PE is language-independent and its embedding remains fixed. This inhibits the capacity of SANs when modelling multiple languages, which have diverse word orders and structures (Gell-Mann and Ruhlen, 2011). Recent work have shown that modeling cross"
2020.acl-main.153,P06-1090,0,0.158944,"Missing"
2020.acl-main.153,W17-4123,0,0.161582,"(APE) (Vaswani et al., 2017) and relative position encoding (RPE) (Shaw et al., 2018) were introduced to better capture the sequential dependencies. However, either absolute or relative PE is language-independent and its embedding remains fixed. This inhibits the capacity of SANs when modelling multiple languages, which have diverse word orders and structures (Gell-Mann and Ruhlen, 2011). Recent work have shown that modeling cross-lingual information (e.g., alignment or reordering) at encoder or attention level improves translation performance for different language pairs (Cohn et al., 2016; Du and Way, 2017; Zhao et al., 2018; Kawara et al., 2018). Inspired by their work, we propose to augment SANs with cross-lingual representations, by encoding reordering indices at embedding level. Taking English⇒Chinese translation task for example, we first reorder the English sentence by deriving a latent bracketing transduction grammar 1679 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1679–1685 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (BTG) tree (Wu, 1997) (Fig. 1a). Similar to absolute position, the reordering information can be"
2020.acl-main.153,D12-1077,0,0.0815036,"respectively. (1) where posabs denotes the numerical position indices, i is the dimension of the position indices and dmodel means hidden size. f (·) alternately employs sin(·) and cos(·) for even and odd dimensions. Accordingly, the position matrix PE can be obtained given the input X = {x1 , . . . , xT } ∈ RT ×dmodel . Then, the position aware output Z is calculated by: Z = X + PEabs Concat (4) h , Wh Where subspace parameters are WQ K ∈ d ×d h d ×d R model k and WV ∈ R model v , where dk , dv 3 3.1 Approach Cross-Lingual Position Representation First, we built a BTG-based reordering model (Neubig et al., 2012) to generate a reordered source sentence according to the word order of its corresponding target sentence. Second, we obtained the reordered word indices posXL that correspond with the input sentence X. To output the cross-lingual position matrix PEXL , we inherit the sinusoidal function in Eq. (1). Formally, the process is: PEXL = f (BTG(X)) 3.2 (6) Integration Strategy As shown in Fig. 2, we propose two strategies to integrate the cross-lingual position encoding (XL PE) into SANs: inputting-level XL (InXL) SANs and head-level (HeadXL) SANs. Inputting-level XL SANs As illustrated in Fig. 2a,"
2020.acl-main.153,P13-2071,0,0.0768783,"Missing"
2020.acl-main.153,P11-1105,0,0.0898006,"Missing"
2020.acl-main.153,D19-1453,0,0.192845,"ual alignment information is only used to preprocess training data, but not necessary at decoding time. For fair comparison, we keep the Transformer decoder unchanged and validate different position representation strategies on the encoder. We conduct all experiments on the T RANSFORMER -B IG with four V100 GPUs. 4.1 Effect of τ in HeadXL SANs Fig. 3 reports the results of different τ for Head XL S ANs. With increasing of XL PE-informed heads, the best BLEU is achieved when #heads = 4, which is therefore left as the default setting for HeadXL. Then, the BLEU score gradually decreases as the 1 Garg et al. (2019) show that sub-word units are beneficial for statistical model. 1681 # System Architecture BLEU #Param. 1 2 3 4 5 6 7 8 9 10 11 Vaswani et al. (2017) Hao et al. (2019) Wang et al. (2019) Chen et al. (2019b) Chen et al. (2019a) Transformer B IG Transformer B IG w/ BiARN Transformer B IG w/ Structure PE Transformer B IG w/ MPRHead Transformer B IG w/ Reorder Emb Transformer B IG + Relative PE + DiSAN + InXL PE + HeadXL PE + Combination 28.4 28.98 28.88 29.11 29.11 28.36 28.71 28.76 28.66 28.72 29.05↑ 213M 323.5M – 289.1M 308.2M 282.55M +0.06M +0.04M +0.01M +0.00M +0.01M This work Table 1: Experi"
2020.acl-main.153,P18-3004,0,0.0229369,"ative position encoding (RPE) (Shaw et al., 2018) were introduced to better capture the sequential dependencies. However, either absolute or relative PE is language-independent and its embedding remains fixed. This inhibits the capacity of SANs when modelling multiple languages, which have diverse word orders and structures (Gell-Mann and Ruhlen, 2011). Recent work have shown that modeling cross-lingual information (e.g., alignment or reordering) at encoder or attention level improves translation performance for different language pairs (Cohn et al., 2016; Du and Way, 2017; Zhao et al., 2018; Kawara et al., 2018). Inspired by their work, we propose to augment SANs with cross-lingual representations, by encoding reordering indices at embedding level. Taking English⇒Chinese translation task for example, we first reorder the English sentence by deriving a latent bracketing transduction grammar 1679 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1679–1685 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (BTG) tree (Wu, 1997) (Fig. 1a). Similar to absolute position, the reordering information can be represented as cross-lingual position (Fi"
2020.acl-main.153,J03-1002,0,0.0249384,"th noting that our approach introduces nearly no additional parameters (+0.01M over 282.55M). 4.3 Alignment Quality Our proposed XL PE intuitively encourages SANs to learn bilingual diagonal alignment, so has the 2 Replace PEXL in Eq. (9) with PEI N -XL in Eq. (8). potential to induce better attention matrices. We explore this hypothesis on the widely used Gold Alignment dataset3 and follow Tang et al. (2019) to perform the alignment. The only difference being that we average the attention matrices across all heads from the penultimate layer (Garg et al., 2019). The alignment error rate (AER, Och and Ney 2003), precision (P) and recall (R) are reported as the evaluation metrics. Tab. 3 summarizes the results. We can see: 1) XL PE allows SANs to learn better attention matrices, thereby improving alignment performance (27.4 / 26.9 vs. 29.7); and 2) combining the two strategies delivers consistent improvements (24.7 vs. 29.7). 4.4 Gain for Context-Free Model Tang et al. (2019) showed that context-free Transformer (directly propagating the source word embeddings with PE to the decoder) achieved comparable results to the best RNN-based model. We argue that XL PE could further enhance the contextfree Tra"
2020.acl-main.153,W18-6319,0,0.025642,"and test sets, respectively. BPE with 32K merge operations is used to handle low-frequency words. For Japanese⇒English, we follow Morishita et al. (2017) to use the first two sections as training data, which consists of 2.0 million sentence pairs. The dev. and test sets contain 1790 and 1812 sentences. For Chinese⇔English, we follow Hassan et al. (2018) to get 20 million Transformer Big 28.8 Head XL SANs 28.6 28.3 0 2 4 6 8 10 12 14 τ (#heads with XL PE) Figure 3: BLEU score on newstest2014 for different τ . sentence pairs. We develop on devtest2017 and test on newstest2017. We use SacreBLEU (Post, 2018) as the evaluation metric with statistical significance test (Collins et al., 2005). We evaluate the proposed XL PE strategies on Transformer. The baseline systems include Relative PE (Shaw et al., 2018) and directional SAN (DiSAN, Shen et al. 2018). We implement them on top of OpenNMT (Klein et al., 2017). In addition, we report the results of previous studies (Hao et al., 2019; Wang et al., 2019; Chen et al., 2019b,a; Du and Way, 2017; Hassan et al., 2018). The reordered source sentences are generated by BTG-based preordering model (Neubig et al., 2012) trained with above sub-word level1 par"
2020.acl-main.153,N18-2074,0,0.116704,"corner is the transduction grammar. (b) the difference between absolute position encoding (APE) and our proposed crosslingual position encoding (XL PE) . Introduction Although self-attention networks (SANs) (Lin et al., 2017) have achieved the state-of-the-art performance on several natural language processing (NLP) tasks (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2018), they possess the innate disadvantage of sequential modeling due to the lack of positional information. Therefore, absolute position encoding (APE) (Vaswani et al., 2017) and relative position encoding (RPE) (Shaw et al., 2018) were introduced to better capture the sequential dependencies. However, either absolute or relative PE is language-independent and its embedding remains fixed. This inhibits the capacity of SANs when modelling multiple languages, which have diverse word orders and structures (Gell-Mann and Ruhlen, 2011). Recent work have shown that modeling cross-lingual information (e.g., alignment or reordering) at encoder or attention level improves translation performance for different language pairs (Cohn et al., 2016; Du and Way, 2017; Zhao et al., 2018; Kawara et al., 2018). Inspired by their work, we"
2020.acl-main.153,R19-1136,0,0.246195,"685 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (BTG) tree (Wu, 1997) (Fig. 1a). Similar to absolute position, the reordering information can be represented as cross-lingual position (Fig. 1b). In addition, we propose two strategies to incorporate cross-lingual position encoding into SANs. We conducted experiments on three commonlycited datasets of machine translation. Results show that exploiting cross-lingual PE consistently improves translation quality . Further analysis reveals that our method improves the alignment quality (§Sec. 4.3) and context-free Transformer (Tang et al., 2019) (§Sec. 4.4). Furthermore, contrastive evaluation demonstrates that NMT models benefits from the cross-lingual information rather than denoising ability (§Sec. 4.5). 2 Background Position Encoding To tackle the position unaware problem, absolute position information is injected into the SANs: PEabs = f (posabs /100002i/dmodel ) ∈ RT ×dmodel (2) Self-Attention The SANs compute the attention of each pair of elements in parallel. It first converts the input into three matrices Q, K, V, representing queries, keys, and values, respectively: {Q, K, V} = {ZWQ , ZWK , ZWV } (3) where WQ , WK , WV ∈ Rd"
2020.acl-main.153,D19-1145,1,0.833308,"Head XL SANs 28.6 28.3 0 2 4 6 8 10 12 14 τ (#heads with XL PE) Figure 3: BLEU score on newstest2014 for different τ . sentence pairs. We develop on devtest2017 and test on newstest2017. We use SacreBLEU (Post, 2018) as the evaluation metric with statistical significance test (Collins et al., 2005). We evaluate the proposed XL PE strategies on Transformer. The baseline systems include Relative PE (Shaw et al., 2018) and directional SAN (DiSAN, Shen et al. 2018). We implement them on top of OpenNMT (Klein et al., 2017). In addition, we report the results of previous studies (Hao et al., 2019; Wang et al., 2019; Chen et al., 2019b,a; Du and Way, 2017; Hassan et al., 2018). The reordered source sentences are generated by BTG-based preordering model (Neubig et al., 2012) trained with above sub-word level1 parallel corpus. At training phase, we first obtain word alignments from parallel data using GIZA++ or FastAlign, and then the training process is to find the optimal BTG tree for source sentence consistent with the order of the target sentence based on the word alignments and parallel data. At decoding phase, we only provide source sentences as input and the model can output reordering indices, whic"
2020.acl-main.153,J97-3002,0,0.824096,"Missing"
2020.acl-main.153,P19-1354,1,0.817503,"Missing"
2020.acl-main.153,L18-1143,0,0.0256608,"al., 2017) and relative position encoding (RPE) (Shaw et al., 2018) were introduced to better capture the sequential dependencies. However, either absolute or relative PE is language-independent and its embedding remains fixed. This inhibits the capacity of SANs when modelling multiple languages, which have diverse word orders and structures (Gell-Mann and Ruhlen, 2011). Recent work have shown that modeling cross-lingual information (e.g., alignment or reordering) at encoder or attention level improves translation performance for different language pairs (Cohn et al., 2016; Du and Way, 2017; Zhao et al., 2018; Kawara et al., 2018). Inspired by their work, we propose to augment SANs with cross-lingual representations, by encoding reordering indices at embedding level. Taking English⇒Chinese translation task for example, we first reorder the English sentence by deriving a latent bracketing transduction grammar 1679 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1679–1685 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (BTG) tree (Wu, 1997) (Fig. 1a). Similar to absolute position, the reordering information can be represented as cros"
2020.acl-main.153,D18-1549,0,\N,Missing
2020.acl-main.269,D15-1075,0,0.0207773,"h the probability 1 − π. = sigmoid((Es + G0 − G00 )/τ ) NLP Benchmarks (5) where G0 and G00 are two independent Gumbel noises (Gumbel, 1954), and τ ∈ (0, ∞) is a temperature parameter. As τ diminishes to zero, a sample from the Gumbel-Sigmoid distribution becomes cold and resembles the one-hot samples. At training time, we can use Gumbel-Sigmoid to obtain Experimental Setup Natural Language Inference aims to classify semantic relationship between a pair of sentences, i.e., a premise and corresponding hypothesis. We conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which has three classes: Entailment, Contradiction and Neutral. We followed Shen et al. (2018b) to use a token2token SAN layer followed by a source2token SAN layer to generate a compressed vector representation of input sentence. The selector is integrated into the token2token SAN layer. Taking the premise representation sp and the hypothesis vector sh as input, their semantic relationship is represented by the concatenation of sp , sh , sp −sh and sp · sh , which is passed to a classification module to generate a categorical distribution over the three classes. We initialize the word embedd"
2020.acl-main.269,P18-1008,0,0.115114,"word order encoding (Yang et al., 2019a) and syntactic structure modeling (Tang et al., 2018). In this work, we concentrate on these two commonly-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representa"
2020.acl-main.269,P18-1198,0,0.0224993,"Missing"
2020.acl-main.269,N19-1423,0,0.0125418,"word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) p"
2020.acl-main.269,C16-1276,1,0.893209,"Missing"
2020.acl-main.269,D19-1082,1,0.814363,"Missing"
2020.acl-main.269,D19-1135,1,0.882131,"urther refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Metho"
2020.acl-main.269,N19-1122,1,0.877134,"Missing"
2020.acl-main.269,D19-1088,1,0.908786,"Missing"
2020.acl-main.269,D18-1317,1,0.881654,"are pretrained on Wikipedia and Gigaword, to initialize our networks, but they are not fixed during training. We choose the better feed-forward networks (FFN) variants of DEEPATT as our standard settings. Machine Translation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datase"
2020.acl-main.269,N19-1359,1,0.870621,"Missing"
2020.acl-main.269,D15-1166,0,0.0438377,"uistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Methodology Self-Attention Networks SANs (Lin et al., 2017), as a variant of attention model (Bahdanau et al., 2015; Luong et al., 2015), compute attention weights between each pair of elements in a single sequence. Given the input layer H = {h1 , · · · , hN } ∈ N ×d , SANs first transform the layer H into the queries Q ∈ N ×d , the keys K ∈ N ×d , and the values V ∈ N ×d with three separate weight matrices. The output layer O is calculated as: R R O = ATT(Q, K)V R R (1) where the alternatives to ATT(·) can be additive attention (Bahdanau et al., 2015) or dot-product attention (Luong et al., 2015). Due to time and space efficiency, we used the dot-product attention in this study, which is computed as: QKT ATT(Q, K) = sof tmax("
2020.acl-main.269,D18-1458,0,0.0466347,"Missing"
2020.acl-main.269,W18-5444,0,0.0310899,"Missing"
2020.acl-main.269,D14-1162,0,0.0828181,"Missing"
2020.acl-main.269,D19-1145,1,0.865883,"Missing"
2020.acl-main.269,N18-1202,0,0.0976303,"Missing"
2020.acl-main.269,D19-1098,0,0.0165419,"y-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (W"
2020.acl-main.269,P16-1162,0,0.016109,"lation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datasets, we followed He et al. (2018) to decrease the layer size to 256 and the number of attention heads to 4. For all the tasks, we applied the selector to the first layer of encoder to better capture lexical and syntactic infor"
2020.acl-main.269,N18-2074,0,0.0355278,"ion (i.e., sequence generation), demonstrate that SSANs consistently outperform the standard SANs (§3). Despite demonstrating the effectiveness of SSANs, the underlying reasons for their strong performance have not been well explained, which poses great challenges for further refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntact"
2020.acl-main.269,D18-1408,0,0.0225505,"9a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (Wu et al., 2018; Hao et al., 2019a; Wang et al., 2019b). These studies show that the introduction of syntactic information can achieve further improvement over SANs, demonstrating its potential weakness on structure modeling. 2.3 Selective Self-Attention Networks In this study, we implement the selective mechanism on SANs by introducing an additional selector, namely SSANs, as illustrated in Figure 1. The selector aims to select a subset of elements from the input sequence, on top of which the standard self-attention (Equation 1) is conducted. We implement the selector with Gumbel-Softmax, which has proven e"
2020.acl-main.269,D18-1475,1,0.809343,"guage representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018, 2019b). This poses a"
2020.acl-main.269,P19-1354,1,0.897977,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,N19-1407,1,0.818121,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,C18-1259,0,0.0489127,"Missing"
2020.acl-main.269,D18-1548,0,0.12373,"to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al.,"
2020.acl-main.269,W13-3516,0,\N,Missing
2020.acl-main.269,D16-1159,0,\N,Missing
2020.emnlp-main.78,P18-1198,0,0.0573306,"Missing"
2020.emnlp-main.78,D18-1457,1,0.883746,"Missing"
2020.emnlp-main.78,D15-1166,0,0.0620799,"et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “# Para.” denotes the trainable parameter size of each model. “+” denotes appending new features to the above row. “↑/⇑” indicates statistical significance (p &lt; 0.05/0.01) over the baseline. the open-source toolkit – fairseq (Ott"
2020.emnlp-main.78,N19-4009,0,0.0341847,"015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “# Para.” denotes the trainable parameter size of each model. “+” denotes appending new features to the above row. “↑/⇑” indicates statistical significance (p &lt; 0.05/0.01) over the baseline. the open-source toolkit – fairseq (Ott et al., 2019). For Transformer, we investigate big, base and small settings. About RNNSearch and LightConv, we employ corresponding configurations in fairseq. The implementation is detailed in Appendix §A.1. All baseline models are trained for 100K updates using Adam optimizer (Kingma and Ba, 2015). Based on the baselines, the proposed pruning and rejuvenation methods are trained with additional 100K updates (i.e. 50K for each one). To rule out the circumstance that more training steps may bring improvements, we also conduct continuous training (ConTrain) as strong baselines and they employ the same traini"
2020.emnlp-main.78,P02-1040,0,0.106342,"(De⇒En) and English⇒French (En⇒Fr) translation tasks. For En⇒De task, we use WMT14 corpus which contains 4 million sentence pairs. The Zh⇒En task is conducted on WMT17 corpus, consisting of 21 million sentence pairs. We follow Dou et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “"
2020.emnlp-main.78,W19-5211,0,0.0345998,"Missing"
2020.emnlp-main.78,K16-1029,0,0.211111,"Missing"
2020.emnlp-main.78,P16-1162,0,0.117646,"rameters and training entire networks (RejTrain). 3 3.1 Experiments Setup Data We conduct experiments on English⇒ German (En⇒De), Chinese⇒English (Zh⇒En), German⇒English (De⇒En) and English⇒French (En⇒Fr) translation tasks. For En⇒De task, we use WMT14 corpus which contains 4 million sentence pairs. The Zh⇒En task is conducted on WMT17 corpus, consisting of 21 million sentence pairs. We follow Dou et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.7"
2020.emnlp-main.78,P16-1008,1,0.860236,"Missing"
2020.emnlp-main.78,D19-1088,1,0.857064,"Missing"
2020.emnlp-main.78,W04-3250,0,0.0607804,"e task, we use WMT14 corpus which contains 4 million sentence pairs. The Zh⇒En task is conducted on WMT17 corpus, consisting of 21 million sentence pairs. We follow Dou et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “# Para.” denotes the trainable parameter size of each"
2020.emnlp-main.78,2020.tacl-1.47,0,0.0432741,"Missing"
2020.emnlp-main.78,D19-1145,1,0.875297,"Missing"
2020.emnlp-main.78,2020.findings-emnlp.432,1,0.837561,"Missing"
2020.emnlp-main.78,D18-1041,0,0.0986037,"ent training phases. For each phase, we select sequentially three models. The solid arrow represents the changes in each phase. The dotted arrow represents the changes from the baseline to the pruning phase. ways: representation visualization and linguistic probing. Furthermore, we study the translation outputs in terms of adequacy and fluency. Escaping from Local Optimum To study how our method help models to escape from local optimum, we analyze the change of source representations during different training phases. The analysis is conducted on the Transformer BASE model and En⇒De. Following Zeng et al. (2018), we feed source sentences in the development set into a checkpoint and output an element-wise averaged vector from representations of the last encoder layer. With the dimension-reduction technique of TruncatedSVD (Du et al., 2017), we can plot the dimensionally reduced values in Figure 2. Among the training phases (i.e. Baseline, ConTrain, PruTrain, RejTrain), we select checkpoints at which interval training updates are equal. As seen, within each phase, the representations change smoothly in direction and quantity. The continuous training still transforms the representations in the same dire"
2020.findings-emnlp.432,D18-1457,1,0.829088,"ution of source words. Related to our work, Li et al. (2019) and Tang et al. (2019b) also conducted word alignment analysis on the same De-En and Zh-En datasets with Transformer models8 . We use similar techniques to examine word alignment in our context; however, we also introduce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed"
2020.findings-emnlp.432,W18-5431,0,0.0330701,"Missing"
2020.findings-emnlp.432,W19-5211,0,0.125059,"duce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed a graduated label smoothing that can improve the inference calibration. In this work, based on our information probing analysis, we simplified the decoder by removing the residual feedforward module in totality, with minimal loss of translation quality and a significant boost of"
2020.findings-emnlp.432,D16-1159,0,0.0466695,"Missing"
2020.findings-emnlp.432,2020.acl-main.269,1,0.828464,"peed (words per second) and “#Infer.” denotes the inference speed (sentences per second). Results are averages of three runs. preting the behaviors of attention modules. Previous studies generally focus on the self-attention in the encoder, which is implemented as multi-head attention. For example, Li et al. (2018) showed that different attention heads in the encoder-side self-attention generally attend to the same position. Voita et al. (2019) and Michel et al. (2019) found that only a few attention heads play consistent and often linguistically-interpretable roles, and others can be pruned. Geng et al. (2020) empirically validated that a selective mechanism can mitigate the problem of word order encoding and structure modeling of encoder-side self-attention. In this work, we investigated the functionalities of decoder-side attention modules for exploiting both source and target information. Interpreting Encoder Attention The encoderattention weights are generally employed to interpret the output predictions of NMT models. Recently, Jain and Wallace (2019) showed that atten4806 IFM -IFM -- 3.5 3.0 2.5 SEM 3.0 2 5.0 TEM 4.010: Comparison Figure of IFM information evolution between the standard and s"
2020.findings-emnlp.432,D18-1548,0,0.0198969,"and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance – a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed. 1 Introduction Transformer models have advanced the state-ofthe-art on a variety of natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019"
2020.findings-emnlp.432,D19-1149,0,0.26795,", 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the e"
2020.findings-emnlp.432,D19-1088,1,0.802727,"cePPL PPL Source AddAdd & Norm & Norm 5.0 5.0 En-Zh Source PPL 3.5 4.0 4.5 4.5 IFM SEM TEM En-De Source PPL 4.0 Softmax Softmax 4.5 5.5 5.5 IFM SEM TEM En-Fr 5.0 Output Output 4.5 Probabilities Probabilities Target TargetPPL PPL IFM SEM TEM EnDe-w/oFFN 5.5 2.0 1 2 3 Decoder 4 Model Standard (Base) Simplified (Base) Fluency 4.00 4.01 Adequacy 3.86 3.87 Table 5: Human evaluation of translation performance of both standard and simplified decoders on 100 samples from En-Zh test set, on the scale of 1 to 5. tion weights are weakly correlated with the contribution of source words to the prediction. He et al. (2019) used the integrated gradients to better estimate the contribution of source words. Related to our work, Li et al. (2019) and Tang et al. (2019b) also conducted word alignment analysis on the same De-En and Zh-En datasets with Transformer models8 . We use similar techniques to examine word alignment in our context; however, we also introduce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different"
2020.findings-emnlp.432,R19-1136,0,0.340244,", 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the e"
2020.findings-emnlp.432,N19-1357,0,0.0626198,"Missing"
2020.findings-emnlp.432,P19-1452,0,0.0230096,".1 Representation Evolution Across Layers In order to quantify and visualize the representation evolution, we design a universal probing scheme to quantify the source (or target) information stored in network representations. Task Description Intuitively, the more the source (or target) information stored in a network representation, the more probably a trained reconstructor could recover the source (or target) sequence. Since the lengths of source sequence and decoder representations are not necessarily the same, the widely-used classification-based probing approaches (Belinkov et al., 2017; Tenney et al., 2019b) cannot be applied to this task. Accordingly, we cast this task as a generation problem – evaluating the likelihood of generating the word sequence conditioned on the input representation. Figure 2 illustrates the architecture of our probing scheme. Given a representation sequence from decoder H = {h1 , . . . , hM } and the source (or target) word sequence to be recovered x = {x1 , . . . , xN } the recovery likelihood is calculated as the perplexity (i.e. negative log-likelihood) of forced-decoding the word sequence: P P L(x|H) = N X − log P (xn |x&lt;n , H) n=1 3 More implementation details ar"
2020.findings-emnlp.432,P16-1008,1,0.925135,"e functionalities of the three modules are well-separated. 2.3 Research Questions Modern Transformer decoder is implemented as multiple identical layers, in which the source and target information are exploited and evolved layerby-layer. One research question arises naturally: RQ1. How do source and target information evolve within the decoder layer-by-layer and module-by-module? RQ2. How does SEM exploit the source information in different layers? In Section 3.2, we investigate how the SEMs transform the source information to the target side in terms of alignment accuracy and coverage ratio (Tu et al., 2016). Experimental results show that higher layers of SEM modules accomplish word alignment, while lower layer ones exploit necessary contexts. This also explains the representation evolution of source information: lower layers collect more source information to obtain a global view of source input, and higher layers extract less aligned source input for accurate translation. Of the three sub-layers, IFM modules conceptually appear to play a key role in merging source and target information – raising our final question: RQ3. How does IFM fuse source and target information on the operation level? I"
2020.findings-emnlp.432,P19-1580,0,0.0203504,"bell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the encoder is fed to the decoder and the training signals for the encoder are back-propaga"
2020.findings-emnlp.432,D19-1085,1,0.891412,"Missing"
2020.findings-emnlp.432,D17-1301,1,0.848009,"ope that our analysis and findings could inspire architectural changes for further improvements, such as 1) improving the word alignment of higher SEMs by incorporating external alignment signals; 2) exploring the stacking order of SEM, TEM and IFM sub-layers, which may provide a more effective way to transform information; 3) further pruning redundant sub-layers for efficiency. Since our analysis approaches are not limited to the Transformer model, it is also interesting to explore other architectures such as RNMT (Chen et al., 2018), ConvS2S (Gehring et al., 2017), or on document-level NMT (Wang et al., 2017, 2019). In addition, our analysis methods can be applied to other sequence-to-sequence tasks such as summarization and grammar error correction, whose source and target sides are in the same language. We leave those tasks for future work. Acknowledgments Tadepalli acknowledges the support of DARPA under grant number N66001-17-2-4030. The authors thank the anonymous reviewers for their insightful and helpful comments. References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine trans"
2020.findings-emnlp.432,2020.acl-main.278,1,0.810191,", 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed a graduated label smoothing that can improve the inference calibration. In this work, based on our information probing analysis, we simplified the decoder by removing the residual feedforward module in totality, with minimal loss of translation quality and a significant boost of both training and inference speeds. 5 Conclusions In this paper, we interpreted NMT Transformer decoder by assessing the evolution of both source 8 We find our results are more similar to that of Tang et al. (2019b). Also, o"
2020.findings-emnlp.432,P19-1354,1,0.925372,"ion (Vaswani et al., 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that"
2020.findings-emnlp.432,D19-1083,0,0.0287233,"To verify that, we remove TEM from the decoder (“SEM⇒IFM”), which significantly increases the alignment error from 0.37 to 0.54 (in Figure 5), and leads to a serious decrease of translation performance (BLEU: 27.45 ⇒ 22.76, in Table 1) on En-De, while results on En-Zh also confirms it (in Figure 6). This indicates that TEM is essential for building word alignment. However, reordering the stacking of TEM and SEM (“SEM⇒TEM⇒IFM”) does not affect the alignment or translation qualities (BLEU: 27.45 vs. 27.61). These results provide empirical support for recent work on merging TEM and SEM modules (Zhang et al., 2019). Robustness to Decoder Depth To verify the robustness of our conclusions, we vary the depth of NMT decoder and train it from scratch. Table 2 demonstrates the results on translation quality, which generally show that more decoder layers bring better performance. Figure 7 shows that SEM behaves similarly regardless of depth. These results demonstrate the robustness of our conclusions. 3.3 Information Fusion in Decoder We now turn to the analysis of IFM. Within the Transformer decoder, IFM plays the critical role of fusing the source and target information by merg4804 0.9 0.9 0.9 0.9 0.8 0.8 0."
2020.wmt-1.34,D18-1457,1,0.850332,"Missing"
2020.wmt-1.34,D18-1045,0,0.0270558,"arge-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target monolingual corpus into source. Then the synthetic parallel corpus is used to train models together with the bilingual data. In this work we apply the noise back-translations method as introduced in (Lample et al., 2018). When translating monolingual data we use an ensemble of two models to get better source translations. We follow (Edunov et al., 2018) to add noise to the synthetic source data. Furthermore, we use a tag at the head of each synthetic source sentence as Caswell et al. (2019) does. To filter the pseudo corpus, we translate the synthetic source into target and calculate a Round-Trip BLEU score, the synthetic pairs are dropped if the BLEU score is lower than 30. Notably, we only apply back translation to the English → German task. We find that back translation decrease the translation quality to Chinese ↔ English tasks in our experiments. 2 https://github.com/mosessmt/mosesdecoder/tree/master/scripts/tokenizer/tokenizer.perl 3 3"
2020.wmt-1.34,D19-1135,1,0.88946,"Missing"
2020.wmt-1.34,N19-4009,0,0.0304603,", 2019) encoder. • B IG D EEP T RANSFORMER is the T RANSFORMER - BIG model with 20 encoder layers. • L ARGER T RANSFORMER is similar to B IG D EEP model except that it uses 8192 as the FFN inner width. The main differences between these models are presented in Table 1. To stabilize the training of deep model, we use the Pre-Norm strategy (Li et al., 2019). The layer normalization was applied to the input of every sub-layer which the computation sequence could be expressed as: normalize → Transform → dropout → residual-add. All models are implemented on top of the open-source toolkit Fairseq3 (Ott et al., 2019). 3.2 Data Augmentation Data augmentation is a commonly used technique to improve the translation quality. There are various of methods to conduct data augmentation such as back-translation (Sennrich et al., 2016a), joint training (Zhang et al., 2018) etc. In this section, we will introduce the methods we used in WMT2020. 3.2.1 Large-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target mono"
2020.wmt-1.34,P16-1009,0,0.261753,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,P16-1162,0,0.542977,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,W19-5341,0,0.0876143,"to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the b"
2020.wmt-1.34,2020.wmt-1.60,1,0.813015,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,W18-6429,1,0.42399,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,W18-6430,0,0.0695928,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,P19-1176,0,0.0535969,"Missing"
2020.wmt-1.34,2020.wmt-1.97,1,0.841946,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,J82-2005,0,0.607939,"Missing"
2020.wmt-1.60,W18-6404,0,0.0212518,"this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness. Approaches We integrated advanced techniques into our systems, including data selection, model ensemble, back/forward translation, larger batch learning, finetuning, and system combination. 4.1 Data Selection Inspired by Ding and Tao (2019), multi-feature language modelling can select high-quality data from a large monolingual or bilingual corpus. We present a four-feature selection criterion, which scoring each sentence by BERT LM (Devlin et al., 2019b), Transformer LM (Bei et al., 2018), N-gram LM (Stolcke, 2002) and FDA (Bic¸ici and Yuret, 2011). Three LMs are complement each other on measuring qualities of sentences while FDA can measure its domain relevance given a in-domain dataset. Sentence pairs in the out-of-domain corpus Checkpoint Average and Model Ensemble 4.3 Finetuning We employ various finetuning strategies at different phases of training. For Sent-Out→Sent-In finetune (same architecture but different data), we first train a sentence-level model on large pseudo-indomain data and then continuously train it on small in-domain data. We apply similar strategy for Do"
2020.wmt-1.60,W11-2131,0,0.0808625,"Missing"
2020.wmt-1.60,N12-1047,0,0.0441032,"decoding. X LM→S ENT B ERT→D OC M BART→S ENT Integration Models IN I N+O UT IN I N+O UT IN I N+O UT I N→I N I N +O UT ∗ I N +O UT Pretrain O UT→I N O UT→I N +O UT I N +O UT I N +O UT I N +O UT BLEU 42.56 59.81 41.87 58.62 45.65 51.12 51.93 54.01 54.59 49.77 51.58 59.61 56.01 57.48 Table 4: BLEU scores of S ENT, D OC, NAT and P RE T RAIN with different finetuning strategies on En⇒De. the sentence reranker contains the best left-to-right (L2R) translation model, R2L (right-to-left) translation model and T2S (target-to-source) translation model. They are integrated by K-best batch MIRA training (Cherry and Foster, 2012) on valid set. 5 Experimental Results Unless otherwise specified, reported BLEU scores are calculated based on combined and tokenized validation set by muti-bleu.perl, which is different from the official evaluation method. 5.1 Ablation Study Table 2 investigates effects of different settings on translation quality. We then apply the best hyperparameters to the models in Section 4 if applicable. Effects of Model Average and Ensemble Following Section 4.2, we averaged top-L checkpoints in S ENT-B model and found that it performs best when L = 5. We followed the same operation for S ENT-S model"
2020.wmt-1.60,N19-1423,0,0.432471,"em combination. Particularly, we proposed a multi-feature data selection on large general-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensemble, back/forward translation and system combination. 1 We experimented mBART after the official submission. 483 Proceedings of the 5th Conference on Machine Translation (WMT), pages 483–491 c Online, November 19–20, 2020. 2020 Associatio"
2020.wmt-1.60,W19-5314,1,0.774159,"tion set and generated a final checkpoint with averaged weights to avoid stochasticity. To combine different models (maybe different architectures), we further ensembled the averaged checkpoints in each model. In our preliminary experiments, we find that this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness. Approaches We integrated advanced techniques into our systems, including data selection, model ensemble, back/forward translation, larger batch learning, finetuning, and system combination. 4.1 Data Selection Inspired by Ding and Tao (2019), multi-feature language modelling can select high-quality data from a large monolingual or bilingual corpus. We present a four-feature selection criterion, which scoring each sentence by BERT LM (Devlin et al., 2019b), Transformer LM (Bei et al., 2018), N-gram LM (Stolcke, 2002) and FDA (Bic¸ici and Yuret, 2011). Three LMs are complement each other on measuring qualities of sentences while FDA can measure its domain relevance given a in-domain dataset. Sentence pairs in the out-of-domain corpus Checkpoint Average and Model Ensemble 4.3 Finetuning We employ various finetuning strategies at dif"
2020.wmt-1.60,2020.wmt-1.3,0,0.0819563,"Missing"
2020.wmt-1.60,2020.tacl-1.47,0,0.105944,"l-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensemble, back/forward translation and system combination. 1 We experimented mBART after the official submission. 483 Proceedings of the 5th Conference on Machine Translation (WMT), pages 483–491 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Tes"
2020.wmt-1.60,W18-6311,0,0.171527,"transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while"
2020.wmt-1.60,D15-1130,0,0.0686515,"ask. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. W"
2020.wmt-1.60,W18-6301,0,0.136336,"y proposed evolved cross-attention (Ding et al., 2020). Technically, we used the most recent effective strategies including back/forward translation, data selection, domain adaptation, batch learning, finetuning, model ensemble and system combination. Particularly, we proposed a multi-feature data selection on large general-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensem"
2020.wmt-1.60,P16-1009,0,0.431512,"w.statmt. org/wmt20/chat-task_results_DA.html. 484 3 https://github.com/Unbabel/BConTrasT. http://www.statmt.org/wmt20/ translation-task.html. 5 https://github.com/ google-research-datasets/Taskmaster. 4 We do not use larger monolingual corpora (e.g. CommonCrawl) and leave this for future work. 2.2 Source Sentence Processing Pre-processing To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generate subwords via Joint BPE (Sennrich et al., 2016b) with 32K merge operations. 3.1 Sentence-level NMT (S ENT) We use standard T RANSFORMER models (Vaswani et al., 2017) with two customized settings. Due to data limitation, we use the small settings (S ENT-S)6 with regular batch size (4096 tokens × 8 GPUs). Based on the base settings (S ENT-B),7 we also empirically adopt big batch learning (Ott et al., 2018) (16348 tokens × 4 GPUs) with larger dropout (0.3). 3.2 Document-level NMT (D OC) To improve discourse properties for chat translation, we re-implement our document-level model (Wang et al., 2017b) on top of T RANS FORMER . Its addition en"
2020.wmt-1.60,P16-1162,0,0.599377,"w.statmt. org/wmt20/chat-task_results_DA.html. 484 3 https://github.com/Unbabel/BConTrasT. http://www.statmt.org/wmt20/ translation-task.html. 5 https://github.com/ google-research-datasets/Taskmaster. 4 We do not use larger monolingual corpora (e.g. CommonCrawl) and leave this for future work. 2.2 Source Sentence Processing Pre-processing To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generate subwords via Joint BPE (Sennrich et al., 2016b) with 32K merge operations. 3.1 Sentence-level NMT (S ENT) We use standard T RANSFORMER models (Vaswani et al., 2017) with two customized settings. Due to data limitation, we use the small settings (S ENT-S)6 with regular batch size (4096 tokens × 8 GPUs). Based on the base settings (S ENT-B),7 we also empirically adopt big batch learning (Ott et al., 2018) (16348 tokens × 4 GPUs) with larger dropout (0.3). 3.2 Document-level NMT (D OC) To improve discourse properties for chat translation, we re-implement our document-level model (Wang et al., 2017b) on top of T RANS FORMER . Its addition en"
2020.wmt-1.60,D19-1633,0,0.0234314,"ls that generate each target word conditioned on previously generated ones, NAT models break the autoregressive factorization and produce target words in parallel (Gu et al., 2018). Although NAT is proposed to speed up the inference, we exploit it to alleviate sequential error accumulation and improve the diversity in conversational translation. To adequately capture the source contexts, we proposed evolved cross-attention for NAT decoder by modeling the local and global attention simultaneously (Ding et al., 2020). Accordingly, we implement our method based on the advanced MaskPredict model (Ghazvininejad et al., 2019)8 , which uses the conditional mask LM (Devlin et al., 2019a) to iteratively generate the target sequence from the masked input. 3.4 Pretraining NMT (P RETRAIN) To transfer the general knowledge to chat translation models, we explore to initialize (part of) model parameters with different pretrained language/generation models. Li et al. (2019) showed 8 https://github.com/facebookresearch/ Mask-Predict. 485 #CP 1 5 10 15 20 ENS En-De 60.32 60.33 60.26 60.19 60.23 60.49 De-En 59.51 59.53 59.42 59.34 59.22 60.08 (a) Model average and ensemble. #BM 4 8 12 14 16 20 En-De 60.33 60.33 60.33 60.34 60."
2020.wmt-1.60,P14-6007,0,0.0288008,"the Tencent AI Lab’s entry into the WMT2020 Chat Translation Task. We explore a breadth of established techniques for building chat translation systems. The paper includes numerous models making use of sentence-level, document-level, non-autoregressive NMT. It also investigates a number of advanced techniques including data selection, model ensemble, finetuing, back/forward translation and initialization using a pretrained LMs. We present extensive experimental results and hope that this work could help both MT researchers and industries to boost the performance of discourse-aware MT systems (Hardmeier, 2014; Wang, 2019). Acknowledgments The authors wish to thank the organizers of WMT2020 Chat Translation for their prompt responses on our questions. The authors also specially thank Dr. Xuebo Liu (University of Macau) and Dr. Siyou Liu (Macao Polytechnic Institute), who kindly support us by their engineering and linguistic suggestions, respectively. Official Results The official automatic evaluation results of our submissions for WMT 2020 are presented in Table 8. For the primary submission, the S YS-1 combines S ENT (ensembled S ENT-B and S ENT-S), D OC and NAT models. As contrastive submissions,"
2020.wmt-1.60,D16-1139,0,0.0422246,"el model on pseudo-in-domain data and then use parts of corresponding parameters to warm-up a document-level model, which will be continuously trained on in-domain data. 4.4 Back/Forward Translation Following Section 2, we obtain processed monolingual data. For back translation (BT), we use the best backward translation model to translate from target to source side and produce the synthetic corpus, which is used to enhance the autoregressive NMT models (Sennrich et al., 2016a). About forward translation (FT), we employ forward translation model to perform sequence distillation for NAT models (Kim and Rush, 2016) . 4.5 System Combination As shown in Figure 1, in order to take full advantages of different systems (Model1 , Model2 and Model3 ), we explore both token- and sentencelevel combination strategies. Token-level We perform token-level combination with confusion network. Concretely, our method follows Consensus Network Minimum Bayes Risk (ConMBR) (Sim et al., 2007), which can be modeled as EConM BR = argminE 0 L(E 0 , Econ ), where Econ was obtained as backbone through performing consensus network decoding. X LM→S ENT B ERT→D OC M BART→S ENT Integration Models IN I N+O UT IN I N+O UT IN I N+O UT"
2020.wmt-1.60,D19-6503,0,0.128451,"9 https://github.com/google-research/ bert. 12 https://github.com/marian-nmt/marian. 13 https://github.com/bicici/FDA. https://github.com/facebookresearch/ XLM. 10 https://github.com/pytorch/fairseq/ tree/master/examples/mbart. 486 Method S ENT-B +Bi-FDA +Bi-FDA-XL +Mono-FDA-XL # Sent. 10K 300K 500K 1M 500K 800K 1M 800K 1M BLEU 41.87 59.36 59.81 59.96 59.86 59.95 59.68 60.36 59.80 Systems S ENT-B S ENT-S D OC NAT Table 3: BLEU scores of S ENT-BASE model on En⇒De task with different FDA variants (three LMs scoring are consistent). S ENT→D OC and we use “h/si” symbols as their pseudo contexts (Kim et al., 2019; Li et al., 2020). Besides, we conduct Sent-Out→Doc-In finetuning (different architectures and data). Specifically, we first train a sentence-level model on pseudo-in-domain data and then use parts of corresponding parameters to warm-up a document-level model, which will be continuously trained on in-domain data. 4.4 Back/Forward Translation Following Section 2, we obtain processed monolingual data. For back translation (BT), we use the best backward translation model to translate from target to source side and produce the synthetic corpus, which is used to enhance the autoregressive NMT mode"
2020.wmt-1.60,2020.acl-main.322,0,0.0386177,"Missing"
2020.wmt-1.60,I17-3009,1,0.817542,". Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth of established techniques for building Chat NMT systems. Specifica"
2020.wmt-1.60,D19-1085,1,0.805322,"owledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in Germa"
2020.wmt-1.60,D17-1301,1,0.937396,". Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth of established techniques for building Chat NMT systems. Specifica"
2020.wmt-1.60,L16-1436,1,0.906273,"resent extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth"
2020.wmt-1.60,2020.wmt-1.97,1,0.642141,"s 483–491 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Test 2,100 Out-of-domain 46,074,573 +filter 33,293,382 +select 1,000,000 Monolingual Out-of-domain De 58,044,806 +filter 56,508,715 +select 1,000,000 Out-of-domain En 34,209,709 +filter 32,823,301 +select 1,000,000 According to the official evaluation results, our systems in De⇒En and De⇐En are respectively ranked 2nd and 4th.2 Furthermore, a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation (Wang et al., 2020) and news translation (Wu et al., 2020) tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Though our empirical experiments, we gain some interesting findings on the chat translation task: 1. The presented data selection method improves the baseline model by up to +18.5 BLEU points. It helps a lot for small-scale data. 2. The large batch learning works well, which makes sentence-level NMT models perform the best among different NMT models. 5. It is difficult to transfer general knowledge from pretrained LMs to the downstream translation task. Data and Processing"
2020.wmt-1.60,2020.wmt-1.34,1,0.813015,"2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Test 2,100 Out-of-domain 46,074,573 +filter 33,293,382 +select 1,000,000 Monolingual Out-of-domain De 58,044,806 +filter 56,508,715 +select 1,000,000 Out-of-domain En 34,209,709 +filter 32,823,301 +select 1,000,000 According to the official evaluation results, our systems in De⇒En and De⇐En are respectively ranked 2nd and 4th.2 Furthermore, a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation (Wang et al., 2020) and news translation (Wu et al., 2020) tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Though our empirical experiments, we gain some interesting findings on the chat translation task: 1. The presented data selection method improves the baseline model by up to +18.5 BLEU points. It helps a lot for small-scale data. 2. The large batch learning works well, which makes sentence-level NMT models perform the best among different NMT models. 5. It is difficult to transfer general knowledge from pretrained LMs to the downstream translation task. Data and Processing 2.1 Data The parallel data we use to t"
2020.wmt-1.60,N19-1095,0,0.0264596,"dels to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our sub"
2020.wmt-1.97,D11-1033,0,0.0270018,"-domain monolingual data from bilingual data in other language pair. Specifically, we collect the English side of the bilingual sentence pairs from Biomedical Translation and UFAL Medical Corpus. The statistics of the in-domain bilingual and monolingual data is listed in Table 2. Bilingual Data 3.3 In-domain bilingual data The in-domain bilingual data is provided by WMT20 biomedical translation shared task. For German-English, we choose Biomedical Translation3 and UFAL Medical Corpus4 to use as the in-domain training data. For Chinese-English out-of-domain (OOD) data, we adopt data selection (Axelrod et al., 2011; Liu et al., 2014) to select the in-house data (8.5M sentence pairs) as the in-domain training data. General-domain bilingual data To alleviate the data scarce problem, we collect generaldomain bilingual data from WMT20 news translation shared task5 . For German-English, we use Europarl-v106 , ParaCrawl-v5.17 , News Commentary-v158 and Wiki Titles-v29 . For 2 https://github.com/pytorch/fairseq (Ott et al., 2019) 3 https://github.com/ biomedical-translation-corpora/corpora 4 https://ufal.mff.cuni.cz/ufal_ medical_corpus 5 http://www.statmt.org/wmt18/ translation-task.html 6 http://www.statmt.o"
2020.wmt-1.97,W19-5403,0,0.0635797,"n shuffled.cs-en shuffled.es-en shuffled.fr-en shuffled.hu-en shuffled.pl-en shuffled.ro-en shuffled.sv-en n/a n/a n/a n/a n/a n/a n/a n/a 37,814,533 n/a n/a n/a n/a n/a n/a n/a 37,814,533 48,243,170 92,999,169 88,526,658 48,783,611 39,442,076 62,034,179 23,142,661 Table 2: The detailed statistics of in-domain training data used in our system. “Zh/En” and “De/En” denote the Chinese-English and German-English bilingual data, respectively. “En” denotes the monolingual English data. use Moses scripts14 to preprocess15 the data and filter the bilingual data with following heuristics rules: Follow Bawden et al. (2019), we use multibleu.perl from Moses16 to compute BLEU scores and report case-sensitive BLEU scores on development and test sets. • Filter out duplicate sentence pairs (Khayrallah and Koehn, 2018; Ott et al., 2018). • Filter out sentence pairs with wrong language (Khayrallah and Koehn, 2018). • Filter out sentences pairs containing more than 120 tokens or fewer than 3. • Filter out sentence pairs with source/target length ratio exceeding 1.5 (Ott et al., 2018). 4.2 Evaluation For German-English, we use the Khresmoi development data as the development set, and use the sentence pairs with the corr"
2020.wmt-1.97,C18-1111,0,0.045754,"Missing"
2020.wmt-1.97,D18-1457,1,0.838269,"on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), p"
2020.wmt-1.97,D19-1082,1,0.890519,"Missing"
2020.wmt-1.97,D19-1135,1,0.809118,"the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), pages 881–886 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Encoder Layer Decoder Layer Attention Heads Emb"
2020.wmt-1.97,N19-1122,1,0.887317,"Missing"
2020.wmt-1.97,2020.emnlp-main.176,1,0.763879,"inal model for the testing. For model inference, the length penalty is set to 0.6 and the beam size is set to 4. The German-English results are listed in Table 3. Our observations are: • Due to the largest model capacity, L ARGE model obtains the best translation performance among the four model variants. • Ensemble decoding with different transformer architectures (E NSEMBLE in Table 3) achieves best translation performance. 4.5 • Leveraging in-domain bilingual data (“+Indomain”) and synthetic bilingual data (“+BT In-domain”) achieves significant translation improvement. Data rejuvenation18 (Jiao et al., 2020) is an approach which exploits the inactive training examples for neural machine translation on large-scale datasets. We adopt the data rejuvenation approach to German⇒English translation task. Experimental results are presented in Tale 7 and the data rejuvenation approach achieves significant improvement over the baseline L ARGE model. 4.4 train the models from scratch. Since the development set and test set have different data distribution, we save checkpoints every epoch and average the last 5 checkpoints rather than choose the model with best validation loss. For model inference, the lengt"
2020.wmt-1.97,W18-2709,0,0.0186969,"3 48,243,170 92,999,169 88,526,658 48,783,611 39,442,076 62,034,179 23,142,661 Table 2: The detailed statistics of in-domain training data used in our system. “Zh/En” and “De/En” denote the Chinese-English and German-English bilingual data, respectively. “En” denotes the monolingual English data. use Moses scripts14 to preprocess15 the data and filter the bilingual data with following heuristics rules: Follow Bawden et al. (2019), we use multibleu.perl from Moses16 to compute BLEU scores and report case-sensitive BLEU scores on development and test sets. • Filter out duplicate sentence pairs (Khayrallah and Koehn, 2018; Ott et al., 2018). • Filter out sentence pairs with wrong language (Khayrallah and Koehn, 2018). • Filter out sentences pairs containing more than 120 tokens or fewer than 3. • Filter out sentence pairs with source/target length ratio exceeding 1.5 (Ott et al., 2018). 4.2 Evaluation For German-English, we use the Khresmoi development data as the development set, and use the sentence pairs with the correct alignment in WMT19 biomedical translation task test set as our test set. For Chinese-English, we use the in-house bilingual test set (1,000 sentence pairs) and the sentence pairs with the c"
2020.wmt-1.97,W17-3204,0,0.0209667,"English bilingual data for the community. The rest of this paper is organized as follows. Section 2 presents our system with four different transformer architectures: D EEP, H YBRID, B IG, L ARGE Transformers. Section 3 describes the training data used in our system, including bilingual data, monolingual data and synthetic bilingual data. Section 4 reports experimental results in two language directions. Finally, we conclude our work in Section 5. Introduction Neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017, NMT) has achieved great progress in recent years. However, as Koehn and Knowles (2017) pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach ("
2020.wmt-1.97,W19-5325,0,0.0377866,"Missing"
2020.wmt-1.97,2020.wmt-1.60,1,0.704639,"Missing"
2020.wmt-1.97,P19-1176,0,0.0310034,"opt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), pages 881–886 c Onli"
2020.wmt-1.97,D17-1155,0,0.0610201,"Missing"
2020.wmt-1.97,P14-2093,1,0.694985,"Missing"
2020.wmt-1.97,N19-4009,0,0.0654804,"Missing"
2020.wmt-1.97,W19-5420,0,0.0561124,"e lack of sufficient in-domain bilingual data, we use an online translation system TranSmart13 to translate the in-domain monolingual English back to Chinese. For German-English, we train a English-German L ARGE model on the combination of in-domain and general-domain bilingual data, and use the model to generate synthetic bilingual data. 4 Experiment We report experimental results in four language pairs: German-English (de/en), English-German (en/de), Chinese-English (zh/en) and EnglishChinese (en/zh). 4.1 Experimental Setup Data Pre-Processing We follow previous work (Saunders et al., 2019; Peng et al., 2019) to 10 http://mteval.cipsc.org.cn:81/ agreement/description 11 https://conferences.unite.un.org/ UNCorpus/ 12 http://data.statmt.org/wikititles/v2/ 13 transmart.qq.com 882 Corpus File Zh/En De/En En Biomedical Translation wmt18training/es-en wmt18training/fr-en wmt18training/pt-en wmt19training/de-en wmt19training/fr-en wmt19training/es-en wmt19training/pt-en wmt20training/it-en wmt20training/ru-en n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 40,398 n/a n/a n/a n/a n/a 287,811 627,576 74,645 40,398 75,049 100,257 49,918 14,756 46,782 UFAL Medical Corpus shuffled.de-en shuffled.cs-en shuffle"
2020.wmt-1.97,W19-5421,0,0.0186626,"r Chinese-English, as we lack of sufficient in-domain bilingual data, we use an online translation system TranSmart13 to translate the in-domain monolingual English back to Chinese. For German-English, we train a English-German L ARGE model on the combination of in-domain and general-domain bilingual data, and use the model to generate synthetic bilingual data. 4 Experiment We report experimental results in four language pairs: German-English (de/en), English-German (en/de), Chinese-English (zh/en) and EnglishChinese (en/zh). 4.1 Experimental Setup Data Pre-Processing We follow previous work (Saunders et al., 2019; Peng et al., 2019) to 10 http://mteval.cipsc.org.cn:81/ agreement/description 11 https://conferences.unite.un.org/ UNCorpus/ 12 http://data.statmt.org/wikititles/v2/ 13 transmart.qq.com 882 Corpus File Zh/En De/En En Biomedical Translation wmt18training/es-en wmt18training/fr-en wmt18training/pt-en wmt19training/de-en wmt19training/fr-en wmt19training/es-en wmt19training/pt-en wmt20training/it-en wmt20training/ru-en n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 40,398 n/a n/a n/a n/a n/a 287,811 627,576 74,645 40,398 75,049 100,257 49,918 14,756 46,782 UFAL Medical Corpus shuffled.de-en sh"
2020.wmt-1.97,P16-1009,0,0.709327,"pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder sta"
2020.wmt-1.97,P16-1162,0,0.826805,"pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder sta"
2020.wmt-1.97,D19-1145,1,0.895388,"Missing"
2020.wmt-1.97,D17-1149,1,0.899559,"Missing"
2020.wmt-1.97,2020.wmt-1.34,1,0.89255,"Missing"
2021.acl-long.266,D16-1162,0,0.0173145,"lists in Table 11. As seen, AT models also suffer from the problem of low-frequency words when using knowledge distillation, and our approach also works for them. Takeaway: Our method works well for general cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulati"
2021.acl-long.266,P05-1066,0,0.372859,"Missing"
2021.acl-long.266,N19-1423,0,0.0209994,"knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks employ Transformer-B IG as their strong teacher except for Ro-En and Small En-De, which are distilled by Transformer-BASE. 4 http://www.statmt.org/wmt19/ translation-task.html Training Traditionally, NAT models are usually trained for 300K steps on regular batch size (i.e. • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from the masked input. We followed its optimal settings to keep the iteration number as 10 and length beam as 5. • Levenshtein Transformer (LevT, Gu et al. 2019) that introduces three steps: deletion, placeholder and token prediction. The decoding iterations adaptively depends on certain conditions. 3435 Model Zh-En Ja-En Law Med. IT Kor. Sub. BLEU ALF BLEU ALF AT 41.5 30.8 27.5 8.6 15.4 AT 25.3 66.2 29.8 70.8 MaskT +LFR 24.2 25.1† 61.5 64.8 28.9 29.6† 66.9 68.9 MaskT +LFR 37.3 38.1† 28.2 28.8 24.6 25.4† 7.3 8.9† 11.2 14.3† LevT +LFR 24.4 25.1† 62.7"
2021.acl-long.266,2021.findings-acl.247,1,0.886121,"Missing"
2021.acl-long.266,2020.coling-main.389,1,0.552002,"Missing"
2021.acl-long.266,2020.emnlp-main.176,1,0.799552,"model for the rest steps. For fair comparison, the total training steps of the proposed method are same as the traditional one. In general, we expect that this training recipe can provide a good trade-off between raw and distilled data (i.e. high-modes and complete vs. low-modes and incomplete). 2.3 Bidirectional Distillation Training Analyzing Bilingual Links in Data KD simplifies the training data by replacing low-frequency target words with high-frequency ones (Zhou et al., 2020). This is able to facilitate easier aligning source words to target ones, resulting in high bilingual coverage (Jiao et al., 2020). Due to the information loss, we argue that KD makes lowfrequency target words have fewer opportunities to align with source ones. To verify this, we propose a method to quantitatively analyze bilingual links from two directions, where low-frequency words similar performance. 3433 are aligned from source to target (s 7→ t) or in an opposite direction (t 7→ s). The method can be applied to different types of data. Here we take s 7→ t links in Raw data as an example to illustrate the algorithm. Given the WMT14 En-De parallel corpus, we employ an unsupervised word alignment method2 (Och and Ney,"
2021.acl-long.266,N13-1073,0,0.0320442,"duct ←− the same analysis method on KD data, and found better t 7→ s links but worse s 7→ t links compared with Raw. Take the Zh-En sentence pair in Ta−→ ble 2 for example, KD retains the source side lowfrequency Chinese words “海克曼” (RawS ) but generates the high-frequency English words “Heck−→ man” instead of the golden “Hackman” (KDT ). On ←− the other hand, KD preserves the low-frequency English words “Hackman” (RawT ) but produces the ←− high-frequency Chinese words “哈克曼” (KDS ). Our Approach Based on analysis results, we propose to train NAT models on bidirectional distil2 The FastAlign (Dyer et al., 2013) was employed to build word alignments for the training datasets. lation by concatenating two kinds of distilled data. The reverse distillation is to replace the source sentences in the original training data with synthetic ones generated by a backward AT teacher.3 Ac←− cording to Equation 3, KD can be formulated as: ←− KD = {(yi , ft7→s (yi ))|yi ∈ Rawt }N i=1 (4) where ft7→s represents an AT-based translation model trained on Raw data for translating text from the target to the source language. Figure 1(c) illustrates the training strategy. First, we employ both fs7→t and ft7→s AT models to"
2021.acl-long.266,D18-1045,0,0.0446041,"mbined pipeline: Raw → KD + KD → KD as out best training strategy. There are many possible ways to implement the general idea of combining two approaches. The aim of this paper is not to explore the whole space but simply to show that one fairly straightforward implementation works well and the idea is reasonable. Nonetheless, we compare possible strategies of combination two approaches as well as demonstrate their complementarity in §3.3. While in main experiments (in §3.2), we valid the combination strategy, namely Low-Frequency Rejuvenation (LFR). 3 This is different from back-translation (Edunov et al., 2018), which is an alternative to leverage monolingual data. 3434 Model Iteration Speed En-De Ro-En BLEU ALF BLEU ALF AT Models Transformer-BASE (Ro-En Teacher) n/a Transformer-B IG (En-De Teacher) n/a 1.0× 27.3 0.8× 29.2 Existing NAT Models NAT (Gu et al., 2018) 1.0 2.4× Iterative NAT (Lee et al., 2018) 10.0 2.0× DisCo (Kasai et al., 2020) 4.8 3.2× Mask-Predict (Ghazvininejad et al., 2019) 10.0 1.5× Levenshtein (Gu et al., 2019) 2.5 3.5× 19.2 21.6 26.8 27.0 27.3 70.5 34.1 73.0 n/a n/a 31.4 30.2 33.3 33.3 33.3 73.6 n/a n/a Our NAT Models Mask-Predict (Ghazvininejad et al., 2019) 27.0 10.0 1.5× +Low"
2021.acl-long.266,D18-1040,0,0.0208174,"rious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulation Our work is related to previous studies on manipulating training data for NMT. Bogoychev and Sennrich (2019) show that forwardand backward-translations (FT/ BT) could both boost the model performances, where FT plays the role of domain adaptation and BT makes the translation fluent. Fadaee and Monz (2018) sample the monolingual data with more difficult words (e.g. rare words) to perform BT, achieving significant improvements compared with randomly sampled BT. Nguyen et al. (2020) diversify the data by applying FT and BT multiply times. However, different from AT, the prerequisite of training a well-performed NAT model is to perform KD. We compared with related works in Table 10 and found that our approach consistently outperforms them. Note that all the ablation studies focus on exploiting the parallel data without augmenting additional data. Non-Autoregressive Translation A variety of approac"
2021.acl-long.266,D19-1633,0,0.280947,"e complementary approaches (i.e. raw pretraining, 3431 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3431–3441 August 1–6, 2021. ©2021 Association for Computational Linguistics bidirectional distillation training and KD finetuning) as a new training strategy for further boosting NAT performance (§2.4). We validated our approach on five translation benchmarks (WMT14 En-De, WMT16 Ro-En, WMT17 Zh-En, WAT17 Ja-En and WMT19 EnDe) over two advanced architectures (Mask Predict, Ghazvininejad et al., 2019; Levenshtein Transformer, Gu et al., 2019). Experimental results show that the proposed method consistently improve translation performance over the standard NAT models across languages and advanced NAT architectures. Extensive analyses confirm that the performance improvement indeed comes from the better lexical translation accuracy especially on low-frequency tokens. Knowledge Distillation Gu et al. (2018) pointed out that NAT models suffer from the multimodality problem, where the conditional independence assumption prevents a model from properly capturing the highly multimodal distributio"
2021.acl-long.266,2020.emnlp-main.76,0,0.0249342,"eneral cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulation Our work is related to previous studies on manipulating training data for NMT. Bogoychev and Sennrich (2019) show that forwardand backward-translations (FT/ BT) could both boost the model perfo"
2021.acl-long.266,2021.naacl-main.313,1,0.78714,"t our approach consistently outperforms them. Note that all the ablation studies focus on exploiting the parallel data without augmenting additional data. Non-Autoregressive Translation A variety of approaches have been exploited to bridge the performance gap between NAT and AT models. Some researchers proposed new model architectures (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Kasai et al., 2020), aided with additional signals (Wang et al., 2019; Ran et al., 2019; Ding et al., 2020), introduced sequential information (Wei et al., 2019; Shao et al., 2019; Guo et al., 2020; Hao et al., 2021), and explored advanced training objectives (Ghazvininejad et al., 2020; Du et al., 2021). Our work is close to the research line on training methods. Ding et al. (2021b) revealed the low-frequency word problem in distilled training data, and introduced an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Ding et al. (2021a) propose a simple and effective training strategy, which progressively feeds different granularity of data into NAT models by leveraging curriculum learning. 5 Conclusion In this study, we propose"
2021.acl-long.266,D16-1139,0,0.116243,"e.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. The translation accuracy of lowfrequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established We closely followed previous works to apply sequence-level knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks employ Transformer-B IG as their strong teacher except for Ro-En and Small En-De, which are distilled by Transformer-BASE. 4 http://www.statmt.org/wmt19/ translation-task.html Training Traditionally, NAT models are usually trained for 300K steps on regular batch size (i.e. • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the t"
2021.acl-long.266,W17-3204,0,0.0572941,"the teacher model. For fair comparison, we leverage the TransformerBASE as the student model, which shares the same model capacity with NAT student (i.e. MaskT). The result lists in Table 11. As seen, AT models also suffer from the problem of low-frequency words when using knowledge distillation, and our approach also works for them. Takeaway: Our method works well for general cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and"
2021.acl-long.266,D18-1149,0,0.314945,"reasonable. Nonetheless, we compare possible strategies of combination two approaches as well as demonstrate their complementarity in §3.3. While in main experiments (in §3.2), we valid the combination strategy, namely Low-Frequency Rejuvenation (LFR). 3 This is different from back-translation (Edunov et al., 2018), which is an alternative to leverage monolingual data. 3434 Model Iteration Speed En-De Ro-En BLEU ALF BLEU ALF AT Models Transformer-BASE (Ro-En Teacher) n/a Transformer-B IG (En-De Teacher) n/a 1.0× 27.3 0.8× 29.2 Existing NAT Models NAT (Gu et al., 2018) 1.0 2.4× Iterative NAT (Lee et al., 2018) 10.0 2.0× DisCo (Kasai et al., 2020) 4.8 3.2× Mask-Predict (Ghazvininejad et al., 2019) 10.0 1.5× Levenshtein (Gu et al., 2019) 2.5 3.5× 19.2 21.6 26.8 27.0 27.3 70.5 34.1 73.0 n/a n/a 31.4 30.2 33.3 33.3 33.3 73.6 n/a n/a Our NAT Models Mask-Predict (Ghazvininejad et al., 2019) 27.0 10.0 1.5× +Low-Frequency Rejuvenation 27.8† 68.4 33.3 72.3 33.9† 70.9 72.4 Levenshtein (Gu et al., 2019) +Low-Frequency Rejuvenation 69.2 33.2 72.8 33.8† 71.1 72.7 2.5 3.5× 27.4 28.2† Table 3: Comparison with previous work on WMT14 En-De and WMT16 Ro-En. “Iteration” indicates the number of iterative refinement wh"
2021.acl-long.266,W17-5706,0,0.0753165,"ding. “ALF” is the translation accuracy on low-frequency words. “† ” indicates statistically significant difference (p &lt; 0.05) from corresponding baselines. 3 3.1 Experiment based on the widely-used automatic alignment tool GIZA++ (Och and Ney, 2003). Setup Models We validated our research hypotheses on two state-of-the-art NAT models: Data Main experiments are conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De, Vaswani et al. 2017), WMT16 Romanian-English (Ro-En, Gu et al. 2018), WMT17 Chinese-English (Zh-En, Hassan et al. 2018), and WAT17 Japanese-English (Ja-En, Morishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the universality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and s"
2021.acl-long.266,N18-1031,0,0.0156507,"words when using knowledge distillation, and our approach also works for them. Takeaway: Our method works well for general cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulation Our work is related to previous studies on manipulating training data for NMT. Bogoyc"
2021.acl-long.266,J03-1002,0,0.127279,"t al., 2020). Due to the information loss, we argue that KD makes lowfrequency target words have fewer opportunities to align with source ones. To verify this, we propose a method to quantitatively analyze bilingual links from two directions, where low-frequency words similar performance. 3433 are aligned from source to target (s 7→ t) or in an opposite direction (t 7→ s). The method can be applied to different types of data. Here we take s 7→ t links in Raw data as an example to illustrate the algorithm. Given the WMT14 En-De parallel corpus, we employ an unsupervised word alignment method2 (Och and Ney, 2003) to produce a word alignment, and then we extract aligned links whose source words are low-frequency (called s 7→ t LFW Links). Second, we randomly select a number of samples from the parallel corpus. For better comparison, the subset should contains the same i in Equation (2) as that of other type of datasets (e.g. i in Equation −→ (3) for KD). Finally, we calculate recall, precision, F1 scores based on low-frequency bilingual links for the subset. Recall (R) represents how many low-frequency source words can be aligned to targets. Precision (P) means how many aligned low-frequency links are"
2021.acl-long.266,P02-1040,0,0.109214,"WAT17 Japanese-English (Ja-En, Morishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the universality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. The translation accuracy of lowfrequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established We closely followed previous works to apply sequence-level knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks employ Transformer-B IG as their strong teacher except for Ro-En and Small"
2021.acl-long.266,N19-1119,0,0.0994126,"Missing"
2021.acl-long.266,2020.acl-main.15,0,0.260218,"he performance improvement indeed comes from the better lexical translation accuracy especially on low-frequency tokens. Knowledge Distillation Gu et al. (2018) pointed out that NAT models suffer from the multimodality problem, where the conditional independence assumption prevents a model from properly capturing the highly multimodal distribution of target translations. Thus, the sequence-level knowledge distillation is introduced to reduce the modes of training data by replacing their original target-side samples with sentences generated by an AT teacher (Gu et al., 2018; Zhou et al., 2020; Ren et al., 2020). Formally, the original parallel data Raw and the −→ distilled data KD can be defined as follows: Contributions Our main contributions are: where fs7→t represents an AT-based translation model trained on Raw data for translating text from the source to the target language. N is the total number of sentence pairs in training data. As shown in Figure 1 (a), well-performed NAT models −→ are generally trained on KD data instead of Raw. • We show the effectiveness of rejuvenating lowfrequency information by pretraining NAT models from raw data. • We provide a quantitative analysis of bilingual lin"
2021.acl-long.266,P16-1162,0,0.0534523,", Gu et al. 2018), WMT17 Chinese-English (Zh-En, Hassan et al. 2018), and WAT17 Japanese-English (Ja-En, Morishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the universality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. The translation accuracy of lowfrequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established We closely followed previous works to apply sequence-level knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks e"
2021.acl-long.266,P19-1125,0,0.0195216,"e compared with related works in Table 10 and found that our approach consistently outperforms them. Note that all the ablation studies focus on exploiting the parallel data without augmenting additional data. Non-Autoregressive Translation A variety of approaches have been exploited to bridge the performance gap between NAT and AT models. Some researchers proposed new model architectures (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Kasai et al., 2020), aided with additional signals (Wang et al., 2019; Ran et al., 2019; Ding et al., 2020), introduced sequential information (Wei et al., 2019; Shao et al., 2019; Guo et al., 2020; Hao et al., 2021), and explored advanced training objectives (Ghazvininejad et al., 2020; Du et al., 2021). Our work is close to the research line on training methods. Ding et al. (2021b) revealed the low-frequency word problem in distilled training data, and introduced an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Ding et al. (2021a) propose a simple and effective training strategy, which progressively feeds different granularity of data into NAT models by leveraging curr"
2021.acl-long.266,D19-1430,0,0.20018,"erent seeds) improves data diversification for NMT, and we leave this for future work. 2.4 Combining Both of Them: Low-Frequency Rejuvenation (LFR) We have proposed two parallel approaches to rejuvenate low-frequency knowledge from authentic (§2.2) and synthetic (§2.3) data, respectively. Intuitively, we combine both of them to further improve the model performance. From data view, two presented training strategies −→ −→ ←− are: Raw → KD (Raw Pretraining) and KD + KD (Bidirectional Distillation Training). Considering the effectiveness of pretraining (Mathis et al., 2021) and clean finetuning (Wu et al., 2019), we introduce −→ ←− −→ a combined pipeline: Raw → KD + KD → KD as out best training strategy. There are many possible ways to implement the general idea of combining two approaches. The aim of this paper is not to explore the whole space but simply to show that one fairly straightforward implementation works well and the idea is reasonable. Nonetheless, we compare possible strategies of combination two approaches as well as demonstrate their complementarity in §3.3. While in main experiments (in §3.2), we valid the combination strategy, namely Low-Frequency Rejuvenation (LFR). 3 This is diffe"
2021.findings-acl.247,P05-1066,0,0.812321,"Missing"
2021.findings-acl.247,N19-1423,0,0.028792,"e preprocessed data via bytepair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations. We evaluated the translation quality with BLEU (Papineni et al., 2002) with statistical significance test (Collins et al., 2005). For fine-grained bilingual knowledge, e.g. word alignment and phrase table, to ensure the source to target mapping more deterministic, we set 0.05 as the probability threshold. Taking WMT14 EnDe for example, there are 3M words and 156M phrases in the original phrase table extracted by • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from the masked input; • Levenshtein Transformer (LevT, Gu et al. 2019) that introduces three steps: deletion, placeholder prediction and token prediction. For regularization, we empirically set the dropout rate as 0.2, and apply weight decay with 0.01 and label smoothing with  = 0.1. We train batches of approximately 128K tokens using Adam (Kingma and Ba, 2015). The learning rate warms up to 5 × 10−4 in the first 10K steps, and then decays with the inverse square-root schedule. We train 50k steps on word-level data and 50k steps on phrase-level da"
2021.findings-acl.247,2021.acl-long.266,1,0.886121,"Missing"
2021.findings-acl.247,2020.acl-main.153,1,0.822376,"Missing"
2021.findings-acl.247,2020.coling-main.389,1,0.381955,"Missing"
2021.findings-acl.247,N10-1140,0,0.0461511,"essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work can be seen as a typical determinism-based curriculum learning (CL) (Bengio et al., 2009) method, where the finer granular2798 Models BLEU Speed Ro-En En-De Zh-En Ja-En AT Models Transformer-BASE (Ro-En Teacher) 1.0× Transformer"
2021.findings-acl.247,D19-1633,0,0.451065,"“high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work can be seen as a typical determinism-based curriculum learning (CL) (Bengio et al., 2009) method, where the finer granular2798 Models BLEU Speed Ro-En En-De Zh-En Ja-En AT Models Transformer-BASE (Ro-En Teacher) 1.0× Transformer-B IG (En-De / Zh-En / Ja-En Teacher) 0.8× 34.1 n/a 27.3 29.2 24.4 25.3 29.2 29.8 Existing NAT Models NAT (Gu et al., 2018) 2.4× Iterative NAT (Lee et al., 2018) 2.0× DisCo (Kasai et al., 2020) 3.2× Levenshtein (Gu et al., 2019) 3.5× Mask-Predict (Ghazvininejad et al., 2019) 1.5× Context-aware NAT (Ding et al., 2020b) 1.5× 31.4 30.2 33.3 33.3 33.3 33.2 19.2 21.6 26.8 27.3 27.0 27.5 n/a n/a n/a n/a 23.2 24.6 n/a n/a n/a n/a n/a 29.4 Our NAT Models Levenshtein (Gu et al., 2019) 3.5× +PMG Training Mask-Predict (Ghazvininejad et al., 2019) 1.5× +PMG Training 33.2 33.8† 33.3 33.7 27.4 27.8 27.0 27.6† 24.4 25.0† 24.0 24.5 29.1 29.6 28.9 29.5† Table 3: Comparison with previous work on WMT16 Ro-En, WMT14 En-De, WMT17 Zh-En and WAT17 Ja-En datasets. “† ” indicates that the proposed method was significantly better than baseline at significance level p<0.05. ities are more"
2021.findings-acl.247,D16-1139,0,0.0522007,"lity against strong NAT baselines. Also, we show that more deterministic fine-grained knowledge can further enhance performance. 1 W ORD P HRASE S ENTENCE 59.8 36.0 29.2 Raw 4 KD 4 57.1 31.7 24.5 -2.7 -4.3 -4.7 59.0 34.2 27.0 -0.8 -1.8 -2.2 Table 1: Translation performance at different granularity on the WMT14 English⇒German dataset. “4” indicates the performance gap between the NAT and AT. independence assumption prevents a model from properly capturing the highly multimodal distribution of target translations. To reduce the modes of training data, sequence-level knowledge distillation (KD) (Kim and Rush, 2016) is widely employed via replacing their original target samples with sentences generated from an AT teacher (Gu et al., 2018; Zhou et al., 2020; Ren et al., 2020). Introduction Non-autoregressive translation (NAT, Gu et al., 2018) has been proposed to improve the decoding efficiency by predicting all tokens independently and simultaneously. Different from autoregressive translation (AT, Vaswani et al., 2017) models that generate each target word conditioned on previously generated ones, NAT models suffer from the multimodality problem (i.e. multiple translations for a single input), in which t"
2021.findings-acl.247,N03-1017,0,0.175871,"gaps between NAT and AT are significant than that of word and phrase in Table 1. Based on the above evidence, it is natural to suspect that the existing sentence-level NAT training is sub-optimal. 2.2 Fine-grained Bilingual Knowledge Phrase table is an essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for"
2021.findings-acl.247,D18-1149,0,0.110574,"Missing"
2021.findings-acl.247,2020.acl-main.41,1,0.866952,"al. (2021) delivered the knowledge from pretrained language models to the NAT models. Above works improve the NAT at the model level, while we improve NAT at the data level. Most related to our work, Ding et al. (2021a) proposed data-level strategies, including reverse distillation and bidirectional distillation, to make the Curriculum Learning Our proposed training strategy is a novel technique for NAT by exploiting curriculum learning (CL). Recent works have shown that CL can help the autoregressive translation (AT) models achieve fast convergence and better results (Platanios et al., 2019; Liu et al., 2020b; Zhan et al., 2021; Zhou et al., 2021). However, CL for non-autoregressive translation (NAT) models has not been well studied. Among the few attempts, Guo et al. (2020a); Liu et al. (2020a) respectively investigated “parameter- and task-level” curriculum learning approaches, while we proposed progressive multi-granularity training for NAT at “datalevel”. To the best of our knowledge, this is the first work to investigate the effects of different granularities of data on NAT models. 5 Conclusion In this paper, we investigated the translation accuracy of different granularities in NAT, and fou"
2021.findings-acl.247,P06-1077,0,0.129713,"rained Bilingual Knowledge Phrase table is an essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work can be seen as a typical determinism-based curriculum learning (CL) (Bengio et al., 2009) method, where the finer granular2798 Models BLEU Speed Ro-En En-De Zh-En Ja-En AT Mod"
2021.findings-acl.247,J03-1002,0,0.0727205,"ificant than that of word and phrase in Table 1. Based on the above evidence, it is natural to suspect that the existing sentence-level NAT training is sub-optimal. 2.2 Fine-grained Bilingual Knowledge Phrase table is an essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work c"
2021.findings-acl.247,P02-1040,0,0.112217,"lishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence pairs, respectively. It is worthy noting that Ro-En, En-De and Zh-En are low-, medium- and high- resource language pairs, and Ja-En is word order divergent language direction. We use the same validation and test datasets with previous works for fair comparison. To avoid unknown works, we preprocessed data via bytepair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations. We evaluated the translation quality with BLEU (Papineni et al., 2002) with statistical significance test (Collins et al., 2005). For fine-grained bilingual knowledge, e.g. word alignment and phrase table, to ensure the source to target mapping more deterministic, we set 0.05 as the probability threshold. Taking WMT14 EnDe for example, there are 3M words and 156M phrases in the original phrase table extracted by • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from the masked input; • Levenshtein Transformer (LevT, Gu et al. 2019) that introduces three steps: del"
2021.findings-acl.247,N19-1119,0,0.490511,"26.8 27.3 27.0 27.5 n/a n/a n/a n/a 23.2 24.6 n/a n/a n/a n/a n/a 29.4 Our NAT Models Levenshtein (Gu et al., 2019) 3.5× +PMG Training Mask-Predict (Ghazvininejad et al., 2019) 1.5× +PMG Training 33.2 33.8† 33.3 33.7 27.4 27.8 27.0 27.6† 24.4 25.0† 24.0 24.5 29.1 29.6 28.9 29.5† Table 3: Comparison with previous work on WMT16 Ro-En, WMT14 En-De, WMT17 Zh-En and WAT17 Ja-En datasets. “† ” indicates that the proposed method was significantly better than baseline at significance level p<0.05. ities are more deterministic than sentences. Thus we compare with typical CL works (Zhang et al., 2019; Platanios et al., 2019) in Section 3.2. SMT methodology. We then filter the items whose translation probability is lower than 0.05 and obtain 0.3M words and 56.5M phrases as the final data. 3 Non-Autoregressive Models We validated our progressive multi-granularity training strategy on two state-of-the-art NAT model structures: 3.1 Experiment Setup Data Experiments were conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence pairs, respectively. It"
2021.findings-acl.247,2020.acl-main.15,0,0.114674,"29.2 Raw 4 KD 4 57.1 31.7 24.5 -2.7 -4.3 -4.7 59.0 34.2 27.0 -0.8 -1.8 -2.2 Table 1: Translation performance at different granularity on the WMT14 English⇒German dataset. “4” indicates the performance gap between the NAT and AT. independence assumption prevents a model from properly capturing the highly multimodal distribution of target translations. To reduce the modes of training data, sequence-level knowledge distillation (KD) (Kim and Rush, 2016) is widely employed via replacing their original target samples with sentences generated from an AT teacher (Gu et al., 2018; Zhou et al., 2020; Ren et al., 2020). Introduction Non-autoregressive translation (NAT, Gu et al., 2018) has been proposed to improve the decoding efficiency by predicting all tokens independently and simultaneously. Different from autoregressive translation (AT, Vaswani et al., 2017) models that generate each target word conditioned on previously generated ones, NAT models suffer from the multimodality problem (i.e. multiple translations for a single input), in which the conditional ∗ NAT AT Liang Ding and Longyue Wang contributed equally to this work. Work was done when Liang Ding and Xuebo Liu were interning at Tencent AI Lab"
2021.findings-acl.247,2020.emnlp-main.83,0,0.0136977,"ni et al., 2017) models that generate each target word conditioned on previously generated ones, NAT models suffer from the multimodality problem (i.e. multiple translations for a single input), in which the conditional ∗ NAT AT Liang Ding and Longyue Wang contributed equally to this work. Work was done when Liang Ding and Xuebo Liu were interning at Tencent AI Lab. Although KD reduces the learning difficulty for NAT, there are still complicated word orders and structures (Gell-Mann and Ruhlen, 2011) in the synthetic sentences, making the NAT performance sub-optimal. To answer this challenge, Saharia et al. (2020); Ran et al. (2021) propose to lowers the bilingual modeling difficulties under the monotonicity assumption, where bilingual sentences are in the same word order. However, they make extensive modifications to model structures or objectives, limiting the applicability of their methods to a boarder range of tasks and languages. Accordingly, we turn to break down the sentencelevel high modes into finer granularities, i.e. bilingual words and phrases, where we assume that finer granularities are easy to be learned by NAT. As shown in Table 1, we analyzed the translation accuracy at three linguisti"
2021.findings-acl.247,P16-1162,0,0.121428,"periment Setup Data Experiments were conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence pairs, respectively. It is worthy noting that Ro-En, En-De and Zh-En are low-, medium- and high- resource language pairs, and Ja-En is word order divergent language direction. We use the same validation and test datasets with previous works for fair comparison. To avoid unknown works, we preprocessed data via bytepair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations. We evaluated the translation quality with BLEU (Papineni et al., 2002) with statistical significance test (Collins et al., 2005). For fine-grained bilingual knowledge, e.g. word alignment and phrase table, to ensure the source to target mapping more deterministic, we set 0.05 as the probability threshold. Taking WMT14 EnDe for example, there are 3M words and 156M phrases in the original phrase table extracted by • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from t"
2021.findings-acl.247,2021.eacl-main.18,0,0.030813,"will leave the exploration of high-quality bilingual knowledge for NAT as a future work. 4 Related Works Non-Autoregressive Translation There still exists a performance gap between AT teacher and its NAT student. To bridge this gap, many studies have been proposed. Ghazvininejad et al. (2019); Gu et al. (2019); Kasai et al. (2020) designed novel model structures to considerably improve the NAT model capacity. Wang et al. (2019); Ran et al. (2021); Ding et al. (2021b); Du et al. (2021) explored to improve the model performance with additional training signals or objectives. Guo et al. (2020b); Su et al. (2021) delivered the knowledge from pretrained language models to the NAT models. Above works improve the NAT at the model level, while we improve NAT at the data level. Most related to our work, Ding et al. (2021a) proposed data-level strategies, including reverse distillation and bidirectional distillation, to make the Curriculum Learning Our proposed training strategy is a novel technique for NAT by exploiting curriculum learning (CL). Recent works have shown that CL can help the autoregressive translation (AT) models achieve fast convergence and better results (Platanios et al., 2019; Liu et al."
2021.findings-acl.247,P16-1008,1,0.849057,"y be sub-optimal. • We propose PMG training to encourage NAT models to learn from easy to hard. The finegrained knowledge distilled by SMT will be dynamically transferred during training. • Experiments across language pairs and model structures show the effectiveness and universality of PMG training. 2 2.1 Methodology Motivation We investigated theories in second-language acquisition: one usually learns a foreign language from word-to-word translation to sentence-to-sentence translation, namely from local to global (Onnis et al., 2008). Bilingual knowledge is at the core of adequacy modeling (Tu et al., 2016), which is a major weakness of the NAT models due to the lacks of autoregressive factorization. Table 2 demonstrates the English⇒Chinese multimodality at different granularities (i.e. word, phrase, sentence levels). As seen, the sentence-level consists of various kinds of modes, including word alignment (“English” vs. “英语”/“英文”), phrase translation (“be good at” vs. “...非常 擅长...”/“...水平 很 高”), and even reordering (“英语” can be subject or object). However, phrase-level modes are less complex with similar structure and word-level modes are simple with token-to-token mapping. Generally, the lower"
2021.findings-acl.247,2002.tmi-tutorials.2,0,0.0906952,"结构 镂空 结构 他 英文 很 好。 他 非常 擅长 英语 。 他的 英语 水平 很 高 。 Table 2: Examples of different translation granularities. ities, there are still some gaps with AT teacher. Also, we showed that finer granularities are easier to be learned, that is, accuracy gap “∆” of WORD is small than that of PHRASE , and SEN TENCE (0.8<1.8<2.2). Thus, we propose a simple and effective training strategy to enhance the ability to handle the sentence-level high modes. More specifically, we generate bilingual lexicons from parallel data by leveraging word alignment and phrase extraction in statistical machine translation (SMT, Zens et al., 2002). Then we guide the NAT model to progressively learn the bilingual knowledge from low to high granularity. Experimental results on four commonly-cited translation benchmarks show that our proposed PROGRESSIVE MULTI - GRANULARITY (PMG) training strategy consistently improves the translation performance. The main contributions are: • Our study reveals that NAT is better at learning fine-grained knowledge. Training with sentences merely may be sub-optimal. • We propose PMG training to encourage NAT models to learn from easy to hard. The finegrained knowledge distilled by SMT will be dynamically t"
2021.findings-acl.247,N19-1189,0,0.112105,"33.3 33.2 19.2 21.6 26.8 27.3 27.0 27.5 n/a n/a n/a n/a 23.2 24.6 n/a n/a n/a n/a n/a 29.4 Our NAT Models Levenshtein (Gu et al., 2019) 3.5× +PMG Training Mask-Predict (Ghazvininejad et al., 2019) 1.5× +PMG Training 33.2 33.8† 33.3 33.7 27.4 27.8 27.0 27.6† 24.4 25.0† 24.0 24.5 29.1 29.6 28.9 29.5† Table 3: Comparison with previous work on WMT16 Ro-En, WMT14 En-De, WMT17 Zh-En and WAT17 Ja-En datasets. “† ” indicates that the proposed method was significantly better than baseline at significance level p<0.05. ities are more deterministic than sentences. Thus we compare with typical CL works (Zhang et al., 2019; Platanios et al., 2019) in Section 3.2. SMT methodology. We then filter the items whose translation probability is lower than 0.05 and obtain 0.3M words and 56.5M phrases as the final data. 3 Non-Autoregressive Models We validated our progressive multi-granularity training strategy on two state-of-the-art NAT model structures: 3.1 Experiment Setup Data Experiments were conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence"
2021.findings-acl.247,2021.iwslt-1.25,1,0.813384,"m pretrained language models to the NAT models. Above works improve the NAT at the model level, while we improve NAT at the data level. Most related to our work, Ding et al. (2021a) proposed data-level strategies, including reverse distillation and bidirectional distillation, to make the Curriculum Learning Our proposed training strategy is a novel technique for NAT by exploiting curriculum learning (CL). Recent works have shown that CL can help the autoregressive translation (AT) models achieve fast convergence and better results (Platanios et al., 2019; Liu et al., 2020b; Zhan et al., 2021; Zhou et al., 2021). However, CL for non-autoregressive translation (NAT) models has not been well studied. Among the few attempts, Guo et al. (2020a); Liu et al. (2020a) respectively investigated “parameter- and task-level” curriculum learning approaches, while we proposed progressive multi-granularity training for NAT at “datalevel”. To the best of our knowledge, this is the first work to investigate the effects of different granularities of data on NAT models. 5 Conclusion In this paper, we investigated the translation accuracy of different granularities in NAT, and found that the NAT models are better at dea"
2021.findings-acl.373,2020.acl-main.705,0,0.0141674,"copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample,"
2021.findings-acl.373,N19-1423,0,0.355934,"ussein Tantawi war anwesend. Table 1: Training objective gap between Seq2Seq LM pre-training and NMT training. LM learns to reconstruct a few source tokens and copy most of them, while NMT learns more translation rather than copying. Underlines denote artificial noises, and highlights indicate expected copying tokens. 2020). As a range of surface, syntactic and semantic information has been encoded in the initialized parameters (Jawahar et al., 2019; Goldberg, 2019), they are expected to bring benefits to NMT models and hence the translation quality. Introduction Self-supervised pre-training (Devlin et al., 2019; Song et al., 2019), which acquires general knowledge from a large amount of unlabeled data to help better and faster learning downstream tasks, has an intuitive appeal for neural machine translation (NMT; Bahdanau et al., 2015; Vaswani et al., 2017). One direct way to utilize pre-trained knowledge is initializing the NMT model with a pre-trained language model (LM) before training it on parallel data (Conneau and Lample, 2019; Liu et al., ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. However, there is a discrepancy between the training objective of sequence-"
2021.findings-acl.373,2021.acl-long.266,1,0.684327,"of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test sets of the IT, Koran, law, and subtitle domains are with 2,000 examples respectively. For training R ANDOM, we used the"
2021.findings-acl.373,2020.emnlp-main.6,0,0.476957,"Missing"
2021.findings-acl.373,P16-1154,0,0.027074,", 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even t"
2021.findings-acl.373,2020.acl-main.244,0,0.0313561,"Missing"
2021.findings-acl.373,P19-1356,0,0.0328339,"Missing"
2021.findings-acl.373,W18-2709,0,0.0142241,"wles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text contains more copying tokens, which sheds light upon future works. 5 Conclusion and Future Work We find that NMT models with pre-training are prone to generate more copying tokens. We introduce a copying ratio and a copying error rate to quantitatively analyze copying behaviors in NMT evaluation. In addition, a simple and effective copying penalty is proposed to enhance the copying behaviors during model inference. Experimental results prove th"
2021.findings-acl.373,D18-1339,0,0.0316042,"is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text c"
2021.findings-acl.373,W17-3204,0,0.0160039,"raining on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah"
2021.findings-acl.373,D11-1034,0,0.0335393,"Missing"
2021.findings-acl.373,2020.acl-main.703,0,0.0349923,"re-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answerin"
2021.findings-acl.373,2020.emnlp-main.210,0,0.43631,"NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural lang"
2021.findings-acl.373,2020.tacl-1.47,0,0.0560822,"Missing"
2021.findings-acl.373,2021.ccl-1.108,0,0.0281972,"Missing"
2021.findings-acl.373,2020.amta-research.14,0,0.0673361,"Missing"
2021.findings-acl.373,W19-5333,0,0.0331562,"Missing"
2021.findings-acl.373,N19-4009,0,0.0143054,"of pre-training on NMT in the perspective of copying behaviors. We expect to provide more evidence for controlling the copying behaviors of NMT models. 2.1 Experimental Setup Data We conducted experiments on the widelyused WMT14 English-German benchmark. We used the processed data provided by Vaswani et al. (2017), which consists of 4.5M sentence pairs.1 We used all the training data for model training. The validation set is newstest2013 of 3,000 examples and the test set is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a tra"
2021.findings-acl.373,W18-6301,0,0.122055,"is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a training batch size of 131K tokens. The hyperparameters keep the same with R ANDOM except the 0.2 label smoothing, 2500 warm-up steps, and 1e-4 maximum learning rate. Evaluation For each model, we selected the checkpoint with the lowest perplexity on the validation set for testing. The beam size is 5 and the length penalty is 0.6. In addition to report1 https://drive.google.com/uc?id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 2 https://github.com/pytorch/fairseq 3 https://github.com/pytor"
2021.findings-acl.373,P02-1040,0,0.11011,"rseq/ blob/master/examples/scaling_nmt/README. md#3-train-a-model 4266 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of"
2021.findings-acl.373,N18-1202,0,0.00927479,"020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural language inference (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monoli"
2021.findings-acl.373,W18-6319,0,0.0179225,"P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. The denominator is the total number of tokens in output sentences. In general, higher Ratio values indicate more copying behaviors produced by the NMT model, and vice versa. Copying E"
2021.findings-acl.373,2020.tacl-1.40,0,0.0209978,"ying errors and thus the BLEU scores get a sharp degradation when setting the copying penalty Figure 3: BLEU scores of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test se"
2021.findings-acl.373,2020.tacl-1.18,0,0.136916,"ll the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of f"
2021.findings-acl.373,W16-2323,0,0.0591289,"tion, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding"
2021.findings-acl.373,2006.amta-papers.25,0,0.166465,"6 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. T"
2021.findings-acl.373,N03-1033,0,0.0595807,"Missing"
2021.findings-acl.373,2020.emnlp-main.208,0,0.123844,"setting CP to 1.2). One possible reason is that the IT domain needs to copy more tokens from the source sentence than translating sentences from other domains, thus the copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Tr"
2021.findings-acl.373,W19-5208,0,0.238508,"Missing"
2021.findings-emnlp.247,W19-5206,0,0.177765,"T and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and EnglishRussian benchmarks show that PT can nicely cowork with BT, leading to state-of-the-art model performances. Extensive analyses show that the tagging mechanism is helpful for enhancing the complementarity between PT and BT by improving the translation of source-original sentences and low-frequency words. Our main contributions are as follows: • We design two probing tasks to investigate the impact of PT and BT on NMT models. • We empirically demonstrate the complementarity between PT and BT. • We show that Tagged BT further improve"
2021.findings-emnlp.247,2021.emnlp-main.263,1,0.833334,"Missing"
2021.findings-emnlp.247,D18-1045,0,0.452877,"Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the"
2021.findings-emnlp.247,2020.acl-main.253,0,0.0509777,"Missing"
2021.findings-emnlp.247,D18-1040,0,0.0187186,"l parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with p"
2021.findings-emnlp.247,2020.emnlp-main.6,0,0.0112886,"ed source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, especially on source-original sentences. Takeaway: 1) PT and BT complementary in terms of originality of sentences; 2) Tagged BT can allevi"
2021.findings-emnlp.247,2021.acl-long.221,1,0.810105,"Missing"
2021.findings-emnlp.247,2020.emnlp-main.210,0,0.605565,"laborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/2K sentence pairs, while those of En-Ru include 2M/3K/3K pairs. Towards better reproducibility, we directly used the BT data provided by Sennrich et al. (2016a)1 , consisting of 2.3M synt"
2021.findings-emnlp.247,2020.acl-main.41,1,0.673903,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2021.findings-acl.373,1,0.886854,"o learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/"
2021.findings-emnlp.247,2020.tacl-1.47,0,0.265899,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2020.acl-main.532,0,0.0613034,"Missing"
2021.findings-emnlp.247,N19-4007,0,0.0122229,"riginal while “All” means the whole testset. Table 2: Translation quality on the En-Ro and En-Ru benchmarks. “+” means incorporating PT and (Tagged) BT into NMT models. 4.2 All Src-Ori denotes the testing data originating in the source language, while Tgt-Ori denotes the data translating from the target language. 2903 Effects of Word Frequency Data augmentation is an effective way to improve the translation quality of low-frequency words (Sennrich et al., 2016b). Thus, we compare the performance of the models on translating different frequencies of words. Specifically, we employed compare-mt (Neubig et al., 2019) to calculate the f-measure of translating low- and high-frequency words (&lt;50 vs. ≥50). As shown in Table 4, PT improves more on translating low-frequency words (58.2 vs. 57.5 scores) while BT performs better on high-frequency words (67.3 vs. 66.7 scores). Furthermore, the combination of PT and tagged BT achieves the best performance on both low- and high-frequency words, leading to an overall improvement on the whole words. Similar phenomenons can be observed by combining self-training and BT (Ding et al., 2021b). Takeaway: 1) PT and BT complementary in terms of frequency of words; 2) Tagged"
2021.findings-emnlp.247,P02-1040,0,0.109581,"ir comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand th"
2021.findings-emnlp.247,W18-6319,0,0.0200641,"., 2020b). Setting To make a fair comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In th"
2021.findings-emnlp.247,2020.tacl-1.18,0,0.187012,"tiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conduct"
2021.findings-emnlp.247,W16-2323,0,0.284038,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1009,0,0.470578,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1162,0,0.668047,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,2006.amta-papers.25,0,0.0314185,"hitectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences"
2021.findings-emnlp.247,D19-1149,0,0.0464751,"Missing"
2021.findings-emnlp.247,D19-1073,0,0.0181896,"n the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with parameter initializa"
2021.findings-emnlp.247,2021.findings-acl.422,1,0.73879,"29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, esp"
2021.findings-emnlp.247,2020.emnlp-main.208,0,0.143525,"al., 2019), which can ac2900 ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2900–2907 November 7–11, 2021. ©2021 Association for Computational Linguistics quire knowledge from unlabeled monolingual data, has shown its effectiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation direct"
2021.findings-emnlp.247,D16-1160,0,0.0238903,"Missing"
2021.findings-emnlp.247,W19-5208,0,0.0274486,"e-art performances on the two benchmarks. Similar tendencies are observed in terms of the TER scores. The above results illustrate the better complementarity between PT and Tagged BT on improving translation quality for NMT models. Analysis We conducted extensive analyses to better understand the improvement of our approach. All results are reported on the En-Ro benchmark. Effects of Sentence Type Recent studies have shown that the evaluation of BT is sensitive to the sentences types, thus we report BLEU scores on the subsets of source-original (Src-Ori) and targetoriginal (Tgt-Ori) datasets (Zhang and Toral, 2019; Model Vanilla + PT + BT + BT + PT + Tagged BT + Tagged BT + PT Tgt 33.7 37.7 38.4 41.2 38.6 41.6 29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and B"
D17-1301,D17-1105,1,0.644977,"Missing"
D17-1301,W12-3156,0,0.0426269,"ation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received relatively little attention fr"
D17-1301,P05-1066,0,0.303284,"Missing"
D17-1301,P15-1166,0,0.0160558,"de and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduced context gate plays an important role."
D17-1301,D13-1176,0,0.00976873,"rical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol en"
D17-1301,P07-2045,0,0.00724727,"Missing"
D17-1301,D15-1166,0,0.0309999,"s been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT model"
D17-1301,P02-1040,0,0.101177,"Missing"
D17-1301,E17-3017,0,0.0145815,"Missing"
D17-1301,P16-1008,1,0.843029,"NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding m"
D17-1301,2011.mtsummit-papers.13,0,0.0990302,"ical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received rel"
D17-1301,N16-1004,0,0.0544309,"or example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduc"
D18-1333,N18-1008,0,0.0193306,"rn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reconstruction score is computed by R(ˆ x"
D18-1333,P05-1066,0,0.120726,"tperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the translation results. It is clear that the proposed m"
D18-1333,P15-1166,0,0.0298915,"ch higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotat"
D18-1333,N16-1101,0,0.0216158,"r. Fortunately, the accuracy of predicting DP positions (DPPs) is much higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction scor"
D18-1333,W10-1737,0,0.232395,"Missing"
D18-1333,P02-1040,0,0.100689,"the DPP-annotated data (“Baseline (+DPPs)”, Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the"
D18-1333,D17-1301,1,0.910434,"Missing"
D18-1333,N16-1113,1,0.686079,"Missing"
D18-1333,P13-1081,0,0.46132,"Missing"
D18-1333,N16-1004,0,0.0282224,"em. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reco"
D19-1085,N18-1118,0,0.038634,"wouldn’t let you buy it? Non-Fixed Error 我 和 露西 只是 要 搬 到 对门。 我们 一 分手 (我) 就 搬 回去。 Once we broke up, I’ll move back. Once we broke up, she’ll move back. Once we broke up, we moved back. Once we broke up, we’ll move back. bilingual hidden representations at decoding steps of previous sentences. They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multi-attention model. More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018). They mainly exploited general anaphora in non-pro-drop languages such as English⇒Russian. 6 Conclusion In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourselevel information for better ZP prediction. Experimental results on both Chinese⇒English and Japanese⇒English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translat"
D19-1085,D13-1135,0,0.0234044,"e hidden states of both the encoder-decoder and the reconstructor to embed the ZPs in the source sentence. Although the calculation of labeling loss relies on explicitly annotated labels, it is only used in training to guide the parameters to learn ZP-enhanced representations. Benefiting from the implicit integration of ZP information, we release the reliance on external ZP prediction model in testing. P (zpt |hrec t ) t=1 T Y (5) 3.2 (4) Discourse-Aware ZP Prediction Discourse information have proven useful for predicting antecedents, which may occur in previous sentences (Zhao and Ng, 2007; Chen and Ng, 2013). Therefore, we further improve ZP prediction with discourse-level context, which is learned together with the joint model. gl (zpt , hrec t ) t=1 where gl (·) is softmax for the ZP labeler. As seen, we integrate the ZP generation component into the ZP translation model. There is no reliance on external ZP prediction models in decoding phase. Encoding Discourse-Level Context Hierarchical structure networks are usually used for modelling discourse context on various natural language processing tasks such query suggestion (Sordoni et al., 2015), dialogue modeling (Serban et al., 2016) and MT (Wa"
D19-1085,W12-4213,0,0.496166,"Missing"
D19-1085,D10-1062,0,0.0858334,"Missing"
D19-1085,P05-1066,0,0.33376,"Missing"
D19-1085,W17-5702,0,0.0150206,"that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translation models in previous work, and achieve a new state-of-the-art on the widelyused subtitle corpus. Manual evaluation confirms that the performance improvement comes from the alleviation of translation errors, which are mainly caused by subjective, objective as well as discourse-aware ZPs. There are two potential extensions to our work. First, we will evaluate our method on other implication phenomena (or called unaligned words (Takeno et al., 2017)) such as tenses and article words for NMT. Second, we will investigate the impact of different contextaware models on ZP translation, including multiattention (Jean et al., 2017b) and context-aware Transformer(Voita et al., 2018). Table 7: Example translations where pronouns in brackets are dropped in original inputs (“I NP.”) but labeled by humans according to references (“R EF.”) and previous sentence (“P RE .”). We italicize some mistranslated errors and highlight the correct ones in bold. overcome the data-level gap, Wang et al. (2016) proposed an automatic approach of ZP annotation by ut"
D19-1085,tiedemann-2012-parallel,0,0.0363956,"Missing"
D19-1085,Q18-1029,1,0.853259,"nces (ˆ x, y) whose source-side sentences are auto-annotated with ZPs. The “+ Reconstruction” is the best model reported in Wang et al. (2018a), which employs two reconstructors to reconstruct the x ˆ from Experiments 4.1 Results on Chinese⇒English Task Setup We conducted translation experiments on both Chinese⇒English and Japanese⇒English translation tasks, since Chinese and Japanese are pro-drop languages while English is not. For Chinese⇒English translation task, we used the data of auto-annotated ZPs (Wang et al., 2018a).3 4 http://www.opensubtitles.org. We followed Wang et al. (2017) and Tu et al. (2018) to use 3 previous sentences as discourse context. 3 5 https://github.com/longyuewangdcu/ tvsub. 925 # Model 1 Baseline 2 3 4 5 Translation #Params BLEU Prediction P R F1 86.7M 31.80 n/a External ZP Prediction (Wang et al., 2018a) + ZP-Annotated Data +0M 32.67 0.67 + Reconstruction +73.8M 35.08 This Work: Joint ZP Prediction and Translation Joint Model +35.6M 36.04† 0.72 + Discourse-Level Context +56.6M 37.11† 0.76 n/a n/a 0.65 0.66 0.68 0.77 0.70 0.77 Table 2: Evaluation of ZP translation and prediction on the Chinese–English data. “#Params” represents the number of parameters used in differe"
D19-1085,W17-4806,0,0.120064,"buy one? Sure. Joey wouldn’t let you buy it? Non-Fixed Error 我 和 露西 只是 要 搬 到 对门。 我们 一 分手 (我) 就 搬 回去。 Once we broke up, I’ll move back. Once we broke up, she’ll move back. Once we broke up, we moved back. Once we broke up, we’ll move back. bilingual hidden representations at decoding steps of previous sentences. They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multi-attention model. More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018). They mainly exploited general anaphora in non-pro-drop languages such as English⇒Russian. 6 Conclusion In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourselevel information for better ZP prediction. Experimental results on both Chinese⇒English and Japanese⇒English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform th"
D19-1085,P18-1117,0,0.418324,"yielded best performances on validation sets. For training the proposed models, the hidden layer sizes of hierarchical model and reconstruction model are 1,000 and 2,000, respectively. We modeled previous three sentences as discourse-level context.5 (6) After we can obtain all sentence-level representations HX = {h−K , . . . , h−1 }, we feed them into a sentence-level encoder to produce a vector that represents the discourse-level context: C = E NCODERsentence (HX ) (7) Here the summary C consists of not only the dependencies between words, but also the relations between sentences. Following Voita et al. (2018), we share the parameters of word-level encoder E NCODERword with the encoder component in standard NMT model. Note that, E NCODERword and E NCODERsentence can be implemented as arbitrary networks, such as recurrent networks (Cho et al., 2014), convolutional networks (Gehring et al., 2017), or self-attention networks (Vaswani et al., 2017). In this study, we used recurrent networks to implement our E NCODER. Integrating Discourse into ZP Prediction We directly feed the discourse-level context to the reconstructor to improve ZP prediction. Specifically, we combine the context vector and the rec"
D19-1085,D10-1086,0,0.182667,"Missing"
D19-1085,W10-1737,0,0.0613995,"Missing"
D19-1085,D17-1301,1,0.929721,"3). Therefore, we further improve ZP prediction with discourse-level context, which is learned together with the joint model. gl (zpt , hrec t ) t=1 where gl (·) is softmax for the ZP labeler. As seen, we integrate the ZP generation component into the ZP translation model. There is no reliance on external ZP prediction models in decoding phase. Encoding Discourse-Level Context Hierarchical structure networks are usually used for modelling discourse context on various natural language processing tasks such query suggestion (Sordoni et al., 2015), dialogue modeling (Serban et al., 2016) and MT (Wang et al., 2017). Therefore, we employ hierarchical encoder (Wang et al., 2017) to encoder discourseTraining and Testing The newly introduced prediction component is trained together with the 1 We introduce “ heosi” to cover the case that a pronoun is missing at the end of a sentence. 2 We employ the pronoun vocabulary used in Wang et al. (2016), which contains 30 distinct Chinese pronouns. 924 level context for NMT. More specifically, we use the previous K source sentences X = {x−K , . . . , x−1 } as the discourse information, which is summarized with a two-layer hierarchical encoder, as shown in Figure 2. F"
D19-1085,D18-1333,1,0.851182,"interactive attention models: enc α ˆ enc = ATTenc (xt−1 , hrec ) t−1 , h α ˆ dec = dec enc ˆt ) ATTdec (xt−1 , hrec ,c t−1 , h Figure 2: Architecture of hierarchical neural encoder. x−K , . . . , x−1 are K previous sentences before the current source sentence “你 烤 的 吗 ?” in a text. (2) (3) encoder-decoder-reconstructor:  J(θ, γ, ψ) = arg max log L(y|x; θ) {z } | θ,γ,ψ likelihood + log R(x|henc , hdec ; θ) | {z } reconstruction  rec + log P (zp|h ; θ, γ) {z } | ZP labeling The interaction between two attention models leads to a better exploitation of the encoder and decoder representations (Wang et al., 2018b). ZP Prediction as Sequence Labelling We cast ZP prediction as a sequence labelling task, where each word is labelled if there is a pronoun missing before it. Given the input x = {x1 , x2 , . . . , xT } with the last word xT being the end-of-sentence tag “ heosi”,1 the output to be labelled is a sequence of labels zp = {zp1 , zp2 , . . . , zpT } with zpt ∈ {N } ∪ Vzp . Among the label set, “N ” denotes no ZP, and Vzp is the vocabulary of pronouns.2 Taking Figure 1 as an example, the label sequence “N N N 它 N N” indicates that the pronoun “它” is missing before the fourth word “吗” in the sourc"
D19-1085,C10-1080,0,0.0468499,"Missing"
D19-1085,N16-1113,1,0.922694,"o components to interact with each other. 2. Our study demonstrates the effectiveness of discourse-level context for ZP prediction. 3. Based on our manually-annotated testset, we conduct extensive analyses to assess ZP prediction and translation. 2 2.1 等 我 搬进来，(我 我) 能 买 台 电视 吗？ Can I get a TV when I move in? When I move in to buy a TV. 这块 蛋糕 很 美味！你 烤 的 (它 它) 吗？ The cake is very tasty! Did you bake it? The cake is delicious! Are you baked? 2.2 Background Bridging Data Gap Between ZP Prediction and Translation Recent efforts have explored ways to bridge the gap of ZP prediction and translation (Wang et al., 2016, 2018a,b) by training both models on the homologous data. The pipeline involves two phases, as described below. Zero Pronoun In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English (Zhao and Ng, 2007). As seen in Table 1, the subject pronoun (“我”) and the object pronoun (“它”) are omitted in Chinese sentences (“Inp.”) while these pronouns are all compulsory Translation-Oriented ZP Prediction Its goal is to recall the ZPs in the source sentence (i.e. prodrop language) with the information of the target sentence (i.e."
D19-1085,P02-1040,0,0.103671,"Missing"
D19-1085,P13-1081,0,0.388518,"Missing"
D19-1085,Y15-1050,0,0.345278,"Missing"
D19-1085,C10-1135,0,0.0554474,"Missing"
D19-1085,N15-1052,0,0.216225,"Missing"
D19-1085,D17-1135,0,0.0842924,"Missing"
D19-1085,D07-1057,0,0.489977,"Can I get a TV when I move in? When I move in to buy a TV. 这块 蛋糕 很 美味！你 烤 的 (它 它) 吗？ The cake is very tasty! Did you bake it? The cake is delicious! Are you baked? 2.2 Background Bridging Data Gap Between ZP Prediction and Translation Recent efforts have explored ways to bridge the gap of ZP prediction and translation (Wang et al., 2016, 2018a,b) by training both models on the homologous data. The pipeline involves two phases, as described below. Zero Pronoun In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English (Zhao and Ng, 2007). As seen in Table 1, the subject pronoun (“我”) and the object pronoun (“它”) are omitted in Chinese sentences (“Inp.”) while these pronouns are all compulsory Translation-Oriented ZP Prediction Its goal is to recall the ZPs in the source sentence (i.e. prodrop language) with the information of the target sentence (i.e. non-pro-drop language) in a paral922 lel corpus. Taking the second case (assuming that Inp. and Ref. are sentence pair in a parallel corpus) in Table 1 for instance, the ZP “它 (it)” is dropped in the Chinese side while its equivalent “it” exists in the English side. It is possib"
D19-1088,D17-1042,0,0.384726,"ral Machine Translation with Word Importance Shilin He1,2 Zhaopeng Tu3∗ Xing Wang3 Longyue Wang3 Michael R. Lyu1,2 Shuming Shi3 1 Department of Computer Science and Engineering, The Chinese University of Hong Kong 2 Shenzhen Research Institute, The Chinese University of Hong Kong 1,2 {slhe,lyu}@cse.cuhk.edu.hk 3 Tencent AI Lab 3 {zptu,brightxwang,vinnylywang,shumingshi}@tencent.com Abstract 2017) or hidden units (Bau et al., 2019; Ding et al., 2017). Another direction focuses on understanding the importance of input words by interpreting the input-output behavior of NMT models. Previous work (Alvarez-Melis and Jaakkola, 2017) treats NMT models as black-boxes and provides explanations that closely resemble the attention scores in NMT models. However, recent studies reveal that attention does not provide meaningful explanations since the relationship between attention scores and model output is unclear (Jain and Wallace, 2019). In this paper, we focus on the second thread and try to open the black-box by exploiting the gradients in NMT generation, which aims to estimate the word importance better. Specifically, we employ the integrated gradients method (Sundararajan et al., 2017) to attribute the output to the input"
D19-1088,W16-1601,0,0.0612708,"Missing"
D19-1088,P17-1080,0,0.0264362,"understanding NMT by identifying undertranslated words. • We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universa"
D19-1088,C08-1018,0,0.0267726,"n) as introduced in Section 3.2. In Section 4.1, to ensure that the translation performance decrease attributes to the selected words instead of the perturbation operations, we randomly select the same number of words to perturb (Random), which serves as a baseline. Since there is no ranking for content words, we randomly select a set of content words as important words. To avoid the potential bias introduced by randomness (i.e., Random and Con• Deletion perturbation removes the selected words from the input sentence, and it can be regarded as a specific instantiation of sentence compression (Cohn and Lapata, 2008). • Mask perturbation replaces embedding vectors of the selected words with all-zero vectors (Arras et al., 2016), which is similar to Deletion perturbation except that it retains the placeholder. • Grammatical Replacement perturbation replaces a word by another word of the same linguistic role (i.e., POS tags), yielding a sentence that is grammatically correct but semantically nonsensical (Chomsky and Lightfoot, 2002; Gulordava et al., 2018), such as “colorless green ideas sleep furiously”. Figure 2 illustrates the experimental results on Chinese⇒English translation with Transformer. It 956 D"
D19-1088,P17-1106,0,0.0239557,"rovide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universality of our findings. Concerning interpreting the input-out"
D19-1088,1997.mtsummit-papers.1,0,0.269192,"Missing"
D19-1088,D18-1548,0,0.0196634,"n, which we leave to the future work. 6 Acknowledgement Discussion and Conclusion We approach understanding NMT by investigating the word importance via a gradient-based method, which bridges the gap between word importance and translation performance. Empirical results show that the gradient-based method is superior to several black-box methods in estimating the word importance. Further analyses show that important words are of distinct syntactic categories on different language pairs, which might support the viewpoint that essential inductive bias should be introduced into the model design (Strubell et al., 2018). Our study also suggests the possibility of detecting the notorious under-translation problem via the gradient-based method. This paper is an initiating step towards the general understanding of NMT models, which may bring some potential improvements, such as Shilin He and Michael R. Lyu were supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14210717 of the General Research Fund), and Microsoft Research Asia (2018 Microsoft Research Asia Collaborative Research Award). We thank the anonymous reviewers for their insightful comments and sugg"
D19-1088,N18-1108,0,0.0186535,"Con• Deletion perturbation removes the selected words from the input sentence, and it can be regarded as a specific instantiation of sentence compression (Cohn and Lapata, 2008). • Mask perturbation replaces embedding vectors of the selected words with all-zero vectors (Arras et al., 2016), which is similar to Deletion perturbation except that it retains the placeholder. • Grammatical Replacement perturbation replaces a word by another word of the same linguistic role (i.e., POS tags), yielding a sentence that is grammatically correct but semantically nonsensical (Chomsky and Lightfoot, 2002; Gulordava et al., 2018), such as “colorless green ideas sleep furiously”. Figure 2 illustrates the experimental results on Chinese⇒English translation with Transformer. It 956 Deletion Mask Deletion Mask 2 2525 202020 202020 2020 151515 3 4 er of Operations 5 Random Random Random Frequency Frequency Frequency Content Content Content Attention Attention Attention Attribution Attribution Attribution 101010 0 0 0 1 11 2 22 3 33 4 4 4 5 55 Number Number ofOperations Operations Number of of Operations (a) Deletion 151515 BLEU BLEU 252525 BLEU BLEU BLEU 252525 BLEU BLEU BLEU m ncy t on tion Replacement (Same POS) Replacem"
D19-1088,P19-1354,1,0.774881,"dentifying undertranslated words. • We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universality of our findings"
D19-1088,P17-1141,0,0.0261181,"comparing words of least importance and human-annotated under-translated words. As seen, our Attribution method consistently and significantly outperforms both Erasure and Attention approaches. By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters. Although the accuracy is not high, it is worth noting that our under-translation method is very simple and straightforward. This is potentially useful for debugging NMT models, e.g., automatic post-editing with constraint decoding (Hokamp and Liu, 2017; Post and Vilar, 2018). In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section 5.1) and design better architectures for specific languages (Section 5.2). Due to the space limitation, we only analyze the results of Chinese⇒English, English⇒French, and English⇒Japanese. We list the results on the reverse directions in Appendix, in which the general conclusions also hold. 5.1 Type Noun Verb Adj. Prep. Dete. Punc. Others ≥2 1 (0, 1) 0 Low Middle High Effect on Detecting Translation Errors In this experiment, we propose to use the"
D19-1088,N19-1357,0,0.368665,"encent AI Lab 3 {zptu,brightxwang,vinnylywang,shumingshi}@tencent.com Abstract 2017) or hidden units (Bau et al., 2019; Ding et al., 2017). Another direction focuses on understanding the importance of input words by interpreting the input-output behavior of NMT models. Previous work (Alvarez-Melis and Jaakkola, 2017) treats NMT models as black-boxes and provides explanations that closely resemble the attention scores in NMT models. However, recent studies reveal that attention does not provide meaningful explanations since the relationship between attention scores and model output is unclear (Jain and Wallace, 2019). In this paper, we focus on the second thread and try to open the black-box by exploiting the gradients in NMT generation, which aims to estimate the word importance better. Specifically, we employ the integrated gradients method (Sundararajan et al., 2017) to attribute the output to the input words with the integration of first-order derivatives. We justify the gradient-based approach via quantitative comparison with black-box methods on a couple of perturbation operations, several language pairs, and two representative model architectures, demonstrating its superiority on estimating word im"
D19-1088,D18-1512,0,0.0622219,"Missing"
D19-1088,W17-5706,0,0.0176538,"first choose two large-scale datasets that are publicly available, i.e., Chinese-English and EnglishFrench. Since English, French, and Chinese all belong to the subject-verb-object (SVO) family, we choose another very different subject-object-verb (SOV) language, Japanese, which might bring some interesting linguistic behaviors in EnglishJapanese translation. For Chinese-English task, we use WMT17 Chinese-English dataset that consists of 20.6M sentence pairs. For English-French task, we use WMT14 English-French dataset that comprises 35.5M sentence pairs. For English-Japanese task, we follow (Morishita et al., 2017) to use the first two sections of WAT17 English-Japanese dataset that consists of 1.9M sentence pairs. Following the standard NMT procedure, we adopt the standard byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all language pairs. We believe that these datasets are large enough to confirm the rationality and validity of our experimental analyses. Evaluation We evaluate the effectiveness of estimating word importance by the translation performance decrease. More specifically, unlike the usual way, we measure the decrease of translation performance when perturbing"
D19-1088,N18-1119,0,0.0171115,"st importance and human-annotated under-translated words. As seen, our Attribution method consistently and significantly outperforms both Erasure and Attention approaches. By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters. Although the accuracy is not high, it is worth noting that our under-translation method is very simple and straightforward. This is potentially useful for debugging NMT models, e.g., automatic post-editing with constraint decoding (Hokamp and Liu, 2017; Post and Vilar, 2018). In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section 5.1) and design better architectures for specific languages (Section 5.2). Due to the space limitation, we only analyze the results of Chinese⇒English, English⇒French, and English⇒Japanese. We list the results on the reverse directions in Appendix, in which the general conclusions also hold. 5.1 Type Noun Verb Adj. Prep. Dete. Punc. Others ≥2 1 (0, 1) 0 Low Middle High Effect on Detecting Translation Errors In this experiment, we propose to use the estimated word importan"
D19-1088,N19-4006,0,0.0510171,"Missing"
D19-1088,P16-1162,0,0.147475,"very different subject-object-verb (SOV) language, Japanese, which might bring some interesting linguistic behaviors in EnglishJapanese translation. For Chinese-English task, we use WMT17 Chinese-English dataset that consists of 20.6M sentence pairs. For English-French task, we use WMT14 English-French dataset that comprises 35.5M sentence pairs. For English-Japanese task, we follow (Morishita et al., 2017) to use the first two sections of WAT17 English-Japanese dataset that consists of 1.9M sentence pairs. Following the standard NMT procedure, we adopt the standard byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all language pairs. We believe that these datasets are large enough to confirm the rationality and validity of our experimental analyses. Evaluation We evaluate the effectiveness of estimating word importance by the translation performance decrease. More specifically, unlike the usual way, we measure the decrease of translation performance when perturbing a set of important words that are of top-most word importance in a sentence. The more translation performance degrades, the more important the word is. We use the standard BLEU score as the evaluation metric for"
D19-1088,D16-1159,0,0.194425,"-the-art results on a mass of language pairs with varying structural differences, such as English-French (Bahdanau et al., 2014; Vaswani et al., 2017) and Chinese-English (Hassan et al., 2018). However, so far not much is known about how and why NMT works, which pose great challenges for debugging NMT models and designing optimal architectures. The understanding of NMT models has been approached primarily from two complementary perspectives. The first thread of work aims to understand the importance of representations by analyzing the linguistic information embedded in representation vectors (Shi et al., 2016; Belinkov et al., ∗ Zhaopeng Tu is the corresponding author. Work was mainly done when Shilin He was interning at Tencent AI Lab. 953 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 953–962, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the feature importance. Starting from this observation, we exploit the intermediate gradients to better estimate word importance, which consistently outperforms its attention counterpart across model a"
D19-1145,W13-2322,0,0.0389766,"of relationship path between words, sequential PE measures the sequential distance between the words. As shown in Figure 1 (a), for each word, absolute sequential position represents the sequential distance to the beginning of the sentence, while relative sequential position measures the relative distance to the queried word (“talk” in the example). The latent structure can be interpreted in various ways, from syntactic tree structures, e.g., constituency tree (Collins, 2003) or dependency tree (K¨ubler et al., 2009), to semantic graph structures, e.g., abstract meaning representation graph (Banarescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the tree depth of the word in the dependency tree as its absolute structural position. Specifically, we treat the main verb (Tapanainen and Jarvi"
D19-1145,J03-4003,0,0.205527,"ctic relationships among input words. Figure 1 shows an example to illustrate the idea of the proposed approach. From the perspective of relationship path between words, sequential PE measures the sequential distance between the words. As shown in Figure 1 (a), for each word, absolute sequential position represents the sequential distance to the beginning of the sentence, while relative sequential position measures the relative distance to the queried word (“talk” in the example). The latent structure can be interpreted in various ways, from syntactic tree structures, e.g., constituency tree (Collins, 2003) or dependency tree (K¨ubler et al., 2009), to semantic graph structures, e.g., abstract meaning representation graph (Banarescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the"
D19-1145,P18-1198,0,0.197153,"we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to encode the distance of each word pair in the tree. We implement our structural encoding strategies on top of T RANSFORMER (Vaswani et al., 2017) and conduct experiments on both NIST Chinese⇒English and WMT14 English⇒German translation tasks. Experimental results show that exploiting structural position encoding strategies consistently boosts performance over both the absolute and relative sequential position representations across language pairs. Linguistic analyses (Conneau et al., 2018) reveal that the proposed structural position representation improves the translation performance with richer syntactic information. Our main contributions are: • Our study demonstrates the necessity and effectiveness of exploiting structural position encoding for S ANs, which benefits from modeling syntactic depth and distance under the latent structure of the sentence. • We propose structural position representations for S ANs to encode the latent structure of the input sentence, which are complementary to their sequential counterparts. 2 Background Self-Attention SANs produce representation"
D19-1145,N19-1423,0,0.0455592,"e to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese⇒English and WMT14 English⇒German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to"
D19-1145,D18-1457,1,0.850735,"and structure position representations1 : asb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utili"
D19-1145,N16-1024,0,0.0650603,"Missing"
D19-1145,P17-2012,0,0.0278999,"Missing"
D19-1145,D19-1082,1,0.844139,"Missing"
D19-1145,D19-1135,1,0.917158,", 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et al. (2019c,b) integrate the recurrence into the S ANs and empirically demonstrate that the hybrid models achieve better performances by modeling structure of sentences. Hao et al. (2019a) further make use of the multi-head attention to form the multi-granularity self-attention, to capture the different granularity phrases in source sentences. The difference is that we treat the position representation as a medium to transfer the structure information from the dependency tree into the S ANs. 1 We also use parameter-free element-wise addition method to combine two absolute position embedding and get 0.28"
D19-1145,P03-1054,0,0.105534,"ST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed position encoding strategies on T RANSFORMER (Vaswani et al., 2017) and implement them on top of THUMT (Zhang et al., 2017). We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the structural structural absolute and relative position as described in Section 3. When using relative structural position encoding, we use clipping distance r = 16. To make a fair comparison, we valid different position encoding strategies on the encoder and keep the T RANSFORMER decoder unchanged. Effect of Position Encoding We first remove the sequential encoding from the Transformer encoder (Model #1) and observe the translation performance degrades dramatically (28.33 − 44.31 = −15.98), which demonstrates the necessity of the position encoding strategie"
D19-1145,W04-3250,0,0.0760548,"t al. (2019c) Transformer-Big + Structural PE + Relative Sequential PE + Structural PE MT04 46.49 47.12↑ 47.01 47.37⇑ Zh⇒En MT05 45.21 45.84 45.65 46.20⇑ MT06 44.87 45.64⇑ 45.87⇑ 46.18⇑ Avg 45.47 46.06 46.00 46.40 En⇒De WMT14 28.98 28.58 28.88 28.90 29.19⇑ Table 2: Evaluation of translation performance on NIST Zh⇒En and WMT14 En⇒De test sets. Hao et al. (2019c) is a Transformer-Big model which adopted an additional recurrence encoder with the attentive recurrent network to model syntactic structure. “↑ / ⇑”: significant over the Transformer-Big (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). Model BASE + Rel. Seq. PE + Stru. PE SeLen 92.20 89.82 89.54 Surface WC Avg 63.00 77.60 63.17 76.50 62.90 76.22 TrDep 44.74 45.09 46.12 Syntactic ToCo BShif 79.02 71.24 78.45 71.40 79.12 72.36 Avg 65.00 64.98 65.87 Tense 89.24 88.74 89.30 SubN 84.69 87.00 85.47 Semantic ObjN SoMo 84.53 52.13 85.53 51.68 84.94 52.90 CoIn 62.47 62.21 62.99 Avg 74.61 75.03 75.12 Table 3: Performance on linguistic probing tasks. The probing tasks were conducted by evaluating linguistics embedded in the Transformer-Base encoder outputs. “Base”, “+ Rel. Seq. PE”, “+ Stru. PE” denote TransformerBase, Transformer-Ba"
D19-1145,P02-1040,0,0.103911,". The experimental results on the development set are shown in Table 1. Table 1: Impact of the position encoding components on Chinese⇒English NIST02 development dataset using Transformer-Base model. “Abs.” and “Rel.” denote absolute and relative position encoding, respectively. “Spd.” denotes the decoding speed (sentences/second) on a Tesla M40, the speed of structural position encoding strategies include the step of dependency parsing. 5 Model Variations Experiment We conduct experiments on the widely used NIST Chinese⇒English and WMT14 English⇒German data, and report the 4-gram BLEU score (Papineni et al., 2002). Chinese⇒English We use the training dataset consists of about 1.25 million sentence pairs. NIST 2002 (MT02) dataset is used as development set. NIST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed posit"
D19-1145,D16-1244,0,0.107899,"Missing"
D19-1145,N18-2074,0,0.405265,"n tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structur"
D19-1145,Q19-1002,0,0.0399602,"Missing"
D19-1145,D18-1548,0,0.0412553,", we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese⇒English and WMT14 English⇒German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this"
D19-1145,P19-1032,0,0.0226086,"ere fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et al. (2019c,b) integrate the"
D19-1145,N19-1122,1,0.884589,"Missing"
D19-1145,P15-1150,0,0.0469481,"ks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structural depths and distances (Hewitt and Manning, 2019). Accordingly, we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to enc"
D19-1145,N19-1419,0,0.0324287,"pture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structural depths and distances (Hewitt and Manning, 2019). Accordingly, we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to encode the distance of each word pair in the tree. We implement our structural encoding strategies on top of T RANSFORMER (Vaswani et al., 2017) and conduct experiments on both NIST Chinese⇒English and WMT14 English⇒German translation tasks. Experimental results show that exploiting structural position encoding strategies consistently boosts performance over both the absolute and relative sequential position representations across language pairs. Lingui"
D19-1145,A97-1011,0,0.169453,"arescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the tree depth of the word in the dependency tree as its absolute structural position. Specifically, we treat the main verb (Tapanainen and Jarvinen, 1997) of the sentence as the origin and use the distance of the dependency path from the target word to the origin as the absolute structural position absstru (xi ) = distancetree (xi , origin), (5) where xi is the target word, tree is the given dependency structure and the origin is the main verb of the tree. In the field of NMT, BPE sub-words and endof-sentence symbol should be carefully handled as they do not appear in the conventional dependency tree. In this work, we assign the BPE sub-words share the absolute structural position of the original word and set the the first larger integer than t"
D19-1145,C18-1255,0,0.0502484,"sb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structur"
D19-1145,P19-1624,1,0.835259,"uctural PE) achieves further improvement up to +0.40 BLEU points and outperforms the Transformer-Big by 0.93 BLEU points. For English⇒German, similar phenomenon is observed, which reveals that the proposed structural position encoding strategy can consistently boost translation performance over both the absolute and relative sequential position representations. 5.3 Linguistic Probing Evaluation We conduct probing tasks3 (Conneau et al., 2018) to evaluate structure knowledge embedded in the encoder output in the variations of the Base model that are trained on En⇒De translation task. We follow Wang et al. (2019) to set model configurations. The experimental results on probing tasks are shown in Table 3, and the BLEU scores of “Base”, “+ Rel. Seq. PE”, “+ Stru. PE” are 3 https://github.com/facebookresearch/ SentEval/tree/master/data/probing 27.31, 27.99 and 28.30. From the table, we can see 1) adding the relative sequential positional embedding achieves improvement over the baseline on semantic tasks (75.03 vs. 74.61). This may indicate the model benefits more from semantic modeling; 2) with the structural positional embedding, the model obtains improvement on syntactic tasks (65.87 v.s. 64.98), which"
D19-1145,D18-1408,0,0.0426954,"PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representation"
D19-1145,D18-1475,1,0.83168,"epresentations1 : asb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree t"
D19-1145,N19-1407,1,0.85324,"S PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et"
D19-1145,P17-4012,0,0.105233,"002 (MT02) dataset is used as development set. NIST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed position encoding strategies on T RANSFORMER (Vaswani et al., 2017) and implement them on top of THUMT (Zhang et al., 2017). We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the structural structural absolute and relative position as described in Section 3. When using relative structural position encoding, we use clipping distance r = 16. To make a fair comparison, we valid different position encoding strategies on the encoder and keep the T RANSFORMER decoder unchanged. Effect of Position Encoding We first remove the sequential encoding from the Transformer encoder (Model #1) and observe the translation performance degrades dramatically (28.33 − 44.31 = −15.98), which demonst"
I17-3009,P07-2045,0,0.0193009,"Missing"
I17-3009,P03-1021,0,0.0209504,"SLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4, Recog indicates NE recognition on the"
I17-3009,J03-1002,0,0.0059894,"20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4,"
I17-3009,D17-1301,1,0.67952,"Missing"
I17-3009,N16-1113,1,0.838057,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
I17-3009,L16-1436,1,0.749687,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
K15-2011,S15-1002,0,0.106015,"ation, are described in (Lin et al., 2014). We consider the embedding models which lead to two different types of intermediate representations. The relational phrase embedding model considers the dependency within words uniformly without considering the second-order effect. The word-pair embedding model considers the secondorder effect of specific combinations within the word-pairs in Arg1 and Arg2. If we plug in a paragraph vector model for the relational phrase embedding model, the model considers the effect of uni-gram within a sentence as a sequence. If we plug in a RNN-LSTM model (Le and Zuidema, 2015), the model considers the effect of uni-gram within a sentence as a tree. Introduction This paper describes the discourse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task (Xue et al., 2015). We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task. We divide the whole process into two stages: the first stage concerns an identification of triples (Arg1, Conn, Arg2) and pairs (Arg1, Arg2) while the second stage concerns a sense classification of the identifi"
K15-2011,P14-2131,0,0.0292953,"the hard research of seeking out individual companies it doesn’t take much to get burned L = word vectors word vectors paragraph ID word vectors the word vectors word vectors cat sat [γ + d(Arg1 , Arg2) − d(Arg1, Arg2 )]+ But where [x]+ denotes the positive part of x, γ &gt; 0 is a margin hyperparameter. S ′ denotes a set of corrupted pair where Arg1 or Arg2 is replaced by a random entity (but not both at the same time). Readers should see the detailed explanation in (Bordes et al., 2013). It is noted that we tried indicator function (alternatively called discrete-valued vector, bucket function (Bansal et al., 2014), binarization of embeddings (Guo et al., 2014)) for embeddings which are converted from real-valued vector. Although we have not tested sufficiently due to the timing constraint, we did not include this method in our experiments since we could not have any gain. Figure 1: Figure shows relational paragraph embeddings. We use a paragraph vector model to obtain the feature for Arg1 and Arg2 (Le and Mikolov, 2014). The paragraph vector model is an idea to obtain a real-valued vector in the similar construction with the word vector model (or word2vec) (Mikolov et al., 2013b) where the detailed exp"
K15-2011,D09-1036,0,0.223578,"Missing"
K15-2011,P09-1077,0,0.156889,"Missing"
K15-2011,N15-1081,0,0.123319,"developed at Dublin City University for participation in the CoNLL 2015 shared task. We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task and especially the sense classification for implicit connectives. 1 Implicit Explicit Prod (2.2) yes/no1 no Word pair (2.3) yes/no2 no Heuristic feat (2.4) no yes Table 1: Overview of features used for implicit/explicit classification. discourse parsing. Production features are proposed in (Lin et al., 2014) and word-pair features are reported in (Lin et al., 2014; Rutherford and Xue, 2015). Heuristic features, which is specific for explicit sense classification, are described in (Lin et al., 2014). We consider the embedding models which lead to two different types of intermediate representations. The relational phrase embedding model considers the dependency within words uniformly without considering the second-order effect. The word-pair embedding model considers the secondorder effect of specific combinations within the word-pairs in Arg1 and Arg2. If we plug in a paragraph vector model for the relational phrase embedding model, the model considers the effect of uni-gram with"
K15-2011,K15-2014,1,0.365842,"urse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task (Xue et al., 2015). We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task. We divide the whole process into two stages: the first stage concerns an identification of triples (Arg1, Conn, Arg2) and pairs (Arg1, Arg2) while the second stage concerns a sense classification of the identified individual triples and pairs. The first phase of the identification of connective and arguments are described in (Wang et al., 2015), which bases on the framework of (Lin et al., 2009) and is also presented in this shared task as a different paper. Hence, we omit the detailed description of the first stage (See (Wang et al., 2015) for identification of connectives and arguments). This paper focuses on the second stage which concerns sense classification. 2 Rel phrase (2.1) yes yes 2.1 Relational Phrase Embedding Features Phrase embeddings (or sentence embeddings) are distributed representation in a higher level than a word level. We used a paragraph vector model to obtain these phrase embeddings (Le and Mikolov, 2014). Upo"
K15-2011,K15-2001,0,0.293003,"t considering the second-order effect. The word-pair embedding model considers the secondorder effect of specific combinations within the word-pairs in Arg1 and Arg2. If we plug in a paragraph vector model for the relational phrase embedding model, the model considers the effect of uni-gram within a sentence as a sequence. If we plug in a RNN-LSTM model (Le and Zuidema, 2015), the model considers the effect of uni-gram within a sentence as a tree. Introduction This paper describes the discourse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task (Xue et al., 2015). We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task. We divide the whole process into two stages: the first stage concerns an identification of triples (Arg1, Conn, Arg2) and pairs (Arg1, Arg2) while the second stage concerns a sense classification of the identified individual triples and pairs. The first phase of the identification of connective and arguments are described in (Wang et al., 2015), which bases on the framework of (Lin et al., 2009) and is also presented in this shared task as a diff"
K15-2011,D14-1012,0,0.0185658,"ies it doesn’t take much to get burned L = word vectors word vectors paragraph ID word vectors the word vectors word vectors cat sat [γ + d(Arg1 , Arg2) − d(Arg1, Arg2 )]+ But where [x]+ denotes the positive part of x, γ &gt; 0 is a margin hyperparameter. S ′ denotes a set of corrupted pair where Arg1 or Arg2 is replaced by a random entity (but not both at the same time). Readers should see the detailed explanation in (Bordes et al., 2013). It is noted that we tried indicator function (alternatively called discrete-valued vector, bucket function (Bansal et al., 2014), binarization of embeddings (Guo et al., 2014)) for embeddings which are converted from real-valued vector. Although we have not tested sufficiently due to the timing constraint, we did not include this method in our experiments since we could not have any gain. Figure 1: Figure shows relational paragraph embeddings. We use a paragraph vector model to obtain the feature for Arg1 and Arg2 (Le and Mikolov, 2014). The paragraph vector model is an idea to obtain a real-valued vector in the similar construction with the word vector model (or word2vec) (Mikolov et al., 2013b) where the detailed explanation can be obtained. In implicit/explicit"
K15-2011,P14-1062,0,0.0351575,"nc as well. The relation of the latter is in an opposite direction. We provided the wordpair model which works for these categories but in a different perspective. Further work includes the mechanism how to make it work for Comp.Cont and Comp.Conc. Although a paragraph vector did not work efficiently, our model has a tentative model which does not have interaction between relational, paragraph, and word embeddings such as in (Denil et al., 2015), which is one immediate challenge. Then, other challenge includes replacement of a paragraph vector model with a convolutional sentence vector model (Kalchbrenner et al., 2014) and RNN-LSTM model (Le and Zuidema, 2015). The former approach is related to the supervised learning instead of unsupervised learning. The latter approach is to employ the structure of tree instead of a sequence. Table 5: Official results for explicit/implicit sense classification for test set. the averaged embedding in a sentence will perform meaning establishment in the intermediate representation which capture the characteristics of Arg1, Arg2, and Conn. First, Comp.Cont or Comp.Conc may include sentence polarity with some additional condition that these polarities may be reversed. Against"
K15-2011,P10-1052,0,0.0313451,"Missing"
K15-2011,J92-4003,0,\N,Missing
K15-2014,prasad-etal-2008-penn,0,0.270113,"g the explicit relations, the nonexplicit part extracts all the adjacent sentence pairs which are not explicit relations and then infers implicit relations. As we mainly focus on explicit relations, in this part, we only apply a simple majority function to give all candidate pairs the same results. 2.1 2.2 Argument Position Classification arg2 is the argument with which the connective is syntactically associated, and its position is fixed once we have located the connective from the previous component (Section 2.1). Thus, the challenging step for this task is to identify the location of arg1. Prasad et al. (2008) show that arg1 may be located in various positions to the connective, such as within the same sentence (SS), before (PS), or after (FS) the sentence containing the connective. Furthermore, arg1 may be adjacent or nonadjacent with connective sentence. arg1 may also contain one or more sentences. Table 2 shows the statistics of each of above scenarios. Relative Position SS FS PS Other Scenarios Connective Classifier As words which can be discourse connectives do not always function as discourse connectives, we need to identify if an instance of a connective candidate is a functional connective"
K15-2014,I11-1120,0,0.0683619,". Understanding such discourse information is clearly an important component of natural language understanding that impacts a wide range of downstream natural language applications. Since Penn Discourse Treebank was released, a number of data driven approaches have been proposed to deal with different challenging subtasks of discourse parsing. As explicit arguments may be intra-sentential or inter-sentential, Lin et al. (2012), Xu et al. (2012), Stepanov and Riccardi (2012) propose to employ argument position classification as heuristic and then apply separated models for argument extraction. Ghosh et al. (2011) regarded argument extraction as a tokenlevel sequence labeling task, applying conditional random fields (CRFs) to label each token in a sentence. Following on this work, Ghosh et al. (2012) 89 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 89–94, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics As shown in Table 1, we use three feature classes: lexical, syntactic and others. Especially, we employ the position of connection as a new feature (i.e., beginning or not), because we observe that the candidates"
K15-2014,E14-1068,0,\N,Missing
K15-2014,C12-2130,0,\N,Missing
K15-2014,W13-5704,0,\N,Missing
L16-1436,P12-2040,0,0.0758794,"consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Bu"
L16-1436,W11-0609,0,0.0490398,"2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing."
L16-1436,itamar-itai-2008-using,0,0.207421,"MT) of conversational material by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discou"
L16-1436,P07-2045,0,0.0127474,"Missing"
L16-1436,matsubara-etal-2002-bilingual,0,0.0781911,"Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Dane"
L16-1436,W12-0117,0,0.0160222,"ed Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged"
L16-1436,J03-1002,0,0.00968861,"Missing"
L16-1436,P03-1021,0,0.0215298,"Missing"
L16-1436,prasad-etal-2008-penn,0,0.0125033,"ue MT system. The rest of the paper is organized as follows. In Section 2, we describe related work. Section 3 describes in detail our approaches to build a dialogue corpus as well as the structure of the generated database. The experimental results for both corpus annotation and translation are reported in Section 4. Finally, Section 5 presents our conclusions and future work plans. 2. Related Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue co"
L16-1436,schmitt-etal-2012-parameterized,0,0.0221098,"g three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags using clues such as format and story structure tags in the script; (2) for a bilingual subtitle, we align each sentence with its tra"
L16-1436,tiedemann-2008-synchronizing,0,0.197336,"aterial by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bri"
L16-1436,tiedemann-2012-parallel,0,0.140544,"tructure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bridging the information in these two kin"
L16-1436,walker-etal-2012-annotated,0,0.171098,"013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags"
L16-1436,O12-1015,1,0.894007,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W12-6310,1,0.929649,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W14-3331,1,0.893281,"Missing"
L16-1436,zhang-etal-2014-dual,0,0.0684567,"eyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ c"
L18-1236,2008.iwslt-papers.1,0,0.0316422,"ese two languages is becoming more and more demanding. However, Chinese and Portuguese belong to distinct language families (SinoTibetan and Romance, respectively) and only a relative much smaller proportion of people have bilingual proficiency of the language pair. Therefore, the use of Chinese– Portuguese MT systems to provide auxiliary translation services between the two sides is highly demanded. Pivot-based machine translation is a commonly used method when large quantities of parallel data are not readily available for some language pairs. Utiyama and Isahara (2007), Wu and Wang (2007), Bertoldi et al. (2008) investigated phrase-level, sentence-level and system-level pivot strategies for low resource translation in SMT. A pivot language, which is usually English, can bridge the source and target languages and make translation possible. However, the domains of these two are often different and thus results in low performance and even ambiguities. A few researchers have investigated how to improve the Chinese–Portuguese MT by incorporating linguistic knowledge into the systems. For instance, Wong and Chao (2010) proposed a hybrid MT system combining rule-based and example-based components. Oliveira"
L18-1236,J93-2003,0,0.203518,"Missing"
L18-1236,P05-1066,0,0.217191,"Missing"
L18-1236,N18-1032,0,0.0158713,"d a large Chinese–Portuguese corpus. Despite both Chinese and Portuguese are populous languages, the language pair itself could be considered as low-resource. Therefore the same technologies could be used to improve machine translation quality in other low-resource language pairs. We conduct experiments on existing and the curated corpora, and compare the performance of different MT models using these corpora. This results of this work can be used by Chinese– Portuguese MT research for comparison purposes. In the future, we will investigate other approaches such as universal low-resource NMT (Gu et al., 2018) and discourse-aware approaches (Wang et al., 2018; Wang et al., 2017; Wang et al., 2016a) for Chinese–Portuguese MT task. Furthermore, we will keep exploring simple yet effective methods to build larger and domain-specific Chinese– Portuguese parallel corpora to further improve MT performance in this language pair. 6. Pivot-based MT As discussed in Section 1, pivot method is commonly used for low-resource MT. To show that increasing parallel data is still essential to improve low-resource MT, we also compare MT models trained with parallel corpus of direct translation pair, with the pivot-bas"
L18-1236,D13-1176,0,0.0747534,"n different and thus results in low performance and even ambiguities. A few researchers have investigated how to improve the Chinese–Portuguese MT by incorporating linguistic knowledge into the systems. For instance, Wong and Chao (2010) proposed a hybrid MT system combining rule-based and example-based components. Oliveira et al. (2010) explored Constraint Synchronous Grammar parsing for SMT. Lu et al. (2014) and Liu and Leal (2016) focused on specific linguistic phenomena (i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performance is still low using these state-of-the-art approaches. To date, there are only a few Chinese–Portuguese parallel corpora available1 (Tiedemann, 2012). OpenSubtitles20182 (Lison and Tiedemann, 2016) has released 6.7 millions Chinese–Portuguese sentence pairs, which are extracted from movie subtitles. These sentences are usually short and simple as most of them are transcripts of conversations in"
L18-1236,P07-2045,0,0.0144502,"Missing"
L18-1236,L16-1147,0,0.0305356,"al. (2014) and Liu and Leal (2016) focused on specific linguistic phenomena (i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performance is still low using these state-of-the-art approaches. To date, there are only a few Chinese–Portuguese parallel corpora available1 (Tiedemann, 2012). OpenSubtitles20182 (Lison and Tiedemann, 2016) has released 6.7 millions Chinese–Portuguese sentence pairs, which are extracted from movie subtitles. These sentences are usually short and simple as most of them are transcripts of conversations in movies, therefore they alone are not suitable to train general-domain MT systems. News-Commentary113 contains data in newswire domain. However, there are only 21.8 thousands of sentence pairs and is thus not sufficient to train robust MT models. To alleviate data scarcity problem, we extracted bilingual data from Macao government websites.4 Macao government documents, as requested by law, are wri"
L18-1236,J03-1002,0,0.0231704,"Missing"
L18-1236,P03-1021,0,0.0652313,"Missing"
L18-1236,P02-1040,0,0.102203,"Missing"
L18-1236,2010.amta-papers.14,0,0.188397,"with the VSM approach as mentioned above to estimate the semantic similarity between the source-language and the target-language documents. In practice, we indexed articles in both sides and then generate a query for each source-side article. Then we use a Chinese–Portuguese SMT system (training data are extracted using the method in Section 2.1) to obtain translated queries. 2.3. Translation based Alignment Through exploring various sentence-alignment methods (e.g. length-based, dictionary-based), we found that translation based alignment is a robust approach especially for comparable data (Sennrich and Volk, 2010; Sennrich and Volk, 2011). The idea is to use machine translated text and BLEU as a similarity score to find reliable alignments which are used as anchor points. The gaps between these anchor points are then filled using BLEU-based and lengthbased heuristics. We use this method to align unaligned paragraphs and sentences. A Chinese–Portuguese SMT system (training data are extracted using the method in Section 2.1) is used to obtain translated paragraphs/sentences. 2.4. Machine Translation MT is a sequence-to-sequence prediction task, which aims to find for the source language sentence the mos"
L18-1236,W11-4624,0,0.314631,"mentioned above to estimate the semantic similarity between the source-language and the target-language documents. In practice, we indexed articles in both sides and then generate a query for each source-side article. Then we use a Chinese–Portuguese SMT system (training data are extracted using the method in Section 2.1) to obtain translated queries. 2.3. Translation based Alignment Through exploring various sentence-alignment methods (e.g. length-based, dictionary-based), we found that translation based alignment is a robust approach especially for comparable data (Sennrich and Volk, 2010; Sennrich and Volk, 2011). The idea is to use machine translated text and BLEU as a similarity score to find reliable alignments which are used as anchor points. The gaps between these anchor points are then filled using BLEU-based and lengthbased heuristics. We use this method to align unaligned paragraphs and sentences. A Chinese–Portuguese SMT system (training data are extracted using the method in Section 2.1) is used to obtain translated paragraphs/sentences. 2.4. Machine Translation MT is a sequence-to-sequence prediction task, which aims to find for the source language sentence the most probable target language"
L18-1236,P16-1162,0,0.011142,"lem observed in the experimental results on our corpus (as described in Section 3.3). We also compare training directly using the 1489 parallel corpus we curated with the pivot-based method, which is a common approach for low-resource MT (as described in Section 1). 4.1. Out-of-Vocabulary As shown in Table 1, vocabulary size is very big on both corpora. However, NMT models typically operate with a fixed vocabulary, which results in the OOV problem. This might contribute to the under-performance of NMT models compared SMT as observed in our experimental results. Joint byte-pair encoding (BPE) (Sennrich et al., 2016) is a simpler and more effective method to handle the OOV problem. It encodes rare and unknown words as sequences of subword units. We use the BPE toolkit13 to process our corpus and train an NMT system on this processed data. The procedures are as follows. The Portuguese and Chinese data are first pre-processed using the same method introduced in Section 3.1. We then train single BPE models on tokenized/segmented both Portuguese and Chinese sides. Finally, we use BPE models to segment the sentences into subword units. After 59,500 joint BPE operations, the network vocabulary sizes are reduced"
L18-1236,tiedemann-2012-parallel,0,0.0324406,"ronous Grammar parsing for SMT. Lu et al. (2014) and Liu and Leal (2016) focused on specific linguistic phenomena (i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performance is still low using these state-of-the-art approaches. To date, there are only a few Chinese–Portuguese parallel corpora available1 (Tiedemann, 2012). OpenSubtitles20182 (Lison and Tiedemann, 2016) has released 6.7 millions Chinese–Portuguese sentence pairs, which are extracted from movie subtitles. These sentences are usually short and simple as most of them are transcripts of conversations in movies, therefore they alone are not suitable to train general-domain MT systems. News-Commentary113 contains data in newswire domain. However, there are only 21.8 thousands of sentence pairs and is thus not sufficient to train robust MT models. To alleviate data scarcity problem, we extracted bilingual data from Macao government websites.4 Macao go"
L18-1236,P16-1008,0,0.0120132,"earchers have investigated how to improve the Chinese–Portuguese MT by incorporating linguistic knowledge into the systems. For instance, Wong and Chao (2010) proposed a hybrid MT system combining rule-based and example-based components. Oliveira et al. (2010) explored Constraint Synchronous Grammar parsing for SMT. Lu et al. (2014) and Liu and Leal (2016) focused on specific linguistic phenomena (i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performance is still low using these state-of-the-art approaches. To date, there are only a few Chinese–Portuguese parallel corpora available1 (Tiedemann, 2012). OpenSubtitles20182 (Lison and Tiedemann, 2016) has released 6.7 millions Chinese–Portuguese sentence pairs, which are extracted from movie subtitles. These sentences are usually short and simple as most of them are transcripts of conversations in movies, therefore they alone are not suitable to train general-do"
L18-1236,N07-1061,0,0.0455532,"in a fast path. Translation services between these two languages is becoming more and more demanding. However, Chinese and Portuguese belong to distinct language families (SinoTibetan and Romance, respectively) and only a relative much smaller proportion of people have bilingual proficiency of the language pair. Therefore, the use of Chinese– Portuguese MT systems to provide auxiliary translation services between the two sides is highly demanded. Pivot-based machine translation is a commonly used method when large quantities of parallel data are not readily available for some language pairs. Utiyama and Isahara (2007), Wu and Wang (2007), Bertoldi et al. (2008) investigated phrase-level, sentence-level and system-level pivot strategies for low resource translation in SMT. A pivot language, which is usually English, can bridge the source and target languages and make translation possible. However, the domains of these two are often different and thus results in low performance and even ambiguities. A few researchers have investigated how to improve the Chinese–Portuguese MT by incorporating linguistic knowledge into the systems. For instance, Wong and Chao (2010) proposed a hybrid MT system combining rule-b"
L18-1236,O12-1015,1,0.830285,"used to train a MT system for further steps. For instance, we could train a SMT system on the sub-corpus in newswire domain and use the system to translate sentences for translation based alignment method. (3) For each aligned articles, we align the paragraphs with a simple but effective method: if the number of paragraphs in the two aligned articles is equal, we align all paragraphs one by one in the same order. Otherwise, we employ translation-based alignment algorithm to find parallel paragraphs; To align articles/documents, we consider the problem as cross-lingual document alignment task (Wang et al., 2012). We employ a document alignment approach using word embedding (Lohar et al., 2016): 1) we initially construct a pseudo-query from a source-language document; 2) and then represent both the target-language documents and the pseudo-query as word vectors to find the average similarity measure between them; 3) finally the word vector based similarity is combined with the term-overlap-based similarity. The Vector Space Model (VSM) (Salton et al., 1975) is one of the overlap based methods. Each document is represented as a vector of terms. The ith document Di in targetside is represented as a vecto"
L18-1236,N16-1113,1,0.86395,"anguages, the language pair itself could be considered as low-resource. Therefore the same technologies could be used to improve machine translation quality in other low-resource language pairs. We conduct experiments on existing and the curated corpora, and compare the performance of different MT models using these corpora. This results of this work can be used by Chinese– Portuguese MT research for comparison purposes. In the future, we will investigate other approaches such as universal low-resource NMT (Gu et al., 2018) and discourse-aware approaches (Wang et al., 2018; Wang et al., 2017; Wang et al., 2016a) for Chinese–Portuguese MT task. Furthermore, we will keep exploring simple yet effective methods to build larger and domain-specific Chinese– Portuguese parallel corpora to further improve MT performance in this language pair. 6. Pivot-based MT As discussed in Section 1, pivot method is commonly used for low-resource MT. To show that increasing parallel data is still essential to improve low-resource MT, we also compare MT models trained with parallel corpus of direct translation pair, with the pivot-based models. We build four Transformer NMT models on large Chinese–English14 and English–P"
L18-1236,L16-1436,1,0.924333,"anguages, the language pair itself could be considered as low-resource. Therefore the same technologies could be used to improve machine translation quality in other low-resource language pairs. We conduct experiments on existing and the curated corpora, and compare the performance of different MT models using these corpora. This results of this work can be used by Chinese– Portuguese MT research for comparison purposes. In the future, we will investigate other approaches such as universal low-resource NMT (Gu et al., 2018) and discourse-aware approaches (Wang et al., 2018; Wang et al., 2017; Wang et al., 2016a) for Chinese–Portuguese MT task. Furthermore, we will keep exploring simple yet effective methods to build larger and domain-specific Chinese– Portuguese parallel corpora to further improve MT performance in this language pair. 6. Pivot-based MT As discussed in Section 1, pivot method is commonly used for low-resource MT. To show that increasing parallel data is still essential to improve low-resource MT, we also compare MT models trained with parallel corpus of direct translation pair, with the pivot-based models. We build four Transformer NMT models on large Chinese–English14 and English–P"
L18-1236,D17-1301,1,0.862387,"uese are populous languages, the language pair itself could be considered as low-resource. Therefore the same technologies could be used to improve machine translation quality in other low-resource language pairs. We conduct experiments on existing and the curated corpora, and compare the performance of different MT models using these corpora. This results of this work can be used by Chinese– Portuguese MT research for comparison purposes. In the future, we will investigate other approaches such as universal low-resource NMT (Gu et al., 2018) and discourse-aware approaches (Wang et al., 2018; Wang et al., 2017; Wang et al., 2016a) for Chinese–Portuguese MT task. Furthermore, we will keep exploring simple yet effective methods to build larger and domain-specific Chinese– Portuguese parallel corpora to further improve MT performance in this language pair. 6. Pivot-based MT As discussed in Section 1, pivot method is commonly used for low-resource MT. To show that increasing parallel data is still essential to improve low-resource MT, we also compare MT models trained with parallel corpus of direct translation pair, with the pivot-based models. We build four Transformer NMT models on large Chinese–Engl"
L18-1236,P07-1108,0,0.0498094,"services between these two languages is becoming more and more demanding. However, Chinese and Portuguese belong to distinct language families (SinoTibetan and Romance, respectively) and only a relative much smaller proportion of people have bilingual proficiency of the language pair. Therefore, the use of Chinese– Portuguese MT systems to provide auxiliary translation services between the two sides is highly demanded. Pivot-based machine translation is a commonly used method when large quantities of parallel data are not readily available for some language pairs. Utiyama and Isahara (2007), Wu and Wang (2007), Bertoldi et al. (2008) investigated phrase-level, sentence-level and system-level pivot strategies for low resource translation in SMT. A pivot language, which is usually English, can bridge the source and target languages and make translation possible. However, the domains of these two are often different and thus results in low performance and even ambiguities. A few researchers have investigated how to improve the Chinese–Portuguese MT by incorporating linguistic knowledge into the systems. For instance, Wong and Chao (2010) proposed a hybrid MT system combining rule-based and example-bas"
N16-1113,P11-2037,0,0.0300195,"Missing"
N16-1113,D13-1135,0,0.169061,"Missing"
N16-1113,D10-1062,0,0.769955,"hat there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our"
N16-1113,P07-2045,0,0.0104744,"ing the approach described in Section 2.1. There are two different language models for the DP annotation (Section 2.1) and translation tasks, respectively: one is trained on the 2.13TB Chinese Web Page Collection Corpus5 while the other one is trained on all extracted 7M English subtitle data (Wang et al., 2016). Corpus Lang. Sentents Train Dev Test ZH EN ZH EN ZH EN 1,037,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5,"
N16-1113,D10-1086,0,0.132,"Missing"
N16-1113,W10-1737,0,0.380169,"Missing"
N16-1113,D09-1106,1,0.87907,"Missing"
N16-1113,P13-2064,1,0.842545,"missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our task is that ZP cont"
N16-1113,C14-1003,0,0.0608436,"Missing"
N16-1113,J03-1002,0,0.0251063,"on. Related work is described in Section 3. The experimental results for both the DP generator and translation are reported in Section 4. Section 5 analyses some real examples which is followed by our conclusion in Section 6. 2 Methodology The architecture of our proposed method is shown in Figure 2, which can be divided into three phases: DP corpus annotation, DP generation, and SMT integration. 2.1 DP Training Corpus Annotation We propose an approach to automatically annotate DPs by utilizing alignment information. Given a parallel corpus, we first use an unsupervised word alignment method (Och and Ney, 2003; Tu et al., 2012) to produce a word alignment. From observing of the alignment matrix, we found it is possible to detect DPs by projecting misaligned pronouns from the non-pro-drop target side (English) to the pro-drop source side (Chinese). In this work, we focus on nominative and accusative pronouns including personal, possessive and reflexive instances, as listed in Table 1. Figure 2: Architecture of proposed method. Category Subjective Personal Objective Personal Possessive Objective Possessive Reflexive Pronouns 我 (I), 我们 (we), 你/你们 (you), 他 (he), 她 (she), 它 (it), 他们/她们/它 们 (they). 我 (me"
N16-1113,P03-1021,0,0.094528,"37,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5, the size of the single hidden layer = 200, iterations = 10, embeddings = 200. The MLP classifier use random initialized embeddings, with the following settings: the size of the single hidden layer = 200, embeddings = 100, iterations = 200. For end-to-end evaluation, case-insensitive BLEU (Papineni et al., 2002) is used to measure 5 Available at http://www.sogou"
N16-1113,P02-1040,0,0.0971952,"on, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, the contributions of this paper include the following: • We propose an automatic method to build a large-scale DP training corpus. Given that the DPs are annotated in the parallel corpus, models trained on this data are more appropriate to the translation task; • Benefit"
N16-1113,W12-4501,0,0.0351916,"Missing"
N16-1113,N07-1029,0,0.0127834,"anslation of missing pronouns by explicitly recalling DPs for both parallel data and monolingual input sentences. More specifically, we extract an additional rule table from the DP-inserted parallel corpus to produce a “pronoun-complete” translation model. In addition, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, th"
N16-1113,W12-4213,0,0.348159,"Missing"
N16-1113,C10-1123,1,0.824671,"et language, so that the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The di"
N16-1113,I11-1145,1,0.817536,"hat the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our t"
N16-1113,C12-2122,1,0.905413,"Missing"
N16-1113,L16-1436,1,0.843791,"Missing"
N16-1113,P13-1081,0,0.692401,"se pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our DP generator. We div"
N16-1113,N13-1125,0,0.0308356,"Missing"
N16-1113,C10-2158,0,0.0510881,"Missing"
N16-1113,P15-2051,0,0.709643,"consists of 1M sentence pairs extracted from movie and TV episode subtitles. We found that there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. Aft"
N16-1113,zhang-etal-2014-dual,0,0.0371089,"Missing"
N16-1113,D07-1057,0,0.339977,"Missing"
N19-1122,D18-1457,1,0.810585,"Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the position information respectively. Shen et al. (2018) introduce a directiona"
N19-1122,P18-1008,0,0.111193,"ang Baosong Yang Florida State University haoj8711@gmail.com Tencent AI Lab brightxwang@tencent.com University of Macau nlp2ct.baosong@gmail.com Longyue Wang Jinfeng Zhang Zhaopeng Tu∗ Tencent AI Lab vinnylywang@tencent.com Florida State University jinfeng@stat.fsu.edu Tencent AI Lab zptu@tencent.com Abstract Recently, the Transformer model (Vaswani et al., 2017) that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence hinders its further improvement of translation capacity (Chen et al., 2018; Dehghani et al., 2019). In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences"
N19-1122,P18-1198,0,0.269586,"and target sequences with shorter path. Among all the model variants, the implementation with shortest path performs best, in which the recurrence encoder is single layer and its output is only fed to the top decoder layer. It consistently outperforms its multiple deep counterparts, such as multiple-layer recurrence encoder and feeding the output of recurrence encoder to all the decoder layers. In addition, our approach indeed generates more informative encoder representations, especially representative on syntactic structure features, through conducting linguistic analyses on probing tasks (Conneau et al., 2018). 2 Background Figure 1 shows the model architecture of Transformer. The encoder is composed of a stack of N identical layers, each of which has two sub-layers. The first sub-layer is a self-attention network, and the second one is a position-wise fully connected feed-forward network. A residual connection (He et al., 2016) is employed around each of two sublayers, followed by layer normalization (Ba et al., Add & Norm Recurrence Encoder Positional Encoding Add & Norm Self-Attention Encoder N× Add & Norm Multi-Head QK> Attention sof tmax( √ )V dk Norm Recurrence (3)Modeling ☯ ⊕ where {Q, K, V}"
N19-1122,N03-1017,0,0.0277516,"r the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentence pairs, En⇒De) and WMT17 Chinese-to-English (20.6M sentence pairs, Zh⇒En) translation tasks. All the data had been tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations2 . We used case-sensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We implemented the proposed approaches on top of the Transformer model (Vaswani et al., 2017). Both in our model and related model of Subsection 5.3, the RNN is implemented with GRU (Cho et al., 2014) for fair comparison. We followed the configurations in Vaswani et al. (2017), and reproduced their reported results on the En⇒De task. We initialized parameters of the proposed models by the pre-trained baseline model. We have tested both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096), and number of attention heads"
N19-1122,D18-1317,1,0.864009,"nce encoder to all the decoder layers. 4 Related Work Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the position information r"
N19-1122,P02-1040,0,0.103507,"lement the Transformer model. In our preliminary experiments, attending over the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentence pairs, En⇒De) and WMT17 Chinese-to-English (20.6M sentence pairs, Zh⇒En) translation tasks. All the data had been tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations2 . We used case-sensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We implemented the proposed approaches on top of the Transformer model (Vaswani et al., 2017). Both in our model and related model of Subsection 5.3, the RNN is implemented with GRU (Cho et al., 2014) for fair comparison. We followed the configurations in Vaswani et al. (2017), and reproduced their reported results on the En⇒De task. We initialized parameters of the proposed models by the pre-trained baseline model. We have tested both Base and Big models, which differ at hidden size (51"
N19-1122,N18-1202,0,0.0618655,"mpirically show that the proposed model benefits from the short-cut effect. Comparison to Reviewer Network Attentive recurrent network are inspired by the reviewer network, which is proposed by Yang et al. (2016) for the image caption generation task. There are two key differences which reflect how we have generalized from the original model. First, we perform attention steps over the source embeddings instead of the encoder representations. The main reason is that the Transformer encoder is implemented as multiple layers, and higher layers generally encode global information, as indicated by Peters et al. (2018). Second, we feed the feature vectors together with the original encoder representations to the decoder. In image caption generation, the source side (i.e. image) contains much more information than the target side (i.e. caption) (Tu et al., 2017). Therefore, they aim at learning a compact and abstractive representation from the source information, which serves as the only input to the decoder. In this work, we focus on leveraging the attention model to better learn 1202 Model BASE the recurrence, which we expect to complement the Transformer model. In our preliminary experiments, attending ov"
N19-1122,P16-1162,0,0.128779,"the attention model to better learn 1202 Model BASE the recurrence, which we expect to complement the Transformer model. In our preliminary experiments, attending over the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentence pairs, En⇒De) and WMT17 Chinese-to-English (20.6M sentence pairs, Zh⇒En) translation tasks. All the data had been tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations2 . We used case-sensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We implemented the proposed approaches on top of the Transformer model (Vaswani et al., 2017). Both in our model and related model of Subsection 5.3, the RNN is implemented with GRU (Cho et al., 2014) for fair comparison. We followed the configurations in Vaswani et al. (2017), and reproduced their reported results on the En⇒De task. We initialized parameters of the proposed models by the pre-trai"
N19-1122,N18-2074,0,0.439798,"work was conducted when Jie Hao and Baosong Yang were interning at Tencent AI Lab. down the sequential assumption to obtain the ability of highly parallel computation: input elements interact with each other simultaneously without regard to their distance. However, prior studies empirically show that the lack of recurrence modeling hinders Transformer from further improvement of translation quality (Dehghani et al., 2019). Modeling recurrence is crucial for capturing several essential properties of input sequence, such as structural representations (Tran et al., 2016) and positional encoding (Shaw et al., 2018), which are exactly the weaknesses of SAN (Tran et al., 2018). Recently, Chen et al. (2018) show that the representations learned by SAN-based and RNNbased encoders are complementary to each other, and merging them can improve translation performance for RNN-based NMT models. Starting from these findings, we propose to directly model recurrence for Transformer with an additional recurrence encoder. The recurrence encoder recurrently reads word embeddings of input sequence and outputs a sequence of hidden states, which serves as an additional information source to the Transformer decoder. In ad"
N19-1122,D18-1548,0,0.049565,"g the output of recurrence encoder to all the decoder layers. 4 Related Work Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the posi"
N19-1122,P18-1117,0,0.0204601,"es Hd to make a target word prediction. It is much simpler than that of the conventional Transformer, which transfers information learned from input sequences across multiple stacking encoder and decoder layers. We expect it outperforms its multiple deep counterparts, such as multiple-layer recurrence encoder and feeding the output of recurrence encoder to all the decoder layers. 4 Related Work Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they"
N19-1122,N16-1036,0,0.016905,"the corresponding author of the paper. This work was conducted when Jie Hao and Baosong Yang were interning at Tencent AI Lab. down the sequential assumption to obtain the ability of highly parallel computation: input elements interact with each other simultaneously without regard to their distance. However, prior studies empirically show that the lack of recurrence modeling hinders Transformer from further improvement of translation quality (Dehghani et al., 2019). Modeling recurrence is crucial for capturing several essential properties of input sequence, such as structural representations (Tran et al., 2016) and positional encoding (Shaw et al., 2018), which are exactly the weaknesses of SAN (Tran et al., 2018). Recently, Chen et al. (2018) show that the representations learned by SAN-based and RNNbased encoders are complementary to each other, and merging them can improve translation performance for RNN-based NMT models. Starting from these findings, we propose to directly model recurrence for Transformer with an additional recurrence encoder. The recurrence encoder recurrently reads word embeddings of input sequence and outputs a sequence of hidden states, which serves as an additional informat"
N19-1122,C18-1255,0,0.042848,"r From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the position information respectively. Shen et al. (2018) introduce a directional self-attention network (DiSA"
N19-1122,D18-1503,0,0.0424227,"ng at Tencent AI Lab. down the sequential assumption to obtain the ability of highly parallel computation: input elements interact with each other simultaneously without regard to their distance. However, prior studies empirically show that the lack of recurrence modeling hinders Transformer from further improvement of translation quality (Dehghani et al., 2019). Modeling recurrence is crucial for capturing several essential properties of input sequence, such as structural representations (Tran et al., 2016) and positional encoding (Shaw et al., 2018), which are exactly the weaknesses of SAN (Tran et al., 2018). Recently, Chen et al. (2018) show that the representations learned by SAN-based and RNNbased encoders are complementary to each other, and merging them can improve translation performance for RNN-based NMT models. Starting from these findings, we propose to directly model recurrence for Transformer with an additional recurrence encoder. The recurrence encoder recurrently reads word embeddings of input sequence and outputs a sequence of hidden states, which serves as an additional information source to the Transformer decoder. In addition to the standard RNN, we propose to implement recurrenc"
N19-1122,Q17-1007,1,0.85447,"There are two key differences which reflect how we have generalized from the original model. First, we perform attention steps over the source embeddings instead of the encoder representations. The main reason is that the Transformer encoder is implemented as multiple layers, and higher layers generally encode global information, as indicated by Peters et al. (2018). Second, we feed the feature vectors together with the original encoder representations to the decoder. In image caption generation, the source side (i.e. image) contains much more information than the target side (i.e. caption) (Tu et al., 2017). Therefore, they aim at learning a compact and abstractive representation from the source information, which serves as the only input to the decoder. In this work, we focus on leveraging the attention model to better learn 1202 Model BASE the recurrence, which we expect to complement the Transformer model. In our preliminary experiments, attending over the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentenc"
N19-1407,D18-1457,1,0.873171,"ng efficiency. Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018). Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018, 2019), Li et al. (2019) proposed to aggregate information captured by different attention heads. Based on these findings, we model interactions among attention heads to exploit the richness of local properties distributed in different heads. 5 Experiments We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks. 4042 For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectiv"
N19-1407,D16-1044,0,0.0672508,"N2 ] (8) V [V b h, V b h are elements in the h-th subspace, where K which are calculated by Equations S 4 and 5 respectively. The union operation means combining the keys and values in different subspaces. The corresponding output is calculated as: e h )V eh ohi = ATT(qhi , K (9) The 2D convolution allows SANs to build relevance between elements across adjacent heads, thus flexibly extract local features from different subspaces rather than merely from an unique head. The vanilla SAN models linearly aggregate features from different heads, and this procedure limits the extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the o"
N19-1407,W18-5431,0,0.423676,"aces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attention heads in an unified framework. Specifically, in order to pay more attention to a local part of the input sequence, we restrict the attention scope to a window of neighboring"
N19-1407,P16-1162,0,0.152427,"eriments We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks. 4042 For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations. Following Shaw et al. (2018), we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; Yang et al., 2018), we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017), and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. 5.1 Effects of Window/Area Size We first investigated the"
N19-1407,N18-2074,0,0.478472,"l allows each head to interact local features with its adjacent subspaces at attention time. We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong T RANSFORMER model (Vaswani et al., 2017) across language pairs. Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Yang et al., 2018; Sperber et al., 2018), our model boosts performance on both translation quality and training efficiency. 4040 Proceedings of NAACL-HLT 2019, pages 4040–4045 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held a talk with SharonBush (a) Vanilla SANs held Bush aheld talka with talkSharon with Sharon (b) 1D"
N19-1407,D14-1181,0,0.00321668,"e extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the output of which is fed to the subsequent SAN layer. Several researches proposed to revise the attention distribution with a parametric localness bias, and succeed on machine translation (Yang et al., 2018) and natural language inference (Guo et al., 2019). While both models introduce additional parameters, our approach is a more lightweight solution without introducing any new parameters. Closely related to this work, Shen et al. (2018a) applied a positional mask to encode temporal order, which only allows SANs to attend to the previous or following tokens in th"
N19-1407,D18-1548,0,0.0713839,"we improve locality modeling from revising attention scope. To make a fair comparison, we re-implemented the above approaches under a same framework. Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018). Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018, 2019), Li et al. (2019) proposed to aggregate information captured by different attention heads. Based on these findings, we model interactions among attention heads to exploit the richness of local properties distributed in different heads. 5 Experiments We conducted experiments with the Transformer model (Vaswani et al., 2017)"
N19-1407,W04-3250,0,0.0278748,"nce improvements over the Transformer baseline. Model T RANSFORMER -BASE + CS ANs T RANSFORMER -B IG + CS ANs WMT14 En⇒De Speed BLEU 1.28 27.31 1.22 28.18⇑ 0.61 28.58 0.50 28.74 WMT17 Zh⇒En Speed BLEU 1.21 24.13 1.16 24.80⇑ 0.58 24.56 0.48 25.01↑ WAT17 Ja⇒En Speed BLEU 1.33 28.10 1.28 28.50↑ 0.65 28.41 0.55 28.73↑ Table 2: Experimental results on WMT14 En⇒De, WMT17 Zh⇒En and WAT17 Ja⇒En test sets. “Speed” denotes the training speed (steps/second). “↑ / ⇑” indicates statistically significant difference from the vanilla self-attention counterpart (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). C NNs, revealing that extracting local features with dynamic weights is superior to assigning fixed parameters. Moreover, while most of the existing approaches (except for Shen et al. (2018a)) introduce new parameters, our methods are parameter-free and thus only marginally affect training efficiency. 5.4 Universality of The Proposed Model To validate the universality of our approach on MT tasks, we evaluated the proposed approach on different language pairs and model settings. Table 2 lists the results on En⇒De, Zh⇒En and Ja⇒En translation tasks. As seen, our model consistently improves tra"
N19-1407,D18-1317,1,0.920185,"ann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato and Tiedemann, 2018; Li et al., 2018), especially in diverse local contexts (Yang et al., 2018). We hypothesis that exploiting local properties across heads can further improve the performance of SANs. To this end, we expand the 1-dimensional window to a 2-dimensional area with the new dimension being the index of attention head. Suppose that the area size is (N + 1) × (M + 1) (N ≤ H), the keys and values in the area are: [ eh = b h− N2 , . . . , K b h, . . . , K b h+ N2 ] (7) K [K [ eh = b h− N2 , . . . , V b h, . . . , V b h+ N2 ] (8) V [V b h, V b h are elements in the h-th subspace, where K which are calculated by Equations S"
N19-1407,N19-1359,1,0.91234,"V b h are elements in the h-th subspace, where K which are calculated by Equations S 4 and 5 respectively. The union operation means combining the keys and values in different subspaces. The corresponding output is calculated as: e h )V eh ohi = ATT(qhi , K (9) The 2D convolution allows SANs to build relevance between elements across adjacent heads, thus flexibly extract local features from different subspaces rather than merely from an unique head. The vanilla SAN models linearly aggregate features from different heads, and this procedure limits the extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the output of which is"
N19-1407,D18-1408,0,0.159869,"tion, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017), which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attention heads in an u"
N19-1407,D15-1166,0,0.0950172,"ers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; Yang et al., 2018), we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017), and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. 5.1 Effects of Window/Area Size We first investigated the effects of window size (1D-CS ANs) and area size (2D-CS ANs) on En⇒De validation set, as plotted in Figure 2. For 1D-CS ANs, the local size with 11 is superior to other settings. This is consistent with Luong et al. (2015) who found that 10 is the best window size in their local attention experiments. Then, we fixed the number of neighboring tokens being 11 and varied the number of heads. As seen, by considering the features across heads (i.e. > 1), 2D-CS ANs further improve the translation quality. However, when the number of heads in attention goes up, the translation quality inversely drops. One possible reason is that the model still has the flexibility of learning a different distribution for each head with few interactions, while a large amount of interactions assumes more heads make “similar contribution"
N19-1407,D16-1244,0,0.171283,"Missing"
N19-1407,N18-1202,0,0.0838108,"the model to attend to a local region via convolution operations (1D-CS ANs, Figure 1(b)). Accordingly, it provides distance-aware 2 Accordingly, the calculation of corresponding output in Equation (2) is modified as: b h )V bh ohi = ATT(qhi , K (3) Approach 2 (6) As seen, SANs are only allowed to attend to the b h, V b h ), instead of all neighboring tokens (e.g., K the tokens in the sequence (e.g., Kh , Vh ). The SAN-based models are generally implemented as multiple layers, in which higher layers tend to learn semantic information while lower layers capture surface and lexical information (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato"
N19-1407,D18-1475,1,0.240925,"e elements. In addition, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017), which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attent"
N19-1407,D17-1150,1,0.868524,"n, SANs are only allowed to attend to the b h, V b h ), instead of all neighboring tokens (e.g., K the tokens in the sequence (e.g., Kh , Vh ). The SAN-based models are generally implemented as multiple layers, in which higher layers tend to learn semantic information while lower layers capture surface and lexical information (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato and Tiedemann, 2018; Li et al., 2018), especially in diverse local contexts (Yang et al., 2018). We hypothesis that exploiting local properties across heads can further improve the performance of SANs. To this end, we expand the 1-dimensional window to a 2-dimensional a"
O12-1015,1998.amta-tutorials.5,0,0.229455,"Document Translation-Based. 144 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) 1. Introduction With the flourishing development of the Internet, the amount of information from a variety of domains is rising dramatically. Although the researchers have done a lot to develop high performance and effective monolingual Information Retrieval (IR), the diversity of information source and the explosive growth of information in different languages drove a great need for IR systems that could cross language boundaries [1]. Cross-Language Information Retrieval (CLIR) has become more important for people to access the information resources written in various languages. Besides, it is of a great significance to alignment documents in multiple languages for Statistical Machine Translation (SMT) systems, of which quality is heavily dependent upon the amount of parallel sentences used in constructing the system. In this paper, we focus on the problems of translation ambiguity, query generation and searching score which are keys to the retrieval performance. First of all, in order to increase the probability that the"
O12-1015,P02-1038,0,0.258696,"CLIR 3.1. Translation Model Currently, the good performing statistical machine translation systems are based on phrase-based models which translate small word sequences at a time. Generally speaking, translation model is common for contiguous sequences of words to translate as a whole. Phrasal translation is certainly significant for CLIR [10], as stated in Section 1. It can do a good job in dealing with term disambiguation. In this work, documents are translated using the translation model provided by Moses, where the log-linear model is considered for training the phrase-based system models [11], and is represented as: M p(e1I |f1 J ) exp( ¦ Om hm (e1I , f1 J )) m 1 M ¦ exp(¦ O h (e' m e '1I m I 1 (2) J , f1 )) m 1 where hm indicates a set of different models, λm means the scaling factors, and the denominator can be ignored during the maximization process. The most important models in Eq. (2) normally are phrase-based models which are carried out in source to target and target to source directions. The source document will maximize the equation to generate the translation including the words most likely to occur in the target document set. 147 Updated: June 9, 2012 Proceedings of the"
O12-1015,2005.mtsummit-papers.11,0,0.10154,"he BLEU (Bilingual Evaluation Understudy) is a classical automatic evaluation method for the translation quality of an MT system [18]. In this evaluation, the translation model is created using the parallel corpus, as described in Section 4. We use another 5,000 sentences from the TestSet1 for 3 Available at http://lucene.apache.org. 151 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) evaluation4. The BLEU value, we obtained, is 32.08. The result is higher than that of the results reported by Koehn in his work [14], of which the BLEU score is 30.1 for the same language pair we used in Europarl corpora. Although we did not use exactly the same data for constructing the translation model, the value of 30.1 was presented as a baseline of the English-Spanish translation quality in Europarl corpora. The BLEU score shows that our translation model performs very well, due to the large number of the training data we used and the pre-processing tasks we designed for cleaning the data. On the other hand, it reveals that the translation quality of our model is good. 5.3. Evaluation of CLIR Model In this section, t"
O12-1015,J03-1002,0,0.0236206,"al Data of Corpus Dataset Training Set TestSet1 TestSet2 Documents 2,900 1,022 23,342 Sentences 1,902,050 80,000 80,000 Size of corpus Words 23,411,545 5,735,464 7,217,827 Ave. words in document 50 6,612 309 4.2. Experimental Setup In order to evaluate our proposed model, the following tools have been used. 1 Available online at http://www.statmt.org/europarl/. 149 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) The probabilistic LMs are constructed on monolingual corpora by using the SRILM [15]. We use GIZA++ [16] to train the word alignment models for different pairs of languages of the Europarl corpus, and the phrase pairs that are consistent with the word alignment are extracted. For constructing the phrase-based statistical machine translation model, we use the open source Moses [17] toolkit, and the translation model is trained based on the log-linear model, as given in Eq. (2). The workflow of constructing the translation model is illustrated in Fig. 2 and it consists of the following main steps2: (1) Preparation of aligned parallel corpus. (2) Preprocessing of training data: tokenization, case c"
O12-1015,P07-2045,0,0.0171069,"llowing tools have been used. 1 Available online at http://www.statmt.org/europarl/. 149 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) The probabilistic LMs are constructed on monolingual corpora by using the SRILM [15]. We use GIZA++ [16] to train the word alignment models for different pairs of languages of the Europarl corpus, and the phrase pairs that are consistent with the word alignment are extracted. For constructing the phrase-based statistical machine translation model, we use the open source Moses [17] toolkit, and the translation model is trained based on the log-linear model, as given in Eq. (2). The workflow of constructing the translation model is illustrated in Fig. 2 and it consists of the following main steps2: (1) Preparation of aligned parallel corpus. (2) Preprocessing of training data: tokenization, case conversion, and sentences filtering where sentences with length greater than fifty words are removed from the corpus in order to comply with the requirement of Moses. (3) A 5-gram LM is trained on Spanish data with the SRILM toolkits. (4) The phrased-based STM model is therefore"
O12-1015,P02-1040,0,0.0824762,"Missing"
O12-1015,J93-2003,0,\N,Missing
O12-5002,P91-1022,0,0.476947,"Missing"
O12-5002,J93-2003,0,0.0808495,"Missing"
O12-5002,2005.mtsummit-papers.11,0,0.0202666,"Missing"
O12-5002,P07-2045,0,0.0102561,"Missing"
O12-5002,1998.amta-tutorials.5,0,0.224329,"Missing"
O12-5002,P02-1038,0,0.130416,"Missing"
O12-5002,J03-1002,0,0.0112601,"Missing"
O12-5002,P02-1040,0,0.0816796,"Missing"
O12-5002,H91-1026,0,\N,Missing
P19-1354,P18-1008,0,0.33076,"@gmail.com, {derekfw,lidiasc}@umac.mo ‡ Tencent AI Lab {vinnylywang,zptu}@tencent.com Abstract its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. Position embedding (Gehring et al., 2017) is generally deployed to capture sequential information for SAN (Vaswani et al., 2017; Shaw et al., 2018). Recent studies claimed that SAN with position embedding is still weak at learning word order information, due to the lack of recurrence structure that is essential for sequence modeling (Shen et al., 2018a; Chen et al., 2018; Hao et al., 2019). However, such claims are mainly based on a theoretical argument, which have not been empirically validated. In addition, this can not explain well why SAN-based models outperform their RNN counterpart in machine translation – a benchmark sequence modeling task (Vaswani et al., 2017). Our goal in this work is to empirically assess the ability of SAN to learn word order. We focus on asking the following research questions: Self-attention networks (SAN) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks, e.g. m"
P19-1354,P18-1163,1,0.851012,"whether NMT model has the ability to tackle the wrong order noises. As a results, we make erroneous word order noises on English-German development set by moving one word to another position, and evaluate the drop of the translation quality of each model. As listed in Figure 6, SAN and DiSAN yield less drops on translation quality than their RNN counterpart, demonstrating the effectiveness of self-attention on ablating wrong order noises. We attribute this to the fact that models (e.g. RNN-based models) will not learn to be robust to errors since they are never observed (Sperber et al., 2017; Cheng et al., 2018). On the contrary, since SAN-based NMT encoder is good at recognizing and reserving anomalous word order information under NMT context, it may raise the ability of decoder on handling noises occurred in the training set, thus to be more robust in translating sentences with anomalous word order. Related Work Exploring Properties of SAN SAN has yielded strong empirical performance in a variety of NLP tasks (Vaswani et al., 2017; Tan et al., 2018; Li et al., 2018; Devlin et al., 2019). In response to these impressive results, several studies have emerged with the goal of understanding SAN on many"
P19-1354,P18-1198,0,0.188484,"es across language pairs and model variants. This is consistent with prior observation on NMT systems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recov"
P19-1354,N19-1423,0,0.651089,"rder, and does the conclusion hold in different scenarios (e.g., translation)? Q2: Is the model architecture the critical factor for learning word order in the downstream tasks such as machine translation? Q3: Is position embedding powerful enough to capture word order information for SAN? Introduction Self-attention networks (SAN, Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Strubell et al., 2018), and language representations (Devlin et al., 2019). The popularity of SAN lies in ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. We approach these questions with a novel probing task – word reordering detection (WRD), which aims to detect the positions of randomly reordered words in the input sentence. We compare SAN with RNN, as well as directional SAN (DiSAN, Shen et al., 2018a) that augments SAN with recurrence modeling. In this study, we focus on the encoders implemented with different architectures, so as to investigate their abilities to learn 3635 Proce"
P19-1354,P17-1080,0,0.0251829,"tence embedding. However, analysis on sentence encodings may introduce confounds, making it difficult to infer whether the relevant information is encoded within the specific position of interest or rather inferred from diffuse information elsewhere in the sentence (Tenney et al., 2019). In this study, we directly probe the token representations for word- and phrase-level properties, which has been widely used for probing token-level representations learned in neural machine translation systems, e.g. part-of-speech, semantic tags, morphology as well as constituent structure (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). 6 Conclusion In this paper, we introduce a novel word reordering detection task which can probe the ability of a model to extract word order information. With the help of the proposed task, we evaluate RNN, 3642 SAN and DiSAN upon Transformer framework to empirically test the theoretical claims that SAN lacks the ability to learn word order. The results reveal that RNN and DiSAN exactly perform better than SAN on extracting word order information in the case they are trained individually for our task. However, there is no evidence that SAN learns less word order inform"
P19-1354,P18-2003,0,0.0207368,"r, analysis on sentence encodings may introduce confounds, making it difficult to infer whether the relevant information is encoded within the specific position of interest or rather inferred from diffuse information elsewhere in the sentence (Tenney et al., 2019). In this study, we directly probe the token representations for word- and phrase-level properties, which has been widely used for probing token-level representations learned in neural machine translation systems, e.g. part-of-speech, semantic tags, morphology as well as constituent structure (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). 6 Conclusion In this paper, we introduce a novel word reordering detection task which can probe the ability of a model to extract word order information. With the help of the proposed task, we evaluate RNN, 3642 SAN and DiSAN upon Transformer framework to empirically test the theoretical claims that SAN lacks the ability to learn word order. The results reveal that RNN and DiSAN exactly perform better than SAN on extracting word order information in the case they are trained individually for our task. However, there is no evidence that SAN learns less word order information under the machine"
P19-1354,D17-1219,0,0.0270644,"models have to learn to recognize both the normal and abnormal word order in a sentence. Position Detector Figure 1 (a) depicts the architecture of the position detector. Let the sequential representations H = {h1 , ..., hN } be the output of each encoder noted in Section 3, which are fed to the output layer (Figure 1 (b)). Since only one pair of “I” and “O” labels should be generated in the output sequence, we cast the task as a pointer detection problem (Vinyals et al., 2015). To this end, we turn to an output layer that commonly used in the reading comprehension task (Wang and Jiang, 2017; Du and Cardie, 2017), which aims to identify the start and end positions of the answer in the given text.2 The output layer consists of two sub-layers, which progressively predicts the prob2 Contrary to reading comprehension in which the start and end positions are ordered, “I” and “O” do not have to be ordered in our tasks, that is, the popped word can be inserted to either left or right position. 3636 • Q1: We compare SAN with two recurrence architectures – RNN and DiSAN on the WRD task, thus to quantify their abilities on learning word order (Section 3.1). abilities of each position being labelled as “I” and “"
P19-1354,D16-1011,0,0.0320337,"rmation between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the learning objective indeed affects more on learning word order information than model architecture in our case. Accuracy According to Layer Several researchers may doubt that the parallel structure of SAN may lead to failure on capturing word order information at higher layers, si"
P19-1354,D18-1317,1,0.861132,"fact that models (e.g. RNN-based models) will not learn to be robust to errors since they are never observed (Sperber et al., 2017; Cheng et al., 2018). On the contrary, since SAN-based NMT encoder is good at recognizing and reserving anomalous word order information under NMT context, it may raise the ability of decoder on handling noises occurred in the training set, thus to be more robust in translating sentences with anomalous word order. Related Work Exploring Properties of SAN SAN has yielded strong empirical performance in a variety of NLP tasks (Vaswani et al., 2017; Tan et al., 2018; Li et al., 2018; Devlin et al., 2019). In response to these impressive results, several studies have emerged with the goal of understanding SAN on many properties. For example, Tran et al. (2018) compared SAN and RNN on language inference tasks, and pointed out that SAN is weak at learning hierarchical structure than its RNN counterpart. Moreover, Tang et al. (2018) conducted experiments on subject-verb agreement and word sense disambiguation tasks. They found that SAN is good at extracting semantic properties, while underperforms RNN on capturing long-distance dependencies. This is in contrast to our intuit"
P19-1354,N19-1359,1,0.842245,"for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises. As a results, we make erroneous word order noi"
P19-1354,P15-1150,0,0.147118,"Missing"
P19-1354,D15-1166,0,0.07096,"ystems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results ve"
P19-1354,D18-1458,0,0.146942,"Missing"
P19-1354,P11-2093,0,0.0246417,"of WMT14 En⇒De data with maximum length to 80. For each sentence in different sets (i.e. training, validation, and test sets), we construct an instance by randomly moving a word to another position. Finally we construct 7M, 10K and 10K samples for training, validating and testing, respectively. Note that a sentence can be sampled multiple times, thus each dataset in the WRD data contains more instances than that in the machine translation data. All the English and German data are tokenized using the scripts in Moses. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To reduce the vocabulary size, all the sentences are processed by byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the data. 4 Experimental Results We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information. Using the above experimental design, we give the following answers: A1: SAN-based encoder trained on the WRD data is indeed harder to learn positional information than the recurrence architectures (Section 4.1), while there is no evidence that 3638 Models RNN SAN DiSAN Insert 78.4 73.2 79.6"
P19-1354,D16-1244,0,0.0955198,"Missing"
P19-1354,N18-1202,0,0.0553809,"reserving more word order information, thus to help for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises"
P19-1354,W18-5431,0,0.0453698,"rder information, thus to help for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises. As a results, we make errone"
P19-1354,P16-1162,0,0.0652226,"test sets), we construct an instance by randomly moving a word to another position. Finally we construct 7M, 10K and 10K samples for training, validating and testing, respectively. Note that a sentence can be sampled multiple times, thus each dataset in the WRD data contains more instances than that in the machine translation data. All the English and German data are tokenized using the scripts in Moses. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To reduce the vocabulary size, all the sentences are processed by byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the data. 4 Experimental Results We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information. Using the above experimental design, we give the following answers: A1: SAN-based encoder trained on the WRD data is indeed harder to learn positional information than the recurrence architectures (Section 4.1), while there is no evidence that 3638 Models RNN SAN DiSAN Insert 78.4 73.2 79.6 Original 73.4 66.0 70.1 Both 68.2 60.1 68.0 ability of SAN to learn word order. The consistency between prior studi"
P19-1354,N18-2074,0,0.225183,"he Ability of Self-Attention Networks to Learn Word Order Baosong Yang† Longyue Wang‡ Derek F. Wong† Lidia S. Chao† Zhaopeng Tu‡∗ † NLP2 CT Lab, Department of Computer and Information Science, University of Macau nlp2ct.baosong@gmail.com, {derekfw,lidiasc}@umac.mo ‡ Tencent AI Lab {vinnylywang,zptu}@tencent.com Abstract its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. Position embedding (Gehring et al., 2017) is generally deployed to capture sequential information for SAN (Vaswani et al., 2017; Shaw et al., 2018). Recent studies claimed that SAN with position embedding is still weak at learning word order information, due to the lack of recurrence structure that is essential for sequence modeling (Shen et al., 2018a; Chen et al., 2018; Hao et al., 2019). However, such claims are mainly based on a theoretical argument, which have not been empirically validated. In addition, this can not explain well why SAN-based models outperform their RNN counterpart in machine translation – a benchmark sequence modeling task (Vaswani et al., 2017). Our goal in this work is to empirically assess the ability of SAN to"
P19-1354,D18-1548,0,0.0539845,": Is recurrence structure obligate for learning word order, and does the conclusion hold in different scenarios (e.g., translation)? Q2: Is the model architecture the critical factor for learning word order in the downstream tasks such as machine translation? Q3: Is position embedding powerful enough to capture word order information for SAN? Introduction Self-attention networks (SAN, Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Strubell et al., 2018), and language representations (Devlin et al., 2019). The popularity of SAN lies in ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. We approach these questions with a novel probing task – word reordering detection (WRD), which aims to detect the positions of randomly reordered words in the input sentence. We compare SAN with RNN, as well as directional SAN (DiSAN, Shen et al., 2018a) that augments SAN with recurrence modeling. In this study, we focus on the encoders implemented with different architectures, so a"
P19-1354,D18-1503,0,0.316521,"ordering detection objective. WRD Encoders We first directly train the encoders on the WRD data, to evaluate the abilities of model architectures. The WRD encoders are randomly initialized and co-trained with the output layer. Accordingly, the detection accuracy can be treated as the learning objective of this group of encoders. Meanwhile, we can investigate the reliability of the proposed WRD task by checking whether the performances of different architectures (i.e. RNN, SAN, and DiSAN) are consistent with previous findings on other benchmark NLP tasks (Shen et al., 2018a; Tang et al., 2018; Tran et al., 2018; Devlin et al., 2019). NMT Encoders To quantify how well different architectures learn word order information with the learning objective of machine translation, we first train the NMT models (both encoder and decoder) on bilingual corpus using the same configuration reported by Vaswani et al. (2017). Then, we fix the parameters of the encoder, and only train the parameter associated with the output layer on the WRD data. In this way, we can probe the representations learned by NMT models, on their abilities to learn word order of input sentences. To cope with WRD task, all the models were tr"
P19-1354,D17-1065,0,0.0520865,"Missing"
P19-1354,D18-1475,1,0.836265,"and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the lea"
P19-1354,N19-1407,1,0.841914,"Missing"
P19-1354,D17-1150,1,0.862188,"ducted on the test set. Clearly, the accuracy of SAN gradually increased with the stacking of layers and consistently outperform that of other models across layers. ever, this kind of stability is destroyed when we pre-train each encoder with a learning objective of machine translation. As seen in Figure 4 (b) and (c), the performance of pre-trained NMT encoders obviously became worse on long-distance cases across language pairs and model variants. This is consistent with prior observation on NMT systems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivia"
P19-1354,P17-1172,0,0.0274143,"urce and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the learning objective indeed affects more on learning word order information than model architecture in our case. Accuracy According to Layer Several researchers may doubt that the parallel structure of SAN may lead to failure on capturing word order information at higher layers, since the position"
P19-1354,D10-1092,0,\N,Missing
P19-1354,D16-1159,0,\N,Missing
P19-1354,W17-5706,0,\N,Missing
P19-1624,D18-1338,0,0.0718853,"+ D EEP (R NN) + D EEP (TAM) T RANSFORMER -B IG + D EEP (R NN) + D EEP (TAM) En⇒De 27.31 28.38⇑ 28.33⇑ 28.58 29.04↑ 29.19⇑ En⇒Fr 39.32 40.15⇑ 40.27⇑ 41.41 41.87 42.04⇑ Table 2: Case-sensitive BLEU scores on WMT14 En⇒De and En⇒Fr test sets. “↑ / ⇑”: significant over T RANSFORMER counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling. vs. 264.1M, not shown in the table). Furthermore, D EEP (TAM) consistently outperforms D EEP (RNN) in the T RANSFORMER -B IG configuration. One possible reason is that the big models benefit more from the improved gradient flow with the transparent attention (Bapna et al., 2018). 3.3 Linguistic Analysis To gain linguistic insights into the global and deep sentence representation, we conducted probing tasks1 (Conneau et al., 2018) to evaluate linguistics knowledge embedded in the encoder output and the sentence representation in the variations of the Base model that are trained on En⇒De translation task. The probing tasks are classification problems that focus on simple linguistic properties of sentences. The 10 probing tasks are categories into three groups: (1) Surface information. (2) Syntactic information. (3) Semantic information. For each task, we trained the cl"
P19-1624,P18-1198,0,0.194725,"of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer linguistic information. The contributions of this paper are: • Our study demonstrates the necessity and effectiveness of exploiting source-side sentential context for NMT, which benefits from fusing useful contextual information across encoder layers. • We propose several strategies to better capture useful sentential context for neural machine translation. Experimental results empirically show that the proposed approaches achieve improvement over the strong baseline model T RANSFORMER. 6197 Proceedings of"
P19-1624,D18-1457,1,0.59051,"ial context representation. 2.3 Deep Sentential Context Deep sentential context is a function of all encoder layers outputs {H1 , . . . , HL }: g = g(H1 , . . . , HL ) = D EEP(g1 , . . . , gL ), (8) where gl is the sentence representation of the l-th layer Hl , which is calculated by Equation 3. The motivation for this mechanism is that recent studies reveal that different encoder layers capture linguistic properties of the input sentence at different levels (Peters et al., 2018), and aggregating layers to better fuse semantic information has proven to be of profound value (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). In this work, we propose to fuse the global information across layers. Choices of D EEP(·) In this work, we investigate two representative functions to aggregate information across layers, which differ at whether the decoding information is taken into account. RNN Intuitively, we can treat G = {g1 , . . . , gL } as a sequence of representations, and recurring all the representations with an RNN: L X βi,l gl , (10) l=1 βi = ATTg (dli−1 , G), (11) where ATTg (·) is an attention model with its own parameters, that specifics which context representations is"
P19-1624,W07-0717,0,0.0324893,"ng the source sentence representation. The deep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the enco"
P19-1624,C18-1276,0,0.0898512,"SFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT. 1 Introduction Sentential context, which involves deep syntactic and semantic structure of the source and target languages (Nida, 1969), is crucial for machine translation. In statistical machine translation (SMT), the sentential context has proven beneficial for predicting local translations (Meng et al., 2015; Zhang et al., 2015). The exploitation of sentential context in neural machine translation (NMT, Bahdanau et al., 2015), however, is not well studied. Recently, Lin et al. (2018) showed that the translation at each time step should be conditioned on the whole target-side context. They introduced a deconvolution-based decoder to provide the global information from the target-side context for guidance of decoding. In this work, we propose simple yet effective approaches to exploiting source-side global sentence-level context for NMT models. We use encoder representations to represent the sourceside context, which are summarized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the w"
P19-1624,P15-1003,0,0.0729807,"context representation. Experimental results on the WMT14 English⇒German and English⇒French benchmarks show that our model consistently improves performance over the strong T RANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT. 1 Introduction Sentential context, which involves deep syntactic and semantic structure of the source and target languages (Nida, 1969), is crucial for machine translation. In statistical machine translation (SMT), the sentential context has proven beneficial for predicting local translations (Meng et al., 2015; Zhang et al., 2015). The exploitation of sentential context in neural machine translation (NMT, Bahdanau et al., 2015), however, is not well studied. Recently, Lin et al. (2018) showed that the translation at each time step should be conditioned on the whole target-side context. They introduced a deconvolution-based decoder to provide the global information from the target-side context for guidance of decoding. In this work, we propose simple yet effective approaches to exploiting source-side global sentence-level context for NMT models. We use encoder representations to represent the source"
P19-1624,N18-1202,0,0.408191,"ized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer ling"
P19-1624,W18-5431,0,0.211353,"l context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer linguistic information. The contrib"
P19-1624,P16-1162,0,0.0894149,"ee appealing strengths. First, TAM dynamically generates the weights βi based on the decoding information at every decoding step dli−1 , while R NN is unaware of the decoder states and the associated parameters are fixed after training. Second, TAM allows the model to adjust the gradient flow to different layers in the encoder depending on its training phase. 3 Experiment We conducted experiments on WMT14 En⇒De and En⇒Fr benchmarks, which contain 4.5M and 35.5M sentence pairs respectively. We reported experimental results with case-sensitive 4gram BLEU score. We used byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations to alleviate the out-of-vocabulary problem. We implemented the proposed approaches on top of T RANSFORMER model (Vaswani et al., 2017). We followed Vaswani et al. (2017) to set the model configurations, and reproduced their reported results. We tested both Base and Big models, which differ at the layer size (512 vs. 1024) and the number of attention heads (8 vs. 16). 3.1 Ablation Study (9) We first investigated the effect of components in the proposed approaches, as listed in Table 1. We use the last RNN state as the sentence representation: g = rL . As seen, the RNN"
P19-1624,N18-1117,0,0.0609503,"Missing"
P19-1624,D16-1159,0,0.0282864,", which are summarized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed"
P19-1624,Q18-1029,1,0.852228,"n which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specif"
P19-1624,2014.amta-researchers.11,0,0.0241312,"representation. The deep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations f"
P19-1624,P15-1162,0,0.0746289,"Missing"
P19-1624,D17-1301,1,0.612456,"eep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent d"
P19-1624,P14-1062,0,0.02126,"the choice of g(·), namely shallow sentential context (Figure 1b) and deep sentential context (Figure 1c), which differ at the encoder layers to be exploited. It should be pointed out that the new parameters introduced in the proposed approach are jointly updated with NMT model parameters in an endto-end manner. 2.2 Shallow Sentential Context Shallow sentential context is a function of the top encoder layer output HL : g = g(HL ) = G LOBAL(HL ), (3) where G LOBAL(·) is the composition function. Choices of G LOBAL(·) Two intuitive choices are mean pooling (Iyyer et al., 2015) and max pooling (Kalchbrenner et al., 2014): G LOBAL MEAN = M EAN(HL ), L G LOBAL MAX = M AX(H ). (4) (5) Recently, Lin et al. (2017) proposed a selfattention mechanism to form sentence representation, which is appealing for its flexibility on extracting implicit global features. Inspired by this, 6198 g3g3 r3 r3 g2g2 r2 r2 g1g1 r1 r1 di-1di-1 g3g3 βi,3βi,3 g2g2 βi,2βi,2 g1g1 r0 r0 (a) R NN gi gi βi,1βi,1 (b) TAM Figure 2: Illustration of the deep functions. “TAM” model dynamically aggregates sentence representations at each decoding step with state di−1 . aggregation repeatedly revises the sentence representations of the sequence with"
P19-1624,C18-1255,0,0.0348102,"entation. 2.3 Deep Sentential Context Deep sentential context is a function of all encoder layers outputs {H1 , . . . , HL }: g = g(H1 , . . . , HL ) = D EEP(g1 , . . . , gL ), (8) where gl is the sentence representation of the l-th layer Hl , which is calculated by Equation 3. The motivation for this mechanism is that recent studies reveal that different encoder layers capture linguistic properties of the input sentence at different levels (Peters et al., 2018), and aggregating layers to better fuse semantic information has proven to be of profound value (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). In this work, we propose to fuse the global information across layers. Choices of D EEP(·) In this work, we investigate two representative functions to aggregate information across layers, which differ at whether the decoding information is taken into account. RNN Intuitively, we can treat G = {g1 , . . . , gL } as a sequence of representations, and recurring all the representations with an RNN: L X βi,l gl , (10) l=1 βi = ATTg (dli−1 , G), (11) where ATTg (·) is an attention model with its own parameters, that specifics which context representations is relevant for each d"
P19-1624,W04-3250,0,0.0589519,"M +18.9M +18.9M +19.9M +26.8M +26.4M n/a R NN TAM Train 1.39 1.08 1.35 1.34 1.22 1.03 1.07 Decode 3.85 3.09 3.45 3.43 3.23 3.14 3.03 BLEU 27.31 27.81 27.58 27.81↑ 28.04⇑ 28.38⇑ 28.33⇑ Table 1: Impact of components on WMT14 En⇒De translation task. BLEU scores in the table are case sensitive. “Train” denotes the training speed (steps/second), and “Decode” denotes the decoding speed (sentences/second) on a Tesla P40. “TAM” denotes the transparent attention model to implement the function D EEP(·). “↑ / ⇑”: significant over T RANSFORMER counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). Model baseline Base model, validating the importance of sentential context in NMT. Among them, attentive mechanism (Row 5) obtains the best performance in terms of BLEU score, while maintains the training and decoding speeds. Therefore, we used the attentive mechanism to implement the function G LOBAL(·) as the default setting in the following experiments. Deep Sentential Context (Rows 6-7) As seen, both R NN and TAM consistently outperform their shallow counterparts, proving the effectiveness of deep sentential context. Introducing deep context significantly improves translation performance"
P19-1624,N19-1359,1,0.869305,"Missing"
P19-1624,P17-2089,0,0.0424251,"Missing"
P19-1624,P12-1079,0,0.0231744,"m all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generati"
P19-1624,D18-1475,1,0.81888,"a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively. Experimental results on W"
P19-1624,C16-1170,0,0.0197359,"ation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al."
P19-1624,Q18-1011,1,0.817489,"t al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively. Experimental results on WMT14 benchmarks show that exploiting sentential context improves performances over the state-of-theart T RANSFORMER model. Linguistic analyses reveal that the proposed approach indeed captures more linguistic information as exp"
R13-1094,D11-1033,0,0.274559,"al. 2005), we allow duplicated sentences during the selection which is similar with. All retrieved sentences with their corresponding target translations are ranked according to their similarity scores. 2.2 Perplexity-Based Model The perplexity of a string s with empirical ngram distribution p given a language model q is:  p ( x )log q ( x ) 2 x  2H ( p ,q ) (3) in which H(p, q) is the cross-entropy between p and q. Selecting segments based on a perplexity threshold is equivalent to selecting based on a cross-entropy threshold, which is more often used for this task (Moore and Lewis, 2010; Axelrod et al., 2011). Supposed that HI(s) and HO(s) are the cross-entropy of a string s according to an in-domain language model LMI and non-indomain LMG respectively trained on in-domain data set I and a partition of general-domain data set G. Considering both source (src) and target (tar) side of parallel training data, there are three variants. The first is basic cross-entropy given by: H I src (s) 2 728 Available at http://lucene.apache.org. (4) and the second is cross-entropy difference (Moore and Lewis, 2010): H I src (s)  HGsrc (s) tional speech in a travel setting. All of them were segmented 4 (Zhang,"
R13-1094,J93-2003,0,0.0499157,"Missing"
R13-1094,W07-0722,0,0.0228688,"tion A well-known problem of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corp"
R13-1094,P11-2071,0,0.0320572,"Missing"
R13-1094,P12-2023,0,0.0140684,"ne translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each sG is given by Score(sG"
R13-1094,W07-0717,0,0.025379,"em of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each"
R13-1094,2005.eamt-1.19,0,0.85304,"ct model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each sG is given by Score(sG )  Sim(sG , R) (1) which means if we could find a better function to measure the similarity between sG and R, G could be replaced by a new sub-corpus Gsub for training a domain-specific SMT system. We focus on two data selection criteria that have been explored for domain adaptation. One comes from the realm of information retrieval (IR), which is defined as the cosine of the angle between two vectors based on term frequencyinverse document frequency (TF-IDF). Hildebrand et al. (2005) showed that it is possible to apply this standard IR technique for both TM adaptation and LM adaptation. It is also similar to the offline data optimization approach proposed by Lüet al. (2007), who re-sample and re727 Proceedings of Recent Advances in Natural Language Processing, pages 727–732, Hissar, Bulgaria, 7-13 September 2013. weight sentences in general corpus, achieving an improvement of about 1 BLEU point over the baseline system. This simple co-occurrence based matching only considers keywords overlap, which may result in weakness in filtering irrelevant data. Thus, it needs a larg"
R13-1094,C12-1096,0,0.0304151,"Missing"
R13-1094,2005.mtsummit-papers.11,0,0.0269798,"Missing"
R13-1094,P07-2045,0,0.00757585,"her penalize it according to space and punctuations edit differences. 3 3.1 3.2 Experimental Setup Corpora Two corpora are needed for the domain adaptation task. Our general corpus includes 5 million English-Chinese parallel sentences comprising various genres such as movie subtitle, law literature, news and novel. The in-domain corpus and test set are randomly selected from the IWSLT2010 (International Workshop on Spoken Language Translation) Chinese-English Dialog task 3 , consisting of transcriptions of conversaThe experiments presented in this paper are carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrasebased SMT system. The translation and the reordering model relied on “grow-diag-final” symmetrized word-to-word alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses. A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting improved Modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. 3.3 http://iwslt2010.fbk.eu/node/33. Baseline System The baseline system was trained on the general corpus with toolkits"
R13-1094,2010.jec-1.4,0,0.0192472,"onal in-domain corpus is employed to build the LM for perplexity-based retrieval (Moore and Lewis, 2010; Axelrod et al., 2011). Edit-Distance-Based Model Given a sentence sG from a general corpus and a sentence sR from the test set or in-domain corpus, the edit distance for these two sequences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, for transforming sG into sR. There are several different implementations of the edit-distance-based retrieval model. We used the normalized Levenshtein similarity score (fuzzy matching score, FMS) proposed by Koehn and Senellart (2010): FMS  1  LEDword (sG , sR ) Max( sG , sR ) (7) in which LEDword is a distance function and |s |is the number of tokens of sentence s. In this study, we employed a word-based Levenshtein edit distance function instead of additionally using a letter-based one. If the score of a sentence exceeds a threshold, we will further penalize it according to space and punctuations edit differences. 3 3.1 3.2 Experimental Setup Corpora Two corpora are needed for the domain adaptation task. Our general corpus includes 5 million English-Chinese parallel sentences comprising various genres such as movie sub"
R13-1094,D07-1036,0,0.0841494,"that are more similar to the target domain but different to others in general corpus. The third one is to sum the cross-entropy difference over both source and target side of the corpus:  H I src (s)  HGsrc (s)   H I tar (s)  H G tar (s) Data Set Sentences Tokens Ave. Len. Test Set 3,500 34,382 9.60 In-domain 17,975 151,797 9.45 Training Set 5,211,281 53,650,998 12.93 (6) Table 1: Corpora statistics. The third variant has been is proven to achieve the best result among the three cross-entropy variants (Axelrod et al., 2011). 2.3 In practice, we followed the experiments conducted by Lü et al. (2007) and Hildebrand et al. (2005), where the test set was used to select indomain data from general corpus. The only difference is that an additional in-domain corpus is employed to build the LM for perplexity-based retrieval (Moore and Lewis, 2010; Axelrod et al., 2011). Edit-Distance-Based Model Given a sentence sG from a general corpus and a sentence sR from the test set or in-domain corpus, the edit distance for these two sequences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, for transforming sG into sR. There are several different implementat"
R13-1094,2012.iwslt-papers.7,0,0.0135626,"computationally-limited environment. 1 Introduction A well-known problem of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a s"
R13-1094,P10-2041,0,0.277716,". As in (Hildebrand et al. 2005), we allow duplicated sentences during the selection which is similar with. All retrieved sentences with their corresponding target translations are ranked according to their similarity scores. 2.2 Perplexity-Based Model The perplexity of a string s with empirical ngram distribution p given a language model q is:  p ( x )log q ( x ) 2 x  2H ( p ,q ) (3) in which H(p, q) is the cross-entropy between p and q. Selecting segments based on a perplexity threshold is equivalent to selecting based on a cross-entropy threshold, which is more often used for this task (Moore and Lewis, 2010; Axelrod et al., 2011). Supposed that HI(s) and HO(s) are the cross-entropy of a string s according to an in-domain language model LMI and non-indomain LMG respectively trained on in-domain data set I and a partition of general-domain data set G. Considering both source (src) and target (tar) side of parallel training data, there are three variants. The first is basic cross-entropy given by: H I src (s) 2 728 Available at http://lucene.apache.org. (4) and the second is cross-entropy difference (Moore and Lewis, 2010): H I src (s)  HGsrc (s) tional speech in a travel setting. All of them w"
R13-1094,J03-1002,0,0.00747202,"nglish-Chinese parallel sentences comprising various genres such as movie subtitle, law literature, news and novel. The in-domain corpus and test set are randomly selected from the IWSLT2010 (International Workshop on Spoken Language Translation) Chinese-English Dialog task 3 , consisting of transcriptions of conversaThe experiments presented in this paper are carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrasebased SMT system. The translation and the reordering model relied on “grow-diag-final” symmetrized word-to-word alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses. A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting improved Modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. 3.3 http://iwslt2010.fbk.eu/node/33. Baseline System The baseline system was trained on the general corpus with toolkits and settings as described above. The baseline BLEU is 29.34 points. This low value is occurred by the fact that he general corpus does not consist of enough sentences on the travel domain and has a lo"
R13-1094,P02-1040,0,0.0855658,"Missing"
R13-1094,W03-1730,0,0.0144502,"Missing"
tian-etal-2014-um,eisele-chen-2010-multiun,0,\N,Missing
tian-etal-2014-um,D07-1103,0,\N,Missing
tian-etal-2014-um,tiedemann-2010-lingua,0,\N,Missing
tian-etal-2014-um,moore-2002-fast,0,\N,Missing
tian-etal-2014-um,C96-2141,0,\N,Missing
tian-etal-2014-um,J03-3002,0,\N,Missing
tian-etal-2014-um,J90-2002,0,\N,Missing
tian-etal-2014-um,P02-1040,0,\N,Missing
tian-etal-2014-um,P07-2045,0,\N,Missing
tian-etal-2014-um,N09-2061,0,\N,Missing
tian-etal-2014-um,J03-1002,0,\N,Missing
tian-etal-2014-um,2005.mtsummit-papers.11,0,\N,Missing
tian-etal-2014-um,ma-cieri-2006-corpus,0,\N,Missing
tian-etal-2014-um,tiedemann-2012-parallel,0,\N,Missing
tian-etal-2014-um,W03-1730,0,\N,Missing
W12-6310,I08-4017,0,0.0426201,"Missing"
W12-6310,I05-3025,0,0.0893904,"Missing"
W12-6310,W10-4137,0,0.0304063,"Missing"
W12-6310,W96-0213,0,0.57737,"Missing"
W12-6310,I05-3027,0,0.110099,"Missing"
W12-6310,O03-4002,0,0.119515,"Missing"
W12-6310,W06-0127,0,0.0482669,"Missing"
W12-6310,W15-3100,0,\N,Missing
W12-6327,W06-0803,0,0.0605728,"Missing"
W12-6327,W06-0129,0,0.0518553,"Missing"
W12-6327,M98-1002,0,0.0937787,"Missing"
W12-6327,W03-0419,0,0.238336,"Missing"
W12-6327,H05-1054,0,0.060689,"Missing"
W12-6327,P02-1060,0,0.124527,"Missing"
W12-6327,D07-1074,0,0.255907,"Missing"
W12-6327,N04-4028,0,0.0805405,"Missing"
W12-6327,doddington-etal-2004-automatic,0,0.172204,"Missing"
W12-6327,W03-0405,0,0.0981105,"Missing"
W12-6327,W15-3100,0,\N,Missing
W12-6327,N04-4000,0,\N,Missing
W13-3605,N12-1086,0,0.0257056,"Missing"
W13-3605,han-etal-2010-using,0,0.0479875,"rror detection and correction are detailed in Section 6, followed by an evaluation, discussion and a conclusion to end the paper. 2 Related Work The issues of grammatical error correction have been discussed from different perspectives for several decades. In this section, we briefly review some related methods. The use of machine learning methods to tackle this problem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classif"
W13-3605,W12-2025,0,0.47925,"stem for English Grammatical Error Correction Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China nlp2ct.{vincent, anson}@gmail.com, {derekfw, lidiasc}@umac.mo, nlp2ct.samuel@gmail.com techniques to improve the writing ability for second language learners. Grammatical error correction is the task of automatically detecting and correction erroneous word usage and ill-formed grammatical constructions in text (Dahlmeier et al., 2012). In recent decades, this special task has gained more attention by some organizations such as the Helping Our Own (HOO) challenge (Dale and Kilgarriff, 2010; Dale et al., 2012). Although the performance of grammatical error correction systems has been improved, it is still mostly limited to dealing with the determiner and preposition error types with a very low recall and precision. This year, the CoNLL-2013 shared task extends to include a more comprehensive list of error types, as shown in Table 1. To take on this challenge, this paper proposes pipe-line architecture in combination with sev"
W13-3605,W12-2030,0,0.016206,"non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classifiers always cannot give a satisfied performance. Thus, grammar rules and probabilistic language model can be used as a simple but effective assistant for correction of spelling (Kantrowitz et al, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan et al., 2012; Rozovskaya et al., 2012). 3 semi-supervised learning method that makes use of a large amount of unlabeled data which is easy to collect. In practice, semi-supervised learning requires less human effort and gives higher accuracy in creating a model. 4 As mentioned before, the corpus"
W13-3605,W13-1703,0,0.181308,"Missing"
W13-3605,W12-2031,0,0.019524,"lem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classifiers always cannot give a satisfied performance. Thus, grammar rules and probabilistic language model can be used as a simple but effective assistant for correction of spelling (Kantrowitz et al, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan et al., 2012; Rozovskaya et al., 2012). 3 semi-supervised learning method that makes use of a l"
W13-3605,N10-1018,0,0.106112,"Missing"
W13-3605,W12-2032,0,0.191472,"rally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classifiers always cannot give a satisfied performance. Thus, grammar rules and probabilistic language model can be used as a simple but effective assistant for correction of spelling (Kantrowitz et al, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan et al., 2012; Rozovskaya et al., 2012). 3 semi-supervised learning method that makes use of a large amount of unlabeled data which is easy to collect. In practice, semi-supervised learning requires less human effort and gives higher accuracy in creating a model. 4 As mentioned before, the corpus contains a low amount of error examples, which results in a high sparsity in the label distribution. In reality, the balance between the error and correct data is crucial for training a robust grammar detection models. Our experiment results demonstrate that too many correct data lead to unfavorable error detection rate. In order to resolv"
W13-3605,P10-2065,0,0.0621966,"l hybrid system is given in Section 5. The methods of grammatical error detection and correction are detailed in Section 6, followed by an evaluation, discussion and a conclusion to end the paper. 2 Related Work The issues of grammatical error correction have been discussed from different perspectives for several decades. In this section, we briefly review some related methods. The use of machine learning methods to tackle this problem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in spee"
W13-3605,C08-1109,0,0.0616285,"ised learning, and the overall hybrid system is given in Section 5. The methods of grammatical error detection and correction are detailed in Section 6, followed by an evaluation, discussion and a conclusion to end the paper. 2 Related Work The issues of grammatical error correction have been discussed from different perspectives for several decades. In this section, we briefly review some related methods. The use of machine learning methods to tackle this problem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or c"
W13-3605,N03-5008,0,\N,Missing
W13-3605,N12-1067,0,\N,Missing
W13-3605,W12-2033,0,\N,Missing
W13-3605,W12-2006,0,\N,Missing
W13-3605,W13-3601,0,\N,Missing
W13-3605,W11-2838,0,\N,Missing
W13-3605,W06-3808,0,\N,Missing
W14-1711,J93-2004,0,0.0460517,"ons ing ted Stem constantli combin idea will result in better solut be formul Specific POS RB VBG NNS MD VB IN JJR NNS VBG VBN Figure 1: The factorized sentence. PoS: Part-of-Speech tags denote the morphosyntactic category of a word. The use of PoS sequences enables us to some extent to recover missing determiners, articles, prepositions, as well as the modal verb in a sentence. Empirical studies (Yuan and Felice, 2013) have demonstrated that the use of this information can greatly improve the accuracy of the grammatical error correction. To obtain the PoS, we adopt the Penn Treebank tag set (Marcus et al., 1993), which contains 45 PoS tags. The Stanford parser (Klein and Manning, 2002) is used to extract the PoS information. Inspired by Yuan and Felice (2013), who used preposition-specific tags to fix the problem of being unable to distinguish between prepositions and obtained good performance, we create specific tags both for determiners (i.e., a, an, the) and prepositions. Table 1 provides an example of this modification, where prepositions, TO and IN, and determiner, After decoding, we will de-truecase all these words. 85 DT, are revised to TO_to, IN_of and DT_the, respectively. 2.3 with the word"
W14-1711,I11-1017,0,0.171203,"Missing"
W14-1711,W14-1701,0,0.0182639,"ng & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checker for five error types in the CoNLL"
W14-1711,W13-3601,0,0.0450902,"Language Processing & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checker for five error"
W14-1711,J03-1002,0,0.00694738,"a number of sentence pairs from the parallel corpus as a development set and a test set, disjoint from the training data. Table 2 summarizes the statistics of all the datasets. Corpus Parallel Corpus Additional Monolingual Dev. Set Test Set Sentences 55,503 Tokens 1,124,521 / 1,114,040 85,254,788 2,033,096,800 500 900 10,532 / 10,438 18,032 / 17,906 Table 2: Statistics of used corpora. The experiments were carried out with MOSES 1.0 4 (Philipp Koehn et al., 2007). The translation and the re-ordering model utilizes the “grow-diag-final” symmetrized word-to-word alignments created with GIZA++5 (Och and Ney, 2003) and the training scripts of MOSES. A 5gram LM was trained using the SRILM toolkit6 (Stolcke et al., 2002), exploiting the improved modified Kneser-Ney smoothing (Kneser and Ney, 1995), and quantizing both probabilities and back-off weights. For the log-linear model training, we take minimum-error-rate training (MERT) method as described in (Och, 2003). The result is evaluated by M2 Scorer (Dahlmeier and Ng, 2012) computing precision, recall and F0.5. 3 http://www.statmt.org/wmt14/translation-task.html. http://www.statmt.org/moses/. 5 http://code.google.com/p/giza-pp/. 6 http://www.speech.sri."
W14-1711,W13-3613,0,0.0162211,"ory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checker for five error types in the CoNLL 2013 shared task. The system implements a cascade of fi"
W14-1711,W13-3605,1,0.895589,"Missing"
W14-1711,P06-1032,0,0.430644,"Missing"
W14-1711,W13-1703,0,0.0471889,"e do not de-duplicate sentences. In all cases, the testing process is carried out as follows. The test set is translated by the first translation model, . The output from the first model is then fed into the second translation model, . The output of the second model is used as the final corrections. The second combination approach is to make use of multiple factors for model construction. The question is whether multiple factors when used together may improve the correction results. In this setting we combine two factors together 3 3.1 Experiment Setup Dataset We pre-process the NUCLE corpus (Dahlmeier et al., 2013) as described in Section 2 for training different translation models. We use both the official golden sentences and additional WMT2014 English monolingual data3 to train an in-domain and a general-domain language model (LM), respectively. These language models are linearly interpolated in the decoding phase. We also randomly select a number of sentence pairs from the parallel corpus as a development set and a test set, disjoint from the training data. Table 2 summarizes the statistics of all the datasets. Corpus Parallel Corpus Additional Monolingual Dev. Set Test Set Sentences 55,503 Tokens 1"
W14-1711,W12-2006,0,0.0399988,"eng, Yi Lu Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checke"
W14-1711,D07-1091,0,0.124464,"Missing"
W14-1711,P07-2045,0,0.0137822,"and a general-domain language model (LM), respectively. These language models are linearly interpolated in the decoding phase. We also randomly select a number of sentence pairs from the parallel corpus as a development set and a test set, disjoint from the training data. Table 2 summarizes the statistics of all the datasets. Corpus Parallel Corpus Additional Monolingual Dev. Set Test Set Sentences 55,503 Tokens 1,124,521 / 1,114,040 85,254,788 2,033,096,800 500 900 10,532 / 10,438 18,032 / 17,906 Table 2: Statistics of used corpora. The experiments were carried out with MOSES 1.0 4 (Philipp Koehn et al., 2007). The translation and the re-ordering model utilizes the “grow-diag-final” symmetrized word-to-word alignments created with GIZA++5 (Och and Ney, 2003) and the training scripts of MOSES. A 5gram LM was trained using the SRILM toolkit6 (Stolcke et al., 2002), exploiting the improved modified Kneser-Ney smoothing (Kneser and Ney, 1995), and quantizing both probabilities and back-off weights. For the log-linear model training, we take minimum-error-rate training (MERT) method as described in (Och, 2003). The result is evaluated by M2 Scorer (Dahlmeier and Ng, 2012) computing precision, recall and"
W14-1711,W13-3607,0,0.17258,"of errors present in an essay are to be detected and corrected (i.e., there is no restriction on the five error types of the 2013 shared task); 2) the official evaluation metric of this year adopts F0.5, weighting precision twice as much as recall. This requires us to explore an alternative universal joint model that can tackle various kinds of grammatical errors as well as join the detection and correction processes together. Regarding grammatical error correction as a process of translation has been shown to be effective (Ehsan and Faili, 2013, Mizumoto et al., 2011, Yoshimoto et al., 2013, Yuan and Felice, 2013). We treat the problematic sentences and golden sentences as pairs of source and target sentences. In SMT, a translation model is trained on a parallel corpus that consists of the source sentences (i.e. sentences that may contain grammatical errors) and the targeted translations (i.e. the grammatically well-formed sentences). The challenge is that we need a large amount of these parallel sentences for constructing such a data-driven SMT system. Some researches (Brockett et al., 2006, Yuan and Felice, 2013) explore generating artificial errors to resolve this sparsity problem. Other studies (Eh"
W14-1711,W13-1700,0,\N,Missing
W14-1711,W13-3608,0,\N,Missing
W14-1711,W13-3604,0,\N,Missing
W14-1711,P03-1021,0,\N,Missing
W14-3328,D11-1033,0,0.0229872,"s: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. ( ) (2) 5 where is a input sentence or"
W14-3328,P13-2119,0,0.0169687,"ecific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore a"
W14-3328,2012.eamt-1.38,0,0.0368196,"Missing"
W14-3328,W11-2123,0,0.0131162,"In our experiments, we not only use all available training data provided by the WMT2014 standard translation task 1 (generaldomain data) and medical translation task 2 (indomain data), but also acquire addition indomain bilingual translations (i.e. dictionary) and monolingual data from online sources. First of all, we collect the medical terminologies from the web. This tiny but significant parallel data are helpful to reduce the out-ofvocabulary words (OOVs) in translation models. In addition, the use of larger language models during decoding is aided by more efficient storage and inference (Heafield, 2011). Thus, we crawl more in-domain monolingual data from the Internet based on domain focused web-crawling approach. In order to detect and remove outdomain data from the crawled data, we not only explore text-to-topic classifier, but also propose an alternative filtering approach combined the existing one (text-to-topic classifier) with perplexity. After carefully pre-processing all the available training data, we apply language model adaptation and translation model adaptation using various kinds of training corpora. Experimental results show that the presented approaches are helpful to further"
W14-3328,P02-1040,0,0.0883673,"Missing"
W14-3328,P07-2045,0,0.0104477,"l text. In addition to these general data filtering steps, we introduce some extra steps to pre-process the training data. The first step is to remove the duplicate sentences. In data-driven methods, the more frequent a term occurs, the higher probabil8 Baseline System http://www.statmt.org/moses/?n=Moses.Baseline. Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. http://www.statmt.org/moses/. 11 http://www.ky"
W14-3328,W07-0733,0,0.0310125,"official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. ( ) (2) 5 where is a input sentence or document, ( ) is the probability of -gram segments estimated from the training set. is the number of tokens of an input string. 3 Pre-processing Both official training data and web-crawled resources are processed using the Moses scripts8, this includes the text tokenization, truecasing and length cleaning. For trusecasing, we use both the target side of parallel corpora and monolingual data to train the trucase models. We consider the tar"
W14-3328,P10-2041,0,0.021345,", 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of b"
W14-3328,P03-1021,0,0.0125664,"ur final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview. 12 http://www.speech.sri.com/projects/srilm/. 10 235 Data Set In-domain Parallel Data Lang. Sent. cs/en 1,770,421 de/en 3,894,099 fr/en 4,579,533 cs/en 12,426,374 Generaldomain Parallel Data de/en 4,421,961 fr/en 36,342,530 In-domain Mono. Data Generaldomain Mono. Data Web-crawled In-domain Mono. Data cs fr de en cs fr de en en cs de fr 106,548 1,424,539 2,222,502 7,802,610 33,408,340 30,850,165 84,633,641 85,254,788 8,448,566 44,198 473,171 852,036 Words 9,373,"
W14-3328,J03-1002,0,0.00467779,"e duplicate sentences. In data-driven methods, the more frequent a term occurs, the higher probabil8 Baseline System http://www.statmt.org/moses/?n=Moses.Baseline. Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview. 12 http://www.speech.sri.com/projects/srilm/. 10 235 Data Set In-domain Parallel Data Lang. Sent. cs/en 1,770"
W14-3328,W13-2803,0,0.0114563,"ta, which would harm the domain-specific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data."
W14-3328,J03-3002,0,\N,Missing
W14-3328,2011.eamt-1.40,0,\N,Missing
W14-3328,W13-2227,0,\N,Missing
W14-3328,W08-0509,0,\N,Missing
W14-3331,D11-1033,0,0.0286066,"ile 5. Translation Model Adaptation the LM adapted systems are listed with values relative to the Baseline2. The results indicate that As shown in Table 2, general-domain parallel LM adaptation can gain a reasonable improvecorpora are around 1 to 7 times larger than the ment if the LMs are trained on more relevant in-domain ones. We suspect if general-domain data for each pair, instead of using the whole corpus is broad enough to cover some in-domain training data. For different systems, their BLEU sentences. To observe the domain-specificity of general-domain corpus, we firstly evaluate sys8 Axelrod et al. (2011) names the selected data as pseudo intems trained on general-domain corpora. In Tadomain data. We adopt both terminologies in this paper. 256 ble 4, we show the BLEU scores of generaldomain systems9 on translating the medical sentences. The BLEU scores of the compared systems are relative to the Baseline2 and the size of the used general-domain corpus is relative to the corresponding in-domain one. For en-cs, cs-en, en-fr and fr-en pairs, the general-domain parallel corpora we used are 6 times larger than the original ones and we obtain the improved BLEU scores by 1.72 up to 3.96 points. While"
W14-3331,P13-2119,0,0.0215483,"49 1,709,594 25.56 7,802,610 cs 33,408,340 567,174,266 3,431,946 16.98 fr 30,850,165 780,965,861 2,142,470 25.31 General-domain Mono. Data de 84,633,641 1,548,187,668 10,726,992 18.29 en 85,254,788 2,033,096,800 4,488,816 23.85 Table 2: Statistics summary of corpora after pre-processing. only use the in-domain training data, but also the scores peak at different values of N. It gives the selected pseudo in-domain data 8 from generalbest results for cs-en, en-fr and de-en pairs when domain corpus to enhance the LMs (Toral, 2013; N=25, en-cs and en-de pairs when N=50, and frRubino et al., 2013; Duh et al., 2013). Firstly, en pair when N=75. Among them, en-cs and eneach sentence s in general-domain monolingual fr achieve the highest BLEU scores. The reason corpus is scored using the cross-entropy differis that their original monolingual (in-domain) ence method in (Moore and Lewis, 2010), which data for training the LMs are not sufficient. is calculated as follows: When introducing the extra pseudo in-domain data, the systems improve the translation quality score(s)  H I ( s)  HG ( s) (1) by around 2 BLEU points. While for cs-en, fr-en and de-en pairs, the gains are small. However, it where H(s) is t"
W14-3331,W08-0509,0,0.0456129,"Missing"
W14-3331,W11-2123,0,0.011601,"t comparisons with the proposed systems described in Section 4, 5, 6 and 7. 5 7 6 http://www.speech.sri.com/projects/srilm/. http://www.nactem.ac.uk/y-matsu/geniass/. Lang. Pair Baseline1 Baseline2 Diff. en-cs 12.92 +4.65 17.57 cs-en 20.85 +10.44 31.29 en-fr 38.31 +0.05 38.36 fr-en 44.27 +0.09 44.36 en-de 17.81 +0.20 18.01 de-en 32.34 +0.16 32.50 Table 1: BLEU scores of two baseline systems trained on original and processed corpora for different language pairs. 4. Language Model Adaptation The use of LMs (trained on large data) during decoding is aided by more efficient storage and inference (Heafield, 2011). Therefore, we not Data are processed according to Moses baseline tutorial: http://www.statmt.org/moses/?n=Moses.Baseline. 255 Data Set Lang. Sent. Words Vocab. Ave. Len. 9,373,482/ 134,998/ 5.29/ cs/en 1,770,421 10,605,222 156,402 5.99 In-domain 52,211,730/ 1,146,262/ 13.41/ de/en 3,894,099 Parallel Data 58,544,608 487,850 15.03 77,866,237/ 495,856/ 17.00/ fr/en 4,579,533 68,429,649 556,587 14.94 180,349,215/ 1,614,023/ 14.51/ cs/en 12,426,374 183,841,805 1,661,830 14.79 General-domain 106,001,775/ 1,912,953/ 23.97/ de/en 4,421,961 Parallel Data 112,294,414 919,046 25.39 1,131,027,766/ 3,149"
W14-3331,W13-2803,0,0.0112352,"4 37.79 In-domain Mono. Data de 2,222,502 53,840,304 1,415,202 24.23 en 199430649 1,709,594 25.56 7,802,610 cs 33,408,340 567,174,266 3,431,946 16.98 fr 30,850,165 780,965,861 2,142,470 25.31 General-domain Mono. Data de 84,633,641 1,548,187,668 10,726,992 18.29 en 85,254,788 2,033,096,800 4,488,816 23.85 Table 2: Statistics summary of corpora after pre-processing. only use the in-domain training data, but also the scores peak at different values of N. It gives the selected pseudo in-domain data 8 from generalbest results for cs-en, en-fr and de-en pairs when domain corpus to enhance the LMs (Toral, 2013; N=25, en-cs and en-de pairs when N=50, and frRubino et al., 2013; Duh et al., 2013). Firstly, en pair when N=75. Among them, en-cs and eneach sentence s in general-domain monolingual fr achieve the highest BLEU scores. The reason corpus is scored using the cross-entropy differis that their original monolingual (in-domain) ence method in (Moore and Lewis, 2010), which data for training the LMs are not sufficient. is calculated as follows: When introducing the extra pseudo in-domain data, the systems improve the translation quality score(s)  H I ( s)  HG ( s) (1) by around 2 BLEU points. Whi"
W14-3331,J03-1002,0,0.0205709,"Missing"
W14-3331,P03-1021,0,0.0280254,"Missing"
W14-3331,W07-0733,0,0.075072,"Missing"
W14-3331,P07-2045,0,0.0125756,"detailed in Section 2 & 3), the results of each method show reasonable gains. We combine individual approach to further improve the performance of our Experimental Setup All available training data from both WMT2014 standard translation task 1 (general-domain data) and medical translation task 2 (in-domain data) are used in this study. The official medical summary development sets (dev) are used for tuning and evaluating all the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.03 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++ 4 (Och and Ney, 1 http://www.statmt.org/wmt14/translation-task.html. http://www.statmt.org/wmt14/medical-task/. 3 http://www.statmt.org/moses/. 4 http://www.kyloo.net/software/doku.php/mgiza:overview. 2 254 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 254–259, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was train"
W14-3331,P10-2041,0,0.0194551,"re-processing. only use the in-domain training data, but also the scores peak at different values of N. It gives the selected pseudo in-domain data 8 from generalbest results for cs-en, en-fr and de-en pairs when domain corpus to enhance the LMs (Toral, 2013; N=25, en-cs and en-de pairs when N=50, and frRubino et al., 2013; Duh et al., 2013). Firstly, en pair when N=75. Among them, en-cs and eneach sentence s in general-domain monolingual fr achieve the highest BLEU scores. The reason corpus is scored using the cross-entropy differis that their original monolingual (in-domain) ence method in (Moore and Lewis, 2010), which data for training the LMs are not sufficient. is calculated as follows: When introducing the extra pseudo in-domain data, the systems improve the translation quality score(s)  H I ( s)  HG ( s) (1) by around 2 BLEU points. While for cs-en, fr-en and de-en pairs, the gains are small. However, it where H(s) is the length-normalized crosscan still achieve a significant improvement of entropy. I and G are the in-domain and general0.60 up to 1.12 BLEU points. domain corpora, respectively. G is a random subset (same size as the I) of the general-domain Lang. N=0 N=25 N=50 N=75 N=100 corpus"
W14-3331,W13-2227,0,\N,Missing
