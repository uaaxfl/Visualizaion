2021.findings-acl.49,Better Combine Them Together! Integrating Syntactic Constituency and Dependency Representations for Semantic Role Labeling,2021,-1,-1,5,1,7618,hao fei,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.117,{MRN}: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction,2021,-1,-1,6,1,7778,jingye li,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.acl-long.372,A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition,2021,-1,-1,4,0.933333,7621,fei li,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER."
2020.findings-emnlp.8,Improving Text Understanding via Deep Syntax-Semantics Communication,2020,-1,-1,3,1,7618,hao fei,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Recent studies show that integrating syntactic tree models with sequential semantic models can bring improved task performance, while these methods mostly employ shallow integration of syntax and semantics. In this paper, we propose a deep neural communication model between syntax and semantics to improve the performance of text understanding. Local communication is performed between syntactic tree encoder and sequential semantic encoder for mutual learning of information exchange. Global communication can further ensure comprehensive information propagation. Results on multiple syntax-dependent tasks show that our model outperforms strong baselines by a large margin. In-depth analysis indicates that our method is highly effective in composing sentence semantics."
2020.findings-emnlp.18,Mimic and Conquer: Heterogeneous Tree Structure Distillation for Syntactic {NLP},2020,-1,-1,3,1,7618,hao fei,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Syntax has been shown useful for various NLP tasks, while existing work mostly encodes singleton syntactic tree using one hierarchical neural network. In this paper, we investigate a simple and effective method, Knowledge Distillation, to integrate heterogeneous structure knowledge into a unified sequential LSTM encoder. Experimental results on four typical syntax-dependent tasks show that our method outperforms tree encoders by effectively integrating rich heterogeneous structure syntax, meanwhile reducing error propagation, and also outperforms ensemble methods, in terms of both the efficiency and accuracy."
2020.emnlp-main.168,Retrofitting Structure-aware Transformer Language Model for End Tasks,2020,-1,-1,3,1,7618,hao fei,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks."
2020.coling-main.53,Modeling Local Contexts for Joint Dialogue Act Recognition and Sentiment Classification with Bi-channel Dynamic Convolutions,2020,-1,-1,3,1,7778,jingye li,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we target improving the joint dialogue act recognition (DAR) and sentiment classification (SC) tasks by fully modeling the local contexts of utterances. First, we employ the dynamic convolution network (DCN) as the utterance encoder to capture the dialogue contexts. Further, we propose a novel context-aware dynamic convolution network (CDCN) to better leverage the local contexts when dynamically generating kernels. We extended our frameworks into bi-channel version (i.e., BDCN and BCDCN) under multi-task learning to achieve the joint DAR and SC. Two channels can learn their own feature representations for DAR and SC, respectively, but with latent interaction. Besides, we suggest enhancing the tasks by employing the DiaBERT language model. Our frameworks obtain state-of-the-art performances against all baselines on two benchmark datasets, demonstrating the importance of modeling the local contexts."
2020.coling-main.263,End to End {C}hinese Lexical Fusion Recognition with Sememe Knowledge,2020,52,0,3,0,21360,yijiang liu,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we present Chinese lexical fusion recognition, a new task which could be regarded as one kind of coreference recognition. First, we introduce the task in detail, showing the relationship with coreference recognition and differences from the existing tasks. Second, we propose an end-to-end model for the task, handling mentions as well as coreference relationship jointly. The model exploits the state-of-the-art contextualized BERT representations as an encoder, and is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model."
2020.coling-main.370,{H}i{T}rans: A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations,2020,-1,-1,2,1,7778,jingye li,Proceedings of the 28th International Conference on Computational Linguistics,0,"Emotion detection in conversations (EDC) is to detect the emotion for each utterance in conversations that have multiple speakers. Different from the traditional non-conversational emotion detection, the model for EDC should be context-sensitive (e.g., understanding the whole conversation rather than one utterance) and speaker-sensitive (e.g., understanding which utterance belongs to which speaker). In this paper, we propose a transformer-based context- and speaker-sensitive model for EDC, namely HiTrans, which consists of two hierarchical transformers. We utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer so that utterance representations could be sensitive to the global context of the conversation. Moreover, we exploit an auxiliary task to make our model speaker-sensitive, called pairwise utterance speaker verification (PUSV), which aims to classify whether two utterances belong to the same speaker. We evaluate our model on three benchmark datasets, namely EmoryNLP, MELD and IEMOCAP. Results show that our model outperforms previous state-of-the-art models."
2020.acl-main.397,{AMR} Parsing with Latent Structural Information,2020,-1,-1,3,0,22891,qiji zhou,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences. We investigate parsing AMR with explicit dependency structures and interpretable latent structures. We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks. The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5{\%} Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8{\%} Smatch F1 on LDC2014T12)."
2020.acl-main.588,Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification,2020,-1,-1,2,0,5885,hao tang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But the improvement is limited due to the noise and instability of dependency trees. To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner. Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning. The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin."
2020.acl-main.627,Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus,2020,54,0,3,1,7618,hao fei,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly."
2020.aacl-main.13,High-order Refining for End-to-end {C}hinese Semantic Role Labeling,2020,-1,-1,3,1,7618,hao fei,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Current end-to-end semantic role labeling is mostly accomplished via graph-based neural models. However, these all are first-order models, where each decision for detecting any predicate-argument pair is made in isolation with local features. In this paper, we present a high-order refining mechanism to perform interaction between all predicate-argument pairs. Based on the baseline graph model, our high-order refining module learns higher-order features between all candidate pairs via attention calculation, which are later used to update the original token representations. After several iterations of refinement, the underlying token representations can be enriched with globally interacted features. Our high-order model achieves state-of-the-art results on Chinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile relieving the long-range dependency issues."
D19-5723,Bacteria Biotope Relation Extraction via Lexical Chains and Dependency Graphs,2019,0,0,5,0,26529,wuti xiong,Proceedings of The 5th Workshop on BioNLP Open Shared Tasks,0,"abstract In this article, we describe our approach for the Bacteria Biotopes relation extraction (BB-rel) subtask in the BioNLP Shared Task 2019. This task aims to promote the development of text mining systems that extract relationships between Microorganism, Habitat and Phenotype entities. In this paper, we propose a novel approach for dependency graph construction based on lexical chains, so one dependency graph can represent one or multiple sentences. After that, we propose a neural network model which consists of the bidirectional long short-term memories and an attention graph convolution neural network to learn relation extraction features from the graph. Our approach is able to extract both intra- and inter-sentence relations, and meanwhile utilize syntax information. The results show that our approach achieved the best F1 (66.3{\%}) in the official evaluation participated by 7 teams."
S16-1141,{WHUN}lp at {S}em{E}val-2016 Task {D}i{MSUM}: A Pilot Study in Detecting Minimal Semantic Units and their Meanings using Supervised Models,2016,20,0,3,0,34304,xin tang,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1138,Multi-prototype {C}hinese Character Embedding,2016,23,9,3,0,34882,yanan lu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Chinese sentences are written as sequences of characters, which are elementary units of syntax and semantics. Characters are highly polysemous in forming words. We present a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings, and explore the usefulness of such character embeddings to Chinese NLP tasks. Evaluation on character similarity shows that multi-prototype embeddings are significantly better than a single-prototype baseline. In addition, used as features in the Chinese NER task, the embeddings result in a 1.74{\%} F-score improvement over a state-of-the-art baseline."
C16-1235,Distance Metric Learning for Aspect Phrase Grouping,2016,46,4,3,0,35812,shufeng xiong,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Aspect phrase grouping is an important task in aspect-level sentiment analysis. It is a challenging problem due to polysemy and context dependency. We propose an Attention-based Deep Distance Metric Learning (ADDML) method, by considering aspect phrase representation as well as context representation. First, leveraging the characteristics of the review text, we automatically generate aspect phrase sample pairs for distant supervision. Second, we feed word embeddings of aspect phrases and their contexts into an attention-based neural network to learn feature representation of contexts. Both aspect phrase embedding and context embedding are used to learn a deep feature subspace for measure the distances between aspect phrases for K-means clustering. Experiments on four review datasets show that the proposed method outperforms state-of-the-art strong baseline methods."
P15-1045,Event-Driven Headline Generation,2015,40,11,4,0,37493,rui sun,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We propose an event-driven model for headline generation. Given an input document, the system identifies a key event chain by extracting a set of structural events that describe them. Then a novel multi-sentence compression algorithm is used to fuse the extracted events, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems."
D15-1211,"A Transition-based Model for Joint Segmentation, {POS}-tagging and Normalization",2015,31,7,5,0,37831,tao qian,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach."
D14-1055,Positive Unlabeled Learning for Deceptive Reviews Detection,2014,24,44,2,1,7620,yafeng ren,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Deceptive reviews detection has attracted significant attention from both business and research communities. However, due to the difficulty of human labeling needed for supervised learning, the problem remains to be highly challenging. This paper proposed a novel angle to the problem by modeling PU (positive unlabeled) learning. A semi-supervised model, called mixing population and individual property PU learning (MPIPUL), is proposed. Firstly, some reliable negative examples are identified from the unlabeled dataset. Secondly, some representative positive examples and negative examples are generated based on LDA (Latent Dirichlet Allocation). Thirdly, for the remaining unlabeled examples (we call them spy examples), which can not be explicitly identified as positive and negative, two similarity weights are assigned, by which the probability of a spy example belonging to the positive class and the negative class are displayed. Finally, spy examples and their similarity weights are incorporated into SVM (Support Vector Machine) to build an accurate classifier. Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines."
C14-1152,Word Sense Induction Using Lexical Chain based Hypergraph Model,2014,47,6,2,0,37831,tao qian,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Word Sense Induction is a task of automatically finding word senses from large scale texts. It is generally considered as an unsupervised clustering problem. This paper introduces a hypergraph model in which nodes represent instances of contexts where a target word occurs and hyperedges represent higher-order semantic relatedness among instances. A lexical chain based method is used for discovering the hyperedges, and hypergraph clustering methods are used for finding word senses among the context instances. Experiments show that this model outperforms other methods in supervised evaluation and achieves comparable performance with other methods in unsupervised evaluation."
C12-1075,Context-Enhanced Personalized Social Summarization,2012,16,6,2,1,37482,po hu,Proceedings of {COLING} 2012,0,None
I11-1054,Social Summarization via Automatically Discovered Social Context,2011,20,9,4,1,37482,po hu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Heavy research has been done in recent years on tasks of traditional summarization. However, social context, which is critical in building high-quality social summarizer for web documents, is usually neglected. To address this issue, we propose a novel summarization approach based on social context. In this approach, social summarization is implemented by first employing the tripartite clustering algorithm to simultaneously discover document context and user context for a specified document. Then sentence relationships intra and inter documents plus intended user communities are taken into account to evaluate the significance of each sentence in different context views. Finally, a few sentences with highest overall scores are selected to form the summary. Experimental results demonstrate the effectiveness of the proposed approach and show the superior performance over several baselines."
Y09-2006,Finding Answers to Definition Questions Using Web Knowledge Bases,2009,12,0,2,1,46689,han ren,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Current researches on Question Answering concern more complex questions than factoid ones. Since definition questions are investigated by many researches, how to acquire accurate answers still becomes a core problem for definition QA. Although some systems use web knowledge bases to improve answer acquisition, we propose an approach that leverage them in an effective way. After summarizing definitions from web knowledge bases and merge them to a definition set, a two-stage retrieval model based on Probabilistic Latent Semantic Analysis is produced to seek documents and sentences in which the topic is similar to those in definition set. Then, an answer ranking model is employed to select both statistically and semantically similar sentences between sentences retrieved and sentences in definition set. Finally, sentences are ranked as answer candidates according to their scores. Experiments indicate following conclusions: 1) specific summarization technologies improves definition QA systems to a better performance; 2) topic based models can be more helpful than centroid-based models for definition QA systems in solving synonym and data sparse problems; 3) shallow semantic analysis is effective to find discriminative characteristics of definitions automatically."
Y09-2037,Document Re-ranking via {W}ikipedia Articles for Definition/Biography Type Questions,2009,11,1,3,0,22033,maofu liu,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"In this paper, we propose a document re-ranking approach based on the Wikipedia articles related to the specific questions to re-order the initial retrieved documents to improve the precision of top retrieved documents in Chinese information retrieval for question answering (IR4QA) system where the questions are definition or biography type. On one hand, we compute the similarity between each document in the initial retrieved results and the related Wikipedia article. On the other hand, we do clustering analysis for the documents based on the K-Means clustering method and compute the similarity between each centroid of the clusters and the Wikipedia article. Then we integrate the two kinds of similarity with the initial ranking score as the last similarity value and re-rank the documents in descending order with this measure. Experiment results demonstrate that this approach can improve the precision of the top relevant documents effectively."
Y09-1021,Query-Focused Multi-Document Summarization Using Co-Training Based Semi-Supervised Learning,2009,17,2,2,1,37482,po hu,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"This paper presents a novel approach to query-focused multi-document summarization. As a good biased summary is expected to keep a balance among query relevance, content salience and information diversity, the approach first makes use of both the content feature and the relationship feature to select a number of sentences via the co- training based semi-supervised learning, which can identify the query relevant sentences beyond a single point of view. Then the ranking algorithm based on Markov chain random walks is employed on the relevant sentences by encouraging content salience and information diversity in a unified framework. The final summary focusing on the integration of relevance, salience and diversity is created after several sentences with the highest overall ranking scores are extracted. We performed experiments on DUC2007 dataset and the evaluation results show that the proposed approach can achieve significant improvement over standard baseline approaches and gain comparable performance to the state-of-the-art systems."
W09-1215,Parsing Syntactic and Semantic Dependencies for Multiple Languages with A Pipeline Approach,2009,17,10,2,1,46689,han ren,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"This paper describes a pipelined approach for CoNLL-09 shared task on joint learning of syntactic and semantic dependencies. In the system, we handle syntactic dependency parsing with a transition-based approach and utilize MaltParser as the base model. For SRL, we utilize a Maximum Entropy model to identify predicate senses and classify arguments. Experimental results show that the average performance of our system for all languages achieves 67.81% of macro F1 Score, 78.01% of syntactic accuracy, 56.69% of semantic labeled F1, 71.66% of macro precision and 64.66% of micro recall."
W08-2115,Automatic {C}hinese Catchword Extraction Based on Time Series Analysis,2008,7,0,2,1,46689,han ren,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"Catchwords refer to those popular words or phrases in a time period. In this paper, we propose a novel approach for automatic extraction of Chinese catchwords. By analyzing features of catchwords, we define three aspects to describe Popular Degree of catchwords. Then we use curve fitting in Time Series Analysis to build Popular Degree Curves of the extracted terms. Finally we give a formula that can calculate Popular Degree values of catchwords and get a ranking list of catchword candidates. Experiments show that the method is effective."
ren-etal-2008-research,A Research on Automatic {C}hinese Catchword Extraction,2008,7,0,2,1,46689,han ren,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Catchwords refer to popular words or phrases within certain area in certain period of time. In this paper, we propose a novel approach for automatic Chinese catchwords extraction. At the beginning, we discuss the linguistic definition of catchwords and analyze the features of catchwords by manual evaluation. According to those features of catchwords, we define three aspects to describe Popular Degree of catchwords. To extract terms with maximum meaning, we adopt an effective ATE algorithm for multi-character words and long phrases. Then we use conic fitting in Time Series Analysis to build Popular Degree Curves of extracted terms. To calculate Popular Degree Values of catchwords, a formula is proposed which includes values of Popular Trend, Peak Value and Popular Keeping. Finally, a ranking list of catchword candidates is built according to Popular Degree Values. Experiments show that automatic Chinese catchword extraction is effective and objective in comparison with manual evaluation."
I08-2103,Sentence Ordering based on Cluster Adjacency in Multi-Document Summarization,2008,12,15,1,1,7622,donghong ji,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"In this paper, we propose a cluster-adjacency based method to order sentences for multi-document summarization tasks. Given a group of sentences to be organized into a summary, each sentence was mapped to a theme in source documents by a semi-supervised classification method, and adjacency of pairs of sentences is learned from source documents based on adjacency of clusters they belong to. Then the ordering of the summary sentences can be derived with the first sentence determined. Experiments and evaluations on DUC04 data show that this method gets better performance than other existing sentence ordering methods."
S07-1037,"{I}2{R}: Three Systems for Word Sense Discrimination, {C}hinese Word Sense Disambiguation, and {E}nglish Word Sense Disambiguation",2007,8,27,2,1,9439,zhengyu niu,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper describes the implementation of our three systems at SemEval-2007, for task 2 (word sense discrimination), task 5 (Chinese word sense disambiguation), and the first subtask in task 17 (English word sense disambiguation). For task 2, we applied a cluster validation method to estimate the number of senses of a target word in untagged data, and then grouped the instances of this target word into the estimated number of clusters. For both task 5 and task 17, We used the label propagation algorithm as the classifier for sense disambiguation. Our system at task 2 achieved 63.9% F-score under unsupervised evaluation, and 71.9% supervised recall with supervised evaluation. For task 5, our system obtained 71.2% micro-average precision and 74.7% macro-average precision. For the lexical sample subtask for task 17, our system achieved 86.4% coarse-grained precision and recall."
W06-1649,Partially Supervised Sense Disambiguation by Learning Sense Number from Tagged and Untagged Corpora,2006,18,0,2,1,9439,zhengyu niu,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"Supervised and semi-supervised sense disambiguation methods will mis-tag the instances of a target word if the senses of these instances are not defined in sense inventories or there are no tagged instances for these senses in training data. Here we used a model order identification method to avoid the misclassification of the instances with undefined senses by discovering new senses from mixed data (tagged and untagged corpora). This algorithm tries to obtain a natural partition of the mixed data by maximizing a stability criterion defined on the classification result from an extended label propagation algorithm over all the possible values of the number of senses (or sense number, model order). Experimental results on SENSEVAL-3 data indicate that it outperforms SVM, a one-class partially supervised classification algorithm, and a clustering based model order identification algorithm when the tagged data is incomplete."
W06-1667,Unsupervised Relation Disambiguation with Order Identification Capabilities,2006,16,3,2,1,49775,jinxiu chen,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts. It works by calculating eigen-vectors of an adjacency graph's Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. This method can address two difficulties encoutered in Hasegawa et al. (2004)'s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)'s hierarchical clustering method and a plain k-means clustering method."
W06-0125,{C}hinese Word Segmentation and Named Entity Recognition Based on a Context-Dependent Mutual Information Independence Model,2006,7,6,4,0,3694,min zhang,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper briefly describes our system in the third SIGHAN bakeoff on Chinese word segmentation and named entity recognition. This is done via a word chunking strategy using a context-dependent Mutual Information Independence Model. Evaluation shows that our system performs well on all the word segmentation closed tracks and achieves very good scalability across different corpora. It also shows that the use of the same strategy in named entity recognition shows promising performance given the fact that we only spend less than three days in total on extending the system in word segmentation to incorporate named entity recognition, including training and formal testing."
P06-2012,Unsupervised Relation Disambiguation Using Spectral Clustering,2006,16,14,2,1,49775,jinxiu chen,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,This paper presents an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts. It works by calculating eigen-vectors of an adjacency graph's Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods.
P06-1017,Relation Extraction Using Label Propagation Based Semi-Supervised Learning,2006,17,69,2,1,49775,jinxiu chen,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Shortage of manually labeled data is an obstacle to supervised relation extraction methods. In this paper we investigate a graph based semi-supervised learning algorithm, a label propagation (LP) algorithm, for relation extraction. It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph, and tries to obtain a labeling function to satisfy two constraints: 1) it should be fixed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task."
N06-2007,Semi-supervised Relation Extraction with Label Propagation,2006,12,9,2,1,49775,jinxiu chen,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"To overcome the problem of not having enough manually labeled relation instances for supervised relation extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method."
P05-1049,Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning,2005,36,93,2,1,9439,zhengyu niu,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods. In this paper we investigate a label propagation based semi-supervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping."
O05-5005,An Unsupervised Approach to {C}hinese Word Sense Disambiguation Based on Hownet,2005,1,3,3,0,1671,hao chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,"The research on word sense disambiguation (WSD) has great theoretical and practical significance in many fields of natural language processing (NLP). This paper presents an unsupervised approach to Chinese word sense disambiguation based on Hownet (an electronic Chinese lexical resource). In our approach, contexts that include ambiguous words are converted into vectors by means of a second-order context method, and these context vectors are then clustered by the k-means clustering algorithm. Lastly, the ambiguous words can be disambiguated after a similarity calculation process is completed. Our experiments involved extraction of terms, and an 82.62% average accuracy rate was achieved."
I05-2045,Unsupervised Feature Selection for Relation Extraction,2005,14,52,2,1,49775,jinxiu chen,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"This paper presents an unsupervised relation extraction algorithm, which induces relations between entity pairs by grouping them into a xe2x80x9cnaturalxe2x80x9d number of clusters based on the similarity of their contexts. Stability-based criterion is used to automatically estimate the number of clusters. For removing noisy feature words in clustering procedure, feature selection is conducted by optimizing a trace based criterion subject to some constraint in an unsupervised manner. After relation clustering procedure, we employ a discriminative category matching (DCM) to find typical and discriminative words to represent different relations. Experimental results show the effectiveness of our algorithm."
I05-1035,Automatic Relation Extraction with Model Order Selection and Discriminative Label Identification,2005,16,17,2,1,49775,jinxiu chen,Second International Joint Conference on Natural Language Processing: Full Papers,0,"In this paper, we study the problem of unsupervised relation extraction based on model order identification and discriminative feature analysis. The model order identification is achieved by stability-based clustering and used to infer the number of the relation types between entity pairs automatically. The discriminative feature analysis is used to find discriminative feature words to name the relation types. Experiments on ACE corpus show that the method is promising."
H05-1114,A Semi-Supervised Feature Clustering Algorithm with Application to Word Sense Disambiguation,2005,32,6,2,1,9439,zhengyu niu,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In this paper we investigate an application of feature clustering for word sense disambiguation, and propose a semisupervised feature clustering algorithm. Compared with other feature clustering methods (ex. supervised feature clustering), it can infer the distribution of class labels over (unseen) features unavailable in training data (labeled data) by the use of the distribution of class labels over (seen) features available in training data. Thus, it can deal with both seen and unseen features in feature clustering process. Our experimental results show that feature clustering can aggressively reduce the dimensionality of feature space, while still maintaining state of the art sense disambiguation accuracy. Furthermore, when combined with a semi-supervised WSD algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation."
W04-1103,Document Re-ranking based on Global and Local Terms,2004,0,7,2,1,49864,lingpeng yang,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,None
W04-1117,A Large-Scale Semantic Structure for {C}hinese Sentences,2004,2,0,2,0,3072,li tang,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"Motivated by a systematic analysis of Chinese semantic relationships, we constructed a Chinese semantic framework based on surface syntactic relationships, deep semantic relationships and feature structure to express dependencies between lexical meanings and conceptual structures, and relations that underlie those lexical meanings. Analyzing the semantic representations of 10000 Chinese sentences, we provide a model of semantically and syntactically annotated sentences from which reliable information on combinatorial possibilities of each semantic item targeted for analysis can be displayed. We also propose a semantic argument xe2x80x93 head relation, xe2x80x98basic conceptual structurexe2x80x99 and the xe2x80x98Head-Driven Principlexe2x80x99. Our results show that we can successfully disambiguate some troublesome sentences, and minimize the redundancy in language knowledge descriptions for natural language processing."
W04-1018,{C}hinese Text Summarization Based on Thematic Area Detection,2004,-1,-1,3,1,37482,po hu,Text Summarization Branches Out,0,None
W04-0847,Optimizing feature set for {C}hinese Word Sense Disambiguation,2004,7,10,2,1,9439,zhengyu niu,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"This article describes the implementation of I2R word sense disambiguation system (I2R xe2x88x92WSD) that participated in one senseval3 task: Chinese lexical sample task. Our core algorithm is a supervised Naive Bayes classifier. This classifier utilizes an optimal feature set, which is determined by maximizing the cross validated accuracy of NB classifier on training data. The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context."
P04-1080,Learning Word Sense With Feature Selection and Order Identification Capabilities,2004,20,3,2,1,9439,zhengyu niu,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"This paper presents an unsupervised word sense learning algorithm, which induces senses of target word by grouping its occurrences into a natural number of clusters based on the similarity of their contexts. For removing noisy words in feature set, feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner. Gaussian mixture model and Minimum Description Length criterion are used to estimate cluster structure and cluster number. Experimental results show that our algorithm can find important feature subset, estimate model order (cluster number) and achieve better performance than another algorithm which requires cluster number to be provided."
tang-etal-2004-model,A Model of Semantic Representations Analysis for {C}hinese Sentences,2004,1,0,2,0,3072,li tang,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Analyzing the semantic Representations of 5000 Chinese sentences and describing a new sentence analysis method that evaluates semantic preference knowledge, we create a model of semantic representation analysis based on the correspondence between lexical meanings and conceptual structures, and relations that underlie those lexical meanings. We also propose a semantical argument-head relation that combines xe2x80x98basic conceptual structurexe2x80x99 and xe2x80x98Head-Driven"
ji-etal-2004-building,Building a Conceptual Graph Bank for {C}hinese Language,2004,5,0,1,1,7622,donghong ji,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
C04-1069,Document Re-ranking Based on Automatically Acquired Key Terms in {C}hinese Information Retrieval,2004,9,17,2,1,49864,lingpeng yang,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"For Information Retrieval, users are more concerned about the precision of top ranking documents in most practical situations. In this paper, we propose a method to improve the precision of top N ranking documents by reordering the retrieved documents from the initial retrieval. To reorder documents, we first automatically extract Global Key Terms from document set, then use extracted Global Key Terms to identify Local Key Terms in a single document or query topic, finally we make use of Local Key Terms in query and documents to reorder the initial ranking documents. The experiment with NTCIR3 CLIR dataset shows that an average 10%-11% improvement and 2%-5% improvement in precision can be achieved at top 10 and 100 ranking documents level respectively."
W00-1215,Semantic Annotation of {C}hinese Phrases Using Recursive Graph,2000,5,5,1,1,7622,donghong ji,Second {C}hinese Language Processing Workshop,0,"In this paper, we propose a recursive graph based scheme for semantic annotation of Chinese phrases. Compared with others, this scheme can fully differentiate those Chinese phrases that comprise the same content words but hold different meanings due to their different word order or some involved function words, and capture the hierarchical conceptual structure of Chinese phrases, which underlies their main semantic information. We also give the guidelines for annotating various commonly used types of Chinese phrases."
P98-1098,Combining a {C}hinese Thesaurus with a {C}hinese Dictionary,1998,7,1,1,1,7622,donghong ji,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"In this paper, we study the problem of combining a Chinese thesaurus with a Chinese dictionary by linking the word entries in the thesaurus with the word senses in the dictionary, and propose a similar word strategy to solve the problem. The method is based on the definitions given in the dictionary, but without any syntactic parsing or sense disambiguation on them at all. As a result, their combination makes the thesaurus specify the similarity between senses which accounts for the similarity between words, produces a kind of semantic classification of the senses defined in the dictionary, and provides reliable information about the lexical items on which the resources don't conform with each other."
C98-1095,Combining a {C}hinese Thesaurus with a {C}hinese Dictionary,1998,7,1,1,1,7622,donghong ji,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"In this paper, we study the problem of combining a Chinese thesaurus with a Chinese dictionary by linking the word entries in the thesaurus with the word senses in the dictionary, and propose a similar word strategy to solve the problem. The method is based on the definitions given in the dictionary, but without any syntactic parsing or sense disambiguation on them at all. As a result, their combination makes the thesaurus specify the similarity between senses which accounts for the similarity between words, produces a kind of semantic classification of the senses defined in the dictionary, and provides reliable information about the lexical items on which the resources don't conform with each other."
W97-1004,Learning New Compositions from Given Ones,1997,16,1,1,1,7622,donghong ji,{C}o{NLL}97: Computational Natural Language Learning,0,"In this paper, we study the problem of learning new compositions of words from given ones with a specific syntactic structure, e.g., A-N or V-N structures. We first cluster words according to the given compositions, then construct a cluster-based compositional frame for each word cluster, which contains both new and given compositions relevant with the words in the cluster. In contrast to other methods, we don't pre-define the number of clusters, and formalize the problem of clustering words as a non-linear optimization one, in which we specify the environments of words based on word clusters to be determined, rather than their neighboring words. To solve the problem, we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm."
W97-0321,Word Sense Disambiguation Based on Structured Semantic Space,1997,9,0,1,1,7622,donghong ji,Second Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a framework, structured semantic space, as a foundation for word sense disarnbiguation tasks, and present a strategy to identify the correct sense of a word in some context based on the space. The semantic space is a set of multidimensional real-valued vectors, which formally describe the contexts of words. Instead of locating all word senses in the space, we only make use of mono-sense words to outline it. We design a merging procedure to establish the dendrogram structure of the space and give an heuristic algorithm to find the nodes (sense clusters) corresponding with sets of similar senses in the dendrogram. Given a word in a particular context' the context would activate some clusters in the dendrogram, based on its similarity with the contexts of the words in the clusters, then the correct sense of the word could be determined by comparing its definitions with those of the words in the clusters."
