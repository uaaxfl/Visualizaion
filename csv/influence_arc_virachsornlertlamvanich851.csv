1997.iwpt-1.16,P93-1005,0,0.0808478,"Missing"
1997.iwpt-1.16,J93-1002,0,0.171651,"d associates a probability with each parse derivation, given by the product of the probability of each change included in the derivation. Further, they also described an algorithm to handle this model within the GLR parsing framework, gaining parse efficiency. However, since their probabilistic model in itself is not intimately coupled with the GLR parsing algorithm, their model needs an additional complex algorithm for training. On the other hand, Briscoe and Carroll proposed the distribution of probabilities directly to each action in an LR table to realize mildly context-sensitive parsing' [3] . Their model overcomes the drawback of context insensitivity of PCFGs by estimating the probability of each LR parsing action according to its left (i.e. LR parse state) and right context (i.e. next input symbol) . The probability of each parse derivation is computed as the product of the probability assigned to each action included in the derivation. Unlike the approach of 1 By ""a mildly context-sensitive model"" , we mean a model that is moderately more context-sensitive than context-free models such as PCFGs, but not a fully context-sensitive one, which would b e intractable in both train"
1997.iwpt-1.16,P96-1025,0,0.0949265,"Missing"
1997.iwpt-1.16,W96-0112,0,0.0992357,"Missing"
1997.iwpt-1.16,E91-1004,0,0.038428,"Missing"
1997.iwpt-1.16,C92-2066,0,0.046663,"Missing"
1997.iwpt-1.16,1995.iwpt-1.26,0,0.157225,"Missing"
C00-2116,J97-2002,0,\N,Missing
C00-2116,C96-2208,1,\N,Missing
C00-2116,C94-1101,0,\N,Missing
C00-2116,P95-1037,0,\N,Missing
C08-2031,N01-1025,0,0.129083,"Missing"
C08-2031,C04-1010,0,0.0620355,"Missing"
C08-2031,W95-0107,0,0.031669,"Missing"
C08-2031,E99-1023,0,0.0312358,"of wi and wj and 6. positions of wi and wj . Assuming that {i0 , i1 , i2 , i4 } are the first four tokens in the remaining input and {s0 , s1 } are the two topmost tokens on the stack, we use the default features including: 1. POS of {i0 , i1 , i2 , i3 , s0 , s1 }, 2. word form of {s0 , i0 , i1 , head(s0 )}, 3. dependency type of s0 and its leftmost and rightmost dependent and the leftmost dependent of i0 . To examine the role of base-NP chunk information in dependency parsing, we include chunk labels in the feature sets of both parsers. BaseNP chunks are represented by using the IOB2 format (Sang and Veenstra, 1999). In the first parsing model, the chunk label of the current word is added as a feature of the root model, while the chunk labels of both considered words are added in the dependency model. We also add a feature showing that both words reside in the same chunk or not to the dependency model. In the second model, we include chunk labels of {s0 , s1 , i0 , i1 , i2 , i3 } as its feature set. We use a section of completely annotated corpus consisting of 2616 sentences to experiment with dependency parsing. The sentence length ranges between 2 words to 20 words with an average of 5.68. These Thai s"
C08-2031,I05-3003,0,0.0451225,"Missing"
C08-2031,C00-2109,1,0.850505,"Missing"
C08-2031,E06-1012,0,0.0534003,"Missing"
C08-2031,N03-1028,0,0.0649361,"Missing"
C94-1091,1993.mtsummit-1.1,0,0.0470517,"Missing"
C94-1091,J93-1007,0,0.0422528,"Missing"
C94-1091,J93-3004,0,0.0511082,"Missing"
C96-2208,C94-1101,0,0.182285,"Missing"
C96-2208,P90-1032,0,0.220697,"Missing"
C96-2208,J90-1003,0,\N,Missing
charoenporn-etal-2006-word,J98-2002,0,\N,Missing
charoenporn-etal-2006-word,C02-1065,0,\N,Missing
charoenporn-etal-2006-word,P98-1013,0,\N,Missing
charoenporn-etal-2006-word,C98-1013,0,\N,Missing
H01-1070,C00-2116,1,0.737742,"ey. To correct these errors, the errorcorrection rules proposed by [1] and [2] is employed. 3.3.2.1 Error-correction Rule Extraction After applying trigram prediction to the training corpus are considered to prepare the error correction rule. The left and right Error-Correction Rules Figure 2: Error-Correction Rule Extraction 3.3.2.2 Rule Reduction In the process of collecting the patterns, there are a lot of redundant patterns collected. For example, patterns no.1-3 in Table 2 should be reduced to pattern 4. To reduce the number of rules, left mutual information and right mutual information ([7]) are employed. When all patterns are shortened, the duplicate patterns are then eliminated in the final. Table 2: Error-Correction Rule Reduction Pattern Error Key Sequences Correct Patterns No. 1. k[lkl9 ¬«µ 2. mpklkl9 ¥µ«µ 3. kkklkl9 µ¬µ«µ 4. lkl9 «µ Left mutual information (Lm) and right mutual information (Rm) are the statistics used to shorten the patterns. Lm and right Rm are defined as follows. p ( xyz ) , p ( x) p ( yz ) p ( xyz ) Rm( xyz ) = , p ( xy ) p ( z ) Lm( xyz ) = where xyz is the pattern being considered, x is the leftmost character of xyz, y is the middle substrin"
H01-1070,C96-2110,0,\N,Missing
I05-1031,P00-1064,0,0.0595142,"Missing"
I05-1031,N04-4008,0,0.0372856,"Missing"
I05-1031,C02-1162,0,0.0573564,"Missing"
I05-1031,O05-2003,0,0.0572515,"Missing"
I05-1031,1997.mtsummit-workshop.7,0,0.0704653,"Missing"
I05-1031,C96-2195,0,0.0715933,"Missing"
I05-1031,C04-1164,0,\N,Missing
I08-1052,C00-1014,0,0.7112,"Missing"
I08-1052,C94-1091,1,0.807876,"Missing"
I08-2091,kaji-watanabe-2006-automatic,0,\N,Missing
kruengkrai-etal-2004-enriching,J98-2002,0,\N,Missing
kruengkrai-etal-2004-enriching,E95-1016,0,\N,Missing
kruengkrai-etal-2004-enriching,C02-1065,0,\N,Missing
kruengkrai-etal-2006-conditional,J00-4006,0,\N,Missing
kruengkrai-etal-2006-conditional,W04-3230,0,\N,Missing
kruengkrai-etal-2006-conditional,C04-1081,0,\N,Missing
kruengkrai-etal-2006-conditional,W03-0430,0,\N,Missing
kruengkrai-etal-2006-conditional,N03-1028,0,\N,Missing
kruengkrai-etal-2006-conditional,N04-4028,0,\N,Missing
kruengkrai-etal-2006-conditional,W97-0126,0,\N,Missing
N03-2035,1987.mtsummit-1.11,0,\N,Missing
N03-2035,P94-1013,0,\N,Missing
P00-1078,C00-2116,1,0.878362,"Missing"
P06-2106,francopoulo-etal-2006-lexical,1,0.861995,"Missing"
P06-2106,W03-1905,1,0.81081,"ess ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and allowing for encoding individual lexical entries as instances of the model (Ide et al., 2003; Bertagna et al., 2004b). In the framework of our project, by situating our work in the context of W3C standards and relying on standardized technologies underlying this community, the original RDF schema for ISLE lexical entries has been made compliant to OWL. The whole data model has been formalized in OWL by using Prot´eg´e 3.2 beta and has been extended to cover the morphological component as well (see Figure 2). Prot´eg´e 3.2 beta has been also used as a tool to instantiate the lexical entries of our sample monolingual lexicons, thus ensuring adherence to the model, encoding coherence an"
P06-2106,bel-etal-2000-simple,1,0.877753,"s: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of the following four research items. There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES1 , PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003) and LIRICS2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources. 2 (4) Evaluation through application classification Figure 1: Relations among research items 1 Introduction 1 (2) Sample lexicons (1) building a description framework of lexical entries (2) building sample lexicons (3) building an upper-layer ontology (4) evaluating the proposed framework through an application Figure 1 illustrates the relations among these research items. Our main aim is the researc"
P06-2106,C94-1091,1,0.485541,"ical operations, which are special lexical entities allowing the user to define multilin3 MILE is based on the experience derived from existing computational lexicons (e.g. LE-PAROLE, SIMPLE, EuroWordNet, etc.). 828 “CL” stands for a classifier. They always follow cardinal numbers in Japanese. Note that different classifiers are used for different nouns. In the above examples, classifier “hiki” is used to count noun “inu (dog)”, while “satsu” for “hon (book)”. The classifier is determined based on the semantic type of the noun. In the Thai language, classifiers are used in various situations (Sornlertlamvanich et al., 1994). The classifier plays an important role in construction with noun to express ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and al"
P06-2106,zhang-etal-2004-distributional,1,0.766907,"the set of postpositions as values of FunctionType instead of conventional function types such as “subj” and “obj”. It might be an user defined data category or language dependent data category. Furthermore, it is preferable to prepare the mapping between Japanese postpositions and conventional function types. This is interesting because it seems more a terminological difference, but the model can be applied also to Japanese. 4 Building sample lexicons 4.1 Swadesh list and basic lexicon The issue involved in defining a basic lexicon for a given language is more complicated than one may think (Zhang et al., 2004). The naive approach of simply taking the most frequent words in a language is flawed in many ways. First, all frequency counts are corpus-based and hence inherit the bias of corpus sampling. For instance, since it is easier to sample written formal texts, words used predominantly in informal contexts are usually underrepresented. Second, frequency of content words is topic-dependent and may vary from corpus to corpus. Last, and most crucially, frequency of a word does not correlate to its conceptual necessity, 4.2 Aligning multilingual lexical entries Since our goal is to build a multilingual"
P06-2106,bertagna-etal-2004-content,1,0.887142,"e morphological, syntactic and semantic layers. Moreover, an intermediate module allows to define mechanisms of linkage and mapping between the syntactic and semantic layers. Within each layer, a basic linguistic information unit is identified; basic units are separated but still interlinked each other across the different layers. Within each of the MLM layers, different types of lexical object are distinguished : fits with as many Asian languages as possible, and contributing to the ISO-TC37/SC4 activities. As a starting point, we employ an existing description framework, the MILE framework (Bertagna et al., 2004a), to describe several lexical entries of several Asian languages. Through building sample lexicons (research item (2)), we will find problems of the existing framework, and extend it so as to fit with Asian languages. In this extension, we need to be careful in keeping consistency with the existing framework. We start with Chinese, Japanese and Thai as target Asian languages and plan to expand the coverage of languages. The research items (2) and (3) also comprise the similar feedback loop. Through building sample lexicons, we refine an upper-layer ontology. An application built in the resea"
P06-2106,Y06-1043,1,\N,Missing
sornlertlamvanich-etal-2010-language,isahara-etal-2008-development,1,\N,Missing
sornlertlamvanich-etal-2010-language,W09-3401,1,\N,Missing
sornlertlamvanich-etal-2010-language,I08-2091,1,\N,Missing
tokunaga-etal-2008-adapting,bel-etal-2000-simple,1,\N,Missing
tokunaga-etal-2008-adapting,P06-2106,1,\N,Missing
tokunaga-etal-2008-adapting,I08-1052,1,\N,Missing
tokunaga-etal-2008-adapting,francopoulo-etal-2006-lexical,1,\N,Missing
tongchim-etal-2006-blind,J90-1003,0,\N,Missing
tongchim-etal-2008-dependency,W03-3023,0,\N,Missing
tongchim-etal-2008-dependency,E99-1026,1,\N,Missing
tongchim-etal-2008-dependency,W02-2016,0,\N,Missing
tongchim-etal-2008-dependency,C04-1040,0,\N,Missing
tongchim-etal-2008-dependency,E06-1012,0,\N,Missing
tongchim-etal-2008-dependency,C04-1010,0,\N,Missing
tongchim-etal-2008-dependency,2006.iwslt-evaluation.9,0,\N,Missing
tongchim-etal-2008-dependency,kruengkrai-etal-2006-conditional,1,\N,Missing
tongchim-etal-2008-dependency,W00-1303,0,\N,Missing
tongchim-etal-2008-dependency,C00-2109,1,\N,Missing
tongchim-etal-2008-dependency,W97-0126,0,\N,Missing
W02-1605,A94-1013,0,0.0145109,"se machine translation. Words and their part-ofspeech tags are the simplest information, which are produced from English analysis module. In this paper, we use words and/or part-of-speech tags around a target word in deciding a word meaning. 2.2 Machine Learning In this section, we will briefly descript three machine leaning techniques, C4.5, C4.5rule and RIPPER. 2.2.1 C4.5 & C4.5Rule C4.5, decision tree, is a traditional classifying technique that proposed by Quinlan [7]. C4.5 have been successfully applied in many NLP problems such as word extraction [9] and sentence boundary disambiguation [2]. So in this paper, we employ C4.5 in our experiments. The induction algorithm proceeds by evaluation content of series of attributes and iteratively building a tree from the attribute values with the leaves of the decision tree being the valued of the goal attribute. At each step of learning procedure, the evolving tree is branched on the attribute that partitions the data items with the highest information gain. Branches will be added until all items in the training set are classified. To reduce the effect of overfitting, C4.5 prunes the entire decision tree constructed. It recursively exami"
W02-1605,1999.mtsummit-1.37,0,0.0474667,"Missing"
W02-1605,J90-2002,0,0.211535,"Missing"
W02-1605,C96-1030,0,0.0910017,"Missing"
W02-1605,H01-1002,0,0.0123873,"duce description length. So in this paper we also employ both techniques of C4.5 and C4.5rule. 2.2.2 RIPPER RIPPER [10] is the one of the famous machine learning techniques applying in NLP problems [4], which was proprosed by William W. Cohen. On his experiment [10] shows that RIPPER is more efficient than C4.5 on noisy data and it scales nearly linearly with the number of examples in a dataset. So we decide to choose RIPPER in evaluating and comparing results with C4.5 and C4.5rule. RIPPER is a propositional rule learning algorithm that constructs a ruleset which classifies the training data [11]. A rule in the constructed ruleset is represented in the form of a conjunction of conditions: if T1 and T2 and ... Tn then class Cx. T1 and T2 and ... Tn is called the body of the rule. Cx is a target class to be learned; it can be a positive or negative class. A condition Ti tests for a particular value of an attribute, and it takes one of four forms: An = v, Ac ≥ θ, Ac ≤ θ and v 3 Overview of The System Input sentence Rule-based MT (ParSit) Translated sentence Context information (words and POS) Machine learning Translated sentence with improving quality Rules or tree from training data Fig"
W02-1605,C00-2116,1,\N,Missing
W09-3420,I08-2091,1,0.885731,"Missing"
W09-3420,W98-0709,0,0.103363,"Missing"
W09-3420,isahara-etal-2008-development,1,\N,Missing
W09-3421,francopoulo-etal-2006-lexical,1,0.857972,"Missing"
W09-3421,bel-etal-2000-simple,1,0.763962,"the advantages of corpusbased approaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian langua"
W09-3421,W03-1905,1,0.826021,"pproaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian languages, we elucidated the shortco"
W09-3421,P06-2106,1,0.822025,"Missing"
W09-3421,tokunaga-etal-2008-adapting,1,0.730656,"Missing"
W09-3421,W06-1001,1,\N,Missing
W11-3204,W10-2406,0,0.0121976,"roenporn, Virach Sornlertlamvanich National Electronics and Computer Technology Center Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand {canasai.kruengkrai,thatsanee.charoenporn,virach.sornlertlamvanich}@nectec.or.th Abstract transliteration units are not given in the training corpus. The process of machine transliteration is very similar to that of phrase-based statistical machine translation (SMT) (Koehn et al., 2003). As a result, a number of previous studies directly applied phrase-based SMT techniques to machine transliteration (Finch and Sumita, 2009; Rama and Gali, 2009; Finch and Sumita, 2010; Avinesh and Parikh, 2010). However, unlike word alignment in phrase-based SMT, character alignment in machine transliteration seems to be monotonic in which reordering of target language characters rarely occurs but is still possible in some language pairs. After alignment, the target language transliteration units can be considered as tags (or labels) of the source language transliteration units. As a result, some previous studies viewed machine transliteration as simply as a sequence labeling problem (Aramaki and Abekawwa, 2009; Shishtla et al., 2009). With this problem setting, the system"
W11-3204,N03-1017,0,0.0217024,"Missing"
W11-3204,P04-1021,0,0.0987426,"transliteration system that performs well in any given language pair without much effort in pre- and post-processing, and parameter tuning. To compare the performance of our system against state-of-the-art approaches, we participated in the machine transliteration shared task conducted as a part of the Named Entities Workshop (NEWS 2011), an IJCNLP 2011 workshop. Specifically, we focus on standard runs where only the corpus (containing parallel names) provided by the shared task is used for training. We submitted results for 10 language pairs. 2 Background 2.1 Motivation 2.2 As discussed in (Li et al., 2004), machine transliteration can be viewed as two levels of decoding: (1) segmenting the source language character string into transliteration units, and (2) relating the source language transliteration units with units in the target language by resolving different combinations of alignments and unit mappings. A transliteration unit could be one or more characters. Typically, the source and target language Problem Setting Here, we formulate the process of machine transliteration based on discriminative learning. Given a character string x in the source language, we need to find the most likely ch"
W11-3204,N06-1014,0,0.0327557,"given the weight vector w(i) . We generate bestk (xt ; w(i) ) using a dynamic programming search (Nagata, 1994). We measure L(yt , ˆy) using the zero-one loss function. Our basic features operate over the window of ±4 source language characters and the target language character bigrams. 5 Development and Final Results In development, we were interested in how the quality of alignment affects the performance of transliteration because errors in alignment inevitably propagate to the learning phase. We used two popular alignment tools, including GIZA++1 (Och and Ney, 2003) and BerkeleyAligner2 (Liang et al., 2006). With their default parameter settings, GIZA++ yields better performance than BerkeleyAligner on all development data sets. As a result, our submitted primary runs on the test data sets are based on the resulting alignments from GIZA++. Our learning algorithm 1 2 30 http://code.google.com/p/giza-pp http://code.google.com/p/berkeleyaligner Language Pair En→Ch Ch→En En→Th Th→En En→Hi En→Ta En→Ka En→Ba En→Pe En→He ACC 0.342 0.131 0.354 0.284 0.436 0.432 0.398 0.455 0.643 0.602 F-score 0.702 0.730 0.854 0.841 0.870 0.896 0.878 0.887 0.943 0.931 MRR 0.406 0.193 0.451 0.402 0.538 0.553 0.502 0.557"
W11-3204,C94-1032,0,0.0962701,", B-y3 i, hx4 , B-y4 y5 i, hx5 , I-y4 y5 i). Figure 6 shows another possible character reordering. We use the same scheme as the previous example. Thus, our training example becomes (hx1 , B-y1 i, hx2 , B-y2 i, hx3 , B-y4 y5 i, hx4 , I-y4 y5 i, hx5 , I-y4 y5 i). To summarize, we examine whether reordering occurs in the target language characters. If so, we merge those target language characters until the alignments become monotonic. where bestk (xt ; w(i) ) represents a set of top k-best outputs given the weight vector w(i) . We generate bestk (xt ; w(i) ) using a dynamic programming search (Nagata, 1994). We measure L(yt , ˆy) using the zero-one loss function. Our basic features operate over the window of ±4 source language characters and the target language character bigrams. 5 Development and Final Results In development, we were interested in how the quality of alignment affects the performance of transliteration because errors in alignment inevitably propagate to the learning phase. We used two popular alignment tools, including GIZA++1 (Och and Ney, 2003) and BerkeleyAligner2 (Liang et al., 2006). With their default parameter settings, GIZA++ yields better performance than BerkeleyAligne"
W11-3204,J03-1002,0,0.00789755,") represents a set of top k-best outputs given the weight vector w(i) . We generate bestk (xt ; w(i) ) using a dynamic programming search (Nagata, 1994). We measure L(yt , ˆy) using the zero-one loss function. Our basic features operate over the window of ±4 source language characters and the target language character bigrams. 5 Development and Final Results In development, we were interested in how the quality of alignment affects the performance of transliteration because errors in alignment inevitably propagate to the learning phase. We used two popular alignment tools, including GIZA++1 (Och and Ney, 2003) and BerkeleyAligner2 (Liang et al., 2006). With their default parameter settings, GIZA++ yields better performance than BerkeleyAligner on all development data sets. As a result, our submitted primary runs on the test data sets are based on the resulting alignments from GIZA++. Our learning algorithm 1 2 30 http://code.google.com/p/giza-pp http://code.google.com/p/berkeleyaligner Language Pair En→Ch Ch→En En→Th Th→En En→Hi En→Ta En→Ka En→Ba En→Pe En→He ACC 0.342 0.131 0.354 0.284 0.436 0.432 0.398 0.455 0.643 0.602 F-score 0.702 0.730 0.854 0.841 0.870 0.896 0.878 0.887 0.943 0.931 MRR 0.406"
W11-3204,W09-3513,0,0.107385,"hine transliteration (Finch and Sumita, 2009; Rama and Gali, 2009; Finch and Sumita, 2010; Avinesh and Parikh, 2010). However, unlike word alignment in phrase-based SMT, character alignment in machine transliteration seems to be monotonic in which reordering of target language characters rarely occurs but is still possible in some language pairs. After alignment, the target language transliteration units can be considered as tags (or labels) of the source language transliteration units. As a result, some previous studies viewed machine transliteration as simply as a sequence labeling problem (Aramaki and Abekawwa, 2009; Shishtla et al., 2009). With this problem setting, the system can apply any powerful discriminative training algorithm (e.g., Conditional Random Fields (CRFs) (Lafferty, 2001)) incorporated with rich features. Our system follows this research direction, but we pay more attention on how to extract appropriate transliteration units and train our model using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2005; McDonald, 2006). In this paper, we describe our system used in the NEWS 2011 machine transliteration shared task. Our system consists of two main components: simple strategi"
W11-3204,W09-3528,0,0.0196353,"ngkrai, Thatsanee Charoenporn, Virach Sornlertlamvanich National Electronics and Computer Technology Center Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand {canasai.kruengkrai,thatsanee.charoenporn,virach.sornlertlamvanich}@nectec.or.th Abstract transliteration units are not given in the training corpus. The process of machine transliteration is very similar to that of phrase-based statistical machine translation (SMT) (Koehn et al., 2003). As a result, a number of previous studies directly applied phrase-based SMT techniques to machine transliteration (Finch and Sumita, 2009; Rama and Gali, 2009; Finch and Sumita, 2010; Avinesh and Parikh, 2010). However, unlike word alignment in phrase-based SMT, character alignment in machine transliteration seems to be monotonic in which reordering of target language characters rarely occurs but is still possible in some language pairs. After alignment, the target language transliteration units can be considered as tags (or labels) of the source language transliteration units. As a result, some previous studies viewed machine transliteration as simply as a sequence labeling problem (Aramaki and Abekawwa, 2009; Shishtla et al., 2009). With this pro"
W11-3204,W10-2413,0,0.0209205,"tlamvanich National Electronics and Computer Technology Center Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand {canasai.kruengkrai,thatsanee.charoenporn,virach.sornlertlamvanich}@nectec.or.th Abstract transliteration units are not given in the training corpus. The process of machine transliteration is very similar to that of phrase-based statistical machine translation (SMT) (Koehn et al., 2003). As a result, a number of previous studies directly applied phrase-based SMT techniques to machine transliteration (Finch and Sumita, 2009; Rama and Gali, 2009; Finch and Sumita, 2010; Avinesh and Parikh, 2010). However, unlike word alignment in phrase-based SMT, character alignment in machine transliteration seems to be monotonic in which reordering of target language characters rarely occurs but is still possible in some language pairs. After alignment, the target language transliteration units can be considered as tags (or labels) of the source language transliteration units. As a result, some previous studies viewed machine transliteration as simply as a sequence labeling problem (Aramaki and Abekawwa, 2009; Shishtla et al., 2009). With this problem setting, the system can apply any powerful dis"
W11-3204,W09-3507,0,0.224427,"Missing"
W11-3204,W11-3202,0,0.0400248,"Missing"
W11-3204,W09-3510,0,0.01758,"sliteration Canasai Kruengkrai, Thatsanee Charoenporn, Virach Sornlertlamvanich National Electronics and Computer Technology Center Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand {canasai.kruengkrai,thatsanee.charoenporn,virach.sornlertlamvanich}@nectec.or.th Abstract transliteration units are not given in the training corpus. The process of machine transliteration is very similar to that of phrase-based statistical machine translation (SMT) (Koehn et al., 2003). As a result, a number of previous studies directly applied phrase-based SMT techniques to machine transliteration (Finch and Sumita, 2009; Rama and Gali, 2009; Finch and Sumita, 2010; Avinesh and Parikh, 2010). However, unlike word alignment in phrase-based SMT, character alignment in machine transliteration seems to be monotonic in which reordering of target language characters rarely occurs but is still possible in some language pairs. After alignment, the target language transliteration units can be considered as tags (or labels) of the source language transliteration units. As a result, some previous studies viewed machine transliteration as simply as a sequence labeling problem (Aramaki and Abekawwa, 2009; Shishtla et al.,"
W11-3303,W05-0909,0,0.0524453,"constraints 21 5 {QoSF ailure ,QoSU navl ,QoSBand ,QoSSat , QoSSec }. The workflow consists of n process, each processes there is set or service with size mi ; 1 ≤ i ≤ n that can fulfill process requirements. Then we introduce set of variable xij to represent decision variable.  There are many related studies about quality of machine translation notably ones include Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and SkipBigram Statistics (Lin and Och, 2004) and METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments (Banerjee and Lavie, 2005). There are researches about quality of service in service composition process such as QoS-Aware Middleware for Web Services Composition - A Qualitative Approach (Yeom, Yun and Min, 2006). There are two important processes that we do not focus in our framework. The first process is discovery process which are managed by Discovery Agent in our framework. Algorithms for discovery services are not included in this paper. There are many related studies in searching non perfect match web service such as Automate Composition and Reliable Execution of Ad-hoc Processes (Binder, Constatinescu, Faltings"
W11-3303,P04-1077,0,\N,Missing
W12-5002,P11-1054,0,0.072423,"A2) Each record has only one subject of relations. The assumption (A1) seems to hold for most of records. We adopt the assumption (A2) from (Hoffmann et al., 2010) that try to extract infobox-like relations from Wikipedia. Also, the assumption (A2) seems to hold for our data since the description provides the details about one cultural object whose name is expressed in the record title. Based on the above two assumptions, we propose our strategy to semi-automatically extract relation instances from the cultural database. We focus on unary relation extraction similar to (Hoffmann et al., 2010; Chen et al., 2011). We assume that the subject of the relation is the record title.2 Each relation remains only one argument to be extracted. We describe our relation templates (Section 2.1) and how to effectively find relation texts in a large database (Section 2.2). We use named entities to reduce semantic drift of the target arguments (Section 2.3). We examine the effect of the distances between the relation surfaces and the target arguments (Section 3.1) and provide preliminary results of our experiments (Section 3.2). The results indicate that our strategy of semantic relation extraction is very promising"
W12-5002,W08-1301,0,0.0412433,"Missing"
W12-5002,P04-1053,0,0.020445,"d by our system. extracted if its argument exactly matches the fact. For example, if our system only extracts the first name while the fact is the whole name, then we consider this instance to be incorrect. Finally, we set the maximum distance between the relation surface and its argument to 5. Table 4 shows the performance of our relation extraction. The overall results are surprisingly good, except those of HAS O LD NAME and IS M ADE B Y . Table 5 shows some samples of relation instances produced by our system. 4 Related work Named entity recognition has been applied to relation extraction. Hasegawa et al. (2004) propose an approach that discovers relations between two named entity types. Their approach clusters pairs of named entities using the similarity of context words intervening between them and assigns labels using frequent context words. In the Thai writing style, sentence boundary markers are absent, and subjects are often omitted. These two issues make it difficult to obtain two named entities in the same sentence. Our approach only considers one named entity and its preceding context words and uses simple templates to determine relation types. Relation extraction can be simplified by focusi"
W12-5002,P10-1030,0,0.145951,"ion instances from a large-scale Web corpus with minimal supervision. It only requires a lightweight noun phrase chunker to identify relation arguments. More advanced approaches like SNE (Kok and Domingos, 2008), R ESOLVER (Yates and Etzioni, 2009) and S HERLOCK (Schoenmackers et al., 2010) exploit the outputs of T EXT RUNNER for learning. Our cultural database allows us to make two assumptions: (A1) Each record belongs to only one main cultural domain. (A2) Each record has only one subject of relations. The assumption (A1) seems to hold for most of records. We adopt the assumption (A2) from (Hoffmann et al., 2010) that try to extract infobox-like relations from Wikipedia. Also, the assumption (A2) seems to hold for our data since the description provides the details about one cultural object whose name is expressed in the record title. Based on the above two assumptions, we propose our strategy to semi-automatically extract relation instances from the cultural database. We focus on unary relation extraction similar to (Hoffmann et al., 2010; Chen et al., 2011). We assume that the subject of the relation is the record title.2 Each relation remains only one argument to be extracted. We describe our relat"
W12-5002,P06-1015,0,0.0336131,"ำ) from the first text segment: เจดียก ลางนำา ตังอยูท ีตำาบลปากนำา (The Mid-River Pagoda is located at Tambon Paknam) Recent research in semantic relation extraction has shown the possibility to automatically find such relation instances. Some approaches rely on high-quality syntactic parsers. For example, DIRT (Lin and Pantel, 2001) and USP (Poon and Domingos, 2009) discover relation instances based on the outputs from dependency parsers. Such parsers and annotated training corpora are difficult to obtain in non-English languages. Pattern-based approaches (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Banko et al., 2007) seem to be more practical for languages with limited NLP resources. For example, T EXT RUNNER (Banko et al., 2007) can efficiently extract relation instances from a large-scale Web corpus with minimal supervision. It only requires a lightweight noun phrase chunker to identify relation arguments. More advanced approaches like SNE (Kok and Domingos, 2008), R ESOLVER (Yates and Etzioni, 2009) and S HERLOCK (Schoenmackers et al., 2010) exploit the outputs of T EXT RUNNER for learning. Our cultural database allows us to make two assumptions: (A1) Each record belongs to only on"
W12-5002,D09-1001,0,0.0197408,"rmal meaning representations of corresponding texts. These relation instances are useful for question answering and other applications. Using this record as an example, we could extract a relation instance IS L OCATEDAT(เจดียก ลางนำา , ตำำบลปำกนำำ) from the first text segment: เจดียก ลางนำา ตังอยูท ีตำาบลปากนำา (The Mid-River Pagoda is located at Tambon Paknam) Recent research in semantic relation extraction has shown the possibility to automatically find such relation instances. Some approaches rely on high-quality syntactic parsers. For example, DIRT (Lin and Pantel, 2001) and USP (Poon and Domingos, 2009) discover relation instances based on the outputs from dependency parsers. Such parsers and annotated training corpora are difficult to obtain in non-English languages. Pattern-based approaches (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Banko et al., 2007) seem to be more practical for languages with limited NLP resources. For example, T EXT RUNNER (Banko et al., 2007) can efficiently extract relation instances from a large-scale Web corpus with minimal supervision. It only requires a lightweight noun phrase chunker to identify relation arguments. More advanced approaches li"
W12-5002,D10-1106,0,0.0140379,"ers and annotated training corpora are difficult to obtain in non-English languages. Pattern-based approaches (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Banko et al., 2007) seem to be more practical for languages with limited NLP resources. For example, T EXT RUNNER (Banko et al., 2007) can efficiently extract relation instances from a large-scale Web corpus with minimal supervision. It only requires a lightweight noun phrase chunker to identify relation arguments. More advanced approaches like SNE (Kok and Domingos, 2008), R ESOLVER (Yates and Etzioni, 2009) and S HERLOCK (Schoenmackers et al., 2010) exploit the outputs of T EXT RUNNER for learning. Our cultural database allows us to make two assumptions: (A1) Each record belongs to only one main cultural domain. (A2) Each record has only one subject of relations. The assumption (A1) seems to hold for most of records. We adopt the assumption (A2) from (Hoffmann et al., 2010) that try to extract infobox-like relations from Wikipedia. Also, the assumption (A2) seems to hold for our data since the description provides the details about one cultural object whose name is expressed in the record title. Based on the above two assumptions, we pro"
W16-5205,W14-2105,0,0.0483701,"Missing"
W16-5205,P13-1045,0,0.0337482,"Missing"
W16-5205,D13-1170,0,0.00531354,"Missing"
W16-5205,C14-1220,0,0.0944968,"Missing"
W16-5205,W14-4012,0,0.0675041,"Missing"
W16-5205,D14-1080,0,0.0468757,"Missing"
W16-5205,C14-1008,0,0.216839,"Missing"
