2020.coling-main.214,P18-1026,0,0.0548162,"Missing"
2020.coling-main.214,P16-1154,0,0.187409,"For each pair, we collect the corresponding knowledge from a large scale Chinese medical knowledge graph CMeKG1 . An example is shown in Figure 1. Then, we adapt a knowledge-aware neural generation model for this task, named MedWriter, which consists of three components: topic encoder, graph encoder and decoder. The topic encoder is used to acquire the representation of topic words, while the graph encoder exploits the specific information from the MKG. Therefore, the model combines the information of topic words with the medical knowledge. Afterwards, we use the decoder with copy mechanism (Gu et al., 2016) to generate medical text. Experimental results demonstrate incorporating knowledge graph into generation model can improve the quality of the generated text and has robust superiority over the competitor methods. 2 Method Given a set of topic words K = {w1 , w2 , ..., ws }, and a knowledge graph represented as a set of triples, i.e., G = {g1 , g2 , g3 , ...}, where each triple gi is comprised of &lt; si , pi , oi &gt; denoting subject, predicate and object respectively, our goal is to generate a natural language text Y = {y1 , y2 , y3 , ...}, which is required to be relevant to the topic, grammatic"
2020.coling-main.214,N19-1238,0,0.0150421,"nections, where x is the total number of the vertices contained in V . If vj is a neighbour of vi , then E(vi , vj ) = E(vj , vi ) = 1, otherwise 0. The graph encoder is composed of a stack of several identical layers similar to (Vaswani et al., 2017), each of which has a multi-head attention sub-layer followed by a feed-forward network sub-layer. Each sub-layer is equipped with a residual connections (He et al., 2016) and a layer normalization (Ba et al., 2016). With the same operation in 2.1, the vertices are converted to an embedding representation e(vi ). Following a similar procedure to (Koncel-Kedziorski et al., 2019), for each vertex vi , in order to obtain the contextual representation, we adopt multi-head attention mechanism to attend over the other vertices adjacent to vi in G0 . It linearly projects the inputs of attention several times with different parameters respectively. All the inputs of attention function come from V , and then the multi-head self-attention can be calculated as: M ulHeadAtt(V ) = [head1 ; head2 ; ...; headn ]W, rvt i = X αij √ W1t e(vj ), dh j∈N headt = {rvt 1 , rvt 2 , ..., rvt x−1 , rvt g }, exp(e(vj )T W2t e(vi ) t T k∈Ni exp(e(vk ) W2 e(vi )) αij = P i (2) (3) where Ni deno"
2020.coling-main.214,P17-1014,0,0.023293,"esponding knowledge graph derived from CMeKG. The statistics of the dataset are shown in Table 1. 3.2 Competitor Methods In order to validate the effectiveness of incorporating knowledge graph into generation model, we compare MedWriter with two competitor methods. The first method is an attention-based sequence-to-sequence model (Sutskever et al., 2014), which only use the topic words as input to generate text, named Seq2Seq. The second method is a variant of the Seq2Seq, which utilizes not only the topic words but also the linearized knowledge graph, named GraphSeq. Borrowing the idea from (Konstas et al., 2017), we flatten the knowledge graph to a linear sequence according to the entity order they appear in the text. Another sequence encoder is employed to encode it. When decoding, both of the two competitor methods are equipped with copy mechanism. 3.3 Settings The model is trained to minimize the negative log-likelihood of the training set with the SGD optimization. The learning rate is set to 0.15. The hidden size of GRU is set to 512. The stack of graph encoder has 6 identical layers. We employ 4 parallel attention layers to perform multi-head attention. The dimension of embedding layer and the"
2020.coling-main.214,W04-1013,0,0.0229457,"ng rate is set to 0.15. The hidden size of GRU is set to 512. The stack of graph encoder has 6 identical layers. We employ 4 parallel attention layers to perform multi-head attention. The dimension of embedding layer and the attention sub-layer are set to 512, while the intermediate dimension of linear sub-layer is set to 2048. The size of the vocabulary is truncated to 50,000. The batch size is set to 32. We train the model for 30 epochs and select the model which achieves the best performance on the validation set. 3.4 Metrics For evaluation, we adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) metrics. BLEU is an n-gram overlapping measure which is widely adopted in the text generation task. BLEU1, BLEU2 and BLEU3 are reported. ROUGE is also a common measure to automatically determine the quality of the generated text. We report the F1 score for ROUGE-L, which measures the longest common sequence (LCS) between the reference and the candidate. 2366 3.5 Results As shown in Table 2, the Seq2Seq method achieves the worst performance in terms of both BLEU and ROUGE since it only uses the topic keywords. The GraphSeq method outperforms the Seq2Seq because it uses the medical knowledge gr"
2020.coling-main.214,W19-1905,0,0.0606746,"Missing"
2020.coling-main.214,P02-1040,0,0.10783,"h the SGD optimization. The learning rate is set to 0.15. The hidden size of GRU is set to 512. The stack of graph encoder has 6 identical layers. We employ 4 parallel attention layers to perform multi-head attention. The dimension of embedding layer and the attention sub-layer are set to 512, while the intermediate dimension of linear sub-layer is set to 2048. The size of the vocabulary is truncated to 50,000. The batch size is set to 32. We train the model for 30 epochs and select the model which achieves the best performance on the validation set. 3.4 Metrics For evaluation, we adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) metrics. BLEU is an n-gram overlapping measure which is widely adopted in the text generation task. BLEU1, BLEU2 and BLEU3 are reported. ROUGE is also a common measure to automatically determine the quality of the generated text. We report the F1 score for ROUGE-L, which measures the longest common sequence (LCS) between the reference and the candidate. 2366 3.5 Results As shown in Table 2, the Seq2Seq method achieves the worst performance in terms of both BLEU and ROUGE since it only uses the topic keywords. The GraphSeq method outperforms the Seq2Seq because it uses th"
2020.findings-emnlp.164,P19-1350,0,0.05638,"Missing"
2020.findings-emnlp.164,H90-1021,0,0.378644,"Missing"
2020.findings-emnlp.164,P19-1185,0,0.0256193,"Lazebnik, 2017). In contrast, only a few attempts have been made to address catastrophic forgetting in natural language forgetting (NLP) field. Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2016) has been adapted to visual question answering (Greco et al., 2019) and language modeling (Wolf et al., 2019). Progressive Neural Network proposed in reinforcement learning (Rusu et al., 2016) has been adopted to semantic slot filling in (Shen et al., 2019). A continual learning architecture preventing catastrophic forgetting via block-sparsity and orthogonality constraints is presented in (Pasunuru and Bansal, 2019) on diverse sentence-pair classification tasks. 1817 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1817–1822 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Output Task 1 Sports Output hout(t) Task 2 Movie News Music Sports Movie News h1(t-1) Music h2(t-1) M1 h1(t) M2b M2c M2 h2(t) M3b M3c h3(t-1) Input, Task ID = 1 Input (a) (b) h3(t) M3 x(t) Figure 1: Deep neural networks a) with requirement on task IDs, and b) without requirement on task IDs, in inference stage. To our best knowledge, none of the previous works in NLP considers the int"
2020.findings-emnlp.164,D19-1126,0,0.0261496,"Missing"
C10-2173,P07-1056,0,0.811797,"supervised learning with an exponential loss function. Second, we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data. Then ADN architecture is trained by the selected labeled data and all unlabeled data. Experiments on five sentiment classification datasets show that ADN outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification. 1 Introduction In recent years, sentiment analysis has received considerable attentions in Natural Language Processing (NLP) community (Blitzer et al., 2007; Dasgupta and Ng, 2009; Pang et al., 2002). Polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis (Dasgupta and Ng, 2009). Sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic. Labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of su"
C10-2173,P09-1079,0,0.0644831,"ith an exponential loss function. Second, we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data. Then ADN architecture is trained by the selected labeled data and all unlabeled data. Experiments on five sentiment classification datasets show that ADN outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification. 1 Introduction In recent years, sentiment analysis has received considerable attentions in Natural Language Processing (NLP) community (Blitzer et al., 2007; Dasgupta and Ng, 2009; Pang et al., 2002). Polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis (Dasgupta and Ng, 2009). Sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic. Labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success (Gamon, 2004) and"
C10-2173,C04-1121,0,0.067638,"Missing"
C10-2173,W02-1011,0,0.0306851,"function. Second, we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data. Then ADN architecture is trained by the selected labeled data and all unlabeled data. Experiments on five sentiment classification datasets show that ADN outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification. 1 Introduction In recent years, sentiment analysis has received considerable attentions in Natural Language Processing (NLP) community (Blitzer et al., 2007; Dasgupta and Ng, 2009; Pang et al., 2002). Polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis (Dasgupta and Ng, 2009). Sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic. Labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success (Gamon, 2004) and is helpful in busin"
C10-2173,P09-1027,0,0.101683,"rning methods. The rest of the paper is organized as follows. Section 2 gives an overview of sentiment classification. The proposed semi-supervised learning method ADN is described in Section 3. Section 4 shows the empirical validation of ADN by comparing its classification performance with previous sentiment classifiers and deep learning methods on sentiment datasets. The paper is closed with conclusion. 2 Sentiment Classification Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). The detailed survey about techniques and approaches of sentiment classification can be seen in the book (Pang and Lee, 2008). In this paper we focus on corpus-based classification method. Corpus-based methods use a labeled corpus to train a sentiment classifier (Wan, 2009). Pang et al. (2002) apply machine learning approach to corpus-based sentiment classification firstly. They found that standard machine learning techniques outperform human-produced baselines. Pang and Lee (2004) apply text-categorization techniques to the subjective portions of the sentiment document. These portions are ex"
C10-2173,P08-2065,0,0.0400838,"labels such as light-hearted and heavy-hearted. Supervised sentiment classification systems are domain-specific and annotating a large scale corpus for each domain is very expensive (Dasgupta and Ng, 2009). There are several solutions for this corpus annotation bottleneck. The first type of solution is using old domain labeled examples to new domain sentiment clas1516 sification. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, which could be used to select a small set of domains to annotate and their trained classifiers would transfer well to many other domains. Li and Zong (2008) study multi-domain sentiment classification, which aims to improve performance through fusing training data from multiple domains. The second type of solution is semisupervised sentiment classification. Sindhwani and Melville (2008) propose a semi-supervised sentiment classification algorithm that utilizes lexical prior knowledge in conjunction with unlabeled data. Dasgupta and Ng (2009) firstly mine the unambiguous reviews using spectral techniques, and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning"
C10-2173,P09-1028,0,0.00909011,"beling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success (Gamon, 2004) and is helpful in business intelligence applications, recommender systems, and message filtering (Pang, et al., 2002). While topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task (Pang, et al., 2002). First, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain-specific contextual cues (Li et al., 2009). For example, although the sentence “The thief tries to protect his excellent reputation” contains the word “excellent”, it tells us nothing about the author’s opinion and in fact could be well embedded in a negative review. Second, sentiment classification systems are typically domain-specific, which makes the expensive process of annotating a large amount of data for each domain and is a bottleneck in building high quality systems (Dasgupta and Ng, 2009). This motivates the task of learning robust sentiment models from minimal supervision (Li, et al., 2009). Recently, semi-supervised learni"
C10-2173,P04-1035,0,0.00909921,"on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). The detailed survey about techniques and approaches of sentiment classification can be seen in the book (Pang and Lee, 2008). In this paper we focus on corpus-based classification method. Corpus-based methods use a labeled corpus to train a sentiment classifier (Wan, 2009). Pang et al. (2002) apply machine learning approach to corpus-based sentiment classification firstly. They found that standard machine learning techniques outperform human-produced baselines. Pang and Lee (2004) apply text-categorization techniques to the subjective portions of the sentiment document. These portions are extracted by efficient techniques for finding minimum cuts in graphs. Gamon (2004) demonstrate that using large feature vectors in combination with feature reduction, high accuracy can be achieved in the very noisy domain of customer feedback data. Xia et al. (2008) propose the sentiment vector space model to represent song lyric document, assign the sentiment labels such as light-hearted and heavy-hearted. Supervised sentiment classification systems are domain-specific and annotating"
C10-2173,P08-2034,0,0.0756788,"iment classifier (Wan, 2009). Pang et al. (2002) apply machine learning approach to corpus-based sentiment classification firstly. They found that standard machine learning techniques outperform human-produced baselines. Pang and Lee (2004) apply text-categorization techniques to the subjective portions of the sentiment document. These portions are extracted by efficient techniques for finding minimum cuts in graphs. Gamon (2004) demonstrate that using large feature vectors in combination with feature reduction, high accuracy can be achieved in the very noisy domain of customer feedback data. Xia et al. (2008) propose the sentiment vector space model to represent song lyric document, assign the sentiment labels such as light-hearted and heavy-hearted. Supervised sentiment classification systems are domain-specific and annotating a large scale corpus for each domain is very expensive (Dasgupta and Ng, 2009). There are several solutions for this corpus annotation bottleneck. The first type of solution is using old domain labeled examples to new domain sentiment clas1516 sification. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, which could be used to select a small set"
C10-2173,C08-1135,0,0.0617367,"formance through fusing training data from multiple domains. The second type of solution is semisupervised sentiment classification. Sindhwani and Melville (2008) propose a semi-supervised sentiment classification algorithm that utilizes lexical prior knowledge in conjunction with unlabeled data. Dasgupta and Ng (2009) firstly mine the unambiguous reviews using spectral techniques, and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning. The third type of solution is unsupervised sentiment classification. Zagibalov and Carroll (2008) describe an automatic seed word selection for unsupervised sentiment classification of product reviews in Chinese. However, unsupervised learning of sentiment is difficult, partially because of the prevalence of sentimentally ambiguous reviews (Dasgupta and Ng, 2009). Using multi-domain sentiment corpus to sentiment classification is also hard to apply, because each domain has a very limited amount of training data, due to annotating a large corpus is difficult and time-consuming (Li and Zong, 2008). So in this paper we focus on semi-supervised approach to sentiment classification. 3  x11 ,"
C12-3059,H05-1103,0,0.0321211,"work has been done on the definition and evaluation of automatic QG. Nielsen (2008) gives the definition of QG and considers this problem as a three-step process. A question taxonomy for QG is proposed in (Nielsen et al., 2008) with a detailed question branch offered. The overall description of the QG task is proposed in (Rus and Graesser, 2009; Rus et al., 2010). Rus et al. (2007) and Vanderwende (2008) have discussed the evaluation of the QG systems. The technique of question generation is essential to some education related fields, such as educational assessment, intelligent tutoring, etc. Brown et al. (2005) have described an approach to automatically generating questions for vocabulary assessment. Hoshino and Nakagawa (2005) have developed a real-time system which generates questions on English grammar and vocabulary. A template based QG method is proposed in (Wang et al., 2008) to evaluate the learners’ understanding after reading a medical material. In conclusion, the purpose of such QG systems is different from our goal in this paper. It should be noted that the nature of automatic question generation is different depending on the application within which it is embedded (Nielsen, 2008). 3 Gen"
C12-3059,P05-1026,0,0.0347775,"he web community oriented question generation. KEYWORDS: statistical question generation, deep belief network, web community. Proceedings of COLING 2012: Demonstration Papers, pages 467–474, COLING 2012, Mumbai, December 2012. 467 1 Introduction Automatic question generation (QG) is a challenging task in the NLP field, and its difficulties are being realized by the researchers gradually. Since 2008, the workshop on QG1 has been offering the shared task and evaluation on this problem. At present, the QG technique tends to be mainly applied in the interaction oriented systems (Rus et al., 2007; Harabagiu et al., 2005) (e.g., computer aided education, help desk, dialog systems, etc.). In most systems, the original source texts are parsed and transformed into the questions with the rules. The parser and rule based methods always maintain considerable accuracy, and the generating results can be directly presented to the users. In this paper, we aim to address the web-community oriented question generation in a statistical learning way. A deep belief network (DBN) is proposed to generate the essential elements of the questions according to the answers, based on the joint distributions of the questions and thei"
C12-3059,W05-0203,0,0.0343597,"nd considers this problem as a three-step process. A question taxonomy for QG is proposed in (Nielsen et al., 2008) with a detailed question branch offered. The overall description of the QG task is proposed in (Rus and Graesser, 2009; Rus et al., 2010). Rus et al. (2007) and Vanderwende (2008) have discussed the evaluation of the QG systems. The technique of question generation is essential to some education related fields, such as educational assessment, intelligent tutoring, etc. Brown et al. (2005) have described an approach to automatically generating questions for vocabulary assessment. Hoshino and Nakagawa (2005) have developed a real-time system which generates questions on English grammar and vocabulary. A template based QG method is proposed in (Wang et al., 2008) to evaluate the learners’ understanding after reading a medical material. In conclusion, the purpose of such QG systems is different from our goal in this paper. It should be noted that the nature of automatic question generation is different depending on the application within which it is embedded (Nielsen, 2008). 3 Generating Questions using the Deep Belief Network Nielsen (2008) defines the question generation task as a 3-step process:"
C12-3059,W10-4234,0,0.0309898,"onclusions and future directions are drawn. 2 Related Work To our knowledge, there is no previous work that concentrates on statistically generating questions from the web content freely posted by users, as we do in this paper. Nevertheless, the basic work has been done on the definition and evaluation of automatic QG. Nielsen (2008) gives the definition of QG and considers this problem as a three-step process. A question taxonomy for QG is proposed in (Nielsen et al., 2008) with a detailed question branch offered. The overall description of the QG task is proposed in (Rus and Graesser, 2009; Rus et al., 2010). Rus et al. (2007) and Vanderwende (2008) have discussed the evaluation of the QG systems. The technique of question generation is essential to some education related fields, such as educational assessment, intelligent tutoring, etc. Brown et al. (2005) have described an approach to automatically generating questions for vocabulary assessment. Hoshino and Nakagawa (2005) have developed a real-time system which generates questions on English grammar and vocabulary. A template based QG method is proposed in (Wang et al., 2008) to evaluate the learners’ understanding after reading a medical mate"
C14-1095,A88-1019,0,0.205343,"Missing"
C14-1095,W00-0729,0,0.0450456,"August 23-29 2014. In this paper, our work focuses on identifying noun phrases, adjective phrase and verb phrases, which are the most difficult aspects of Kazakh phrase recognition analysis. This is achieved by using rules are ME method. 2 Related work There are a variety of techniques used for phrase recognition, which include rule-based technique, statistical technique, and a combination of them. Church&apos;s (1988) approach used manual or semiautomatic annotation phrase corpus as a training corpus. Another popular method is to use a Chunk parsing for statistics model to determine the boundary (Koeling, 2000). Chunk parsing was first introduced by Abney (1991), which is one of the most widely used syntactic parsing methods. The main idea of chunk parsing lies in seeking the appropriate breakthrough point, and decomposing the full parsing problems into a syntax topology statistical structure and syntactic relations. Zhao and Huang (1998) are pioneers in Chinese phrase studies; Tsinghua University had also completed its TCT (Tsinghua Chinese Treebank) for Chinese (Zhou, 2004). The method has been also applied into studies of other languages, such as Kazakh Base NP recognition (Altenbek et al, 2009),"
C14-1095,J96-1002,0,0.0498339,"s. The main idea of chunk parsing lies in seeking the appropriate breakthrough point, and decomposing the full parsing problems into a syntax topology statistical structure and syntactic relations. Zhao and Huang (1998) are pioneers in Chinese phrase studies; Tsinghua University had also completed its TCT (Tsinghua Chinese Treebank) for Chinese (Zhou, 2004). The method has been also applied into studies of other languages, such as Kazakh Base NP recognition (Altenbek et al, 2009), and Uyghur Base VP Recognition by CRF (Mamatmin et al, 2012). Maximum Entropy was first introduced to NLP area by Berger et al (1996) and Della Pietra et al. (1997). Maximum Entropy is an extremely flexible technique for linguistic modelling. It can use a virtually unrestricted and rich feature set in the framework of a probability model. It is a conditional, discriminative model and allows mutually dependent variables (Ratnaparkhi, 1999). 3 3.1 Kazakh Phase Parsing Kazakh Morphology Morphological analysis is an important task in natural language processing research. It was developed for different languages, included English (Porter, 1980), Finnish (Karttunen, 1983), Turkish (Oflazer, 1994; Gülşen, 2004), and Arabic (Beesle"
C14-1095,C96-1017,0,0.18446,"(1996) and Della Pietra et al. (1997). Maximum Entropy is an extremely flexible technique for linguistic modelling. It can use a virtually unrestricted and rich feature set in the framework of a probability model. It is a conditional, discriminative model and allows mutually dependent variables (Ratnaparkhi, 1999). 3 3.1 Kazakh Phase Parsing Kazakh Morphology Morphological analysis is an important task in natural language processing research. It was developed for different languages, included English (Porter, 1980), Finnish (Karttunen, 1983), Turkish (Oflazer, 1994; Gülşen, 2004), and Arabic (Beesley, 1996). Comparing with other languages, the Kazakh morphological system uses a large number of suffixes and a small number of prefixes. Every word has a root, or a stem (Milat, 2003;Zhang 2004). The basic Kazakh phrase is an adjacent and non-nested phrase which does not contain recursive structure. 3.2 The Categories of Kazakh Phrase Parsing is one of the most basic and fundamental components in natural language processing. Chunk parsing intends to obtain a fragment without thinking deeply. A Kazakh phrase is composed of two or more than two words which connected with meaning and grammatical structu"
C14-1095,A88-1000,0,\N,Missing
C14-1127,P07-1056,0,0.0758491,"n. All the reviews in the dataset are used to train the HDBN with unsupervised learning. After training, we can determine the label of the new data through: arg max hN (x) j 3 3.1 (14) Experiments Experimental setup We evaluate the performance of the proposed HDBN method using five sentiment classification datasets. The first dataset is MOV (Pang et al., 2002), which is a classical movie review dataset. The other four datasets contain products reviews come from the multi-domain sentiment classification corpus, including books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT) (Blitzer et al., 2007). Each dataset contains 1,000 positive and 1,000 negative reviews. The experimental setup is same as (Zhou et al., 2010). We divide the 2,000 reviews into ten equalsized folds randomly, maintaining balanced class distributions in each fold. Half of the reviews in each fold are random selected as training data and the remaining reviews are used for test. Only the reviews in the training data set are used for the selection of labeled reviews by active learning. All the algorithms are tested with cross-validation. We compare the classification performance of HDBN with four representative semi-sup"
C14-1127,P09-1079,0,0.0871649,"uming (Chapelle et al., 2006). On the other hand, it is much easier to obtain a large number of unlabeled reviews, such as the growing availability and popularity of online review sites and personal blogs (Pang and Lee, 2008). In recent years, a new approach called semi-supervised learning, which uses large amount of unlabeled data together with labeled data to build better learners (Zhu, 2007), has been developed in the machine learning community. There are several works have been done in semi-supervised learning for sentiment classification, and get competitive performance (Li et al., 2010; Dasgupta and Ng, 2009; Zhou et al., 2010). However, most of the existing semi-supervised learning methods are still far from satisfactory. As shown by several researchers (Salakhutdinov and Hinton, 2007; Hinton et al., 2006), deep architecture, which composed of multiple levels of non-linear operations, is expected to perform well in semi-supervised learning because of its capability of modeling hard artificial intelligent tasks. Deep belief networks (DBN) is a representative deep learning algorithm achieving notable success for text classification, which is a directed belief nets with many hidden layers construct"
C14-1127,P10-1043,0,0.176103,"ion on products and services (Liu et al., 2010). These reviews will not only help other users make better judgements but they are also useful resources for manufacturers of products to keep track and manage customer opinions (Wei and Gulla, 2010). However, there are large amount of reviews for every topic, it is difficult for a user to manually learn the opinions of an interesting topic. Sentiment classification, which aims to classify a text according to the expressed sentimental polarities of opinions such as ’positive’ or ’negtive’, ’thumb up’ or ’thumb down’, ’favorable’ or ’unfavorable’ (Li et al., 2010), can facilitate the investigation of corresponding products or services. In order to learn a good text classifier, a large number of labeled reviews are often needed for training (Zhen and Yeung, 2010). However, labeling reviews is often difficult, expensive or time consuming (Chapelle et al., 2006). On the other hand, it is much easier to obtain a large number of unlabeled reviews, such as the growing availability and popularity of online review sites and personal blogs (Pang and Lee, 2008). In recent years, a new approach called semi-supervised learning, which uses large amount of unlabeled"
C14-1127,W02-1011,0,0.0172741,"activities are replaced by deterministic, real valued probabilities. 2.5 Classification using HDBN The training procedure of HDBN is given in Algorithm 1. For the training of HDBN architecture, the parameters are random initialized with normal distribution. All the reviews in the dataset are used to train the HDBN with unsupervised learning. After training, we can determine the label of the new data through: arg max hN (x) j 3 3.1 (14) Experiments Experimental setup We evaluate the performance of the proposed HDBN method using five sentiment classification datasets. The first dataset is MOV (Pang et al., 2002), which is a classical movie review dataset. The other four datasets contain products reviews come from the multi-domain sentiment classification corpus, including books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT) (Blitzer et al., 2007). Each dataset contains 1,000 positive and 1,000 negative reviews. The experimental setup is same as (Zhou et al., 2010). We divide the 2,000 reviews into ten equalsized folds randomly, maintaining balanced class distributions in each fold. Half of the reviews in each fold are random selected as training data and the remaining reviews are"
C14-1127,P10-1042,0,0.0225114,"nt classification datasets, and show that HDBN is competitive with previous semi-supervised learning algorithm. Experiments are also conducted to verify the effectiveness of our proposed method with different number of unlabeled reviews. 1 Introduction Recently, more and more people write reviews and share opinions on the World Wide Web, which present a wealth of information on products and services (Liu et al., 2010). These reviews will not only help other users make better judgements but they are also useful resources for manufacturers of products to keep track and manage customer opinions (Wei and Gulla, 2010). However, there are large amount of reviews for every topic, it is difficult for a user to manually learn the opinions of an interesting topic. Sentiment classification, which aims to classify a text according to the expressed sentimental polarities of opinions such as ’positive’ or ’negtive’, ’thumb up’ or ’thumb down’, ’favorable’ or ’unfavorable’ (Li et al., 2010), can facilitate the investigation of corresponding products or services. In order to learn a good text classifier, a large number of labeled reviews are often needed for training (Zhen and Yeung, 2010). However, labeling reviews"
C14-1127,C10-2173,1,0.696819,"2006). On the other hand, it is much easier to obtain a large number of unlabeled reviews, such as the growing availability and popularity of online review sites and personal blogs (Pang and Lee, 2008). In recent years, a new approach called semi-supervised learning, which uses large amount of unlabeled data together with labeled data to build better learners (Zhu, 2007), has been developed in the machine learning community. There are several works have been done in semi-supervised learning for sentiment classification, and get competitive performance (Li et al., 2010; Dasgupta and Ng, 2009; Zhou et al., 2010). However, most of the existing semi-supervised learning methods are still far from satisfactory. As shown by several researchers (Salakhutdinov and Hinton, 2007; Hinton et al., 2006), deep architecture, which composed of multiple levels of non-linear operations, is expected to perform well in semi-supervised learning because of its capability of modeling hard artificial intelligent tasks. Deep belief networks (DBN) is a representative deep learning algorithm achieving notable success for text classification, which is a directed belief nets with many hidden layers constructed by restricted Bol"
C14-1127,W04-3253,0,\N,Missing
C14-1127,C10-2036,0,\N,Missing
C14-1127,C08-1135,0,\N,Missing
C14-1127,P08-2065,0,\N,Missing
C14-1127,P04-1035,0,\N,Missing
C14-1127,P06-2079,0,\N,Missing
C14-1127,P08-2034,0,\N,Missing
C14-1127,P02-1053,0,\N,Missing
C14-1127,P09-1027,0,\N,Missing
C14-1127,W06-3808,0,\N,Missing
C14-1127,P07-1055,0,\N,Missing
C14-1127,C10-1072,0,\N,Missing
C16-1117,S15-2035,1,0.883208,"e. The proposed models are detailedly described in Section 3. Experiments are arranged in Section 4. Discussion and conclusion are at last. 2 2.1 Related Work Answer Quality Tagging In the literature, methods for answer quality tagging in cQA can be roughly divided into the following four groups: sparse feature-based methods, translation models, parsing trees, and deep CNNs. Sparse feature-based methods are the mostly widely applied and have the longest research duration. Early studies such as Agichtein et al. (2008) and Suryanoto et al. (2009), and later studies such as Yih et al. (2013) and Hou et al. (2015) all achieved not bad results using simple classifiers with manually constructed sparse features. However, feature engineering is time consuming and has low extensibility to other domains. Translation models relied on large-scale of training pairs and can effectively bridge the semantic gap in many cases (Berger et al., 2000; Riezler et al., 2007; Surdeanu et al., 2008). One difficulty is that large parallel training dataset is hard to obtain. The similarity computed from parsing trees is a direct criterion to measure the semantic correlation between two sentences. Typical works are tree edit"
C16-1117,P13-2146,1,0.797311,"accept it (Agichtein et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 The first two authors have equal contributions. Corresponding author 3 https://answers.yahoo.com/question/index?qid=20070918225025AA5Jz0G 2 1231 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1231–1241, Osaka, Japan, December 11-17 2016. 2008). The inefficiency of textual similarity can be partly complemented with the help of these specific features (Hu et al., 2013; Tran et al., 2015). But the definition and validation of them need too much laborious cost and might not be appropriate when transferring to newly built systems. In this paper, we focus on a novel concept of contextual feature, the label dependency, which we think plays a pivotal role when predicting the answer quality in cQA. Intuitively, being a rational answerer, when we decide to answer a question, we face the following situations:  If it is a new question (without an answer), just answer it.  If it has been answered but all the existed answers are bad, we provide our good answer.  If"
C16-1117,D15-1068,0,0.520816,"ssified into sparse feature-based methods since they mainly incorporated sparse encodings for multiple features (Wang et al., 2010). The issue of discontinuous word features are later tackled by CNN (Kim, 2014; Hu et al., 2014), following some applications for answer selection in cQA (Yu et al., 2014; Qiu and Huang, 2015; Shen et al., 2015). Zhou et al. (2015)’s work modelled the content correlations using LSTM along QA sequences but they ignored the constraints among quality tags. Their method is insufficient in context learning also due to the neglect of sequence-level dependency modelling. Joty et al. (2015) proposed a graph-cut approach on the judgement of good or bad answers and gains improvement from the baselines. Joty et al. (2016) is an improved version of Joty et al. (2015) in which the authors introduced jointly learning approaches to capture global dependency. The above two works validated the importance of label dependency but they divided the tagging task into two subtasks and built their models in a distinct way. 2.2 Combining Deep Neural Networks and Graphical Models Wöllmer et al. (2011) was one of the earliest studies that combine neural networks and graphical models. They appended"
C16-1117,N16-1084,0,0.0962822,". The issue of discontinuous word features are later tackled by CNN (Kim, 2014; Hu et al., 2014), following some applications for answer selection in cQA (Yu et al., 2014; Qiu and Huang, 2015; Shen et al., 2015). Zhou et al. (2015)’s work modelled the content correlations using LSTM along QA sequences but they ignored the constraints among quality tags. Their method is insufficient in context learning also due to the neglect of sequence-level dependency modelling. Joty et al. (2015) proposed a graph-cut approach on the judgement of good or bad answers and gains improvement from the baselines. Joty et al. (2016) is an improved version of Joty et al. (2015) in which the authors introduced jointly learning approaches to capture global dependency. The above two works validated the importance of label dependency but they divided the tagging task into two subtasks and built their models in a distinct way. 2.2 Combining Deep Neural Networks and Graphical Models Wöllmer et al. (2011) was one of the earliest studies that combine neural networks and graphical models. They appended an LSTM layer on top of a Hidden Markov Model in automatic speech recognition. However, the LSTM they applied was only a shallow a"
C16-1117,D14-1181,0,0.154467,"l (ARC-I) is a stacked ensemble of the above networks, which can be seen as a combination of RCNN (Zhou et al., 2015) and LSTM-CRF (Huang et al., 2015). In ARC-I, LSTM is applied on the sequence of encoded QA matching pairs, with CRF on the final layer to memorize transition probabilities over the tag sequence. The main improvement from Zhou et al. (2015) is the addition of backward LSTM and CRF. And the difference from Huang et al. (2015) is that we adopt the LSTM-CRF model in comment-level (actually sentence-level within this paper) sequence tagging, with the help of CNN sentence modelling (Kim, 2014). ARC-II is a novel and much simpler model, with the integration of the attention mechanism. In ARCII, the question and its answers are linearly connected in a sequence and encoded by CNN. An attentionbased LSTM is then applied on the encoded sequence. Through attention, the model learns how much the question and the context affect the predicting of the current answer. And similar to ARC-I, a CRF layer is appended at last. Using a simple attention function (described in Section 3), ARC-II reduces the size of the parameter space and trains faster than ARC-I. We carried out extensive experiments"
C16-1117,N16-1030,0,0.0224457,"n a distinct way. 2.2 Combining Deep Neural Networks and Graphical Models Wöllmer et al. (2011) was one of the earliest studies that combine neural networks and graphical models. They appended an LSTM layer on top of a Hidden Markov Model in automatic speech recognition. However, the LSTM they applied was only a shallow architecture. Only in recent two or three years did some researchers begin to explore the combination of deep neural networks and graphical models. Huang et al. (2015) proposed the LSTM-CRF model for POS, chunking and NER, and produced stateof-the-art (or close to) accuracies. Lample et al. (2016) applied character and word embeddings in LSTM-CRF and generated good results on NER for four languages. Ma et al. (2016) added a CNN layer on word and character embedding and outperformed previous works in POS tagging and NER. As far as we know, all the related studies were settled on word-level tasks. In most cases, sentencelevel sequence labelling is quite distinct from the word-level in that the dependencies between adjacent sentences are not hard. So that the constraints for tags are weak than word-level tasks, demanding additional information to reinforce the constraints (i.e. sentence m"
C16-1117,P16-1101,0,0.117755,"Missing"
C16-1117,P07-1059,0,0.0427903,"s, parsing trees, and deep CNNs. Sparse feature-based methods are the mostly widely applied and have the longest research duration. Early studies such as Agichtein et al. (2008) and Suryanoto et al. (2009), and later studies such as Yih et al. (2013) and Hou et al. (2015) all achieved not bad results using simple classifiers with manually constructed sparse features. However, feature engineering is time consuming and has low extensibility to other domains. Translation models relied on large-scale of training pairs and can effectively bridge the semantic gap in many cases (Berger et al., 2000; Riezler et al., 2007; Surdeanu et al., 2008). One difficulty is that large parallel training dataset is hard to obtain. The similarity computed from parsing trees is a direct criterion to measure the semantic correlation between two sentences. Typical works are tree edit distance (Punyakanok et al., 2004; Yao et al., 2013) and convolutional tree kernels (Severyn and Moschitti, 2013). But one of the drawbacks of parsing tree-based methods is that most existed parsers perform badly in low-quality sentences (i.e. spoken style language in cQA or daily dialogues). In the research field of deep learning, Deep Belief Ne"
C16-1117,D13-1044,0,0.0200904,"se features. However, feature engineering is time consuming and has low extensibility to other domains. Translation models relied on large-scale of training pairs and can effectively bridge the semantic gap in many cases (Berger et al., 2000; Riezler et al., 2007; Surdeanu et al., 2008). One difficulty is that large parallel training dataset is hard to obtain. The similarity computed from parsing trees is a direct criterion to measure the semantic correlation between two sentences. Typical works are tree edit distance (Punyakanok et al., 2004; Yao et al., 2013) and convolutional tree kernels (Severyn and Moschitti, 2013). But one of the drawbacks of parsing tree-based methods is that most existed parsers perform badly in low-quality sentences (i.e. spoken style language in cQA or daily dialogues). In the research field of deep learning, Deep Belief Networks (DBN) can also be classified into sparse feature-based methods since they mainly incorporated sparse encodings for multiple features (Wang et al., 2010). The issue of discontinuous word features are later tackled by CNN (Kim, 2014; Hu et al., 2014), following some applications for answer selection in cQA (Yu et al., 2014; Qiu and Huang, 2015; Shen et al.,"
C16-1117,N03-1028,0,0.047964,") j 1 and the weight αij is computed by  ij   n 1 exp(eik ) k 1 where eij  siT  h j (11) In ARC-II, we degenerate Eq. 11 into simply computing the similarity between sentences, that is eij  hiT  h j . Thus, the attention context for an answer derives from the similarity with the question as well as the associations with other answers. The correlation with previous answers reflect the information of dependency while the correlation with later answers perhaps convey the information of comments. 3.6 CRF CRF has been shown superior in many sequence labelling tasks (Lafferty et al., 2001;Sha and Pereira, 2003; Quattoni et al.,2004). Given an observation sequence X={x1, x2, …, xn}, CRF jointly models the probability of the entire sequence of labels Y={ y1, y2, …, yn } by using the discriminative probability to yi given xi and the transition probability between adjacent labels. The original probability model of CRF is written as: n p (Y |X ;W , b)   ( y i i 1 i 1 , yi , X ) (12) n   ( y y ' ( X ) i 1 i ' i 1 , yi' , X ) where  i ( y ', y, xi )  exp(WyT', y xi  by ', y ) are potential functions, and ( X ) denotes the set of possible label sequences given X. In this work, the observed v"
C16-1117,P08-1082,0,0.0337744,"deep CNNs. Sparse feature-based methods are the mostly widely applied and have the longest research duration. Early studies such as Agichtein et al. (2008) and Suryanoto et al. (2009), and later studies such as Yih et al. (2013) and Hou et al. (2015) all achieved not bad results using simple classifiers with manually constructed sparse features. However, feature engineering is time consuming and has low extensibility to other domains. Translation models relied on large-scale of training pairs and can effectively bridge the semantic gap in many cases (Berger et al., 2000; Riezler et al., 2007; Surdeanu et al., 2008). One difficulty is that large parallel training dataset is hard to obtain. The similarity computed from parsing trees is a direct criterion to measure the semantic correlation between two sentences. Typical works are tree edit distance (Punyakanok et al., 2004; Yao et al., 2013) and convolutional tree kernels (Severyn and Moschitti, 2013). But one of the drawbacks of parsing tree-based methods is that most existed parsers perform badly in low-quality sentences (i.e. spoken style language in cQA or daily dialogues). In the research field of deep learning, Deep Belief Networks (DBN) can also be"
C16-1117,S15-2038,0,0.171921,"ein et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 The first two authors have equal contributions. Corresponding author 3 https://answers.yahoo.com/question/index?qid=20070918225025AA5Jz0G 2 1231 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1231–1241, Osaka, Japan, December 11-17 2016. 2008). The inefficiency of textual similarity can be partly complemented with the help of these specific features (Hu et al., 2013; Tran et al., 2015). But the definition and validation of them need too much laborious cost and might not be appropriate when transferring to newly built systems. In this paper, we focus on a novel concept of contextual feature, the label dependency, which we think plays a pivotal role when predicting the answer quality in cQA. Intuitively, being a rational answerer, when we decide to answer a question, we face the following situations:  If it is a new question (without an answer), just answer it.  If it has been answered but all the existed answers are bad, we provide our good answer.  If it has been answere"
C16-1117,P10-1125,1,0.844389,"s is a direct criterion to measure the semantic correlation between two sentences. Typical works are tree edit distance (Punyakanok et al., 2004; Yao et al., 2013) and convolutional tree kernels (Severyn and Moschitti, 2013). But one of the drawbacks of parsing tree-based methods is that most existed parsers perform badly in low-quality sentences (i.e. spoken style language in cQA or daily dialogues). In the research field of deep learning, Deep Belief Networks (DBN) can also be classified into sparse feature-based methods since they mainly incorporated sparse encodings for multiple features (Wang et al., 2010). The issue of discontinuous word features are later tackled by CNN (Kim, 2014; Hu et al., 2014), following some applications for answer selection in cQA (Yu et al., 2014; Qiu and Huang, 2015; Shen et al., 2015). Zhou et al. (2015)’s work modelled the content correlations using LSTM along QA sequences but they ignored the constraints among quality tags. Their method is insufficient in context learning also due to the neglect of sequence-level dependency modelling. Joty et al. (2015) proposed a graph-cut approach on the judgement of good or bad answers and gains improvement from the baselines."
C16-1117,N13-1106,0,0.07952,"Missing"
C16-1117,P13-1171,0,0.0284973,"ntroduce the literature. The proposed models are detailedly described in Section 3. Experiments are arranged in Section 4. Discussion and conclusion are at last. 2 2.1 Related Work Answer Quality Tagging In the literature, methods for answer quality tagging in cQA can be roughly divided into the following four groups: sparse feature-based methods, translation models, parsing trees, and deep CNNs. Sparse feature-based methods are the mostly widely applied and have the longest research duration. Early studies such as Agichtein et al. (2008) and Suryanoto et al. (2009), and later studies such as Yih et al. (2013) and Hou et al. (2015) all achieved not bad results using simple classifiers with manually constructed sparse features. However, feature engineering is time consuming and has low extensibility to other domains. Translation models relied on large-scale of training pairs and can effectively bridge the semantic gap in many cases (Berger et al., 2000; Riezler et al., 2007; Surdeanu et al., 2008). One difficulty is that large parallel training dataset is hard to obtain. The similarity computed from parsing trees is a direct criterion to measure the semantic correlation between two sentences. Typica"
C16-1117,P15-2117,1,0.90382,"ntial) A4: you can go to filipino souq you cab get it for QR 25. (Good) A5: ... available at designated retail outlets in the Philippines. u would easily find it while processing some of ur papers at POEA Ortigas. (Good) Figure 1: Example for cQA thread. To efficiently model the contextual information, we propose two neural network-based models with different combination modes of Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM) and Conditional Random Fields (CRF). The first model (ARC-I) is a stacked ensemble of the above networks, which can be seen as a combination of RCNN (Zhou et al., 2015) and LSTM-CRF (Huang et al., 2015). In ARC-I, LSTM is applied on the sequence of encoded QA matching pairs, with CRF on the final layer to memorize transition probabilities over the tag sequence. The main improvement from Zhou et al. (2015) is the addition of backward LSTM and CRF. And the difference from Huang et al. (2015) is that we adopt the LSTM-CRF model in comment-level (actually sentence-level within this paper) sequence tagging, with the help of CNN sentence modelling (Kim, 2014). ARC-II is a novel and much simpler model, with the integration of the attention mechanism. In ARCII, the"
D17-1065,D14-1181,0,0.00298967,"asonably approach the onehot representations of the discrete words. The structure of the approximation layer is illustrated on the right-hand side of Figure 1. Concretely, the approximation layer takes the output hi of the generator and a random noise zi as the input, and reuses the word projection layer (pre-trained in the standard generator) to estimate the probability distribution of wi . Note that, the noise zi added to hi forms a latent feature Pre-training the CNN-based Discriminator CNN has been proven to be an appropriate classifier for many NLP tasks, such as sentence classification (Kim, 2014) and matching (Hu et al., 2014). Therefore, in this paper we adopt a CNNbased discriminator as shown in Figure 1. For the convenience of further discussions, we introduce rˆ to denote the underlying (distributional) fake response produced by the decoder. In other words, rˆ stands for a sequence of word distributions projected from the hidden layers of the decoder RNN, based on which one would sample the output response utterance in a traditional Seq2Seq generator. The detailed architecture of the discriminator is described as follows. Firstly, the input of the discriminator consist of the word"
D17-1065,N16-1014,0,0.694097,"ure can be performed end-to-end in an unsupervised manner, based on human-generated conversational utterances (typically query-response pairs mined from social networks). One of the potential applications of such neural response generators is to improve the capability of existing conversational interfaces (informally also known as chatbots) by enabling them to go beyond predefined tasks and chat with human users in an open domain. However, previous research has indicated that na¨ıve implementations of Seq2Seq based conversation models tend to suffer from the so-called “safe response” problem (Li et al., 2016a), i.e. such models tend to generate non-informative responses that can be associated to most queries, e.g. “I don’t know”, “I think so”, etc. This is due to the fundamental nature of statistical models, which fit sufficiently observed examples better than insufficiently observed ones. Concretely, the space of open-domain conversations is so large that in any sub-sample of it (i.e. a training set), the distribution of most pieces of information are relatively much sparser when compared to safe response patterns. Furthermore, since a safe response can be of relevance to a large amount of diver"
D17-1065,D14-1179,0,0.00444748,"Missing"
D17-1065,P16-1094,0,0.630041,"ure can be performed end-to-end in an unsupervised manner, based on human-generated conversational utterances (typically query-response pairs mined from social networks). One of the potential applications of such neural response generators is to improve the capability of existing conversational interfaces (informally also known as chatbots) by enabling them to go beyond predefined tasks and chat with human users in an open domain. However, previous research has indicated that na¨ıve implementations of Seq2Seq based conversation models tend to suffer from the so-called “safe response” problem (Li et al., 2016a), i.e. such models tend to generate non-informative responses that can be associated to most queries, e.g. “I don’t know”, “I think so”, etc. This is due to the fundamental nature of statistical models, which fit sufficiently observed examples better than insufficiently observed ones. Concretely, the space of open-domain conversations is so large that in any sub-sample of it (i.e. a training set), the distribution of most pieces of information are relatively much sparser when compared to safe response patterns. Furthermore, since a safe response can be of relevance to a large amount of diver"
D17-1065,D17-1230,0,0.0928777,"Missing"
D17-1065,L16-1147,0,0.00879631,"decoder is also adequate to decode words from its hidden states. Therefore, the adversarial training here is to adjust the “wording strategy” of the generative model, i.e. the way it organises the semantic contents during the decoding (or in other words, the way it realises the hidden states). Preliminary experiments show that this trick significantly improves the grammaticalness of the generated responses. The gradient of the generator can be computed as: ∂Gloss ∂Vrˆ ∂Vrˆ ∂θG ∂Gloss ∂Vrˆ ∂G = ∂Vrˆ ∂G ∂θG ∇gD,G (θG ) = Datasets We test our model on two datasets: Baidu Tieba and OpenSubtitles (Lison and Tiedemann, 2016). The Baidu Tieba dataset is composed of single-turn conversations collected from the threads of Baidu Tieba1 , of which the utterance length ranging from 3 to 30 words. The OpenSubtitles dataset contains movie scripts organised by characters, where we follow Li et al. (2016a) to retain subtitles containing 5-50 words in the following experiments. From each of the two datasets, we sample 5,000,000 unique singleturn conversations as the training data, 200,000 additional unique pairs for validation, and another 10,000 as the test set. with the parameters of G frozen, before the adversarial train"
D17-1065,C16-1063,1,0.821605,"rd sequences only, while Zhou et al. (2016) and Iulian et al. (2017) have demonstrated that the comprehensibility of the generated responses can benefit from multiview training with respect to words, coarse tokens and utterances. Moreover, Sordoni et al. (2015) proposed a context-aware response generation model that goes beyond single-turn conversations. In addition, attention mechanisms were introduced to Seq2Seq-based models to capture topic and dialog focus information by Shang et al. (2015) and Chen et al. (2017), which had been proven to be helpful for improving query-response relevance (Wu et al., 2016). Additional features such as persona information (Li et al., 2016b) and latent semantics (Zhou et al., 2017; Serban et al., 2017) have also been proven beneficial within this context. When compared to previous work, this paper is focused on single-turn conversation modeling, and employs a GAN to yield informative responses. To the best of our knowledge, Reinforcement Learning (RL) is first introduced to address the above problem (Li et al., 2017; Yu et al., 2017), where the score predicted by a discriminator was used as the reinforcement to train the generator, yielding a hybrid model of GAN"
D17-1065,D16-1230,0,0.0189218,"Secondly, the practical deployment of a chat-oriented conversational system will usually decode an N-best list of candidate responses, from which it random samples the final reply. Considering that all the annotators use Mandarin as their first language, the above evaluation is only done on the Tieba dataset. For automatic evaluations, the following commonly accepted metrics are employed. Note here, the goal of our model is to obtain responses not only semantically relevant to the corresponding queries, but also of good diversity and novelty. Therefore, in this work, embedding-based metrics (Liu et al., 2016) are adopted to evaluate semantic the relevance between queries and their corresponding generated responses, while dist-1, dist-2 (Li et al., 2016a) are used as diversity measures. In addition, we also introduce a Novelty measure as detailed below. Relevance Metrics: The following three word embedding based metrics3 are used to compute the semantic relevance of two utterances. The Greedy metric is to greedily match words in two given utterances based on the cosine similarities of their embeddings, and to average the obtained scores (Rus and Lintean, 2012). Alternatively, an utterance represent"
D17-1065,P08-1028,0,0.0430339,"016a) are used as diversity measures. In addition, we also introduce a Novelty measure as detailed below. Relevance Metrics: The following three word embedding based metrics3 are used to compute the semantic relevance of two utterances. The Greedy metric is to greedily match words in two given utterances based on the cosine similarities of their embeddings, and to average the obtained scores (Rus and Lintean, 2012). Alternatively, an utterance representation can be obtained by averaging the embeddings of all the words in that utterance, of which the cosine similarity gives the Average metric (Mitchell and Lapata, 2008). In addition, one can also achieve an utterance representation by taking the largest extreme values among the embedding vectors of all the words it contains, before computing the cosine similarities between utterance vectors, which yields the Extreme metric (Forgues et al., 2014). Diversity Metrics: To measure the informativeness and diversity of the generated responses, we follow the dist-1 and dist-2 metrics proposed by Li et al. (2016a) and Chen et al. (2017), and introduce a Novelty metric. The dist-1 (dist2) is defined as the number of unique unigrams (bigrams for dist-2). A common drawb"
D17-1065,D16-1036,0,0.00548232,"g phase of the Seq2Seq model usually involves sampling discrete words from the predicted distributions, which will be fed into the training of the discriminator. The sampling procedure is non-differentiable, and will therefore break the back-propagation. 2 Related Work Inspired by recent advances in Neural Machine Translation (NMT), Ritter et al. (2011) and Vinyals and Le (2015) have shown that singleturn short-text conversations can be modelled as a generative process trained using query-response pairs accumulated on social networks. Earlier works focused on paired word sequences only, while Zhou et al. (2016) and Iulian et al. (2017) have demonstrated that the comprehensibility of the generated responses can benefit from multiview training with respect to words, coarse tokens and utterances. Moreover, Sordoni et al. (2015) proposed a context-aware response generation model that goes beyond single-turn conversations. In addition, attention mechanisms were introduced to Seq2Seq-based models to capture topic and dialog focus information by Shang et al. (2015) and Chen et al. (2017), which had been proven to be helpful for improving query-response relevance (Wu et al., 2016). Additional features such"
D17-1065,D11-1054,0,0.0188744,"e discriminative part can evaluate the quality of the generated utterances from diverse dimensions according to human-produced responses. However, unlike the image generation problems, training such a GAN for text generation here is not straightforward. The decoding phase of the Seq2Seq model usually involves sampling discrete words from the predicted distributions, which will be fed into the training of the discriminator. The sampling procedure is non-differentiable, and will therefore break the back-propagation. 2 Related Work Inspired by recent advances in Neural Machine Translation (NMT), Ritter et al. (2011) and Vinyals and Le (2015) have shown that singleturn short-text conversations can be modelled as a generative process trained using query-response pairs accumulated on social networks. Earlier works focused on paired word sequences only, while Zhou et al. (2016) and Iulian et al. (2017) have demonstrated that the comprehensibility of the generated responses can benefit from multiview training with respect to words, coarse tokens and utterances. Moreover, Sordoni et al. (2015) proposed a context-aware response generation model that goes beyond single-turn conversations. In addition, attention"
D17-1065,W12-2018,0,0.0185011,"ore, in this work, embedding-based metrics (Liu et al., 2016) are adopted to evaluate semantic the relevance between queries and their corresponding generated responses, while dist-1, dist-2 (Li et al., 2016a) are used as diversity measures. In addition, we also introduce a Novelty measure as detailed below. Relevance Metrics: The following three word embedding based metrics3 are used to compute the semantic relevance of two utterances. The Greedy metric is to greedily match words in two given utterances based on the cosine similarities of their embeddings, and to average the obtained scores (Rus and Lintean, 2012). Alternatively, an utterance representation can be obtained by averaging the embeddings of all the words in that utterance, of which the cosine similarity gives the Average metric (Mitchell and Lapata, 2008). In addition, one can also achieve an utterance representation by taking the largest extreme values among the embedding vectors of all the words it contains, before computing the cosine similarities between utterance vectors, which yields the Extreme metric (Forgues et al., 2014). Diversity Metrics: To measure the informativeness and diversity of the generated responses, we follow the dis"
D17-1065,P15-1152,0,0.163588,"Missing"
D17-1065,N15-1020,0,0.0526258,"l therefore break the back-propagation. 2 Related Work Inspired by recent advances in Neural Machine Translation (NMT), Ritter et al. (2011) and Vinyals and Le (2015) have shown that singleturn short-text conversations can be modelled as a generative process trained using query-response pairs accumulated on social networks. Earlier works focused on paired word sequences only, while Zhou et al. (2016) and Iulian et al. (2017) have demonstrated that the comprehensibility of the generated responses can benefit from multiview training with respect to words, coarse tokens and utterances. Moreover, Sordoni et al. (2015) proposed a context-aware response generation model that goes beyond single-turn conversations. In addition, attention mechanisms were introduced to Seq2Seq-based models to capture topic and dialog focus information by Shang et al. (2015) and Chen et al. (2017), which had been proven to be helpful for improving query-response relevance (Wu et al., 2016). Additional features such as persona information (Li et al., 2016b) and latent semantics (Zhou et al., 2017; Serban et al., 2017) have also been proven beneficial within this context. When compared to previous work, this paper is focused on sin"
D19-5706,D17-1070,0,0.0780072,"Missing"
D19-5706,N19-1423,0,0.0327908,"logy) challenges (e.g., the CHEMDNER (Chemical compound and drug name recognition) track (Leaman et al., 2013)), 33 Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, pages 33–37 1 Association for Computational Linguistics c Hong Kong, China, November 4, 2019. 2019 Figure 1: Overview architecture of our system for the PharmaCoNER task training set, 250 cases as the development set and 250 cases as the test set. We participated in this shared task and developed a pipeline system based on two latest deep learning methods: BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and Bpool (Bi-LSTM with max/mean pooling) (Conneau et al., 2017). The system developed on the training and development sets achieved a micro-average F1-score of 0.9105 on track 1 and a microaverage F1-score of 0.8391 on track 2 on the independent test set. 2 2.2 We split each clinical case into sentences using ‘;’, ‘?’, ‘!’, ‘
’ or ‘.’ which is not in numbers, and further split each sentence into tokens using the method proposed by Liu (Liu et al., 2015), which was specially designed for clinical text. We adopted Ab3P tools 1 to extract full names of abbreviations, and SPACCC_POS-TAGGER tool"
D19-5706,S15-2051,0,0.0482063,"Buzhou Tang1,2* Xiaolong Wang1, Qingcai Chen1,2, Jun Yan3, Yi Zhou4* 1 Department of Computer Science, Harbin Institute of Technology, Shenzhen, China, 518055 2 Peng Cheng Laboratory 3 Yidu Cloud (Beijing) Technology Co., Ltd, Beijing 4 Sun YAT-SEN UNIVERSITY {xiongying0929, hyhang7, chenshuai726, tangbuzhou, qingcai.chen}@gmail.com shenyedan@stu.hit.edu.cn, wangxl@insun.hit.edu.cn, Jun.YAN@Yiducloud.cn, zhouyi@sysu.edu.cn * Corresponding author the i2b2 (the Center of Informatics for Integrating Biology and Bedside) challenges (Uzuner et al., 2011), SemEval (Semantic Evaluation) challenges (Elhadad et al., 2015) and the ShARe/CLEF eHealth Evaluation Lab shared tasks (Kelly et al., 2016). A large number of various kinds of methods have been proposed for biomedical entity recognition and normalization. Lots of machine learning methods such as conditional random fields (CRF) (Lafferty et al., 2001), structured support vector machines (SSVM) (Tsochantaridis et al., 2005) and bidirectional long-short-term memory with conditional random fields (BiLSTM-CRF) (Huang et al., 2015) have been applied for biomedical entity recognition, support vector machines (SVM) (Grouin et al., 2010) and ranking based on convo"
D19-5706,D19-5701,0,0.048637,"Missing"
I05-1072,P79-1022,0,0.165026,"Missing"
I05-1072,J92-4003,0,0.0162619,"ar language models. It was first proposed by IBM in speech recognition [1] and achieved great success. Then Hmm has a wide range of applications in many domains, such as OCR [2], handwriting recognition [3], machine translation [4], Chinese pinyin-to-character conversion [5] and so on. To improve Hmm’s predictive power, one of Hmm hypotheses [6] named limited history hypothesis, is usually relaxed and higher-order Hmm is proposed. But as the order of Hmm increases, its parameter space explodes at an exponential rate, which may result in several severe problems, such as data sparseness problem [7], system resource exhaustion problem and so on. From another point of view, this paper relaxes the other Hmm hypothesis, named stationary hypothesis, makes use of time information and proposes non-stationary Hmm (NSHmm). This paper first defines NSHmm in a formalized form, and then discusses how to represent time information in NSHmm. After that, the algorithms of NSHmm are provided and the parameter space complexity is calculated. Moreover, to further reduce the parameter space, a R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 827 – 837, 2005. © Springer-Verlag Berlin Heidelberg 2005 828"
I05-1072,J93-2003,0,\N,Missing
I05-3001,W03-1721,0,0.061811,"raining material for participants and they serve as a gold standard for evaluating the performance of participant systems. This paper presents a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. Especially a segmentation error rate of a gold standard corpus could be obtained with our approach. As we know a particular Chinese character string occurring more than once in a corpus may be assigned different segmentations. Those differences are considered as segmentation inconsistencies by some researchers (Wu, 2003; Chen, 2003). Segmentation consistency is also considered as one of the quality criteria of an annotated Chinese corpus (Sun, 1999). But in order to provide a more clearer description of those segmentation differences we define a new This paper proposes a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. A particular Chinese character string occurring more than once in a corpus may be assigned different segmentations during a segmentation process. Based on these differences our approach outputs the segmentation error candida"
I05-3001,W03-1727,0,0.0265932,"provide training material for participants and they serve as a gold standard for evaluating the performance of participant systems. This paper presents a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. Especially a segmentation error rate of a gold standard corpus could be obtained with our approach. As we know a particular Chinese character string occurring more than once in a corpus may be assigned different segmentations. Those differences are considered as segmentation inconsistencies by some researchers (Wu, 2003; Chen, 2003). Segmentation consistency is also considered as one of the quality criteria of an annotated Chinese corpus (Sun, 1999). But in order to provide a more clearer description of those segmentation differences we define a new This paper proposes a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. A particular Chinese character string occurring more than once in a corpus may be assigned different segmentations during a segmentation process. Based on these differences our approach outputs the segmentation"
I05-3001,P03-1035,1,0.825212,"ntation inconsistency S3. A character string inconsistently between a test data and its training In the close test of Bakeoff1, participants could only use training material from the training data for the particular corpus being testing on. No other material was allowed (Sproat and Emerson, 2003). As we know that the test data should be consistent with the training data based on a general definition of Chinese words. That is if we collect all words seen in the training data and store them into a lexicon, then each word in a test set is either a lexicon word or an OOV (out of vocabulary) word (Gao et al., 2003). In another word, if a character string has been treated as one word, i.e. a lexicon word, in the training data, the same occurrence should be taken in the data. This situation could be divided into the following two cases further: S3.1 A word identified in a training data has been segmented into multiple words in corresponding test data; S3.2 A word identified in a test data has been segmented into multiple words in corresponding training data. Chen (2003) describes inconsistency problem found in cases S1, S2 and S3.1 of PK corpora. For example, he gives the amount of unique text fragments t"
I05-3001,E03-1068,0,0.0259954,"e. Thus a segmentation variation (type) consists of more than one variation instances in corpora C. And a variation instance may include one or more than one tokens. Definition 4: If a variation instance is an incorrect segmentation, it is called an error instance (EI). The definitions of segmentation variation, variation instance and error instance (EI) clearly distinguish those inconsistent components, so we can count the number of segmentation errors (in tokens) exactly. The term variation is also used to express other annotation inconsistency in a corpus by other researchers. For example, Dickinson and Meurers (2003) used variation to describe POS (Part-of-Speech) inconsistency in an annotated corpus. Example 1: Segmentation variations (Bakeoff1 PK corpus): Word &quot;ㄝৠ[deng3-tong2]&quot; is segmented as &quot;ㄝৠ&quot; (equal) and &quot;ㄝৠ&quot; (et al. with). Word &quot;咘䞥[huang2-jin1-zhou1]&quot; is segmented as &quot;咘䞥&quot; (golden week) and &quot; 咘䞥&quot; (gold week). Word &quot;[⋕⥝⏙ބbing1-qing1-yu4-jie2]&quot; is segmented as &quot;( &quot;⋕⥝⏙ބpure and noble) and &quot;( &quot;⋕⥝⏙ބice clear jade clean). In example 1, Words like “ㄝৠ”, “咘䞥  ” and “  ” ⋕ ⥝ ⏙ ބare segmentation variation types. Segmentations “ㄝৠ” and “ ㄝ  ৠ ” are two variation instances of segmentation va"
I05-3001,W03-1719,0,\N,Missing
I05-3001,W03-1726,0,\N,Missing
I08-1008,J96-1002,0,0.14677,"Missing"
I08-1008,P04-1024,0,0.691486,"and name entity recognition. Unfortunately, little attention has been given to name origin recognition (NOR) so far in the literature. In this paper, we are interested in two kinds of name origin recognition: the origin of names written in English (ENOR) and the origin of names written in Chinese (CNOR). For ENOR, the origins include English (Eng), Japanese (Jap), Chinese Mandarin Pinyin (Man) and Chinese Cantonese Jyutping (Can). For CNOR, they include three origins: Chinese (Chi, for both Mandarin and Cantonese), Japanese and English (refer to Latinscripted language). Unlike previous work (Qu and Grefenstette, 2004; Li et al., 2006; Li et al., 2007) where NOR was formulated with a generative model, we regard the NOR task as a classification problem. We further propose using a discriminative learning algorithm (Maximum Entropy model: MaxEnt) to solve the problem. To draw direct comparison, we conduct experiments on the same personal name corpora as that in the previous work by Li et al. (2006). We show that the MaxEnt method effectively incorporates diverse features and outperforms previous methods consistently across all test cases. The rest of the paper is organized as follows: in section 2, we review"
I08-1008,P07-1016,1,\N,Missing
I08-1008,Y04-1029,0,\N,Missing
I08-1008,J98-4003,0,\N,Missing
I08-4026,J96-1002,0,0.00860045,"Missing"
I08-4026,W06-0127,0,0.0743341,"Missing"
I08-4026,W06-0126,0,\N,Missing
I11-1168,H93-1012,0,0.198979,"Missing"
I11-1168,P01-1070,0,0.0691857,"Missing"
I11-1168,passonneau-2006-measuring,0,\N,Missing
I13-1086,P09-1017,0,0.027162,"Missing"
I13-1086,O09-1005,0,0.0257122,"a in two ways. One is augmenting a small number of labeled documents with large amounts of unlabeled ones to guide the learning model iteratively, so that the new classifier can label the unlabeled documents (Jiang, 2009; Nigam et al., 2000). In such studies, the bootstrapping technique is often used to label the unlabeled documents and refine the initial classifier (Gliozzo et al., 2009; Ko and Seo, 2009). The other is collecting training corpora from the Web. Such works use the class name and its associated terms to collect training corpora iteratively (Huang et al., 2005). Cheng (2009) and Day et al. (2009) firstly sample the Web with a set of given class names, and then query the keywords manually populated from each class by search engines for retrieving quality training documents. Huang et al. (2004) proposed a LiveClassifier system which also makes use of search engines for automatically constructing training classifier. Since the machines become more and more intelligent, it is reasonable to expect the automatic construction of text classifiers by given just the objective categories. As trade-off solutions, existing researches usually provide additional information to the category terms to"
I13-1148,P11-1092,0,0.0177207,"s. Two examples are shown below: with: a/empty big apples ~ Category empty without: the United States ~ Category the For each category in a, the, and empty, we use the whole with instances and randomly take samples of without ones, making up the training instances for each category. We consider all the with samples useful because each of them has an observed wrong article which indicates that the correct article is easily misused as the wrong one. Different ratios of with : without are experimented in our work to see how much the number of without samples, which is mentioned in previous work (Dahlmeier and Ng, 2011), affects the result in our model. Maximum entropy (ME) is employed for classification which has been proven to have good performance for heterogeneous features in natural language processing tasks. We have also tried several other classifiers including SVM, decision tree, and Naïve Bayes but finally found ME performs better. 3.2 Confidence Tuning ME returns with confidence of each category for a given testing instance. However, for different instances, the distributions of predicted scores vary a lot. In some instances, the classifier may have a very high predicted score to a certain category"
I13-1148,D12-1052,0,0.0453828,"Missing"
I13-1148,P05-1045,0,0.00550498,"a multi classification task. Three categories including a/an, the and empty are assigned to specify the correct article forms in corresponding positions (a and an are distinguished according to pronunciation of the following word). For training, developing and testing, all noun phrases (NPs) are chosen as candidates to be corrected. We extract related features based on the context of an NP and do feature selection afterwards. 2.1 Feature Extraction A series of syntactic and semantic features are extracted with the help of NLP tools like Stanford parser (Klein and Manning, 2003), Stanfordner (Finkel et al., 2005) and WordNet (Fellbaum, 1999). We adopt syntactic features such as the surface word, word n-gram, part-of-speech (POS), POS n-gram, constituent parse tree, dependency parse tree, name entity type and headword; semantic features like noun category and noun hypernym. Some extended features are extracted based on them and some previous work (Dahlmeier and Ng, 2012b; Felice and Pulman, 2008). Through feature extraction, we get over 90 groups of different features. After binarization, the dimensionality exceeds to about 350 thousand in which many features occur only once. We tried to prune all spar"
I13-1148,W12-2027,0,0.0312894,"Missing"
I13-1148,P03-1054,0,0.00633602,"Selection We take article correction as a multi classification task. Three categories including a/an, the and empty are assigned to specify the correct article forms in corresponding positions (a and an are distinguished according to pronunciation of the following word). For training, developing and testing, all noun phrases (NPs) are chosen as candidates to be corrected. We extract related features based on the context of an NP and do feature selection afterwards. 2.1 Feature Extraction A series of syntactic and semantic features are extracted with the help of NLP tools like Stanford parser (Klein and Manning, 2003), Stanfordner (Finkel et al., 2005) and WordNet (Fellbaum, 1999). We adopt syntactic features such as the surface word, word n-gram, part-of-speech (POS), POS n-gram, constituent parse tree, dependency parse tree, name entity type and headword; semantic features like noun category and noun hypernym. Some extended features are extracted based on them and some previous work (Dahlmeier and Ng, 2012b; Felice and Pulman, 2008). Through feature extraction, we get over 90 groups of different features. After binarization, the dimensionality exceeds to about 350 thousand in which many features occur on"
I13-1148,P12-2064,0,0.0294831,"Missing"
I13-1148,C08-1109,0,0.0294968,"r different instances, the distributions of predicted scores vary a lot. In some instances, the classifier may have a very high predicted score to a certain category which means the classifier is confident enough to perform this prediction while for some other instances, two or more categories may share close scores, the case of which means the classifier hesitates when telling them apart. Our confidence tuning strategy (Tuning) on the predicted results is based on a comparison between the observed category and the predicted category. It is similar to the “thresholding” approach described in (Tetreault and Chodorow, 2008). The main idea of this confidence tuning strategy is: the selection between keep and drop is based on the difference between confidence of the predicted category and the observed category. If this difference goes beyond a threshold t, the prediction is proposed while if it is under t, we won’t do any corrections. The confidence threshold is generated through hill-climbing in development data aiming at maximizing F1 of the result. 4 4.1 Experiments Data Set and Evaluation Metrics The NUCLE corpus (Dahlmeier and Ng, 2011) introduced by National University of Singapore contains 1414 essays writt"
I13-1148,I08-2082,0,0.0634222,"Missing"
I13-1148,C08-1022,0,\N,Missing
I13-1148,W12-2025,0,\N,Missing
I17-1072,D14-1181,0,0.00323452,"conversation modelling mainly focuses on the one-turn conversations (aka. message-response pairs) (Banchs and Li, 2012; Ameixa et al., 2014; Ritter et al., 2011; Ji et al., 2014), while recent works show more interest in multi-turn dialogues. The generation-based approaches model the context and generate the responses at the same time (Vinyals and Le, 2015), while the retrieval based studies model the sessions after knowing all utterances, which is more relevant to this work. Xu et al. (2016) represent sequence of utterances with recurrent neural networks (RNNs). The topic 714 softmax layer (Kim, 2014). The illustration of the utterance model for R0 is shown in Figure 2(b). In this paper, all representations of utterances are attracted with CNN structure described in Figure 2(a). An n-word utterance can be represented as: e1:n = e1 ⊕ e2 ⊕ ... ⊕ en (2) or intention in a dialogue session is relatively constant. In this perspective, all the utterances in the same session is homogenous and could be composed within RNNs. However, the influences of user’s queries and the agent’s responses are different in predicting user’s emotion. Therefore, a targeted structure considering such heterogenicity i"
I17-1072,P12-3007,0,0.0256903,"ative feedbacks depends on a complicated semantic mechanism and conversational contexts play an important role in this issue, this paper proposes to address the problem by learning to represent the possible determinant with different models. Especially, the proposed architecture based on Gated Convolutional Recurrent Neural Networks (GCRNN) is used to represent sequence of relations between the last response and the earlier utterances. Experimentally, it out2.2 Conversation Modelling Traditional conversation modelling mainly focuses on the one-turn conversations (aka. message-response pairs) (Banchs and Li, 2012; Ameixa et al., 2014; Ritter et al., 2011; Ji et al., 2014), while recent works show more interest in multi-turn dialogues. The generation-based approaches model the context and generate the responses at the same time (Vinyals and Le, 2015), while the retrieval based studies model the sessions after knowing all utterances, which is more relevant to this work. Xu et al. (2016) represent sequence of utterances with recurrent neural networks (RNNs). The topic 714 softmax layer (Kim, 2014). The illustration of the utterance model for R0 is shown in Figure 2(b). In this paper, all representations"
I17-1072,W02-1011,0,0.0210838,"nse selection or generation, which tend to intuitively take semantic relevance oriented features for model training. The adoption of user satisfaction is possible to provide a different view to optimize the models, so as to further improve user experience along the road beyond relevance, e.g., avoiding the responses that are relevant but lack of sociality (Higashinaka et al., 2015). 2 2.1 Related Work Emotion prediction Predicting sentiment category of text has been extensively studied. Most works focus on the sentiment orientations expressed by the writers in movie/product reviews or tweets (Pang et al., 2002; Hu and Liu, 2004; Go et al., 2009). However, the reader’s emotion is not always consistent with that of the writer’s (Yang et al., 2007). Thus, Lin et al. (2007) explore to predict the feelings that readers may have after reading particular articles. However, in this dissatisfaction prediction task, the user is not only the reader of the session text, but also the writer of some utterances. Therefore, some clues of the particular user’s emotion may be contained in the context. Modelling emotion in human-human conversation has been explored (Herzig et al., 2016; Tokuhisa and Terashima, 2006)."
I17-1072,W16-3609,0,0.0217392,"movie/product reviews or tweets (Pang et al., 2002; Hu and Liu, 2004; Go et al., 2009). However, the reader’s emotion is not always consistent with that of the writer’s (Yang et al., 2007). Thus, Lin et al. (2007) explore to predict the feelings that readers may have after reading particular articles. However, in this dissatisfaction prediction task, the user is not only the reader of the session text, but also the writer of some utterances. Therefore, some clues of the particular user’s emotion may be contained in the context. Modelling emotion in human-human conversation has been explored (Herzig et al., 2016; Tokuhisa and Terashima, 2006). However, triggers for negative emotion in human-computer dialogue might be different (e.g., low readability or relevance). The works of Tokuhisa et al. (2009) and Yu et al. (2016)’s analysed the emotion of a particular utterance in human-computer dialogue based on the textual features containing in the very sentence. Different from their studies analysing the explicit textual feedback, this work predicts the impending emotion based on the context because the cause of emotion of readers contains in the existing text (Li and Xu, 2014). It is not a trivial task to"
I17-1072,D11-1054,0,0.0278271,"emantic mechanism and conversational contexts play an important role in this issue, this paper proposes to address the problem by learning to represent the possible determinant with different models. Especially, the proposed architecture based on Gated Convolutional Recurrent Neural Networks (GCRNN) is used to represent sequence of relations between the last response and the earlier utterances. Experimentally, it out2.2 Conversation Modelling Traditional conversation modelling mainly focuses on the one-turn conversations (aka. message-response pairs) (Banchs and Li, 2012; Ameixa et al., 2014; Ritter et al., 2011; Ji et al., 2014), while recent works show more interest in multi-turn dialogues. The generation-based approaches model the context and generate the responses at the same time (Vinyals and Le, 2015), while the retrieval based studies model the sessions after knowing all utterances, which is more relevant to this work. Xu et al. (2016) represent sequence of utterances with recurrent neural networks (RNNs). The topic 714 softmax layer (Kim, 2014). The illustration of the utterance model for R0 is shown in Figure 2(b). In this paper, all representations of utterances are attracted with CNN struc"
I17-1072,W15-4611,0,0.0200498,"user satisfaction can be taken as a metric for evaluating the quality of a given dialogue session and the overall performance of a dialogue agent. In practice, user satisfaction could also be considered as the additional criterion for response selection or generation, which tend to intuitively take semantic relevance oriented features for model training. The adoption of user satisfaction is possible to provide a different view to optimize the models, so as to further improve user experience along the road beyond relevance, e.g., avoiding the responses that are relevant but lack of sociality (Higashinaka et al., 2015). 2 2.1 Related Work Emotion prediction Predicting sentiment category of text has been extensively studied. Most works focus on the sentiment orientations expressed by the writers in movie/product reviews or tweets (Pang et al., 2002; Hu and Liu, 2004; Go et al., 2009). However, the reader’s emotion is not always consistent with that of the writer’s (Yang et al., 2007). Thus, Lin et al. (2007) explore to predict the feelings that readers may have after reading particular articles. However, in this dissatisfaction prediction task, the user is not only the reader of the session text, but also th"
I17-1072,P15-1152,0,0.0249131,"relation with Architecture-I in Hu et al.’s (2014) work, where the representations of the query and the response are learned with two CNNs respectively and the concatenation of the representations is used as input of a multi-layer perceptron (MLP) classifier that measures the appropriateness. 3.3 Utterance Sequence Modelling As shown in example in Figure 1, the latest response is active and related to the query, but may not appropriate in the context. Recent works encode the sequence of utterances with recurrent neural network based encoder-decoder to generate responses (Serban et al., 2016; Shang et al., 2015). However, in this work, the prediction is made with all existing utterances being known. The convolutional recurrent neural network has been proven to be effective in encoding the sequence of representations of text (Kalchbrenner and Blunsom, 2013; Utterance Modelling The last response of the robot R0 is the most straightforward factor that may cause the antipathy towards the agent. Predicting the negative emotions according to the latest response can be considered as reader-side emotion classification. In this work, such single sentence is modelled with convolutional neural network (CNN) wit"
I17-1072,D13-1170,0,0.00300636,"ix that mapping sentence representations in UPM is constant after training and the contribution of each sentence to the conversation is relatively fixed. While the GRUs in the CRNN could select the information resource flexibly through the reset gate r and update gate z, controlling the influence of particular sentence according to the context (Chung et al., 2014). Moreover, the gating process is a kind of multiplicative operation between sentence embeddings. Such multiplicative compositional functions are more expressive in simulating interaction between abstract features than additive ones (Socher et al., 2013; ˙Irsoy and Cardie, 2015). Thus, CRNN based models handle the interaction between utterances in a more flexible way than UPM. 澤澢澦 澤澢澬 澤澢澫 澤澢澪 澤澢澩 澤澢澨 澤澢澧 濅澡澦 濆澡澦 濅澡澥 34. 濆澡澥 濅澤 濆澤 64. Figure 3: The probabilities of negative feedback for each utterance in sessions in Figure 1. The line chart of the sequence of probabilities of the two models are shown in Figure 3. The tendencies of these two line are similar. It is due to the fact that the gates controlled by R0 shrink the activations of recurrent layer and adjust the scales of values without changing the quadrant or feature space. For the sam"
I17-1072,W06-1323,0,0.0820344,"s or tweets (Pang et al., 2002; Hu and Liu, 2004; Go et al., 2009). However, the reader’s emotion is not always consistent with that of the writer’s (Yang et al., 2007). Thus, Lin et al. (2007) explore to predict the feelings that readers may have after reading particular articles. However, in this dissatisfaction prediction task, the user is not only the reader of the session text, but also the writer of some utterances. Therefore, some clues of the particular user’s emotion may be contained in the context. Modelling emotion in human-human conversation has been explored (Herzig et al., 2016; Tokuhisa and Terashima, 2006). However, triggers for negative emotion in human-computer dialogue might be different (e.g., low readability or relevance). The works of Tokuhisa et al. (2009) and Yu et al. (2016)’s analysed the emotion of a particular utterance in human-computer dialogue based on the textual features containing in the very sentence. Different from their studies analysing the explicit textual feedback, this work predicts the impending emotion based on the context because the cause of emotion of readers contains in the existing text (Li and Xu, 2014). It is not a trivial task to predict negative feedbacks in"
I17-1072,W13-3214,0,0.0200285,"ptron (MLP) classifier that measures the appropriateness. 3.3 Utterance Sequence Modelling As shown in example in Figure 1, the latest response is active and related to the query, but may not appropriate in the context. Recent works encode the sequence of utterances with recurrent neural network based encoder-decoder to generate responses (Serban et al., 2016; Shang et al., 2015). However, in this work, the prediction is made with all existing utterances being known. The convolutional recurrent neural network has been proven to be effective in encoding the sequence of representations of text (Kalchbrenner and Blunsom, 2013; Utterance Modelling The last response of the robot R0 is the most straightforward factor that may cause the antipathy towards the agent. Predicting the negative emotions according to the latest response can be considered as reader-side emotion classification. In this work, such single sentence is modelled with convolutional neural network (CNN) with max pooling, and then classified in the full-connected 715 瀆瀂濹瀇瀀濴瀋 濡濕濬澔濤濣濣濠濝濢濛 瀆瀂濹瀇瀀濴瀋 濠濟濣 濚濥濨 濚濥濨 濚濥濨 濚濥濨 濖濡濡 濖濡濡 濖濡濡 濖濡濡 濖濡濡 濅澡澥 濆澡澥 濅澤 濆澤 濆澤 澜濘澝 澜濖澝 瀆瀂濹瀇瀀濴瀋 瀆瀂濹瀇瀀濴瀋 濗濣濢濪濣濠濩濨濙 濠濟濣 澜濕澝 濖濡濡 濖濡濡 濅澤 濆澤 濚濥濨 濚濥濨 濚濥濨 濚濥濨 濖濡濡 濖濡濡 濖濡濡 濖濡濡 濅澡澥 濆澡澥 濅澤 濆澤"
I17-1072,P15-1151,0,0.0244842,"ing two annotators agreement the third annotator decision gold standard negative in gold standard positive in gold standard are gated by representation of R0 : xt = c t m t (9) where ct is the output of convolutional neural network sentence model. And the matching gate mt is influenced by the particular utterance ct and the latest response c0 . mt = σ (Wm ct + Um c0 ) (10) While the input of the last time step is the representation of R0 itself (x0 = c0 ). Gating operation has been shown effective in further mapping abstract feature of convolutional result by involving additional information (Wang et al., 2015; Dauphin et al., 2016). With such structure, the emotional consistency of utterances could be extracted and the influence of latest response on negative feedback could be encoded. 4 4.1 Amount 2,000,000 260,867 40,000 28,651 11,349 30,039 17,618 12,421 Table 1: Statistical information of the dataset. the requirements (non-negative sessions or negative sessions with 3 three turns or more utterances before the negative expressions) are used as goldstandard dataset. Some statistical information of the dataset is shown in Table 1. 4.2 Pre-training 4.2.1 Fragment Extraction The manually labelled g"
I17-1072,C16-1063,1,0.852396,"utterance model for R0 is shown in Figure 2(b). In this paper, all representations of utterances are attracted with CNN structure described in Figure 2(a). An n-word utterance can be represented as: e1:n = e1 ⊕ e2 ⊕ ... ⊕ en (2) or intention in a dialogue session is relatively constant. In this perspective, all the utterances in the same session is homogenous and could be composed within RNNs. However, the influences of user’s queries and the agent’s responses are different in predicting user’s emotion. Therefore, a targeted structure considering such heterogenicity is proposed in this work. Wu et al. (2016) represent the relevance between utterances with CRNN architecture. Different from their work focusing on word-level matching with attention pooling on the convolutional result, we leverage a gate operation to simulate the sentence-level interaction. 3 where en is the embedding of nth word in the utterance and ⊕ refers to concatenation operator. In the convolutional process, the word window starting with the ith word and scanned by a s-width filter j can be represented ei:i+s−1 . And the activations corresponding with filter j in convolutional layer can be computed as:  Predicting Methodology"
I17-1072,W16-3608,0,0.0149038,"explore to predict the feelings that readers may have after reading particular articles. However, in this dissatisfaction prediction task, the user is not only the reader of the session text, but also the writer of some utterances. Therefore, some clues of the particular user’s emotion may be contained in the context. Modelling emotion in human-human conversation has been explored (Herzig et al., 2016; Tokuhisa and Terashima, 2006). However, triggers for negative emotion in human-computer dialogue might be different (e.g., low readability or relevance). The works of Tokuhisa et al. (2009) and Yu et al. (2016)’s analysed the emotion of a particular utterance in human-computer dialogue based on the textual features containing in the very sentence. Different from their studies analysing the explicit textual feedback, this work predicts the impending emotion based on the context because the cause of emotion of readers contains in the existing text (Li and Xu, 2014). It is not a trivial task to predict negative feedbacks in the conversation flows between human being and dialogue agents. Generally, people tend to not express their dissatisfaction explicitly, thus, there are generally no clear signals be"
I17-1072,C08-1111,0,\N,Missing
O06-3002,W00-1201,0,0.0600264,"Missing"
O06-3002,P02-1055,0,0.0427147,"Missing"
O06-3002,P80-1024,0,0.683241,"Missing"
O06-3002,W04-1107,1,0.822826,"Missing"
O06-3002,W01-0706,0,0.0366842,"Missing"
O06-3002,W00-0726,0,0.0816367,"Missing"
O06-3002,W00-0730,0,0.453634,"Missing"
O06-3002,P01-1043,0,0.0456828,"Missing"
O06-3002,W00-0729,0,0.0603513,"Missing"
O06-3002,J93-2004,0,0.0278933,"Missing"
O06-3002,W03-1025,0,0.0251577,"Missing"
O06-3002,W00-0731,0,0.060197,"Missing"
O06-3002,P03-1063,0,0.0411775,"Missing"
O06-3002,W95-0107,0,0.0593771,"Missing"
O06-3002,W96-0213,0,0.288989,"Missing"
O06-3002,xia-etal-2000-developing,0,0.0581492,"Missing"
O06-3002,P00-1015,1,0.895631,"Missing"
O06-3002,J96-1002,0,\N,Missing
O06-5005,J05-4005,0,0.0451038,"Missing"
O06-5005,P03-1035,0,0.0502292,"Missing"
O06-5005,Y98-1021,0,0.0502985,"Missing"
O06-5005,I05-3030,1,0.725124,"Missing"
O06-5005,P97-1041,0,0.05078,"Missing"
O06-5005,C04-1081,0,0.0656178,"Missing"
O06-5005,J98-1004,0,0.0663461,"Missing"
O06-5005,J96-3004,0,0.142893,"Missing"
O06-5005,W03-1728,0,0.0596835,"Missing"
O06-5005,W03-1709,0,0.0439466,"Missing"
O07-4002,J92-4003,0,0.227321,"Missing"
O07-4002,W05-1107,0,0.0321058,"Missing"
O07-4002,H05-1027,0,0.0676543,"Missing"
O07-4002,N03-1018,0,0.0434421,"Missing"
O07-4002,C88-1071,0,0.386032,"Missing"
O07-4002,I05-1072,1,0.848235,"Missing"
O07-4002,J93-2003,0,\N,Missing
O07-5006,J96-1002,0,0.109977,"Missing"
O07-5006,J92-4003,0,0.355499,"Missing"
O07-5006,P00-1031,0,0.378763,"Missing"
O07-5006,I05-3017,0,0.0879708,"Missing"
O07-5006,O01-2002,0,0.563826,"Missing"
O07-5006,H05-1027,0,0.22853,"Missing"
O07-5006,O07-5006,1,0.0512899,"Missing"
O07-5006,P98-2124,0,0.376924,"Missing"
O07-5006,C02-1089,0,0.395758,"Missing"
O07-5006,O04-1009,0,0.283232,"Missing"
O07-5006,I05-2010,0,0.227717,"Missing"
O07-5006,P06-2108,0,0.348954,"Missing"
O07-5006,I05-1072,1,0.678103,"Missing"
O07-5006,P98-2239,0,\N,Missing
O07-5006,C98-2234,0,\N,Missing
O07-5006,C98-2119,0,\N,Missing
P10-1125,P09-1082,0,0.411816,"y, conclusions and future directions are drawn in Section 6. 2 Related Work The value of the naturally generated questionanswer pairs has not been recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages (Jijkoun and de Rijke, 2005; Riezler et al., 2007) or service call-center dialogues (Berger et al., 2000). Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers has been presented in (Jeon et al., 2006). Bernhard and Gurevych (2009) have developed a translation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word information, instead of the combination of different kinds of features. Because people have considerable freedom to post on forums, there are a great number of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory st"
P10-1125,P08-1081,0,0.558628,"idu.com http://answers.yahoo.com 1230 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230–1238, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics answer semantic modeling. Most researchers try to introduce structural features or users’ behavior to improve the models performance, by contrast, the effect of textual features is not obvious. 2. How to train a model so that it has good performance on both cQA and forum datasets? So far, people have been doing QA researches on the cQA and the forum datasets separately (Ding et al., 2008; Surdeanu et al., 2008), and no one has noticed the relationship between the two kinds of data. Since both the cQA systems and the online forums are open platforms for people to communicate, the QA pairs in the cQA systems have similarity with those in the forums. In this case, it is highly valuable and desirable to propose a training strategy to improve the model’s performance on both of the two kinds of datasets. In addition, it is possible to avoid the expensive and arduous hand-annotating work by introducing the method. To solve the first problem, we present a deep belief network (DBN) to"
P10-1125,P08-1019,0,0.0414057,"ch features are used in (Ding et al., 2008; Cong et al., 2008), and have significantly improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs are also extracted from email contents as the main elements of email summarization (Shrestha and McKeown, 2004). 3 The Deep Belief Network for QA pairs Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic relevance between questions and answers using only co-occurrence features. It is clear that the semantic link exists between the question and its answers, even though they have totally different lex"
P10-1125,D08-1043,0,0.12929,"lassification problem is an intuitive idea, thus there are some studies trying to solve it from this view (Hong and Davison, 2009; Wang et al., 2009). Especially Hong and Davison (2009) have achieved a rather high precision on the corpora with less noise, which also shows the importance of “social” features. In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA achieves (Jeon et al., 2005). Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. (2008) use translate models to bridge the lexical gap between queries and questions in QA collections. The SMT based methods are effective on modeling the semantic relationship between questions and answers and expending users’ queries in answer retrieval (Riezler et al., 2007; Berger et al., 1231 2000; Bernhard and Gurevych, 2009). In (Surdeanu et al., 2008), the translation model is used to provide features for answer ranking. The structural features (e.g., authorship, acknowledgement, post position, etc), also called non-textual features, play an important role in answer extraction. Such features"
P10-1125,P09-1083,0,0.0191267,"improved the performance. The studies of Jeon et al. (2006) and Hong et al. (2009) show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs are also extracted from email contents as the main elements of email summarization (Shrestha and McKeown, 2004). 3 The Deep Belief Network for QA pairs Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic relevance between questions and answers using only co-occurrence features. It is clear that the semantic link exists between the question and its answers, even though they have totally different lexical representations. Thus a specially designed model may learn semantic knowledge b"
P10-1125,P07-1059,0,0.0944598,"is needed in our strategy. The rest of this paper is organized as follows: Section 2 surveys the related work. Section 3 introduces the deep belief network for answer detection. In Section 4, the homogenous data based learning strategy is described. Experimental result is given in Section 5. Finally, conclusions and future directions are drawn in Section 6. 2 Related Work The value of the naturally generated questionanswer pairs has not been recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages (Jijkoun and de Rijke, 2005; Riezler et al., 2007) or service call-center dialogues (Berger et al., 2000). Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers has been presented in (Jeon et al., 2006). Bernhard and Gurevych (2009) have developed a translation based method to find answers. Surdeanu et al. (2008) propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. (2008), for we also aim to rank the candidate answers reasonably, but our ranking algorithm need"
P10-1125,C04-1128,0,0.00951994,"than the textual features. In this case, the mining of textual features tends to be ignored. There are also some other research topics in this field. Cong et al. (2008) and Wang et al. (2009) both propose the strategies to detect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. (2008). A graph based algorithm is presented to answer opinion questions (Li et al., 2009). In email summarization field, the QA pairs are also extracted from email contents as the main elements of email summarization (Shrestha and McKeown, 2004). 3 The Deep Belief Network for QA pairs Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic relevance between questions and answers using only co-occurrence features. It is clear that the semantic link exists between the question and its answers, even though they have totally different lexical representations. Thus a specially designed model may learn semantic knowledge by reconstructing a great number of questions using the information in the corresponding answers. In this section, we propose a deep belief network for model"
P10-1125,P08-1082,0,0.628011,"ers.yahoo.com 1230 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230–1238, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics answer semantic modeling. Most researchers try to introduce structural features or users’ behavior to improve the models performance, by contrast, the effect of textual features is not obvious. 2. How to train a model so that it has good performance on both cQA and forum datasets? So far, people have been doing QA researches on the cQA and the forum datasets separately (Ding et al., 2008; Surdeanu et al., 2008), and no one has noticed the relationship between the two kinds of data. Since both the cQA systems and the online forums are open platforms for people to communicate, the QA pairs in the cQA systems have similarity with those in the forums. In this case, it is highly valuable and desirable to propose a training strategy to improve the model’s performance on both of the two kinds of datasets. In addition, it is possible to avoid the expensive and arduous hand-annotating work by introducing the method. To solve the first problem, we present a deep belief network (DBN) to model the semantic rele"
P13-2146,P10-1125,1,0.600305,"Missing"
P13-2146,I11-1042,0,\N,Missing
P13-4012,P08-1081,0,0.0720162,"Missing"
P13-4012,C10-3004,0,\N,Missing
P13-4012,P12-3007,0,\N,Missing
P13-4012,P12-3006,0,\N,Missing
P14-2139,W06-1615,0,0.0434173,"Related works TTL has been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of ___________________ 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing"
P14-2139,P13-2084,0,0.0282056,"two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method (Jiang and Zhou, 2004) in transfer learning (Cheng and Li, 2009). His experiment showed that their approach would work when the source domain and the target domain share similar distributions. How to reduce negative transfers is still a problem in transfer learning. 3 Our Approach In"
P14-2139,P09-1027,0,0.379913,"are used beating the performance degradation curse of most transfer learning methods when training data reaches certain size. The rest of the paper is organized as follows. Section 2 introduces related works in transfer learning, cross lingual opinion analysis, and class noise detection technology. Section 3 presents our algorithm. Section 4 gives performance evaluation. Section 5 concludes this paper. 2 Related works TTL has been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of __"
P14-2139,W03-1730,0,0.0705095,"Missing"
P14-2139,P11-1033,0,0.0361894,"TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of ___________________ 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods"
P14-2139,P12-1060,0,0.431442,"focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of ___________________ 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s c"
P14-5005,P00-1031,0,0.0797473,"tistical Machine Translation (SMT) strategies. Compared with the above mentioned systems, WINGS is closer to retrieval-based writing assistants in terms of function. However, WINGS can provide more intelligent suggestions because of the introduction of NLP techniques, e.g., word vector representation and topic model. Related Work 2.1 Input Method Chinese input method is one of the most important tools for Chinese PC users. Nowadays, Pinyin-based input method is the most popular one. The main strategy that Pinyin-based input method uses is automatically converting Pinyin to Chinese characters (Chen and Lee, 2000). In recent years, more and more intelligent strategies have been adopted by different input methods, such as Triivi 6 , an English input method that attempts to increase writing speed by suggesting words and phrases, and PRIME (Komatsu et al., 2005), an English/Japanese input system that utilizes visited documents to predict the user’s next word to be input. In our system the basic process was Pinyin  Characters (words)  Writing Suggestions (including words and sentences). We mainly focused on writing suggestions from Characters (words) in this paper. As the Pinyin-to-Character was the unde"
P14-5005,P00-1067,0,\N,Missing
P14-5005,P12-3027,0,\N,Missing
P15-1130,P14-2111,0,0.0213239,"to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on a simple form known as Elman network (Elman, 1990). Recent works used RNNs to predict words or characters in a sequence (Chrupała, 2014; Zhang and Lapata, 2014). Treating opinion expression extraction as a sequence labelling 1344 h output hidden h input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. h h problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on using LSTM in predicting the semantic relatedness of two sentences and senti"
P15-1130,P11-1015,0,0.0236621,"y=f H X i ! bTi vi RXWSXW JDWH (5)  δht = f ′ ath 3.2 H X h′ δht+1 ′ whh′ LQSXW JDWH (6) Unsupervised trained word embeddings represent the syntactic and semantic similarity. However, in specific tasks, the importance and functions of different words vary. Negation words have similar unsupervised trained representations with other adverbs, but they make distinctive contributions in sentiment expressions. Besides the function words, tuning word vectors of sentiment words into polarity-representable ones turns out to be an effective way to improve the performance of sentiment classifiers. (Maas et al., 2011; Labutov and Lipson, 2013). Such tuned vectors work together with the deep models, gaining the ability to describe complex linguistic phenomena. RNN-TLT: To this end, we modify the word vectors in the Trainable Lookup-Table (TLT) via back propagation to get a better embedding of words. The gradient of lookup-table layer is: H  X h=1 δht wih = H X h=1 δht wih (7) where identity function g (x) = x is considered as the activation function of lookup-table layer. 3.3  Elman Network with Trainable Lookup-Table δit = g ′ ati IRUJHW JDWH &(& Then the gradients of hidden layer of previous time s"
P15-1130,S13-2052,0,0.0200388,"Missing"
P15-1130,W02-1011,0,0.0352316,"pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence information makes it difficult to figure out the polarity properly (Pang et al., 2002). A typical case is that the sentiment word in a negation phrase tends to express opposite sentiment to that of the context. Distributed representations of words can ease the sparseness, but there are limitations to the unsupervised-learned ones. Words with special functions in specific tasks are not distinguished. Such as negation words, which play a special role in polarity classification, are represented similarly with other adverbs. Such similarities will limit the compositional models’ abilities of describing a sentiment-specific interaction between words. Moreover, word vectors trained b"
P15-1130,P12-1092,0,0.0110122,"Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014)"
P15-1130,D14-1080,0,0.0811026,"alysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on a simple form known as Elman network (Elman, 1990). Recent works used RNNs to predict words or characters in a sequence (Chrupała, 2014; Zhang and Lapata, 2014). Treating opinion expression extraction as a sequence labelling 1344 h output hidden h input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. h h problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on using LSTM in predicting the semantic relatedness of two sentences and sentiment classification (Tai et al., 2015). Fig.1 shows the illustration of a recurrent network. By using self-connected layers, RNNs allow information cyclically encoded inside the networks. Such structures make it possible to get a fix-length representation of a whole tweet by temporally composing word vectors. The recurrent architecture"
P15-1130,P14-1062,0,0.0121458,"2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Yessenalina and Cardie, 2011). Recursive neural networks model contextual interaction in binary trees (Socher et al., 2011; Socher et al., 2013). Words in the complex utterances are considered as leaf nodes and composed in a bottomup fashion. However, it’s difficult to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a"
P15-1130,P13-2087,0,0.0107546,"RXWSXW JDWH (5)  δht = f ′ ath 3.2 H X h′ δht+1 ′ whh′ LQSXW JDWH (6) Unsupervised trained word embeddings represent the syntactic and semantic similarity. However, in specific tasks, the importance and functions of different words vary. Negation words have similar unsupervised trained representations with other adverbs, but they make distinctive contributions in sentiment expressions. Besides the function words, tuning word vectors of sentiment words into polarity-representable ones turns out to be an effective way to improve the performance of sentiment classifiers. (Maas et al., 2011; Labutov and Lipson, 2013). Such tuned vectors work together with the deep models, gaining the ability to describe complex linguistic phenomena. RNN-TLT: To this end, we modify the word vectors in the Trainable Lookup-Table (TLT) via back propagation to get a better embedding of words. The gradient of lookup-table layer is: H  X h=1 δht wih = H X h=1 δht wih (7) where identity function g (x) = x is considered as the activation function of lookup-table layer. 3.3  Elman Network with Trainable Lookup-Table δit = g ′ ati IRUJHW JDWH &(& Then the gradients of hidden layer of previous time steps can be recursively com"
P15-1130,D11-1014,0,0.545183,"ness, but there are limitations to the unsupervised-learned ones. Words with special functions in specific tasks are not distinguished. Such as negation words, which play a special role in polarity classification, are represented similarly with other adverbs. Such similarities will limit the compositional models’ abilities of describing a sentiment-specific interaction between words. Moreover, word vectors trained by cooccurrence statistics in a small window of context effectively represent the syntactic and semantic similarity. Thus, words like good and bad have very similar representations (Socher et al., 2011). It’s problematic for sentiment classifiers. Sentiment is expressed by phrases rather than by words (Socher et al., 2013). Seizing such sequence information would help to analyze complex sentiment expressions. One possible method to leverage context is connecting embeddings of words in a window and compose them to a fixlength vector (Collobert et al., 2011). However, window-based methods may have difficulty reaching long-distance words and simply connected vectors do not always represent the interactions of context properly. Theoretically, a recurrent neural network could process the whole se"
P15-1130,D13-1170,0,0.431042,"istinguished. Such as negation words, which play a special role in polarity classification, are represented similarly with other adverbs. Such similarities will limit the compositional models’ abilities of describing a sentiment-specific interaction between words. Moreover, word vectors trained by cooccurrence statistics in a small window of context effectively represent the syntactic and semantic similarity. Thus, words like good and bad have very similar representations (Socher et al., 2011). It’s problematic for sentiment classifiers. Sentiment is expressed by phrases rather than by words (Socher et al., 2013). Seizing such sequence information would help to analyze complex sentiment expressions. One possible method to leverage context is connecting embeddings of words in a window and compose them to a fixlength vector (Collobert et al., 2011). However, window-based methods may have difficulty reaching long-distance words and simply connected vectors do not always represent the interactions of context properly. Theoretically, a recurrent neural network could process the whole sentence of arbitrary length by encoding the context cyclically. However, the length of reachable context is often limited w"
P15-1130,W11-2207,0,0.0114892,"ng complex linguistic phenomena with gates and constant error carousels in the LSTM blocks. 2 Related Work 2.1 Microblogs Sentiment Analysis There have been a large amount of works on sentiment analysis over tweets. Some research makes use of social network information (Tan et al., 2011; Calais Guerra et al., 2011). These works reveal that social network relations of opinion holders could bring an influential bias to the textual models. While some other works utilize the microblogging features uncommon in the formal literature, such as hashtags, emoticons (Hu et al., 2013a; Liu et al., 2012). Speriosu et al. (2011) propose a unified graph propagation model to leverage textual features (such as emoticons) as well as social information. Semantic concept or entity based approaches lead another research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural"
P15-1130,P15-1150,0,0.0135004,"Missing"
P15-1130,P14-1146,0,0.0584451,"improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Yessenalina and Cardie"
P15-1130,P10-1040,0,0.014328,"r research direction. Saif et al. (2012a; 2012b) make use of sentiment-topic features and entities extracted by a third-party service to ease data sparsity. Aspect-based models are also exploited to improve the tweet-level classifier (Lek and Poo, 2013). 2.2 Representation Learning and Deep Models Bengio et al. (2003) use distributed representations for words to fight the curse of dimensionality when training a neural probabilistic language model. Such word vectors ease the syntactic and semantic sparsity of bag-of-words representations. Much recent research has explored such representations (Turian et al., 2010; Huang et al., 2012). Recent works reveal that modifying word vectors during training could capture polarity information for the sentiment words effectively (Socher et al., 2011; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalch"
P15-1130,P13-2090,0,0.0241317,"bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. 1 Introduction Twitter and other similar microblogs are rich resources for opinions on various kinds of products and events. Detecting sentiment in microblogs is a challenging task that has attracted increasing research interest in recent years (Hu et al., 2013b; Volkova et al., 2013). Go et al. (2009) carried out the pioneer work of predicting sentiment in tweets using machine learning technology. They conducted comprehensive experiments on multiple classifiers based on bag-of-words feature. Such feature is widely used because it’s simple and surprisingly efficient in many tasks. However, there are also disadvantages of bag-of-words features represented by onehot vectors. Firstly, it bears a data sparsity issue (Saif et al., 2012a). In tweets, irregularities and 140-character limitation exacerbate the sparseness. Secondly, losing sequence information makes it difficult to"
P15-1130,D11-1016,0,0.007408,"11; Tang et al., 2014). It would be also insightful to analyze the embeddings that changed the most during training. We conduct a comparison between initial and tuned vectors and show how the tuned vectors of task-distinctive function words cooperate with the proposed architecture to capture sequence information. Distributed word vectors help in various NLP tasks when using in neural models (Collobert et al., 2011; Kalchbrenner et al., 2014). Composing these representations to fix-length vectors that contain phrase or sentence level information also improves performance of sentiment analysis (Yessenalina and Cardie, 2011). Recursive neural networks model contextual interaction in binary trees (Socher et al., 2011; Socher et al., 2013). Words in the complex utterances are considered as leaf nodes and composed in a bottomup fashion. However, it’s difficult to get a binary tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks ("
P15-1130,D14-1074,0,0.0084991,"tree structure from the irregular short comments like tweets. Not requiring structure information or parser, long short-term memory models encode the context in a chain and accommodate complex linguistic phenomena with structure of gates and constant error carousels. 3 Recurrent Neural Networks for Sentiment Analysis Recurrent Neural Networks (RNN) have gained attention in NLP field since Mikolov et al. (2010) developed a statistical language model based on a simple form known as Elman network (Elman, 1990). Recent works used RNNs to predict words or characters in a sequence (Chrupała, 2014; Zhang and Lapata, 2014). Treating opinion expression extraction as a sequence labelling 1344 h output hidden h input t-1 t t+1 Figure 1: Illustration of simple recurrent neural network. The input of the hidden layer comes from both input layer and the hidden layer activations of previous time step. h h problem, Irsoy and Cardie (2014) leverage deep RNN models and achieve new state-of-the-art results for fine-grained extraction task. The lastest work propose a tree-structured LSTM and conduct a comprehensive study on using LSTM in predicting the semantic relatedness of two sentences and sentiment classification (Tai"
P15-2117,P08-1081,0,0.0257038,"ated Work Figure 2: The architecture of R-CNN Prior studies on answer selection generally treated this challenge as a classification problem via employing machine learning methods, which rely on exploring various features to represent QA pair. Huang et al. (2007) integrated textual features with structural features of forum threads to represent the candidate QA pairs, and used support vector machine (SVM) to classify the candidate pairs. Beyond typical features, Shah and Pomerantz (2010) trained a logistic regression (LR) classifier with user metadata to predict the quality of answers in CQA. Ding et al. (2008) proposed an approach based on conditional random fields (CRF), which can capture contextual features from the answer sequence for the semantic matching between question and answer. Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The translation-based methods suffer from the informal words or phrases in Q&A archives, and perform less applicability in new domains. 3 Approach We consider the answer selection problem in CQA as a sequence labeling task"
P15-2117,P13-2146,1,0.853873,"mation retrieval systems. To recognize matching answers for a question, typical approaches model semantic matching between question and answer by exploring various features (Wang et al., 2009a; Shah and Pomerantz, 2010). Some studies exploit syntactic tree structures (Wang et al., 2009b; Moschitti et al., 2007) to measure the semantic matching between question and answer. However, these approaches require high-quality data and various external resources which may be quite difficult to obtain. To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly. One disadvantage of these approaches lies in that Recently, recurrent neural network (RNN), especially Long Short-Term Memory (LSTM) (Hochreiter et al., 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence. And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art performance on sentiment"
P15-2117,P10-1125,1,0.642369,"struction and information retrieval systems. To recognize matching answers for a question, typical approaches model semantic matching between question and answer by exploring various features (Wang et al., 2009a; Shah and Pomerantz, 2010). Some studies exploit syntactic tree structures (Wang et al., 2009b; Moschitti et al., 2007) to measure the semantic matching between question and answer. However, these approaches require high-quality data and various external resources which may be quite difficult to obtain. To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly. One disadvantage of these approaches lies in that Recently, recurrent neural network (RNN), especially Long Short-Term Memory (LSTM) (Hochreiter et al., 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence. And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art perform"
P15-2117,D14-1181,0,0.00361001,"learn the distributed representation of question-answer pair directly. One disadvantage of these approaches lies in that Recently, recurrent neural network (RNN), especially Long Short-Term Memory (LSTM) (Hochreiter et al., 2001), has been proved superiority in various tasks (Sutskever et al., 2014; Srivastava et al., 2015) and it models long term and short term information of the sequence. And also, there are some works on using convolutional neural networks (CNNs) to learn the representations of sentence or short text, which achieve state-of-the-art performance on sentiment classification (Kim, 2014) and short text matching (Hu et al., 2014). In this paper, we address the answer selection problem as a sequence labeling task, which identifies the matching quality of each answer in the answer sequence of a question. Firstly, CNNs are used to learn the joint representation of question answer (QA) pair. Then the learnt joint repre∗ * Corresponding author 713 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 713–718, c Beijing, China, July 26-31, 2015. 2015 Asso"
P15-2117,S15-2047,0,0.0412824,"Missing"
P15-2117,P11-1066,0,0.0128192,"sed support vector machine (SVM) to classify the candidate pairs. Beyond typical features, Shah and Pomerantz (2010) trained a logistic regression (LR) classifier with user metadata to predict the quality of answers in CQA. Ding et al. (2008) proposed an approach based on conditional random fields (CRF), which can capture contextual features from the answer sequence for the semantic matching between question and answer. Additionally, the translation-based language model was also used for QA matching by transferring the answer to the corresponding question (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The translation-based methods suffer from the informal words or phrases in Q&A archives, and perform less applicability in new domains. 3 Approach We consider the answer selection problem in CQA as a sequence labeling task. To label the matching quality of each answer for a given question, our approach models the semantic links between successive answers, as well as the semantic relevance between question and answer. Figure 2 summarizes the recurrent architecture of our model (RCNN). The motivation of R-CNN is to learn the useful context to improve the performance of answer selection. The an"
P15-2117,P07-1098,0,0.0108037,"ntuitively, other answers of the question are beneficial to judge the quality of the current answer. Introduction Answer selection in community question answering (CQA), which recognizes high-quality responses to obtain useful question-answer pairs, is greatly valuable for knowledge base construction and information retrieval systems. To recognize matching answers for a question, typical approaches model semantic matching between question and answer by exploring various features (Wang et al., 2009a; Shah and Pomerantz, 2010). Some studies exploit syntactic tree structures (Wang et al., 2009b; Moschitti et al., 2007) to measure the semantic matching between question and answer. However, these approaches require high-quality data and various external resources which may be quite difficult to obtain. To take advantage of a large quantity of raw data, deep learning based approaches (Wang et al., 2010; Hu et al., 2013) are proposed to learn the distributed representation of question-answer pair directly. One disadvantage of these approaches lies in that Recently, recurrent neural network (RNN), especially Long Short-Term Memory (LSTM) (Hochreiter et al., 2001), has been proved superiority in various tasks (Su"
S15-2014,S12-1051,0,0.0288259,"enge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts similarity has become stronger in many new applications. In this paper, we proposed a SVM-based solution to compute the semantic similarity between two sentences which is the goal of SemEval 2015 Task 2. Knowledge-based and corpus-based features were involved in our solution. We used the combination of the word similarity to estimate sentence similarity. And the training data of SemEval 2012 (Agirre et al., 2012) was used to train our model. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. The evaluation results showed that our solution has good generalization ability on the test dataset of SemEval 2015 which is very different from our training set in terms of the sources of the sentences. Some of the relatively new technologies such as Word2Vec (Mikolov et al., 2013) and Sentence2Vec (Le and Mikolov, 2014) are potential methods to"
S15-2014,P06-4018,0,0.00628906,"al 2015), pages 80–84, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics 3 Feature engineering ܬሺܵଵ ǡ ܵଶ ሻ ൌ  ȁܵଵ ܵ תଶ ȁ ȁܵଵ ܵ ଶ ȁ Considering the training set used in our system, we were trying to generate features which have little relation with the sources where the sentences came from. Four kinds of features are included in our model. They are literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity. Where ܵଵ and ܵଶ are the collections of Part-OfSpeech tags of each sentence. We used the NLTK toolkit (Bird, 2006) to tag each sentence. Since Jaccard distance measure only cares about the appearance of the tags, and ignores the order of them, it can reduce the impact of the tense change. 3.1 3.3 Literal Similarity Intuitively, a pair of sentences that look similar to each other may be similar semantically. For example: S1: A boy is playing a guitar. S2: A man is playing a guitar. S3: Someone is drawing. Apparently, S1 and S2 look more similar and they are closer in semantics than S1 and S3. We chose the Edit Distance (also known as Levenshtein Distance) over characters to measure the similarity between t"
S15-2014,C04-1100,0,0.0340728,"English subtask). The system uses a support vector machine model with literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. 1 Introduction Semantic Text Similarity (STS) plays an important role in many Natural language processing tasks, such as Question Answering (Narayanan and Harabagiu, 2004), Machine Translation (Beale et al., 1995), Automatic Summarization (Wang et al., 2008) and Word Sense Disambiguation (Navigli and Velardi, 2005). Since STS is an essential challenge in NLP, that has attracted a significant amount of attention by the research community. SemEval has held tasks about STS for four years in a row, from which we can see the importance and difficulty of this challenge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts si"
S15-2014,niraula-etal-2014-dare,0,0.0455481,"Missing"
S15-2014,S12-1060,0,0.0427783,"Missing"
S15-2035,S15-2047,0,0.113265,"Missing"
S15-2037,P13-2146,1,0.120161,"ng positive answers, but they rely on large numbers of hand-crafted features, and require various external resources which may be difficult to obtain. Furthermore, they suffer from the limitation of requiring task-specific feature extraction for new domain. Recently the works about neural network-based distributed sentence models (Socher et al., 2012; Kalchbrenner et al., 2014) have achieved successes in natural language processing (NLP). As a consequence of this success, it appears natural to attempt to solve question answering using similar techniques. To recognize the high-quality answers, Hu et al. (2013) learned the joint representation for each question-answer pair 210 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 210–214, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Figure 1. The architecture of comment labeling system based on deep learning by taking both of the textual and non-textual features as the input of multi-DBN model. To achieve the answer sentence selection, Yu et al. (2014) proposed convolution neural networks based models to represent the question and answer sentences. For the semantic matching b"
S15-2037,S15-2047,0,0.0495026,"Missing"
S15-2037,P08-1081,0,0.0272022,"neural networks (R&CNN) approach to assign the labels to comments given a question. Based on the distributed representations learned form 2-dimensional CNN (2D-CNN) matching, our approach achieves to comment sequence learning and predict the classes of comments. Using the word embedding trained by provided Qatar Living data, R&CNN not only models the semantic relevance for question and comment, but also captures the correlative context in comment sequence for predicting the class of comment. The experimental results show that our system performs better performances than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptive on the development and test dataset. 2 with a recurrent neural network over the convolutional neural networks. Given a question, our approach achieves to learn the semantic relevance between question and comment by 2D-CNN matching and generate the distributed representation of each question-comment pair. After that, our approach uses the RNN to model the semantic correlations in comment sequence, and makes the quality predictions for the comment sequence with the captured context. 2.1 Convolutional Neural Networks question-comment match"
S15-2037,D12-1110,0,\N,Missing
S15-2037,P14-1062,0,\N,Missing
S15-2140,S10-1063,0,0.154862,"ral language understanding and so on. A lot of researches have been attracted on this topic in the past years. Many methods were proposed and many toolkits were implemented for temporal information annotation. TIMEN (Llorens et al., 2012a) is a communitydriven tool using rule-based method based on knowledge base to solve the temporal expression normalization problem. TARSQI Toolkit (Verhagen and Pustejovsky, 2008) is a modular system for automatic temporal information annotation. The toolkit can extract temporal expressions, events and recognize temporal relations by its different components. Llorens et al. (2010) used CRF models based on semantic information to annotate temporal information according to TimeML scheme, and their TIPSem system got outstanding performance results in TempEval-2. Steve (2013) piped machine-learning models in his ClearTK system to annotate temporal information using a small set of features. His system got best performance for temporal relation identification in TempEval-3. The TIMEN toolkit was integrated into the ClearTK system for temporal expression normalization. Llorens et al. (2012b) proposed an automatic method to improve the correctness of each individual annotation"
S15-2140,S15-2134,0,0.0904725,"n Abstract This paper presents the HITSZ-ICRC system designed for the QA TempEval challenge in SemEval-2015. The system used an integration approach to annotate temporal information by merging temporal annotation results from different temporal annotators. TIPSemB, ClearTK and TARSQI were used as temporal annotators to get candidate temporal annotation results. Evaluation demonstrated that our system was effective for improving the performance of temporal information annotation, and achieved recalls of 0.18, 0.26 and 0.19 on Blog, News and Wikipedeia test sets. 1 Introduction The QA TempEval (Llorens et al., 2015) in SemEval-2015 is a temporal information annotation challenge, which is a follow-up task after TempEval-1 (Verhagen et al., 2007), TempEval-2 (Verhagen et al., 2010) and TempEval-3(UzZaman et al., 2013). QA TempEval task is similar to the task ABC in TempEval-3, requires participant system (1) extracting and normalizing temporal expressions, (2) extracting events and (3) identifying temporal relations on plain documents. Temporal information annotation should follow TimeML scheme (Pustejovsky et al., 2003a). Difference between QA TempEval task and task ABC in TempEval-3 is evaluation method:"
S15-2140,C08-3012,0,0.204889,"ion-answering(QA) accuracy in the given temporal QA system (UzZaman et al., 2012) based on temporal knowledge produced from participant’s annotation. Temporal annotation is useful in information retrieval, QA, natural language understanding and so on. A lot of researches have been attracted on this topic in the past years. Many methods were proposed and many toolkits were implemented for temporal information annotation. TIMEN (Llorens et al., 2012a) is a communitydriven tool using rule-based method based on knowledge base to solve the temporal expression normalization problem. TARSQI Toolkit (Verhagen and Pustejovsky, 2008) is a modular system for automatic temporal information annotation. The toolkit can extract temporal expressions, events and recognize temporal relations by its different components. Llorens et al. (2010) used CRF models based on semantic information to annotate temporal information according to TimeML scheme, and their TIPSem system got outstanding performance results in TempEval-2. Steve (2013) piped machine-learning models in his ClearTK system to annotate temporal information using a small set of features. His system got best performance for temporal relation identification in TempEval-3."
S15-2140,S07-1014,0,0.0304378,"tegration approach to annotate temporal information by merging temporal annotation results from different temporal annotators. TIPSemB, ClearTK and TARSQI were used as temporal annotators to get candidate temporal annotation results. Evaluation demonstrated that our system was effective for improving the performance of temporal information annotation, and achieved recalls of 0.18, 0.26 and 0.19 on Blog, News and Wikipedeia test sets. 1 Introduction The QA TempEval (Llorens et al., 2015) in SemEval-2015 is a temporal information annotation challenge, which is a follow-up task after TempEval-1 (Verhagen et al., 2007), TempEval-2 (Verhagen et al., 2010) and TempEval-3(UzZaman et al., 2013). QA TempEval task is similar to the task ABC in TempEval-3, requires participant system (1) extracting and normalizing temporal expressions, (2) extracting events and (3) identifying temporal relations on plain documents. Temporal information annotation should follow TimeML scheme (Pustejovsky et al., 2003a). Difference between QA TempEval task and task ABC in TempEval-3 is evaluation method: in all previous TempEval tasks, annotated result was evaluated by the temporal information annotation accuracy based on manually a"
S15-2140,S13-2001,0,0.0673085,"notation results from different temporal annotators. TIPSemB, ClearTK and TARSQI were used as temporal annotators to get candidate temporal annotation results. Evaluation demonstrated that our system was effective for improving the performance of temporal information annotation, and achieved recalls of 0.18, 0.26 and 0.19 on Blog, News and Wikipedeia test sets. 1 Introduction The QA TempEval (Llorens et al., 2015) in SemEval-2015 is a temporal information annotation challenge, which is a follow-up task after TempEval-1 (Verhagen et al., 2007), TempEval-2 (Verhagen et al., 2010) and TempEval-3(UzZaman et al., 2013). QA TempEval task is similar to the task ABC in TempEval-3, requires participant system (1) extracting and normalizing temporal expressions, (2) extracting events and (3) identifying temporal relations on plain documents. Temporal information annotation should follow TimeML scheme (Pustejovsky et al., 2003a). Difference between QA TempEval task and task ABC in TempEval-3 is evaluation method: in all previous TempEval tasks, annotated result was evaluated by the temporal information annotation accuracy based on manually annotated test corpus; in QA TempEval, annotated result was evaluated by t"
S15-2140,S13-2002,0,0.0523387,"cting annotation results from all annotators using temporal annotation corrector; Step4: integrating all candidate annotation results to get final temporal annotation result using temporal result merger. The temporal information annotation process of our system is shown in figure 1. 2 Integration Approach for Temporal Information Annotation QA TempEval task required participant system to annotate temporal expressions, events and temporal relations following TimeML scheme. Many toolkits are available for temporal information annotation, such as TARSQI (Verhagen and Pustejovsky, 2008), ClearTK (Bethard, 2013) TIPSemB (Llorens et al., 2010) and so on. Each toolkit can be used as a temporal annotator to get candidate annotation result. But annotation results from current toolkits cannot be used for QA TempEval directly because some annotations do not in the TimeML format. For example, time expression normalization values in some results are in independent format, such as “20140804AF”, should be as “2014-08-04TAF”; some time expressions are not normalized and are set to “default_norm” or no value; some toolkits change source text content after annotating temporal information, such as changing adjacen"
S15-2140,llorens-etal-2012-timen,0,\N,Missing
S15-2140,S10-1010,0,\N,Missing
W06-0134,C04-1081,0,0.0994604,"Missing"
W06-0134,I05-3030,1,\N,Missing
W08-1601,J07-1004,0,0.0235196,"Missing"
W08-1601,W03-1605,0,0.0471032,"Missing"
W08-1601,P05-3006,0,0.0424751,"Missing"
W08-1601,N04-1008,0,\N,Missing
W08-1601,W01-1203,0,\N,Missing
W08-1601,P01-1070,0,\N,Missing
W08-2130,W08-2121,0,0.101062,"Missing"
W08-2130,J96-1002,0,0.00874004,"Missing"
W08-2130,W04-2705,0,\N,Missing
W08-2130,H05-1066,0,\N,Missing
W08-2130,J05-1004,0,\N,Missing
W09-1217,taule-etal-2008-ancora,0,0.051769,"Missing"
W09-1217,burchardt-etal-2006-salsa,0,0.0621901,"Missing"
W09-1217,kawahara-etal-2002-construction,0,0.0157413,"n problem, and the Maximum Entropy Models are used for them in our system. For semantic parsing and predicate classifying, we focus on finding optimized features on multiple languages. The average Macro F1 Score of our system is 73.97 for joint task in closed challenge. 1 2 Introduction The task for CoNLL-2009 is an extension of the CoNLL-2008 shared task to multiple languages: English (Surdeanu et al., 2008), Catalan plus Spanish (Mariona Taul´e et al., 2008), Chinese (Martha Palmer et al., 2009), Czech (Jan Hajiˇc et al., 2006), German (Aljoscha Burchardt et al., 2006) and Japanese (Daisuke Kawahara et al., 2002). Compared to the CoNLL-2008 shared task, the predicates are given for us in semantic dependencies task. Therefore, we have only need to label the semantic roles of nouns and verbs, and the frames of predicates. In this paper, a joint syntactic and semantic dependency parsing system submitted to the CoNLL109 System Description Generally Speaking, a syntactic and semantic dependency parsing system is usually divided into four separate subtasks: syntactic parsing, predicate identification, predicate classification, and semantic role labeling. In the CoNLL-2009 shared task, the predicate identifi"
W09-1217,W08-2130,1,0.8761,"Missing"
W09-1217,W08-2121,0,\N,Missing
W09-1217,W09-1201,0,\N,Missing
W09-1217,W08-2134,0,\N,Missing
W10-3002,W10-3001,0,0.331325,"Missing"
W10-3002,P09-2044,0,0.186821,"Missing"
W10-3002,W09-1304,0,0.237696,"icates a token inside of a chunk, and O indicates a token outside of a chunk. Then a two-layer cascaded classifier is built for prediction. There are a CRF classifier and a large margin-based classifier in the first layer and a CRF classifier in the second layer. In the first layer, the following features are used in our system: • The lemma and POS sequences of the hedge predicted by each classifier. • The times of a token classified into B cue, I cue and O cue by the first two classifiers. • Whether a token is the last token of the hedge predicted by each classifier. 2.2 We follow the way of Morante and Daelemans (2009) to represent the scope of a hedge, where F scope indicates a token at the beginning of a scope sequence, L scope indicates a token at the last of a scope sequence, and NONE indicates others. In this phase, we do preprocessing by GDep Tagger (version beta1)2 at first, which does lemma extraction, part-of-speech (POS), chunking, named entity recognition (NER) and dependency parse for feature extraction. For the output of GDep Tagger, we deal with the lemma and chunk tag using the same way mentioned in the last section. Then, a CRF classifier is built for prediction, which uses the following fea"
W10-3002,I05-2046,0,0.0674622,"Missing"
W10-3002,W08-0606,0,0.409641,"Missing"
W11-1724,esuli-sebastiani-2006-sentiwordnet,0,0.0287299,"Missing"
W11-1724,P07-1123,0,0.0327815,"nts, reviews and recommendations in different languages have been shared on the Internet. Accordingly, automated opinion analysis has attracted growing attentions. Opinion analysis, also known as sentiment analysis, sentiment classification, and opinion mining, aims to identify opinions in text and classify their sentiment polarity (Pang and Lee, 2008). Cross lingual opinion analysis (CLOA) techniques are investigated to improve opinion analysis in TL through leveraging the opinion-related resources, such as dictionaries and annotated corpus in SL. Some CLOA works used bilingual dictionaries (Mihalcea et al., 2007), or aligned corpus (Kim and Hovy, 2006) to align the expressions between source and target languages. These works are puzzled by the limited aligned opinion resources. Alternatively, some works used machine translation system to do the opinion expression alignment. Banea et al. (2008) proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language. Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two"
W11-1724,N06-1026,0,0.0295223,"nt languages have been shared on the Internet. Accordingly, automated opinion analysis has attracted growing attentions. Opinion analysis, also known as sentiment analysis, sentiment classification, and opinion mining, aims to identify opinions in text and classify their sentiment polarity (Pang and Lee, 2008). Cross lingual opinion analysis (CLOA) techniques are investigated to improve opinion analysis in TL through leveraging the opinion-related resources, such as dictionaries and annotated corpus in SL. Some CLOA works used bilingual dictionaries (Mihalcea et al., 2007), or aligned corpus (Kim and Hovy, 2006) to align the expressions between source and target languages. These works are puzzled by the limited aligned opinion resources. Alternatively, some works used machine translation system to do the opinion expression alignment. Banea et al. (2008) proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language. Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language,"
W11-1724,D08-1014,0,0.0864079,"n text and classify their sentiment polarity (Pang and Lee, 2008). Cross lingual opinion analysis (CLOA) techniques are investigated to improve opinion analysis in TL through leveraging the opinion-related resources, such as dictionaries and annotated corpus in SL. Some CLOA works used bilingual dictionaries (Mihalcea et al., 2007), or aligned corpus (Kim and Hovy, 2006) to align the expressions between source and target languages. These works are puzzled by the limited aligned opinion resources. Alternatively, some works used machine translation system to do the opinion expression alignment. Banea et al. (2008) proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language. Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. 182 Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 182–188, c 24 June, 2011, Portland, Oregon, USA 2011 Association for Computational Linguistics These works d"
W11-1724,P09-1027,0,0.614445,", such as dictionaries and annotated corpus in SL. Some CLOA works used bilingual dictionaries (Mihalcea et al., 2007), or aligned corpus (Kim and Hovy, 2006) to align the expressions between source and target languages. These works are puzzled by the limited aligned opinion resources. Alternatively, some works used machine translation system to do the opinion expression alignment. Banea et al. (2008) proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language. Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. 182 Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 182–188, c 24 June, 2011, Portland, Oregon, USA 2011 Association for Computational Linguistics These works directly used all of the translation of annotated corpus in source language as the training data for target language without considering the following two problems: (1) the machine translation errors propaga"
W11-1724,P07-1056,0,0.0222921,"guage training corpus by random. The opinion analysis results are evaluated with simplified Chinese testing dataset SCt under lenient and strict evaluation standard 2 , respectively, as described in (Seki et al., 2008). Note Lang. SCs SCt T Cs ENs SC TC EN subjective/objective Lenient Strict Training 424 130/294  Test 4877 1869/3008 898/2022 Training 1365 740/625  Training 1694 648/1046  Data Total Table 1: The NTCIR-7 MOAT Corpora(unit:sentence). In the document-level review polarity classification experiment,, we used the dataset adopted in (Wan, 2009). Its English subset is collected by Blitzer et al. (2007), which contains a collection of 8,000 product reviews about four types of products: books, DVDs, electronics and kitchen appliances. For each type of products, there are 1,000 positive reviews and 1,000 negative ones, respectively. The Chinese subset has 451 positive reviews and 435 negative reviews of electronics products such as mp3 players, mobile phones etc. In our experiments, the Chinese subset is further split into two parts randomly: TL training dataset and test set. The cross lingual review polarity classification task is then denoted by DocSC: EN→SC. In this study, Google Translate3"
W11-1724,D08-1058,0,0.344601,"Missing"
W12-4507,W11-1901,0,0.130573,"Missing"
W12-4507,W11-1906,0,0.348789,"Missing"
W12-4507,W12-4501,0,0.0983706,"Missing"
W12-4507,W11-1903,0,0.313376,"Missing"
W12-4507,P06-1005,0,0.372104,"Missing"
W12-4507,D10-1048,0,0.0555973,"Missing"
W12-4507,N09-1065,0,0.197838,"Missing"
W12-4507,W11-1904,0,\N,Missing
W12-4507,W11-1902,0,\N,Missing
W13-3616,C08-1109,0,0.049533,"r, for different samples, the distribution of predicted scores varies a lot. For some samples, the classifier may have a very high predicted score for a certain category which means the classifier is confident enough to perform this prediction. But for some other samples, two or more categories may share close scores, the case of which means the classifier hesitates when telling them apart. We introduce a confidence tuning approach on the predicted results through a comparison between the observed category and the predicted category which is similar to the “thresholding” approach described in Tetreault and Chodorow (2008). The main idea of the confidence tuning algorithm is: the choice between keep and drop is based on the difference between the confidence scores of the predicted category and the observed category. If this difference goes beyond a threshold t, the prediction is kept while if it is under t, we won’t do any corrections. We believe this tuning strategy is especially appropriate in this task since to distinguish whether the observed category is correct or not affects a lot to the predicted result. The confidence threshold for each category is generated through a hill climbing algorithm in the deve"
W13-3616,P08-1021,0,0.358867,"The other type is machine learning based approach which considers most on local context including syntactic and semantic features. Han et al. (2006) take maximum entropy as their classifier and apply some simple parameter tuning methods. Felice and Pulman (2008) present their classifier-based models together with a few representative features. Seo et al. (2012) invite a meta-learning approach and show its effectiveness. Dahlmeier and Ng (2011) introduce an alternating structure optimization based approach. Most of the works mentioned above focus on determiner and preposition errors. Besides, Lee and Seneff (2008) propose a method to correct verb form errors through combining the features of parse trees and n-gram counts. To our knowledge, no one focused on noun form errors in specific researches. In this paper, we propose a hybrid model to solve the problem of GEC for five error types. 1 http://clt.mq.edu.au/research/projects/hoo/hoo2012 115 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115–122, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics Machine learning based methods are applied to solve determiner (Art"
W13-3616,C10-1011,0,0.220842,"nsubj with the verb determines the form of this verb. Through observation, we find that the verbs to be considered in SVA contain only bes (including am, is, are, was, were) and the verbs in simple present tense whose POSs are labeled with VBZ (singular) or VBP(plural). To pick out the noun subject is easy except for the verb that contained in a subordinate clause. We use semantic role labeling (SRL) to help solve this problem in which the coordinated can be extracted through a trace with the label “RArgument”. The following Figure is an example generated by the SRL toolkit mate-tools (Bernd Bohnet, 2010)2. Figure 2. SRL for the demo sentence “Jack, who will show me the way, is very tall.” The subject of the verb show can be traced through R-A0 -> A0. However, the performance of this part is partly correlated with the noun form that may have errors in the original text and the wrong SRL result brought about because of wrong sentence grammars. 2 118 http://code.google.com/p/mate-tools/ 4.2 Verb Form The cases are more complicated in the verb form error correction task. Modal, aspect and voice are all forms that should be considered for a verb. And sometimes, two or more forms are combined toget"
W13-3616,P11-1092,0,0.0376199,"ethods also exist in HOO shared tasks1 such as the web 1TB n-gram features used by Dahlmeier and Ng (2012a) and the large-scale ngram model described by Heilman et al. (2012). The other type is machine learning based approach which considers most on local context including syntactic and semantic features. Han et al. (2006) take maximum entropy as their classifier and apply some simple parameter tuning methods. Felice and Pulman (2008) present their classifier-based models together with a few representative features. Seo et al. (2012) invite a meta-learning approach and show its effectiveness. Dahlmeier and Ng (2011) introduce an alternating structure optimization based approach. Most of the works mentioned above focus on determiner and preposition errors. Besides, Lee and Seneff (2008) propose a method to correct verb form errors through combining the features of parse trees and n-gram counts. To our knowledge, no one focused on noun form errors in specific researches. In this paper, we propose a hybrid model to solve the problem of GEC for five error types. 1 http://clt.mq.edu.au/research/projects/hoo/hoo2012 115 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Share"
W13-3616,W12-2025,0,0.0696013,"Missing"
W13-3616,W12-2027,0,0.0374082,"Missing"
W13-3616,D12-1052,0,0.0697444,"Missing"
W13-3616,W13-1703,0,0.120185,"Missing"
W13-3616,C08-1022,0,0.121805,"Missing"
W13-3616,P12-2064,0,0.0318575,"Missing"
W13-3616,I08-2082,0,0.0248687,"responding author terminer vs. noun number and preposition vs. verb form. Generally, for GEC on annotated data such as the NUCLE corpus (Dahlmeier et al., 2013) in this year’s shared task which contains both original errors and human annotations, there are two main types of approaches. One of them is the employment of external language materials. Although there are minor differences on strategies, the main idea of this approach is to use frequencies as a filter, such as n-gram counts, and take those phrases that have relatively high frequencies as the correct ones. Typical works are shown in (Yi et al., 2008) and (Bergsma et al., 2009). Similar methods also exist in HOO shared tasks1 such as the web 1TB n-gram features used by Dahlmeier and Ng (2012a) and the large-scale ngram model described by Heilman et al. (2012). The other type is machine learning based approach which considers most on local context including syntactic and semantic features. Han et al. (2006) take maximum entropy as their classifier and apply some simple parameter tuning methods. Felice and Pulman (2008) present their classifier-based models together with a few representative features. Seo et al. (2012) invite a meta-learning"
W13-3616,W00-1427,0,0.063949,"Missing"
W13-3616,N12-1067,0,\N,Missing
W13-3616,P05-1045,0,\N,Missing
W14-6808,P06-2008,0,0.174721,"work Oriented Intelligent Computation, Harbin Institute of Technology Shenzhen Graduate School, China † windseedxy@gmail.com ‡xiaoqiang.jeseph@gmail.com ing interactions and then providing feedbacks to the dialogue managers. Problematic situations reflect that a human user is not satisfied with answers that a conversational system offers. From one perspective, some of these un-satisfactions can be captured through a human user’s dialogue acts. For example, if a user repeats requesting the same question or frequently changes topics, it is likely that the system provides unsatisfactory answers (Chai et al., 2006). From another perspective, some explicit manners (i.e. sentiment-related expressions or dissatisfied feelings) that reflect the change of a user’s mentality would also indicate a problematic situation occurs. Some previous systems use surveys to capture users’ satisfactions: they let users to vote or evaluate whether the system has perfectly help them complete certain tasks (Hastie et al., 2002; Higashinaka et al., 2010) so as to collect users’ satisficing scores. However, for a real-world conversational application, there are very few users who are willing to provide this kind of feedbacks."
W14-6808,W09-3926,0,0.0630585,"Missing"
W14-6808,W11-2020,0,0.053084,"al. (2006) proposed the definition of user intent and incorporate a few matching features to predict utterance-level problematic situations (whether an immediate answer is satisfactory). Engelbrecht et al. (2009) employ the Hidden Markov Model (HMM) to model the whole dialogue into a sequence where each node of the sequence corresponds to the quality of the utterance. Higashinaka et al. (2010a; 2010b) also use HMM to model the good/bad sequence and testing the effects of turn-wise and overall ratings. Similar spirit also exists in (Hara et al., 2010). Support Vector Machines (SVM) are used by Schmitt et al. (2011) for the quality prediction on the CMU’s Let’s Go Bus Information system (Raux et al., 2006) and ASR features are compared in their experiments. 2.2 There are many factors that could affect the performance of judging whether a dialogue is problematic or not, i.e. time attributes like the total time of a dialogue and the time delays between utterances (Hastie et al., 2002; Walker et al., 2002; Möller et al., 2008), dialogue acts that may reflect user intents (Hastie et al., 2002) and users’ satisfaction ratings toward the system’s performance (Hastie et al., 2002). To avoid the side effects by"
W14-6808,P02-1049,0,0.412474,"be captured through a human user’s dialogue acts. For example, if a user repeats requesting the same question or frequently changes topics, it is likely that the system provides unsatisfactory answers (Chai et al., 2006). From another perspective, some explicit manners (i.e. sentiment-related expressions or dissatisfied feelings) that reflect the change of a user’s mentality would also indicate a problematic situation occurs. Some previous systems use surveys to capture users’ satisfactions: they let users to vote or evaluate whether the system has perfectly help them complete certain tasks (Hastie et al., 2002; Higashinaka et al., 2010) so as to collect users’ satisficing scores. However, for a real-world conversational application, there are very few users who are willing to provide this kind of feedbacks. The dialogue materials for this research come from a Chinese online chatting robot—BIT, which is developed for chatting and entertainment. It also integrates real-time data query functions about share price, weather report, post-code and telephone area code lookup. In addition to queries about real-time data, the corpus is totally open-domain and the number of topics that a dialogue could be rel"
W14-6808,W10-4304,0,0.0262215,"Missing"
W15-4415,C14-1028,0,0.0653043,"014) ant the task held in this year. Following, we briefly introduce some previous work related to Chinese grammatical error diagnosis. Wu et al. proposed two types of language models to detect the error types of word order, omission and redundant, corresponding to three of the types in the shared task. Chang et al. (2012) proposed a probabilistic first-order inductive learning algorithm for error classification and outperformed some basic classifiers. Lee et al. (2014) introduced a sentence level judgment system which integrated several predefined rules and N-gram based statistical features. Cheng et al. (2014) shown several methods including CRF and SVM, together with frequency learning from a large N-gram corpus, to detect and correct word ordering errors. In the last year’s shared task, there are also some novel ideas and results for the error diagnosis. Chang et al. (2014)’s work included manually constructed rules and rules that automatically generated, the latter of which are something like frequent patterns from the training corpus. Zhao et al. (2014)’s employed a parallel corpus from the web, which is a language exchange website called Lang-8, and used this corpus to training a statistical m"
W15-4415,C14-2015,0,0.0411766,"nough, and very few previous works are related to Chinese grammatical error correction. Typical ones are the CFL 2014 shared task (Yu et al., 2014) ant the task held in this year. Following, we briefly introduce some previous work related to Chinese grammatical error diagnosis. Wu et al. proposed two types of language models to detect the error types of word order, omission and redundant, corresponding to three of the types in the shared task. Chang et al. (2012) proposed a probabilistic first-order inductive learning algorithm for error classification and outperformed some basic classifiers. Lee et al. (2014) introduced a sentence level judgment system which integrated several predefined rules and N-gram based statistical features. Cheng et al. (2014) shown several methods including CRF and SVM, together with frequency learning from a large N-gram corpus, to detect and correct word ordering errors. In the last year’s shared task, there are also some novel ideas and results for the error diagnosis. Chang et al. (2014)’s work included manually constructed rules and rules that automatically generated, the latter of which are something like frequent patterns from the training corpus. Zhao et al. (2014"
W15-4415,W13-3601,0,0.0357688,"work and propose several directions for improvement. The following of this paper is organized as: Section 2 briefly introduces the literature in this community. Section 3 shows some observations towards the data provided. Section 4 introduces the feature extraction and learning methods we used for the shared task. Section 5 includes experiments and result analysis. And future work and conclusion are arranged at last. 2 Related Work In the community of grammatical error correction, more work focused on the language of English such as those researches during the CoNLL2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). A number of English language materials and annotated corpus can be used such that the research on this language went deeper. However, the resource for Chinese is far from enough, and very few previous works are related to Chinese grammatical error correction. Typical ones are the CFL 2014 shared task (Yu et al., 2014) ant the task held in this year. Following, we briefly introduce some previous work related to Chinese grammatical error diagnosis. Wu et al. proposed two types of language models to detect the error types of word order, omission and redundant, corresponding to"
W15-4415,W14-1701,0,\N,Missing
Y08-1051,W00-0737,0,0.0687747,"Missing"
Y08-1051,W00-0726,0,0.126142,"Missing"
Y08-1051,W00-0729,0,0.228237,"orks; graphical models; conditional random fields; support vector machines; generalization ability 1. Introduction Text chunking is an intermediate step towards full parsing, which consists of dividing a text in syntactically correlated parts of words. Tasks of chunking are extracting the non-overlapping segments from a stream of data and identifying them with non-recursive cores of various types of phrases. It can be solved as sequential labeling. Many probabilistic graphical models such as Hidden Markov Models (HMMs) (Zhou et al.,2000; Sang and Buchholz, 2000), Maximum Entropy Models (MEs) (Koeling, 2000), Conditional Random Fields (CRFs) (Lafferty et al., 2001), Semi-Markov Random Fields (Collins, 2002a) have been applied to chunking for their abilities to deal with structured data by taking advantages of the potential of interactions in a factored way (Jordan al., 1998). However, the condition of the probabilistic infinite samples assumption cannot be satisfied in practice and over fitting problem cannot be avoided. On the other hand, the tasks of chunking can be recognized as a classifying problem, statistic machine learning techniques are also often applied to chunking and various machine"
Y08-1051,W03-1728,0,0.0643982,"Missing"
Y08-1051,W95-0107,0,0.0994109,"on for text chunking — Inside/Outside representation. In order to describe the chunking more precisely, Uchimotoetal proposes a new representation for Japanese named entity extraction task (Uchimotoetal., 2000), and Xue introduces another new representation for Chinese segmentation task (Xue, 2003). We called them as Start/End representation. These two types are mentioned in (Taku Kudo and Yuji Matsumoto, 2000). In this paper, a new Start/End presentation will be introduced into chunking. 1、Inside/Outside This representation uses the following set of three tags for representing proper chunks (Ramshaw and Marcus, 1995). I Current token is inside of a chunk. O Current token is outside of any chunk. B Current token is the beginning of a chunk which immediately follows another chunk. Tjong Kim Sang calls this method as IOB1 representation, and introduces three alternative versions — IOB2, IOE1and IOE2 (Tjong Kim Sang and Veenstra, 1999). IOB2 A B tag is given for every token at the beginning of a chunk. Other tokens are the same as IOB1. IOE1 An E tag is used to mark the last token of a chunk immediately preceding another chunk. IOE2 An E tag is given for every token at the end of a chunk. 2、Start/End This rep"
Y08-1051,P00-1042,0,0.0611872,"Missing"
Y08-1051,W02-1001,0,0.10571,"Introduction Text chunking is an intermediate step towards full parsing, which consists of dividing a text in syntactically correlated parts of words. Tasks of chunking are extracting the non-overlapping segments from a stream of data and identifying them with non-recursive cores of various types of phrases. It can be solved as sequential labeling. Many probabilistic graphical models such as Hidden Markov Models (HMMs) (Zhou et al.,2000; Sang and Buchholz, 2000), Maximum Entropy Models (MEs) (Koeling, 2000), Conditional Random Fields (CRFs) (Lafferty et al., 2001), Semi-Markov Random Fields (Collins, 2002a) have been applied to chunking for their abilities to deal with structured data by taking advantages of the potential of interactions in a factored way (Jordan al., 1998). However, the condition of the probabilistic infinite samples assumption cannot be satisfied in practice and over fitting problem cannot be avoided. On the other hand, the tasks of chunking can be recognized as a classifying problem, statistic machine learning techniques are also often applied to chunking and various machine learning approaches have been proposed for chunking such as SVMs (Cortes and Vapnik, 1995; Vapnik, 1"
Y08-1051,W96-0213,0,0.222616,"Missing"
Y08-1051,N01-1025,0,0.0719109,"Missing"
Y15-1006,S15-2045,0,0.0422242,"Missing"
Y15-1006,S12-1059,0,0.06988,"Missing"
Y15-1006,P06-4018,0,0.0136687,"Missing"
Y15-1006,W04-2502,0,0.0617941,"Missing"
Y15-1006,P02-1040,0,0.0918698,"Missing"
Y15-1006,S12-1060,0,0.0555482,"Missing"
Y15-1006,stefanescu-etal-2014-latent,0,0.0438937,"Missing"
Y15-1006,Q14-1018,0,0.0434042,"Missing"
Y15-1006,P94-1019,0,\N,Missing
Y15-1006,S14-2039,0,\N,Missing
Y15-1006,S13-1005,0,\N,Missing
