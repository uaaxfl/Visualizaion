2021.nodalida-main.28,{NLI} Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance,2021,-1,-1,4,1,2672,aarne talman,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a dataset constitutes a good testbed for evaluating the models{'} meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction. Inversely, a large decrease in model accuracy indicates that the original dataset provides a proper challenge to the models{'} reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks."
2021.nodalida-main.37,Boosting Neural Machine Translation from {F}innish to {N}orthern {S}{\\'a}mi with Rule-Based Backtranslation,2021,-1,-1,4,1,2697,mikko aulamo,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"We consider a low-resource translation task from Finnish into Northern S{\'a}mi. Collecting all available parallel data between the languages, we obtain around 30,000 sentence pairs. However, there exists a significantly larger monolingual Northern S{\'a}mi corpus, as well as a rule-based machine translation (RBMT) system between the languages. To make the best use of the monolingual data in a neural machine translation (NMT) system, we use the backtranslation approach to create synthetic parallel data from it using both NMT and RBMT systems. Evaluating the results on an in-domain test set and a small out-of-domain set, we find that the RBMT backtranslation outperforms NMT backtranslation clearly for the out-of-domain test set, but also slightly for the in-domain data, for which the NMT backtranslation model provided clearly better BLEU scores than the RBMT. In addition, combining both backtranslated data sets improves the RBMT approach only for the in-domain test set. This suggests that the RBMT system provides general-domain knowledge that cannot be found from the relative small parallel training data."
2021.konvens-1.25,Towards a balanced annotated Low {S}axon dataset for diachronic investigation of dialectal variation,2021,-1,-1,3,0,5583,janine siewert,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.emnlp-main.664,An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation,2021,-1,-1,4,1,4093,alessandro raganato,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results. In this paper, we investigate the benefits of an explicit alignment to language labels in Transformer-based MNMT models in the zero-shot context, by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label. We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs."
2021.bsnlp-1.8,Creating an Aligned {R}ussian Text Simplification Dataset from Language Learner Data,2021,-1,-1,2,0,12042,anna dmitrieva,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,0,"Parallel language corpora where regular texts are aligned with their simplified versions can be used in both natural language processing and theoretical linguistic studies. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for English and Simple English, but many other languages lack such data. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of Russian literature texts adapted for learners of Russian as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general."
2021.americasnlp-1.29,The {H}elsinki submission to the {A}mericas{NLP} shared task,2021,-1,-1,4,1,9977,raul vazquez,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,0,"The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail."
2021.acl-srw.35,On the differences between {BERT} and {MT} encoder spaces and how to address them in translation tasks,2021,-1,-1,4,1,9977,raul vazquez,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Various studies show that pretrained language models such as BERT cannot straightforwardly replace encoders in neural machine translation despite their enormous success in other tasks. This is even more astonishing considering the similarities between the architectures. This paper sheds some light on the embedding spaces they create, using average cosine similarity, contextuality metrics and measures for representational similarity for comparison, revealing that BERT and NMT encoder representations look significantly different from one another. In order to address this issue, we propose a supervised transformation from one into the other using explicit alignment and fine-tuning. Our results demonstrate the need for such a transformation to improve the applicability of BERT in MT."
2020.wmt-1.40,The {MUCOW} word sense disambiguation test suite at {WMT} 2020,2020,-1,-1,3,0,263,yves scherrer,Proceedings of the Fifth Conference on Machine Translation,0,"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. MUCOW is created automatically using existing resources, and the evaluation process is also entirely automated. We evaluate all participating systems of the language pairs English -{\textgreater} Czech, English -{\textgreater} German, and English -{\textgreater} Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress - at least to the extent that it is measurable by the MUCOW method - in that area over the last year."
2020.wmt-1.139,The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT},2020,-1,-1,1,1,2675,jorg tiedemann,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the development of a new benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages and tools for creating state-of-the-art translation models from that collection. The main goal is to trigger the development of open translation tools and models with a much broader coverage of the World{'}s languages. Using the package it is possible to work on realistic low-resource scenarios avoiding artificially reduced setups that are common when demonstrating zero-shot or few-shot learning. For the first time, this package provides a comprehensive collection of diverse data sets in hundreds of languages with systematic language and script annotation and data splits to extend the narrow coverage of existing benchmarks. Together with the data release, we also provide a growing number of pre-trained baseline models for individual language pairs and selected language groups."
2020.vardial-1.3,{LSDC} - A comprehensive dataset for Low {S}axon Dialect Classification,2020,-1,-1,4,0,5583,janine siewert,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"We present a new comprehensive dataset for the unstandardised West-Germanic language Low Saxon covering the last two centuries, the majority of modern dialects and various genres, which will be made openly available in connection with the final version of this paper. Since so far no such comprehensive dataset of contemporary Low Saxon exists, this provides a great contribution to NLP research on this language. We also test the use of this dataset for dialect classification by training a few baseline models comparing statistical and neural approaches. The performance of these models shows that in spite of an imbalance in the amount of data per dialect, enough features can be learned for a relatively high classification accuracy."
2020.semeval-1.205,{LT}@{H}elsinki at {S}em{E}val-2020 Task 12: Multilingual or Language-specific {BERT}?,2020,-1,-1,4,0,15295,marc pamies,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper presents the different models submitted by the LT@Helsinki team for the SemEval 2020 Shared Task 12. Our team participated in sub-tasks A and C; titled offensive language identification and offense target identification, respectively. In both cases we used the so-called Bidirectional Encoder Representation from Transformer (BERT), a model pre-trained by Google and fine-tuned by us on the OLID and SOLID datasets. The results show that offensive tweet classification is one of several language-based tasks where BERT can achieve state-of-the-art results."
2020.lrec-1.452,An Evaluation Benchmark for Testing the Word Sense Disambiguation Capabilities of Machine Translation Systems,2020,-1,-1,3,1,4093,alessandro raganato,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Lexical ambiguity is one of the many challenging linguistic phenomena involved in translation, i.e., translating an ambiguous word with its correct sense. In this respect, previous work has shown that the translation quality of neural machine translation systems can be improved by explicitly modeling the senses of ambiguous words. Recently, several evaluation test sets have been proposed to measure the word sense disambiguation (WSD) capability of machine translation systems. However, to date, these evaluation test sets do not include any training data that would provide a fair setup measuring the sense distributions present within the training data itself. In this paper, we present an evaluation benchmark on WSD for machine translation for 10 language pairs, comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW."
2020.lrec-1.467,{O}pus{T}ools and Parallel Corpus Diagnostics,2020,-1,-1,4,1,2697,mikko aulamo,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper introduces OpusTools, a package for downloading and processing parallel corpora included in the OPUS corpus collection. The package implements tools for accessing compressed data in their archived release format and make it possible to easily convert between common formats. OpusTools also includes tools for language identification and data filtering as well as tools for importing data from various sources into the OPUS format. We show the use of these tools in parallel corpus creation and data diagnostics. The latter is especially useful for the identification of potential problems and errors in the extensive data set. Using these tools, we can now monitor the validity of data sets and improve the overall quality and consitency of the data collection."
2020.lrec-1.470,"The {FISKM{\\\O}} Project: Resources and Tools for {F}innish-{S}wedish Machine Translation and Cross-Linguistic Research""",2020,-1,-1,1,1,2675,jorg tiedemann,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper presents FISKM{\""O}, a project that focuses on the development of resources and tools for cross-linguistic research and machine translation between Finnish and Swedish. The goal of the project is the compilation of a massive parallel corpus out of translated material collected from web sources, public and private organisations and language service providers in Finland with its two official languages. The project also aims at the development of open and freely accessible translation services for those two languages for the general purpose and for domain-specific use. We have released new data sets with over 3 million translation units, a benchmark test set for MT development, pre-trained neural MT models with high coverage and competitive performance and a self-contained MT plugin for a popular CAT tool. The latter enables offline translation without dependencies on external services making it possible to work with highly sensitive data without compromising security concerns."
2020.iwslt-1.10,The {U}niversity of {H}elsinki Submission to the {IWSLT}2020 Offline {S}peech{T}ranslation Task,2020,0,0,4,1,9977,raul vazquez,Proceedings of the 17th International Conference on Spoken Language Translation,0,"This paper describes the University of Helsinki Language Technology group{'}s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text. In line with this year{'}s task objective, we train both cascade and end-to-end systems for spoken language translation. We opt for an end-to-end multitasking architecture with shared internal representations and a cascade approach that follows a standard procedure consisting of ASR, correction, and MT stages. We also describe the experiments that served as a basis for the submitted systems. Our experiments reveal that multitasking training with shared internal representations is not only possible but allows for knowledge-transfer across modalities."
2020.findings-emnlp.49,Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation,2020,60,1,3,1,4093,alessandro raganato,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed {--} non-learnable {--} attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios."
2020.eamt-1.13,{MT} for subtitling: User evaluation of post-editing productivity,2020,-1,-1,4,0,20837,maarit koponen,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"This paper presents a user evaluation of machine translation and post-editing for TV subtitles. Based on a process study where 12 professional subtitlers translated and post-edited subtitles, we compare effort in terms of task time and number of keystrokes. We also discuss examples of specific subtitling features like condensation, and how these features may have affected the post-editing results. In addition to overall MT quality, segmentation and timing of the subtitles are found to be important issues to be addressed in future work."
2020.eamt-1.61,{OPUS}-{MT} {--} Building open translation services for the World,2020,-1,-1,1,1,2675,jorg tiedemann,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"This paper presents OPUS-MT a project that focuses on the development of free resources and tools for machine translation. The current status is a repository of over 1,000 pre-trained neural machine translation models that are ready to be launched in on-line translation services. For this we also provide open source implementations of web applications that can run efficiently on average desktop hardware with a straightforward setup and installation."
2020.coling-main.575,{XED}: A Multilingual Dataset for Sentiment Analysis and Emotion Detection,2020,-1,-1,4,1,15296,emily ohman,Proceedings of the 28th International Conference on Computational Linguistics,0,"We introduce XED, a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. We use Plutchik{'}s core emotions to annotate the dataset with the addition of neutral to create a multilabel multiclass dataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to show that XED performs on par with other similar datasets and is therefore a useful tool for sentiment analysis and emotion detection."
2020.cl-2.5,A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation,2020,4,0,4,1,9977,raul vazquez,Computational Linguistics,0,"Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence representations by incorporating an intermediate crosslingual shared layer, which we refer to as attention bridge. This layer exploits the semantics from each language and develops into a language-agnostic meaning representation that can be efficiently used for transfer learning. We systematically study the impact of the size of the attention bridge and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that there is no conflict between translation performance and the use of sentence representations in downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. Nevertheless, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. Similarly, we show that trainable downstream tasks benefit from multilingual models, whereas additional language signals do not improve performance in non-trainable benchmarks. This is an important insight that helps to properly design models for specific applications. Finally, we also include an in-depth analysis of the proposed attention bridge and its ability to encode linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks."
2020.blackboxnlp-1.13,Controlling the Imprint of Passivization and Negation in Contextualized Representations,2020,-1,-1,3,0,12485,hande celikkanat,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of embeddings for word instances having the same meaning. We explore the imprint of two specific linguistic alternations, namely passivization and negation, on the representations generated by neural models trained with two different objectives: masked language modeling and translation. Our exploration methodology is inspired by an approach previously proposed for removing societal biases from word vectors. We show that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation. We also find clear differences in how the respective features generalize across datasets."
2020.amta-pemdt.6,{MT} for Subtitling: Investigating professional translators{'} user experience and feedback,2020,-1,-1,4,0,20837,maarit koponen,Proceedings of 1st Workshop on Post-Editing in Modern-Day Translation,0,None
2020.acl-demos.20,{O}pus{F}ilter: A Configurable Parallel Corpus Filtering Toolbox,2020,-1,-1,3,1,2697,mikko aulamo,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation."
W19-6129,Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,2019,23,1,5,1,2672,aarne talman,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text. To our knowledge this will be the largest publicly available dataset with prosodic labels. We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature-based classifiers to neural network systems for the prediction of discretized prosodic prominence. We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10{\%} of the training data. Finally we discuss the dataset in light of the results and point to future research and plans for further improving both the dataset and methods of predicting prosodic prominence from text. The dataset and the code for the models will be made publicly available.
W19-6146,The {OPUS} Resource Repository: An Open Package for Creating Parallel Corpora and Machine Translation Services,2019,0,2,2,1,2697,mikko aulamo,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"This paper presents a flexible and powerful system for creating parallel corpora and for running neural machine translation services. Our package provides a scalable data repository backend that offers transparent data pre-processing pipelines and automatic alignment procedures that facilitate the compilation of extensive parallel data sets from a variety of sources. Moreover, we develop a web-based interface that constitutes an intuitive frontend for end-users of the platform. The whole system can easily be distributed over virtual machines and implements a sophisticated permission system with secure connections and a flexible database for storing arbitrary metadata. Furthermore, we also provide an interface for neural machine translation that can run as a service on virtual machines, which also incorporates a connection to the data repository software."
W19-5441,The {U}niversity of {H}elsinki Submission to the {WMT}19 Parallel Corpus Filtering Task,2019,0,0,3,1,9977,raul vazquez,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"This paper describes the University of Helsinki Language Technology group{'}s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of filters to remove the {`}bad{'} quality sentences. Then, we produced scores for each sentence by weighting these features with a classification model. This methodology allowed us to build a simple and reliable system that is easily adaptable to other language pairs."
W19-5347,The {U}niversity of {H}elsinki Submissions to the {WMT}19 News Translation Task,2019,22,0,8,1,2672,aarne talman,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs: English-German, English-Finnish and Finnish-English. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish."
W19-5354,The {M}u{C}o{W} Test Suite at {WMT} 2019: Automatically Harvested Multilingual Contrastive Word Sense Disambiguation Test Sets for Machine Translation,2019,0,2,3,1,4093,alessandro raganato,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"Supervised Neural Machine Translation (NMT) systems currently achieve impressive translation quality for many language pairs. One of the key features of a correct translation is the ability to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types. We present MuCoW, a multilingual contrastive test suite that covers 16 language pairs with more than 200 thousand contrastive sentence pairs, automatically built from word-aligned parallel corpora and the wide-coverage multilingual sense inventory of BabelNet. We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using NMT pretrained models. The MuCoW test suite is available at http://github.com/Helsinki-NLP/MuCoW."
W19-4304,An Evaluation of Language-Agnostic Inner-Attention-Based Representations in Machine Translation,2019,0,1,4,1,4093,alessandro raganato,Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019),0,"In this paper, we explore a multilingual translation model with a cross-lingually shared layer that can be used as fixed-size sentence representation in different downstream tasks. We systematically study the impact of the size of the shared layer and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that the performance in translation does correlate with trainable downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. On the other hand, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. We hypothesize that the training procedure on the downstream task enables the model to identify the encoded information that is useful for the specific task whereas non-trainable benchmarks can be confused by other types of information also encoded in the representation of a sentence."
W19-4305,Multilingual {NMT} with a Language-Independent Attention Bridge,2019,0,3,3,1,9977,raul vazquez,Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019),0,"In this paper, we propose an architecture for machine translation (MT) capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the semantics from each language for translation and develops into a language-agnostic meaning representation that can efficiently be used for transfer learning. We present a new framework for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning."
W19-2509,Revisiting {NMT} for Normalization of Early {E}nglish Letters,2019,0,0,4,0.289855,107,mika hamalainen,"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"This paper studies the use of NMT (neural machine translation) as a normalization method for an early English letter corpus. The corpus has previously been normalized so that only less frequent deviant forms are left out without normalization. This paper discusses different methods for improving the normalization of these deviant forms by using different approaches. Adding features to the training data is found to be unhelpful, but using a lexicographical resource to filter the top candidates produced by the NMT model together with lemmatization improves results."
W19-2005,Measuring Semantic Abstraction of Multilingual {NMT} with Paraphrase Recognition and Generation Tasks,2019,0,1,1,1,2675,jorg tiedemann,Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {NLP},0,"In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language. The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation. This is further supported by a study on paraphrase generation that we also include at the end of the paper."
J19-2006,What Do Language Representations Really Represent?,2019,21,1,4,0,996,johannes bjerva,Computational Linguistics,0,"A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just as it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, whereas genetic relationships{---}a convenient benchmark used for evaluation in previous work{---}appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another."
D19-6506,Analysing concatenation approaches to document-level {NMT} in two different domains,2019,0,1,2,0,263,yves scherrer,Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019),0,"In this paper, we investigate how different aspects of discourse context affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore, we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context, arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the models are indeed affected by the manipulation of the test data, providing a different view on document-level translation quality than absolute sentence-level scores."
W18-6425,The {U}niversity of {H}elsinki submissions to the {WMT}18 news task,2018,0,4,5,1,4093,alessandro raganato,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the University of Helsinki{'}s submissions to the WMT18 shared news translation task for English-Finnish and English-Estonian, in both directions. This year, our main submissions employ a novel neural architecture, the Transformer, using the open-source OpenNMT framework. Our experiments couple domain labeling and fine tuned multilingual models with shared vocabularies between the source and target language, using the provided parallel data of the shared task and additional back-translations. Finally, we compare, for the English-to-Finnish case, the effectiveness of different machine translation architectures, starting from a rule-based approach to our best neural model, analyzing the output and highlighting future research."
W18-6439,The {M}e{MAD} Submission to the {WMT}18 Multimodal Translation Task,2018,0,10,9,0,13980,stigarne gronroos,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the MeMAD project entry to the WMT Multimodal Machine Translation Shared Task. We propose adapting the Transformer neural machine translation (NMT) architecture to a multi-modal setting. In this paper, we also describe the preliminary experiments with text-only translation systems leading us up to this choice. We have the top scoring system for both English-to-German and English-to-French, according to the automatic metrics for flickr18. Our experiments show that the effect of the visual features in our system is small. Our largest gains come from the quality of the underlying text-only NMT system. We find that appropriate use of additional data is effective."
W18-6205,Creating a Dataset for Multilingual Fine-grained Emotion-detection Using Gamification-based Annotation,2018,0,2,3,1,15296,emily ohman,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"This paper introduces a gamified framework for fine-grained sentiment analysis and emotion detection. We present a flexible tool, \textit{Sentimentator}, that can be used for efficient annotation based on crowd sourcing and a self-perpetuating gold standard. We also present a novel dataset with multi-dimensional annotations of emotions and sentiments in movie subtitles that enables research on sentiment preservation across languages and the creation of robust multilingual emotion detection tools. The tools and datasets are public and open-source and can easily be extended and applied for various purposes."
W18-5431,An Analysis of Encoder Representations in Transformer-Based Machine Translation,2018,0,23,2,1,4093,alessandro raganato,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the \textit{Transformer} is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics."
W18-4510,Normalizing Early {E}nglish Letters to Present-day {E}nglish Spelling,2018,-1,-1,4,0.289855,107,mika hamalainen,"Proceedings of the Second Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"This paper presents multiple methods for normalizing the most deviant and infrequent historical spellings in a corpus consisting of personal correspondence from the 15th to the 19th century. The methods include machine translation (neural and statistical), edit distance and rule-based FST. Different normalization methods are compared and evaluated. All of the methods have their own strengths in word normalization. This calls for finding ways of combining the results from these methods to leverage their individual strengths."
W18-3901,Language Identification and Morphosyntactic Tagging: The Second {V}ar{D}ial Evaluation Campaign,2018,0,13,10,0.233954,622,marcos zampieri,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"We present the results and the findings of the Second VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects. The campaign was organized as part of the fifth edition of the VarDial workshop, collocated with COLING{'}2018. This year, the campaign included five shared tasks, including two task re-runs {--} Arabic Dialect Identification (ADI) and German Dialect Identification (GDI) {--}, and three new tasks {--} Morphosyntactic Tagging of Tweets (MTT), Discriminating between Dutch and Flemish in Subtitles (DFS), and Indo-Aryan Language Identification (ILI). A total of 24 teams submitted runs across the five shared tasks, and contributed 22 system description papers, which were included in the VarDial workshop proceedings and are referred to in this report."
L18-1275,"{O}pen{S}ubtitles2018: Statistical Rescoring of Sentence Alignments in Large, Noisy Parallel Corpora",2018,0,16,2,0.543546,2642,pierre lison,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-4801,Findings of the 2017 {D}isco{MT} Shared Task on Cross-lingual Pronoun Prediction,2017,0,3,5,0,4241,sharid loaiciga,Proceedings of the Third Workshop on Discourse in Machine Translation,0,"We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the target-language lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document. We offered four subtasks, each for a different language pair and translation direction: English-to-French, English-to-German, German-to-English, and Spanish-to-English. Five teams participated in the shared task, making submissions for all language pairs. The evaluation results show that most participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin."
W17-4811,Neural Machine Translation with Extended Context,2017,18,0,1,1,2675,jorg tiedemann,Proceedings of the Third Workshop on Discourse in Machine Translation,0,"We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases."
W17-4731,Rule-based Machine translation from {E}nglish to {F}innish,2017,0,2,2,0,23882,arvi hurskainen,Proceedings of the Second Conference on Machine Translation,0,None
W17-4733,The {H}elsinki Neural Machine Translation System,2017,0,6,3,0.426469,655,robert ostling,Proceedings of the Second Conference on Machine Translation,0,None
W17-1201,Findings of the {V}ar{D}ial Evaluation Campaign 2017,2017,0,26,6,0.254031,622,marcos zampieri,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"We present the results of the VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects, which we organized as part of the fourth edition of the VarDial workshop at EACL{'}2017. This year, we included four shared tasks: Discriminating between Similar Languages (DSL), Arabic Dialect Identification (ADI), German Dialect Identification (GDI), and Cross-lingual Dependency Parsing (CLP). A total of 19 teams submitted runs across the four tasks, and 15 of them wrote system description papers."
W17-1216,Cross-lingual dependency parsing for closely related languages - {H}elsinki{'}s submission to {V}ar{D}ial 2017,2017,7,1,1,1,2675,jorg tiedemann,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, Slovak seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the fully supervised models trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the baseline are rather modest. Norwegian works best with information coming from Swedish whereas Danish contributes surprisingly little."
I17-1018,Character-based Joint Segmentation and {POS} Tagging for {C}hinese using Bidirectional {RNN}-{CRF},2017,23,2,3,0.740741,28950,yan shao,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"We present a character-based model for joint segmentation and POS tagging for Chinese. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and lower-than-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our model is accurate and robust across datasets in different sizes, genres and annotation schemes. We obtain state-of-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and POS tagging."
E17-2102,Continuous multilinguality with language vectors,2017,5,23,2,0.426469,655,robert ostling,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages."
W16-4801,Discriminating between Similar Languages and {A}rabic Dialect Identification: A Report on the Third {DSL} Shared Task,2016,0,47,6,0.0716545,3599,shervin malmasi,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"We present the results of the third edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the VarDial{'}2016 workshop at COLING{'}2016. The challenge offered two subtasks: subtask 1 focused on the identification of very similar languages and language varieties in newswire texts, whereas subtask 2 dealt with Arabic dialect identification in speech transcripts. A total of 37 teams registered to participate in the task, 24 teams submitted test results, and 20 teams also wrote system description papers. High-order character n-grams were the most successful feature, and the best classification approaches included traditional supervised learning methods such as SVM, logistic regression, and language models, while deep learning approaches did not perform very well."
W16-4315,The Challenges of Multi-dimensional Sentiment Analysis Across Languages,2016,10,0,3,1,15296,emily ohman,"Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",0,"This paper outlines a pilot study on multi-dimensional and multilingual sentiment analysis of social media content. We use parallel corpora of movie subtitles as a proxy for colloquial language in social media channels and a multilingual emotion lexicon for fine-grained sentiment analyses. Parallel data sets make it possible to study the preservation of sentiments and emotions in translation and our assessment reveals that the lexical approach shows great inter-language agreement. However, our manual evaluation also suggests that the use of purely lexical methods is limited and further studies are necessary to pinpoint the cross-lingual differences and to develop better sentiment classifiers."
W16-4020,Tagging {I}ngush - Language Technology For Low-Resource Languages Using Resources From Linguistic Field Work,2016,5,0,1,1,2675,jorg tiedemann,Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities ({LT}4{DH}),0,"This paper presents on-going work on creating NLP tools for under-resourced languages from very sparse training data coming from linguistic field work. In this work, we focus on Ingush, a Nakh-Daghestanian language spoken by about 300,000 people in the Russian republics Ingushetia and Chechnya. We present work on morphosyntactic taggers trained on transcribed and linguistically analyzed recordings and dependency parsers using English glosses to project annotation for creating synthetic treebanks. Our preliminary results are promising, supporting the goal of bootstrapping efficient NLP tools with limited or no task-specific annotated data resources available."
W16-2326,"Phrase-Based {SMT} for {F}innish with More Data, Better Models and Alternative Alignment and Translation Tools",2016,27,3,1,1,2675,jorg tiedemann,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper summarises the contributions of the teams at the University of Helsinki, Uppsala University and the University of Turku to the news translation tasks for translating from and to Finnish. Our models address the problem of treating morphology and data coverage in various ways. We introduce a new efficient tool for word alignment and discuss factorisations, gappy language models and reinflection techniques for generating proper Finnish output. The results demonstrate once again that training data is the most effective way to increase translation performance."
W16-2345,Findings of the 2016 {WMT} Shared Task on Cross-lingual Pronoun Prediction,2016,20,11,5,0.555556,5894,liane guillou,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe the design, the evaluation setup, and the results of the 2016 WMT shared task on cross-lingual pronoun prediction. This is a classification task in which participants are asked to provi ..."
W16-2356,A Linear Baseline Classifier for Cross-Lingual Pronoun Prediction,2016,0,1,1,1,2675,jorg tiedemann,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
L16-1147,{O}pen{S}ubtitles2016: Extracting Large Parallel Corpora from Movie and {TV} Subtitles,2016,9,146,2,0.543546,2642,pierre lison,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs."
L16-1559,Finding Alternative Translations in a Large Corpus of Movie Subtitle,2016,0,10,1,1,2675,jorg tiedemann,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"OpenSubtitles.org provides a large collection of user contributed subtitles in various languages for movies and TV programs. Subtitle translations are valuable resources for cross-lingual studies and machine translation research. A less explored feature of the collection is the inclusion of alternative translations, which can be very useful for training paraphrase systems or collecting multi-reference test suites for machine translation. However, differences in translation may also be due to misspellings, incomplete or corrupt data files, or wrongly aligned subtitles. This paper reports our efforts in recognising and classifying alternative subtitle translations with language independent techniques. We use time-based alignment with lexical re-synchronisation techniques and BLEU score filters and sort alternative translations into categories using edit distance metrics and heuristic rules. Our approach produces large numbers of sentence-aligned translation alternatives for over 50 languages provided via the OPUS corpus collection."
2016.eamt-2.8,{OPUS} {--} parallel corpora for everyone,2016,-1,-1,1,1,2675,jorg tiedemann,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
W15-5401,Overview of the {DSL} Shared Task 2015,2015,31,32,4,0.408163,622,marcos zampieri,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,"We present the results of the 2 nd edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the LT4VarDialxe2x80x992015 workshop and focused on the identification of very similar languages and language varieties. Unlike in the 2014 edition, in 2015 we had an Others category with languages that were not seen on training. Moreover, we had two test datasets: one using the original texts (test set A), and one with named entities replaced by placeholders (test set B). Ten teams participated in the task, and the best-performing system achieved 95.54% average accuracy on test set A, and 94.01% on test set B."
W15-3908,Boosting {E}nglish-{C}hinese Machine Transliteration via High Quality Alignment and Multilingual Resources,2015,12,4,2,0.740741,28950,yan shao,Proceedings of the Fifth Named Entity Workshop,0,"This paper presents our machine transliteration systems developed for the NEWS 2015 machine transliteration shared task. Our systems are applied to two tasks: English to Chinese and Chinese to English. For standard runs, in which only official data sets are used, we build phrase-based transliteration models with refined alignments provided by the M2M-aligner. For non-standard runs, we add multilingual resources to the systems designed for the standard runs and build different language specific transliteration systems. Linear regression is adopted to rerank the outputs afterwards, which significantly improves the overall transliteration performance."
W15-3021,Morphological Segmentation and {OPUS} for {F}innish-{E}nglish Machine Translation,2015,16,5,1,1,2675,jorg tiedemann,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,This paper describes baseline systems for Finnish-English and English-Finnish machine translation using standard phrasebased and factored models including morphological features. We experiment with compound splitting and morphological segmentation and study the effect of adding noisy out-of-domain data to the parallel and the monolingual training data. Our results stress the importance of training data and demonstrate the effectiveness of morphological pre-processing of Finnish.
W15-2501,Pronoun-Focused {MT} and Cross-Lingual Pronoun Prediction: Findings of the 2015 {D}isco{MT} Shared Task on Pronoun Translation,2015,48,33,4,1,670,christian hardmeier,Proceedings of the Second Workshop on Discourse in Machine Translation,0,"We describe the design, the evaluation setup, and the results of the DiscoMT 2015 shared task, which included two subtasks, relevant to both the machine translation (MT) and the discourse communities: (i) pronoun-focused translation, a practical MT task, and (ii) cross-lingual pronoun prediction, a classification task that requires no specific MT expertise and is interesting as a machine learning task in its own right. We focused on the Englishxe2x80x90French language pair, for which MT output is generally of high quality, but has visible issues with pronoun translation due to differences in the pronoun systems of the two languages. Six groups participated in the pronoun-focused translation task and eight groups in the cross-lingual pronoun prediction task."
W15-2508,Part-of-Speech Driven Cross-Lingual Pronoun Prediction with Feed-Forward Neural Networks,2015,12,6,3,0,36929,jimmy callin,Proceedings of the Second Workshop on Discourse in Machine Translation,0,"For some language pairs, pronoun translation is a discourse-driven task which requires information that lies beyond its local context. This motivates the task of predicting the correct pronoun give ..."
W15-2515,Baseline Models for Pronoun Prediction and Pronoun-Aware Translation,2015,7,7,1,1,2675,jorg tiedemann,Proceedings of the Second Workshop on Discourse in Machine Translation,0,This paper presents baseline models for the cross-lingual pronoun prediction task and the pronoun-focused translation task at DiscoMT 2015. We present simple yet effective classifiers for the former and discuss the impact of various contextual features on the prediction performance. In the translation task we rely on the document-level decoder Docent and a cross-sentence target language-model over selected words based on the parts-of-speech of the aligned source language words.
W15-2137,Cross-Lingual Dependency Parsing with {U}niversal {D}ependencies and Predicted {P}o{S} Labels,2015,20,17,1,1,2675,jorg tiedemann,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,This paper presents cross-lingual models for dependency parsing using the first release of the universal dependencies data set. We systematically compare annotation projection with monolingual baseline models and study the effect of predicted PoS labels in evaluation. Our results reveal the strong impact of tagging accuracy especially with models trained on noisy projected data sets. This paper quantifies the differences that can be observed when replacing gold standard labels and our results should influence application developers that rely on crosslingual models that are not tested in realis-
W15-1824,Improving the Cross-Lingual Projection of Syntactic Dependencies,2015,17,11,1,1,2675,jorg tiedemann,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,None
W14-5307,A Report on the {DSL} Shared Task 2014,2014,23,40,4,0.408163,622,marcos zampieri,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"This paper summarizes the methods, results and findings of the Discriminating between Similar Languages (DSL) shared task 2014. The shared task provided data from 13 different languages and varieties divided into 6 groups. Participants were required to train their systems to discriminate between languages on a training and development set containing 20,000 sentences from each language (closed submission) and/or any other dataset (open submission). One month later, a test set containing 1,000 unidentified instances per language was released for evaluation. The DSL shared task received 22 inscriptions and 8 final submissions. The best system obtained 95.7% average accuracy."
W14-4203,Cross-lingual Dependency Parsing of Related Languages with Rich Morphosyntactic Tagsets,2014,50,10,2,0,21438,vzeljko agic,Proceedings of the {EMNLP}{'}2014 Workshop on Language Technology for Closely Related Languages and Language Variants,0,"This paper addresses cross-lingual dependency parsing using rich morphosyntactic tagsets. In our case study, we experiment with three related Slavic languages: Croatian, Serbian and Slovene. Four different dependency treebanks are used for monolingual parsing, direct cross-lingual parsing, and a recently introduced crosslingual parsing approach that utilizes statistical machine translation and annotation projection. We argue for the benefits of using rich morphosyntactic tagsets in cross-lingual parsing and empirically support the claim by showing large improvements over an impoverished common feature representation in form of a reduced part-of-speech tagset. In the process, we improve over the previous state-of-the-art scores in dependency parsing for all three languages."
W14-4015,Word{'}s Vector Representations meet Machine Translation,2014,10,3,2,0,16565,eva garcia,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,Distributed vector representations of words are useful in various NLP tasks. We briefly review the CBOW approach and propose a bilingual application of this architecture with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup.
W14-3312,Anaphora Models and Reordering for Phrase-Based {SMT},2014,28,6,3,1,670,christian hardmeier,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for Englishxe2x80x90French. Furthermore, we investigate post-ordering and tunable POS distortion models for Englishxe2x80x90 German."
W14-3334,Estimating Word Alignment Quality for {SMT} Reordering Tasks,2014,40,0,2,0.399657,634,sara stymne,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"Previous studies of the effect of word alignment on translation quality in SMT generally explore link level metrics only and mostly do not show any clear connections between alignment and SMT quality. In this paper, we specifically investigate the impact of word alignment on two pre-reordering tasks in translation, using a wider range of quality indicators than previously done. Experiments on Germanxe2x80x90English translation show that reordering may require alignment models different from those used by the core translation system. Sparse alignments with high precision on the link level, for translation units, and on the subset of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks."
W14-1614,Treebank Translation for Cross-Lingual Parser Induction,2014,29,21,1,1,2675,jorg tiedemann,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"Cross-lingual learning has become a popular approach to facilitate the development of resources and tools for low density languages. Its underlying idea is to make use of existing tools and annotations in resource-rich languages to create similar tools and resources for resource-poor languages. Typically, this is achieved by either projecting annotations across parallel corpora, or by transferring models from one or more source languages to a target language. In this paper, we explore a third strategy by using machine translation to create synthetic training data from the original source-side annotations. Specifically, we apply this technique to dependency parsing, using a cross-lingually unified treebank for adequate evaluation. Our approach draws on annotation projection but avoids the use of noisy source-side annotation of an unrelated parallel corpus and instead relies on manual treebank annotation in combination with statistical machine translation, which makes it possible to train fully lexicalized parsers. We show that this approach significantly outperforms delexicalized transfer parsing.% despite the error-prone translation step."
guillou-etal-2014-parcor,{P}ar{C}or 1.0: A Parallel Pronoun-Coreference Corpus to Support Statistical {MT},2014,27,22,4,0.555556,5894,liane guillou,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present ParCor, a parallel corpus of texts in which pronoun coreference  reduced coreference in which pronouns are used as referring expressions  has been annotated. The corpus is intended to be used both as a resource from which to learn systematic differences in pronoun use between languages and ultimately for developing and testing informed Statistical Machine Translation systems aimed at addressing the problem of pronoun coreference in translation. At present, the corpus consists of a collection of parallel English-German documents from two different text genres: TED Talks (transcribed planned speech), and EU Bookshop publications (written text). All documents in the corpus have been manually annotated with respect to the type and location of each pronoun and, where relevant, its antecedent. We provide details of the texts that we selected, the guidelines and tools used to support annotation and some corpus statistics. The texts in the corpus have already been translated into many languages, and we plan to expand the corpus into these other languages, as well as other genres, in the future."
skadins-etal-2014-billions,Billions of Parallel Words for Free: Building and Using the {EU} Bookshop Corpus,2014,18,25,2,0.833333,18908,raivis skadicnvs,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The European Union is a great source of high quality documents with translations into several languages. Parallel corpora from its publications are frequently used in various tasks, machine translation in particular. A source that has not systematically been explored yet is the EU Bookshop  an online service and archive of publications from various European institutions. The service contains a large body of publications in the 24 official of the EU. This paper describes our efforts in collecting those publications and converting them to a format that is useful for natural language processing in particular statistical machine translation. We report our procedure of crawling the website and various pre-processing steps that were necessary to clean up the data after the conversion from the original PDF files. Furthermore, we demonstrate the use of this dataset in training SMT models for English, French, German, Spanish, and Latvian."
C14-1175,Rediscovering Annotation Projection for Cross-Lingual Parser Induction,2014,0,0,1,1,2675,jorg tiedemann,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,None
W13-5606,Experiences in Building the Let{'}s {MT}! Portal on {A}mazon {EC}2,2013,0,0,1,1,2675,jorg tiedemann,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"In this presentation I will discuss the design and implementation of Letxe2x80x99s MT!, a collaborative platform for building statistical machine translation systems. The goal of this platform is to make MT technology, that has been developed in academia, accessible for professional translators, freelancers and every-day users without requiring technical skills and deep background knowledge of the approaches used in the backend of the translation engine. The main challenge in this project was the development of a robust environment that can serve a growing community and large numbers of user requests. The key for success is a distributed environment that allows a maximum of scalability and robustness. With this in mind, we developed a modular platform that can be scaled by adding new nodes to the different components of the system. We opted for a cloud-based solution based on Amazon EC2 to create a cost-efficient environment that can dynamically be adjusted to user needs and system load. In the presentation I will explain our design of the distributed resource repository, the SMT training facilities and the actual translation service. I will mention issues of data security and optimization of the training procedures in order to fit our setup and the expected usage of the system."
W13-5634,Statistical Machine Translation with Readability Constraints,2013,26,15,2,0.399657,634,sara stymne,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"This paper presents experiments with document-level machine translation with readability constraints. We describe the task of producing simplified translations from a given source with the aim to optimize machine translation for specific target users such as language learners. In our approach, we introduce global features that are known to affect readability into a documentlevel SMT decoding framework. We show that the decoder is capable of incorporating those features and that we can influence the readability of the output as measured by common metrics. This study presents the first attempt of jointly performing machine translation and text simplification, which is demonstrated through the case of translating parliamentary texts from English to Swedish."
W13-3308,Feature Weight Optimization for Discourse-Level {SMT},2013,36,8,3,0.399657,634,sara stymne,Proceedings of the Workshop on Discourse in Machine Translation,0,"We present an approach to feature weight optimization for document-level decoding. This is an essential task for enabling future development of discourse-level statistical machine translation, as it allows easy integration of discourse features in the decoding process. We extend the framework of sentence-level feature weight optimization to the document-level. We show experimentally that we can get competitive and relatively stable results when using a standard set of features, and that this framework also allows us to optimize documentlevel features, which can be used to model discourse phenomena."
W13-2229,Tunable Distortion Limits and Corpus Cleaning for {SMT},2013,17,11,3,0.399657,634,sara stymne,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We describe the Uppsala University system for WMT13, for English-to-German translation. We use the Docent decoder, a local search decoder that translates at the document level. We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Docent. We also investigate cleaning of the noisy Common Crawl corpus. We show that we can use alignment-based filtering for cleaning with good results. Finally we investigate effects of corpus selection for recasing."
R13-1088,Analyzing the Use of Character-Level Translation with Sparse and Noisy Datasets,2013,35,12,1,1,2675,jorg tiedemann,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"This paper provides an analysis of character-level machine translation models used in pivot-based translation when applied to sparse and noisy datasets, such as crowdsourced movie subtitles. In our experiments, we find that such characterlevel models cut the number of untranslated words by over 40% and are especially competitive (improvements of 2-3 BLEU points) in the case of limited training data. We explore the impact of character alignment, phrase table filtering, bitext size and the choice of pivot language on translation quality. We further compare cascaded translation models to the use of synthetic training data via multiple pivots, and we find that the latter works significantly better. Finally, we demonstrate that neither word- nor character-BLEU correlate perfectly with human judgments, due to BLEUxe2x80x99s sensitivity to length."
P13-4033,{D}ocent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation,2013,20,30,3,1,670,christian hardmeier,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We describe Docent, an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units. By taking translation to the document level, our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware SMT models. 1 Motivation"
D13-1037,Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction,2013,23,19,2,1,670,christian hardmeier,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"This paper addresses the task of predicting the correct French translations of third-person subject pronouns in English discourse, a problem that is relevant as a prerequisite for machine translation and that requires anaphora resolution. We present an approach based on neural networks that models anaphoric links as latent variables and show that its performance is competitive with that of a system with separate anaphora resolution while not requiring any coreference-annotated training data. This demonstrates that the information contained in parallel bitexts can successfully be used to acquire knowledge about pronominal anaphora in an unsupervised way."
W12-3112,Tree Kernels for Machine Translation Quality Estimation,2012,17,33,3,1,670,christian hardmeier,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes Uppsala University's submissions to the Quality Estimation (QE) shared task at WMT 2012. We present a QE system based on Support Vector Machine regression, using a number of explicitly defined features extracted from the Machine Translation input, output and models in combination with tree kernels over constituency and dependency parse trees for the input and output sentences. We confirm earlier results suggesting that tree kernels can be a useful tool for QE system construction especially in the early stages of system design."
P12-3008,{L}ets{MT}!: Cloud-Based Platform for Do-It-Yourself Machine Translation,2012,10,13,3,0,11079,andrejs vasicljevs,Proceedings of the {ACL} 2012 System Demonstrations,0,"To facilitate the creation and usage of custom SMT systems we have created a cloud-based platform for do-it-yourself MT. The platform is developed in the EU collaboration project LetsMT!. This system demonstration paper presents the motivation in developing the LetsMT! platform, its main features, architecture, and an evaluation in a practical use case."
P12-2059,Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages,2012,26,62,2,0,1636,preslav nakov,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline."
tiedemann-etal-2012-distributed,A Distributed Resource Repository for Cloud-Based Machine Translation,2012,4,0,1,1,2675,jorg tiedemann,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we present the architecture of a distributed resource repository developed for collecting training data for building customized statistical machine translation systems. The repository is designed for the cloud-based translation service integrated in the Let'sMT! platform which is about to be launched to the public. The system includes important features such as automatic import and alignment of textual documents in a variety of formats, a flexible database for meta-information using modern key-value stores and a grid-based backend for running off-line processes. The entire system is very modular and supports highly distributed setups to enable a maximum of flexibility and scalability. The system uses secure connections and includes an effective permission management to ensure data integrity. In this paper, we also take a closer look at the task of sentence alignment. The process of alignment is extremely important for the success of translation models trained on the platform. Alignment decisions significantly influence the quality of SMT engines."
tiedemann-2012-parallel,"Parallel Data, Tools and Interfaces in {OPUS}",2012,14,419,1,1,2675,jorg tiedemann,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project."
kotze-etal-2012-large,Large aligned treebanks for syntax-based machine translation,2012,55,4,4,0,43267,gideon kotze,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present a collection of parallel treebanks that have been automatically aligned on both the terminal and the nonterminal constituent level for use in syntax-based machine translation. We describe how they were constructed and applied to a syntax- and example-based machine translation system called Parse and Corpus-Based Machine Translation (PaCo-MT). For the language pair Dutch to English, we present evaluation scores of both the nonterminal constituent alignments and the MT system itself, and in the latter case, compare them with those of Moses, a current state-of-the-art statistical MT system, when trained on the same data."
E12-1015,Character-Based Pivot Translation for Under-Resourced Languages and Domains,2012,22,28,1,1,2675,jorg tiedemann,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,In this paper we investigate the use of character-level translation models to support the translation from and to under-resourced languages and textual domains via closely related pivot languages. Our experiments show that these low-level models can be successful even with tiny amounts of training data. We test the approach on movie subtitles for three language pairs and legal texts for another language pair in a domain adaptation task. Our pivot translations outperform the baselines by a large margin.
D12-1108,Document-Wide Decoding for Phrase-Based Statistical Machine Translation,2012,31,47,3,1,670,christian hardmeier,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Independence between sentences is an assumption deeply entrenched in the models and algorithms used for statistical machine translation (SMT), particularly in the popular dynamic programming beam search decoding algorithm. This restriction is an obstacle to research on more sophisticated discourse-level models for SMT. We propose a stochastic local search decoding method for phrase-based SMT, which permits free document-wide dependencies in the models. We explore the stability and the search parameters of this method and demonstrate that it can be successfully used to optimise a document-level semantic language model."
C12-1160,Efficient Discrimination Between Closely Related Languages,2012,11,12,1,1,2675,jorg tiedemann,Proceedings of {COLING} 2012,0,"In this paper, we revisit the problem of language identification with the focus on proper discrimination between closely related languages. Strong similarities between certain languages make it very hard to classify them correctly using standard methods that have been proposed in the literature. Dedicated models that focus on specific discrimination tasks help to improve the accuracy of general-purpose language identification tools. We propose and compare methods based on simple document classification techniques trained on parallel corpora of closely related languages and methods that emphasize discriminating features in terms of blacklisted words. Our experiments demonstrate that these techniques are highly accurate for the difficult task of discriminating between Bosnian, Croatian and Serbian. The best setup yields an absolute improvement of over 9% in accuracy over the best performing baseline using a state-of-the-art language identification tool."
W11-2144,The {U}ppsala-{FBK} systems at {WMT} 2011,2011,17,5,2,1,670,christian hardmeier,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper presents our submissions to the shared translation task at WMT 2011. We created two largely independent systems for English-to-French and Haitian Creole-to-English translation to evaluate different features and components from our ongoing research on these language pairs. Key features of our systems include anaphora resolution, hierarchical lexical reordering, data selection for language modelling, linear transduction grammars for word alignment and syntax-based decoding with monolingual dependency information."
2011.mtsummit-systems.5,{L}ets{MT}!: Cloud-Based Platform for Building User Tailored Machine Translation Engines,2011,-1,-1,3,0,34986,andrejs vasiljevs,Proceedings of Machine Translation Summit XIII: System Presentations,0,None
W10-3304,Finding Medical Term Variations using Parallel Corpora and Distributional Similarity,2010,28,9,2,0.769231,12096,lonneke plas,Proceedings of the 6th Workshop on {O}ntologies and {L}exical {R}esources,0,We describe a method for the identification of medical term variations using parallel corpora and measures of distributional similarity. Our approach is based on automatic word alignment and standard phrase extraction techniques commonly used in statistical machine translation. Combined with pattern-based filters we obtain encouraging results compared to related approaches using similar datadriven techniques.
W10-2602,Context Adaptation in Statistical Machine Translation Using Models with Exponentially Decaying Cache,2010,20,47,1,1,2675,jorg tiedemann,Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,0,We report results from a domain adaptation task for statistical machine translation (SMT) using cache-based adaptive language and translation models. We apply an exponential decay factor and integrate the cache models in a standard phrase-based SMT decoder. Without the need for any domain-specific resources we obtain a 2.6% relative improvement on average in BLEU scores using our dynamic adaptation procedure.
W10-1728,To Cache or Not To Cache? Experiments with Adaptive Models in Statistical Machine Translation,2010,11,14,1,1,2675,jorg tiedemann,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,We report results of our submissions to the WMT 2010 shared translation task in which we applied a system that includes adaptive language and translation models. Adaptation is implemented using exponentially decaying caches storing previous translations as the history for new predictions. Evidence from the cache is then mixed with the global background model. The main problem in this setup is error propagation and our submissions essentially failed to improve over the competitive baseline. There are slight improvements in lexical choice but the global performance decreases in terms of BLEU scores.
tiedemann-2010-lingua,Lingua-Align: An Experimental Toolbox for Automatic Tree-to-Tree Alignment,2010,23,14,1,1,2675,jorg tiedemann,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,In this paper we present an experimental toolbox for automatic tree-to-tree alignment based on a binary classification model. The aligner implements a recurrent architecture for structural prediction using history features and a sequential classification procedure. The discriminative base classifier uses a log-linear model in the current setup which enables simple integration of various features extracted from the data. The Lingua-Align toolbox provides a flexible framework for feature extraction including contextual properties and implements several alignment inference procedures. Various settings and constraints can be controlled via a simple frontend or called from external scripts. Lingua-Align supports different treebank formats and includes additional tools for conversion and evaluation. In our experiments we can show that our tree aligner produces results with high quality and outperforms unsupervised techniques proposed otherwise. It also integrates well with another existing tool for manual tree alignment which makes it possible to quickly integrate additional training material and to run semi-automatic alignment strategies.
2010.eamt-1.21,{E}nglish to {B}angla Phrase-Based Machine Translation,2010,14,11,2,0,39341,zahurul islam,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,"Machine Translation (MT) is the task of automatically translating a text from one language to another. In this work we describe a phrase-based Statistical Machine Translation (SMT) system that translates English sentences to Bangla. A transliteration module is added to handle outof-vocabulary (OOV) words. This is especially useful for low-density languages like Bangla for which only a limited amount of training data is available. Furthermore, a special component for handling preposition is implemented to treat systematic grammatical differences between English and Bangla. We have shown the improvement of our system through effective impacts on the BLEU, NIST and TER scores. The overall BLEU score of our system is 11.7 and for short sentences it is 23.3."
W09-4205,Evidence-Based Word Alignment,2009,18,0,1,1,2675,jorg tiedemann,"Proceedings of the Workshop on Natural Language Processing Methods and Corpora in Translation, Lexicography, and Language Learning",0,"In this paper we describe word alignment experiments using an approach based on a disjunctive combination of alignment evidence. A wide range of statistical, orthographic and positional clues can be combined in this way. Their weights can easily be learned from small amounts of hand-aligned training data. We can show that this evidence-based approach can be used to improve the baseline of statistical alignment and also outperforms a discriminative approach based on a maximum entropy classifier."
W09-4206,A Discriminative Approach to Tree Alignment,2009,15,13,1,1,2675,jorg tiedemann,"Proceedings of the Workshop on Natural Language Processing Methods and Corpora in Translation, Lexicography, and Language Learning",0,In this paper we propose a discriminative framework for automatic tree alignment. We use a rich feature set and a log-linear model trained on small amounts of hand-aligned training data. We include contextual features and link dependencies to improve the results even further. We achieve an overall F-score of almost 80% which is significantly better than other scores reported for this task.
2009.eamt-1.3,Character-Based {PSMT} for Closely Related Languages,2009,10,42,1,1,2675,jorg tiedemann,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"Translating unknown words between related languages using a character-based statistical machine translation model can be beneficial. In this paper, we describe a simple method to combine character-based models with standard word-based models to increase the coverage of a phrase-based SMT system. Using this approach, we can show a modest improvement when translating between Norwegian and Swedish. The potentials of applying character-based models to closely related languages is also illustrated by applying the character model on its own. The performance of such an approach is similar to the word-level baseline and closer to the reference in terms of string similarity."
2009.eamt-1.16,Translating Questions for Cross-Lingual {QA},2009,14,1,1,1,2675,jorg tiedemann,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,In this paper we investigate possibilities of the development of a task-specific translation component for cross-language question answering. We focus on the optimization of phrase-based SMT for models trained on very limited data resources. We also look at the combination of such systems with another approach based on example-based MT with proportional analogies. In our experiments we could improve a strong baseline of a general purpose MT engine with more than 5 BLEU points.
W08-1803,Simple is Best: Experiments with Different Document Segmentation Strategies for Passage Retrieval,2008,17,15,1,1,2675,jorg tiedemann,Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering,0,"Passage retrieval is used in QA to filter large document collections in order to find text units relevant for answering given questions. In our QA system we apply standard IR techniques and index-time passaging in the retrieval component. In this paper we investigate several ways of dividing documents into passages. In particular we look at semantically motivated approaches (using coreference chains and discourse clues) compared with simple window-based techniques. We evaluate retrieval performance and the overall QA performance in order to study the impact of the different segmentation approaches. From our experiments we can conclude that the simple techniques using fixed-sized windows clearly outperform the semantically motivated approaches, which indicates that uniformity in size seems to be more important than semantic coherence in our setup."
W08-1807,Using Lexico-Semantic Information for Query Expansion in Passage Retrieval for Question Answering,2008,20,11,2,1,12096,lonneke plas,Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering,0,"In this paper we investigate the use of several types of lexico-semantic information for query expansion in the passage retrieval component of our QA system. We have used four corpus-based methods to acquire semantically related words, and we have used one hand-built resource. We evaluate our techniques on the Dutch CLEF QA track. In our experiments expansions that try to bridge the terminological gap between question and document collection do not result in any improvements. However, expansions bridging the knowledge gap show modest improvements."
tiedemann-2008-synchronizing,Synchronizing Translated Movie Subtitles,2008,10,25,1,1,2675,jorg tiedemann,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper addresses the problem of synchronizing movie subtitles, which is necessary to improve alignment quality when building a parallel corpus out of translated subtitles. In particular, synchronization is done on the basis of aligned anchor points. Previous studies have shown that cognate filters are useful for the identification of such points. However, this restricts the approach to related languages with similar alphabets. Here, we propose a dictionary-based approach using automatic word alignment. We can show an improvement in alignment quality even for related languages compared to the cognate-based approach."
W06-2405,Identifying idiomatic expressions using automatic word-alignment,2006,14,67,2,0,49002,begona moiron,Proceedings of the Workshop on Multi-word-expressions in a multilingual context,0,"For NLP applications that require some sort of semantic interpretation it would be helpful to know what expressions exhibit an idiomatic meaning and what expressions exhibit a literal meaning. We investigate whether automatic word-alignment in existing parallel corpora facilitates the classification of candidate expressions along a continuum ranging from literal and transparent expressions to idiomatic and opaque expressions. Our method relies on two criteria: (i) meaning predictability that is measured as semantic entropy and (ii), the overlap between the meaning of an expression and the meaning of its component words. We approximate the mentioned overlap as the proportion of default alignments. We obtain a significant improvement over the baseline with both measures."
P06-2111,Finding Synonyms Using Automatic Word Alignment and Measures of Distributional Similarity,2006,19,81,2,1,12096,lonneke plas,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"There have been many proposals to extract semantically related words using measures of distributional similarity, but these typically are not able to distinguish between synonyms and other types of semantically related words such as antonyms, (co)hyponyms and hypernyms. We present a method based on automatic word alignment of parallel corpora consisting of documents translated into multiple languages and compare our method with a monolingual syntax-based method. The approach that uses aligned multilingual data to extract synonyms shows much higher precision and recall scores for the task of synonym extraction than the monolingual syntax-based approach."
tiedemann-2006-isa,{ISA} {\\&} {ICA} - Two Web Interfaces for Interactive Alignment of Bitexts alignment of parallel texts,2006,10,15,1,1,2675,jorg tiedemann,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,ISA and ICA are two web interfaces for interactive alignment of parallel texts. ISA provides an interface for automatic and manual sentence alignment. It includes cognate filters and uses structural markup to improve automatic alignment and provides intuitive tools for editing them. Alignment results can be saved to disk or sent via e-mail. ICA provides an interface to the clue aligner from the Uplug toolbox. It allows one to set various parameters and visualizes alignment results in a two-dimensional matrix. Word alignments can be edited and saved to disk.
H05-1118,Integrating Linguistic Knowledge in Passage Retrieval for Question Answering,2005,17,19,1,1,2675,jorg tiedemann,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In this paper we investigate the use of linguistic knowledge in passage retrieval as part of an open-domain question answering system. We use annotation produced by a deep syntactic dependency parser for Dutch, Alpino, to extract various kinds of linguistic features and syntactic units to be included in a multi-layer index. Similar annotation is produced for natural language questions to be answered by the system. From this we extract query terms to be sent to the enriched retrieval index. We use a genetic algorithm to optimize the selection of features and syntactic units to be included in a query. This algorithm is also used to optimize further parameters such as keyword weights. The system is trained on questions from the competition on Dutch question answering within the Cross-Language Evaluation Forum (CLEF). We could show an improvement of about 15% in mean total reciprocal rank compared to traditional information retrieval using plain text keywords (including stemming and stop word removal)."
tiedemann-nygaard-2004-opus,The {OPUS} Corpus - Parallel and Free: \\url{http://logos.uio.no/opus},2004,3,37,1,1,2675,jorg tiedemann,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,The OPUS corpus is a growing collection of translated documents collected from the internet. The current version contains about 30 million words in 60 languages. The entire corpus is sentence aligned and it also contains linguistic markup for certain languages.
weijnitz-etal-2004-mt,{MT} Goes Farming: Comparing Two Machine Translation Approaches on a New Domain,2004,-1,-1,5,0,52303,per weijnitz,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
C04-1031,Word to word alignment strategies,2004,17,25,1,1,2675,jorg tiedemann,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,Word alignment is a challenging task aiming at the identification of translational relations between words and multi-word units in parallel corpora. Many alignment strategies are based on links between single words. Different strategies can be used to find the optimal word alignment using such one-to-one word links including relations between multi-word units. In this paper seven algorithms are compared using a word alignment approach based on association clues and an English-Swedish bitext together with a handcrafted reference alignment used for evaluation.
E03-1026,Combining Clues for Word Alignment,2003,17,70,1,1,2675,jorg tiedemann,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, a word alignment approach is presented which is based on a combination of clues. Word alignment clues indicate associations between words and phrases. They can be based on features such as frequency, part-of-speech, phrase type, and the actual wordform strings. Clues can be found by calculating similarity measures or learned from word aligned data. The clue alignment approach, which is proposed in this paper, makes it possible to combine association clues taking different kinds of linguistic information into account. It allows a dynamic tokenization into token units of varying size. The approach has been applied to an English/Swedish parallel text with promising results."
2003.mtsummit-systems.14,{MATS} {--} a glass box machine translation system,2003,4,5,5,0,49365,anna hein,Proceedings of Machine Translation Summit IX: System Presentations,0,"MATS is a fully automatic machine translation system with the unification-based translation engine MULTRA as its core (see e.g. Sagvall Hein, 1993, 1997). The system was developed in a co-operative project between the Department of Linguistics, Uppsala University, the bus and truck manufacturing company Scania CV AB, and the translation company Explicon AB. The focus of the project was the scaling up of MULTRA for translation from Swedish to English in the automotive service domain (Sagvall Hein et al., 2002). The scaling-up effort also implied eliminating separate morphological processing by storing the lexical data in a bi-lingual lexical database with a built-in morphology (Tiedemann, 2002). Here we focus on the transparency aspects of the MATS system. The translation proceeds in a number of distinct steps from an SGML version of the source document to an SGML version of the target document. The output of each step is, optionally, presented to the user for inspection. The transparency of the system is most useful for grammar developers and teaching purposes. Another interface will be developed for end users. MATS runs via a web-based interface, and if a step fails for a certain input, the corresponding part is highlighted in a colour specific to that step. The outcome of the different steps is collected and summarised in an evaluation report. The interface provides a great variety of presentation, tracing, and evaluation options"
hein-etal-2002-scaling,Scaling Up an {MT} Prototype for Industrial Use - Databases and Data Flow,2002,6,12,3,0.263096,49365,anna hein,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In a cooperative project between Uppsala University, the bus and truck manufacturing company Scania CV AB, and the translation company Explicon AB, issues of scaling up the transfer-based machine translation prototype MULTRA for industrial use is beeing investigated. The project is limited to one domain, automotive service literature, and one translation direction, Swedish to English, but issues concerning the change of domain, translation direction and language pair are also considered. Three focal points of the project work have been the design and implementation of the new MATS system, including the redesign, porting and integration of MULTRA, the redesign and implementation of the dictionaries of the language modules as a lexical database, and the scaling up of the dictionaries and the grammars. The system is currently trained on a corpus of aligned bitexts from the automotive service domain. The coverage of the lexical data is almost complete, and validated by professional translators, but the grammars are still limited. Despite the incomplete state of the grammars, the system already translates more than a third of the segments in the corpus. Preliminary evaluations of system performance and coverage have been made, and further development of evaluation methods and metrics are in progress."
tiedemann-2002-matslex,{M}ats{L}ex - a Multilingual Lexical Database for Machine Translation,2002,4,12,1,1,2675,jorg tiedemann,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"MatsLex represents a relational database which can be used to store multilingual lexical data in a central and coherent lexicon. Tools and interfaces have been implemented to maintain the database and to apply its contents to different multilingual applications. MatsLex has been developed to feed different modules of a machine translation system with appropriate data, monolingual as well as bilingual. The database gives the user full control of the lexicon. In the paper, features and interfaces of the database are discussed as well as the connection to the machine translation engine."
W01-1723,{U}plug{W}eb{--}Corpus Tools on the Web,2001,-1,-1,1,1,2675,jorg tiedemann,Proceedings of the 13th Nordic Conference of Computational Linguistics ({NODALIDA} 2001),0,None
ahrenberg-etal-2000-evaluation,Evaluation of Word Alignment Systems,2000,15,50,4,0,5260,lars ahrenberg,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This project evaluates two different systems that generate wordalignments on English-Swedish data. The systems to be used are the Giza system, that may generate a variety of statistical translati ..."
W99-1022,Word Alignment Step by Step,2000,0,10,1,1,2675,jorg tiedemann,Proceedings of the 12th Nordic Conference of Computational Linguistics ({NODALIDA} 1999),0,None
W99-0626,Automatic Construction of Weighted String Similarity Measures,1999,7,66,1,1,2675,jorg tiedemann,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,String similarity metrics are used for several purposes in text-processing. One task is the extraction of cognates from bilingual text. In this paper three approaches to the automatic generation of language dependent string matching functions are present
J99-1007,Book Reviews: Linguistic Databases,1999,-1,-1,1,1,2675,jorg tiedemann,Computational Linguistics,0,None
W98-1613,Extraction of Translation Equivalents from Parallel Corpora,1998,11,32,1,1,2675,jorg tiedemann,Proceedings of the 11th Nordic Conference of Computational Linguistics ({NODALIDA} 1998),0,None
