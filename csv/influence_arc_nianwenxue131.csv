2011.mtsummit-papers.22,P11-2037,0,0.234008,"order to produce ﬂuent translations. While it is easy to see that knowing the number information of a Chinese noun would help the MT system output a word with the correct number inﬂection in the target language, predicting noun number is also useful in less obvious ways. Chinese is a pro-drop language that allows pronouns to be dropped. When translated into a non-pro-drop language such as English, these pronouns will have to be recovered and this has been the topic of a few recent research efforts in recovering dropped pronouns (Yang and Xue, 2010; Chung and Gildea, 2010; Kong and Zhou, 2010; Cai et al., 2011). A part of recovering these dropped pronouns, for Chinese, would include determining whether the pronoun should be singular or plural. To determine this, it will be necessary to know the number of its antecedent, which is where automatic prediction of noun number in Chinese may prove to be very useful. The task of deciding whether a noun is singular or plural is a mindless process for any ﬂuent speaker of Chinese, but is not nearly as straightforward to determine algorithmically. This is where having number information readily available in Chinese becomes much less trivial. In this paper, we"
2011.mtsummit-papers.22,D10-1062,0,0.0118734,"ll have to be factored into the MT models in order to produce ﬂuent translations. While it is easy to see that knowing the number information of a Chinese noun would help the MT system output a word with the correct number inﬂection in the target language, predicting noun number is also useful in less obvious ways. Chinese is a pro-drop language that allows pronouns to be dropped. When translated into a non-pro-drop language such as English, these pronouns will have to be recovered and this has been the topic of a few recent research efforts in recovering dropped pronouns (Yang and Xue, 2010; Chung and Gildea, 2010; Kong and Zhou, 2010; Cai et al., 2011). A part of recovering these dropped pronouns, for Chinese, would include determining whether the pronoun should be singular or plural. To determine this, it will be necessary to know the number of its antecedent, which is where automatic prediction of noun number in Chinese may prove to be very useful. The task of deciding whether a noun is singular or plural is a mindless process for any ﬂuent speaker of Chinese, but is not nearly as straightforward to determine algorithmically. This is where having number information readily available in Chinese becom"
2011.mtsummit-papers.22,P03-1059,0,0.0846869,"Missing"
2011.mtsummit-papers.22,D10-1086,0,0.0130097,"nto the MT models in order to produce ﬂuent translations. While it is easy to see that knowing the number information of a Chinese noun would help the MT system output a word with the correct number inﬂection in the target language, predicting noun number is also useful in less obvious ways. Chinese is a pro-drop language that allows pronouns to be dropped. When translated into a non-pro-drop language such as English, these pronouns will have to be recovered and this has been the topic of a few recent research efforts in recovering dropped pronouns (Yang and Xue, 2010; Chung and Gildea, 2010; Kong and Zhou, 2010; Cai et al., 2011). A part of recovering these dropped pronouns, for Chinese, would include determining whether the pronoun should be singular or plural. To determine this, it will be necessary to know the number of its antecedent, which is where automatic prediction of noun number in Chinese may prove to be very useful. The task of deciding whether a noun is singular or plural is a mindless process for any ﬂuent speaker of Chinese, but is not nearly as straightforward to determine algorithmically. This is where having number information readily available in Chinese becomes much less trivial."
2011.mtsummit-papers.22,N03-5008,0,0.0582206,"Missing"
2011.mtsummit-papers.22,P06-1055,0,0.0723617,"Missing"
2011.mtsummit-papers.22,C10-2158,1,0.484835,"rphological forms will have to be factored into the MT models in order to produce ﬂuent translations. While it is easy to see that knowing the number information of a Chinese noun would help the MT system output a word with the correct number inﬂection in the target language, predicting noun number is also useful in less obvious ways. Chinese is a pro-drop language that allows pronouns to be dropped. When translated into a non-pro-drop language such as English, these pronouns will have to be recovered and this has been the topic of a few recent research efforts in recovering dropped pronouns (Yang and Xue, 2010; Chung and Gildea, 2010; Kong and Zhou, 2010; Cai et al., 2011). A part of recovering these dropped pronouns, for Chinese, would include determining whether the pronoun should be singular or plural. To determine this, it will be necessary to know the number of its antecedent, which is where automatic prediction of noun number in Chinese may prove to be very useful. The task of deciding whether a noun is singular or plural is a mindless process for any ﬂuent speaker of Chinese, but is not nearly as straightforward to determine algorithmically. This is where having number information readily av"
2019.lilt-18.2,W13-2322,0,0.305218,"Missing"
2019.lilt-18.2,P13-2131,0,0.136899,"Missing"
2019.lilt-18.2,doddington-etal-2004-automatic,0,0.151518,"(AMR) is a novel annotation framework to represent the “meaning” of a sentence with a single rooted, acyclic1 , directed graph (Banarescu et al., 2013), departing from previous practices of performing partial semantic annotation that focuses on certain aspect of meaning. Some well-known examples of partial semantic annotation eﬀorts include the annotation of predicate-argument structure of verbs (Palmer et al., 2005, Xue and Palmer, 2009) and predicative or relational nouns (Meyers et al., 2004), the annotation of entities and relations along the lines of Automatic Content Extraction project (Doddington et al., 2004), the annotation of discourse relations (e.g., the Penn Discourse TreeBank (Prasad et al., 2008)), as well as the annotation of temporal relations (Pustejovsky et al., 2003) and factuality (Saurí and Pustejovsky, 2009). The choice to annotate aspects of meaning instead of “whole-sentence” meaning is predicated on the assumption that focusing on a single aspect of meaning is more likely to lead to consistent annotation and consistently annotated data in turn lead to more accurate machine learning based automatic systems. This is a reasonable assumption when the meaning components of a sentence"
2019.lilt-18.2,P14-1134,0,0.0455521,"Missing"
2019.lilt-18.2,J16-4009,0,0.0357011,"Missing"
2019.lilt-18.2,W16-1702,1,0.721326,"Missing"
2019.lilt-18.2,W04-2705,0,0.212363,"Missing"
2019.lilt-18.2,S14-2008,0,0.0705466,"Missing"
2019.lilt-18.2,J05-1004,0,0.237125,"umber of such inferred concepts per sentence is 2.88. These statistics will have to be taken into account when developing automatic Chinese AMR parsers. 1 Introduction Abstract Meaning Representation (AMR) is a novel annotation framework to represent the “meaning” of a sentence with a single rooted, acyclic1 , directed graph (Banarescu et al., 2013), departing from previous practices of performing partial semantic annotation that focuses on certain aspect of meaning. Some well-known examples of partial semantic annotation eﬀorts include the annotation of predicate-argument structure of verbs (Palmer et al., 2005, Xue and Palmer, 2009) and predicative or relational nouns (Meyers et al., 2004), the annotation of entities and relations along the lines of Automatic Content Extraction project (Doddington et al., 2004), the annotation of discourse relations (e.g., the Penn Discourse TreeBank (Prasad et al., 2008)), as well as the annotation of temporal relations (Pustejovsky et al., 2003) and factuality (Saurí and Pustejovsky, 2009). The choice to annotate aspects of meaning instead of “whole-sentence” meaning is predicated on the assumption that focusing on a single aspect of meaning is more likely to lea"
2019.lilt-18.2,D14-1048,0,0.0459722,"Missing"
2019.lilt-18.2,prasad-etal-2008-penn,0,0.457198,"acyclic1 , directed graph (Banarescu et al., 2013), departing from previous practices of performing partial semantic annotation that focuses on certain aspect of meaning. Some well-known examples of partial semantic annotation eﬀorts include the annotation of predicate-argument structure of verbs (Palmer et al., 2005, Xue and Palmer, 2009) and predicative or relational nouns (Meyers et al., 2004), the annotation of entities and relations along the lines of Automatic Content Extraction project (Doddington et al., 2004), the annotation of discourse relations (e.g., the Penn Discourse TreeBank (Prasad et al., 2008)), as well as the annotation of temporal relations (Pustejovsky et al., 2003) and factuality (Saurí and Pustejovsky, 2009). The choice to annotate aspects of meaning instead of “whole-sentence” meaning is predicated on the assumption that focusing on a single aspect of meaning is more likely to lead to consistent annotation and consistently annotated data in turn lead to more accurate machine learning based automatic systems. This is a reasonable assumption when the meaning components of a sentence are not well-understood. However, such a fragmented approach to meaning annotation also leads to"
2019.lilt-18.2,xue-etal-2014-interlingua,1,0.95862,"ctic structure. For example, a single concept or relation in AMR can be posited to represent meaning conveyed in discontinuous constructions such as “as · · · as · · ·” which can be collapsed into a single relation :compared-to. Third, since AMR abstracts away from elements of surface syntactic structure such as word order and morpho-syntactic markers, which account for much of the cross-linguistic variations, 4 / LiLT volume 18, issue (1) June 2019 it makes a more portable semantic annotation framework across languages, as the preliminary AMR annotation on Chinese and Czech has demonstrated (Xue et al., 2014). There are always two sides to every coin and aﬀording annotators unconstrained freedom to make up new concepts can lead to inconsistent and unusable annotation without carefully designed guidelines that specify when a new concept can be inferred and when a discontinuous pattern can be mapped to a single concept/relation. Although annotating meaning representation independently of syntactic structures serves to speed up annotation, in automatic meaning representation parsing, morpho-syntactic structures often serve as important clues that can be used to derive the semantic representation. Som"
2020.conll-shared.1,W13-2322,0,0.190653,"owards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that mod (domain) mod (domain) other (ARG1)-of exemplify-01 ARG0 and op1 cotton soybean op2 op3 rice op4 et-cetera Figure 4: Abstract Meaning Representation (AMR) for the running example A simila"
2020.conll-shared.1,W15-0128,1,0.796384,"des). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). Elementary Dependency Structures The EDS graphs (Oepen and Lønning, 2006) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015).3 Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) encode English Resource Semantics in a variablefree semantic dependency graph—not limited to bi-lexical dependencies—where graph nodes correspond to logical predications and edges to labeled argument positions. The EDS conversion from underspecified logical forms to directed graphs discards partial information on semantic scope from the full ERS, which makes these graphs abstractly— if not linguistically—similar to Abstract Meaning Representation (see below). Nodes in EDS are in principle independent of surface lexical units, b"
2020.conll-shared.1,D16-1134,1,0.827452,"(UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency fr"
2020.conll-shared.1,W13-0101,1,0.816811,"imilar 〈2:9〉 sempos adj.denot ACT #Benef sempos x EXT almost 〈23:29〉 sempos adv.denot.grad.neg coref.gram #Gen sempos x RSTR other 〈53:58〉 sempos adj.denot Figure 2: Semantic dependency graphs for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Prague Tectogrammatical Graphs (PTG). In addition to node properties, visualized similarly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al.,"
2020.conll-shared.1,E17-2039,1,0.896998,"Missing"
2020.conll-shared.1,2020.emnlp-main.195,0,0.0866269,"with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite"
2020.conll-shared.1,2020.conll-shared.2,1,0.940067,"Structure (DRS), the meaning representations at the core of Discourse Representation Theory (DRT; Kamp and Reyle, 1993; Van der Sandt, 1992; Asher, 1993). DRSs can model many challenging semantic phenomena including quantifiers, negation, scope, pronoun resolution, presupposition accommodation, and discourse structure. Moreover, they are directly translatable into first-order logic formulas to account for logical inference. DRG used in the shared task represents a type of graph encoding of DRS that makes the graphs structurally as close as possible to the structures found in other frameworks; Abzianidze et al. (2020) provide more details on the design choices in the DRG encoding. The source DRS annotations are taken from data release 3.0.0 of the Parallel Meaning Bank (PMB; Abzianidze et al., 2017; Bos et al., 2017).6 Although the annotations in the PMB are compositionally derived from lexical semantics, anchoring information is not explicit in its DRSs; thus, (like AMR) the DRG framework formally instantiates Flavor (2) of meaning representations. The DRG of the running example is given in Figure 5. The concepts (vissualized as oval shapes) are represented by WordNet 3.0 senses and semantic roles (in dia"
2020.conll-shared.1,W19-1201,1,0.906261,"Missing"
2020.conll-shared.1,2020.lrec-1.234,1,0.763502,"s (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP task series, to go well beyond the impressionistic observations from §3 and ideally lead to contrastive refinement across linguistic sc"
2020.conll-shared.1,2020.conll-shared.7,1,0.75652,"Missing"
2020.conll-shared.1,D19-1393,0,0.0309981,"nt alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR"
2020.conll-shared.1,2020.findings-emnlp.89,0,0.016475,"ebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (20"
2020.conll-shared.1,2020.acl-main.119,0,0.179058,"s similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that do not enforce strict correspondences between tokens in the input sentence and the concepts in meaning representation graphs. The iterative inference framework is also based on an encoder–decoder architecture. The encoder takes the sentence as input and computes contextualized token embeddings that are used as text memory by a decoder that iteratively predicts the next node given the text memory and a predicted parent node in the partially constructed graph memory at the previous time step, and then identifies the parent node for the newly pre"
2020.conll-shared.1,2020.conll-shared.6,0,0.0921787,"guage per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu 1 Background and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the correspon"
2020.conll-shared.1,P13-2131,0,0.187673,"Missing"
2020.conll-shared.1,W11-2927,1,0.757531,"was not formally enforced. 5 Evaluation Following the previous edition of the shared task, the official MRP metric for the task is the microaverage F1 score across frameworks over all tuple types that encode ‘atoms’ of information in MRP graphs. The cross-framework metric uniformly evaluates graphs of different flavors, regardless of a specific framework exhibiting (a) labeled or unlabeled nodes or edges, (b) nodes with or without anchors, and (c) nodes and edges with optional properties and attributes, respectively (see Table 4). The MRP metric generalizes earlier frameworkspecific metrics (Dridan and Oepen, 2011; Cai and Knight, 2013; Hershcovich et al., 2019a) in terms of decomposing each graph into sets of typed tuples, as indicated in Figure 6. To quantify graph similarity in terms of tuple overlap, a correspondence relation between the nodes of the goldstandard and system graphs must be determined. Adapting a search procedure for the NP-hard maximum common edge subgraph (MCES) isomorphism problem, the MRP scorer will search for the node-to-node correspondence that maximizes the intersection of tuples between two graphs, where node identifiers (m and n in Figure 6) act like variables that can be e"
2020.conll-shared.1,K19-2007,0,0.0722366,"ground and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the corresponding (but richer) frameworks in 2020, EDS and PTG, respectively, and the original bi-lexical semantic dependency graphs remain independently available (Oepen et al., 2015). 1 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 1–22 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics (a) a unifying formal model over different semantic graph banks (§2)"
2020.conll-shared.1,1997.iwpt-1.10,0,0.772333,"Missing"
2020.conll-shared.1,W19-1202,0,0.0200921,"ntation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP"
2020.conll-shared.1,K19-2016,0,0.0872997,"UCCA, and AMR. This allows a comparison on nearly equal grounds: as Table 9 shows, in terms of LPPS F1 , the state-of-the-art has substantially improved for EDS and AMR parsing, but stayed the same for UCCA. However, as mentioned in §6, remote edge detection for UCCA improved substantially, though it carries only a small weight in terms of overall scores due to the scarcity of remote edges. For EDS, the strongest results were obtained in the MRP 2019 official competition by SUDA– Alibaba (Zhang et al., 2019c). However, in the post-evaluation stage, they were outperformed by the Peking system (Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard"
2020.conll-shared.1,D19-1278,0,0.0153051,"Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets"
2020.conll-shared.1,N18-2020,1,0.822488,"ed on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency frame and, thus, correspond more closely to the n"
2020.conll-shared.1,P19-4007,0,0.0514516,"Missing"
2020.conll-shared.1,2020.acl-main.629,0,0.125349,"Missing"
2020.conll-shared.1,N18-1104,0,0.0512423,"tudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite limited training data in the case of UCCA and DRG. Furthermore, the evaluation sets for most of the frameworks comprise different text types and subject matters—offering some hope of robustness to"
2020.conll-shared.1,N18-1000,0,0.197513,"Missing"
2020.conll-shared.1,K19-2006,0,0.10434,"been included in the CoNLL 2009 Shared Task on Semantic Role Labeling (Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019),"
2020.conll-shared.1,S19-2002,0,0.0495413,"Missing"
2020.conll-shared.1,hajic-etal-2012-announcing,0,0.357504,"Missing"
2020.conll-shared.1,kingsbury-palmer-2002-treebank,0,0.452155,"o distinguish different types of the underlying DRS elements. logy broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 4 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 1. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 4 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
2020.conll-shared.1,J16-4009,1,0.877647,"d (d) increased crossfertilization of parsing approaches (§7). 2 tute ordered graphs. A natural way to visualize a bi-lexical dependency graph is to draw its edges as semicircles in the halfplane above the sentence. An ordered graph is called noncrossing if in such a drawing, the semicircles intersect only at their endpoints (this property is a natural generalization of projectivity as it is known from dependency trees). A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. Increased terminological uniformity and guidance in how to navigate this rich and diverse landscape are among the desirable side-effects of the MRP task series. The following paragraphs provide semi-formal definiti"
2020.conll-shared.1,P17-1104,1,0.878575,".78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRG"
2020.conll-shared.1,P19-1232,0,0.0175512,"Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Compara"
2020.conll-shared.1,P18-1035,1,0.856893,"e invited to develop parsing systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a"
2020.conll-shared.1,W19-6202,0,0.0118155,"sed factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by"
2020.conll-shared.1,S19-2001,1,0.83448,"Missing"
2020.conll-shared.1,2020.findings-emnlp.288,0,0.0119332,"with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general"
2020.conll-shared.1,P19-1450,0,0.182109,"istinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four"
2020.conll-shared.1,P18-1040,0,0.0323045,", summarization, or text generation. Maybe equally importantly, the MRP task design capitalizes on uniformity of representations and evaluation, enabling resource creators and parser developers to more closely (inter)relate representations and parsing approaches across a diverse range of semantic graph frameworks. This facilitates DRG is a novel graph representation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also m"
2020.conll-shared.1,W19-1203,0,0.0982514,"he UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets are language-neutral. The mismatch between German tokens and English lemmas of senses must be expected to add additional complexity to"
2020.conll-shared.1,2020.acl-main.607,0,0.0482936,"Missing"
2020.conll-shared.1,K19-2001,1,0.563644,"Missing"
2020.conll-shared.1,P18-1037,0,0.0412388,"tokens and English lemmas of senses must be expected to add additional complexity to German DRG parsing. Direct comparison to non-MRP results is impossible: we are using a new version of AMRbank. Gold-standard tokenization is not provided for any of the frameworks. We use the MRP scorer. However, general trends appear consistent with recent developments. Pretrained embeddings and crosslingual transfer help; but multi-task learning less so. There is yet progress to be made in sharing information between parsers for different frameworks and making better use of their overlap. Prior to MRP 2019, Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. Lyu et al. (2020) recently improved the latent alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on al"
2020.conll-shared.1,K19-2003,1,0.845577,"on marks in the left or right periphery of a normalized anchor. Assuming the string Oh no! as a hypothetical parser input, the following anchorings will all be considered equivalent: {h0 : 6i}, {h0 : 2i, h3 : 6i}, {h0 : 1i, h1 : 6i}, and {h0 : 5i}. 6 Six teams submitted parser outputs to the shared task within the official evaluation period. In addition, we received two submissions after the submission deadline, which we mark as ‘unofficial’. We further include results from an additional ‘reference’ system by one of the task co-organizers, namely EDS outputs from the grammar-based ERG parser (Oepen and Flickinger, 2019). Table 5 presents an overview of the participating systems and the tracks and frameworks they submitted results for. All official systems submitted results for the cross-framework track (across all frameworks), and additionally five of them submitted results to the cross-lingual track as well (where TJU-BLCU did not submit UCCA parser outputs in the cross-lingual track). We note that the shared task explicitly allowed partial submissions, in order to lower the bar for participation (which is no doubt substantial). Two of the teams—ISCAS and TJUBLCU—declined the invitation to submit a system d"
2020.conll-shared.1,P14-5010,0,0.0027367,"ces of ‘raw’ sentence strings and (b) in pre-tokenized, partof-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premium-quality morpho-syntactic dependency analyses were provided to participants, called the MRP 2020 companion parses. These parses were obtained using a prerelease of the ‘future’ UDPipe architecture (Straka, 2018; Straka and Strakov´a, 2020), trained on available gold-standard UD 2.x treebanks, for English augmented with conversions from PTB-style annotations in the WSJ and OntoNotes corpora (Hovy et al., 2006), using the UD-style CoreNLP 4.0 tokenizer (Manning et al., 2014) and jack-knifing where appropriate (to avoid overlap with the texts underlying the MRP semantic graphs). Table 4: Different tuple types per framework. on-line CodaLab infrastructure. Teams were allowed to make repeated submissions, but only the most recent successful upload to CodaLab within the evaluation period was considered for the official, primary ranking of submissions. Task participants were encouraged to process all inputs using the same general parsing system, but—owing to inevitable fuzziness about what constitutes ‘one’ parser—this constraint was not formally enforced. 5 Evaluatio"
2020.conll-shared.1,S15-2153,1,0.842966,"Missing"
2020.conll-shared.1,J93-2004,0,0.0699436,", as fully lexically anchored and wholly unanchored, respectively, leading to the categorization of mixed forms of anchoring as Flavor (1), and allow for the presence of ordered graphs, in principle at least, at all levels of the hierarchy.2 Meaning Representation Frameworks The shared task combines five distinct frameworks for graph-based meaning representation, each with its specific formal and linguistic assumptions. This section reviews the frameworks and presents English example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 4 are prewhere unanchored nodes for unexpressed material beyond the surface string can be postulated (Schuster and Manning, 2016). Whether or not these nodes occupy a well-defined position in the otherwise total order of basic UD nodes remains an open questi"
2020.conll-shared.1,S16-1166,0,0.0329712,"context of the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the su"
2020.conll-shared.1,S14-2008,1,0.896688,"Missing"
2020.conll-shared.1,S17-2090,0,0.0310415,"the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although mo"
2020.conll-shared.1,2020.conll-shared.4,0,0.136232,"(Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word"
2020.conll-shared.1,2020.conll-shared.8,0,0.415434,"Missing"
2020.conll-shared.1,P17-1186,0,0.154444,"Missing"
2020.conll-shared.1,2020.lrec-1.497,1,0.867008,"Missing"
2020.conll-shared.1,W19-1204,0,0.0450391,"Missing"
2020.conll-shared.1,W16-6401,0,0.0666192,"Missing"
2020.conll-shared.1,Q18-1043,1,0.868093,"Missing"
2020.conll-shared.1,2020.conll-shared.5,0,0.310999,"impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1"
2020.conll-shared.1,N18-2040,1,0.887249,"Missing"
2020.conll-shared.1,L16-1376,0,0.0817331,"Missing"
2020.conll-shared.1,D17-1129,1,0.843784,"contextualized token embeddings with XLM-R (Conneau et al., 2019) on the encoder side, and then on the decoder side, uses separate attention heads to predict the node labels, identify anchors for nodes, and predict edges between nodes, as well as edge labels. Because the label set for nodes is typically very large, rather than predicting the node labels directly, the PERIN system reduces the search space by predicting ‘relative rules’ that can be used to map surface token strings to node labels in meaning representation graphs, an idea that is similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that d"
2020.conll-shared.1,P19-1454,0,0.0201296,"extualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by the fact that two of the 2020 frameworks (PTG and DR"
2020.conll-shared.1,D18-1263,0,0.0141923,"ng systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction"
2020.conll-shared.1,2020.emnlp-main.196,0,0.450632,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.wmt-1.104,0,0.540185,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.lt4hala-1.20,0,0.0828616,"Missing"
2020.conll-shared.1,2020.conll-shared.3,1,0.64593,"Missing"
2020.conll-shared.1,W15-3502,1,0.865834,"arly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges fr"
2020.conll-shared.1,P19-1009,0,0.173641,"Missing"
2020.conll-shared.1,D19-1392,0,0.119567,"Missing"
2020.conll-shared.1,N18-1063,1,0.899522,"Missing"
2020.conll-shared.1,K19-2014,0,0.104365,"Missing"
2020.conll-shared.1,P18-1016,1,0.879706,"true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient),"
2020.emnlp-main.432,L16-1557,0,0.0126394,"s languages and genres. The present work extends temporal dependency trees to the temporal dependency graphs, and crowd-sourced temporal dependency graphs on English news articles. 7.2 Crowdsourcing Temporal Relations Early studies on crowdsourcing temporal relations usually focus on some subtask of this problem. Snow et al. (2008) crowdsources the relations of a subset of verb event pairs from TimeBank (Pustejovsky et al., 2003b) whose relations are either “strictly before” or “strictly after”. Ng and Kan (2012) only focuses on the relation between events and time expressions from news data. Caselli et al. (2016) conducts crowdsourcing experiments on both temporal relation annotation and event / time expression extraction. In the time expression extraction experiments, they ask crowd workers to select time expressions directly from the raw text. In contrast, we give crowd workers time expression candidates and ask them binary questions. Our approach prevents crowd workers from selecting wrong textual spans. Ning et al. (2018) comes up with a multi-axis approach for event temporal relation annotation (see Ning et al., 2018 Section 2 and Appendix A for more details about their multi-axis model). The mul"
2020.emnlp-main.432,P14-2082,0,0.135578,"ounts to constructing a fully connected graph in which every event is connected to another event. With  this pair-wise approach, a text of n events has n2 event pairs that need to be considered. As the value of n increases, the number of event pairs quickly becomes very large. In practice, attempts to achieve complete annotation often fell far short and had to settle with covering all event pairs within a short text window (e.g., within adjacent sentences). Even with this restriction, it is still difficult to produce a large data set. For example, there are only 36 articles in TimeBankDense (Cassidy et al., 2014). On the other end of the spectrum, approaches that allow the annotators to select a subset of the event pairs to annotate often end up with sparse and inconsistent annotation, as different annotators often select different event pairs to annotate. For example, while the TimeBank corpus has relatively more articles, but only annotates a relatively small number of temporal relations (6,418 in total). There are also efforts that report improved annotation consistency by focusing on specific syntactic constructions (Bethard et al., 2007) or one aspect of temporal annotation (Reimers et al., 2016)"
2020.emnlp-main.432,D14-1082,0,0.0159788,"tactic patterns to identify candidate reference events. For example, for events 5372 in complement clauses their reference events are typically the matrix events. The event in the subject of a sentence depends on the main verb for temporal interpretation (3a). In (3b), the event in a purpose clause depends on the main verb. When there is a temporal conjunction (3c), it provides clue for the temporal dependency between the event in the temporal modifier and the event in the main clause. Based on this discovery, we extract event pairs in the following structures with Stanford dependency parser (Chen and Manning, 2014): complement clauses, relative clauses, temporal conjunctions, arguments and predicates, and purpose expressions. (3) a. The landslide hit the village. b. I got up at 6am to take the train. c. Right before I got to the station, the train left. Cross-sentence annotation In wikinews articles, each paragraph is usually a self-contained discourse segment. We assume that the first event of a discourse segment starts a new temporal chain and does not have a reference event. In addition, to make the annotation problem practical, we limit the maximum number of reference event candidates to be 4 when p"
2020.emnlp-main.432,P12-1010,0,0.0160093,"st data. The test data is annotated by experts, and the validation data is generated from crowdsourced annotation as follows: if there is no agreement for one question, i.e. 6 https://github.com/yuchenz/tdp_ ranking Baseline Neural te,te e,te e,e full te,te e,te e,e full Unlabeled F dev test 0.80 0.82 0.54 0.70 0.64 0.61 0.62 0.68 0.88 0.93 0.62 0.77 0.7 0.77 0.69 0.79 Labeled F dev test 0.80 0.82 0.46 0.58 0.26 0.34 0.41 0.51 0.88 0.93 0.53 0.66 0.5 0.58 0.55 0.66 Table 8: Experiment results of the baseline system and the neural ranking model. 7 7.1 Related Work Temporal Dependency Structure Kolomiyets et al. (2012) are the first work that use the term temporal dependencies, and they extract timelines from narrative stories as temporal dependency trees. However, in their work, only events are included as nodes in the dependency tree, and the parent of each node is not explicitly defined as the reference event of the child event. Zhang and Xue (2018b) first defined a temporal dependency tree structure that have both events and time expressions as nodes in the tree, and attempted 5375 to explicitly define the parent of each event or time expression as the reference event or time expression of the child nod"
2020.emnlp-main.432,P14-5010,0,0.00445009,"Missing"
2020.emnlp-main.432,C12-1129,0,0.0270871,"ata, and to both news reports and narrative stories, indicating this framework can be applied across languages and genres. The present work extends temporal dependency trees to the temporal dependency graphs, and crowd-sourced temporal dependency graphs on English news articles. 7.2 Crowdsourcing Temporal Relations Early studies on crowdsourcing temporal relations usually focus on some subtask of this problem. Snow et al. (2008) crowdsources the relations of a subset of verb event pairs from TimeBank (Pustejovsky et al., 2003b) whose relations are either “strictly before” or “strictly after”. Ng and Kan (2012) only focuses on the relation between events and time expressions from news data. Caselli et al. (2016) conducts crowdsourcing experiments on both temporal relation annotation and event / time expression extraction. In the time expression extraction experiments, they ask crowd workers to select time expressions directly from the raw text. In contrast, we give crowd workers time expression candidates and ask them binary questions. Our approach prevents crowd workers from selecting wrong textual spans. Ning et al. (2018) comes up with a multi-axis approach for event temporal relation annotation"
2020.emnlp-main.432,P18-1122,0,0.23688,"ing, the ability to achieve a large-scale highquality temporally annotated data set has been the bottleneck in advancing the state of the art in this area. Even though the first temporal annotation scheme, TimeML (Pustejovsky et al., 2003a; Saur´ı et al., 2006), was proposed over a decade ago, temporally annotated data is still relatively scarce. The largest data set that we are aware of is the data set used in TempEval-3 (UzZaman et al., 2013), and it consists of 276 articles from the TimeBank Corpus (Pustejovsky et al., 2003b) and the AQUAINT Corpus. This data set was later re-annotated by (Ning et al., 2018) to improve its annotation consistency using a crowdsourcing approach. There are many challenges that have contributed to this state of affairs. Temporal relations are often 1 https://github.com/Jryao/temporal_ dependency_graphs_crowdsourcing confounded with modalities (How do you order an event that actually happened with one that might happen?). Some events are ambiguous between an instantaneous and stative reading (Does “marriage” refer to the start of the marriage or does it refer to the state when marriage is in effect?) and this complicates its temporal relation with other events. While"
2020.emnlp-main.432,W11-0419,0,0.0378967,"phenomena like this, we extend the temporal dependency tree to temporal dependency graph, where each event always has a reference time that is a time expression or a meta node. We call this the reference timex. Optionally it can also have another event as its reference time, and we call this the reference event. The reference event is optional because not all events have an reference event. For example, in (2), went does not have another event as its reference time and only has a reference timex. In TDG, the reference timex of an event is the most specific (i.e., the smallest) narrative time (Pustejovsky and Stubbs, 2011) that the event can be placed into. If such a narrative time is not available, this event should be anchored to DCT. Depend-on yesterday Includes went Includes Before had Figure 1: Temporal Dependency Structure for (2). 3 Crowdsourcing Strategy Crowdsourcing is generally accepted as a costeffective alternative to the traditional annotation approach where annotators are provided detailed guidelines and carefully trained to meet certain consistency threshold before productive annotation can start. In a crowdsourcing setting, we oftentimes hire a much larger set of annotators that are not profess"
2020.emnlp-main.432,P16-1207,0,0.0128005,"Cassidy et al., 2014). On the other end of the spectrum, approaches that allow the annotators to select a subset of the event pairs to annotate often end up with sparse and inconsistent annotation, as different annotators often select different event pairs to annotate. For example, while the TimeBank corpus has relatively more articles, but only annotates a relatively small number of temporal relations (6,418 in total). There are also efforts that report improved annotation consistency by focusing on specific syntactic constructions (Bethard et al., 2007) or one aspect of temporal annotation (Reimers et al., 2016), but this comes at the cost of incomplete annotation. One promising recent approach to get out of this 5368 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5368–5380, c November 16–20, 2020. 2020 Association for Computational Linguistics dilemma is to focus on dependencies between time expressions, between time expressions and events, and between events (Zhang and Xue, 2018b), based on the observation that time expressions and events are often expressed in relative terms and their temporal location needs to be understood with a reference time in m"
2020.emnlp-main.432,D08-1027,0,0.0407442,"Missing"
2020.emnlp-main.432,S13-2001,0,0.0193789,"Missing"
2020.emnlp-main.432,J88-2006,0,0.823551,"pend on today, as it may or may not happen on that day. In contrast, in (1c), the temporal location of re-examine can only be understood with respect to the DCT, not said. Another example of one event depends on another event for its temporal interpretation is (1d), where the temporal interpretation of demanded depends on entered (it happened after entered), the temporal interpretation of assaulted depends on demanded, and the temporal interpretation of left depends on assaulted. This is a linguistic phenomenon known as temporal anaphora (Reichenbach, 1949; Partee, 1973, 1984; Hinrichs, 1986; Webber, 1988; Bohnemeyer, 2009) that has been extensively studied in computational linguistics. The working hypothesis of this dependency-based approach is that by annotating the dependencies, additional temporal relations can be inferred, via transitivity, or via common sense reasoning. This hypothesis seems to have been born out in (1d): Based on the dependencies, one can additionally infer that assaulted happened after entered, for example. Zhang and Xue (2018b) made the assumption that there is exactly one reference time for each event or time expression. With this assumption, the temporal relations b"
2020.emnlp-main.432,D18-1371,1,0.422748,"here are also efforts that report improved annotation consistency by focusing on specific syntactic constructions (Bethard et al., 2007) or one aspect of temporal annotation (Reimers et al., 2016), but this comes at the cost of incomplete annotation. One promising recent approach to get out of this 5368 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5368–5380, c November 16–20, 2020. 2020 Association for Computational Linguistics dilemma is to focus on dependencies between time expressions, between time expressions and events, and between events (Zhang and Xue, 2018b), based on the observation that time expressions and events are often expressed in relative terms and their temporal location needs to be understood with a reference time in mind. Consider the examples in (1): (1) a. He left on Friday. He left home at 9:00am. b. The Pentagon said today that it would reexamine the question. c. The Pentagon said today that it will reexamine the question. d. Ricky New entered the store carrying a large stick, demanded money, assaulted the clerk with the stick, and left with an undisclosed amount of money. In (1a), the interpretation of the time expression 9:00a"
2020.emnlp-main.432,L18-1490,1,0.471819,"here are also efforts that report improved annotation consistency by focusing on specific syntactic constructions (Bethard et al., 2007) or one aspect of temporal annotation (Reimers et al., 2016), but this comes at the cost of incomplete annotation. One promising recent approach to get out of this 5368 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5368–5380, c November 16–20, 2020. 2020 Association for Computational Linguistics dilemma is to focus on dependencies between time expressions, between time expressions and events, and between events (Zhang and Xue, 2018b), based on the observation that time expressions and events are often expressed in relative terms and their temporal location needs to be understood with a reference time in mind. Consider the examples in (1): (1) a. He left on Friday. He left home at 9:00am. b. The Pentagon said today that it would reexamine the question. c. The Pentagon said today that it will reexamine the question. d. Ricky New entered the store carrying a large stick, demanded money, assaulted the clerk with the stick, and left with an undisclosed amount of money. In (1a), the interpretation of the time expression 9:00a"
2020.emnlp-main.432,S19-1019,1,0.88156,"allowing each event to have a reference time expression, a reference event, or both. Compared with TDT, TDG does not substantially increase the number of temporal relations in a text that need to be annotated while improving its expressiveness. We also investigate the feasibility of annotating TDGs from scratch via crowdsourcing, meaning we start with identifying events and time expressions, and then annotate the temporal relations between them. Previous work on crowdsourcing temporal annotations relies on the events and time expressions already identified in the TimeBank (Ning et al., 2018; Zhang and Xue, 2019), and this limits the possibility of expanding temporally annotated datasets beyond what already exist. We show that with a carefully designed annotation strategy, annotating TDGs via crowdsourcing is feasible. We annotated a corpus of 500 Wikinews articles with this approach, and created the largest corpus in terms of the number of articles and the number of event or time expression pairs. The remainder of the paper is organized as follows. We provide a brief overview of the TDT representation and propose our extension in Section 2. We present our crowdsourcing strategy in Section 3. We prese"
2020.paclic-1.65,W13-2322,0,0.0380154,"Missing"
2020.paclic-1.65,P14-1134,0,0.0268151,"encoding the same semantic information as their regular verb counterparts. Since the meaning of LVCs differ from usual predicative structures or the direct aggregation of its semantic components, LVCs, as one of the least explored areas of MWEs in computational linguistics, pose a number of challenges in computational grammar, such as automatic word alignment, annotation and semantic representation. In the model of Abstract Meaning Representation (AMR), it is often assumed that the LVCs and its corresponding regular verb construction (RVC) share the same representation (Banarescu et al. 2012; Flanigan et al. 2014, Bu et al. 2016). AMR is a semantic framework addressing the predicate-argument relation of the whole sentence (to be more fully described in Section 4). However, corpus data suggest that LVCs and RVCs have slightly different semantic meaning. In Urdu, for example, LVs play a central role in the meaning and morphosyntactic choices of the whole construction in (1). Although the LVs par ‘fall’ and ḍaal ‘put’ both occur with ciikh ‘scream’ in Urdu, the LV in (1a), which involves an involuntary action, is preceded by an unmarked nominative subject, whereas the LV ḍaal ‘put’ in (1b), denoting a co"
2021.acl-long.122,J12-2003,0,0.0797182,"Missing"
2021.acl-long.122,N19-1423,0,0.00579591,"rpus and FactBank. 3 Neural Modal Dependency Parsing In this section, we introduce our parser for modal dependency parsing. Our modal dependency parser is inspired by Zhang and Xue (2018a), who introduce a ranking model for temporal dependency parsing. As the temporal dependency tree used to train their model is similar in structure to our modal dependency tree, it is reasonable to adopt their model as the starting point. Our model is also inspired by Ross et al. (2020), who extend Zhang and Xue (2018a) by replacing the Bi-LSTM encoder with contextualized neural language models, such as BERT (Devlin et al., 2019). Specifically, our modal dependency parser constructs a modal 5 https://aylien.com/blog/ free-coronavirus-news-dataset 1544 dependency tree by incrementally identifying the parent node for each child node in textual order. For each child node, the parser ranks the candidate parent nodes and selects the one with the highest score as its parent node. Since the nodes in a modal dependency tree are events or conceivers, to parse a text into a modal dependency tree, we need to first extract the events and conceivers, then build the modal dependency structure. Since Zhang and Xue (2018a)’s pipeline"
2021.acl-long.122,W09-3012,0,0.126768,"Missing"
2021.acl-long.122,D15-1189,0,0.0199613,"bility of a source purely based on the information within a single text, but linking each event to its source or chain of sources allows us to verify the factuality of the event against other sources and our world knowledge. Therefore, identifying the level of certainty with which is an event is asserted together with its source is a crucial first step in assessing the factuality of the event. Previous work on factuality assessment has focused on determining the level of certainty that is asserted on events and framed it as a classification or regression problem (Saur´ı and Pustejovsky, 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018; Qian et al., 2018). However, as we discussed above, the level of certainty alone is insufficient in determining the factuality of an event. In this work, we adopt a factuality representation framework proposed in (Vigus et al., 2019) called modal dependency structure (MDS). A modal dependency structure is formally a document-level structure where nodes are events and sources, known as conceivers while edges represent the modal strength, or the level of certainty that the conceiver holds towards an event. Figure 1 shows the modal dependency struc"
2021.acl-long.122,P14-5010,0,0.00554646,"Missing"
2021.acl-long.122,matsuyoshi-etal-2010-annotating,0,0.0971354,"Missing"
2021.acl-long.122,L16-1699,0,0.0527857,"Missing"
2021.acl-long.122,P18-1122,0,0.0549285,"Missing"
2021.acl-long.122,P19-1432,0,0.0116116,"-workers to annotate modal dependency structures with considerable consistency and produce modal dependency annotation at scale. Automatic factuality assessment Existing work typically casts factuality assessment as a classification or regression problem. For example, Saur´ı and Pustejovsky (2012) and Prabhakaran et al. (2015) adopt rule-based and feature-based machine learning approaches to factuality classification. More recently, Qian et al. (2018) predict event factuality via a Generative Adversarial Networks based model. Rudinger et al. (2018) design two LSTM based models, and Pouran Ben Veyseh et al. (2019) use a graph-based neural network model for event factuality prediction. Our work departs from previous practice and recasts factuality assessment as modal dependency parsing to simultaneously predict the source and its level of certainty over an event, and exposes both for downstream applications. 5 Conclusion and Future Work In this paper, we proposed a novel approach to factuality assessment by casting it as a modal dependency parsing problem. We first built a large data set annotated with modal dependency structures via crowdsourcing, and demonstrated the quality of this data set with a ca"
2021.acl-long.122,2020.emnlp-main.689,1,0.844949,"or an event as parent. MDS FactBank Doc 353 208 Conc 2,938 - Event 24,016 9,488 Table 4: Number of documents, conceivers, and events in this corpus and FactBank. 3 Neural Modal Dependency Parsing In this section, we introduce our parser for modal dependency parsing. Our modal dependency parser is inspired by Zhang and Xue (2018a), who introduce a ranking model for temporal dependency parsing. As the temporal dependency tree used to train their model is similar in structure to our modal dependency tree, it is reasonable to adopt their model as the starting point. Our model is also inspired by Ross et al. (2020), who extend Zhang and Xue (2018a) by replacing the Bi-LSTM encoder with contextualized neural language models, such as BERT (Devlin et al., 2019). Specifically, our modal dependency parser constructs a modal 5 https://aylien.com/blog/ free-coronavirus-news-dataset 1544 dependency tree by incrementally identifying the parent node for each child node in textual order. For each child node, the parser ranks the candidate parent nodes and selects the one with the highest score as its parent node. Since the nodes in a modal dependency tree are events or conceivers, to parse a text into a modal depe"
2021.acl-long.122,N18-1067,0,0.0247917,"Missing"
2021.acl-long.122,ruppenhofer-rehbein-2012-yes,0,0.0350048,"ers from a different sentence just as easily as from the same sentence, it is much more difficult for the system to attach a child node to a distant conceiver. Addressing this challenge will be crucial to further improve the performance of the model. 4 Related Work Factuality Annotation While there is a significant amount of research on annotating factuality or modality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009; Matsuyoshi et al., 2010; Soni et al., 2014; Lee et al., 2015; Prabhakaran et al., 2015; Minard et al., 2016), factuality and opinions (Son et al., 2014), senses of modal verbs (Ruppenhofer and Rehbein, 2012), and credibility in social media (Mitra and Gilbert, 2015), a few of them are particularly related to our work. Our annotation is closely related to FactBank Saur´ı and Pustejovsky (2009) in that both annotate the level of certainty that the source asserts over an event, but FactBank does not explicitly represent their relations in a hierarchical structure and is annotated by expert annotators. Like our work, Lee et al. (2015) also annotate event factuality via crowdsourcing, but they only annotate the level of certainty from the perspective of the author, to the exclusion of nonauthor concei"
2021.acl-long.122,J12-2002,0,0.0869992,"Missing"
2021.acl-long.122,P14-2068,0,0.0484845,"Missing"
2021.acl-long.122,W19-3321,0,0.0321499,"Missing"
2021.acl-long.122,D18-1371,1,0.930745,"esentations when multiple sources are possiIn this section we first provide additional detail for the modal dependency structure, and then present our strategy of decomposing the modal dependency structure into subtasks that are suitable for crowdsourcing. We also evaluate the quality of our annotated data set, and provide statistics relevant for training MDS parsing models. 1541 2.1 Modal Dependency Structure The modal dependency structure builds on the annotation scheme of FactBank (Saur´ı and Pustejovsky, 2009) and is inspired by the structured approach in temporal dependency annotation in Zhang and Xue (2018b). Like FactBank, the modal dependency structure combines epistemic strength full, partial, neutral with polarity values positive, negative to define a set of six values for modal strength. Table 1 shows the modal strength values (i.e. labels) used in modal dependency structures and their corresponding values in FactBank. As illustrated in Figure 1, these values are represented as edge labels in the modal dependency structure. Readers are referred to (Vigus et al., 2019) for how these six values are defined. Modal Dependency full positive (Pos) partial positive (Prt) neutral positive (Neut) n"
2021.acl-long.122,L18-1490,1,0.927987,"esentations when multiple sources are possiIn this section we first provide additional detail for the modal dependency structure, and then present our strategy of decomposing the modal dependency structure into subtasks that are suitable for crowdsourcing. We also evaluate the quality of our annotated data set, and provide statistics relevant for training MDS parsing models. 1541 2.1 Modal Dependency Structure The modal dependency structure builds on the annotation scheme of FactBank (Saur´ı and Pustejovsky, 2009) and is inspired by the structured approach in temporal dependency annotation in Zhang and Xue (2018b). Like FactBank, the modal dependency structure combines epistemic strength full, partial, neutral with polarity values positive, negative to define a set of six values for modal strength. Table 1 shows the modal strength values (i.e. labels) used in modal dependency structures and their corresponding values in FactBank. As illustrated in Figure 1, these values are represented as edge labels in the modal dependency structure. Readers are referred to (Vigus et al., 2019) for how these six values are defined. Modal Dependency full positive (Pos) partial positive (Prt) neutral positive (Neut) n"
2021.acl-long.138,D15-1109,0,0.143338,"ation snippet by modeling dependencies between pronouns with general conditional random fields. A major shortcoming of these DPR methods is that they overlook the discourse relation (e.g., reply, question) between conversa1752 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1752–1763 August 1–6, 2021. ©2021 Association for Computational Linguistics tional utterances when exploiting the context of the dropped pronoun. At the same time, previous CDP methods (Li et al., 2014; Afantenos et al., 2015; Shi and Huang, 2019) first predict the relation for each utterance pair and then construct the discourse structure for the conversation with a decoding algorithm. The effectiveness of these methods are compromised since the utterances might be incomplete when they have dropped pronouns. To overcome these shortcomings, we propose a novel neural model called DiscProReco to perform DPR and CDP jointly. Figure 1 is a Chinese conversation snippet between two speakers A and B that illustrates the advantages of such a joint approach. In this example, a pronoun “你 (you)” is dropped in utterance B3 ."
2021.acl-long.138,P16-1074,0,0.0289272,"al. (2017) further employed a linear-chain CRF to jointly predict the position and type of the dropped pronouns in a single utterance using hand-crafted features. Due to the powerful semantic modeling capability of deep learning, Zhang et al. (2019); Yang et al. (2019) introduced neural network methods to recover the dropped pronoun by modeling its semantics from the context. All these methods represent the utterances without considering the relationship between utterances, which is important to identify the referents. Zero pronoun resolution is also a closely related line of research to DPR (Chen and Ng, 2016; Yin et al., 2017, 2018). The main difference between DPR and zero pronoun resolution task is that DPR considers both anaphoric and non-anaphoric pronouns, and doesn’t attempt to resolve it to a referent. 1759 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016). A deep sequential model (Shi and Huang, 2019) was further presented to predict the discourse dependencies utilizing both local information"
2021.acl-long.138,2020.acl-main.747,0,0.061749,"Missing"
2021.acl-long.138,D19-1015,0,0.153,"overy (DPR) aims to locate the position of the dropped pronoun and identify its type. Conversational discourse parsing (CDP) is another important task that aims to analyze the discourse relations among utterances Corresponding author 张 老师 在家 吗 ？ reply Introduction ∗ A1: reply in a conversation, and plays a vital role in understanding multi-turn conversations. Existing work regards DPR and CDP as two independent tasks and tackles them separately. As an early attempt of DPR, Yang et al. (2015) employ a Maximum Entropy classifier to predict the position and type of dropped pronouns. Zhang et al. (2019) and Yang et al. (2019) attempt to recover the dropped pronouns by modeling the referents with deep neural networks. More recently, Yang et al. (2020) attempt to jointly predict all dropped pronouns in a conversation snippet by modeling dependencies between pronouns with general conditional random fields. A major shortcoming of these DPR methods is that they overlook the discourse relation (e.g., reply, question) between conversa1752 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
2021.acl-long.138,P19-1237,0,0.0210415,"ntal Results We list the experimental results of our approach and the baselines in Table 2. For the STAC dataset, we also reported the original results of the STAC benchmark from an existing paper (Shi and Huang, 2019), and apply our DiscProReco to this corpus. For the SPDPR dataset, we ran the baseline methods with the same parameter settings. From the results we can see that the variant of our approach DiscProReco (w/o DPR) outperforms the baselines of discourse parsing. We attribute this to the effectiveness of the biaffine attention mechanism for dependency parsing task (Yan et al., 2020; Ji et al., 2019). However, our approach DiscProReco still significantly outperforms all the compared models. We attribute this to the joint training of the CDP task and the DPR task. The parameter sharing mechanism makes these two tasks benefits each other. Note that the results for the joint model is not available for STAC as STAC is not annotated with dropped pronouns. DPR CDP 56 54 52 50 48 46 44 0.2 0.4 0.6 0.8 1.0 1.2 Ratio between to Figure 4: Exploratory results. (a) Interaction between DPR and CDP; (b) Effects of parameters (i.e., α and β) recovery. Moreover, Figure 4 (b) illustrate the performance of"
2021.acl-long.138,D14-1220,0,0.191,"ouns in a conversation snippet by modeling dependencies between pronouns with general conditional random fields. A major shortcoming of these DPR methods is that they overlook the discourse relation (e.g., reply, question) between conversa1752 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1752–1763 August 1–6, 2021. ©2021 Association for Computational Linguistics tional utterances when exploiting the context of the dropped pronoun. At the same time, previous CDP methods (Li et al., 2014; Afantenos et al., 2015; Shi and Huang, 2019) first predict the relation for each utterance pair and then construct the discourse structure for the conversation with a decoding algorithm. The effectiveness of these methods are compromised since the utterances might be incomplete when they have dropped pronouns. To overcome these shortcomings, we propose a novel neural model called DiscProReco to perform DPR and CDP jointly. Figure 1 is a Chinese conversation snippet between two speakers A and B that illustrates the advantages of such a joint approach. In this example, a pronoun “你 (you)” is d"
2021.acl-long.138,P18-2023,0,0.0479469,"Missing"
2021.acl-long.138,D17-1159,0,0.0298965,"equential token states X and C, and revises them as syntactic token states as HX and HC by exploring the syntactic dependencies between the tokens based on a directed GCN. Specifically, for each input utterance in X and C, we first extract syntactic dependencies between the tokens with Stanford’s Stanza dependency parser (Qi et al., 2020). Using the output of the dependency parser, we construct a syntactic dependency graph for each utterance in which the nodes represents the tokens and the edges correspond to the extracted syntactic dependencies between the tokens. Following the practices of (Marcheggiani and Titov, 2017; Vashishth et al., 2018), three types of edges are defined in the graph. The node states are initialized by the sequential token states X and C, and then message passing is performed over the constructed graph using the directed GCN (Kipf and Welling, 2017), referred to as SynGCN. The syntactic dependency representation of token wi,n after (k + 1)-th GCN layer is defined as: hk+1 wi,n = ReLU P k u∈N+ (wi,n ) ge · Wke hku + bke  , where Wke ∈ Rd×d and bke ∈ Rd are the edgespecific parameters, N+ (wi,n ) = N (wi,n ) ∪ {wi,n } is the set of wi,n ’s neighbors including itself, and ReLU(·) = ma"
2021.acl-long.138,C12-1115,0,0.0359272,"ds represent the utterances without considering the relationship between utterances, which is important to identify the referents. Zero pronoun resolution is also a closely related line of research to DPR (Chen and Ng, 2016; Yin et al., 2017, 2018). The main difference between DPR and zero pronoun resolution task is that DPR considers both anaphoric and non-anaphoric pronouns, and doesn’t attempt to resolve it to a referent. 1759 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016). A deep sequential model (Shi and Huang, 2019) was further presented to predict the discourse dependencies utilizing both local information of two utterances and the global information of existing constructed discourse structure. All these methods consider how to do relation prediction independently. However, in this work, we explore the connection between the CDP and DPR, and attempt to make these two tasks mutually enhance each other. 7 Conclusion This paper presents that dropped pronoun recovery and conversational discourse par"
2021.acl-long.138,N16-1013,0,0.0237078,"ip between utterances, which is important to identify the referents. Zero pronoun resolution is also a closely related line of research to DPR (Chen and Ng, 2016; Yin et al., 2017, 2018). The main difference between DPR and zero pronoun resolution task is that DPR considers both anaphoric and non-anaphoric pronouns, and doesn’t attempt to resolve it to a referent. 1759 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016). A deep sequential model (Shi and Huang, 2019) was further presented to predict the discourse dependencies utilizing both local information of two utterances and the global information of existing constructed discourse structure. All these methods consider how to do relation prediction independently. However, in this work, we explore the connection between the CDP and DPR, and attempt to make these two tasks mutually enhance each other. 7 Conclusion This paper presents that dropped pronoun recovery and conversational discourse parsing are two strongly related tasks. To make them benefit from"
2021.acl-long.138,2020.acl-demos.14,0,0.145894,"overy layer explores the referent semantics from the context C and predicts the dropped pronouns in each utterance. 3.2 Syntactic Dependency Encoding Layer As the sequential token states overlook longdistance dependencies among tokens in a utterance, this layer takes in the sequential token states X and C, and revises them as syntactic token states as HX and HC by exploring the syntactic dependencies between the tokens based on a directed GCN. Specifically, for each input utterance in X and C, we first extract syntactic dependencies between the tokens with Stanford’s Stanza dependency parser (Qi et al., 2020). Using the output of the dependency parser, we construct a syntactic dependency graph for each utterance in which the nodes represents the tokens and the edges correspond to the extracted syntactic dependencies between the tokens. Following the practices of (Marcheggiani and Titov, 2017; Vashishth et al., 2018), three types of edges are defined in the graph. The node states are initialized by the sequential token states X and C, and then message passing is performed over the constructed graph using the directed GCN (Kipf and Welling, 2017), referred to as SynGCN. The syntactic dependency repr"
2021.acl-long.138,P19-1003,0,0.0278226,"(a) Interaction between DPR and CDP; (b) Effects of parameters (i.e., α and β) recovery. Moreover, Figure 4 (b) illustrate the performance of DPR and CDP when the ratio between α to β varies gradually. Results show that the performance of CDP remains stable, while the performance of DPR increases at beginning and then decrease sharply as the ratio increases, indicating that DiscProReco framework should pay more attention to DPR during the optimizing process. 6 Related Work Dropped pronoun recovery is a critical technique that can benefit many downstream applications (Wang et al., 2016, 2018; Su et al., 2019). Yang et al. (2015) for the first time proposed this task, and utilized a Maximum Entropy classifier to recover the dropped pronouns in text messages. Giannella et al. (2017) further employed a linear-chain CRF to jointly predict the position and type of the dropped pronouns in a single utterance using hand-crafted features. Due to the powerful semantic modeling capability of deep learning, Zhang et al. (2019); Yang et al. (2019) introduced neural network methods to recover the dropped pronoun by modeling its semantics from the context. All these methods represent the utterances without consi"
2021.acl-long.138,P18-1149,0,0.0167781,", and revises them as syntactic token states as HX and HC by exploring the syntactic dependencies between the tokens based on a directed GCN. Specifically, for each input utterance in X and C, we first extract syntactic dependencies between the tokens with Stanford’s Stanza dependency parser (Qi et al., 2020). Using the output of the dependency parser, we construct a syntactic dependency graph for each utterance in which the nodes represents the tokens and the edges correspond to the extracted syntactic dependencies between the tokens. Following the practices of (Marcheggiani and Titov, 2017; Vashishth et al., 2018), three types of edges are defined in the graph. The node states are initialized by the sequential token states X and C, and then message passing is performed over the constructed graph using the directed GCN (Kipf and Welling, 2017), referred to as SynGCN. The syntactic dependency representation of token wi,n after (k + 1)-th GCN layer is defined as: hk+1 wi,n = ReLU P k u∈N+ (wi,n ) ge · Wke hku + bke  , where Wke ∈ Rd×d and bke ∈ Rd are the edgespecific parameters, N+ (wi,n ) = N (wi,n ) ∪ {wi,n } is the set of wi,n ’s neighbors including itself, and ReLU(·) = max(0, ·) is the Rectified"
2021.acl-long.138,N16-1113,0,0.0132274,"e 4: Exploratory results. (a) Interaction between DPR and CDP; (b) Effects of parameters (i.e., α and β) recovery. Moreover, Figure 4 (b) illustrate the performance of DPR and CDP when the ratio between α to β varies gradually. Results show that the performance of CDP remains stable, while the performance of DPR increases at beginning and then decrease sharply as the ratio increases, indicating that DiscProReco framework should pay more attention to DPR during the optimizing process. 6 Related Work Dropped pronoun recovery is a critical technique that can benefit many downstream applications (Wang et al., 2016, 2018; Su et al., 2019). Yang et al. (2015) for the first time proposed this task, and utilized a Maximum Entropy classifier to recover the dropped pronouns in text messages. Giannella et al. (2017) further employed a linear-chain CRF to jointly predict the position and type of the dropped pronouns in a single utterance using hand-crafted features. Due to the powerful semantic modeling capability of deep learning, Zhang et al. (2019); Yang et al. (2019) introduced neural network methods to recover the dropped pronoun by modeling its semantics from the context. All these methods represent the"
2021.acl-long.138,W16-1720,1,0.898548,"Missing"
2021.acl-long.138,2020.tacl-1.6,0,0.0274065,"pronouns. Experimental Results We list the experimental results of our approach and the baselines in Table 2. For the STAC dataset, we also reported the original results of the STAC benchmark from an existing paper (Shi and Huang, 2019), and apply our DiscProReco to this corpus. For the SPDPR dataset, we ran the baseline methods with the same parameter settings. From the results we can see that the variant of our approach DiscProReco (w/o DPR) outperforms the baselines of discourse parsing. We attribute this to the effectiveness of the biaffine attention mechanism for dependency parsing task (Yan et al., 2020; Ji et al., 2019). However, our approach DiscProReco still significantly outperforms all the compared models. We attribute this to the joint training of the CDP task and the DPR task. The parameter sharing mechanism makes these two tasks benefits each other. Note that the results for the joint model is not available for STAC as STAC is not annotated with dropped pronouns. DPR CDP 56 54 52 50 48 46 44 0.2 0.4 0.6 0.8 1.0 1.2 Ratio between to Figure 4: Exploratory results. (a) Interaction between DPR and CDP; (b) Effects of parameters (i.e., α and β) recovery. Moreover, Figure 4 (b) illustrate"
2021.acl-long.138,N19-1095,1,0.913434,"aims to locate the position of the dropped pronoun and identify its type. Conversational discourse parsing (CDP) is another important task that aims to analyze the discourse relations among utterances Corresponding author 张 老师 在家 吗 ？ reply Introduction ∗ A1: reply in a conversation, and plays a vital role in understanding multi-turn conversations. Existing work regards DPR and CDP as two independent tasks and tackles them separately. As an early attempt of DPR, Yang et al. (2015) employ a Maximum Entropy classifier to predict the position and type of dropped pronouns. Zhang et al. (2019) and Yang et al. (2019) attempt to recover the dropped pronouns by modeling the referents with deep neural networks. More recently, Yang et al. (2020) attempt to jointly predict all dropped pronouns in a conversation snippet by modeling dependencies between pronouns with general conditional random fields. A major shortcoming of these DPR methods is that they overlook the discourse relation (e.g., reply, question) between conversa1752 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1752–1763 Augus"
2021.acl-long.138,2020.findings-emnlp.13,1,0.650294,"that aims to analyze the discourse relations among utterances Corresponding author 张 老师 在家 吗 ？ reply Introduction ∗ A1: reply in a conversation, and plays a vital role in understanding multi-turn conversations. Existing work regards DPR and CDP as two independent tasks and tackles them separately. As an early attempt of DPR, Yang et al. (2015) employ a Maximum Entropy classifier to predict the position and type of dropped pronouns. Zhang et al. (2019) and Yang et al. (2019) attempt to recover the dropped pronouns by modeling the referents with deep neural networks. More recently, Yang et al. (2020) attempt to jointly predict all dropped pronouns in a conversation snippet by modeling dependencies between pronouns with general conditional random fields. A major shortcoming of these DPR methods is that they overlook the discourse relation (e.g., reply, question) between conversa1752 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1752–1763 August 1–6, 2021. ©2021 Association for Computational Linguistics tional utterances when exploiting the context of the dropped prono"
2021.acl-long.138,P15-2051,1,0.868055,"You) can give her a call. Or you can call her tomorrow morning. expansion pl eo na st ic She will be at home tomorrow. Figure 1: Top: A conversation snippet in which the dropped pronoun is shown in bracket. Bottom: Pronoun recovery results by two baselines and the proposed DiscProReco. Baselines which ignore the relation “(B3 expands B2) replies A2” mistakenly recover the dropped pronoun 你(you) as 我(I) since the utterance B3 is considered semantically similar to A2 . Pronouns are often dropped in Chinese conversations as the identity of the pronoun can be inferred from the context (Kim, 2000; Yang et al., 2015) without causing the sentence to be incomprehensible. The task of dropped pronoun recovery (DPR) aims to locate the position of the dropped pronoun and identify its type. Conversational discourse parsing (CDP) is another important task that aims to analyze the discourse relations among utterances Corresponding author 张 老师 在家 吗 ？ reply Introduction ∗ A1: reply in a conversation, and plays a vital role in understanding multi-turn conversations. Existing work regards DPR and CDP as two independent tasks and tackles them separately. As an early attempt of DPR, Yang et al. (2015) employ a Maximum E"
2021.acl-long.138,D17-1135,0,0.0226859,"employed a linear-chain CRF to jointly predict the position and type of the dropped pronouns in a single utterance using hand-crafted features. Due to the powerful semantic modeling capability of deep learning, Zhang et al. (2019); Yang et al. (2019) introduced neural network methods to recover the dropped pronoun by modeling its semantics from the context. All these methods represent the utterances without considering the relationship between utterances, which is important to identify the referents. Zero pronoun resolution is also a closely related line of research to DPR (Chen and Ng, 2016; Yin et al., 2017, 2018). The main difference between DPR and zero pronoun resolution task is that DPR considers both anaphoric and non-anaphoric pronouns, and doesn’t attempt to resolve it to a referent. 1759 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016). A deep sequential model (Shi and Huang, 2019) was further presented to predict the discourse dependencies utilizing both local information of two utterances"
2021.acl-long.138,P18-1053,0,0.0439635,"Missing"
2021.emnlp-demo.19,W13-2322,0,0.0387792,"el, and in the third sentence, the edge label :NEG indicates that Edmund Pope (who corresponds to the person node) as a conceiver/source has a full negative epistemic stance (Boye, 2012) towards the do event. Uniform Meaning Representation is a graph-based cross-linguistically applicable semantic representation that was recently developed with the goal of supporting interpretable natural language applications that require deep semantic analysis of texts (Van Gysel et al., 2021). UMR has two components: a sentence-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example"
2021.emnlp-demo.19,N13-3004,0,0.0251768,"ure. At the sentence level, UMR-Writer makes clear distinctions between the annotation of lexicalized and abstract concepts, named entity types, attributes, and relations. At the document level, UMR-Writer has separate functionalities for annotating temporal and modal dependencies as well as coreference. UMR-Writer allows the user to easily switch between the sentence-level and documentlevel views with a simple click. Fundamentally, UMR is a representation based on relations between concepts, and there are a number of tools that support annotation for relations. Some examples include Anafora (Chen and Styler, 2013), MAE (Stubbs, 2011; Rim, 2016), WebAnno (Eckart de Castilho et al., 2016), and BRAT (Stenetorp et al., 2012). Anafora is a web-based tool that supports the annotation of relations between text 3.1 Importing Source text into UMR-Writer spans. MAE is a standalone tool that offers flexible for Annotation and versatile schema support for complex relation Annotators can upload their source data in the form sets. WebAnno supports semantic role labelling or event annotations, and it enables the annota- of single files for annotation from the upload page. UMR-Writer can parse and render plain text fo"
2021.emnlp-demo.19,W16-4011,0,0.0390984,"Missing"
2021.emnlp-demo.19,2020.acl-demos.35,0,0.0609907,"Missing"
2021.emnlp-demo.19,H89-1022,0,0.757965,"Missing"
2021.emnlp-demo.19,J05-1004,0,0.758657,"such a tool and how they are addressed. 1 1.1 Introduction UMR Overview semantic concepts (including word senses, entity types etc.) and edges represent relations (participant roles and general semantic relations). The solid lines represent sentence-level relations while the dashed lines represent semantic relations that go beyond sentence boundaries. The direction of the arrows is always from parent to child, at both the sentence- and document level. For instance, at the sentence level, taste-01 is an eventive concept labeled with the first sense of the lemma “taste” as defined in PropBank (Palmer et al., 2005), and a person concept with the name “Edmund Pope” is its ARG0. The concept taste-01 also has an aspect attribute with the value State. The pronoun “he” in the third sentence is decomposed into a person concept with a person attribute 3rd and number attribute Singular, indicating third person singular. At the document level, the person concept mapped from the pronoun “he” refers to the same entity as the person concept in the first sentence, as indicated by the dashed line connecting these nodes. The event taste-01 in the first sentence occurs before document creation time (DCT), as indicated"
2021.emnlp-demo.19,E12-2021,0,0.0671642,"Missing"
2021.emnlp-demo.19,W11-0416,0,0.0419944,"MR-Writer makes clear distinctions between the annotation of lexicalized and abstract concepts, named entity types, attributes, and relations. At the document level, UMR-Writer has separate functionalities for annotating temporal and modal dependencies as well as coreference. UMR-Writer allows the user to easily switch between the sentence-level and documentlevel views with a simple click. Fundamentally, UMR is a representation based on relations between concepts, and there are a number of tools that support annotation for relations. Some examples include Anafora (Chen and Styler, 2013), MAE (Stubbs, 2011; Rim, 2016), WebAnno (Eckart de Castilho et al., 2016), and BRAT (Stenetorp et al., 2012). Anafora is a web-based tool that supports the annotation of relations between text 3.1 Importing Source text into UMR-Writer spans. MAE is a standalone tool that offers flexible for Annotation and versatile schema support for complex relation Annotators can upload their source data in the form sets. WebAnno supports semantic role labelling or event annotations, and it enables the annota- of single files for annotation from the upload page. UMR-Writer can parse and render plain text format tion of semant"
2021.emnlp-demo.19,W19-3321,1,0.900068,"Missing"
2021.emnlp-demo.19,2020.emnlp-main.432,1,0.749736,"-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example in Figwell as coreference. UMR abstracts away from ure 1, UMR is a fairly complex representation that syntactic representations and preserves semantic has many dimensions, and we need to address a relations within and across sentences. Building number of challenges in order to develop a tool a corpus of UMRs could potentially be very use- that makes UMR annotation practical. First of all, ful to NLP practitioners in multiple fields, such as the UMR annotation scheme involves both closed information extracti"
2021.emnlp-demo.19,D18-1371,1,0.82262,"o components: a sentence-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example in Figwell as coreference. UMR abstracts away from ure 1, UMR is a fairly complex representation that syntactic representations and preserves semantic has many dimensions, and we need to address a relations within and across sentences. Building number of challenges in order to develop a tool a corpus of UMRs could potentially be very use- that makes UMR annotation practical. First of all, ful to NLP practitioners in multiple fields, such as the UMR annotation scheme involves both close"
2021.emnlp-demo.19,L18-1490,1,0.778314,"o components: a sentence-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example in Figwell as coreference. UMR abstracts away from ure 1, UMR is a fairly complex representation that syntactic representations and preserves semantic has many dimensions, and we need to address a relations within and across sentences. Building number of challenges in order to develop a tool a corpus of UMRs could potentially be very use- that makes UMR annotation practical. First of all, ful to NLP practitioners in multiple fields, such as the UMR annotation scheme involves both close"
2021.emnlp-demo.8,Q14-1022,0,0.0306677,"to see the timeline for geolocations such as each U.S. state individually, instead of the aggregate for the entire U.S.. Figure 6 is a screenshot showing the 10 timelines for Lockdown for the top-10 most frequently mentioned U.S. states. The screenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 17"
2021.emnlp-demo.8,P19-3006,1,0.898393,"Missing"
2021.emnlp-demo.8,P15-1017,0,0.0183681,"analysis for a pair of events: when the user clicks on the edge {Lockdown, Causes, EconomicCrisis}, the UI shows a strong correlation between the two upward curves. For another edge “Lockdown mitigates COVID19”, the UI shows a negative correlation near the end: as Lockdown rises, COVID-19 slightly falls towards the end. Figure 6: Event popularity timeseries in 01-05/2020 for Lockdown, for top-10 frequently mentioned US states. 9 Related Work Extracting events. Event extraction has been studied using feature-based approaches (Huang and Riloff, 2012; Ji and Grishman, 2008), or neural networks (Chen et al., 2015; Nguyen et al., 2016a; Wadden et al., 2019; Liu et al., 2020). GDELT (Leetaru and Schrodt, 2013) creates an event database for the conflict and mediation domain. It has very few event types related to COVID-19. To adapt event extraction to new domains, Chen et al. (2019) developed a user-in-the-loop rapid event customization system. Nguyen et al. (2016b) proposed a neural model for event type extension given seed examples. Peng et al. (2016) developed a minimally supervised approach using triggers gathered from ACE annotation guideline. Use case 3: analyses targeted at geolocations. The event"
2021.emnlp-demo.8,C16-1007,0,0.0206941,"ifornia and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Ohio. Association for Computational Linguistics. Elizabeth Boschee, Marjorie Freedman, Saurabh Khanwalkar, Anoop Kumar, Amit Srivastava, and Ralph Weischedel. 2014. Researching persons & organizations: Awake: From text to an entity-centric knowledge base. In 2014 IEEE"
2021.emnlp-demo.8,K19-1062,0,0.0117289,"ually, instead of the aggregate for the entire U.S.. Figure 6 is a screenshot showing the 10 timelines for Lockdown for the top-10 most frequently mentioned U.S. states. The screenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Ohio. Association for Computational Linguistics"
2021.emnlp-demo.8,2021.emnlp-main.440,1,0.655832,"Missing"
2021.emnlp-demo.8,D19-1296,0,0.014458,"reenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Ohio. Association for Computational Linguistics. Elizabeth Boschee, Marjorie Freedman, Saurabh Khanwalkar, Anoop Kumar, Amit Srivastava, and Ralph Weischedel. 2014. Researching persons & organizations: Awake: From text to a"
2021.emnlp-demo.8,N16-1034,0,0.0168391,"r of events: when the user clicks on the edge {Lockdown, Causes, EconomicCrisis}, the UI shows a strong correlation between the two upward curves. For another edge “Lockdown mitigates COVID19”, the UI shows a negative correlation near the end: as Lockdown rises, COVID-19 slightly falls towards the end. Figure 6: Event popularity timeseries in 01-05/2020 for Lockdown, for top-10 frequently mentioned US states. 9 Related Work Extracting events. Event extraction has been studied using feature-based approaches (Huang and Riloff, 2012; Ji and Grishman, 2008), or neural networks (Chen et al., 2015; Nguyen et al., 2016a; Wadden et al., 2019; Liu et al., 2020). GDELT (Leetaru and Schrodt, 2013) creates an event database for the conflict and mediation domain. It has very few event types related to COVID-19. To adapt event extraction to new domains, Chen et al. (2019) developed a user-in-the-loop rapid event customization system. Nguyen et al. (2016b) proposed a neural model for event type extension given seed examples. Peng et al. (2016) developed a minimally supervised approach using triggers gathered from ACE annotation guideline. Use case 3: analyses targeted at geolocations. The event timeline visualizati"
2021.emnlp-demo.8,W16-1618,0,0.0121886,"r of events: when the user clicks on the edge {Lockdown, Causes, EconomicCrisis}, the UI shows a strong correlation between the two upward curves. For another edge “Lockdown mitigates COVID19”, the UI shows a negative correlation near the end: as Lockdown rises, COVID-19 slightly falls towards the end. Figure 6: Event popularity timeseries in 01-05/2020 for Lockdown, for top-10 frequently mentioned US states. 9 Related Work Extracting events. Event extraction has been studied using feature-based approaches (Huang and Riloff, 2012; Ji and Grishman, 2008), or neural networks (Chen et al., 2015; Nguyen et al., 2016a; Wadden et al., 2019; Liu et al., 2020). GDELT (Leetaru and Schrodt, 2013) creates an event database for the conflict and mediation domain. It has very few event types related to COVID-19. To adapt event extraction to new domains, Chen et al. (2019) developed a user-in-the-loop rapid event customization system. Nguyen et al. (2016b) proposed a neural model for event type extension given seed examples. Peng et al. (2016) developed a minimally supervised approach using triggers gathered from ACE annotation guideline. Use case 3: analyses targeted at geolocations. The event timeline visualizati"
2021.emnlp-demo.8,P08-1030,0,0.0797777,"lso click on a edge to perform a correlation analysis for a pair of events: when the user clicks on the edge {Lockdown, Causes, EconomicCrisis}, the UI shows a strong correlation between the two upward curves. For another edge “Lockdown mitigates COVID19”, the UI shows a negative correlation near the end: as Lockdown rises, COVID-19 slightly falls towards the end. Figure 6: Event popularity timeseries in 01-05/2020 for Lockdown, for top-10 frequently mentioned US states. 9 Related Work Extracting events. Event extraction has been studied using feature-based approaches (Huang and Riloff, 2012; Ji and Grishman, 2008), or neural networks (Chen et al., 2015; Nguyen et al., 2016a; Wadden et al., 2019; Liu et al., 2020). GDELT (Leetaru and Schrodt, 2013) creates an event database for the conflict and mediation domain. It has very few event types related to COVID-19. To adapt event extraction to new domains, Chen et al. (2019) developed a user-in-the-loop rapid event customization system. Nguyen et al. (2016b) proposed a neural model for event type extension given seed examples. Peng et al. (2016) developed a minimally supervised approach using triggers gathered from ACE annotation guideline. Use case 3: analy"
2021.emnlp-demo.8,P18-1212,0,0.0239682,"Missing"
2021.emnlp-demo.8,N18-1077,0,0.0128856,"r geolocations such as each U.S. state individually, instead of the aggregate for the entire U.S.. Figure 6 is a screenshot showing the 10 timelines for Lockdown for the top-10 most frequently mentioned U.S. states. The screenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Oh"
2021.emnlp-demo.8,2020.emnlp-main.128,0,0.0187649,"ge {Lockdown, Causes, EconomicCrisis}, the UI shows a strong correlation between the two upward curves. For another edge “Lockdown mitigates COVID19”, the UI shows a negative correlation near the end: as Lockdown rises, COVID-19 slightly falls towards the end. Figure 6: Event popularity timeseries in 01-05/2020 for Lockdown, for top-10 frequently mentioned US states. 9 Related Work Extracting events. Event extraction has been studied using feature-based approaches (Huang and Riloff, 2012; Ji and Grishman, 2008), or neural networks (Chen et al., 2015; Nguyen et al., 2016a; Wadden et al., 2019; Liu et al., 2020). GDELT (Leetaru and Schrodt, 2013) creates an event database for the conflict and mediation domain. It has very few event types related to COVID-19. To adapt event extraction to new domains, Chen et al. (2019) developed a user-in-the-loop rapid event customization system. Nguyen et al. (2016b) proposed a neural model for event type extension given seed examples. Peng et al. (2016) developed a minimally supervised approach using triggers gathered from ACE annotation guideline. Use case 3: analyses targeted at geolocations. The event timeline visualization also allows the user to see the timeli"
2021.emnlp-demo.8,P18-1049,0,0.014649,"as each U.S. state individually, instead of the aggregate for the entire U.S.. Figure 6 is a screenshot showing the 10 timelines for Lockdown for the top-10 most frequently mentioned U.S. states. The screenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Ohio. Association for Computa"
2021.emnlp-demo.8,W16-5706,0,0.062499,"Missing"
2021.emnlp-demo.8,D19-1121,1,0.852123,"ion prediction. The model is trained with a blended dataset consisting of the Entities, Events, Simple and Complex Cause Assertion Annotation datasets 6 released by LDC 7 , and 1.5K temporal relation instances generated by applying the LearnIt temporal relation extraction patterns to 10,000 sampled Gigaword (Parker et al., 2011) articles. and correlation analysis (e.g., will a stricter lockdown improve or deteriorate the economy?). In order to support these analyses, we produce a timeseries of a popularity score for each event type over time (a.k.a., event timeline). Extending our prior work (Min and Zhao, 2019), we define the popularity score for event type e at time t as: P opularity(e)t = 7 Constructing a TCAG t0 ∈[t− T2 ,t+ T2 ] Ne,t0 cMt0 System Demonstration Datasets. We run Excavator on the following two corpora to produce a TCAG for COVID-19: the first corpus is 1.2 million articles 9 from the Aylien Coronavirus News Dataset 10 , which contains 1.6 million COVID-related articles published between November 2019 and July 2020 that are from ∼440 news sources. We only kept the articles that are published between January and May 2020, since the corpus contains fewer articles in other months. The s"
2021.emnlp-demo.8,D16-1038,0,0.022051,"Missing"
2021.emnlp-demo.8,2020.louhi-1.12,0,0.0119965,"ire U.S.. Figure 6 is a screenshot showing the 10 timelines for Lockdown for the top-10 most frequently mentioned U.S. states. The screenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Ohio. Association for Computational Linguistics. Elizabeth Boschee, Marjorie Freedman, Saurabh Khanwal"
2021.emnlp-demo.8,2020.acl-demos.14,0,0.0219152,"Missing"
2021.emnlp-demo.8,W13-4004,0,0.0229261,"-10 most frequently mentioned U.S. states. The screenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Ohio. Association for Computational Linguistics. Elizabeth Boschee, Marjorie Freedman, Saurabh Khanwalkar, Anoop Kumar, Amit Srivastava, and Ralph Weischedel. 2014. Researching p"
2021.emnlp-demo.8,2021.acl-long.122,1,0.803575,"Missing"
2021.emnlp-demo.8,W18-1506,0,0.0240098,"ntioned U.S. states. The screenshot shows that the curves for California and New York go much higher than other states. This roughly matches the stricter lockdown policies implemented in the two states during this time period, compared with other states. Such targeted analysis Extracting causal and temporal relations. There are a lot of work in temporal (D’Souza 68 and Ng, 2013; Chambers et al., 2014; Ning et al., 2018b; Meng and Rumshisky, 2018; Han et al., 2019; Vashishtha et al., 2020; Wright-Bettner et al., 2020) and causal (Bethard and Martin, 2008; Do et al., 2011; Riaz and Girju, 2013; Roemmele and Gordon, 2018; Hashimoto, 2019) relation extraction. Mirza and Tonelli (2016) and Ning et al. (2018a) extract both in a single framework. pages 2895–2905, Florence, Italy. Association for Computational Linguistics. Steven Bethard and James H. Martin. 2008. Learning semantic links from a corpus of parallel temporal and causal relations. In Proceedings of ACL-08: HLT, Short Papers, pages 177–180, Columbus, Ohio. Association for Computational Linguistics. Elizabeth Boschee, Marjorie Freedman, Saurabh Khanwalkar, Anoop Kumar, Amit Srivastava, and Ralph Weischedel. 2014. Researching persons & organizations: Awa"
2021.emnlp-demo.8,N19-4008,0,0.0552685,"Missing"
2021.emnlp-demo.8,2020.findings-emnlp.363,0,0.0574091,"Missing"
2021.emnlp-demo.8,D19-1585,0,0.0272323,"user clicks on the edge {Lockdown, Causes, EconomicCrisis}, the UI shows a strong correlation between the two upward curves. For another edge “Lockdown mitigates COVID19”, the UI shows a negative correlation near the end: as Lockdown rises, COVID-19 slightly falls towards the end. Figure 6: Event popularity timeseries in 01-05/2020 for Lockdown, for top-10 frequently mentioned US states. 9 Related Work Extracting events. Event extraction has been studied using feature-based approaches (Huang and Riloff, 2012; Ji and Grishman, 2008), or neural networks (Chen et al., 2015; Nguyen et al., 2016a; Wadden et al., 2019; Liu et al., 2020). GDELT (Leetaru and Schrodt, 2013) creates an event database for the conflict and mediation domain. It has very few event types related to COVID-19. To adapt event extraction to new domains, Chen et al. (2019) developed a user-in-the-loop rapid event customization system. Nguyen et al. (2016b) proposed a neural model for event type extension given seed examples. Peng et al. (2016) developed a minimally supervised approach using triggers gathered from ACE annotation guideline. Use case 3: analyses targeted at geolocations. The event timeline visualization also allows the use"
2021.emnlp-demo.8,2020.nlpcovid19-acl.1,0,0.0283038,"time t as: P opularity(e)t = 7 Constructing a TCAG t0 ∈[t− T2 ,t+ T2 ] Ne,t0 cMt0 System Demonstration Datasets. We run Excavator on the following two corpora to produce a TCAG for COVID-19: the first corpus is 1.2 million articles 9 from the Aylien Coronavirus News Dataset 10 , which contains 1.6 million COVID-related articles published between November 2019 and July 2020 that are from ∼440 news sources. We only kept the articles that are published between January and May 2020, since the corpus contains fewer articles in other months. The second corpus is the COVID-19 Open Research Dataset (Wang et al., 2020). It contains coronavirus-related research from PubMed’s PMC corpus, a corpus maintained by the WHO, and bioRxiv and medRxiv pre-prints. As of 11/08/2020, it contains over 300,000 scholarly articles. We combine these two corpora because news and research articles are complementary: news are rich in real-world events and are up to date, while analytical articles contain more causal relationships. Therefore, combining them is likely to lead to a more comprehensive analysis and new insights. We aggregate all extracted events and causal and temporal relations across the corpus to construct a TCAG."
C02-1145,P00-1058,0,0.0194968,"Missing"
C02-1145,W00-1207,0,0.0132515,"Missing"
C02-1145,xia-etal-2000-developing,1,0.653451,"is the corpus applicable, and finally (iv) what future work we anticipate. Introduction The Penn Chinese Treebank (CTB) is an ongoing project, with its objective being to create a segmented Chinese corpus annotated with POS tags and syntactic brackets. The first installment of the project (CTB-I) consists of Xinhua newswire between the years 1994 and 1998, totaling 100,000 words, fully segmented, POS-tagged and syntactically bracketed and it has been released to the public via the Penn Linguistic Data Consortium (LDC). The preliminary results of this phase of the project have been reported in Xia et al (2000). Currently the second installment of the project, the 400,000-word CTB-II is being developed and is expected to be completed early in the year 2003. CTB-II will follow the standards set up in the segmentation (Xia 2000b), POS tagging (Xia 2000a) and bracketing guidelines (Xue and Xia 2000) and it will use articles from Peoples&apos; Daily, Hong Kong newswire and material translated into Chinese from other languages in addition to the Xinhua newswire used in CTB-I in an effort to diversify the sources. The availability of CTB-I changed our approach to CTB-II considerably. Due to the existence of CT"
C02-1145,W96-0213,0,\N,Missing
C02-1145,H01-1026,1,\N,Missing
C10-2156,W01-1313,0,0.0706534,"argets, and the selection of annotation targets should be subject to syntactic, semantic and discourse constraints. 1 Introduction Event-based temporal inference is a fundamental natural language technology that attempts to determine the temporal location of an event as well as the temporal ordering between events. It supports a wide range of natural language applications such as Information Extraction, Question Answering and Text Summarization. For some genres of text (such as news), a temporal ordering of events can be the most informative summarization of a document (Mani and Wilson, 2000; Filatova and Hovy, 2001). Temporal inference is especially important for multi-document summarization where events extracted from multiple documents need to be put in a chronological order (Lin and Hovy, 2001; Barzilay et al., 2002) to make logical sense. Event-based temporal inference is also necessary for Question Answering (Harabagiu and Bejan, 2005; Harabagiu and Bejan, 2006). For example, to answer “When Being able to infer the temporal location of an event in Chinese text has many additional applications. Besides Information Extraction, Question Answering and Text Summarization, knowing the temporal location of"
C10-2156,harabagiu-bejan-2006-answer,0,0.189965,"guage applications such as Information Extraction, Question Answering and Text Summarization. For some genres of text (such as news), a temporal ordering of events can be the most informative summarization of a document (Mani and Wilson, 2000; Filatova and Hovy, 2001). Temporal inference is especially important for multi-document summarization where events extracted from multiple documents need to be put in a chronological order (Lin and Hovy, 2001; Barzilay et al., 2002) to make logical sense. Event-based temporal inference is also necessary for Question Answering (Harabagiu and Bejan, 2005; Harabagiu and Bejan, 2006). For example, to answer “When Being able to infer the temporal location of an event in Chinese text has many additional applications. Besides Information Extraction, Question Answering and Text Summarization, knowing the temporal location of an event is also highly valuable to Machine Translation. To translate a language like Chinese into a language like English in which tense is grammatically marked with inflectional morphemes, an MT system will have to infer the necessary temporal information to determine the correct tense for verbs. Statistical MT systems, the currently dominant research p"
C10-2156,P00-1010,0,0.0569428,"select the annotation targets, and the selection of annotation targets should be subject to syntactic, semantic and discourse constraints. 1 Introduction Event-based temporal inference is a fundamental natural language technology that attempts to determine the temporal location of an event as well as the temporal ordering between events. It supports a wide range of natural language applications such as Information Extraction, Question Answering and Text Summarization. For some genres of text (such as news), a temporal ordering of events can be the most informative summarization of a document (Mani and Wilson, 2000; Filatova and Hovy, 2001). Temporal inference is especially important for multi-document summarization where events extracted from multiple documents need to be put in a chronological order (Lin and Hovy, 2001; Barzilay et al., 2002) to make logical sense. Event-based temporal inference is also necessary for Question Answering (Harabagiu and Bejan, 2005; Harabagiu and Bejan, 2006). For example, to answer “When Being able to infer the temporal location of an event in Chinese text has many additional applications. Besides Information Extraction, Question Answering and Text Summarization, knowin"
C10-2156,miltsakaki-etal-2004-penn,0,0.0921781,"Missing"
C10-2156,J05-1004,0,0.0152327,"(4) 广东 [e1 举行] [e2 研讨会] [e3 Guangdong hold symposium 介绍] [e4 税改] 及 加工 introduce tax reform and processing 贸易 台帐 制度 trade accounting regulation “Guangdong held a symposium introducing the tax reform and the accounting regulations on processing trade.” An alternative to using the notion of predicateargument structure in determining the subordinating/subordinated events is to resort to syntactic relations such as the verb and its object. The net result would be the same for Example (4). However, the same argument that motivates the annotation of the predicate-argument structures in the Propbank (Palmer et al., 2005) and the Chinese Propbank (Xue and Palmer, 2009) also applies to temporal annotation. That is, the predicate-argument structure and temporal relations tend to hold constant in spite of the syntactic alternations and variations. For example, the temporal relation between the noun 研讨会 (“symposium”) event and the verb 举行 (“hold”) event remains the same in (5) in spite of the change in the syntactic relation between them. If only event pairs in a verb-object relation are annotated, the temporal relation between e2 and e1 in (5) would be lost. (5) [e2 研讨会] 在 广东 [e1 举行] symposium PREP Guangdong hold"
C10-2158,H91-1060,0,0.0705439,"enn Treebank was first constructed, it is only recently that they are starting to receive the attention they deserve. Works on automatic detection of empty categories started to emerge (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006) after substantial progress has been made in statistical syntactic parsing. This progress has been achieved after over a decade of intensive research on syntactic parsing that has essentially left the empty categories behind (Collins, 1999; Charniak, 2000). Empty categories were and still are routinely pruned out in parser evaluations (Black et al., 1991). They have been excluded from the parser development and evaluation cycle not so much because their importance was not understood, but because researchers haven’t figured out 1382 Coling 2010: Poster Volume, pages 1382–1390, Beijing, August 2010 IP NP-PN-TPC-1 NP-SBJ VP NR PN VC 宁波 ningbo Ningbo 我 wo I 是 shi be VP VP QP-ADV OD CLP VV NP-OBJ 第三 disan third M 来 lai come -NONE次 ci “Ningbo, this is the third time I came here.” *T*-1 Figure 1: A CTB tree with empty categories a way to incorporate the empty category detection in the parsing process. In fact, the detection of empty categories relies"
C10-2158,P04-1041,0,0.0190718,"Missing"
C10-2158,P04-1082,0,0.704323,"on site of an dislocated phrase, thus effectively reconstructing the canonical structure of a sentence, allowing easy extraction of its predicate-argument structure. For example, in Figure 1, the empty category *T*1 is coindexed with the dislocated topic NP 宁 Although empty categories have been an integral part of the syntactic representation of a sentence ever since the Penn Treebank was first constructed, it is only recently that they are starting to receive the attention they deserve. Works on automatic detection of empty categories started to emerge (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006) after substantial progress has been made in statistical syntactic parsing. This progress has been achieved after over a decade of intensive research on syntactic parsing that has essentially left the empty categories behind (Collins, 1999; Charniak, 2000). Empty categories were and still are routinely pruned out in parser evaluations (Black et al., 1991). They have been excluded from the parser development and evaluation cycle not so much because their importance was not understood, but because researchers haven’t figured out 1382 Coling 2010: Poster Volume, pages 1382–"
C10-2158,A00-2018,0,0.412615,"egories have been an integral part of the syntactic representation of a sentence ever since the Penn Treebank was first constructed, it is only recently that they are starting to receive the attention they deserve. Works on automatic detection of empty categories started to emerge (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006) after substantial progress has been made in statistical syntactic parsing. This progress has been achieved after over a decade of intensive research on syntactic parsing that has essentially left the empty categories behind (Collins, 1999; Charniak, 2000). Empty categories were and still are routinely pruned out in parser evaluations (Black et al., 1991). They have been excluded from the parser development and evaluation cycle not so much because their importance was not understood, but because researchers haven’t figured out 1382 Coling 2010: Poster Volume, pages 1382–1390, Beijing, August 2010 IP NP-PN-TPC-1 NP-SBJ VP NR PN VC 宁波 ningbo Ningbo 我 wo I 是 shi be VP VP QP-ADV OD CLP VV NP-OBJ 第三 disan third M 来 lai come -NONE次 ci “Ningbo, this is the third time I came here.” *T*-1 Figure 1: A CTB tree with empty categories a way to incorporate t"
C10-2158,P03-1055,0,0.700326,"is to mark the extraction site of an dislocated phrase, thus effectively reconstructing the canonical structure of a sentence, allowing easy extraction of its predicate-argument structure. For example, in Figure 1, the empty category *T*1 is coindexed with the dislocated topic NP 宁 Although empty categories have been an integral part of the syntactic representation of a sentence ever since the Penn Treebank was first constructed, it is only recently that they are starting to receive the attention they deserve. Works on automatic detection of empty categories started to emerge (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006) after substantial progress has been made in statistical syntactic parsing. This progress has been achieved after over a decade of intensive research on syntactic parsing that has essentially left the empty categories behind (Collins, 1999; Charniak, 2000). Empty categories were and still are routinely pruned out in parser evaluations (Black et al., 1991). They have been excluded from the parser development and evaluation cycle not so much because their importance was not understood, but because researchers haven’t figured out 1382 Coling 2010: Poster Vol"
C10-2158,P00-1022,0,0.0180122,"Missing"
C10-2158,P02-1018,0,0.927644,"mpty categories is to mark the extraction site of an dislocated phrase, thus effectively reconstructing the canonical structure of a sentence, allowing easy extraction of its predicate-argument structure. For example, in Figure 1, the empty category *T*1 is coindexed with the dislocated topic NP 宁 Although empty categories have been an integral part of the syntactic representation of a sentence ever since the Penn Treebank was first constructed, it is only recently that they are starting to receive the attention they deserve. Works on automatic detection of empty categories started to emerge (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006) after substantial progress has been made in statistical syntactic parsing. This progress has been achieved after over a decade of intensive research on syntactic parsing that has essentially left the empty categories behind (Collins, 1999; Charniak, 2000). Empty categories were and still are routinely pruned out in parser evaluations (Black et al., 1991). They have been excluded from the parser development and evaluation cycle not so much because their importance was not understood, but because researchers haven’t figured out 1382"
C10-2158,J93-2004,0,0.0349664,"is important to many natural language applications. When translated into another language, for example, these dropped pronouns may have to be made explicit and replaced with overt pronouns or noun phrases if the target language does not allow dropped pronouns. 1 Introduction The use of empty categories to represent the syntactic structure of a sentence is the hallmark of the generative linguistics and they represent an important source of information in treebanks annotated in this linguistic tradition. The use of empty categories in the annotation of treebanks started with the Penn Treebank (Marcus et al., 1993), and this practice is continued in the Chinese Treebank (CTB) (Xue et al., 2005) and the Arabic Treebank, the Penn series of treebanks. Empty categories come in a few different varieties, serving different purposes. One use of empty categories is to mark the extraction site of an dislocated phrase, thus effectively reconstructing the canonical structure of a sentence, allowing easy extraction of its predicate-argument structure. For example, in Figure 1, the empty category *T*1 is coindexed with the dislocated topic NP 宁 Although empty categories have been an integral part of the syntactic re"
C10-2158,C02-1078,0,0.0144102,"y occupy the first few rows of a table that sorted based on feature performance. 5 Related work The problem of empty category detection has been studied both in the context of reference resolution and syntactic parsing. In the reference resolution literature, empty category detection manifests itself in the form of zero anaphora (or zero pronoun) detection and resolution. Zero anaphora resolution has been studied as a computational problem for many different languages. For example, (Ferr´andez and Peral, 2000) describes an algorithm for detecting and resolving zero pronouns in Spanish texts. (Seki et al., 2002) and (Lida et al., 2007) reported work on zero pronoun detection and resolution in Japanese. Zero anaphora detection and resolution for Chinese has been studied as well. Converse (2006) studied Chinese pronominal anaphora resolution, including zero anaphora resolution, although there is no attempt to automatically detect the zero anaphors in text. Her work only deals with anaphora resolution, assuming the zero anaphors have already been detected. Chinese zero anaphora identification and resolution have been studied in a machine learning frameworking in (Zhao and Ng, 2007) and (Peng and Araki,"
C10-2158,D07-1057,0,0.127183,"ns in Spanish texts. (Seki et al., 2002) and (Lida et al., 2007) reported work on zero pronoun detection and resolution in Japanese. Zero anaphora detection and resolution for Chinese has been studied as well. Converse (2006) studied Chinese pronominal anaphora resolution, including zero anaphora resolution, although there is no attempt to automatically detect the zero anaphors in text. Her work only deals with anaphora resolution, assuming the zero anaphors have already been detected. Chinese zero anaphora identification and resolution have been studied in a machine learning frameworking in (Zhao and Ng, 2007) and (Peng and Araki, 2007). The present work studies empty category recovery as part of the effort to fully parse natural language text and as such our work is not limited to just recovering zero anaphors. We are also interested in other types of empty categories such as traces. Our work is thus more closely related to the work of (Johnson, 2002), (Dienes and Dubey, 2003), (Campbell, 2004) and (Gabbard et 1388 al., 2006). Johnson (2002) describes a pattern-matching algorithm for recovering empty nodes from phrase structure trees. The idea was to extract minimal connected tree fragments that c"
C10-2158,N06-1024,0,0.447225,"slocated phrase, thus effectively reconstructing the canonical structure of a sentence, allowing easy extraction of its predicate-argument structure. For example, in Figure 1, the empty category *T*1 is coindexed with the dislocated topic NP 宁 Although empty categories have been an integral part of the syntactic representation of a sentence ever since the Penn Treebank was first constructed, it is only recently that they are starting to receive the attention they deserve. Works on automatic detection of empty categories started to emerge (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006) after substantial progress has been made in statistical syntactic parsing. This progress has been achieved after over a decade of intensive research on syntactic parsing that has essentially left the empty categories behind (Collins, 1999; Charniak, 2000). Empty categories were and still are routinely pruned out in parser evaluations (Black et al., 1991). They have been excluded from the parser development and evaluation cycle not so much because their importance was not understood, but because researchers haven’t figured out 1382 Coling 2010: Poster Volume, pages 1382–1390, Beijing, August 2"
C10-2158,D07-1027,0,0.198568,"Missing"
C10-2158,J03-4003,0,\N,Missing
C14-1143,W07-2441,0,0.0323312,"en word alignments and syntactic structures will arise in many places. In this section, we illustrate the issues through languageparticular function words, where the problems are most frequently found. Due to language-particular idiosyncrasy and lack of lexical content, these function words usually do not have a translation counterpart, which presents a great challenge to alignment annotation. There are two logical possibilities of dealing with these words, both of which are represented in existing annotation practice. The ﬁrst is to leave them unaligned or link them to a ﬁctitious NULL word (Ahrenberg, 2007; Brown et al., 1990), and the second, which also seems to be the more common pratice, is to attach these function words to a word that has a translation counterpart, and then align the function word and its host with the counterpart of the host (Melamed, 1998; Li et al., 2009). For ease of discussion, below we will refer to the latter pratice as the &quot;glue-to-a-host&quot; strategy (GTAHS). Both approaches are less than desirable: the former leaves the function words unaccounted for, and the latter leads to incompatibility issues we discuss in detail below. First note that, by attaching language-par"
C14-1143,J90-2002,0,0.786724,"s and syntactic structures will arise in many places. In this section, we illustrate the issues through languageparticular function words, where the problems are most frequently found. Due to language-particular idiosyncrasy and lack of lexical content, these function words usually do not have a translation counterpart, which presents a great challenge to alignment annotation. There are two logical possibilities of dealing with these words, both of which are represented in existing annotation practice. The ﬁrst is to leave them unaligned or link them to a ﬁctitious NULL word (Ahrenberg, 2007; Brown et al., 1990), and the second, which also seems to be the more common pratice, is to attach these function words to a word that has a translation counterpart, and then align the function word and its host with the counterpart of the host (Melamed, 1998; Li et al., 2009). For ease of discussion, below we will refer to the latter pratice as the &quot;glue-to-a-host&quot; strategy (GTAHS). Both approaches are less than desirable: the former leaves the function words unaccounted for, and the latter leads to incompatibility issues we discuss in detail below. First note that, by attaching language-particular function word"
C14-1143,P10-1146,0,0.0742391,"and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been independently conceived based on monolingual corpora (Chiang, 2010). In this paper, we re"
C14-1143,P07-1003,0,0.129505,"tation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been independently conceived based on monolingual corpora (Chiang, 2010). In this paper, we report an effort in building a Hierarchically Aligned Chinese-English Parallel Treebank (HACEPT) where we manually do wordlevel and phrase-level alignments simultaneously on pa"
C14-1143,W14-4904,1,0.679452,"ese. There are both reordering (difference in the relative order of powerhouse and economy) and unaligned function words (Chinese 的 and English of ) in the phrase alignment in Figure 2c. Figure 2d provides an example where the aligned phrases have structural divergence caused by crosslinguistic differences between Chinese and English, which we will discuss in some detail in Section 4. 4 A common incompatibility issue between parse trees During the annotation process, we encountered some incompatibility issues between parse trees. For a comprehensive and detailed discussion of the issues, see (Deng and Xue, 2014). Here we report the most common issue, which is caused by differences between treebank annotation guidelines. As already mentioned, the English parse trees we use are annotated based on the original PTB annotation stylebook (Bies et al., 1995) as well as its extensions (Warner et al., 2004), while the Chinese parse trees are annotated based on the CTB annotation guidelines (Xue and Xia, 2000) and its extensions (Zhang and Xue, 2012). Since PTB and CTB are independently annotated, there are some differences in how certain structures are annotated. The main issue is that certain structures are"
C14-1143,J94-4004,0,0.594184,"per, we describe how we construct the HACEPT and discuss issues arising in the construction process. In Section 2, we discuss the problems of word alignment done without considering its interaction with syntactic structures. In Section 3, we describe our annotation procedure where we perform word-level and phrase-level alignments simultaneously in a coordinated manner, and show how our approach is free of the problems discussed in Section 2. In Section 4, we report a common incompatibility issue between parse trees and propose a solution. We also compare the issue with translation divergence (Dorr, 1994) and show that they are different in nature and occurrence frequency. In Section 5, we present the results of two experiments we have done on our annotation to show the intuitiveness of our approach and the linguistic and engineering potentials of our corpus. We then describe related work in Section 6 and conclude our paper in Section 7. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1511 Proceedings of COLING 2014, the 25th Intern"
C14-1143,P03-2041,0,0.172506,"ds) unaligned on the word level, and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been independently conceived based on monolingual corpora (Ch"
C14-1143,W08-0306,0,0.0212815,"with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been independently conceived based on monolingual corpora (Chiang, 2010). In this paper, we report an effort in building a Hierarchically Aligned Chinese-English Parallel Treebank (HACEPT) where we manually do wordlevel and phrase-level alignments simultaneously on parallel phrase-based p"
C14-1143,N04-1035,0,0.158823,"ain innovation of our approach is that we leave words without a translation counterpart (which are mostly language-particular function words) unaligned on the word level, and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structu"
C14-1143,P06-1121,0,0.0926078,"approach is that we leave words without a translation counterpart (which are mostly language-particular function words) unaligned on the word level, and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from di"
C14-1143,2006.amta-papers.8,0,0.107607,"leave words without a translation counterpart (which are mostly language-particular function words) unaligned on the word level, and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic ann"
C14-1143,li-etal-2012-parallel,1,0.841252,"nning of the new year <&gt; 新 年 伊始 d. to quickly and efﬁciently solve the problem <&gt; 迅速 有效地 解决 问题 Given the incompatibilities between existing word alignments and syntactic structures, in the next section we describe an approach where we perform word-level and phrase-level alignments simultaneously on parallel phrase-based parse trees, attempting to construct a hierarchically aligned corpus where word alignments are harmonized with syntactic structures. 3 Annotation speciﬁcation and procedure The data we annotate is the Chinese-English portion of the Parallel Aligned Treebank (PAT) described in (Li et al., 2012). Our data consists of two batches, one of which is weblogs and the other of which is postings from online discussion forums. The English sentences in the data set are annotated based on the original Penn TreeBank (PTB) annotation stylebook (Bies et al., 1995) as well as its extensions (Warner et al., 2004), while the Chinese sentences in the data set are annotated based on the Chinese TreeBank (CTB) annotation guidelines (Xue and Xia, 2000) and its extensions (Zhang and Xue, 2012). The PAT only has word alignments, which are done under the GTAHS, and no phrase alignments. The main departure o"
C14-1143,W08-0308,0,0.521193,"e mostly language-particular function words) unaligned on the word level, and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been independently conceived"
C14-1143,P06-1077,0,0.101611,"nterpart (which are mostly language-particular function words) unaligned on the word level, and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been i"
C14-1143,P09-1063,0,0.391461,"on the word level, and locate and align the appropriate phrases which encapsulate them. In doing so, we harmonize word-level and phraselevel alignments. We show that this type of annotation can be performed with high inter-annotator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been independently conceived based on monolingual corpora (Chiang, 2010). In th"
C14-1143,J93-2004,0,0.0511199,"es the preposition to apple and the whole PP is aligned with 苹果. With the GTAHS, the unambiguous Chinese 苹果 ends up being aligned with more than one English string. This kind of spurious ambiguity is very common given the GTAHS. The second issue is that, by attaching function words to a host, the GTAHS effectively creates rudimentary syntactic structures, which are often incompatible with the syntactic structures annotated based on existing treebanking annotation standards. For example, all the aligned multi-word strings underlined in (2) do not correspond to a constituent in a Penn TreeBank (Marcus et al., 1993) or Chinese TreeBank (Xue et al., 2005) parse tree: (2) a. If I were him <&gt; 如果 我 是 他 的话 b. He is visiting Beijing <&gt; 他 正 访问 北京 1512 c. the beginning of the new year <&gt; 新 年 伊始 d. to quickly and efﬁciently solve the problem <&gt; 迅速 有效地 解决 问题 Given the incompatibilities between existing word alignments and syntactic structures, in the next section we describe an approach where we perform word-level and phrase-level alignments simultaneously on parallel phrase-based parse trees, attempting to construct a hierarchically aligned corpus where word alignments are harmonized with syntactic structures. 3"
C14-1143,megyesi-etal-2010-english,0,0.0245672,"for reordering, which is an important issue that MT systems are trying to address. Chunk No. 1 2 3 4 5 micro-average precision 0.91 0.92 0.89 0.88 0.89 0.90 recall 0.86 0.80 0.89 0.88 0.89 0.85 F1-measure 0.89 0.86 0.89 0.88 .086 0.87 Table 1: Statistics of IAA 6 +REO -REO total Annotator 1 2 1 2 1 2 +UFW 6,473 6,670 7,328 7,797 13,801 14,467 -UFW 379 379 6,872 7,334 7,251 7,713 total 6,852 7,049 14,200 15,131 21,052 22,180 Table 2: Statistics of phrase alignment by types Related work Parallel treebanks are not something new. However, most of the existing parallel treebanks (Li et al., 2012; Megyesi et al., 2010) do not have phrase alignments. Some (Sulger et al., 2013; Kapanadze, 2012) do have phrase alignments, but neither discussion about the interaction between word-level and phraselevel alignments nor report of IAA is provided. There have been a few recent attempts at automatically aligning subtrees (comparable to our phrases) in the context of MT research, and the automatic alignments are evaluated against a small manually aligned data set. For example, (Tinsley et al., 2007) evaluated an unsupervised algorithm on 810 parsed English-French pairs annotated with subtree alignment. (Xiao and Zhu, 2"
C14-1143,N10-1014,0,0.0139176,"ator consistency and have both linguistic and engineering potentials. 1 Introduction The value of human annotated syntactic structures for Statistical Machine Translation has been clearly demonstrated in string-to-tree (Galley et al., 2004; Galley et al., 2006; Huang et al., 2006), tree-to-string (Liu et al., 2006; Liu and Gildea, 2008), and tree-to-tree (Eisner, 2003; Liu et al., 2009; Chiang, 2010) models. One recurring issue which hampers the utility of syntactic structures is the incompatibility between word alignments and syntactic structures (Denero and Klein, 2007; Fossum et al., 2008; Pauls et al., 2010). The incompatibility arises because word alignments and syntactic structures are established independently of each other. In the case of tree-to-tree models, there is also the issue of incompatible parallel tree structures resulting from divergent syntactic annotation standards that have been independently conceived based on monolingual corpora (Chiang, 2010). In this paper, we report an effort in building a Hierarchically Aligned Chinese-English Parallel Treebank (HACEPT) where we manually do wordlevel and phrase-level alignments simultaneously on parallel phrase-based parse trees. In this p"
C14-1143,P13-1054,0,0.0206769,"s are trying to address. Chunk No. 1 2 3 4 5 micro-average precision 0.91 0.92 0.89 0.88 0.89 0.90 recall 0.86 0.80 0.89 0.88 0.89 0.85 F1-measure 0.89 0.86 0.89 0.88 .086 0.87 Table 1: Statistics of IAA 6 +REO -REO total Annotator 1 2 1 2 1 2 +UFW 6,473 6,670 7,328 7,797 13,801 14,467 -UFW 379 379 6,872 7,334 7,251 7,713 total 6,852 7,049 14,200 15,131 21,052 22,180 Table 2: Statistics of phrase alignment by types Related work Parallel treebanks are not something new. However, most of the existing parallel treebanks (Li et al., 2012; Megyesi et al., 2010) do not have phrase alignments. Some (Sulger et al., 2013; Kapanadze, 2012) do have phrase alignments, but neither discussion about the interaction between word-level and phraselevel alignments nor report of IAA is provided. There have been a few recent attempts at automatically aligning subtrees (comparable to our phrases) in the context of MT research, and the automatic alignments are evaluated against a small manually aligned data set. For example, (Tinsley et al., 2007) evaluated an unsupervised algorithm on 810 parsed English-French pairs annotated with subtree alignment. (Xiao and Zhu, 2013) also developed unsupervised subtree alignment method"
C14-1143,C10-1118,0,0.0159012,"is provided. There have been a few recent attempts at automatically aligning subtrees (comparable to our phrases) in the context of MT research, and the automatic alignments are evaluated against a small manually aligned data set. For example, (Tinsley et al., 2007) evaluated an unsupervised algorithm on 810 parsed English-French pairs annotated with subtree alignment. (Xiao and Zhu, 2013) also developed unsupervised subtree alignment methods (EM and Variational Bayes) and evaluated their automatic alignment model on 637 sentences from the Chinese TreeBank (and use the other 99 for tuning). (Sun et al., 2010b; Sun et al., 2010a) also report work on aligning subtrees and evaluate their impact on MT. However, we are not aware of any attempt to systematically harmonize word alignment with the alignment of phrases, or subtrees, or to systematically study the incompatibilities between parallel parse trees. 1518 7 Conclusion In this paper we report our effort on the construction of a Chinese-English parallel treebank with both word-level and phrase-level alignments. When constructing the treebank, we systematically consider the interaction between word alignments and phrase alignments, and try to harmo"
C14-1143,P10-1032,0,0.190417,"is provided. There have been a few recent attempts at automatically aligning subtrees (comparable to our phrases) in the context of MT research, and the automatic alignments are evaluated against a small manually aligned data set. For example, (Tinsley et al., 2007) evaluated an unsupervised algorithm on 810 parsed English-French pairs annotated with subtree alignment. (Xiao and Zhu, 2013) also developed unsupervised subtree alignment methods (EM and Variational Bayes) and evaluated their automatic alignment model on 637 sentences from the Chinese TreeBank (and use the other 99 for tuning). (Sun et al., 2010b; Sun et al., 2010a) also report work on aligning subtrees and evaluate their impact on MT. However, we are not aware of any attempt to systematically harmonize word alignment with the alignment of phrases, or subtrees, or to systematically study the incompatibilities between parallel parse trees. 1518 7 Conclusion In this paper we report our effort on the construction of a Chinese-English parallel treebank with both word-level and phrase-level alignments. When constructing the treebank, we systematically consider the interaction between word alignments and phrase alignments, and try to harmo"
C14-1143,2007.mtsummit-papers.62,0,0.521456,"d alignments and syntactic structures. See the discussion of the concrete example in Figure 1 below to see the points made here. Next we discuss our annotation procedure in detail. Our annotators are presented with sentence pairs that come with parallel parse trees. The task of the annotator is to decide, ﬁrst on the word level and then on the phrase level, if a word or phrase needs to be aligned at all, and if so, to which word or phrase it should be aligned. The decisions about word alignment and phrase alignment are not independent, and must obey well-formedness constraints as outlined in (Tinsley et al., 2007): a. A non-terminal node can only be aligned once. b. if Node nc is aligned to Node ne , then the descendants of nc can only be aligned to descendants of ne . c. if Node nc is aligned to Node ne , then the ancestors of nc can only be aligned to ancestors of ne . This means that once a word alignment is in place, it puts constraints on phrase alignments. A pair of non-terminal nodes (nc , ne ) cannot be aligned if a word that is a descendant of nc is aligned to a word that is not a descendant of ne on the word level. Let us use the concrete example in Figure 1 to illustrate the annotation proce"
C14-1143,W12-6306,1,0.590567,"nd procedure The data we annotate is the Chinese-English portion of the Parallel Aligned Treebank (PAT) described in (Li et al., 2012). Our data consists of two batches, one of which is weblogs and the other of which is postings from online discussion forums. The English sentences in the data set are annotated based on the original Penn TreeBank (PTB) annotation stylebook (Bies et al., 1995) as well as its extensions (Warner et al., 2004), while the Chinese sentences in the data set are annotated based on the Chinese TreeBank (CTB) annotation guidelines (Xue and Xia, 2000) and its extensions (Zhang and Xue, 2012). The PAT only has word alignments, which are done under the GTAHS, and no phrase alignments. The main departure of our approach is that we loosen the requirement that every word in a sentence pair needs to be word-aligned. On the word level, we only align words that have an equivalent in terms of lexical meaning and grammatical function. For words that do not have a translation counterpart, we leave them unaligned and locate the appropriate phrases in which they appear to be aligned. This way, we eliminate both the redundancies and spurious ambiguities discussed in Section 2. Since phrase ali"
D08-1074,W00-0205,0,0.034392,"e target verb itself and its POS tag represents this information. Resultatives in the form of resultative verb compound and the DER construction, quantifiers in the object are other surface reflections of the abstract notion of boundedness. The fact that these features have contributed to the determination of the temporal location of situations to certain extent lends support to Smith’s theoretical claim. 5 Related work Inferring the temporal location is a difficult problem that is not yet very well understood. It has not been studied extensively in the context of Natural Language Processing. Olson et al (2000; 2001) realized the importance of using the aspectual information (both grammatical and lexical aspect) to infer tense in the context of a Chinese-English Machine Translation system. They encoded the aspectual information such as telicity as part of the Lexical Conceptual Structure and use it to heuristically infer tense when generating the English output. This rule-based approach is not very suited for modeling the temporal location information in Chinese. As they themselves noted, aspectual information can only be used as a tendency rather than a deterministic rule. We believe this problem"
D08-1074,2001.mtsummit-papers.47,0,0.0783487,"Missing"
D08-1074,xue-etal-2008-annotating,1,0.451642,"ed in are expressed as clauses centered around a verb, and for the sake of convenience we mark the “tense” on the verb itself instead of the entire clause. However, when inferring the temporal location of a situation, we have to take into consideration the entire clause, because the arguments and modifiers of a verb are just as important as the verb itself when determining the temporal location of the situation. The annotation is performed on data selected from the Chinese Treebank (Xue et al., 2005), and more detailed descriptions and justifications for the annotation scheme is described in (Xue et al., 2008). Data selection is important for tense annotation because, unlike POS-tagging and syntactic annotation, which applies equally well to different genres of text, temporal annotation in more relevant in some genres than others. The data selection task is made easier by the fact that the Chinese Treebank is already annotated with POS tags and Penn Treebank-style syntactic structures. Therefore we were able to just select articles based on how many constituents in the article are annotated with the temporal function tag -TMP. We have annotated 42 articles in total, and all verbs in an article are"
D14-1204,li-etal-2012-parallel,1,0.831952,"automatic inference of the semantic tense of events in Chinese is a very challenging task (Xue, 2008; Ye et al., 2006; Liu et al., 2011). There are at least two reasons why this is a difficult problem. First, since Chinese does not have grammatical tense which could serve as an important clue when annotating the semantic tense of an event, generating consistent annotation for Chinese semantic tense has proved to be a challenge. Xue and Zhang (2014) use a “distant annotation” method to address this problem. They take advantage of an English-Chinese parallel corpus with manual word alignments (Li et al., 2012) , and perform annotation on the English side, which provides more explicit information such as grammatical tense that helps annotators decide the appropriate semantic tense. The annotations are then projected to the Chinese side via the word alignments. They show consistent annotation agreements on semantic tense. Second, the lack of grammatical tense also makes automatic inference of Chinese semantic tense challenging since the grammatical tense would be an important source of information for predicting the semantic tense. Previous work has shown that it is very difficult to achieve high acc"
D14-1204,I11-1125,0,0.174444,"of the temporal location of an event. In this paper we address the problem of inferring the semantic tense, or the temporal location of an event (e.g., present, past, future) in Chinese text. The semantic tense is defined relative to the utterance time or document creation time, and it does not always agree with the grammatical tense in languages like English where there is grammatical tense. Inferring semantic tense potentially benefits natural language processing tasks such as Machine Translation and Information Extraction (Xue, 2008; Reichart and Rappoport, 2010; Ye et al., 2006; Ye, 2007; Liu et al., 2011), but previous work has shown that automatic inference of the semantic tense of events in Chinese is a very challenging task (Xue, 2008; Ye et al., 2006; Liu et al., 2011). There are at least two reasons why this is a difficult problem. First, since Chinese does not have grammatical tense which could serve as an important clue when annotating the semantic tense of an event, generating consistent annotation for Chinese semantic tense has proved to be a challenge. Xue and Zhang (2014) use a “distant annotation” method to address this problem. They take advantage of an English-Chinese parallel co"
D14-1204,J93-2004,0,0.0467317,"Missing"
D14-1204,D10-1032,0,0.451016,"h, the grammatical tense of a verb is a strong indicator of the temporal location of an event. In this paper we address the problem of inferring the semantic tense, or the temporal location of an event (e.g., present, past, future) in Chinese text. The semantic tense is defined relative to the utterance time or document creation time, and it does not always agree with the grammatical tense in languages like English where there is grammatical tense. Inferring semantic tense potentially benefits natural language processing tasks such as Machine Translation and Information Extraction (Xue, 2008; Reichart and Rappoport, 2010; Ye et al., 2006; Ye, 2007; Liu et al., 2011), but previous work has shown that automatic inference of the semantic tense of events in Chinese is a very challenging task (Xue, 2008; Ye et al., 2006; Liu et al., 2011). There are at least two reasons why this is a difficult problem. First, since Chinese does not have grammatical tense which could serve as an important clue when annotating the semantic tense of an event, generating consistent annotation for Chinese semantic tense has proved to be a challenge. Xue and Zhang (2014) use a “distant annotation” method to address this problem. They ta"
D14-1204,xue-zhang-2014-buy,1,0.574002,"Machine Translation and Information Extraction (Xue, 2008; Reichart and Rappoport, 2010; Ye et al., 2006; Ye, 2007; Liu et al., 2011), but previous work has shown that automatic inference of the semantic tense of events in Chinese is a very challenging task (Xue, 2008; Ye et al., 2006; Liu et al., 2011). There are at least two reasons why this is a difficult problem. First, since Chinese does not have grammatical tense which could serve as an important clue when annotating the semantic tense of an event, generating consistent annotation for Chinese semantic tense has proved to be a challenge. Xue and Zhang (2014) use a “distant annotation” method to address this problem. They take advantage of an English-Chinese parallel corpus with manual word alignments (Li et al., 2012) , and perform annotation on the English side, which provides more explicit information such as grammatical tense that helps annotators decide the appropriate semantic tense. The annotations are then projected to the Chinese side via the word alignments. They show consistent annotation agreements on semantic tense. Second, the lack of grammatical tense also makes automatic inference of Chinese semantic tense challenging since the gra"
D14-1204,D08-1074,1,0.911808,"e in English, the grammatical tense of a verb is a strong indicator of the temporal location of an event. In this paper we address the problem of inferring the semantic tense, or the temporal location of an event (e.g., present, past, future) in Chinese text. The semantic tense is defined relative to the utterance time or document creation time, and it does not always agree with the grammatical tense in languages like English where there is grammatical tense. Inferring semantic tense potentially benefits natural language processing tasks such as Machine Translation and Information Extraction (Xue, 2008; Reichart and Rappoport, 2010; Ye et al., 2006; Ye, 2007; Liu et al., 2011), but previous work has shown that automatic inference of the semantic tense of events in Chinese is a very challenging task (Xue, 2008; Ye et al., 2006; Liu et al., 2011). There are at least two reasons why this is a difficult problem. First, since Chinese does not have grammatical tense which could serve as an important clue when annotating the semantic tense of an event, generating consistent annotation for Chinese semantic tense has proved to be a challenge. Xue and Zhang (2014) use a “distant annotation” method to"
D14-1204,W06-0107,0,0.366606,"verb is a strong indicator of the temporal location of an event. In this paper we address the problem of inferring the semantic tense, or the temporal location of an event (e.g., present, past, future) in Chinese text. The semantic tense is defined relative to the utterance time or document creation time, and it does not always agree with the grammatical tense in languages like English where there is grammatical tense. Inferring semantic tense potentially benefits natural language processing tasks such as Machine Translation and Information Extraction (Xue, 2008; Reichart and Rappoport, 2010; Ye et al., 2006; Ye, 2007; Liu et al., 2011), but previous work has shown that automatic inference of the semantic tense of events in Chinese is a very challenging task (Xue, 2008; Ye et al., 2006; Liu et al., 2011). There are at least two reasons why this is a difficult problem. First, since Chinese does not have grammatical tense which could serve as an important clue when annotating the semantic tense of an event, generating consistent annotation for Chinese semantic tense has proved to be a challenge. Xue and Zhang (2014) use a “distant annotation” method to address this problem. They take advantage of a"
D17-1129,D15-1198,0,0.0308664,"014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasis on modeling the formal properties of the AMR graph. One practical implementation of HRG-based parsing is that of (Peng et al., 2015; Peng and Gildea, 2016). The adoption of Combinatory Categorical Grammar (CCG) in AMR parsing has also been explored in (Artzi et al., 2015; Misra and Artzi, 2016), where a number of extensions have been proposed to enable CCG to work on the broad-coverage AMR corpus. More recently, Foland and Martin (2016; 2017) describe a neural network based model that decomposes the AMR parsing task into a series of subproblems. Their system first identifies the concepts using a Bidirectional LSTM Recurrent Neural Network (Hochreiter and Schmidhuber, 1997), and then locates and labels the arguments and attributes for each predicate, and finally constructs the AMR using the concepts and relations identified in previous steps. (Barzdins and Gos"
D17-1129,W13-2322,0,0.233021,"rser: concept identification and alignment. We first build a Bidirectional LSTM based concept identifier that is able to incorporate richer contextual information to learn sparse AMR concept labels. We then extend an HMM-based word-to-concept alignment model with graph distance distortion and a rescoring method during decoding to incorporate the structural information in the AMR graph. We show integrating the two components into an existing AMR parser results in consistently better performance over the state of the art on various datasets. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation where the meaning of a sentence is encoded as a rooted, directed graph. A number of AMR parsers have been developed in recent years (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016a), and the initial benefit of AMR parsing has been demonstrated in various downstream applications such as Information Extraction (Pan et al., 2015; Huang et al., 2016), Machine Comprehension (Sachan and Xing, 2016), and Language Generation (Flanigan et al., 2016b; Butler, 2016). However, AMR pa"
D17-1129,S16-1176,0,0.310248,"tzi et al., 2015; Misra and Artzi, 2016), where a number of extensions have been proposed to enable CCG to work on the broad-coverage AMR corpus. More recently, Foland and Martin (2016; 2017) describe a neural network based model that decomposes the AMR parsing task into a series of subproblems. Their system first identifies the concepts using a Bidirectional LSTM Recurrent Neural Network (Hochreiter and Schmidhuber, 1997), and then locates and labels the arguments and attributes for each predicate, and finally constructs the AMR using the concepts and relations identified in previous steps. (Barzdins and Gosko, 2016) first applies the sequence-tosequence model (Sutskever et al., 2014) typically used in neural machine translation to AMR parsing by simply treating the pre-order traversal of AMR as foreign language strings. (Peng et al., 2017) also adopts the sequence-to-sequence model for neural AMR parsing and focuses on reducing data sparsity in neural AMR parsing with categorization of the concept and relation labels. In contrast, (Konstas et al., 2017) adopts a different approach and tackles the data sparsity problem with a self-training procedure that can utilize a large set of unannotated external cor"
D17-1129,J93-2003,0,0.0714735,"This mapping will then be used as reference data to train concept identification models. The JAMR aligner (Flanigan et al., 2014) greedily aligns a span of words to graph fragments using a set of heuristics. While it can easily incorporate information from additional linguistic sources such as WordNet, it is not adaptable to other domains. Unsupervised aligners borrow techniques from Machine Translation and treat sentence-to-AMR alignment as a word alignment problem between a source sentence and its linearized AMR graph (Pourdamghani et al., 2014) and solve it with IBM word alignment models (Brown et al., 1993). However, the distortion model in the IBM models is based on the linear distance between source side words while the linear order of the AMR concepts has no linguistic significance, unlike word order in natural language. A more appropriate sentence-to-AMR alignment model should be one that takes the hierarchical structure of the AMR into account. We develop a Hidden Markov Model (HMM)-based sentence-to-AMR alignment method with a novel Graph Distance distortion model to take advantage of the structural information in AMR, and apply a structural constraint to re-score the posterior during deco"
D17-1129,W16-0601,0,0.181108,"by defining a set of generative actions that maps words in the sentence to their AMR concepts and use a local classifier to learn these actions. Given such sparse data, making full use of contextual information is crucial to accurate concept labeling. Bidirectional LSTM has shown its success on many sequence labeling tasks since it is able to combine contextual information from both directions and avoid manual feature engineering. However, it is non-trivial to formalize concept identification as a sequence labeling problem because of the large concept label set. Inspired by Foland and Martin (2016; 2017), who first apply the Bidirectional LSTM to AMR concept identification by categorizing the large labels into a finite set of predefined types, we propose to address concept identification using Bidirectional LSTM with Factored Concept Labels (FCL), where we re-group the concept label set based on their shared graph structure. This makes it possible for different concepts to be represented by one common label that captures the shared semantics of these concepts. 1257 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268 c Copenhagen, Denmark, Se"
D17-1129,P17-1112,0,0.217428,"applies the sequence-tosequence model (Sutskever et al., 2014) typically used in neural machine translation to AMR parsing by simply treating the pre-order traversal of AMR as foreign language strings. (Peng et al., 2017) also adopts the sequence-to-sequence model for neural AMR parsing and focuses on reducing data sparsity in neural AMR parsing with categorization of the concept and relation labels. In contrast, (Konstas et al., 2017) adopts a different approach and tackles the data sparsity problem with a self-training procedure that can utilize a large set of unannotated external corpus. (Buys and Blunsom, 2017) design a generic transitionbased system for semantic graph parsing and apply sequence-to-sequence framework to learn the 1258 transformation from natural language sequences to action sequences. 3 Concept Identification with Bidirectional LSTM In this section, we first introduce how we categorize AMR concepts using Factored Concept Labels. We then integrate character-level information into a Bidirectional LSTM through Convolutional Neural Network (CNN)-based embeddings. 3.2 Factored Concept Labels To be able to fit AMR’s large concept label space into a sequence labeling framework, redefining"
D17-1129,P13-2131,0,0.233543,". Then we report the final results by incorporating both components to CAMR (Wang et al., 2016). At the model development stage, we mainly use the dataset LDC2015E86 used in the SemEval Shared Task (May, 2016). Note that this dataset includes :wiki relations where every named entity concept is linked to its wikipedia entry. We remove this information in the training data throughout the development of our models. At the final testing stage, we add wikification using an off-theshelf AMR wikifier (Pan et al., 2015) as a postprocessing step. All AMR parsing results are evaluated using the Smatch (Cai and Knight, 2013) scorer. 5.1 Input word,NER word,NER,CNN R 80.6 82.7 F1 80.9 83.0 Acc 85.4 87.0 Table 1: Performance of Bidirectional LSTM with different input. Table 1 shows the performance on the development set of LDC2015E86, where the precision, recall and F-score are computed by treating hotheri as the negative label and accuracy is calculated using all labels. We include accuracy here since correctly predicting words that don’t invoke concepts is also important. We can see that utilizing CNN-based character level embedding yields an improvement of around 2 percentage points absolute for both F-score and"
D17-1129,P13-1091,0,0.0273916,"et of actions that transform the dependency tree into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasis on modeling the formal properties of the AMR graph. One practical implementation of HRG-based parsing is that of (Peng et al., 2015; Peng and Gildea, 2016). The adoption of Combinatory Categorical Grammar (CCG) in AMR parsing has also been explored in (Artzi et al., 2015; Misra and Artzi, 2016), where a number of extensions have been proposed to enable CCG to work on the broad-coverage AMR corpus. More recently, Foland and Martin (2016; 2017) describe a neural network based model that decomposes the AMR parsing task into a s"
D17-1129,P07-1003,0,0.207356,"on and picks the closer English word. The rescaling factor can be viewed as a selection filter for decoding, where it relies on the graph depth difference ∆d to control the effect of learned distortion probability. Note that after the rescaling, the resulting distortion probability no longer satisfies the normalization constraint. However, we only apply this during decoding time and experiments show that the typical threshold γ = 0.5 still works well for our case. 4.3 Combining Both Directions Empirical results show that combining alignments from both directions improve the alignment quality (DeNero and Klein, 2007; Och and Ney, 2003; Liang et al., 2006). To combine the alignments, we adopt a slightly modified version of posterior thresholding, competitive thresholding, as proposed in (DeNero and Klein, 2007), which tends to select alignments that form a contiguous span. 1262 velopment set. Adding other labels that include P REDICATE, N ON - PREDICATE and C ONST gives us 116 canonical labels. U NK label is added to handle the unseen concepts. In the Bidirectional LSTM, the hyperparameter settings are as follows: word embedding dimension dwd = 128, NER tag embedding dimension dt = 8, character embedding"
D17-1129,S16-1186,0,0.479692,"Missing"
D17-1129,N16-1087,0,0.00881954,"Missing"
D17-1129,P14-1134,0,0.175513,"ts. 1257 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Accurate concept identification also crucially depends on the word-to-AMR-concept alignment. Since there is no manual alignment in the AMR annotation, typically either a rule-based or unsupervised aligner is applied to the training data to extract the mapping between words and concepts. This mapping will then be used as reference data to train concept identification models. The JAMR aligner (Flanigan et al., 2014) greedily aligns a span of words to graph fragments using a set of heuristics. While it can easily incorporate information from additional linguistic sources such as WordNet, it is not adaptable to other domains. Unsupervised aligners borrow techniques from Machine Translation and treat sentence-to-AMR alignment as a word alignment problem between a source sentence and its linearized AMR graph (Pourdamghani et al., 2014) and solve it with IBM word alignment models (Brown et al., 1993). However, the distortion model in the IBM models is based on the linear distance between source side words whi"
D17-1129,S16-1185,0,0.193821,"esses this problem by defining a set of generative actions that maps words in the sentence to their AMR concepts and use a local classifier to learn these actions. Given such sparse data, making full use of contextual information is crucial to accurate concept labeling. Bidirectional LSTM has shown its success on many sequence labeling tasks since it is able to combine contextual information from both directions and avoid manual feature engineering. However, it is non-trivial to formalize concept identification as a sequence labeling problem because of the large concept label set. Inspired by Foland and Martin (2016; 2017), who first apply the Bidirectional LSTM to AMR concept identification by categorizing the large labels into a finite set of predefined types, we propose to address concept identification using Bidirectional LSTM with Factored Concept Labels (FCL), where we re-group the concept label set based on their shared graph structure. This makes it possible for different concepts to be represented by one common label that captures the shared semantics of these concepts. 1257 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268 c Copenhagen, Denm"
D17-1129,P17-1043,0,0.539718,"Missing"
D17-1129,P16-1001,0,0.0090489,"improvements to CAMR (Wang et al., 2016), a state-of-the-art transition-based AMR parser, results in consistently better Smatch scores over the state of the art on various datasets. The rest of paper is organized as follows. Section 2 describes related work on AMR parsing. Section 3 describes our improved LSTM based concept identification model, and Section 4 describes our alignment method. We present experimental results in Section 5, and conclude in Section 6. 2 Related Work Existing AMR parsers are either transition-based or graph-based. Transition-based AMR parsers (Wang et al., 2015b,a; Goodman et al., 2016a,b), focus on modeling the correspondence between the dependency tree and the AMR graph of a sentence by designing a small set of actions that transform the dependency tree into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (M"
D17-1129,S16-1180,0,0.0696558,"improvements to CAMR (Wang et al., 2016), a state-of-the-art transition-based AMR parser, results in consistently better Smatch scores over the state of the art on various datasets. The rest of paper is organized as follows. Section 2 describes related work on AMR parsing. Section 3 describes our improved LSTM based concept identification model, and Section 4 describes our alignment method. We present experimental results in Section 5, and conclude in Section 6. 2 Related Work Existing AMR parsers are either transition-based or graph-based. Transition-based AMR parsers (Wang et al., 2015b,a; Goodman et al., 2016a,b), focus on modeling the correspondence between the dependency tree and the AMR graph of a sentence by designing a small set of actions that transform the dependency tree into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (M"
D17-1129,P15-1143,0,0.01519,"ee into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasis on modeling the formal properties of the AMR graph. One practical implementation of HRG-based parsing is that of (Peng et al., 2015; Peng and Gildea, 2016). The adoption of Combinatory Categorical Grammar (CCG) in AMR parsing has also been explored in (Artzi et al., 2015; Misra and Artzi, 2016), where a number of extensions have been proposed to enable CCG to work on the broad-coverage AMR corpus. More recently, Foland and Martin (2016; 2017) describe a neural network based model that decomposes the AMR parsing task into a series of subproblems. Their system first identifies"
D17-1129,P16-1025,0,0.0224349,"Missing"
D17-1129,E17-1035,1,0.466212,"decomposes the AMR parsing task into a series of subproblems. Their system first identifies the concepts using a Bidirectional LSTM Recurrent Neural Network (Hochreiter and Schmidhuber, 1997), and then locates and labels the arguments and attributes for each predicate, and finally constructs the AMR using the concepts and relations identified in previous steps. (Barzdins and Gosko, 2016) first applies the sequence-tosequence model (Sutskever et al., 2014) typically used in neural machine translation to AMR parsing by simply treating the pre-order traversal of AMR as foreign language strings. (Peng et al., 2017) also adopts the sequence-to-sequence model for neural AMR parsing and focuses on reducing data sparsity in neural AMR parsing with categorization of the concept and relation labels. In contrast, (Konstas et al., 2017) adopts a different approach and tackles the data sparsity problem with a self-training procedure that can utilize a large set of unannotated external corpus. (Buys and Blunsom, 2017) design a generic transitionbased system for semantic graph parsing and apply sequence-to-sequence framework to learn the 1258 transformation from natural language sequences to action sequences. 3 Co"
D17-1129,W13-2263,0,0.0179736,"linear locality assumption does not hold among linearized AMR concepts, we choose instead to encode the distortion probability through graph distance, which is given by: ct(d(j, j 0 )) 00 0 j 00 ct(d(j , j )) Pgd (j |j 0 , G) = P (5) The graph distance d(j, j 0 ) is the length of shortest path on AMR graph G from concept j to concept j 0 . Note that we have to artificially normalize Pgd (j |j 0 , G), because unlike the linear distance between word tokens in a sentence, there can be multiple concepts that can have the same distance from the j 0 -th concept in the AMR graph, as pointed out in (Kondo et al., 2013). During training, just like the original HMMbased aligner, an EM algorithm can be applied to update the parameters of the model. 4.2 Improved Decoding with Posterior Rescoring So far, we have integrated the graph structure information into the forward direction (English to AMR). To also improve the reverse direction model (AMR to English), we choose to use the graph structure to rescore the posterior during decoding time. Compared with Viterbi decoding, posterior thresholding has shown better results in word alignment tasks (Liang et al., 2006). Given threshold γ, for all possible alignments,"
D17-1129,D14-1048,0,0.361324,"to the training data to extract the mapping between words and concepts. This mapping will then be used as reference data to train concept identification models. The JAMR aligner (Flanigan et al., 2014) greedily aligns a span of words to graph fragments using a set of heuristics. While it can easily incorporate information from additional linguistic sources such as WordNet, it is not adaptable to other domains. Unsupervised aligners borrow techniques from Machine Translation and treat sentence-to-AMR alignment as a word alignment problem between a source sentence and its linearized AMR graph (Pourdamghani et al., 2014) and solve it with IBM word alignment models (Brown et al., 1993). However, the distortion model in the IBM models is based on the linear distance between source side words while the linear order of the AMR concepts has no linguistic significance, unlike word order in natural language. A more appropriate sentence-to-AMR alignment model should be one that takes the hierarchical structure of the AMR into account. We develop a Hidden Markov Model (HMM)-based sentence-to-AMR alignment method with a novel Graph Distance distortion model to take advantage of the structural information in AMR, and ap"
D17-1129,P17-1014,0,0.139497,"abels the arguments and attributes for each predicate, and finally constructs the AMR using the concepts and relations identified in previous steps. (Barzdins and Gosko, 2016) first applies the sequence-tosequence model (Sutskever et al., 2014) typically used in neural machine translation to AMR parsing by simply treating the pre-order traversal of AMR as foreign language strings. (Peng et al., 2017) also adopts the sequence-to-sequence model for neural AMR parsing and focuses on reducing data sparsity in neural AMR parsing with categorization of the concept and relation labels. In contrast, (Konstas et al., 2017) adopts a different approach and tackles the data sparsity problem with a self-training procedure that can utilize a large set of unannotated external corpus. (Buys and Blunsom, 2017) design a generic transitionbased system for semantic graph parsing and apply sequence-to-sequence framework to learn the 1258 transformation from natural language sequences to action sequences. 3 Concept Identification with Bidirectional LSTM In this section, we first introduce how we categorize AMR concepts using Factored Concept Labels. We then integrate character-level information into a Bidirectional LSTM thr"
D17-1129,N06-1014,0,0.174303,"-th concept in the AMR graph, as pointed out in (Kondo et al., 2013). During training, just like the original HMMbased aligner, an EM algorithm can be applied to update the parameters of the model. 4.2 Improved Decoding with Posterior Rescoring So far, we have integrated the graph structure information into the forward direction (English to AMR). To also improve the reverse direction model (AMR to English), we choose to use the graph structure to rescore the posterior during decoding time. Compared with Viterbi decoding, posterior thresholding has shown better results in word alignment tasks (Liang et al., 2006). Given threshold γ, for all possible alignments, we select the final alignment based on the following criteria: a = {(i, j) : p(aj = i |g, e) &gt; γ} (6) where the state probability p(aj = i |g, e) is computed using the forward-backward algorithm. The forward algorithm is defined as: αj,i X = αj−1,i0 p(aj = i |aj−1 = i0 )p(gj |eaj ) i0 (7) To incorporate the graph structure, we rescale the distortion probability in reverse direction model as: pnew (aj = i |aj−1 = i0 ) ∆d = p(aj = i |aj−1 = i0 )e (8) where the scaling factor ∆d = dj − dj−1 is the graph depth difference between the adjacent AMR co"
D17-1129,S16-1166,0,0.604747,"AMR. It is aligned to English word “our” and its depth in graph dj−1 is 3. While the word distance-based distortion prefers an alignment near “our”, the correct alignment needs a longer distortion. 5 Experiments We first test the performance of our Bidirectional LSTM concept identifier and HMM-based aligner as standalone tasks, where we investigate the effectiveness of each component in AMR parsing. Then we report the final results by incorporating both components to CAMR (Wang et al., 2016). At the model development stage, we mainly use the dataset LDC2015E86 used in the SemEval Shared Task (May, 2016). Note that this dataset includes :wiki relations where every named entity concept is linked to its wikipedia entry. We remove this information in the training data throughout the development of our models. At the final testing stage, we add wikification using an off-theshelf AMR wikifier (Pan et al., 2015) as a postprocessing step. All AMR parsing results are evaluated using the Smatch (Cai and Knight, 2013) scorer. 5.1 Input word,NER word,NER,CNN R 80.6 82.7 F1 80.9 83.0 Acc 85.4 87.0 Table 1: Performance of Bidirectional LSTM with different input. Table 1 shows the performance on the develo"
D17-1129,D15-1136,0,0.131252,"as follows. Section 2 describes related work on AMR parsing. Section 3 describes our improved LSTM based concept identification model, and Section 4 describes our alignment method. We present experimental results in Section 5, and conclude in Section 6. 2 Related Work Existing AMR parsers are either transition-based or graph-based. Transition-based AMR parsers (Wang et al., 2015b,a; Goodman et al., 2016a,b), focus on modeling the correspondence between the dependency tree and the AMR graph of a sentence by designing a small set of actions that transform the dependency tree into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasi"
D17-1129,D16-1183,0,0.0127322,"al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasis on modeling the formal properties of the AMR graph. One practical implementation of HRG-based parsing is that of (Peng et al., 2015; Peng and Gildea, 2016). The adoption of Combinatory Categorical Grammar (CCG) in AMR parsing has also been explored in (Artzi et al., 2015; Misra and Artzi, 2016), where a number of extensions have been proposed to enable CCG to work on the broad-coverage AMR corpus. More recently, Foland and Martin (2016; 2017) describe a neural network based model that decomposes the AMR parsing task into a series of subproblems. Their system first identifies the concepts using a Bidirectional LSTM Recurrent Neural Network (Hochreiter and Schmidhuber, 1997), and then locates and labels the arguments and attributes for each predicate, and finally constructs the AMR using the concepts and relations identified in previous steps. (Barzdins and Gosko, 2016) first applies"
D17-1129,P16-2079,0,0.0728555,"Missing"
D17-1129,J03-1002,0,0.028169,"English word. The rescaling factor can be viewed as a selection filter for decoding, where it relies on the graph depth difference ∆d to control the effect of learned distortion probability. Note that after the rescaling, the resulting distortion probability no longer satisfies the normalization constraint. However, we only apply this during decoding time and experiments show that the typical threshold γ = 0.5 still works well for our case. 4.3 Combining Both Directions Empirical results show that combining alignments from both directions improve the alignment quality (DeNero and Klein, 2007; Och and Ney, 2003; Liang et al., 2006). To combine the alignments, we adopt a slightly modified version of posterior thresholding, competitive thresholding, as proposed in (DeNero and Klein, 2007), which tends to select alignments that form a contiguous span. 1262 velopment set. Adding other labels that include P REDICATE, N ON - PREDICATE and C ONST gives us 116 canonical labels. U NK label is added to handle the unseen concepts. In the Bidirectional LSTM, the hyperparameter settings are as follows: word embedding dimension dwd = 128, NER tag embedding dimension dt = 8, character embedding dimension dc = 50,"
D17-1129,W15-3904,0,0.0168354,"ation originating from the word form itself. As we have discussed above, in some of the M ULTICONCEPT cases the concepts are associated with the word forms themselves and won’t benefit from its contextual information. For example, in “unprecedented”, the prefix “un” itself already gives enough information to predict the FCL label hxi : polarity -, which indicates negative polarity. In order to incorporate such morphological and shape information, we choose to add a convolutional layer to extract character-level representations. A similar technique has been applied to Named Entity Recognition (Santos and Guimaraes, 2015; Chiu and Nichols, 2015) and we only provide a brief description of the architecture here. For a word w composed of characters {c1 , c2 , . . . , cl }, where l is the length of word w, we learn a character embedding matrix Wc ∈ Rdc ×|Vc |, where dc is the character embedding dimension defined by the user and Vc is character vocabulary size. After retrieving the character embedding chi for each character ci in word w, we 1260 obtain a sequence of vectors {ch1 , ch2 , . . . , chl }. This serves as the input to convolutional layer. concept. The relation (also called AMR role token) between conce"
D17-1129,J05-1004,0,0.0693588,"Missing"
D17-1129,N15-1119,0,0.0470743,"aligner as standalone tasks, where we investigate the effectiveness of each component in AMR parsing. Then we report the final results by incorporating both components to CAMR (Wang et al., 2016). At the model development stage, we mainly use the dataset LDC2015E86 used in the SemEval Shared Task (May, 2016). Note that this dataset includes :wiki relations where every named entity concept is linked to its wikipedia entry. We remove this information in the training data throughout the development of our models. At the final testing stage, we add wikification using an off-theshelf AMR wikifier (Pan et al., 2015) as a postprocessing step. All AMR parsing results are evaluated using the Smatch (Cai and Knight, 2013) scorer. 5.1 Input word,NER word,NER,CNN R 80.6 82.7 F1 80.9 83.0 Acc 85.4 87.0 Table 1: Performance of Bidirectional LSTM with different input. Table 1 shows the performance on the development set of LDC2015E86, where the precision, recall and F-score are computed by treating hotheri as the negative label and accuracy is calculated using all labels. We include accuracy here since correctly predicting words that don’t invoke concepts is also important. We can see that utilizing CNN-based cha"
D17-1129,S16-1183,0,0.0133999,"anguage output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasis on modeling the formal properties of the AMR graph. One practical implementation of HRG-based parsing is that of (Peng et al., 2015; Peng and Gildea, 2016). The adoption of Combinatory Categorical Grammar (CCG) in AMR parsing has also been explored in (Artzi et al., 2015; Misra and Artzi, 2016), where a number of extensions have been proposed to enable CCG to work on the broad-coverage AMR corpus. More recently, Foland and Martin (2016; 2017) describe a neural network based model that decomposes the AMR parsing task into a series of subproblems. Their system first identifies the concepts using a Bidirectional LSTM Recurrent Neural Network (Hochreiter and Schmidhuber, 1997), and then locates and labels the arguments and attributes for each predic"
D17-1129,K15-1004,0,0.0755561,"AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasis on modeling the formal properties of the AMR graph. One practical implementation of HRG-based parsing is that of (Peng et al., 2015; Peng and Gildea, 2016). The adoption of Combinatory Categorical Grammar (CCG) in AMR parsing has also been explored in (Artzi et al., 2015; Misra and Artzi, 2016), where a number of extensions have been proposed to enable CCG to work on the broad-coverage AMR corpus. More recently, Foland and Martin (2016; 2017) describe a neural network based model that decomposes the AMR parsing task into a series of subproblems. Their system first identifies the concepts using a Bidirectional LSTM Recurrent Neural Network (Hochreiter and Schmidhuber, 1997), and then locates and labels the arguments and at"
D17-1129,C96-2141,0,0.171286,"Sentence to its AMR graph = I Y Pd (ai |ai−1 , J)Pt (ei |gai ) (3) i=1 Given an AMR graph G and English sentence e = {e1 , e2 , . . . , ei , . . . , eI }, in order to fit them into the traditional word alignment framework, the AMR graph G is normally linearized using depth first search by printing each node as soon as it it visited. The re-entrance node is printed but not expanded to preserve the multiple mentions of where Pd is the distortion model and Pt is the translation model. Traditionally, the distortion probability Pd (j |j 0 , J) is modeled to depend only on the jump width (j −j 0 ) (Vogel et al., 1996) and is defined as: ct(j − j 0 ) Pd (j |j 0 , J) = PJ (4) 00 0 j 00 =1 ct(j − j ) 1261 where ct(j − j 0 ) is the count of jump width. This formula simultaneously satisfies the normalization constraint and captures the locality assumption that words that are adjacent in the source sentence tend to align to words that are closer in the target sentence. As the linear locality assumption does not hold among linearized AMR concepts, we choose instead to encode the distortion probability through graph distance, which is given by: ct(d(j, j 0 )) 00 0 j 00 ct(d(j , j )) Pgd (j |j 0 , G) = P (5) The gr"
D17-1129,S16-1181,1,0.913509,"de words while the linear order of the AMR concepts has no linguistic significance, unlike word order in natural language. A more appropriate sentence-to-AMR alignment model should be one that takes the hierarchical structure of the AMR into account. We develop a Hidden Markov Model (HMM)-based sentence-to-AMR alignment method with a novel Graph Distance distortion model to take advantage of the structural information in AMR, and apply a structural constraint to re-score the posterior during decoding time. We present experimental results that show incorporating these two improvements to CAMR (Wang et al., 2016), a state-of-the-art transition-based AMR parser, results in consistently better Smatch scores over the state of the art on various datasets. The rest of paper is organized as follows. Section 2 describes related work on AMR parsing. Section 3 describes our improved LSTM based concept identification model, and Section 4 describes our alignment method. We present experimental results in Section 5, and conclude in Section 6. 2 Related Work Existing AMR parsers are either transition-based or graph-based. Transition-based AMR parsers (Wang et al., 2015b,a; Goodman et al., 2016a,b), focus on modeli"
D17-1129,P15-2141,1,0.458696,"ncorporating these two improvements to CAMR (Wang et al., 2016), a state-of-the-art transition-based AMR parser, results in consistently better Smatch scores over the state of the art on various datasets. The rest of paper is organized as follows. Section 2 describes related work on AMR parsing. Section 3 describes our improved LSTM based concept identification model, and Section 4 describes our alignment method. We present experimental results in Section 5, and conclude in Section 6. 2 Related Work Existing AMR parsers are either transition-based or graph-based. Transition-based AMR parsers (Wang et al., 2015b,a; Goodman et al., 2016a,b), focus on modeling the correspondence between the dependency tree and the AMR graph of a sentence by designing a small set of actions that transform the dependency tree into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanni"
D17-1129,N15-1040,1,0.521012,"ncorporating these two improvements to CAMR (Wang et al., 2016), a state-of-the-art transition-based AMR parser, results in consistently better Smatch scores over the state of the art on various datasets. The rest of paper is organized as follows. Section 2 describes related work on AMR parsing. Section 3 describes our improved LSTM based concept identification model, and Section 4 describes our alignment method. We present experimental results in Section 5, and conclude in Section 6. 2 Related Work Existing AMR parsers are either transition-based or graph-based. Transition-based AMR parsers (Wang et al., 2015b,a; Goodman et al., 2016a,b), focus on modeling the correspondence between the dependency tree and the AMR graph of a sentence by designing a small set of actions that transform the dependency tree into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanni"
D17-1129,P15-1095,0,0.215936,"Missing"
D17-1129,D16-1065,0,0.15423,"sers are either transition-based or graph-based. Transition-based AMR parsers (Wang et al., 2015b,a; Goodman et al., 2016a,b), focus on modeling the correspondence between the dependency tree and the AMR graph of a sentence by designing a small set of actions that transform the dependency tree into the AMR graph. Pust et al. (2015) formulates AMR parsing as a machine translation problem in which the sentence is the source language input and the AMR is the target language output. AMR parsing systems that focus on modeling the graph aspect of the AMR includes JAMR (Flanigan et al., 2014, 2016a; Zhou et al., 2016), which treats AMR parsing as a procedure for searching for the Maximum Spanning Connected Subgraphs (MSCGs) from an edge-labeled, directed graph of all possible relations. Parsers based on Hyperedge Replacement Grammars (HRG) (Chiang et al., 2013; Bj¨orklund et al., 2016; Groschwitz et al., 2015) put more emphasis on modeling the formal properties of the AMR graph. One practical implementation of HRG-based parsing is that of (Peng et al., 2015; Peng and Gildea, 2016). The adoption of Combinatory Categorical Grammar (CCG) in AMR parsing has also been explored in (Artzi et al., 2015; Misra and"
D18-1371,S13-2002,0,0.0317179,"usion matrix. Rows are gold relation labels and columns are automatic relation labels. “be, af, ov, in, de” stand for “before, after, overlap, include, and depend-on”. 7 Table 6: Parent node confusion matrix. Rows are gold parents and columns are automatically parsed parents. “pre” means the parent is the immediate previous node of the child event, “far” means the parent is further back from the child event. be 1 1 1 0 0 3 be 367 1 74 3 0 445 Related Work Related Work on Temporal Relation Modeling There is a significant amount of research on temporal relation extraction (Bethard et al., 2007; Bethard, 2013; Chambers and Jurafsky, 2008; Chambers et al., 2014; Ning et al., 2018a). Most of the previous work models temporal relation extraction as pair-wise classification between individual pairs of events and/or time expressions. Some of the models also add a global reasoning step to local pair-wise classification, typically using Integer Linear Programming, to exploit the transitivity property of temporal relations (Chambers and Jurafsky, 2008). Such a pair-wise clas3346 sification approach is often dictated by the way the data is annotated. In most of the widely used temporal data sets, temporal"
D18-1371,bethard-etal-2012-annotating,0,0.0223124,"uch a pair-wise clas3346 sification approach is often dictated by the way the data is annotated. In most of the widely used temporal data sets, temporal relations between individual pairs of events and/or time expressions are annotated independently of one another (Pustejovsky et al., 2003; Chambers et al., 2014; Styler IV et al., 2014; O’Gorman et al., 2016; Mostafazadeh et al., 2016). Our work is most closely related to that of Kolomiyets et al. (2012), which also treats temporal relation modeling as temporal dependency structure parsing. However, their dependency structure, as described in Bethard et al. (2012), is only over events, excluding time expressions which are an important source of temporal information, and it also excludes states (stative events), which makes the temporal dependency structure incomplete. Moreover, their corpus only consists of data in the narrative stories domain. We instead choose to develop our model based on the data set described in Zhang and Xue (2018), which introduces a more comprehensive and linguistically grounded annotation scheme for temporal dependency structures. This structure includes both events and time expressions, and uses the linguistic notion of tempo"
D18-1371,S17-2093,0,0.0681232,"e has the advantage that (1) it can be easily used to infer additional temporal relations between time expressions and/or events that are not directly connected via the transitivity properties of temporal relations, (2) it is computationally more efficient because a model does not need to consider all pairs of time expressions and events in a text, and (3) it is easier to use for downstream applications such as timeline construction. However, most existing automatic systems are pair-wise models trained with traditional statistical classifiers using a large number of manually crafted features (Bethard et al., 2017). The few exceptions include the work of Kolomiyets et al. (2012), which describes a temporal dependency parser based on traditional feature-based classifiers, and Dligach et al. (2017), which describes a system using neural network based models to classify individual temporal relations. More recently, a semi-structured approach has also been proposed (Ning et al., 2018b). In this work, taking advantage of a newly available data set annotated with temporal dependency structures – the Temporal Dependency Tree (TDT) Corpus1 (Zhang and Xue, 2018), we develop a neural temporal dependency structure"
D18-1371,Q14-1022,0,0.0173191,"and columns are automatic relation labels. “be, af, ov, in, de” stand for “before, after, overlap, include, and depend-on”. 7 Table 6: Parent node confusion matrix. Rows are gold parents and columns are automatically parsed parents. “pre” means the parent is the immediate previous node of the child event, “far” means the parent is further back from the child event. be 1 1 1 0 0 3 be 367 1 74 3 0 445 Related Work Related Work on Temporal Relation Modeling There is a significant amount of research on temporal relation extraction (Bethard et al., 2007; Bethard, 2013; Chambers and Jurafsky, 2008; Chambers et al., 2014; Ning et al., 2018a). Most of the previous work models temporal relation extraction as pair-wise classification between individual pairs of events and/or time expressions. Some of the models also add a global reasoning step to local pair-wise classification, typically using Integer Linear Programming, to exploit the transitivity property of temporal relations (Chambers and Jurafsky, 2008). Such a pair-wise clas3346 sification approach is often dictated by the way the data is annotated. In most of the widely used temporal data sets, temporal relations between individual pairs of events and/or"
D18-1371,D08-1073,0,0.165198,"beats two strong baselines on both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area. 1 Introduction Temporal relation classification is important for a range of NLP applications that include but are not limited to story timeline construction, question answering, summarization, etc. Most work on temporal information extraction models the task as a pair-wise classification problem (Bethard et al., 2007; Chambers et al., 2007; Chambers and Jurafsky, 2008; Ning et al., 2018a): given an individual pair of time expressions and/or events, the system predicts whether they are temporally related and which specific relation holds between them. An alternative approach is to model the temporal relations in a text as a temporal dependency structure (TDS) for the entire text (Kolomiyets et al., 2012). Such a temporal dependency structure has the advantage that (1) it can be easily used to infer additional temporal relations between time expressions and/or events that are not directly connected via the transitivity properties of temporal relations, (2) i"
D18-1371,D17-1018,0,0.139326,"model is as follows: gi,yi0 = [xi , xyi0 , ti , tyi0 , ndi,yi0 , ssi,yi0 ] 4.5.2 Attention Model on Time and Event Representation In the basic neural model, a straight-forward sumpooling is used as the multi-word time expression and event representation. However, multi-word event expressions usually have meaning-bearing head words. For example, in the event “took a trip”, “trip” is more representative than “took” and “a”. Therefore, we add an attention mechanism (Bahdanau et al., 2014) over the Bi-LSTM output vectors in each multi-word expression to learn a task-specific notion of headedness (Lee et al., 2017): αt = tanh(W · wt∗ ) exp[αt ] wi,t = PEN D(i) k=ST ART (i) exp[αk ] PEN D(i) ˆ i = t=ST ART (i) wi,t · wt∗ x ˆ i is a weighted sum of Bi-LSTM output where x vectors in span i. The weights wi,t are automatically learned. The final pair representation for our attention model is as follows: ˆi, x ˆ yi0 ] gi,yi0 = [xi , xyi0 , ti , tyi0 , ndi,yi0 , ssi,yi0 , x This model variation is also beneficial in an end-to-end system, where time expression and event spans are automatically extracted in Stage 1. When extracted spans are not guaranteed correct time expressions and events, an attention layer o"
D18-1371,W16-1007,0,0.0218913,"me expressions. Some of the models also add a global reasoning step to local pair-wise classification, typically using Integer Linear Programming, to exploit the transitivity property of temporal relations (Chambers and Jurafsky, 2008). Such a pair-wise clas3346 sification approach is often dictated by the way the data is annotated. In most of the widely used temporal data sets, temporal relations between individual pairs of events and/or time expressions are annotated independently of one another (Pustejovsky et al., 2003; Chambers et al., 2014; Styler IV et al., 2014; O’Gorman et al., 2016; Mostafazadeh et al., 2016). Our work is most closely related to that of Kolomiyets et al. (2012), which also treats temporal relation modeling as temporal dependency structure parsing. However, their dependency structure, as described in Bethard et al. (2012), is only over events, excluding time expressions which are an important source of temporal information, and it also excludes states (stative events), which makes the temporal dependency structure incomplete. Moreover, their corpus only consists of data in the narrative stories domain. We instead choose to develop our model based on the data set described in Zhang"
D18-1371,N18-1077,0,0.114681,"both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area. 1 Introduction Temporal relation classification is important for a range of NLP applications that include but are not limited to story timeline construction, question answering, summarization, etc. Most work on temporal information extraction models the task as a pair-wise classification problem (Bethard et al., 2007; Chambers et al., 2007; Chambers and Jurafsky, 2008; Ning et al., 2018a): given an individual pair of time expressions and/or events, the system predicts whether they are temporally related and which specific relation holds between them. An alternative approach is to model the temporal relations in a text as a temporal dependency structure (TDS) for the entire text (Kolomiyets et al., 2012). Such a temporal dependency structure has the advantage that (1) it can be easily used to infer additional temporal relations between time expressions and/or events that are not directly connected via the transitivity properties of temporal relations, (2) it is computationall"
D18-1371,P18-1122,0,0.406832,"both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area. 1 Introduction Temporal relation classification is important for a range of NLP applications that include but are not limited to story timeline construction, question answering, summarization, etc. Most work on temporal information extraction models the task as a pair-wise classification problem (Bethard et al., 2007; Chambers et al., 2007; Chambers and Jurafsky, 2008; Ning et al., 2018a): given an individual pair of time expressions and/or events, the system predicts whether they are temporally related and which specific relation holds between them. An alternative approach is to model the temporal relations in a text as a temporal dependency structure (TDS) for the entire text (Kolomiyets et al., 2012). Such a temporal dependency structure has the advantage that (1) it can be easily used to infer additional temporal relations between time expressions and/or events that are not directly connected via the transitivity properties of temporal relations, (2) it is computationall"
D18-1371,W16-5706,0,0.0542526,"Missing"
D18-1371,P07-2044,0,0.0383201,"recognized, our parser beats two strong baselines on both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area. 1 Introduction Temporal relation classification is important for a range of NLP applications that include but are not limited to story timeline construction, question answering, summarization, etc. Most work on temporal information extraction models the task as a pair-wise classification problem (Bethard et al., 2007; Chambers et al., 2007; Chambers and Jurafsky, 2008; Ning et al., 2018a): given an individual pair of time expressions and/or events, the system predicts whether they are temporally related and which specific relation holds between them. An alternative approach is to model the temporal relations in a text as a temporal dependency structure (TDS) for the entire text (Kolomiyets et al., 2012). Such a temporal dependency structure has the advantage that (1) it can be easily used to infer additional temporal relations between time expressions and/or events that are not directly connected via the transitivity properties"
D18-1371,E17-2118,0,0.0407191,"perties of temporal relations, (2) it is computationally more efficient because a model does not need to consider all pairs of time expressions and events in a text, and (3) it is easier to use for downstream applications such as timeline construction. However, most existing automatic systems are pair-wise models trained with traditional statistical classifiers using a large number of manually crafted features (Bethard et al., 2017). The few exceptions include the work of Kolomiyets et al. (2012), which describes a temporal dependency parser based on traditional feature-based classifiers, and Dligach et al. (2017), which describes a system using neural network based models to classify individual temporal relations. More recently, a semi-structured approach has also been proposed (Ning et al., 2018b). In this work, taking advantage of a newly available data set annotated with temporal dependency structures – the Temporal Dependency Tree (TDT) Corpus1 (Zhang and Xue, 2018), we develop a neural temporal dependency structure parser using minimal hand-crafted linguistic features. One of the advantages of neural network based models is that they are easily adaptable to new domains, and we demonstrate this ad"
D18-1371,Q16-1023,0,0.0311943,"vents in a text form multiple disconnected subgraphs. Like other work before them, their annotation scheme only covers events, to the exclusion of time expressions. 7.2 Related Work on Neural Dependency Parsing Most prior work on neural dependency parsing is aimed at syntactic dependency parsing, i.e. parsing a sentence into a dependency tree that represents the syntactic relations among the words. Recent work on dependency parsing typically uses transition-based or graph-based architectures combined with contextual vector representations learned with recurrent neural networks (e.g. BiLSTMs) (Kiperwasser and Goldberg, 2016). Temporal dependency parsing is, however, different from syntactic dependency parsing. In temporal dependency parsing, for each event or time expression, there is more than one other event or time expression that can serve as its reference time, while the most closely related one is selected as the gold standard reference time parent. This naturally falls into a ranking process where all possible reference times are ranked and the best is selected. In this sense our neural ranking model for temporal dependency parsing is closely related to the neural ranking model for coreference resolution d"
D18-1371,P12-1010,0,0.598253,"ions that include but are not limited to story timeline construction, question answering, summarization, etc. Most work on temporal information extraction models the task as a pair-wise classification problem (Bethard et al., 2007; Chambers et al., 2007; Chambers and Jurafsky, 2008; Ning et al., 2018a): given an individual pair of time expressions and/or events, the system predicts whether they are temporally related and which specific relation holds between them. An alternative approach is to model the temporal relations in a text as a temporal dependency structure (TDS) for the entire text (Kolomiyets et al., 2012). Such a temporal dependency structure has the advantage that (1) it can be easily used to infer additional temporal relations between time expressions and/or events that are not directly connected via the transitivity properties of temporal relations, (2) it is computationally more efficient because a model does not need to consider all pairs of time expressions and events in a text, and (3) it is easier to use for downstream applications such as timeline construction. However, most existing automatic systems are pair-wise models trained with traditional statistical classifiers using a large"
D18-1371,P14-1069,1,0.835231,". The code is publicly available5 . For Stage 1, all models are trained with Adam optimizer with early stopping and learning rate 0.001. The dimensions of word embeddings, POS tag embeddings, Bi-LSTM output vectors, and MLP hidden layers are tuned on the dev set to 256, 5 https://github.com/yuchenz/tdp_ ranking news r f .74 .78 .81 .82 .73 .77 Table 3: Stage 1 cross-validation on span detection and binary time/event recognition. Table 2: Features in the logistic regression system. 5.3 p .81 .83 .81 32, 256, and 256 respectively. POS tags in Stage 1 are acquired using the joint POS tagger from Wang and Xue (2014). The tagger is trained on Chinese Treebank 7.0 (Xue et al., 2010). For Stage 2, the dimensions of word embeddings, time/event type embeddings, Bi-LSTM output vectors, and MLP hidden layers are tuned on the dev set to 32, 16, 32, and 32 respectively. The optimizer is Adam with early stopping and learning rate 0.001. 5.3.1 End-to-End System Evaluation Stage 1: Time and Event Recognition For Stage 1 in the pipeline, we perform BIO tagging with the full set of time expression and event types (i.e. a 11-way classification on all extracted spans). Extracted spans will be nodes in the final dependen"
D18-1371,L18-1490,1,0.367388,"sing a large number of manually crafted features (Bethard et al., 2017). The few exceptions include the work of Kolomiyets et al. (2012), which describes a temporal dependency parser based on traditional feature-based classifiers, and Dligach et al. (2017), which describes a system using neural network based models to classify individual temporal relations. More recently, a semi-structured approach has also been proposed (Ning et al., 2018b). In this work, taking advantage of a newly available data set annotated with temporal dependency structures – the Temporal Dependency Tree (TDT) Corpus1 (Zhang and Xue, 2018), we develop a neural temporal dependency structure parser using minimal hand-crafted linguistic features. One of the advantages of neural network based models is that they are easily adaptable to new domains, and we demonstrate this advantage by evaluating our temporal dependency parser on data from two domains: news reports and narrative stories. Our results show that our model beats a strong logistic regression baseline. Direct comparison with existing models is impossible because the only similar dataset used in previous work that we are aware of is not available to us (Kolomiyets et al.,"
E14-1068,P13-2013,0,0.547136,"erted that exact questions weren’t replicated. When referred to the questions that match, she said it was coincidental. When we switch out the coreferential subject for an arbitrary uncoreferential pronoun as we do in (3), we are more inclined to classify the relation as C ONTINGENCY. 7 Conclusions Related work Word-pair features are known to work very well in predicting senses of discourse relations in an artificially generated corpus (Marcu and Echihabi, 2002). But when used with a realistic corpus, model parameter estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based on specific theories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest basel"
E14-1068,J92-4003,0,0.261212,"eis.edu Attapol T. Rutherford Department of Computer Science Brandeis University Waltham, MA 02453, USA tet@brandeis.edu Abstract Existing systems, which make heavy use of word pairs, suffer from data sparsity problem as a word pair in the training data may not appear in the test data. A better representation of two adjacent sentences beyond word pairs could have a significant impact on predicting the sense of the discourse relation that holds between them. Data-driven theory-independent word classification such as Brown clustering should be able to provide a more compact word representation (Brown et al., 1992). Brown clustering algorithm induces a hierarchy of words in a large unannotated corpus based on word co-occurrences within the window. The induced hierarchy might give rise to features that we would otherwise miss. In this paper, we propose to use the cartesian product of Brown cluster assignment of the sentence pair as an alternative abstract word representation for building an implicit discourse relation classifier. Through word-level semantic commonalities revealed by Brown clusters and entity-level relations revealed by coreference resolution, we might be able to paint a more complete pic"
E14-1068,N10-1043,0,0.0277323,"e two types of relations have distinctive coreference patterns. 651 word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010). Louis et al. (2010) uses multiple aspects of coreference as features to classify implicit discourse relations without much success while suggesting many aspects that are worth exploring. In a corpus study by Louis and Nenkova (2010), coreferential rates alone cannot explain all of the relations, and more complex coreference patterns have to be considered. It is possible that coreferential subject patterns suggest temporal coherence between the two sentences without using an explicit discourse connective. C ONTINGENCY relations, which can only indicate causal relationships when realized implicitly, impose the temporal ordering of events in the arguments; i.e. if Arg1 is causally related to Arg2, then the event described in Arg1 must temporally precede the one in Arg2. Therefore, C ONTIN GENCY and T EMPORAL can be highly c"
E14-1068,W10-4310,0,0.546857,"Missing"
E14-1068,de-marneffe-etal-2006-generating,0,0.053339,"Missing"
E14-1068,P05-1045,0,0.0105041,"ments have the same subjects or not and another binary feature indicating whether the main verbs are similar or not. For our purposes, the two subjects are said to be the same if they are coreferential or assigned to the same Brown cluster. We notice that C OMPARISON relations usually have different subjects for the same main verbs and that T EMPORAL relations usually have the same subjects but different main verbs. Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word c"
E14-1068,P02-1047,0,0.784763,"coreferential rate. The coreference patterns differ substantially and meaningfully across discourse relations and deserve further exploration. (3) He also asserted that exact questions weren’t replicated. When referred to the questions that match, she said it was coincidental. When we switch out the coreferential subject for an arbitrary uncoreferential pronoun as we do in (3), we are more inclined to classify the relation as C ONTINGENCY. 7 Conclusions Related work Word-pair features are known to work very well in predicting senses of discourse relations in an artificially generated corpus (Marcu and Echihabi, 2002). But when used with a realistic corpus, model parameter estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based"
E14-1068,J93-2004,0,0.0467319,"Missing"
E14-1068,P03-1054,0,0.00914601,"ature indicating whether the main verbs of the two arguments have the same subjects or not and another binary feature indicating whether the main verbs are similar or not. For our purposes, the two subjects are said to be the same if they are coreferential or assigned to the same Brown cluster. We notice that C OMPARISON relations usually have different subjects for the same main verbs and that T EMPORAL relations usually have the same subjects but different main verbs. Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard"
E14-1068,W12-1614,0,0.591843,"ure set size is orders of magnitude smaller than using the actual words, which can generate O(V 2 ) distinct binary features where V is the size of the vocabulary. 4.3 4.4 Feature selection and training sample reweighting The nature of the task and the dataset poses at least two problems in creating a classifier. First, the classification task requires a large number of features, some of which are too rare and inconducive to parameter estimation. Second, the label distribution is highly imbalanced (Table 1) and this might degrade the performance of the classifiers (Japkowicz, 2000). Recently, Park and Cardie (2012) and Wang et al. (2012) addressed these problems directly by optimally select a subset of features and training samples. Unlike previous work, we do not discard any of data in the training set to balance the label distribution. Instead, we reweight the training samples in each class during parameter estimation such that the performance on the development set is maximized. In addition, the Coreference-based features We want to take advantage of the semantics of the sentence pairs even more by considering how coreferential entities play out in the sentence pairs. We consider various inter-senten"
E14-1068,W11-1902,0,0.139615,"whether the main verbs are similar or not. For our purposes, the two subjects are said to be the same if they are coreferential or assigned to the same Brown cluster. We notice that C OMPARISON relations usually have different subjects for the same main verbs and that T EMPORAL relations usually have the same subjects but different main verbs. Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 cluster"
E14-1068,D13-1094,0,0.0671583,"he coreference chains used in this study are detected automatically from the training set through Stanford CoreNLP suite (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). T EM PORAL relations have a significantly higher coreferential rate than the other three relations (p &lt; 0.05, pair-wise t-test corrected for multiple comparisons). The differences between C OMPARI SON , C ONTINGENCY , and E XPANSION , however, are not statistically significant (Figure 2). The choice to use or not to use a discourse connective is strongly motivated by linguistic features at the discourse levels (Patterson and Kehler, 2013). Additionally, it is very uncommon to have temporally-related sentences without using explicit discourse connectives. The difference in coreference patterns might be one of the factors that influence the choice of using a discourse connective to signal a T EMPORAL relation. If sentences are coreferentially linked, then it might be more natural to drop a discourse connective because the temporal ordering can be easily inferred without it. For example, Subject coreference Coreferential rate 0.5 0.4 0.3 0.2 0.1 0.0 y y n n n al al nc nc or or sio sio iso ge ge ar an an mp mp n p i p p e e t T T"
E14-1068,J13-4004,0,0.0931987,"verbs are similar or not. For our purposes, the two subjects are said to be the same if they are coreferential or assigned to the same Brown cluster. We notice that C OMPARISON relations usually have different subjects for the same main verbs and that T EMPORAL relations usually have the same subjects but different main verbs. Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 clusters were induced from"
E14-1068,P09-1077,0,0.768943,"and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 clusters were induced from RCV1 corpus, which contains about 63 million tokens from Reuters English newswire. Then we take the Cartesian product of the Brown cluster assignments of the words in Arg1 and the ones of the words in Arg2. For example, suppose Arg1 has two words w1,1 , w1,2 , Arg2 has three words w2,1 , w2,2 , w2,3 , and then B(.) maps a word to its Brow"
E14-1068,prasad-etal-2008-penn,0,0.951328,"Missing"
E14-1068,D10-1048,0,0.0749113,"binary feature indicating whether the main verbs are similar or not. For our purposes, the two subjects are said to be the same if they are coreferential or assigned to the same Brown cluster. We notice that C OMPARISON relations usually have different subjects for the same main verbs and that T EMPORAL relations usually have the same subjects but different main verbs. Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 20"
E14-1068,D11-1141,0,0.0424132,"Missing"
E14-1068,W00-1308,0,0.0383251,"Missing"
E14-1068,N03-1033,0,0.00936942,"Missing"
E14-1068,P10-1040,0,0.00540584,"than et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 clusters were induced from RCV1 corpus, which contains about 63 million tokens from Reuters English newswire. Then we take the Cartesian product of the Brown cluster assignments of the words in Arg1 and the ones of the words in Arg2. For example, suppose Arg1 has two words w1,1 , w1,2 , Arg2 has three words w2,1 , w2,2 , w2,3 , and then B(.) maps a word to its Brown cluster assignment. A word wij is replaced by its corresponding Brown cluster assignment bij = B(wij ). The resulting word pair features are (b1,1 , b2,1 ), (b1,1 , b2,2 ), (b1,1 , b2,3 ), (b1,2 , b2,1 ), (b1,2 , b2,2 ), an"
E14-1068,C12-1168,0,0.476089,"Missing"
E14-1068,miltsakaki-etal-2004-penn,0,\N,Missing
E14-1068,C08-2022,0,\N,Missing
E17-1027,P14-1129,0,0.0999089,"Missing"
E17-1027,D14-1080,0,0.0812409,"Missing"
E17-1027,Q15-1024,0,0.126333,"rs: The principle of compositionality leads us to believe that the semantics of the argument vector should be determined by the syntactic structures and the meanings of the constituents. For a fair comparison with the sequential model, we apply the same formulation of LSTM on the binarized constituent parse tree. The hidden state vector now corresponds to a constituent in the tree. These hidden state vectors are then used in the same fashion as the sequential LSTM. The mathematical formulation is the same as Tai et al. (2015). This model is similar to the recursive neural networks proposed by Ji and Eisenstein (2015). Our model differs from their model in several ways. We use the LSTM networks instead of the “vanilla” RNN formula and expect better results due to less complication with vanishing and exploding gradients during training. Furthermore, our purpose is to compare the influence of the model structures. Therefore, we must use LSTM cells in both sequential and tree LSTM models for a fair and meaningful comparison. The more indepth comparison of our work and recursive neural network model by Ji and Eisenstein (2015) is provided in the discussion section. h1 = tanh(W1 · a1 + W2 · a2 + bh1 ) where W1"
E17-1027,N16-1037,0,0.169768,"they have not been much explored in the realm of discourse parsing. LSTM models have been notably used to encode the meaning of source language sentence in neural machine translation (Cho et al., 2014; Devlin et al., 2014) and recently used to encode the meaning of an entire sentence to be used as features (Kiros et al., 2015). Many neural architectures have been explored and evaluated, but there is no single technique that is decidedly better across all tasks. The LSTM-based models such as Kiros et al. (2015) perform well across tasks but do not outperform some other strong neural baselines. Ji et al. (2016) uses a joint discourse language model to improve the performance on the coarse-grained label in the PDTB, but in our case, we would like to deduce how well LSTM fares in fine-grained implicit discourse relation classification, which is more practical for application. sets, and our findings are valid across languages. Our Chinese model outperforms all of the feature sets known to work well in English despite using only word vectors. The choice of neural architecture used for inducing Chinese word vectors turns out to be crucial. Chinese word vectors from Skipgram model perform consistently bet"
E17-1027,D12-1050,0,0.0282312,"v, Model Architectures Following previous work, we assume that the two arguments of an implicit discourse relation are given so that we can focus on predicting the senses of the implicit discourse relations. The input to our model is a pair of text segments called Arg1 and Arg2, and the label is one of the senses defined in the Penn 282 at word position t, we compute the corresponding hidden state vector st and the memory cell vector from the previous step, using standard formula for LSTM. The argument vectors are the results of applying a pooling function over the hidden state vectors. 2014; Blacoe and Lapata, 2012; Mikolov et al., 2013b; Braud and Denis, 2015). The Arg1 vector a1 and Arg2 vector a2 are computed by applying element-wise pooling function f on all of the N1 1 word vectors in Arg1 w1:N and all of the N2 word 1 2 vectors in Arg2 w1:N2 respectively: a1i = f (s11:N1 ,i ) 1 a1i = f (w1:N ) 1 ,i a2i = f (s21:N2 ,i ) 2 a2i = f (w1:N ) 2 ,i In addition to the three pooling functions that we describe in the previous subsection, we also consider using only the last hidden state vector, which should theoretically be able to encode the semantics of the entire word sequence. We consider three differen"
E17-1027,D15-1262,0,0.229217,"we assume that the two arguments of an implicit discourse relation are given so that we can focus on predicting the senses of the implicit discourse relations. The input to our model is a pair of text segments called Arg1 and Arg2, and the label is one of the senses defined in the Penn 282 at word position t, we compute the corresponding hidden state vector st and the memory cell vector from the previous step, using standard formula for LSTM. The argument vectors are the results of applying a pooling function over the hidden state vectors. 2014; Blacoe and Lapata, 2012; Mikolov et al., 2013b; Braud and Denis, 2015). The Arg1 vector a1 and Arg2 vector a2 are computed by applying element-wise pooling function f on all of the N1 1 word vectors in Arg1 w1:N and all of the N2 word 1 2 vectors in Arg2 w1:N2 respectively: a1i = f (s11:N1 ,i ) 1 a1i = f (w1:N ) 1 ,i a2i = f (s21:N2 ,i ) 2 a2i = f (w1:N ) 2 ,i In addition to the three pooling functions that we describe in the previous subsection, we also consider using only the last hidden state vector, which should theoretically be able to encode the semantics of the entire word sequence. We consider three different pooling functions namely max, summation, and"
E17-1027,D15-1278,0,0.0227129,"not be complete without explaining our results in relation to the recursive neural network model proposed by Ji and Eisenstein (2015). Why do sequential LSTM models outperform recursive neural networks or tree LSTM models? Although this first comes as a surprise to us, the results are consistent with recent works that use sequential LSTM to encode syntactic information. For example, Vinyals et al. (2015) use sequential LSTM to encode the features for syntactic parse output. Tree LSTM seems to show improvement when there is a need to model longdistance dependency in the data (Tai et al., 2015; Li et al., 2015). Furthermore, the benefits of tree LSTM are not readily apparent for a model that discards the syntactic categories in the intermediate nodes and makes no distinction between heads and their dependents, which are at the core of syntactic representations. Another point of contrast between our work and Ji and Eisenstein’s (2015) is the modeling choice for inter-argument interaction. Our experimental results show that the hidden layers are an important contributor to the performance for all of our models. We choose linear inter-argument interaction instead of bilinear interaction, and this decis"
E17-1027,D09-1036,0,0.676043,"Missing"
E17-1027,P09-1077,0,0.759739,"discourse relations has proved to be notoriously hard and remained one of the last missing pieces in an end-to-end discourse parser (Xue et al., 2015). In the absence of explicit discourse connectives, implicit discourse relations have to be inferred from their two arguments. Previous approaches on inferring implicit discourse relations have typically relied on features extracted from their two arguments. These features include the Cartesian products of the word tokens in the two arguments as well as features manually crafted from various lexicons such as verb classes and sentiment lexicons (Pitler et al., 2009; Rutherford and Xue, 2014). These lexicons are used mainly to offset the data sparsity problem created by pairs of word tokens used directly as features. Neural network models are an attractive alternative for this task, but it is not clear how well they will fare with a small dataset, typically found in discourse annotation projects. Many neural approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four"
E17-1027,prasad-etal-2008-penn,0,0.640866,"igured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Further, we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective. For the first time for this task, we compile and publish outputs from previous neural and nonneural systems to establish the standard for further comparison. 1 Introduction The discourse structure of a natural language text has been analyzed and conceptualized under various frameworks (Mann and Thompson, 1988; Lascarides and Asher, 2007; Prasad et al., 2008). The Penn Discourse TreeBank (PDTB) and the Chinese Discourse Treebank (CDTB), currently the largest corpora annotated with discourse structures in English and Chinese respectively, view the discourse structure of a text as a set of discourse relations (Prasad et al., 2008; Zhou and Xue, 2012). Each discourse relation (e.g. causal or temporal) is grounded by a discourse connective (e.g. because or meanwhile) taking two text segments as argu281 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 281–291, c Va"
E17-1027,P02-1047,0,0.215223,"ME + Word pairs ME + All feature sets Our best feedforward variant 77.14 80.81 82.34 82.36 82.48 82.98 83.13 84.16 85.45 Table 5: Our best feedforward variant significantly outperforms the systems with surface features (p < 0.05). ME=Maximum Entropy model Number of Hidden Layers CBOW 0 1 2 Skipgram Accuracy 0.84 0.82 0.80 0.78 100 200 300 100 200 300 Dimensionality of word vectors Figure 5: Comparing the accuracies across Chinese word vectors for feedforward model. dependency rule pairs, production rule pairs (Lin et al., 2009), Brown cluster pairs (Rutherford and Xue, 2014), and word pairs (Marcu and Echihabi, 2002). We use information gain criteria to select the best subset of each feature set, which is crucial in feature-based discourse parsing. Chinese word vectors are induced through CBOW and Skipgram architecture in word2vec (Mikolov et al., 2013a) on Chinese Gigaword corpus (Graff and Chen, 2005) using default settings. The number of dimensions that we try are 50, 100, 150, 200, 250, and 300. We induce 1,000 and 3,000 Brown clusters on the Gigaword corpus. Table 5 shows the results for the models which are best tuned on the number of hidden units, hidden layers, and the types of word vectors. The f"
E17-1027,K16-2010,0,0.0283883,"tting used in CoNLL 2015-2016 Shared Task. To compare our results against previous systems, we compile all of the official system outputs, and make them publicly available. The label set is modified by the shared task organizers into 15 different senses including EntRel as another sense (Xue et al., 2015; Xue et al., 2016). We use the 300-dimensional word vector used in the previous experiment and tune the number of hidden layers and hidden units on the development set. We consider the following models: Bidirectional-LSTM (Akanksha and Eisenstein, 2016), two flavors of convolutional networks (Qin et al., 2016; Wang and Lan, 2016), two variations of simple argument pooling (Mihaylov and Frank, 2016; Schenk et al., 2016), and the best system using surface features alone (Wang and Lan, 2015). The comparison results and brief system descriptions are shown in Table 4. Our model presents the state-of-the-art system on the blind test set in English. We once again confirm that manual features are not necessary for this task and that our feedforward network outperforms the best available LSTM and convolutional networks in many settings despite its simplicity. While performing well in-domain, convolutional"
E17-1027,J93-2004,0,0.0601867,"Missing"
E17-1027,E14-1068,1,0.860517,"has proved to be notoriously hard and remained one of the last missing pieces in an end-to-end discourse parser (Xue et al., 2015). In the absence of explicit discourse connectives, implicit discourse relations have to be inferred from their two arguments. Previous approaches on inferring implicit discourse relations have typically relied on features extracted from their two arguments. These features include the Cartesian products of the word tokens in the two arguments as well as features manually crafted from various lexicons such as verb classes and sentiment lexicons (Pitler et al., 2009; Rutherford and Xue, 2014). These lexicons are used mainly to offset the data sparsity problem created by pairs of word tokens used directly as features. Neural network models are an attractive alternative for this task, but it is not clear how well they will fare with a small dataset, typically found in discourse annotation projects. Many neural approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four binary classification (Pitl"
E17-1027,K16-2014,0,0.0388664,"systems, we compile all of the official system outputs, and make them publicly available. The label set is modified by the shared task organizers into 15 different senses including EntRel as another sense (Xue et al., 2015; Xue et al., 2016). We use the 300-dimensional word vector used in the previous experiment and tune the number of hidden layers and hidden units on the development set. We consider the following models: Bidirectional-LSTM (Akanksha and Eisenstein, 2016), two flavors of convolutional networks (Qin et al., 2016; Wang and Lan, 2016), two variations of simple argument pooling (Mihaylov and Frank, 2016; Schenk et al., 2016), and the best system using surface features alone (Wang and Lan, 2015). The comparison results and brief system descriptions are shown in Table 4. Our model presents the state-of-the-art system on the blind test set in English. We once again confirm that manual features are not necessary for this task and that our feedforward network outperforms the best available LSTM and convolutional networks in many settings despite its simplicity. While performing well in-domain, convolutional networks degrade sharply when tested on the blind slightly out-of-domain dataset. 5.2 Acc."
E17-1027,N15-1081,1,0.826695,"pairs of word tokens used directly as features. Neural network models are an attractive alternative for this task, but it is not clear how well they will fare with a small dataset, typically found in discourse annotation projects. Many neural approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four binary classification (Pitler et al., 2008; Rutherford and Xue, 2014) , 4-way coarse sense classification (Rutherford and Xue, 2015) , and intermediate sense classification (Lin et al., 2009). CoNLL Shared Task introduces a unified scheme for evaluation along with a new unseen test set in English in 2015 (Xue et al., 2015) and in Chinese in 2016 (Xue et al., 2016). We want to corroboInferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this task is not unified, so we could hardly draw clear conclusions about the effectiveness of various architectures. Here, we propo"
E17-1027,K16-2005,0,0.249378,"Missing"
E17-1027,W12-1614,0,0.617306,"roach for this task is to use surface features derived from various semantic lexicons (Pitler et al., 2009), reducing the number of parameters by mapping raw word tokens in the arguments of discourse relations to a limited number of entries in a semantic lexicon such as polarity and verb classes. Along the same vein, Brown cluster assignments have also been used as a general purpose lexicon that requires no human manual annotation (Rutherford and Xue, 2014). However, these solutions still suffer from the data sparsity problem and almost always require extensive feature selection to work well (Park and Cardie, 2012; Lin et al., 2009; Ji and Eisenstein, 2015). The work we report here explores the use of the expressive power of distributed representations to overcome the data sparsity problem found in the traditional feature engineering paradigm. Neural network modeling has been explored to some extent in the context of this task. Recently, Braud and Denis (2015) tested various word vectors as features for implicit discourse relation classification and show that distributed features achieve the same level of accuracy as onehot representations in some experimental settings. Ji et al. (2015; 2016) advance t"
E17-1027,D14-1162,0,0.0816999,"Missing"
E17-1027,P15-1150,0,0.149259,"Missing"
E17-1027,P06-1055,0,0.0605873,"Missing"
E17-1027,K15-2002,0,0.0411252,"Missing"
E17-1027,K16-2004,0,0.0666199,"L 2015-2016 Shared Task. To compare our results against previous systems, we compile all of the official system outputs, and make them publicly available. The label set is modified by the shared task organizers into 15 different senses including EntRel as another sense (Xue et al., 2015; Xue et al., 2016). We use the 300-dimensional word vector used in the previous experiment and tune the number of hidden layers and hidden units on the development set. We consider the following models: Bidirectional-LSTM (Akanksha and Eisenstein, 2016), two flavors of convolutional networks (Qin et al., 2016; Wang and Lan, 2016), two variations of simple argument pooling (Mihaylov and Frank, 2016; Schenk et al., 2016), and the best system using surface features alone (Wang and Lan, 2015). The comparison results and brief system descriptions are shown in Table 4. Our model presents the state-of-the-art system on the blind test set in English. We once again confirm that manual features are not necessary for this task and that our feedforward network outperforms the best available LSTM and convolutional networks in many settings despite its simplicity. While performing well in-domain, convolutional networks degrade shar"
E17-1027,K15-2001,1,0.909064,"niversity Waltham, MA, USA teruth@yelp.com vera@coli.uni-saarland.de xuen@brandeis.edu Abstract ments (Prasad et al., 2008). Implicit discourse relations are those where discourse connectives are omitted from the text and yet the discourse relations still hold. While classifying explicit discourse relations is relatively easy, as the discourse connective itself provides a strong cue for the discourse relation (Pitler et al., 2008), the classification of implicit discourse relations has proved to be notoriously hard and remained one of the last missing pieces in an end-to-end discourse parser (Xue et al., 2015). In the absence of explicit discourse connectives, implicit discourse relations have to be inferred from their two arguments. Previous approaches on inferring implicit discourse relations have typically relied on features extracted from their two arguments. These features include the Cartesian products of the word tokens in the two arguments as well as features manually crafted from various lexicons such as verb classes and sentiment lexicons (Pitler et al., 2009; Rutherford and Xue, 2014). These lexicons are used mainly to offset the data sparsity problem created by pairs of word tokens used"
E17-1027,K16-2001,1,0.924705,"approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four binary classification (Pitler et al., 2008; Rutherford and Xue, 2014) , 4-way coarse sense classification (Rutherford and Xue, 2015) , and intermediate sense classification (Lin et al., 2009). CoNLL Shared Task introduces a unified scheme for evaluation along with a new unseen test set in English in 2015 (Xue et al., 2015) and in Chinese in 2016 (Xue et al., 2016). We want to corroboInferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this task is not unified, so we could hardly draw clear conclusions about the effectiveness of various architectures. Here, we propose neural network models that are based on feedforward and long-short term memory architecture and systematically study the effects of varying structures. To our surprise, the best-configured feedforward architecture outperforms LSTM-"
E17-1027,P12-1008,1,0.861782,"compile and publish outputs from previous neural and nonneural systems to establish the standard for further comparison. 1 Introduction The discourse structure of a natural language text has been analyzed and conceptualized under various frameworks (Mann and Thompson, 1988; Lascarides and Asher, 2007; Prasad et al., 2008). The Penn Discourse TreeBank (PDTB) and the Chinese Discourse Treebank (CDTB), currently the largest corpora annotated with discourse structures in English and Chinese respectively, view the discourse structure of a text as a set of discourse relations (Prasad et al., 2008; Zhou and Xue, 2012). Each discourse relation (e.g. causal or temporal) is grounded by a discourse connective (e.g. because or meanwhile) taking two text segments as argu281 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 281–291, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Output layer rate this new evaluation scheme by running more benchmark results and providing the output under this evaluation scheme. We systematically compare the relative advantages of different neural architecture"
E17-1035,D15-1198,0,0.418719,", they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources. 1 ARG0 ARG1 person ARG2 name name op1 genius “Ryan” Figure 1: An example of AMR graph representing the meaning of: “Ryan’s description of himself: a genius.” Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015). Most of these parsers have used external resources such as dependency parses, semantic lexicons, etc., to tackle the sparsity issue. Recently, Sutskever et al. (2014) introduced a neural network model for solving the general sequence-to-sequence problem, and Bahdanau et al. (2014) proposed a related model with an attention mechanism that is capable of handling long sequences. Both models achieve state-of-the-art results on large scale machine translation tasks. However, sequence-to-sequence models mostly work well for large scale parallel data, usually"
E17-1035,W13-2322,0,0.101641,"usually involving millions of sentence pairs. Vinyals et al. (2015) present a method which linearizes parse trees into a sequence structure and therefore a sequence-to-sequence model can be applied to the constituent parsing task. Competitive results have been achieved with an attention model on the Penn Treebank dataset, with only 40K annotated sentences. AMR parsing is a much harder task in that the target vocabulary size is much larger, while the size of the dataset is much smaller. While for constituent parsing we only need to predict nonIntroduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in app"
E17-1035,S16-1176,0,0.542997,"4; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. Barzdins and Gosko (2016) applied a similar approach where AMR graphs are linearized using depth-first search and both concepts and relations are treated as tokens (see Figure 3). Due to the data sparsity issue, their AMR parsing results are significantly lower than state-of-the-art models when using the neural attention model. In this paper, we present a method which linearizes AMR graphs in a way that captures the interaction of concepts and relations. To overcome the data sparsity issue for the target vocabulary, we propose a categorization strategy which first maps low frequency concepts and entity subgraphs to a"
E17-1035,P06-1055,0,0.0217691,"ere the original graph structure can be reconstructed with the linearization result. Even though we call it a sequence, its core idea is actually generating a graph structure from top-down. In practice, they found that using the attention model is more data efficient and works well on the parsing task. They also reversed the input sentence and normalized the part-of-speech tags. After decoding, the output parse tree is recovered from the output sequence of the decoder in a postprocessing procedure. Overall, the sequence-tosequence model is able to match the performance of the Berkeley Parser (Petrov et al., 2006). 3 AMR Linearization Barzdins and Gosko (2016) present a similar linearization procedure where the depth-first traversal result of an AMR graph is used as the AMR sequence (see Figure 3). The bracketing structure of AMR is hard to maintain because the prediction of relation (with left parenthesis) and the prediction of an isolated right parenthesis are not correlated. As a result, the output AMR sequences usually have parentheses that do not match. We present a linearization strategy which captures the bracketing structure of AMR and the connection between relations and concepts. Figure 3 sho"
E17-1035,P13-2131,0,0.0620234,"e input sequence and AMR concepts/relations. Second, as argued by Liu et al. (2016), the sequenceto-sequence model tries to calculate the attention vector and predict the current output label simultaneously. This makes it impossible for the learned 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We first preprocess the input sentences with tokenization and lemmatization. Then we extract 370 the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009). For training all the neural AMR parsing systems, we use 256 for both hidden layer size and word embedding size. Stochastic gradient descent is used to optimize the cross-entropy loss function and we set the drop out rate to be 0.5. We train our model for 150 epochs with initial learning rate of 0.5 and learning rate decay factor 0.95 if the model doesn’t improve for the 3 last epochs. 5.2 6"
E17-1035,D14-1048,0,0.40301,"Missing"
E17-1035,N16-1101,0,0.0164067,")? is the character-level neural AMR parser. F 0.52 0.43 Table 5 gives the comparison of our system to some of the teams participating in SemEval16 Task 8. Since a large portion of the teams extend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches. We can see from the table that our system fails to outperform the stateTable 3: Supervised attention impact on development set Because we have relations in the AMR graph, the alignment problem here is different from the 372 parameters. Firat et al. (2016) builds a multilingual neural system where the attention mechanism can be shared between different language pairs. Our work could be seen as parallel efforts to handle the sparsity problem since we focus on the input/output categorization and external alignment, which are both handy for low-resource languages. In this paper, we haven’t used any syntactic parser. However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity"
E17-1035,D15-1136,0,0.64323,"illed their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources. 1 ARG0 ARG1 person ARG2 name name op1 genius “Ryan” Figure 1: An example of AMR graph representing the meaning of: “Ryan’s description of himself: a genius.” Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015). Most of these parsers have used external resources such as dependency parses, semantic lexicons, etc., to tackle the sparsity issue. Recently, Sutskever et al. (2014) introduced a neural network model for solving the general sequence-to-sequence problem, and Bahdanau et al. (2014) proposed a related model with an attention mechanism that is capable of handling long sequences. Both models achieve state-of-the-art results on large scale machine translation tasks. However, sequence-to-sequence models mostly work well for large scale parallel data, usually involving millions"
E17-1035,P14-1134,0,0.742379,"notations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. B"
E17-1035,W09-1119,0,0.039536,"sly. This makes it impossible for the learned 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We first preprocess the input sentences with tokenization and lemmatization. Then we extract 370 the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009). For training all the neural AMR parsing systems, we use 256 for both hidden layer size and word embedding size. Stochastic gradient descent is used to optimize the cross-entropy loss function and we set the drop out rate to be 0.5. We train our model for 150 epochs with initial learning rate of 0.5 and learning rate decay factor 0.95 if the model doesn’t improve for the 3 last epochs. 5.2 60 55 50 Smatch 45 35 30 25 20 Baseline Model We first inspect the influence of utilizing categorization on the input and output sequence. Table 1 shows the Smatch evaluation score on development set. P 0.4"
E17-1035,P82-1020,0,0.847308,"Missing"
E17-1035,S16-1166,0,0.128355,"et al., 2014; Vinyals et al., 2015). However, we only have 16k sentences in the AMR training data and our output vocabulary size is quite large, which makes it hard for the model to learn a useful alignment between the input sequence and AMR concepts/relations. Second, as argued by Liu et al. (2016), the sequenceto-sequence model tries to calculate the attention vector and predict the current output label simultaneously. This makes it impossible for the learned 5 Experiments We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016). The dataset contains 16,833 training, 1,368 development and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc. All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013). 5.1 Experiment Settings We first preprocess the input sentences with tokenization and lemmatization. Then we extract 370 the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009). For training all the neural AMR parsing systems, we use 256 for both hidden layer size and word embedding size. Stochastic gradient descent is used to optimize the"
E17-1035,P15-2141,1,0.951473,"cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. Barzdins and Gosko ("
E17-1035,J05-1004,1,0.206863,"e dataset is much smaller. While for constituent parsing we only need to predict nonIntroduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Assoc"
E17-1035,S16-1183,1,0.893498,"Missing"
E17-1035,N15-1040,1,0.873586,"cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR. These properties render the AMR representation useful in applications like question answering and semanticsbased machine translation. The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Recently, a sizable new corpus of English/AMR pairs (LDC2015E86) has been released. Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b; *Both authors contribute equally. 366 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366–375, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics two RNNs: a forward RNN and a backward RNN. The forward RNN can be seen as a recurrent function defined as follows: terminal labels and the output vocabulary is limited to 128 symbols, AMR parsing has both concepts and relation labels, and the target vocabulary size consists of tens of thousands of symbols. Barzdins and Gosko ("
E17-1035,K15-1004,1,0.929731,"on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources. 1 ARG0 ARG1 person ARG2 name name op1 genius “Ryan” Figure 1: An example of AMR graph representing the meaning of: “Ryan’s description of himself: a genius.” Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015). Most of these parsers have used external resources such as dependency parses, semantic lexicons, etc., to tackle the sparsity issue. Recently, Sutskever et al. (2014) introduced a neural network model for solving the general sequence-to-sequence problem, and Bahdanau et al. (2014) proposed a related model with an attention mechanism that is capable of handling long sequences. Both models achieve state-of-the-art results on large scale machine translation tasks. However, sequence-to-sequence models mostly work well for large scale parallel data, usually involving millions of sentence pairs. V"
E17-1035,D16-1163,0,0.0293668,"Missing"
E17-1035,S16-1181,1,\N,Missing
J08-2004,P98-1013,0,0.41118,"Missing"
J08-2004,W04-0819,0,0.0389042,"Missing"
J08-2004,boas-2002-bilingual,0,0.0171647,"em. A semantic role represents a semantic relation between a predicate and one of its arguments. Typical semantic roles include agent, patient, source, goal, and so forth, that are core to a predicate, as well as location, time, manner, cause, and so on, that are peripheral. Such semantic information is important in answering who, what, when, where, and why questions therefore is crucial to natural language processing (NLP) tasks such as question-answering (Narayanan and Harabagiu 2004), information extraction (Surdeanu et al. 2005), summarization (Melli et al. 2005), and machine translation (Boas 2002). Any NLP task that requires some form of semantic interpretation could potentially beneﬁt from a high performance semantic role labeling system. For an automatic system, a semantic role labeling task involves locating the linguistic units, typically words or phrases, in natural language text that serve as arguments ∗ 1777 Exposition Drive, Boulder, CO 80309. E-mail: Nianwen.Xue@colorado.edu. Submission received: 15 July 2006; revised submission received: 4 May 2007; accepted for publication: 19 June 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Nu"
J08-2004,burchardt-etal-2006-salsa,0,0.0413723,"igniﬁcant amount of semantically interpreted corpora from which automatic systems can learn. The recent activities in semantic role labeling (Carreras and M`arquez 2004b, 2005; Litkowski 2004) have in large part been driven by the availability of semantically annotated corpora such as the FrameNet (Baker, Fillmore, and Lowe 1998), Proposition Bank (Palmer, Gildea, and Kingsbury 2005), and Nombank (Meyers et al. 2004) projects for English; the tectogrammatical layer annotation of the Prague Dependency Treebank (Sgall, Panevov´a, and Hajiˇcov´a 2004) for Czech; and the Salsa Project for German (Burchardt et al. 2006). These semantically annotated corpora not only provide the training and test material for the development of machine learning systems, but also effectively deﬁne semantic role labeling as a task. PropBank and FrameNet have been the two most widely used corpora in developing automatic semantic role labeling systems. Although both corpora provide predicate–argument structure annotation, they use very different semantic role labels, especially for the core arguments of each predicate. In FrameNet, the semantic roles of a predicate (called a Lexical Unit (LU)) are organized by semantic frames, wh"
J08-2004,W04-2415,0,0.0273661,"Missing"
J08-2004,W04-2412,0,0.0608104,"Missing"
J08-2004,W05-0620,0,0.347833,"Missing"
J08-2004,H05-1084,0,0.0202596,"Missing"
J08-2004,N06-1024,0,0.049334,"Missing"
J08-2004,J02-3001,0,0.793041,"ling system. For an automatic system, a semantic role labeling task involves locating the linguistic units, typically words or phrases, in natural language text that serve as arguments ∗ 1777 Exposition Drive, Boulder, CO 80309. E-mail: Nianwen.Xue@colorado.edu. Submission received: 15 July 2006; revised submission received: 4 May 2007; accepted for publication: 19 June 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 2 to a predicate and assigning semantic role labels to them based on the context in which they occur. Since the seminal work of Gildea and Jurafsky (2002), statistical and machine learning approaches have been the predominant research paradigm in semantic role labeling, like most of the subﬁelds in natural language processing and computational linguistics. A prerequisite for statistical and machine learning approaches to semantic role labeling is the availability of a signiﬁcant amount of semantically interpreted corpora from which automatic systems can learn. The recent activities in semantic role labeling (Carreras and M`arquez 2004b, 2005; Litkowski 2004) have in large part been driven by the availability of semantically annotated corpora su"
J08-2004,W04-2416,0,0.0204882,"ormulated as a binary classiﬁcation task that separates constituents that are arguments or adjuncts to a predicate from those that are not related to the predicate in question. By lumping together argument and adjunct labels, the positive and negative sample imbalance is alleviated somewhat. In addition, it has been shown that argument detection and argument classiﬁcation need different sets of features (Xue and Palmer 2004). A system cannot take advantage of this if both are done in one fell swoop. With a powerful machine-learning algorithm, argument detection can be done with high accuracy (Hacioglu et al. 2004; Pradhan, Ward et al. 2004), provided that the appropriate features are used. The positive and negative sample imbalance can only be partially addressed by having a separate argument detection stage. Even with a binary classiﬁcation task, the number of negative samples is still overwhelmingly larger than the positive samples. In addition, it does not take advantage of the fact that the arguments and adjuncts of a predicate, verbal or nominalized, are related to the predicate itself in linguistically wellunderstood structural conﬁgurations. The overwhelming majority of the arguments and adjunc"
J08-2004,W05-0623,0,0.0141082,"Missing"
J08-2004,W06-1617,0,0.0687953,"al. 2004); SVM (Bejan et al. 2004; Moldovan et al. 2004; Ngai et al. 2004); Memory-based learning (Baldewein et al. 2004), as well as Generative models (Thompson, Patwardhan, and Arnold 2004). Bejan et al. (2004) achieved the best result using an SVM classiﬁer combined with improved linguistic features. They achieved an F1 measure of 0.763 in their internal evaluation, and 0.831 using the more lenient ofﬁcial Senseval-3 scorer. Compared with the large body of work on the semantic role labeling on verbs, the argument structure analysis of nominal predicates has so far received less attention. Jiang and Ng (2006) reports a semantic role labeling system on nominal predicates, also using the maximum entropy approach. Their system achieves F1 scores of 0.727 and 0.691, respectively, on gold-standard and automatic parses, indicating semantic role labeling of nominal predicates is a much more difﬁcult problem than that of verbs for English as well. Outside the narrow domain of semantic role labeling, there has been a steady accumulation of work on semantic analysis of nouns and a gradual expansion of the domain in which the semantic analysis is performed. Lapata (2002) developed a probabilistic model for t"
J08-2004,kipper-etal-2006-extending,0,0.0117259,"d in the same syntactic position as a result of syntactic alternations (Levin 1993) or other syntactic processes. In addition, different framesets of a verb take different sets of arguments that demonstrate different syntactic patterns. Thus, predicate–argument structure analysis at the PropBank annotation level represents a crucial leap towards proper representation of semantic structure from the syntactic structure. Should the need for more general semantic roles arise, these predicate-speciﬁc semantic roles can be mapped (Yi, Loper, and Palmer 2007) to FrameNet-style or even VerbNet-style (Kipper et al. 2006) labels. Using the semantic annotation of the Chinese PropBank and NomBank as training and test material, we were able for the ﬁrst time to develop a Chinese semantic role labeling system that is trained and tested on semantically annotated Chinese corpora of signiﬁcant sizes. Using parses produced with different levels of automation (a fully automatic parser, a parser with correct segmentation, a parser with both correct segmentation and POS-tagging, and treebank gold-standard parses), we were able to quantify the impact of different Chinese language processing components on the semantic anal"
J08-2004,W05-0625,0,0.0240182,"e been explored. The relative merits of using a full syntactic parser that provides hierarchical structures (Xue and Palmer 2004) vs. a shallow chunker (Pradhan, Hacioglu et al. 2005; Hacioglu et al. 2004) has been studied extensively. Noting that parsing errors are difﬁcult or even impossible to recover at the semantic 249 Computational Linguistics Volume 34, Number 2 role labeling stage, Yi and Palmer (2005) experimented with integrating semantic role labeling with a Maximum Entropy-based parser, effectively treating semantic role labels as function tags on the constituents in a parse tree. Koomen et al. (2005), Pradhan, Ward et al. (2005), M`arquez et al. (2005), and Tsai et al. (2005) pursued alternative approaches to make their semantic role labeling systems more robust by combining the output of multiple systems. Punyakanok, Roth, and Yi (2005), in particular, achieved the best performance (F1 = 0.794) on the WSJ test set in the 2005 CoNLL shared task by combining multiple semantic role labeling systems using an integer linear programming technique (Punyakanok et al. 2004). Pradhan, Hacioglu et al. (2005) reported the best result (F1 = 0.684) on the Brown test set using the WSJ data as the train"
J08-2004,W04-0832,0,0.0327058,"Missing"
J08-2004,W05-1001,0,0.0264899,"Missing"
J08-2004,J02-3004,0,0.00696169,"so far received less attention. Jiang and Ng (2006) reports a semantic role labeling system on nominal predicates, also using the maximum entropy approach. Their system achieves F1 scores of 0.727 and 0.691, respectively, on gold-standard and automatic parses, indicating semantic role labeling of nominal predicates is a much more difﬁcult problem than that of verbs for English as well. Outside the narrow domain of semantic role labeling, there has been a steady accumulation of work on semantic analysis of nouns and a gradual expansion of the domain in which the semantic analysis is performed. Lapata (2002) developed a probabilistic model for the interpretation of nominalizations, focusing on the semantic relation between the noun head and its prenominal modiﬁer in a nominalized compound (i.e., whether the prenominal modiﬁer is an underlying subject or direct object of the verb from which the nominalized head is derived). Their model achieved a very high accuracy of 0.861 when evaluated on data extracted from the British National Corpus. Girju et al. (2004) and Moldovan and Badulescu (2005) extended the domain of linguistic analysis to that of noun phrases. In particular, they focused on the stu"
J08-2004,W04-0803,0,0.042029,"to them based on the context in which they occur. Since the seminal work of Gildea and Jurafsky (2002), statistical and machine learning approaches have been the predominant research paradigm in semantic role labeling, like most of the subﬁelds in natural language processing and computational linguistics. A prerequisite for statistical and machine learning approaches to semantic role labeling is the availability of a signiﬁcant amount of semantically interpreted corpora from which automatic systems can learn. The recent activities in semantic role labeling (Carreras and M`arquez 2004b, 2005; Litkowski 2004) have in large part been driven by the availability of semantically annotated corpora such as the FrameNet (Baker, Fillmore, and Lowe 1998), Proposition Bank (Palmer, Gildea, and Kingsbury 2005), and Nombank (Meyers et al. 2004) projects for English; the tectogrammatical layer annotation of the Prague Dependency Treebank (Sgall, Panevov´a, and Hajiˇcov´a 2004) for Czech; and the Salsa Project for German (Burchardt et al. 2006). These semantically annotated corpora not only provide the training and test material for the development of machine learning systems, but also effectively deﬁne semanti"
J08-2004,W05-0632,0,0.0793771,"n rapid improvement in the semantic role labeling accuracy of English verbs, fueled by the development of PropBank (Palmer, Gildea, and Kingsbury 2005), which annotates the verbs in the one-millionword Penn Treebank with semantic role labels. A wide range of statistical and machine learning techniques have been applied to the semantic role labeling of verbs, using PropBank as training and test material. The machine-learning techniques used include Support Vector Machines (Pradhan, Ward et al. 2004; Tsai et al. 2005), Maximum Entropy (Xue and Palmer 2004; Haghighi, Toutanova, and Manning 2005; Liu et al. 2005; Yi and Palmer 2005), Conditional Random Fields (Cohn and Blunsom 2005), and many others. Because semantic role labeling is a complex task based on a wide range of lower level natural language techniques, many different preprocessing, integration, and combination techniques have been explored. The relative merits of using a full syntactic parser that provides hierarchical structures (Xue and Palmer 2004) vs. a shallow chunker (Pradhan, Hacioglu et al. 2005; Hacioglu et al. 2004) has been studied extensively. Noting that parsing errors are difﬁcult or even impossible to recover at the semantic"
J08-2004,W03-1025,0,0.00653248,"ever, our experiments show that even such a simple classiﬁcation can be used to provide features that improve the semantic role labeling performance. 4.3 Using Automatic Parses Previous work (Sun and Jurafsky 2004) on Chinese semantic role labeling uses a parser that assumes correct (hand-crafted) segmentation. As word segmentation is a very challenging problem that has attracted a large body of research by itself, it is still unclear how well semantic role tagging in Chinese can be performed in realistic situations. In our experiments, we implemented a Maximum Entropy–based parser similar to Luo (2003). The parser performs Chinese word segmentation, POS tagging, and parsing in one integrated system. The parser is trained on the Xinhua news and Broadcast news portion of the Chinese Treebank, which has 498K words. Tested on the held-out test data, the parser achieved an unlabeled precision and recall of 0.889 and 0.868, respectively, for the combined word segmentation and parsing accuracy. When the word segmentation is singled out for evaluation, the parser achieved an F-score of 0.969. It is important to point out that these results cannot be directly compared with most of the results report"
J08-2004,J93-2004,0,0.0307343,"Missing"
J08-2004,H05-1081,0,0.110115,"Missing"
J08-2004,W04-2705,0,0.714822,"most of the subﬁelds in natural language processing and computational linguistics. A prerequisite for statistical and machine learning approaches to semantic role labeling is the availability of a signiﬁcant amount of semantically interpreted corpora from which automatic systems can learn. The recent activities in semantic role labeling (Carreras and M`arquez 2004b, 2005; Litkowski 2004) have in large part been driven by the availability of semantically annotated corpora such as the FrameNet (Baker, Fillmore, and Lowe 1998), Proposition Bank (Palmer, Gildea, and Kingsbury 2005), and Nombank (Meyers et al. 2004) projects for English; the tectogrammatical layer annotation of the Prague Dependency Treebank (Sgall, Panevov´a, and Hajiˇcov´a 2004) for Czech; and the Salsa Project for German (Burchardt et al. 2006). These semantically annotated corpora not only provide the training and test material for the development of machine learning systems, but also effectively deﬁne semantic role labeling as a task. PropBank and FrameNet have been the two most widely used corpora in developing automatic semantic role labeling systems. Although both corpora provide predicate–argument structure annotation, they use"
J08-2004,W04-0413,0,0.115882,"most of the subﬁelds in natural language processing and computational linguistics. A prerequisite for statistical and machine learning approaches to semantic role labeling is the availability of a signiﬁcant amount of semantically interpreted corpora from which automatic systems can learn. The recent activities in semantic role labeling (Carreras and M`arquez 2004b, 2005; Litkowski 2004) have in large part been driven by the availability of semantically annotated corpora such as the FrameNet (Baker, Fillmore, and Lowe 1998), Proposition Bank (Palmer, Gildea, and Kingsbury 2005), and Nombank (Meyers et al. 2004) projects for English; the tectogrammatical layer annotation of the Prague Dependency Treebank (Sgall, Panevov´a, and Hajiˇcov´a 2004) for Czech; and the Salsa Project for German (Burchardt et al. 2006). These semantically annotated corpora not only provide the training and test material for the development of machine learning systems, but also effectively deﬁne semantic role labeling as a task. PropBank and FrameNet have been the two most widely used corpora in developing automatic semantic role labeling systems. Although both corpora provide predicate–argument structure annotation, they use"
J08-2004,H05-1112,0,0.0230893,"n of work on semantic analysis of nouns and a gradual expansion of the domain in which the semantic analysis is performed. Lapata (2002) developed a probabilistic model for the interpretation of nominalizations, focusing on the semantic relation between the noun head and its prenominal modiﬁer in a nominalized compound (i.e., whether the prenominal modiﬁer is an underlying subject or direct object of the verb from which the nominalized head is derived). Their model achieved a very high accuracy of 0.861 when evaluated on data extracted from the British National Corpus. Girju et al. (2004) and Moldovan and Badulescu (2005) extended the domain of linguistic analysis to that of noun phrases. In particular, they focused on the study of four nominal constructions: complex nominals in which a head noun is modiﬁed by other nouns or adjectives derived from nouns, genitives, adjective phrases, and adjective clauses. In general, previous work on nominals, perhaps with the exception of Nakov and Hearst (2006) all attempt to specify a ﬁnite set of semantic relations between the nouns and their modiﬁers in the spirit of Levi (1979). The PropBank/NomBank approach to 250 Xue Semantic Role Labeling of Chinese Predicates seman"
J08-2004,W04-0841,0,0.0209727,"sing a joint-learning strategy to rule out such conﬂicting argument labels. The semantic role labeling performance on the FrameNet data set has also improved signiﬁcantly from Gildea and Jurafsky’s (2002) early results, thanks mostly to the Senseval-3 semantic role labeling competition (Litkowski 2004). Participants of Senseval-3 have used a variety of machine learning algorithms to tackle the semantic role labeling problem: Maximum Entropy (Baldewein et al. 2004; Ngai et al. 2004; Kwon, Fleischman, and Hovy 2004); Boosting, SNOW, and Decision Lists (Ngai et al. 2004); SVM (Bejan et al. 2004; Moldovan et al. 2004; Ngai et al. 2004); Memory-based learning (Baldewein et al. 2004), as well as Generative models (Thompson, Patwardhan, and Arnold 2004). Bejan et al. (2004) achieved the best result using an SVM classiﬁer combined with improved linguistic features. They achieved an F1 measure of 0.763 in their internal evaluation, and 0.831 using the more lenient ofﬁcial Senseval-3 scorer. Compared with the large body of work on the semantic role labeling on verbs, the argument structure analysis of nominal predicates has so far received less attention. Jiang and Ng (2006) reports a semantic role labeling sys"
J08-2004,C04-1100,0,0.0484434,"ction Semantic role labeling (SRL) is the task of identifying arguments for a predicate and assigning semantically meaningful labels to them. A semantic role represents a semantic relation between a predicate and one of its arguments. Typical semantic roles include agent, patient, source, goal, and so forth, that are core to a predicate, as well as location, time, manner, cause, and so on, that are peripheral. Such semantic information is important in answering who, what, when, where, and why questions therefore is crucial to natural language processing (NLP) tasks such as question-answering (Narayanan and Harabagiu 2004), information extraction (Surdeanu et al. 2005), summarization (Melli et al. 2005), and machine translation (Boas 2002). Any NLP task that requires some form of semantic interpretation could potentially beneﬁt from a high performance semantic role labeling system. For an automatic system, a semantic role labeling task involves locating the linguistic units, typically words or phrases, in natural language text that serve as arguments ∗ 1777 Exposition Drive, Boulder, CO 80309. E-mail: Nianwen.Xue@colorado.edu. Submission received: 15 July 2006; revised submission received: 4 May 2007; accepted"
J08-2004,W04-0845,0,0.00939866,"me semantic role label cannot be assigned to more than one core argument. Toutanova, Haghighi, and Manning (2005) address this by using a joint-learning strategy to rule out such conﬂicting argument labels. The semantic role labeling performance on the FrameNet data set has also improved signiﬁcantly from Gildea and Jurafsky’s (2002) early results, thanks mostly to the Senseval-3 semantic role labeling competition (Litkowski 2004). Participants of Senseval-3 have used a variety of machine learning algorithms to tackle the semantic role labeling problem: Maximum Entropy (Baldewein et al. 2004; Ngai et al. 2004; Kwon, Fleischman, and Hovy 2004); Boosting, SNOW, and Decision Lists (Ngai et al. 2004); SVM (Bejan et al. 2004; Moldovan et al. 2004; Ngai et al. 2004); Memory-based learning (Baldewein et al. 2004), as well as Generative models (Thompson, Patwardhan, and Arnold 2004). Bejan et al. (2004) achieved the best result using an SVM classiﬁer combined with improved linguistic features. They achieved an F1 measure of 0.763 in their internal evaluation, and 0.831 using the more lenient ofﬁcial Senseval-3 scorer. Compared with the large body of work on the semantic role labeling on verbs, the argumen"
J08-2004,J05-1004,0,0.514854,"Missing"
J08-2004,W05-0634,0,0.0368136,"Missing"
J08-2004,N04-4036,0,0.0614628,"Missing"
J08-2004,P05-1072,0,0.0174217,"Missing"
J08-2004,N04-1030,0,0.122758,"Missing"
J08-2004,W05-0639,0,0.0395563,"Missing"
J08-2004,C04-1197,0,0.023913,"Missing"
J08-2004,W04-2706,0,0.0186313,"Missing"
J08-2004,N04-1032,0,0.0929051,"Missing"
J08-2004,W04-0857,0,0.0325774,"Missing"
J08-2004,P05-1073,0,0.0412681,"Missing"
J08-2004,W05-0638,0,0.0128777,"ole labeling, using the FrameNet corpus as training and test material. Since then, there has been rapid improvement in the semantic role labeling accuracy of English verbs, fueled by the development of PropBank (Palmer, Gildea, and Kingsbury 2005), which annotates the verbs in the one-millionword Penn Treebank with semantic role labels. A wide range of statistical and machine learning techniques have been applied to the semantic role labeling of verbs, using PropBank as training and test material. The machine-learning techniques used include Support Vector Machines (Pradhan, Ward et al. 2004; Tsai et al. 2005), Maximum Entropy (Xue and Palmer 2004; Haghighi, Toutanova, and Manning 2005; Liu et al. 2005; Yi and Palmer 2005), Conditional Random Fields (Cohn and Blunsom 2005), and many others. Because semantic role labeling is a complex task based on a wide range of lower level natural language techniques, many different preprocessing, integration, and combination techniques have been explored. The relative merits of using a full syntactic parser that provides hierarchical structures (Xue and Palmer 2004) vs. a shallow chunker (Pradhan, Hacioglu et al. 2005; Hacioglu et al. 2004) has been studied exte"
J08-2004,xue-2006-annotating,1,0.910774,"arguments. In this article, we 1 They restated their results in Chen, Sun, and Jurafsky (2005) due to an error in retraining the Collins parser (Collins 1999) on Chinese, which led to inﬂated Chinese syntactic parsing and therefore semantic role labeling results. 227 Computational Linguistics Volume 34, Number 2 report work on the semantic role labeling of Chinese predicates for both verbs and their nominalizations, exploiting two recently completed corpora, the Chinese PropBank (Xue and Palmer 2003), a corpus that annotates the predicate–argument structure of verbs, and the Chinese NomBank (Xue 2006a), a companion corpus that annotates the predicate-argument structure of nominalized predicates in Chinese. Both corpora are built on top of the Chinese Treebank (Xue et al. 2005), in the sense that the semantic role labels are assigned to constituents in the parse tree. The Chinese PropBank and Nombank adopted the English PropBank predicatespeciﬁc approach in representing the semantic roles of the core arguments. In the absence of a Chinese linguistic ontology like the semantic frames developed for the English FrameNet Project, using the PropBank-style of semantic roles allows faster develop"
J08-2004,N06-1055,1,0.900694,"arguments. In this article, we 1 They restated their results in Chen, Sun, and Jurafsky (2005) due to an error in retraining the Collins parser (Collins 1999) on Chinese, which led to inﬂated Chinese syntactic parsing and therefore semantic role labeling results. 227 Computational Linguistics Volume 34, Number 2 report work on the semantic role labeling of Chinese predicates for both verbs and their nominalizations, exploiting two recently completed corpora, the Chinese PropBank (Xue and Palmer 2003), a corpus that annotates the predicate–argument structure of verbs, and the Chinese NomBank (Xue 2006a), a companion corpus that annotates the predicate-argument structure of nominalized predicates in Chinese. Both corpora are built on top of the Chinese Treebank (Xue et al. 2005), in the sense that the semantic role labels are assigned to constituents in the parse tree. The Chinese PropBank and Nombank adopted the English PropBank predicatespeciﬁc approach in representing the semantic roles of the core arguments. In the absence of a Chinese linguistic ontology like the semantic frames developed for the English FrameNet Project, using the PropBank-style of semantic roles allows faster develop"
J08-2004,W03-1707,1,0.673053,"tecting and classifying the heads of arguments to avoid the hard problem of getting the correct text spans for the arguments. In this article, we 1 They restated their results in Chen, Sun, and Jurafsky (2005) due to an error in retraining the Collins parser (Collins 1999) on Chinese, which led to inﬂated Chinese syntactic parsing and therefore semantic role labeling results. 227 Computational Linguistics Volume 34, Number 2 report work on the semantic role labeling of Chinese predicates for both verbs and their nominalizations, exploiting two recently completed corpora, the Chinese PropBank (Xue and Palmer 2003), a corpus that annotates the predicate–argument structure of verbs, and the Chinese NomBank (Xue 2006a), a companion corpus that annotates the predicate-argument structure of nominalized predicates in Chinese. Both corpora are built on top of the Chinese Treebank (Xue et al. 2005), in the sense that the semantic role labels are assigned to constituents in the parse tree. The Chinese PropBank and Nombank adopted the English PropBank predicatespeciﬁc approach in representing the semantic roles of the core arguments. In the absence of a Chinese linguistic ontology like the semantic frames develo"
J08-2004,W04-3212,1,0.893726,"Missing"
J08-2004,N07-1069,0,0.0385733,"Missing"
J08-2004,W04-0817,0,\N,Missing
J08-2004,W04-2610,0,\N,Missing
J08-2004,J03-4003,0,\N,Missing
J08-2004,C98-1013,0,\N,Missing
J08-2004,W05-0622,0,\N,Missing
J08-2004,P03-1002,0,\N,Missing
J08-2004,P02-1031,0,\N,Missing
J08-2004,W05-0627,0,\N,Missing
J17-3002,J00-1004,0,0.102522,"Missing"
J17-3002,2008.amta-srw.1,0,0.284162,"ase alignment. In the meantime, it is also clear from the examples that we have provided that not all non-terminal nodes in syntactic parses need to be aligned for the purpose of extracting Hiero-style rules. This suggests that the syntactic trees in existing treebanks simultaneously have too much and too little structure and are not optimal for the purpose of translation rule extraction, as they are not designed for any particular natural language applications. This observation is consistent with the findings of other researchers—in particular, those of Lavie, Parlikar, and Ambati (2008) and Ambati and Lavie (2008). However, when there is sufficient structure in the parse trees on both sides that are hierarchically aligned, Hiero-style translation rules that capture the translation divergences between Chinese and English can be extracted. We suspect that this observation generalizes to other language pairs as well. This suggests that one way to advance MT research is to build hierarchically aligned treebanks that systematically consider the interaction of word and phrase alignment with the syntactic structure of each sentence in a sentence pair. This is in contrast with the current state of affairs wher"
J17-3002,W13-2322,0,0.0544178,"eaued, the field is poised to climb up the Vauquois Pyramid (Vauquois 1968) and start exploring the utility of semantic representations for SMT systems, mirroring the progression of earlier rule-based approaches in the previous incarnation of MT research. The hope is that more abstract semantic representations can better address translation divergences than syntactic structures, and this advantage will hopefully offset the potential harm caused by the expected drop in the accuracy of semantic analyzers that are more difficult to develop than syntactic parsers. The development of the AMR Bank (Banarescu et al. 2013) is the latest attempt in that direction, although it is important to point out that the significance of such semantically annotated corpora goes far beyond the narrow purpose of MT, and that the AMR Bank is neither the first nor the only attempt to develop semantically annotated resources that can be used for MT purposes. Similar efforts include a series of head-driven phrase structure grammar– based resources (e.g., LingGO Redwoods Treebank and the DeepBank) (Oepen et al. 2002; Flickinger, Zhang, and Kordoni 2012; Bender et al. 2015; Flickinger, Oepen, and Bender 2017), the semantic layers o"
J17-3002,baran-etal-2012-annotating,1,0.886454,"Missing"
J17-3002,W15-0128,0,0.0245022,"han syntactic parsers. The development of the AMR Bank (Banarescu et al. 2013) is the latest attempt in that direction, although it is important to point out that the significance of such semantically annotated corpora goes far beyond the narrow purpose of MT, and that the AMR Bank is neither the first nor the only attempt to develop semantically annotated resources that can be used for MT purposes. Similar efforts include a series of head-driven phrase structure grammar– based resources (e.g., LingGO Redwoods Treebank and the DeepBank) (Oepen et al. 2002; Flickinger, Zhang, and Kordoni 2012; Bender et al. 2015; Flickinger, Oepen, and Bender 2017), the semantic layers of which are based on Minimal Recursion Semantics (MRS) (Copestake et al. 2005), a semantic representation framework that has been adopted in modern semantic transfer-based MT systems (Lønning et al. 2004). As the field prepares to take this next step of incorporating semantic representation into the SMT paradigm, it is worth asking: i) Are translation divergences properly represented by the kind of syntax-based translation rules such as hierarchical phrase pairs used in current systems? ii) Can these rules be properly extracted from e"
J17-3002,J93-2003,0,0.0745876,"Missing"
J17-3002,A00-2018,0,0.152811,"Missing"
J17-3002,P03-1012,0,0.0230971,"pora without using syntactic structures in existing work, but we believe a hierarchically aligned treebank can be used to extract translation rules that can better preserve lexical dependencies or constructions in language. Of course, in order to achieve competitive performance, the hierarchically aligned corpus needs to be either automatically reproduced or acquired on a large scale. 559 Computational Linguistics Volume 43, Number 3 6.2 Related Work on Alignment There is also a wealth of work that addresses the issue of “cohesion” or compatibility between word alignments and syntactic trees. Cherry and Lin (2003) developed a probabilistic model to improve word alignment using the dependency tree structure as features to influence word alignment decisions. Cherry and Lin (2006) develop an approach that shrinks the search space for word alignment by bringing cohesion constraints imposed by the dependency structure inside an Inverse Transduction Grammar framework (Wu 1997). DeNero and Klein (2007) demonstrate how word alignments that do not respect the constituent structure of the target sentence hinder the extraction of generalizable translation rules and propose an unsupervised word alignment model tha"
J17-3002,P06-2014,0,0.0408239,"serve lexical dependencies or constructions in language. Of course, in order to achieve competitive performance, the hierarchically aligned corpus needs to be either automatically reproduced or acquired on a large scale. 559 Computational Linguistics Volume 43, Number 3 6.2 Related Work on Alignment There is also a wealth of work that addresses the issue of “cohesion” or compatibility between word alignments and syntactic trees. Cherry and Lin (2003) developed a probabilistic model to improve word alignment using the dependency tree structure as features to influence word alignment decisions. Cherry and Lin (2006) develop an approach that shrinks the search space for word alignment by bringing cohesion constraints imposed by the dependency structure inside an Inverse Transduction Grammar framework (Wu 1997). DeNero and Klein (2007) demonstrate how word alignments that do not respect the constituent structure of the target sentence hinder the extraction of generalizable translation rules and propose an unsupervised word alignment model that takes into account the constituent structure of the target sentence. May and Knight (2007) use rules extracted from a syntax-based MT model to re-align the words in"
J17-3002,P05-1033,0,0.0415449,"e next section, we look into the question of whether the divergences reported in this section can be syntactically captured. 4. Can the Translation Divergences Be Captured by Syntax-Based Translation Rules? Having extracted and characterized the translation divergences empirically using HACEPT, in this section we try to answer the question whether these translation divergences can be captured by the kind of syntax-based rules used in modern SMT systems. Given a pair of hierarchically aligned parse trees, we show that a simple algorithm can be followed to extract Hiero-style rules described in Chiang (2005). In comparison with Chiang’s approach, which relies on word-aligned sentence pairs to extract translation rules, the procedure of translation rule extraction is much simpler for us because we have hierarchically aligned parse trees. This is how we extract a hierarchical translation rule from a phrase alignment: Given a pair of hierarchically aligned trees Ts and Tt , find all aligned node pairs (ns , nt ). For 17 Our interpretation of the divergence here is slightly different from that of Dorr (1994). According to Dorr, “in (VII), the event is lexically realized as the main verb break in Engl"
J17-3002,J07-2003,0,0.138708,"Missing"
J17-3002,1995.tmi-1.2,0,0.646655,"(Dave, Parikh, and Bhattacharyya 2001; Gupta and Chatterjee 2001, 2003; Sinha, Mahesh, and Thakur 2005b, 2005a), and Sanskrit (Mishra and Mishra 2009). Translation divergence is also a challenge for MT approaches based on semantic transfer. An early survey of translation divergences in the context of transfer-based MT is provided by Lindop and Tsujii (1991). Most of the MT field has shifted to the SMT paradigm since the pioneering effort at IBM, but a few transfer-based MT efforts persevered. One recent such effort is the LOGON system (Lønning et al. 2004; Oepen et al. 2007), which uses MRS (Copestake et al. 1995, 2005) as a semantic representation framework. MT systems based on semantic transfer are designed to systematically 21 We use AMR as an example, but similar remarks could also apply to MRS (Copestake et al. 2005) and Discourse Representation Structures (Kamp and Reyle 1993). 558 Deng and Xue Chinese–English Translation Divergences handle “syntactic divergences” such as word order differences, but “lexical-semantic divergences” (caused by how certain meaning is encoded lexically) often need to be tackled with additional semantic transfer rules. The challenges that translation divergences pose"
J17-3002,W06-1628,0,0.085382,"Missing"
J17-3002,P07-1003,0,0.0383712,"559 Computational Linguistics Volume 43, Number 3 6.2 Related Work on Alignment There is also a wealth of work that addresses the issue of “cohesion” or compatibility between word alignments and syntactic trees. Cherry and Lin (2003) developed a probabilistic model to improve word alignment using the dependency tree structure as features to influence word alignment decisions. Cherry and Lin (2006) develop an approach that shrinks the search space for word alignment by bringing cohesion constraints imposed by the dependency structure inside an Inverse Transduction Grammar framework (Wu 1997). DeNero and Klein (2007) demonstrate how word alignments that do not respect the constituent structure of the target sentence hinder the extraction of generalizable translation rules and propose an unsupervised word alignment model that takes into account the constituent structure of the target sentence. May and Knight (2007) use rules extracted from a syntax-based MT model to re-align the words in a sentence pair. Fossum (2010) reports work in which she uses syntactic features to correct word alignment errors, and uses word alignment information to correct syntactic parsing errors. Riesa and Marcu (2010) and Riesa,"
J17-3002,W14-4904,1,0.904645,"sting treebanks is not optimal for extracting such translation rules. We also discuss the implications of our study for attempts to bridge translation divergences by devising shared semantic representations across languages. Our quantitative results lend further support to the observation that although it is possible to bridge some translation divergences with semantic representations, other translation divergences are open-ended, thus building a semantic representation that captures all possible translation divergences may be impractical. Note: Part of Section 2 of this paper has appeared in Deng and Xue (2014c). All the other contents are new. ∗ Department of Chinese Languages and Literature, Tsinghua University, Beijing, China. E-mail: ddeng@tsinghua.edu.cn. ∗∗ Computer Science Department, Brandeis University, 415 South Street, Waltham MA 02453. E-mail: xuen@brandeis.edu. Submission received: 25 May 2015; revised version received: 16 September 2016; accepted for publication: 15 December 2016. doi:10.1162/COLI a 00292 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational"
J17-3002,C14-1143,1,0.884166,"Missing"
J17-3002,W15-1001,1,0.890803,"Missing"
J17-3002,P05-1067,0,0.247855,"erences can be described along two dimensions: whether they use syntactic structures on the source side or the target side or both, and whether they use phrase structures or dependency structures. String-to-tree systems model the syntactic structures of target language sentences (Galley et al. 2004, 2006) and tree-to-string systems model the syntactic structures of source language sentences (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006; Liu et al. 2007; Liu and Gildea 2008). Tree-to-tree systems model the syntactic structures of both source and target language sentences (Eisner 2003; Ding and Palmer 2005; Cowan, Kuˇcerov´a, and Collins 2006; Zhang et al. 2008; ¨ and Liu 2009). Early syntax-based models generally use phrase structure (or Liu, Lu, constituent structure) trees and later syntax-based systems also use dependency trees (Shen, Xu, and Weischedel 2008). The general observation about syntax-based models is that although incorporating syntactic structures has led to solid gains, there are also challenges that have prevented syntax-based systems from realizing their full potential. The first challenge is the inevitable errors acquired in automatic syntactic parsing. Syntax-based systems"
J17-3002,J94-4004,0,0.178361,"lly produced parse trees are used because we want to isolate genuine translation divergences resulting from different syntactic 523 Computational Linguistics Volume 43, Number 3 realizations between the source and target languages from artificial ones caused by parsing errors.1 There are three main findings from our study: 1) The translation divergences are much more diverse than previously realized. Previous discussions of translation divergence are mostly qualitative in nature, covering a few widely recognized linguistic differences between languages without the necessary empirical support (Dorr 1994). Using an actual corpus allows us to not only extract all possible divergences that actually occur in the corpus but also quantify each type of translation divergence. This quantitative information can guide MT researchers to focus on high pay-off translation divergences when designing their MT systems. For example, some high-profile translation divergence such as “head-switching” that are frequently mentioned in MT work (e.g., Ding and Palmer 2005) turn out to be very rare in our corpus, whereas other translation divergences that are barely mentioned exist in large quantities. 2) For the mos"
J17-3002,dorr-etal-2002-duster,0,0.652512,"or us to exhaustively examine all possible translation divergences that appear in naturally occurring data, without limiting ourselves to a predefined set of translation divergences gathered from linguistic knowledge. The translation divergences that Dorr examined are all related to verbs and the realization of their arguments. Although they reflect important cross-lingual differences, they are certainly not the only translation divergences, as we have demonstrated. The use of an annotated corpus also allows us to automatically compute the distribution of the translation divergences. Although Dorr et al. (2002) also attempt to quantify the translation divergences in their data, their computation is based on a predetermined set of translation divergences, and does not necessarily cover all possible translation divergences. We do not distinguish “syntactic” and “lexical-semantic” translation divergences, because, as we show in Section 4, the Hiero-style translation rules used in statistical translation models can encapsulate both “syntactic” and “lexical-semantic” translation divergences in a uniform manner, making such a distinction largely irrelevant. An empirical investigation of the translation di"
J17-3002,P03-2041,0,0.104817,"ow in Section 4, the Hiero-style translation rules used in statistical translation models can encapsulate both “syntactic” and “lexical-semantic” translation divergences in a uniform manner, making such a distinction largely irrelevant. An empirical investigation of the translation divergences of the kind we describe here also sheds light on the feasibility of developing synchronous grammars for MT. Early work such as Wu (1997) and (Alshawi, Bangalore, and Douglas 2000) assumes a form of context-free grammar that has very strict restrictions on the types of grammatical rules that are allowed. Eisner (2003) argues for the need to accommodate nonisomorphic syntactic structures in MT systems and proposes using synchronous tree substitution grammars that are collections of pairs of aligned elementary trees as basic units of the grammar. Ding and Palmer (2005) implemented a statistical MT model that uses synchronous dependency insertion grammars, the basic units of which are also elementary trees, but their elementary trees are subgraphs of a dependency tree rather than a phrase structure tree. However, none of this previous work attempts to demonstrate that their form of synchronous grammar can acc"
J17-3002,P06-1121,0,0.187411,"Missing"
J17-3002,N04-1035,0,0.363246,"guages and reordering in a sentence pair often involves syntactic constituents rather than individual words. It is natural then to expect that incorporating syntactic structures into SMT models would lead to improved MT accuracy. Various approaches have been proposed to incorporate syntactic structures, and their differences can be described along two dimensions: whether they use syntactic structures on the source side or the target side or both, and whether they use phrase structures or dependency structures. String-to-tree systems model the syntactic structures of target language sentences (Galley et al. 2004, 2006) and tree-to-string systems model the syntactic structures of source language sentences (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006; Liu et al. 2007; Liu and Gildea 2008). Tree-to-tree systems model the syntactic structures of both source and target language sentences (Eisner 2003; Ding and Palmer 2005; Cowan, Kuˇcerov´a, and Collins 2006; Zhang et al. 2008; ¨ and Liu 2009). Early syntax-based models generally use phrase structure (or Liu, Lu, constituent structure) trees and later syntax-based systems also use dependency trees (Shen, Xu, and Weischedel 2008). The general obs"
J17-3002,2002.tmi-tutorials.1,0,0.0766115,"Missing"
J17-3002,2003.mtsummit-papers.19,0,0.160688,"Missing"
J17-3002,2007.tmi-papers.11,0,0.0303715,"t. Our work differs from this line of research in that we systematically consider the interaction between word and phrase alignment, and propose an alignment scheme that operates on syntactic parses rather than on just words in a sentence pair. There have been previous attempts to align non-terminal nodes in a tree (sometimes referred to as “subtree alignment”) in the context of syntax-based SMT. Tinsley et al. (2007) proposes an algorithm that automatically aligns the phrase structure trees of a sentence pair, as well as a set of well-formedness constraints that such alignments have to obey. Hearne et al. (2007) automatically align a parallel English–French treebank using this algorithm and study translation divergences between the two languages as reflected in the aligned subtrees. However, their study does not go beyond translation divergences that have been previously observed such as those described in Dorr (1994) and does not attempt to quantify them. Lavie, Parlikar, and Ambati (2008) propose an algorithm to automatically align the nonterminal nodes between pairs of wordaligned phrase structure trees in a parallel corpus, and extract aligned subtrees from this corpus to create a syntax-based ph"
J17-3002,D09-1024,0,0.0246248,"and Marcu (2011) use a large number of syntactic features from source and target language syntax to perform word alignment in a discriminative machine learning framework. Wang and Zong (2013) use dependency structure to constrain word alignment in a generative framework. All of these works recognize the need to use syntactic structure to influence word alignment (or vice versa), but, typically, work on automatic word alignment focuses on statistical modeling and does not discuss the linguistic basis of word alignment and how it interacts with syntactic structure, perhaps with the exception of Hermjakob (2009), who recognizes the difficulty of aligning what he calls “orphan” function words, words that do not have an equivalent in the other language. He ultimately adopts a G-TAHS, which seems to be the only plausible option short of performing hierarchical alignment. Our work differs from this line of research in that we systematically consider the interaction between word and phrase alignment, and propose an alignment scheme that operates on syntactic parses rather than on just words in a sentence pair. There have been previous attempts to align non-terminal nodes in a tree (sometimes referred to a"
J17-3002,2006.amta-papers.8,0,0.110194,"Missing"
J17-3002,P09-5002,0,0.0130524,"Journal newswire articles that the parsers are generally trained on. Syntax-based 522 Deng and Xue Chinese–English Translation Divergences SMT systems are only competitive when used in conjunction with techniques such as packed forests, which relax the need to use one-best parses (Mi and Huang 2008; Mi, Huang, and Liu 2008). The second challenge is that the constraints imposed by the syntactic structures have been shown to be too stringent, and prevent useful syntaxbased translation rules that do not obey syntactic constituent boundaries from being used in SMT models and hurt MT performance (Koehn 2009). Effective techniques have been developed to address this issue by identifying heuristics to relax the constraints of syntactic structures when extracting translation rules (Zollmann and Venugopal 2006). As the gain of incorporating syntactic information has plateaued, the field is poised to climb up the Vauquois Pyramid (Vauquois 1968) and start exploring the utility of semantic representations for SMT systems, mirroring the progression of earlier rule-based approaches in the previous incarnation of MT research. The hope is that more abstract semantic representations can better address trans"
J17-3002,N03-1017,0,0.0317103,"Missing"
J17-3002,W08-0411,0,0.061329,"Missing"
J17-3002,li-etal-2012-parallel,1,0.894399,"Missing"
J17-3002,W08-0308,0,0.0492433,"Missing"
J17-3002,P07-1089,0,0.0674431,"Missing"
J17-3002,P06-1077,0,0.135656,"Missing"
J17-3002,P09-1063,0,0.0545627,"Missing"
J17-3002,H94-1020,0,0.582972,"Missing"
J17-3002,J93-2004,0,0.0590937,"Missing"
J17-3002,D07-1038,0,0.0363806,"pendency tree structure as features to influence word alignment decisions. Cherry and Lin (2006) develop an approach that shrinks the search space for word alignment by bringing cohesion constraints imposed by the dependency structure inside an Inverse Transduction Grammar framework (Wu 1997). DeNero and Klein (2007) demonstrate how word alignments that do not respect the constituent structure of the target sentence hinder the extraction of generalizable translation rules and propose an unsupervised word alignment model that takes into account the constituent structure of the target sentence. May and Knight (2007) use rules extracted from a syntax-based MT model to re-align the words in a sentence pair. Fossum (2010) reports work in which she uses syntactic features to correct word alignment errors, and uses word alignment information to correct syntactic parsing errors. Riesa and Marcu (2010) and Riesa, Irvine, and Marcu (2011) use a large number of syntactic features from source and target language syntax to perform word alignment in a discriminative machine learning framework. Wang and Zong (2013) use dependency structure to constrain word alignment in a generative framework. All of these works reco"
J17-3002,D08-1022,0,0.0244661,"Penn TreeBank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994), syntactic parsing accuracy for other languages is considerably lower (Wang and Xue 2014). In addition, even for English, there is considerable performance degradation when the data that needs to be parsed is different from the Wall Street Journal newswire articles that the parsers are generally trained on. Syntax-based 522 Deng and Xue Chinese–English Translation Divergences SMT systems are only competitive when used in conjunction with techniques such as packed forests, which relax the need to use one-best parses (Mi and Huang 2008; Mi, Huang, and Liu 2008). The second challenge is that the constraints imposed by the syntactic structures have been shown to be too stringent, and prevent useful syntaxbased translation rules that do not obey syntactic constituent boundaries from being used in SMT models and hurt MT performance (Koehn 2009). Effective techniques have been developed to address this issue by identifying heuristics to relax the constraints of syntactic structures when extracting translation rules (Zollmann and Venugopal 2006). As the gain of incorporating syntactic information has plateaued, the field is poise"
J17-3002,P08-1023,0,0.0762262,"Missing"
J17-3002,E99-1010,0,0.172144,"Missing"
J17-3002,C02-2025,0,0.0531969,"semantic analyzers that are more difficult to develop than syntactic parsers. The development of the AMR Bank (Banarescu et al. 2013) is the latest attempt in that direction, although it is important to point out that the significance of such semantically annotated corpora goes far beyond the narrow purpose of MT, and that the AMR Bank is neither the first nor the only attempt to develop semantically annotated resources that can be used for MT purposes. Similar efforts include a series of head-driven phrase structure grammar– based resources (e.g., LingGO Redwoods Treebank and the DeepBank) (Oepen et al. 2002; Flickinger, Zhang, and Kordoni 2012; Bender et al. 2015; Flickinger, Oepen, and Bender 2017), the semantic layers of which are based on Minimal Recursion Semantics (MRS) (Copestake et al. 2005), a semantic representation framework that has been adopted in modern semantic transfer-based MT systems (Lønning et al. 2004). As the field prepares to take this next step of incorporating semantic representation into the SMT paradigm, it is worth asking: i) Are translation divergences properly represented by the kind of syntax-based translation rules such as hierarchical phrase pairs used in current"
J17-3002,2007.tmi-papers.18,0,0.0785901,"Missing"
J17-3002,N07-1051,0,0.128959,"Missing"
J17-3002,D11-1046,0,0.0425392,"Missing"
J17-3002,P10-1017,0,0.0186352,"(Wu 1997). DeNero and Klein (2007) demonstrate how word alignments that do not respect the constituent structure of the target sentence hinder the extraction of generalizable translation rules and propose an unsupervised word alignment model that takes into account the constituent structure of the target sentence. May and Knight (2007) use rules extracted from a syntax-based MT model to re-align the words in a sentence pair. Fossum (2010) reports work in which she uses syntactic features to correct word alignment errors, and uses word alignment information to correct syntactic parsing errors. Riesa and Marcu (2010) and Riesa, Irvine, and Marcu (2011) use a large number of syntactic features from source and target language syntax to perform word alignment in a discriminative machine learning framework. Wang and Zong (2013) use dependency structure to constrain word alignment in a generative framework. All of these works recognize the need to use syntactic structure to influence word alignment (or vice versa), but, typically, work on automatic word alignment focuses on statistical modeling and does not discuss the linguistic basis of word alignment and how it interacts with syntactic structure, perhaps wi"
J17-3002,P08-1066,0,0.0649872,"Missing"
J17-3002,2005.mtsummit-posters.4,0,0.0789315,"Missing"
J17-3002,2005.eamt-1.33,0,0.0884852,"Missing"
J17-3002,2006.eamt-1.2,0,0.0268255,"3). 558 Deng and Xue Chinese–English Translation Divergences handle “syntactic divergences” such as word order differences, but “lexical-semantic divergences” (caused by how certain meaning is encoded lexically) often need to be tackled with additional semantic transfer rules. The challenges that translation divergences pose for interlingua- and semantic transfer–based approaches are similar, and so are the solutions. For example, Dorr (1994) introduced a set of markers in her Lexical Conceptual Structure lexicon to indicate how specific lexical semantic concepts should be mapped or realized. Stymne and Ahrenberg (2006) also introduced additional rules within the MRS framework to handle certain lexical-semantic divergences between English and Swedish. We investigate translation divergence in a very different time, when the use of largescale parallel corpora in SMT is the standard practice, and we semi-automatically extract instances of translation divergence from a parallel corpus annotated based on a carefully designed hierarchical alignment scheme that preserves the integrity of lexical dependencies (which can alternatively be viewed as constructions or patterns). That makes it possible for us to exhaustiv"
J17-3002,P10-1032,0,0.0594145,"Missing"
J17-3002,2007.mtsummit-papers.62,0,0.214162,"s and syntactic structures. For an illustration of the points made here, see the discussion of the concrete example in Figure 1. Next let us introduce our annotation procedure. Our annotators are presented with sentence pairs that come with parallel parse trees. The task of the annotator is to decide, first on the word level and then on the phrase level, if a word or phrase needs to be aligned at all, and if so, to which word or phrase it should be aligned. The decisions about word alignment and phrase alignment are not independent, and must obey the well-formedness constraints as outlined in Tinsley et al. (2007): A. A non-terminal node can only be aligned once. B. If node nc is aligned to node ne , then all the descendants of nc can only be aligned to the descendants of ne . C. If node nc is aligned to node ne , then all the ancestors of nc can only be aligned to the ancestors of ne . This means that once a word alignment is in place, it puts constraints on phrase alignments. A pair of non-terminal nodes (nc , ne ) cannot be aligned if a word that is a descendant of nc is aligned to a word that is not a descendant of ne on the word level. Let us use the concrete example in Figure 1 to illustrate the"
J17-3002,P14-1069,1,0.905861,"Missing"
J17-3002,Q13-1024,0,0.0135403,"ervised word alignment model that takes into account the constituent structure of the target sentence. May and Knight (2007) use rules extracted from a syntax-based MT model to re-align the words in a sentence pair. Fossum (2010) reports work in which she uses syntactic features to correct word alignment errors, and uses word alignment information to correct syntactic parsing errors. Riesa and Marcu (2010) and Riesa, Irvine, and Marcu (2011) use a large number of syntactic features from source and target language syntax to perform word alignment in a discriminative machine learning framework. Wang and Zong (2013) use dependency structure to constrain word alignment in a generative framework. All of these works recognize the need to use syntactic structure to influence word alignment (or vice versa), but, typically, work on automatic word alignment focuses on statistical modeling and does not discuss the linguistic basis of word alignment and how it interacts with syntactic structure, perhaps with the exception of Hermjakob (2009), who recognizes the difficulty of aligning what he calls “orphan” function words, words that do not have an equivalent in the other language. He ultimately adopts a G-TAHS, w"
J17-3002,J97-3002,0,0.772109,"ergences, and does not necessarily cover all possible translation divergences. We do not distinguish “syntactic” and “lexical-semantic” translation divergences, because, as we show in Section 4, the Hiero-style translation rules used in statistical translation models can encapsulate both “syntactic” and “lexical-semantic” translation divergences in a uniform manner, making such a distinction largely irrelevant. An empirical investigation of the translation divergences of the kind we describe here also sheds light on the feasibility of developing synchronous grammars for MT. Early work such as Wu (1997) and (Alshawi, Bangalore, and Douglas 2000) assumes a form of context-free grammar that has very strict restrictions on the types of grammatical rules that are allowed. Eisner (2003) argues for the need to accommodate nonisomorphic syntactic structures in MT systems and proposes using synchronous tree substitution grammars that are collections of pairs of aligned elementary trees as basic units of the grammar. Ding and Palmer (2005) implemented a statistical MT model that uses synchronous dependency insertion grammars, the basic units of which are also elementary trees, but their elementary tr"
J17-3002,C10-2158,1,0.880287,"Missing"
J17-3002,P08-1064,0,0.073824,"Missing"
J17-3002,W12-6306,1,0.802023,"ods that are compatible with treebanks. The need to attach a function word to its head demonstrates the need to refer to syntactic structures, but this has never been systematically considered. 530 Deng and Xue Chinese–English Translation Divergences to Chinese. The English sentences in the data set are annotated based on the original Penn TreeBank (PTB) annotation stylebook (Bies et al. 1995) as well as its extensions (Warner et al. 2004), while the Chinese sentences in the data set are annotated based on the Chinese TreeBank (CTB) annotation guidelines (Xue and Xia 1998) and its extensions (Zhang and Xue 2012). The Parallel Aligned Treebank only has word alignments, which are done under the GTAHS, and no phrase alignments. The main departure of our approach is that we loosen the requirement that every word in a sentence pair needs to be word-aligned.6 On the word level, we only align words that have an equivalent in terms of lexical meaning and grammatical function.7 For words that do not have a translation counterpart, we leave them unaligned and locate the appropriate phrases in which they appear to be aligned. This immediately eliminates the spurious ambiguities discussed for the GTAHS. Because"
J17-3002,W06-3119,0,0.0455648,"conjunction with techniques such as packed forests, which relax the need to use one-best parses (Mi and Huang 2008; Mi, Huang, and Liu 2008). The second challenge is that the constraints imposed by the syntactic structures have been shown to be too stringent, and prevent useful syntaxbased translation rules that do not obey syntactic constituent boundaries from being used in SMT models and hurt MT performance (Koehn 2009). Effective techniques have been developed to address this issue by identifying heuristics to relax the constraints of syntactic structures when extracting translation rules (Zollmann and Venugopal 2006). As the gain of incorporating syntactic information has plateaued, the field is poised to climb up the Vauquois Pyramid (Vauquois 1968) and start exploring the utility of semantic representations for SMT systems, mirroring the progression of earlier rule-based approaches in the previous incarnation of MT research. The hope is that more abstract semantic representations can better address translation divergences than syntactic structures, and this advantage will hopefully offset the potential harm caused by the expected drop in the accuracy of semantic analyzers that are more difficult to deve"
J17-3002,C08-1144,0,0.150488,"ly several years after the introduction of word-based models, phrase-based models were proposed that better use local context and handle the translation of non-compositional phrases to yield superior translation accuracy (Och 1999; Koehn, Och, and Marcu 2003). Hierarchical phrase-based models (Chiang 2005, 2007) further advanced the state of the art by allowing non-terminals in phrase-based translation rules called hierarchical phrase pairs, which effectively capture long-distance lexical dependencies because the yields of the non-terminals are of variable lengths and can be arbitrarily long (Zollmann et al. 2008). Along this main thread, in the last decade there has been intensive research on incorporating syntactic trees produced by syntactic parsers trained on human-annotated treebanks into an SMT model. The attempt to provide syntactic information for SMT models is driven by the widely accepted assumption that word order varies in systematic ways among languages and reordering in a sentence pair often involves syntactic constituents rather than individual words. It is natural then to expect that incorporating syntactic structures into SMT models would lead to improved MT accuracy. Various approache"
J17-3002,W06-3601,0,\N,Missing
J17-3002,xue-etal-2014-interlingua,1,\N,Missing
K15-2001,P09-1075,0,0.0149588,"course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative a"
K15-2001,P12-1007,0,0.0215894,"omputational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the com"
K15-2001,P14-1048,0,0.0131111,"tational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “d"
K15-2001,W12-1622,0,0.047129,"anguage Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain cruci"
K15-2001,P14-1002,0,0.0686762,"re generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learn"
K15-2001,K15-2004,0,0.0849821,"Missing"
K15-2001,P13-2013,0,0.189614,"uly 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard”"
K15-2001,P13-1047,0,0.0359978,"ociation for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based lear"
K15-2001,W14-4320,0,0.0531135,"of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The re"
K15-2001,K15-2006,0,0.17442,"Missing"
K15-2001,P14-1003,0,0.00827391,"or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overvi"
K15-2001,K15-2007,0,0.107459,"Missing"
K15-2001,D09-1036,1,0.729252,"Missing"
K15-2001,P14-5010,0,0.00276217,", 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants to deploy their systems on a remote virtual machine, and to use the TIRA web platform (tira.io) to run their syste"
K15-2001,W11-1901,1,0.0252812,"Nianwen Xue∗ Hwee Tou Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks ("
K15-2001,J93-2004,0,0.058942,"if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, f"
K15-2001,W12-4501,1,0.544745,"Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et"
K15-2001,W04-2703,1,0.817359,"Missing"
K15-2001,W15-0210,1,0.806958,"shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annot"
K15-2001,K15-2010,0,0.14262,"Missing"
K15-2001,prasad-etal-2008-penn,1,0.768574,"course relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or imp"
K15-2001,J05-1004,0,0.00719659,"wo arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, following the minimality principle for selecting all and only t"
K15-2001,J14-4007,1,0.868703,"een discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or implicitly, and takes two"
K15-2001,W12-1614,0,0.349783,"ared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the rela"
K15-2001,E14-1068,1,0.369296,"s. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overview paper is structured as follows. In Section"
K15-2001,N07-1051,0,0.00516358,"like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants t"
K15-2001,K15-2012,0,0.0750505,"Missing"
K15-2001,C08-2022,0,0.230737,"arsing (SDP). In the course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques o"
K15-2001,P09-1077,0,0.665554,"seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared tas"
K15-2001,N09-1064,0,0.0519034,"Missing"
K15-2001,K15-2002,0,0.195871,"Standard “shallow” architectures typically make use of discrete features while neural networks generally use continuous real-valued features such as word and paragraph embeddings. For discourse connective and argument extraction, token level features extracted from a fixed window centered on the target word token are generally used, and so are features extracted from syntactic parses. Distributional representations such as Brown clusters have generally been used to determine the senses (Chiarcos and Schenk, 2015; Devi et al., 2015; Kong et al., 2015; Song et al., 2015; Stepanov et al., 2015; Wang and Lan, 2015; Wang et al., 2015; Yoshida et al., 2015), although one team also used them in the sequence labeling task for argument extraction (Nguyen et al., 2015). Additional resources used by some systems for sense determination include word embeddings (Chiarcos and Schenk, 2015; Wang et al., 2015), VerbNet classes (Devi et al., 2015; Kong et al., 2015), and the MPQA polarity lexicon (Devi et al., 2015; Kong et al., 2015; Wang and Lan, 2015). Table 4 provides a summary of the different approaches. 6 Results Table 5 shows the performance of all participating systems across the three test evaluation sets"
K15-2001,C12-1168,0,0.04565,"c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and"
K15-2001,K15-2014,0,0.132851,"Missing"
K15-2001,K15-2015,0,0.0995344,"Missing"
K15-2001,C10-2172,0,0.24464,"nth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an idea"
K15-2001,W01-1605,0,\N,Missing
K15-2001,K15-2003,0,\N,Missing
K16-2001,K16-2003,0,0.135731,"nvolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a"
K16-2001,K16-2015,0,0.0587712,"6) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitt"
K16-2001,K16-2018,0,0.0493477,"Missing"
K16-2001,P13-2013,0,0.0614398,"unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder, 2016; Wang and Lan, 2016;"
K16-2001,K16-2016,0,0.0652388,"olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn) syntactic parses, Brown clusters none Word embeddings, parse trees,"
K16-2001,K15-2004,0,0.140524,"Missing"
K16-2001,K16-2009,0,0.166348,"IST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labe"
K16-2001,K16-2021,0,0.296275,"lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn)"
K16-2001,K16-2013,0,0.191078,"tional overhead. On TIRA, the blind test set can only be accessed in the Connective identification The identification of discourse connectives is not a simple dictionary lookup as some discourse connective expressions are ambiguous and may function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia T"
K16-2001,K16-2011,0,0.0335294,"Missing"
K16-2001,K16-2017,0,0.107933,"ay function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for i"
K16-2001,K16-2008,0,0.0961919,"Missing"
K16-2001,K16-2022,0,0.0226855,"Missing"
K16-2001,P09-1077,0,0.372145,"ourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder,"
K16-2001,I05-3025,1,0.0947611,"as follows: • News articles were extracted from the Wikinews XML dump2 using the publicly available WikiExtractor.py script.3 • Additional processing was done to remove any remaining XML annotations and produce a raw text version of each article (including its title and date). 4 Evaluation The scorer that computes all of the available evaluation metrics is open-source with some contribution from the participants during the task period6 . • Articles written purely in simplified Chinese were identified using the Dragon Mapper4 Python library, and segmented using the NUS Chinese word segmenter (Low et al., 2005). 4.1 Main evaluation metric: End-to-end discourse parsing 1 https://zh.wikinews.org/ https://dumps.wikimedia.org/zhwikinews/20151020/ zhwikinews-20151020-pages-meta-current.xml.bz2 3 http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 4 http://dragonmapper.readthedocs.io/en/latest/index. html A shallow discourse parser (SDP) is evaluated based on the end-to-end F1 score on a per2 5 6 4 https://www.seas.upenn.edu/~pdtb/tools.shtml#annotator http://www.github.com/attapol/conll16st. Sense Definition Alternative Causation Condition Conjunction Contrast Relation between two alternatives Relation"
K16-2001,P14-5010,0,0.00268188,"nefits from the precise evaluation of the progress and improvement since the system is based off the exact same implementation. • Brown clusters (implementation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and L"
K16-2001,prasad-etal-2008-penn,1,0.839291,"ilingual Shallow Discourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper"
K16-2001,K16-2014,0,0.241056,"Missing"
K16-2001,J14-4007,1,0.889259,"ourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper is structured as follo"
K16-2001,K16-2020,0,0.0356651,"Missing"
K16-2001,K16-2010,0,0.282836,"ources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters n"
K16-2001,K16-2002,0,0.416621,", VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a classifier for each individual discourse connective expression (Oepen et al., 2016), and the other is to train one classifier for all discourse connective expressions (Wang and Lan, 2016; Kong et al., 2015; Laali et al., 2016). Alternatively, connective identification is treated as a token-level sequence labeling task, solved with sequence labeling models like CRF (Stepanov and Riccardi, 2016). Argument extraction Different strategies were used for extracting the arguments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that"
K16-2001,E14-1068,1,0.449193,"ent algorithms and models can be more meaningfully compared. In the open track, the focus of the evaluation is on the overall performance and the use of all possible means to improve the performance of a task. This distinction was easier to maintain for early CoNLL tasks such as noun phrase chunking and named entity recognition, where competitive performance could be achieved without having to use resources other than the provided training set. However, this is no longer true for a high-level task like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants in the closed track to use the following linguistic resources, in addition to the training set: For English, For purposes of evaluation, an explicit discourse connective predicted by a parser is considered correct if and only if the predicted raw connective includes the gold raw connective head, while allowing for the tokens of the pre"
K16-2001,K16-2019,0,0.096966,"Missing"
K16-2001,K16-2007,1,0.844548,"relations. A variety of neural network architectures are represented. (Schenk et al., 2016) used a feedforward neural network, with dependency structures used to re-weight the word embeddings used as input to the network. (Wang and Lan, 2016; Qin et al., 2016) achieved competitive performance using a Convolutional Neural Network architecture for this subtask. Finally, (Weiss and Bajec, 2016) produced competitive results with a focused RNN. Word embeddings were typically used as input to the neural network models and different pooling methods have been used to derive the vectors for arguments. Rutherford and Xue (2016) used simple summation pooling in a feedforward network and achieved competitive performance in classifying implicit discourse relation senses. Relation sense classification All systems have separate classifiers for explicit and implicit discourse connectives. For explicit relations, the discourse connective itself is the best predictor of the discourse relation. Many discourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse rela"
K16-2001,K16-2012,0,0.156471,"Missing"
K16-2001,K15-2002,0,0.156384,"al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discourse relations in a particular context, and for predicting the senses of the discourse relations. Here we briefly summarize the approaches used in each subtask. We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams have been asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. Sta"
K16-2001,K16-2004,0,0.188138,"ves in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes,"
K16-2001,P14-1069,1,0.15687,"ation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discours"
K16-2001,K16-2006,0,0.0402375,"ments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that parallels the PDTB annotation strategy for implicit discourse relations: for each pair of adjacent sentences that do not straddle a paragraph boundary, if an explicit discourse relation does not already exist, posit 8 ID Institution Learning methods Resources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16"
K16-2001,K15-2001,1,0.641779,"prises a long pipeline, and it is hard for teams that do not have a pre-existing system to put together a competitive full system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactical"
K16-2001,P12-1008,1,0.873541,"ull system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactically as a subordinating or coordinating conjunction, or a discourse adverbial. Implicit discourse relations are cases wher"
K16-2007,Q15-1024,0,0.227356,"Missing"
K16-2007,N16-1037,0,0.0821157,"Missing"
K16-2007,P09-1077,0,0.263955,"-Explicit Neural Discourse Parser in English and Chinese Attapol T. Rutherford∗ Yelp San Francisco, CA, USA teruth@yelp.com Nianwen Xue Brandeis University Waltham, MA, USA xuen@brandeis.edu Abstract bel sets and different languages. Neural network is attractive in this regard as we do not need handcrafted linguistic resources, which are not readily available in all languages. The past neural network models for this task focus on top-level senses (Ji et al., 2016) or require parses (Ji and Eisenstein, 2015), redundant surface features (Rutherford and Xue, 2014), or extensive semantic lexicon (Pitler et al., 2009). The results from these systems are not likely to extend to languages that do not have as much linguistic resources as English. Therefore, we come up with a neural network model that requires no parses and specific model tuning. The only extra ingredient is word vectors, which are easily obtained through large amount of unannotated data. Our past studies have indicated that feedforward neural networks outperform more complicated models such as long-short term memory models and perform comparably with systems with traditional surface features in this task (Rutherford et al., 2016). But we want"
K16-2007,E14-1068,1,0.779048,"are already made available 1 . Baseline The winning system from last year’s task serves as a strong baseline for English. We choose this system because it represents one of the strongest systems that utilizes exclusively surface features and extensive semantic lexicon (Wang and Lan, 2015). This approach uses a MaxEnt model loaded with millions of features. We use Brown cluster pair features as the baseline for Chinese as there is no previous system for Chinese. We use 3,200 clusters to create features and perform feature selection on the development set based on the information gain criteria (Rutherford and Xue, 2014). We end up with 10,000 features total. Model description The Arg1 vector a1 and Arg2 vector a2 are computed by applying element-wise pooling function 1 f on all of the N1 word vectors in Arg1 w1:N and 1 2 all of the N2 word vectors in Arg2 w1:N2 respectively: a1i = a2i = N X j=1 N X j=1 Experiments 1 wj,i 2 wj,i Inter-argument interaction is modeled directly by the hidden layers that take argument vectors as features. Discourse relations cannot be determined based on the two arguments individually. Instead, the sense of the relation can only be determined when the arguments in a discourse rel"
K16-2007,K15-2002,0,0.0987526,"a et al., 2010; Bastien et al., 2012). The gradient computation is done with symbolic differentiation, a functionality provided by Theano. The models are trained on CPUs on Intel Xeon X5690 3.47GHz, using only a single core per model. The models converge in minutes. The implementation, the training script, and the trained model are already made available 1 . Baseline The winning system from last year’s task serves as a strong baseline for English. We choose this system because it represents one of the strongest systems that utilizes exclusively surface features and extensive semantic lexicon (Wang and Lan, 2015). This approach uses a MaxEnt model loaded with millions of features. We use Brown cluster pair features as the baseline for Chinese as there is no previous system for Chinese. We use 3,200 clusters to create features and perform feature selection on the development set based on the information gain criteria (Rutherford and Xue, 2014). We end up with 10,000 features total. Model description The Arg1 vector a1 and Arg2 vector a2 are computed by applying element-wise pooling function 1 f on all of the N1 word vectors in Arg1 w1:N and 1 2 all of the N2 word vectors in Arg2 w1:N2 respectively: a1i"
K16-2007,K15-2001,1,0.863867,"rt results on the English blind test set, which is used as the main criteria in this competition. 1 Introduction In the context of CoNLL 2016 Shared Task, we participate partially in the English and Chinese supplementary evaluation, which is discourse relation sense classification (Xue et al., 2016). We focus on identifying the sense of non-explicit discourse relations in both English and Chinese. Previous studies including the results from CoNLL 2015 Shared Task have shown that classifying the senses of implicit discourse relations is the most difficult part of the task of discourse parsing (Xue et al., 2015). Therefore, we focus exclusively on this particular challenging subtask. We want our system to be robust such that the system can be easily trained to handle different la∗ Work performed while being a student at Brandeis 55 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 55–59, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 3 Our system is ranked the first on the English dataset and the third on the Chinese dataset. The accuracy on the English blind test set is 0.3767, and the accuracy on the Chinese blind te"
K16-2007,K16-2001,1,0.852425,"nd simple feed-forward training procedure, which we have previously shown to work better than some of the more sophisticated neural architecture such as long-short term memory model. Our Chinese model outperforms feature-based model and performs competitively against other teams. Our model obtains the state-of-the-art results on the English blind test set, which is used as the main criteria in this competition. 1 Introduction In the context of CoNLL 2016 Shared Task, we participate partially in the English and Chinese supplementary evaluation, which is discourse relation sense classification (Xue et al., 2016). We focus on identifying the sense of non-explicit discourse relations in both English and Chinese. Previous studies including the results from CoNLL 2015 Shared Task have shown that classifying the senses of implicit discourse relations is the most difficult part of the task of discourse parsing (Xue et al., 2015). Therefore, we focus exclusively on this particular challenging subtask. We want our system to be robust such that the system can be easily trained to handle different la∗ Work performed while being a student at Brandeis 55 Proceedings of the 54th Annual Meeting of the Association"
K19-2001,W13-0101,1,0.863351,"or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanchored. The UCCA graph for the running example (see the bottom of Figure 2) includes a single scene, whose main relation is the Process (P) evoked by apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such t"
K19-2001,D15-1198,0,0.0255627,"9 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating"
K19-2001,K19-2008,0,0.403223,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,D17-1130,0,0.100948,"Missing"
K19-2001,W13-2322,0,0.179348,"aphs need not be rooted trees: Argument sharing across units will give rise to reentrant nodes much like in the other frameworks. For example, technique in Figure 2 is both a Participant in the scene evoked by similar and a Center in the parent unit. UCCA in principle also supports implicit (unexpressed) units which do not correspond to any tokens, but these are currently excluded from parsing evaluation and, thus, suppressed in the UCCA graphs distributed in the context of the shared task. Abstract Meaning Representation Finally, the shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchor6 DM PSD EDS UCCA AMR Flavor 0 0 1 1 2 TRAIN Text Type Sentences Tokens newspaper 35,656"
K19-2001,S16-1176,0,0.0127804,"ition-based, as well as sequence-to-sequence systems. Existing 19 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target"
K19-2001,basile-etal-2012-developing,0,0.0638169,"Missing"
K19-2001,W15-0128,1,0.854139,"ing example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first recast as Elementary Dependency Structures (EDS; Oepen and Lønning, 2006; see below) and then further simplified into pure bi-lexical semantic dependencies, dubbed DELPH-IN MRS Bi-Lexical Dependencies (or DM). As a Flavor (0) framework, graph nodes in DM are restricted to surface tokens. But DM graphs are neither lexica"
K19-2001,D16-1134,1,0.82449,"y Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior"
K19-2001,P17-1112,0,0.291422,"vich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment a"
K19-2001,P13-2131,0,0.502571,"range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design shies away from such formal purit"
K19-2001,K19-2006,0,0.10424,"Missing"
K19-2001,K19-2013,0,0.128351,"Missing"
K19-2001,W11-2927,1,0.84568,"quely determined as the character range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design sh"
K19-2001,P12-2074,1,0.629774,"and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid overlap of morpho-syntactic training data with the texts underlying the semantic graphs of the shared task, we performed five-fold jack-knifing on the WSJ and EWT corpora. For compatibility with the majority of the training data, the ‘raw’ input strings for the MRP semantic graphs were tokenized using the PTB-style REPP rules of Dridan and Oepen (2012) and input to UDPipe in pre-tokenized form. Whether as merely a source of state-of-the-art PTBstyle tokenization, or as a vantage point for approaches to meaning representation parsing that start from explicit syntactic structure, the optional morpho-syntactic companion data offers community value in its own right. respectively; see Table 2. The shared task has, for the first time, repackaged the five graph banks into a uniform and normalized abstract representation with a common serialization format. The common interchange format for semantic graphs implements the abstract model of Kuhlmann a"
K19-2001,K19-2007,0,0.24925,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,K19-2015,0,0.0373872,"Missing"
K19-2001,P18-1038,0,0.0751759,"recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment as a preprocessing step during training. They developed their own rule-based alignment method, complemented by Pourdamghani et al. (2014),"
K19-2001,K19-2016,0,0.133819,"Missing"
K19-2001,S14-2080,0,0.0289716,"lti-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results"
K19-2001,S15-2154,0,0.216771,"Missing"
K19-2001,N18-2020,1,0.780781,"iversal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanch"
K19-2001,S16-1186,0,0.0968157,"Missing"
K19-2001,N18-1104,0,0.0451061,"iding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by usi"
K19-2001,P14-1134,0,0.12208,"of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into the sum of the scores of its nodes and edges. A popular choice for predicting the edge is to feed the output of an LSTM encoder to a biaffine classifier to predict if an edge exists between a pair of nodes as well as the label of the edge (SJTU– NICT, SUDA–Alibaba, Hitachi, and JBNU), with slight variations as to the input to the LSTM encoder. Due to the difference in anchoring betw"
K19-2001,S15-2161,0,0.0179913,"and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text"
K19-2001,kingsbury-palmer-2002-treebank,0,0.377337,"iyadarshi, 2017). The AMR example graph in Figure 3 has a topology broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 3 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 2. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 3 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
K19-2001,hajic-etal-2012-announcing,0,0.575983,"Missing"
K19-2001,P19-4002,1,0.832307,"listed. 12 tial submissions declined the invitation to submit a system description for publication in the shared task proceedings (and one team asked to remain anonymous), such that only limited information is available about these parsers, and they will not be considered in further detail in §7. Finally, based on input by task participants, Table 4 also provides an indication of which submissions employed multi-task learning (MTL) and a high-level characterization of the overall parsing approach. The distinction between transition-, factorization-, and composition-based architectures follows Koller et al. (2019) and is discussed in more detail in §7 below. In some submissions there can of course be elements of more than one of these high-level architecture types. Also, not all of the teams who indicate the use of multi-task learning actually apply it across different semantic graph frameworks, but in some cases rather to multiple sub-tasks within the parsing architecture for a single framework.11 The main task results are summarized in Table 6, showing average MRP scores across frameworks, broken down by the different component pieces (see §5 above). These cross-framework averages can only be meaning"
K19-2001,P17-1104,1,0.901739,"e, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BEL in the version of TUPA used in the shared task. CUHK developed a transition-based parser with a general transition system suited for all five frameworks, by including a variable-arity R ESOLVE action. F"
K19-2001,P17-1014,0,0.0341005,"ly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the s"
K19-2001,P18-1035,1,0.82103,"rticipants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; or Stanovsky and Dagan, 2018). The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the training and evaluation data for the task, packaged in a uniform graph abstraction and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of extra training data,"
K19-2001,K19-2011,0,0.145598,"3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by the symbol “∦”), arrived after the closing deadli"
K19-2001,S19-2001,1,0.68669,"ng representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-level analysis beyond surfac"
K19-2001,J16-4009,1,0.942174,"ng (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning 1 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 1–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2001 tations across frameworks, the MRP 2019 shared task is regrettably limited to parsing English for the time being. 2 A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. The following paragraphs provide semi-formal definitions of core graph-theoretic concepts that can be meaningfully applied across the range of frameworks represented in the shared task. Hierarchy of Formal Flavors"
K19-2001,K19-2002,1,0.70568,"ing parser outputs for all target frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission,"
K19-2001,K19-2010,0,0.116608,"Missing"
K19-2001,K19-2004,0,0.0682072,"Missing"
K19-2001,W12-3602,1,0.783003,"NNS n rice NN n ADDR.m ACT ADDR.m PAT RSTR technique NN as IN p conj ADDR.m PAT similar JJ ARG2 ADDR.m EXT be almost VBZ RB ev-w218f2 impossible JJ RSTR apply other VB JJ ev-w119f2 CONJ.m APPS.m crop NNS as IN APPS.m cotton NN CONJ.m soybean NNS CONJ.m and CC rice NN Figure 1: Bi-lexical semantic dependencies for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first"
K19-2001,W19-3304,1,0.80829,"dicated in each cell: ShanghaiTech, for example, ranks much higher in the framework-specific SDP metric than in the official MRP ranks. These divergences likely reflect the more limited scope of the SDP approach to scoring, which essentially only considers labeled edges (and top nodes, as a pseudo-edge) but ignores node labels, properties, and anchors (which all used to be provided as part of the parser inputs in the original SDP parsing tasks; see §3 above). trastive, phenomena-oriented studies would likely be called for, as for example the comparison of parsing accuracies for EDS vs. AMR by Lin and Xue (2019). 7 Overview of Approaches The participating systems in the shared task have approached this multi-meaning representation task in a variety of ways, which we characterize into three broad families of approaches: transition-, factorization-, or composition-based architectures. Transition-Based Architectures In these parsing system, the meaning representation graph is generated via a series of actions, in a process that is very similar to dependency tree parsing, with the difference being that the actions for graph parsing need to allow reentrancies, as well as (possibly) non-token nodes, labels"
K19-2001,S19-2002,0,0.0756992,"Missing"
K19-2001,P19-1450,0,0.171664,"used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results on PSD. Lindemann et al. (2019) trained a composition-based parser on DM, PAS, PSD, AMR and EDS, using the Apply–Modify algebra, on which the Saarland submission to the shared task is based. They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. UCCA parsing was first tackled by Hershcovich et al. (2017), who used a neural transition-based parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared t"
K19-2001,S17-2156,0,0.033905,"Missing"
K19-2001,P18-1037,0,0.260682,"le anchoring in the parsing system has a significant impact on parser performance. Some of the participating systems follow early approaches in AMR parsing and use a separate ‘alignment’ model to provide hard anchorings and then proceed with the rest of the parsing process (e.g. the HIT-SCIR system) assuming the alignments are already in place. Other submissions use a soft alignment component that is trained jointly with other components of their systems. For example, the Amazon and the SUDA– Alibaba parsers jointly model anchoring, node detection, and edge detection, adopting the approach of Lyu and Titov (2018), while the SJTU–NICT system uses a sequence-to-sequence model with a pointer-generator network to predict the concepts in AMR, following Zhang et al. (2019a). That sequence-to-sequence model is trained jointly with other components of their system. Benefits of Multi-Task Learning Another research question the shared task seeks to advance is whether and how multi-task learning (MTL) helps with multi-framework meaning representation parsing. The term, in fact, seems to be applied somewhat variably in the system descriptions. In one sense, it is equated with traditional joint learning, where dif"
K19-2001,W03-3017,0,0.384782,"t and a buffer for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the n"
K19-2001,J93-2004,0,0.0674913,"DM are restricted to surface tokens. But DM graphs are neither lexically fully covering nor rooted trees, i.e. some tokens do not contribute to the graph, and for some nodes there are multiple incoming edges. In the example DM graph in Figure 1, technique semantically depends on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacusection reviews the frameworks and presents example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 3 are presented in order of (arguably) increasing ‘abstraction’ from the surface string, i.e. ranging from ordered Flavor (0) to unanchored Flavor (2). Two of the frameworks in the shared task present simplifications into bi-lexical semantic dependencies (i."
K19-2001,S16-1166,0,0.537725,"‘balkanization’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate targe"
K19-2001,W17-7306,0,0.0913721,"Missing"
K19-2001,S17-2090,0,0.515681,"tion’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-l"
K19-2001,K19-2003,1,0.784391,"rget frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission, the extra training data is li"
K19-2001,P13-2017,0,0.074515,"Missing"
K19-2001,S15-2153,1,0.900924,"Missing"
K19-2001,E06-1011,0,0.0181086,"of the Saarland parser, the lexical items are produced by a BiLSTM-based supertagger, and the best derivation is selected in a tree dependency parsing process where the edge between a head and its argument or modifier is labeled with the derivation operation. In the case of the Peking system, the SHRG rules are extracted with a context-free parser, and the derivation is scored by a sum of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into t"
K19-2001,S14-2008,1,0.915565,"Missing"
K19-2001,S14-2056,1,0.708772,"g covert quantifiers (e.g. on bare nominals, labeled udef q3 ), the two-place such+as p relation, as well as the implicit conj(unction) relation (which reflects recursive decomposition of the coordinate structure Prague Semantic Dependencies Another instance of simplification from richer syntacticosemantic representations into Flavor (0) bi-lexical semantic dependencies is the reduction of tectogrammatical trees (or t-trees) from the linguistic school of Functional Generative Description (FGD; Sgall et al., 1986; Hajiˇc et al., 2012) into what are called Prague Semantic Dependencies (or PSD). Miyao et al. (2014) sketch the nature of this conversion, which essentially collapses empty (or generated, in FGD terminology) t-tree nodes with corresponding surface nodes and forward-projects incoming dependencies onto all members of paratactic constructions, e.g. the appositive and coordinate structures in the bottom of Figure 1. The PSD graph for our running example has many of the same dependency edges as the DM one (albeit using a different labeling scheme and inverse directionality in a few cases), but it analyzes the predicative copula as semantically contentful and does not treat almost as ‘scoping’ ove"
K19-2001,K19-2009,0,0.0353841,"Missing"
K19-2001,N12-2006,0,0.0317062,"parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014),"
K19-2001,P18-1173,0,0.0339442,"Missing"
K19-2001,N18-1135,0,0.164463,"Missing"
K19-2001,K19-2012,1,0.876098,"Missing"
K19-2001,W15-3502,1,0.870363,"y apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 3 has a topology broa"
K19-2001,D14-1048,0,0.0260749,"quences of ‘raw’ sentence strings and (b) in pretokenized, part-of-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premiumquality English morpho-syntactic analyses were provided to participants, described in more detail below. These parser outputs are referred to as the MRP 2019 morpho-syntactic companion trees. Additional companion data available to participants includes automatically generated reference anchorings (commonly called ‘alignments’ in AMR parsing) for the AMR graphs in the training data, obtained from the JAMR and ISI tools of Flanigan et al. (2016) and Pourdamghani et al. (2014), respectively. Because some of the semantic graph banks involved in the shared task had originally been released by the Linguistic Data Consortium (LDC), the training data was made available to task participants by the LDC under no-cost evaluation licenses. Upon completion of the competition, all task data (including system submissions and evaluation results) are being prepared for general release through the LDC, while those subsets that are copyright-free will also become available for direct, open-source download. Additional Resources For reasons of comparability and fairness, the shared t"
K19-2001,P18-1016,1,0.787871,"hnique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Elementary Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of"
K19-2001,L16-1376,0,0.145661,"Missing"
K19-2001,I05-2038,0,0.0146077,"he full novel have long served as a common reference point for AMR, and gold-standard DM and EDS graphs could be converted from the ERS inter-annotator agreement study by Bender et al. (2015). For PSD and UCCA, the 100-sentence subset used for MRP evaluation has been annotated specifically for the shared task. 5 See http://svn.nlpl.eu/mrp/2019/public/ resources.txt for the full list of seventeen generally available third-party resources, including a broad range of large English corpora and distributed word representations. 8 Corpus, as well as to the PTB-style annotations of the GENIA Corpus (Tateisi et al., 2005). This conversion targets Universal Dependencies (UD; McDonald et al., 2013; Nivre, 2015) version 2.x, so that the resulting gold-standard annotations could be concatenated with the UD English Web Treebank (Silveira et al., 2014), for a total of 2.2 million tokens annotated with lemmas, Universal and PTBstyle parts of speech, and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid ove"
K19-2001,silveira-etal-2014-gold,0,0.0451563,"Missing"
K19-2001,N18-2040,1,0.872943,"Missing"
K19-2001,P19-1446,0,0.0232282,"Missing"
K19-2001,D17-1129,1,0.913091,"Missing"
K19-2001,N15-1040,1,0.925482,"Missing"
K19-2001,K19-2005,0,0.0368897,"Missing"
K19-2001,W03-3023,0,0.0220925,"r for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BE"
K19-2001,P19-1009,0,0.3054,"Missing"
K19-2001,D19-1392,0,0.220016,"Missing"
K19-2001,K19-2014,0,0.380663,"rov 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by t"
L16-1145,W13-2322,1,0.860349,"Missing"
L16-1145,bies-etal-2014-incorporating,1,0.844507,"ish Treebank within BOLT improved annotation quality by adding several rounds of Quality Control (QC) to the annotation process. The first QC process consists of a series of specific searches for approximately 200 types of potential inconsistencies and parser or annotation errors. Any errors found in these searches were hand corrected. An additional QC process then identifies repeated text and structures, and flags non-matching annotations. Identified annotation errors are also manually corrected. A special effort for English translation Treebank is the annotation of alternative translations (Bies et al. 2014). Both literal and fluent translation alternates are annotated for word-level tokenization and part-of-speech, whereas only the fluent translation alternates are annotated as part of the syntactic structure of the tree. 3.1.2 Arabic Treebank Arabic Treebank started with Penn Arabic Treebank guidelines, enhanced first with the GALE (Global Autonomous Language Exploitation) project and now with the BOLT project (Maamouri et al. 2011). The two efforts of Arabic Treebank within the GALE project are enhancement of Penn style Treebank annotation (Maamouri & Bies, 2010) and the creation of CATiB (Col"
L16-1145,bonial-etal-2014-propbank,1,0.859958,"or challenge for Arabic was with special features of the Egyptian dialect. It was sometimes very difficult to decide if an MSA word form in the dialect had an equivalent meaning or a slightly different meaning. Additionally, as a pro-drop language, Arabic poses special issues for coreference. Co-reference chains creation problems were initially solved by manual annotation but are now able to be created during post-processing. The English PropBank has focused on expanding predicate annotation beyond the verb and is now annotating verbs, eventive nouns, adjectives, and light verb constructions (Bonial et al. 2014). In English, light verbs are semantically bleached verbs. The argument structure for light verb and non-light verb instances are different. In light verb constructions, the real predicate is generally the nominalized predicate that the light verb supports. A major focus for English PropBank has been to unify FrameFiles across these different parts of speech. This means that the frame used for 'bathe' is always identical to that used for 'bath'. The goal of this expansion is to provide event semantic representations for the entire sentence, specifically pieces most often missed when looking so"
L16-1145,cotterell-callison-burch-2014-multi,0,0.0685218,"Missing"
L16-1145,W10-0702,0,0.0308468,"nd facilitating linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2010-enriching,1,0.823206,"ting linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2012-parallel,1,0.828457,"-9 (-NONE- *T*))))))) (. .)) ) POS annotation is stored in .pos files, where POS tags are attached to tokenized/segmented word units, each sentence per line, as shown in the following example. 好_VA 的_SP 呗_SP ，_PU 来_VV 的话_SP 打_VV 我 _PN 电话_NN 就_AD 可以_VV ，_PU 或者_CC 报_VV 我_PN 名字_NN ，_PU 我_PN 定_VV 了_AS 包厢_NN 3.2 Word Alignment 3.2.1 Alignment Approach Word alignment guidelines are developed based on guidelines for the Blinker and ARCADE projects, enriched during GALE by adding tagging guidelines (Li et al. 2010), and further enhanced to tackle new genres and language features for the BOLT project (Li et al. 2012). The alignment annotation involves a process of 2-pass annotation plus one round of cross-file checking. The initial alignment by junior annotators goes through quality control by senior annotators, and is then followed by a cross-file check by lead annotators for consistency. The word alignment tool is developed by LDC (Figure 3), allowing annotators to align source and translation words as well as to label both alignment links and individual words. Alignment is performed on two pairs of languages: Chinese-English and Egyptian-English. Chinese alignment is performed at two levels: character-"
L16-1145,maamouri-etal-2014-developing,1,0.87281,"Missing"
L16-1145,N13-1039,0,0.101377,"Missing"
L18-1490,D08-1073,0,0.0842389,"to each other, an automatic system has to make a similar choice to predict the temporal relations between either all pairs of events and time expressions, or only a subset of the temporal  relations. If it chooses to do the former, there will be n2 pairs for n events and time expressions. Not only is this computationally expensive, there could be conflicting predictions due to the transitivity of temporal relations (e.g. “A before B” and “B before C” imply “A before C”, which a pair-wise approach may make conflicting predictions) and additional steps are necessary to resolve such conflicts (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). We propose a novel annotation approach to address this dilemma. Specifically we propose to build a dependency tree structure for the entire document where the nodes of the tree are events and time expressions, as well as a few pre-defined “meta” nodes that are not anchored to a span of text in the document The building blocks of this dependency structure are pairs of events and time expressions in which the child event/time expression depends on its parent event/time expression for its temporal interpretation. The dependency relation is based on the"
L18-1490,D12-1062,0,0.0193583,"ilar choice to predict the temporal relations between either all pairs of events and time expressions, or only a subset of the temporal  relations. If it chooses to do the former, there will be n2 pairs for n events and time expressions. Not only is this computationally expensive, there could be conflicting predictions due to the transitivity of temporal relations (e.g. “A before B” and “B before C” imply “A before C”, which a pair-wise approach may make conflicting predictions) and additional steps are necessary to resolve such conflicts (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). We propose a novel annotation approach to address this dilemma. Specifically we propose to build a dependency tree structure for the entire document where the nodes of the tree are events and time expressions, as well as a few pre-defined “meta” nodes that are not anchored to a span of text in the document The building blocks of this dependency structure are pairs of events and time expressions in which the child event/time expression depends on its parent event/time expression for its temporal interpretation. The dependency relation is based on the well-established notion of temporal anapho"
L18-1490,P12-1010,0,0.432414,"rent types of time expressions in depth and propose a novel definition, as far as we know, for the reference time of a time expression (§3.2.1.). • We produce an annotate corpus with this temporal structure that covers two very different genres, news and narratives and achieved high inter-annotator In the next few sections, we will briefly discuss related work (§2.), describe our annotation scheme (§3.), and present our annotation experiments (§4.). We summarize our work in §5. 2. Related Work Using a dependency structure to represent temporal relations in a document has been proposed before (Kolomiyets et al., 2012), but our work is more comprehensive and linguistically grounded in the following ways. First, their dependency structure is based on events, to the exclusion of time expressions. Time expressions are a strong source of temporal location information for events and excluding them will result in incomplete temporal structures. We cover both events and time expressions to form a complete temporal structure for a text. Second, they exclude stative events such as modalized events, while we provide a more complete temporal structure that include stative events. Third, although they link events in a"
L18-1490,W16-1007,0,0.217067,"e NLP community in recent years. Most of the research attention has been devoted to defining the “semantic” aspect of this problem – the identification of a set of semantic relations between pairs of events, between an event and a time expression, or between pairs of time expressions. Representative work in this vein includes TimeML (Pustejovsky et al., 2003a), a rich temporal relation markup language that is based on and extends Allen’s Interval Algebra (Allen, 1984). TimeML has been further enriched and extended for annotation in other domains (O’Gorman et al., 2016; Styler IV et al., 2014; Mostafazadeh et al., 2016). Corpora annotated with these schemes (Pustejovsky et al., 2003b; O’Gorman et al., 2016) are shown to have stable inter-annotator agreements, validating the temporal relations proposed in the TimeML. Through a series of TempEval shared tasks (Verhagen et al., 2007a; Verhagen et al., 2010a; UzZaman et al., 2012; Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017), there has also been significant amount of research on building automatic systems aimed at predicting temporal relations. Less attention, however, has been given to the “structural” aspect of temporal relation modeling –"
L18-1490,W16-5706,0,0.068055,"Missing"
L18-1490,S07-1014,0,0.338044,"me expressions. Representative work in this vein includes TimeML (Pustejovsky et al., 2003a), a rich temporal relation markup language that is based on and extends Allen’s Interval Algebra (Allen, 1984). TimeML has been further enriched and extended for annotation in other domains (O’Gorman et al., 2016; Styler IV et al., 2014; Mostafazadeh et al., 2016). Corpora annotated with these schemes (Pustejovsky et al., 2003b; O’Gorman et al., 2016) are shown to have stable inter-annotator agreements, validating the temporal relations proposed in the TimeML. Through a series of TempEval shared tasks (Verhagen et al., 2007a; Verhagen et al., 2010a; UzZaman et al., 2012; Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017), there has also been significant amount of research on building automatic systems aimed at predicting temporal relations. Less attention, however, has been given to the “structural” aspect of temporal relation modeling – answering the question of which other events or time expressions a given time expression or event depends on for the interpretation of its temporal location. Having an answer to this question is important to both linguistic annotation and computational modeling. F"
L18-1490,J88-2006,0,0.789736,"nodes of the tree are events and time expressions, as well as a few pre-defined “meta” nodes that are not anchored to a span of text in the document The building blocks of this dependency structure are pairs of events and time expressions in which the child event/time expression depends on its parent event/time expression for its temporal interpretation. The dependency relation is based on the well-established notion of temporal anaphora where an event or time expression can only be interpreted with respect to its reference time (Reichenbach, 1947; Partee, 1973; Partes, 1984; Hinrichs, 1986; Webber, 1988; Bohnemeyer, 2009). In each dependency relation in our dependency structure, the parent is the antecedent and the child is the anaphor that depends on its antecedent for its temporal interpretation. Consider the following examples: 1. He arrived on Thursday. He got here at 8:00am. 3098 agreements for each genre. An analysis of the annotated data show that temporal structures are very genre-dependent, a conclusion that has implications for how the temporal structure of a text can be parsed. 2. He arrived at school, walked to his classroom, and then the class began. In (1), the antecedent is “T"
L18-1490,K16-2001,1,0.788778,"ed on the transitive property of temporal relations, but we argue that this dependency structure is an intuitive starting point that makes annotation as well as the computational modeling more constrained and tractable. We annotate a corpus of 235 documents with temporal dependency structures, with 48 documents double-annotated to evaluate inter-annotator agreement. The annotated data are chosen from two different genres, new data from the Xinhua newswire portion of the Chinese TreeBank (Xue et al., 2005) and Wikipedia news data used for CoNLL Shared Task on Shallow Discourse Parsing in 2016 (Xue et al., 2016), and narrative story data from Grimm fairy tales. The two genres are chosen because the temporal structure of texts from those two genres unfolds in very different ways: news reports are primarily in report discourse mode in the sense of (Smith, 2003) while Grimm fairy tales are primarily in narrative mode and time advances in those two genres in very different ways, as we will discuss in more detail in Section 4.2.. We report a stable and high inter-annotator agreement for both genres, which validates the intuitiveness of our approach. This corpus is publicly available.1 The main contributio"
L18-1490,P09-1046,0,0.0318987,"system has to make a similar choice to predict the temporal relations between either all pairs of events and time expressions, or only a subset of the temporal  relations. If it chooses to do the former, there will be n2 pairs for n events and time expressions. Not only is this computationally expensive, there could be conflicting predictions due to the transitivity of temporal relations (e.g. “A before B” and “B before C” imply “A before C”, which a pair-wise approach may make conflicting predictions) and additional steps are necessary to resolve such conflicts (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). We propose a novel annotation approach to address this dilemma. Specifically we propose to build a dependency tree structure for the entire document where the nodes of the tree are events and time expressions, as well as a few pre-defined “meta” nodes that are not anchored to a span of text in the document The building blocks of this dependency structure are pairs of events and time expressions in which the child event/time expression depends on its parent event/time expression for its temporal interpretation. The dependency relation is based on the well-established notion"
L18-1490,D14-1204,1,0.840711,"n, unless stated explicitly, events for us include both eventive and stative situations. Adopting the minimal span approach along the lines of (O’Gorman et al., 2016), only the headword of an event is labeled in actual annotation. Since different events tend to have different temporal behaviors in how they relate to other events or time expressions(Wuyun, 2016), we also assign a coarse event classification label to each event before linking them to other other events or time expressions to form a dependency structure. Adapting the inventory of situation entity types from Smith (2003) and from Zhang and Xue (2014), we define the following eight categories for events. • An Event is a process that happens or occurs. It is the only eventive type in this classification set that advances the time in a text. An example event is “I went to school yesterday”. • A State is a situation that holds during some time interval. It is stative and describes some property or state of an object, a situation, or the world. For example, “she was very shy” describes a state. The remaining event types are all statives that describe an eventive process. • A Habitual event describes the state of a regularly repeating event, as"
L18-1490,S10-1010,0,\N,Missing
L18-1490,S15-2136,0,\N,Missing
L18-1490,S17-2093,0,\N,Missing
li-etal-2012-parallel,maamouri-etal-2008-enhancing,1,\N,Missing
li-etal-2012-parallel,D11-1018,0,\N,Missing
li-etal-2012-parallel,W04-2208,0,\N,Missing
li-etal-2012-parallel,W06-2717,0,\N,Missing
li-etal-2012-parallel,W04-1602,1,\N,Missing
li-etal-2012-parallel,H05-1012,0,\N,Missing
li-etal-2012-parallel,J03-1002,0,\N,Missing
li-etal-2012-parallel,grimes-etal-2012-automatic,1,\N,Missing
li-etal-2012-parallel,li-etal-2010-enriching,1,\N,Missing
li-etal-2012-parallel,W11-4305,0,\N,Missing
li-etal-2012-parallel,N06-1014,0,\N,Missing
N06-1055,P98-1013,0,0.0191521,"s the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and"
N06-1055,W04-2415,0,0.0694211,"Missing"
N06-1055,W04-2412,0,0.124208,"Missing"
N06-1055,W05-0620,0,0.173685,"Missing"
N06-1055,J02-3001,0,0.719986,"of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments"
N06-1055,W05-0623,0,0.13274,"Missing"
N06-1055,W05-0625,0,0.0648602,"ement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments performed on nominalized predicates in Chinese, taking advantage of a newly completed corpus, the Chinese Nombank"
N06-1055,J02-3004,0,0.0525852,"bered) arguments should not have the same semantic role label for Chinese nominalized predicates is not as rigid as it is for English verbs. However further error analysis is needed to substantiate this speculation. 5 Related Work Compared with large body of work on the SRL of verbal predicates, there has been relatively little work done in analyzing the predicate-argument structure of nominalized predicates. There are even less work done for the nominalized predicates for Chinese. (Hull and Comez, 1996) implemented a rule-based system for identifying the arguments for nominal predicates and (Lapata, 2002) has a system that interprets the relation between the head of noun compound and its head, but no meaningful comparison can be made between our work and theirs. Perhaps the closest work to that of ours is that of (Pradhan et al., 2004a), where they reported preliminary work for analyzing the predicate-argument structure of Chinese nominalizations, using a small data set of 437 630 proposition for 22 nominalizations taken from the Chinese Treebank. Since different data sets are used, the results cannot be meaningfully compared. The results reported here for nominalized predicates are consistent"
N06-1055,W03-1025,0,0.0775371,"Missing"
N06-1055,W04-2705,0,0.0780903,"sociation for Computational Linguistics text and its output is then fed into our SRL system. We also discuss whether verb data can be used to improve the SRL accuracy of nominalized predicates. Finally we describe a preliminary experiment that uses reranking techniques to improve the SRL accuracy on hand-crafted parses. Section 5 attempts to put our results in perspective in the context of related work. Section 6 concludes our paper. 2 The Chinese Nombank The Chinese Nombank extends the general annotation framework of the English Proposition Bank (Palmer et al., 2005) and the English Nombank (Meyers et al., 2004) to the annotation of nominalized predicates in Chinese. Like the English Nombank project, the Chinese Nombank adds a layer of semantic annotation to the Chinese TreeBank (CTB), a syntactically annotated corpus of 500 thousand words. The Chinese Nombank annotates two types of elements that are associated with the nominalized predicate: argument-like elements that are expected of this predicate, and adjunct-like elements that modify this predicate. Arguments are assigned numbered labels (prefixed by ARG, e.g., ARG0...ARGn) while adjuncts receive a functional tag (e.g., TMP for temporal, LOC for"
N06-1055,J05-1004,0,0.688314,"le manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005)."
N06-1055,N04-4036,0,0.451303,"ccuracy for nominalized predicates, which showed insignificant improvement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments performed on nominalized predicates in Chine"
N06-1055,N04-1030,0,0.476782,"ccuracy for nominalized predicates, which showed insignificant improvement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments performed on nominalized predicates in Chine"
N06-1055,N04-1032,0,0.0519303,", 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments performed on nominalized predicates in Chinese, taking advantage of a newly completed corpus, the Chinese Nombank (Xue, 2006), which we describe in greater detail in Section 2. The rest of the paper is organized as follows. Section 3 describes the architecture of our system as well as the features we used in our experiments. In Section 4 we describe"
N06-1055,P05-1073,0,0.158802,"wed insignificant improvement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments performed on nominalized predicates in Chinese, taking advantage of a newly completed corpu"
N06-1055,W04-3212,1,0.890129,"predicates, which showed insignificant improvement. 1 Introduction Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al., 1998) and the Propbank (Palmer et al., 2005). It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al., 2003; Pradhan et al., 2004b; Xue and Palmer, 2004; Toutanova et al., 2005; Koomen et al., 2005). It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments performed on nominalized predicates in Chinese, taking advantage of"
N06-1055,xue-2006-annotating,1,0.799456,"It has been the shared task for the CoNLL competition for two consecutive years (Carreras and M`arquez, 2004b; Carreras and M`arquez, 2005). This line of research has also expanded from English to other languages (Sun and Jurafsky, 2004; Xue and Palmer, 2005). So far, however, most of the research efforts have focused on analyzing the predicate-argument structure of verbs, largely due to absence of annotated data for other predicate types. In this paper, we report SRL experiments performed on nominalized predicates in Chinese, taking advantage of a newly completed corpus, the Chinese Nombank (Xue, 2006), which we describe in greater detail in Section 2. The rest of the paper is organized as follows. Section 3 describes the architecture of our system as well as the features we used in our experiments. In Section 4 we describe the experimental setups and report our experimental results. We first present experiments that use hand-crafted parses as input, providing a measurement of how well the Nombank annotation can be bootstrapped from the syntactic structure in the treebank. We then describe a more realistic experimental setup in which an automatic parser is first used to parse unsegmented ra"
N06-1055,C98-1013,0,\N,Missing
N13-1125,H91-1060,0,0.387373,"Missing"
N13-1125,P11-2037,0,0.385243,"pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation. Some ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC detection, where empty categories are first located and typed. The second stage 1051 Proceedings of NAACL-HLT 2013, pages 1051–1060, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics is EC resolution, where empty categories are linked to an overt element if possible. In this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data. More concretely, EC detection involves (i) identifying the position of the EC, relative to some overt word tokens in the same sentence, and (ii) dete"
N13-1125,P04-1082,0,0.718598,"nouns explicit if the target language does not allow pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation. Some ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC detection, where empty categories are first located and typed. The second stage 1051 Proceedings of NAACL-HLT 2013, pages 1051–1060, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics is EC resolution, where empty categories are linked to an overt element if possible. In this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data. More concretely, EC detection involves (i) identifying the position of the EC, relative to som"
N13-1125,P01-1017,0,0.0233831,"the semantic role labeling system is not sufficient to make a difference. class all -Horizontal -Vertical -Gr Cons -V head -Trans -SRL correct 903 827 865 887 891 899 900 prec .653 .627 .652 .646 .651 .654 .657 rec .512 .469 .490 .483 .505 .509 .510 F1 .574 (.561) .536 (.524) .559 (.547) .565 (.552) .569 (.556) .573 (.560) .574 (.561) Table 4: Contribution of feature groups 5 Related Work The work reported here follows a fruitful line of research on EC detection and resolution, mostly in English. Empty categories have initially been left behind in research on syntactic parsing (Collins, 1999; Charniak, 2001) for efficiency reasons, but more recent work has shown that EC detection can be effectively integrated into the parsing process (Schmid, 2006; Cai et al., 2011). In the meantime, both pre-processing and post-processing approaches have been explored in previous work as alternatives. Johnson (2002) has showed that empty categories can be added to the skeletal parses with reasonable accuracy with a simple pattern-matching algorithm in a postprocessing step. Dienes and Dubey (2003b; 2003a) achieved generally superior accuracy using a machine learning framework without having to refer to the synta"
N13-1125,D10-1062,0,0.372301,"of a sentence in both English and Chinese, the prevalence of dropped nouns in Chinese text gives EC detection added significance and urgency. They are not only an important component of the syntactic parse of a sentence, but are also essential to a wide range of NLP applications. For example, any meaningful tracking of entities and events in natural language text would have to include those represented by dropped pronouns. If Chinese is translated into a different language, it is also necessary to render these dropped pronouns explicit if the target language does not allow pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation. Some ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC de"
N13-1125,W03-1005,0,0.697631,", it is also necessary to render these dropped pronouns explicit if the target language does not allow pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation. Some ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC detection, where empty categories are first located and typed. The second stage 1051 Proceedings of NAACL-HLT 2013, pages 1051–1060, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics is EC resolution, where empty categories are linked to an overt element if possible. In this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data. More concretely, EC detection involves (i) identifyi"
N13-1125,P03-1055,0,0.918566,", it is also necessary to render these dropped pronouns explicit if the target language does not allow pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation. Some ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC detection, where empty categories are first located and typed. The second stage 1051 Proceedings of NAACL-HLT 2013, pages 1051–1060, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics is EC resolution, where empty categories are linked to an overt element if possible. In this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data. More concretely, EC detection involves (i) identifyi"
N13-1125,N06-1024,0,0.182991,"ostprocessing step. Dienes and Dubey (2003b; 2003a) achieved generally superior accuracy using a machine learning framework without having to refer to the syntactic structure in the skeletal parses. They described their approach as a pre-processing step for parsing because they only use as features morphosyntactic clues (passives, gerunds and to-infinitives) that can be found in certain function words and partof-speech tags. Even better results, however, were obtained by Campbell (2004) in a postprocessing step that makes use of rules inspired by work in theoretical linguistics. Gabbard et al (2006) reported further improvement largely by recasting the Campbell rules as features to seven different machine learning classifiers. We adopted a machine-learning based postprocessing approach based on insights gained from prior work in English and on Chinese-specific considerations. All things being equal, we believe that a machine learning approach that can exploit partial 1058 information is more likely to succeed than deterministic rules that have to make reference to morphosyntactic clues such as to-infinitives and gerunds that are largely non-existent in Chinese. Without these clues, we be"
N13-1125,J02-3001,0,0.0918687,"ures are designed to exploit the hierarchical structure of the syntactic tree. Our hierarchical features are based on the following observations. An empty category is always located between its left frontier and right frontier, anchored by t and p. Given the lowest common ancestor A of p and t, the right frontier is the path from t to A and the left frontier is the path from the p to A. We also define a path feature from h to t, which constrains the distance between the EC and its head, just as it constrains the distance between a predicate and its argument in the semantic role labeling task (Gildea and Jurafsky, 2002). Given the lowest common ancestor A0 of h and t, the path from h to t is the path from h to A0 and from A0 to t. In Figure 3, assuming that t is 迅速 (“rapidly”) and h is 崛起 (“take off”), the vertical features exIP NP DNP NP NN VP NP DEG NN NN VV 使 make NP NP IP NP QP Q CLP NR 资本 结构 的 优化 capital structure DE optimization NP NN VP AD VV *PRO* M 一 青岛 Qingdao one VP ADVP 批 企业 CL enterprise 迅速 崛起 rapidly take off ""The optimization of the capital structure has led to the rapid take-off of a host of enterprises in Qingdao."" Figure 3: Empty category on the right frontier tracted include: 1. The string"
N13-1125,P02-1031,0,0.0176731,"n. The task of semantic role labeling is typically defined as one of detecting and classifying arguments for verbal or nominal predicates, with more work done so far on verbal than nominal predicates. Although empty categories are annotated as arguments to verbal predicates in linguistic resources such as the English (Palmer et al., 2005) and Chinese (Xue and Palmer, 2009) Propbanks, they are often left out in semantic role labeling systems trained on these resources. This is because the best performing semantic role labeling systems rely on syntactic features extracted from automatic parses (Gildea and Palmer, 2002; Punyakanok et al., 2005) and the parsers that produce them do not generally reproduce empty categories. As a result, current semantic role labeling systems can only recover explicit arguments. However, assuming that all the explicit arguments to a predicate are detected and classified, one can infer the empty arguments of a predicate from its explicit arguments, given a list of expected arguments for the predicate. The list of expected arguments can be found in the “frame files” that are used to guide probank annotation. We defined a semantic role feature category on h when it is a verb and"
N13-1125,P02-1018,0,0.922181,"ferent language, it is also necessary to render these dropped pronouns explicit if the target language does not allow pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation. Some ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC detection, where empty categories are first located and typed. The second stage 1051 Proceedings of NAACL-HLT 2013, pages 1051–1060, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics is EC resolution, where empty categories are linked to an overt element if possible. In this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data. More concretely, EC detectio"
N13-1125,D10-1086,0,0.178847,"e to make reference to morphosyntactic clues such as to-infinitives and gerunds that are largely non-existent in Chinese. Without these clues, we believe a preprocessing approach that does not take advantage of skeletal parses is unlikely to succeed either. The work we report here also builds on emerging work in Chinese EC detection. Yang and Xue (2010) reported work on detecting just the presence and absence of empty categories without further classifying them. Chung and Gildea (2010) reported work on just detecting just a small subset of the empty categories posited in the Chinese TreeBank. Kong and Zhou (2010) worked on Chinese zero anaphora resolution, where empty category detection is a subtask. More recently, Cai et al (2011) has successfully integrated EC detection into phrasestructure based syntactic parsing and reported stateof-the-art results in both English and Chinese. 6 Conclusions and Future Work We described a novel approach to detecting empty categories (EC) represented in dependency trees and a new metric for measuring EC detection accuracy. The new metric takes into account not only the position and type of an EC, but also the head it is a dependent of in a dependency structure. We a"
N13-1125,P04-1042,0,0.0879095,"n if we make the target of classification a tuple that consists of the following word token and the head of the EC. From Figure 2, it should be clear that while *OP* and *T* both precede the same word token 涉 及 (“involve”), they have different heads, which are 的 (DE) and 涉及 respectively. Dependency-based EC detection also has other nice properties. For ECs that are arguments of their verbal head, when they are resolved to some overt element, the dependency between the referent of the EC and its head will be naturally established. This can be viewed as an alternative to the approach adopted by Levy and Manning (2004), where phrase structure parses are augmented to recover non-local dependencies. Dependency structures are also easily decomposable into head/dependency pairs and this makes the evaluation more straightforward. Each classification instance can be evaluated independently of other parts of the dependency structure. 2.3 One pass vs two passes With pairs of tokens (h, t) as the classification target, all possible pairs in a sentence will have to be considered and there will be a large number of (h, t) tuples that are not associated with an EC, leading to a highly imbalanced data set. One can conce"
N13-1125,J93-2004,0,0.0427286,"e stringent metric. 1 Introduction In modern theoretical linguistics, empty categories (ECs) are an important piece of machinery in representing the syntactic structure of a sentence and they are used to represent phonologically null elements such as dropped pronouns and traces of dislocated elements. They have also found their way into largescale treebanks which have played an important role in advancing the state of the art in syntactic parsing. In phrase-structure treebanks, ECs have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Together with labeled brackets and function tags, they make up the full syntactic representation of a sentence. The use of ECs captures some cross-linguistic commonalities and differences. For example, while both the Penn English TreeBank (PTB) (Marcus et al., 1993) and the Chinese TreeBank (CTB) (Xue et al., 2005) use traces to represent the extraction site of a dislocated element, dropped pronouns (represented as *pro*s) are much more widespread in the CTB. This is because Chinese is a pro-drop language (Huang, 1984) that allows the subject to be dropped in more contexts"
N13-1125,J05-1004,0,0.0489626,"on h and constrained by word distance: it is only used when h immediately precedes t. This feature category is intended to capture transitive verbs that are missing an object. 3.6 Semantic role features There are apparent connections between semantic role labeling and EC detection. The task of semantic role labeling is typically defined as one of detecting and classifying arguments for verbal or nominal predicates, with more work done so far on verbal than nominal predicates. Although empty categories are annotated as arguments to verbal predicates in linguistic resources such as the English (Palmer et al., 2005) and Chinese (Xue and Palmer, 2009) Propbanks, they are often left out in semantic role labeling systems trained on these resources. This is because the best performing semantic role labeling systems rely on syntactic features extracted from automatic parses (Gildea and Palmer, 2002; Punyakanok et al., 2005) and the parsers that produce them do not generally reproduce empty categories. As a result, current semantic role labeling systems can only recover explicit arguments. However, assuming that all the explicit arguments to a predicate are detected and classified, one can infer the empty argu"
N13-1125,W05-0639,0,0.0158635,"ole labeling is typically defined as one of detecting and classifying arguments for verbal or nominal predicates, with more work done so far on verbal than nominal predicates. Although empty categories are annotated as arguments to verbal predicates in linguistic resources such as the English (Palmer et al., 2005) and Chinese (Xue and Palmer, 2009) Propbanks, they are often left out in semantic role labeling systems trained on these resources. This is because the best performing semantic role labeling systems rely on syntactic features extracted from automatic parses (Gildea and Palmer, 2002; Punyakanok et al., 2005) and the parsers that produce them do not generally reproduce empty categories. As a result, current semantic role labeling systems can only recover explicit arguments. However, assuming that all the explicit arguments to a predicate are detected and classified, one can infer the empty arguments of a predicate from its explicit arguments, given a list of expected arguments for the predicate. The list of expected arguments can be found in the “frame files” that are used to guide probank annotation. We defined a semantic role feature category on h when it is a verb and the value of this feature"
N13-1125,P06-1023,0,0.619128,"does not allow pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation. Some ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC detection, where empty categories are first located and typed. The second stage 1051 Proceedings of NAACL-HLT 2013, pages 1051–1060, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics is EC resolution, where empty categories are linked to an overt element if possible. In this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data. More concretely, EC detection involves (i) identifying the position of the EC, relative to some overt word tokens in the same sent"
N13-1125,C10-2158,1,0.566077,"tely, EC detection involves (i) identifying the position of the EC, relative to some overt word tokens in the same sentence, and (ii) determining the type of EC, e.g., whether it is a dropped pronoun or a trace. We focus on EC detection here because most of the ECs in the Chinese Treebank are either not resolved to an overt element or linked to another EC. For example, dropped pronouns (*pro*) are not resolved, and traces (*T*) in relative clauses are linked to an empty relative pronoun (*OP*). In previous work, ECs are either represented linearly, where ECs are indexed to the following word (Yang and Xue, 2010) or attached to nodes in a phrase structure tree (Johnson, 2002; Dienes and Dubey, 2003b; Gabbard et al., 2006). In a linear representation where ECs are indexed to the following word, it is difficult to represent consecutive ECs because that will mean more than one EC will be indexed to the same word (making the classification task more complicated). While in English consecutive ECs are relatively rare, in Chinese this is very common. For example, it is often the case that an empty relative pronoun (*OP*) is followed immediately by a trace (*T*). Another issue with the linear representation o"
N13-1125,J03-4003,0,\N,Missing
N13-1125,W09-1201,1,\N,Missing
N15-1040,W13-2322,0,0.695346,"ntuitive and capture the regularities in the mapping between the dependency structure and the AMR of a sentence. Third, our parser runs in nearly linear time in practice in spite of a worst-case complexity of O(n2 ). 1 Sameer Pradhan Harvard Medical School Sameer.Pradhan@ childrens.harvard.edu want want-01 xcomp nsubj arrest police det to dobj ARG0 police prep aux pobj Karras Abstract Meaning Representation (AMR) is a rooted, directed, edge-labeled and leaf-labeled graph that is used to represent the meaning of a sentence. The AMR formalism has been used to annotate the AMR Annotation Corpus (Banarescu et al., 2013), a corpus of over 10 thousand sentences that is still undergoing expansion. The building blocks for an AMR representation are concepts and relations between them. Understanding these concepts and their relations is crucial to understanding the meaning of a sentence and could potentially benefit a number of natural language applications such (a) Dependency tree location Singapore name Singapore name nn Michael arrest-01 ARG1 person in The Introduction ARG1 ARG0 op1 “Michael” op2 “Karras” (b) AMR graph Figure 1: Dependency tree and AMR graph for the sentence, “The police want to arrest Micheal"
N15-1040,D12-1133,0,0.00852229,"and parsing state s. score(t, s) = ω ~ · φ(t, s) (1) where ω ~ is the weight vector and φ is a function that extracts the feature vector representation for one possible state-action pair ht, si. First, the algorithm initializes the state s with the sentence w and its dependency tree Dw . At each iteration, it gets all the possible actions for current state s (line 3). Then, it chooses the action with the highest score given by function score() and applies it to s (line 4-5). When the current state reaches a terminal state, the parser stops and returns the parsed graph. 371 As pointed out in (Bohnet and Nivre, 2012), constraints can be added to limit the number of possible actions to be evaluated at line 3. There could be formal constraints on states such as the constraint that the SWAP action should not be applied twice to the same pair of nodes. We could also apply soft constraints to filter out unlikely concept labels, relation labels and candidate nodes k for REATTACH and REENTRANCE. In our parser, we enforce the constraint that NEXT-NODE-lc can only choose from concept labels that co-occur with the current node’s lemma in the training data. We also empirically set the constraint that REATTACHk could"
N15-1040,P13-2131,0,0.556982,"Missing"
N15-1040,P13-1091,0,0.0291139,"oncept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet. 7 Conclusion and Future Work We presented a novel transition-based parsing algorithm that takes the dependency tree of a sentence as input and transforms it into an Abstract Meaning Representation graph through a sequence of actions. We show that our approach is linguistically intuitive and our experimental results also show that our parser outperformed the previous best reported results by a significant margin. In future work we p"
N15-1040,P04-1015,0,0.103759,"Missing"
N15-1040,W02-1001,0,0.237841,"at NEXT-NODE-lc can only choose from concept labels that co-occur with the current node’s lemma in the training data. We also empirically set the constraint that REATTACHk could only choose k among σ0 ’s grandparents and great grandparents. Additionally, REENTRANCEk could only choose k among its siblings. These constraints greatly reduce the search space, thus speeding up the parser. 4 Learning 4.1 Learning Algorithm As stated in section 3.2, the parameter of our model is weight vector ω ~ in the score function. To train the weight vector, we employ the averaged perceptron learning algorithm (Collins, 2002). Algorithm 2 Learning algorithm Input: sentence w = w0 . . . wn , Dw , Gw Output: ω ~ 1: s ← s0 (Dw , w) 2: while s ∈ / St do 3: T ← all possible actions according to s 4: bestT ← arg maxt∈T score(t, s) 5: goldT ← oracle(s, Gw ) 6: if bestT 6= goldT then 7: ω ~ ←ω ~ − φ(bestT, s) + φ(goldT, s) 8: end if 9: s ← apply goldT to s 10: end while For each sentence w and its corresponding AMR annotation GAM R in the training corpus, we could get the dependency tree Dw of w with a dependency parser. Then we represent GAM R as span graph Gw , which serves as our learning target. The learning algorithm"
N15-1040,P14-1134,0,0.815762,"in the sentence and the nodes in the dependency tree, AMR is an abstract representation where the word order of the corresponding sentence is not maintained. In addition, some words become abstract concepts or relations while other words are simply deleted because they do not contribute to meaning. The alignment between the word tokens and the concepts is non-trivial, but in order to learn the transition from a dependency tree to an AMR graph, we have to first establish the alignment between the word tokens in the sentence and the concepts in the AMR. We use the aligner that comes with JAMR (Flanigan et al., 2014) to produce this alignment. The JAMR aligner attempts to greedily align every concept or graph fragment in the AMR graph with a contiguous word token sequence in the sentence. s0,1 :ROOT want-01 ARG1 ARG0 police s3,4 :want-01 arrest-01 ARG0 ARG1 ARG1 ARG0 person s5,6 :arrest-01 ARG0 s2,3 :police name ARG1 name op1 “Micheal” (a) AMR graph op2 “Karras” s6,8 : person+name (b) Span graph Figure 2: AMR graph and its span graph for the sentence, “The police want to arrest Micheal Karras.” We use a data structure called span graph to represent an AMR graph that is aligned with the word tokens in a se"
N15-1040,P03-1054,0,0.012039,"Missing"
N15-1040,P14-5010,0,0.00390732,"Missing"
N15-1040,W03-3017,0,0.0532707,"tion. They treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For rela374 tion identification, they adopt the graph-based techniques for non-projective dependency parsing. Instead of finding maximum-scoring trees over words, they propose an algorithm to find the maximum spanning connected subgraph (MSCG) over concept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet. 7 Conclusion"
N15-1040,J08-4003,0,0.0520299,"set that our model predicts consists of both those from the concepts in the original AMR graph and those as a result of collapsing the AMR subgraphs. person name s6,8 :person+name name op1 op2 “Micheal” “Karras” Figure 3: Collapsed nodes Representing AMR graph this way allows us to formulate the AMR parsing problem as a joint learning problem where we can design a set of actions to simultaneously predict the concepts (nodes) and relations (arcs) in the AMR graph as well as the labels on them. 3 Transition-based AMR Parsing 3.1 Transition System Similar to transition-based dependency parsing (Nivre, 2008), we define a transition system for AMR parsing as a quadruple S = (S, T, s0 , St ), where • S is a set of parsing states (configurations). • T is a set of parsing actions (transitions), each of which is a function t : S → S. • s0 is an initialization function, mapping each input sentence w and its dependency tree D to an initial state. 368 • St ⊆ S is a set of terminal states. Each state (configuration) of our transition-based parser is a triple (σ, β, G). σ is a buffer that stores indices of the nodes which have not been processed and we write σ = σ0 |σ 0 to indicate that σ0 is the topmost e"
N15-1040,P09-1040,0,0.0284183,"Missing"
N15-1040,C08-1095,0,0.186364,"eat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For rela374 tion identification, they adopt the graph-based techniques for non-projective dependency parsing. Instead of finding maximum-scoring trees over words, they propose an algorithm to find the maximum spanning connected subgraph (MSCG) over concept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet. 7 Conclusion and Future Work We prese"
N15-1040,P13-1014,0,0.0521965,"r set to null if σ0 is a leaf node. G is initialized with all the nodes and edges of D. Initially, all the nodes of G have a span length of one and all the labels for nodes and edges are set to null. As the parsing procedure goes on, the parser will process all the nodes and their outgoing edges in dependency tree D in a bottom-up left-right manner, and at each state certain action will be applied to the current node or edge. The parsing process will terminate when both σ and β are empty. The most important part of the transition-based parser is the set of actions (transitions). As stated in (Sartorio et al., 2013), the design space of possible actions is actually infinite since the set of parsing states is infinite. However, if the problem is amenable to transition-based parsing, we can design a finite set of actions by categorizing all the possible situations we run into in the parsing process. In §5.2 we show this is the case here and our action set can account for almost all the transformations from dependency trees to AMR graphs. We define 8 types of actions for the actions set T , which is summarized in Table 1. The action set could be divided into two categories based on conditions of buffer β. W"
N15-1040,W03-3023,0,0.212539,"tion and relation identification. They treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For rela374 tion identification, they adopt the graph-based techniques for non-projective dependency parsing. Instead of finding maximum-scoring trees over words, they propose an algorithm to find the maximum spanning connected subgraph (MSCG) over concept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet."
N15-1040,N07-1050,0,\N,Missing
N15-1081,P13-2013,0,0.641487,"ONTINGENCY relation. Dropping the connective and adding the relation as a training sample adds noise to the training set and can only hurt the performance. In addition, certain types of explicit discourse relations have no corresponding implicit discourse relations. For example, discourse relations of the type C ONTINGENY.C ONDITION are almost always expressed with an explicit discourse connective and do not exist in implicit relations. We believe this also explains the lack of success in previous attempts to boost the performance of implicit discourse relation detection with this approach. (Biran and McKeown, 2013; Pitler et al., 2009). This suggests that in order for this approach to work, we need to identify instances of explicit discourse relations that closely match the characteristics of implicit discourse relations. In this paper, we propose two criteria for selecting such explicit discourse relation instances: omission rate and context differential. Our selection criteria 800 first classify discourse connectives by their distributional properties and suggest that not all discourse connectives are truly optional and not all implicit and explicit discourse relations are equivalent, contrary to com"
N15-1081,P06-4018,0,0.0111751,"s large size of 2.9 billion words and its similarity to the Wall Street Journal data from the PDTB. The source of the corpus is drawn from six distinct international sources of English newswire dating from 1994 - 2006. We use this corpus to extract weakly labeled data for the experiment. 2.2 Discourse relation extraction pattern We extract instances of explicit discourse relations from the Gigaword Corpus that have the same patterns as the implicit discourse relations in the PDTB, using simple regular expressions. We first sentencesegment the Gigaword Corpus using the NLTK sentence segmenter (Bird, 2006). We then write a set of rules to prevent some common erroneous cases such as because vs because of from being included. If a discourse connective is a subordinating conjunction, then we use the following pattern: (Clause 1) (connective) (clause 2). Clause 1 and capitalized clause 2 are then used as Arg1 and Arg2 respectively. If a discourse connective is a coordinating conjunction or discourse adverbial, we use the following pattern: (Sentence 1). (Connective),(clause 2). Sentence 1 and Clause 2 with the first word capitalized are used as Arg1 and Arg2 respectively. Although there are obvious"
N15-1081,P12-1007,0,0.0108476,"freely omissible connectives improve the performance of the system without additional features. 1 (1) [The city’s Campaign Finance Board has refused to pay Mr Dinkins $95,142 in matching funds]Arg1 because [his campaign records are incomplete]Arg2 . (2) [So much of the stuff poured into its Austin, Texas, offices that its mail rooms there simply stopped delivering it]Arg1 . Implicit=so [Now, thousands of mailers, catalogs and sales pitches go straight into the trash]Arg2 . Introduction The analysis of discourse-level structure has received increasing attention from the field in recent years (Feng and Hirst, 2012; Patterson and Kehler, 2013; Li et al., 2014). Discourse-level analysis is typically concerned with relations between clauses and sentences, linguistic units that go beyond sentence boundaries. There are a few conceptions of the discourse structure representation of a text such as a tree (Mann and Thompson, 1988), or a graph (Wolf et al., 2005). In the work we describe here, we adopt the view of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which views a text as Determining the sense of an explicit discourse relation such as (1) is straightforward since “because” is a strong indic"
N15-1081,W14-4327,0,0.126941,"ting the sense of discourse connective, which can be done with high accuracy (Pitler et al., 2008). However, in the absence of an explicit discourse connective, inferring the sense of a discourse relation has proved to a very challenging task (Park and Cardie, 2012; Rutherford and Xue, 2014). The sense is no longer localized on one or two discourse connectives and must now be inferred solely based on its two textual arguments. Given the limited amount of annotated data in comparison to the number of features needed, the process of building a classifier is plagued by the data sparsity problem (Li and Nenkova, 2014). As a result, the classification accuracy of implicit discourse relations remains much 799 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 799–808, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics lower than that of explicit discourse relations (Pitler et al., 2008). One potential method for reducing the data sparsity problem is through a distantly supervised learning paradigm, which is the direction we take in this work. Distant supervision approaches make use of prior knowledge or heuristics to"
N15-1081,D14-1220,0,0.0245033,"nce of the system without additional features. 1 (1) [The city’s Campaign Finance Board has refused to pay Mr Dinkins $95,142 in matching funds]Arg1 because [his campaign records are incomplete]Arg2 . (2) [So much of the stuff poured into its Austin, Texas, offices that its mail rooms there simply stopped delivering it]Arg1 . Implicit=so [Now, thousands of mailers, catalogs and sales pitches go straight into the trash]Arg2 . Introduction The analysis of discourse-level structure has received increasing attention from the field in recent years (Feng and Hirst, 2012; Patterson and Kehler, 2013; Li et al., 2014). Discourse-level analysis is typically concerned with relations between clauses and sentences, linguistic units that go beyond sentence boundaries. There are a few conceptions of the discourse structure representation of a text such as a tree (Mann and Thompson, 1988), or a graph (Wolf et al., 2005). In the work we describe here, we adopt the view of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which views a text as Determining the sense of an explicit discourse relation such as (1) is straightforward since “because” is a strong indicator that the relation between the two argumen"
N15-1081,E12-1062,0,0.0126258,"l language processing due to the recent capability to process large amount of data. These approaches are known to be particularly useful for relation extraction tasks because training data provided do not suffice for the task and are difficult to obtain (Riloff et al., 1999; Yao et al., 2010). For example, Mintz et al. (2009) acquire a large amount of weakly labeled data based on the Freebase knowledge base and improves the performance of relation extraction. Distantly supervised learning has also recently been demonstrated to be useful for text classification problems (Speriosu et al., 2011; Marchetti-Bowick and Chambers, 2012). For example, Thamrongrattanarit et al. (2013) use simple heuristics to gather weakly labeled data to perform text classification with no manually annotated training data. Discourse connectives have been studied and classified based on their syntactic properties such subordinating conjunction, adverbials, etc. (Fraser, 2006; Fraser, 1996). While providing a useful insight into how discourse connectives fit into utterances, the syntactic classification does not seem suitable for selecting useful discourse connectives for our purposes of distant supervision for our task. 5 Conclusion and Future"
N15-1081,P02-1047,0,0.839796,"14), and the distantly supervised approach using explicit discourse relations 806 Comp. 2 4 1 2 0 Sense Cont. Exp. 6 10 2 5 0 5 0 0 3 3 Temp. 1 3 0 3 2 Table 6: The sense distribution by connective class. has not shown satisfactory results (Pitler et al., 2009; Park and Cardie, 2012; Wang et al., 2012; Sporleder and Lascarides, 2008) Explicit discourse relations have been used to remedy the sparsity problem or gain extra features with limited success (Biran and McKeown, 2013; Pitler et al., 2009). Our heuristics for extracting discourse relations has been explored in the unsupervised setting (Marcu and Echihabi, 2002), but it has never been evaluated on the gold standard data to show its true efficacy. Our distant supervision approach chooses only certain types of discourse connectives to extract weakly labeled data and is the first of its kind to improve the performance in this task tested on the manually annotated data. Distant supervision approaches have recently been explored in the context of natural language processing due to the recent capability to process large amount of data. These approaches are known to be particularly useful for relation extraction tasks because training data provided do not s"
N15-1081,P09-1113,0,0.0327678,"ervision approach chooses only certain types of discourse connectives to extract weakly labeled data and is the first of its kind to improve the performance in this task tested on the manually annotated data. Distant supervision approaches have recently been explored in the context of natural language processing due to the recent capability to process large amount of data. These approaches are known to be particularly useful for relation extraction tasks because training data provided do not suffice for the task and are difficult to obtain (Riloff et al., 1999; Yao et al., 2010). For example, Mintz et al. (2009) acquire a large amount of weakly labeled data based on the Freebase knowledge base and improves the performance of relation extraction. Distantly supervised learning has also recently been demonstrated to be useful for text classification problems (Speriosu et al., 2011; Marchetti-Bowick and Chambers, 2012). For example, Thamrongrattanarit et al. (2013) use simple heuristics to gather weakly labeled data to perform text classification with no manually annotated training data. Discourse connectives have been studied and classified based on their syntactic properties such subordinating conjunct"
N15-1081,W12-1614,0,0.848509,"we describe here, we adopt the view of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which views a text as Determining the sense of an explicit discourse relation such as (1) is straightforward since “because” is a strong indicator that the relation between the two arguments is C ONTINGENCY.C AUSE. This task effectively amounts to disambiguating the sense of discourse connective, which can be done with high accuracy (Pitler et al., 2008). However, in the absence of an explicit discourse connective, inferring the sense of a discourse relation has proved to a very challenging task (Park and Cardie, 2012; Rutherford and Xue, 2014). The sense is no longer localized on one or two discourse connectives and must now be inferred solely based on its two textual arguments. Given the limited amount of annotated data in comparison to the number of features needed, the process of building a classifier is plagued by the data sparsity problem (Li and Nenkova, 2014). As a result, the classification accuracy of implicit discourse relations remains much 799 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 799–808, c Denver, Colorado, May 31 – June 5, 20"
N15-1081,D13-1094,0,0.126176,"ectives improve the performance of the system without additional features. 1 (1) [The city’s Campaign Finance Board has refused to pay Mr Dinkins $95,142 in matching funds]Arg1 because [his campaign records are incomplete]Arg2 . (2) [So much of the stuff poured into its Austin, Texas, offices that its mail rooms there simply stopped delivering it]Arg1 . Implicit=so [Now, thousands of mailers, catalogs and sales pitches go straight into the trash]Arg2 . Introduction The analysis of discourse-level structure has received increasing attention from the field in recent years (Feng and Hirst, 2012; Patterson and Kehler, 2013; Li et al., 2014). Discourse-level analysis is typically concerned with relations between clauses and sentences, linguistic units that go beyond sentence boundaries. There are a few conceptions of the discourse structure representation of a text such as a tree (Mann and Thompson, 1988), or a graph (Wolf et al., 2005). In the work we describe here, we adopt the view of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which views a text as Determining the sense of an explicit discourse relation such as (1) is straightforward since “because” is a strong indicator that the relation betwe"
N15-1081,P09-1077,0,0.754465,"pping the connective and adding the relation as a training sample adds noise to the training set and can only hurt the performance. In addition, certain types of explicit discourse relations have no corresponding implicit discourse relations. For example, discourse relations of the type C ONTINGENY.C ONDITION are almost always expressed with an explicit discourse connective and do not exist in implicit relations. We believe this also explains the lack of success in previous attempts to boost the performance of implicit discourse relation detection with this approach. (Biran and McKeown, 2013; Pitler et al., 2009). This suggests that in order for this approach to work, we need to identify instances of explicit discourse relations that closely match the characteristics of implicit discourse relations. In this paper, we propose two criteria for selecting such explicit discourse relation instances: omission rate and context differential. Our selection criteria 800 first classify discourse connectives by their distributional properties and suggest that not all discourse connectives are truly optional and not all implicit and explicit discourse relations are equivalent, contrary to commonly held beliefs in"
N15-1081,prasad-etal-2008-penn,0,0.611975,"into the trash]Arg2 . Introduction The analysis of discourse-level structure has received increasing attention from the field in recent years (Feng and Hirst, 2012; Patterson and Kehler, 2013; Li et al., 2014). Discourse-level analysis is typically concerned with relations between clauses and sentences, linguistic units that go beyond sentence boundaries. There are a few conceptions of the discourse structure representation of a text such as a tree (Mann and Thompson, 1988), or a graph (Wolf et al., 2005). In the work we describe here, we adopt the view of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which views a text as Determining the sense of an explicit discourse relation such as (1) is straightforward since “because” is a strong indicator that the relation between the two arguments is C ONTINGENCY.C AUSE. This task effectively amounts to disambiguating the sense of discourse connective, which can be done with high accuracy (Pitler et al., 2008). However, in the absence of an explicit discourse connective, inferring the sense of a discourse relation has proved to a very challenging task (Park and Cardie, 2012; Rutherford and Xue, 2014). The sense is no longer localized on one or two"
N15-1081,E14-1068,1,0.486242,"dopt the view of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which views a text as Determining the sense of an explicit discourse relation such as (1) is straightforward since “because” is a strong indicator that the relation between the two arguments is C ONTINGENCY.C AUSE. This task effectively amounts to disambiguating the sense of discourse connective, which can be done with high accuracy (Pitler et al., 2008). However, in the absence of an explicit discourse connective, inferring the sense of a discourse relation has proved to a very challenging task (Park and Cardie, 2012; Rutherford and Xue, 2014). The sense is no longer localized on one or two discourse connectives and must now be inferred solely based on its two textual arguments. Given the limited amount of annotated data in comparison to the number of features needed, the process of building a classifier is plagued by the data sparsity problem (Li and Nenkova, 2014). As a result, the classification accuracy of implicit discourse relations remains much 799 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 799–808, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Co"
N15-1081,W11-2207,0,0.00623746,"n the context of natural language processing due to the recent capability to process large amount of data. These approaches are known to be particularly useful for relation extraction tasks because training data provided do not suffice for the task and are difficult to obtain (Riloff et al., 1999; Yao et al., 2010). For example, Mintz et al. (2009) acquire a large amount of weakly labeled data based on the Freebase knowledge base and improves the performance of relation extraction. Distantly supervised learning has also recently been demonstrated to be useful for text classification problems (Speriosu et al., 2011; Marchetti-Bowick and Chambers, 2012). For example, Thamrongrattanarit et al. (2013) use simple heuristics to gather weakly labeled data to perform text classification with no manually annotated training data. Discourse connectives have been studied and classified based on their syntactic properties such subordinating conjunction, adverbials, etc. (Fraser, 2006; Fraser, 1996). While providing a useful insight into how discourse connectives fit into utterances, the syntactic classification does not seem suitable for selecting useful discourse connectives for our purposes of distant supervision"
N15-1081,I13-1119,0,0.0150761,"to process large amount of data. These approaches are known to be particularly useful for relation extraction tasks because training data provided do not suffice for the task and are difficult to obtain (Riloff et al., 1999; Yao et al., 2010). For example, Mintz et al. (2009) acquire a large amount of weakly labeled data based on the Freebase knowledge base and improves the performance of relation extraction. Distantly supervised learning has also recently been demonstrated to be useful for text classification problems (Speriosu et al., 2011; Marchetti-Bowick and Chambers, 2012). For example, Thamrongrattanarit et al. (2013) use simple heuristics to gather weakly labeled data to perform text classification with no manually annotated training data. Discourse connectives have been studied and classified based on their syntactic properties such subordinating conjunction, adverbials, etc. (Fraser, 2006; Fraser, 1996). While providing a useful insight into how discourse connectives fit into utterances, the syntactic classification does not seem suitable for selecting useful discourse connectives for our purposes of distant supervision for our task. 5 Conclusion and Future Directions We propose two selection criteria f"
N15-1081,C12-1168,0,0.58266,"fficient for a class of discourse connectives to boost the performance. The two selection criteria are both necessary for the success of this paradigm. 4 Related work Previous work on implicit discourse relation classification have focused on supervised learning approaches (Lin et al., 2010; Rutherford and Xue, 2014), and the distantly supervised approach using explicit discourse relations 806 Comp. 2 4 1 2 0 Sense Cont. Exp. 6 10 2 5 0 5 0 0 3 3 Temp. 1 3 0 3 2 Table 6: The sense distribution by connective class. has not shown satisfactory results (Pitler et al., 2009; Park and Cardie, 2012; Wang et al., 2012; Sporleder and Lascarides, 2008) Explicit discourse relations have been used to remedy the sparsity problem or gain extra features with limited success (Biran and McKeown, 2013; Pitler et al., 2009). Our heuristics for extracting discourse relations has been explored in the unsupervised setting (Marcu and Echihabi, 2002), but it has never been evaluated on the gold standard data to show its true efficacy. Our distant supervision approach chooses only certain types of discourse connectives to extract weakly labeled data and is the first of its kind to improve the performance in this task teste"
N15-1081,D10-1099,0,0.0233816,"s true efficacy. Our distant supervision approach chooses only certain types of discourse connectives to extract weakly labeled data and is the first of its kind to improve the performance in this task tested on the manually annotated data. Distant supervision approaches have recently been explored in the context of natural language processing due to the recent capability to process large amount of data. These approaches are known to be particularly useful for relation extraction tasks because training data provided do not suffice for the task and are difficult to obtain (Riloff et al., 1999; Yao et al., 2010). For example, Mintz et al. (2009) acquire a large amount of weakly labeled data based on the Freebase knowledge base and improves the performance of relation extraction. Distantly supervised learning has also recently been demonstrated to be useful for text classification problems (Speriosu et al., 2011; Marchetti-Bowick and Chambers, 2012). For example, Thamrongrattanarit et al. (2013) use simple heuristics to gather weakly labeled data to perform text classification with no manually annotated training data. Discourse connectives have been studied and classified based on their syntactic prop"
N15-1081,C08-2022,0,\N,Missing
N18-2040,P16-1025,0,0.023845,"al., 2013) is a semantic representation where the meaning of a sentence is encoded as a rooted, directed and acyclic graph. AMR parsing has received a significant amount of attention in the NLP research community. Since the release of the AMR bank a number of AMR parsers have been developed in recent years (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016). The initial benefit of AMR parsing has also been demonstrated in various downstream applications such as Information Extraction (Pan et al., 2015; Huang et al., 2016), Machine Comprehension (Sachan and Xing, 2016), and Natural Language Generation (Flanigan et al., 2016; Butler, 2016). In this paper, we present the first AMR parser built using the Chinese AMR Bank (Li et al., 2016). We adopt the transition-based parsing framework first proposed for English (Wang et al., 2015b, 2016), where AMR parsing is modeled as a dependency tree to AMR graph transformation using a set of linguistically motivated actions. We briefly describe the Chinese AMR Bank in Section 2, present the transition-based Chinese AMR 3 Transition-based AMR Parsing In a transition-based AM"
N18-2040,D15-1198,0,0.0745144,"Missing"
N18-2040,W16-1702,1,0.908226,"paper in Section 5. This paper presents the first AMR parser built on the Chinese AMR bank. By applying a transition-based AMR parsing framework to Chinese, we first investigate how well the transitions first designed for English AMR parsing generalize to Chinese and provide a comparative analysis between the transitions for English and Chinese. We then perform a detailed error analysis to identify the major challenges in Chinese AMR parsing that we hope will inform future research in this area. 1 2 The Chinese AMR Bank In our experiment, we use a pre-release version of the Chinese AMR Bank (Li et al., 2016)1 consisting of 10,149 sentences extracted from the Chinese Treebank (CTB) 8.0 (Xue et al., 2005) 2 , which mainly consists of Chinese texts of web logs and discussion forums. The average sentence length is 22.43 words. Similar to English, the Chinese AMRs are also represented as rooted, directed and acyclic graphs that share the abstract concepts and relations used in the English AMR Bank. The sense-disambiguated predicates are drawn from the frame files developed for the Chinese Propbank(Xue and Palmer, 2009), just as the sensedisambiguated predicates in the AMR Bank are drawn from the Propb"
N18-2040,W13-2322,0,0.442579,"Missing"
N18-2040,P14-5010,0,0.00299425,"de future research. Experiment Settings We use the 10,149 sentences from the Chinese AMR Bank and split the data according to their original CTB8.0 document IDs, where articles 5061-5558 are used as the training set, articles 5000-5030 are used as the development set and articles 5031-5060 are used as the test set. The train/development/test ratio in this dataset is 7608/1264/1277. As the data are drawn from the Chinese Treebank where words are manually segmented, we will simply use the gold segmentation in our experiments. We then process the whole Chinese dataset using the Stanford CoreNLP (Manning et al., 2014) toolkit to get the POS and Named Entity tags. To get the dependency parse for the Chinese data, we use the transition-based constituent parser in (Wang and Xue, 2014) to first parse the Chinese sentences into constituent trees, which are then transformed into dependency trees using the converter in the Stanford CoreNLP toolkit. Note that this Chinese constituent parser also uses the Chinese Treebank 8.0 to train its model. To avoid training on the parser on AMR test set, we train the constituent parser using a 10-fold cross-validation with each fold parsed using a model trained on the other 9"
N18-2040,W16-0601,0,0.0252337,"Missing"
N18-2040,S16-1166,0,0.127031,"v! Test! 0.520! 8 (1) 他 帮 了 我 很 大 的 忙 。 He helped PAST me very big DE business “He helped me a lot.” 0.500! Precision! Recall! F-score! Figure 3: Parsing performance on the development and test set Being able to identify the linguistic environment for each action helps us understand what the parser actually does when actions are applied. More importantly, making the relation between the Figure 3 presents the parsing performance on the development and test set in terms of the 249 Smatch score. Compared with the state of the art in English AMR parsing, which is in the high 60 percentage points (May, 2016), this initial parsing performance here is very strong, considering the model is trained on a smaller training set. The Chinese AMR parsing model also does not benefit from the more extensive feature engineering that has been done for English AMR parsing. For example, the English AMR parser, CAMR, uses semantic roles and coreference features that are not available to the Chinese AMR parser. The other important factor is that most of the Chinese linguistic analyzers (dependency parsers, named entity taggers, etc.) have a lower accuracy than their English counterparts, and when used as preproces"
N18-2040,P13-2131,0,0.313233,"Missing"
N18-2040,J05-1004,0,0.241949,"Missing"
N18-2040,E17-1051,0,0.0730008,"an English. This indicates that named entity is one of the bottlenecks in Chinese AMR parsing. This indicates that improving named entity recognition, either as a preprocessing step or as an integral part of the parsing model, is crucial to Chinese AMR parsing. Fine-grained Error Analysis So far, all of our experiments are evaluated using the Smatch score, where only precision, recall and F-score are reported based on the overall performance of the parser. To gain more insights, we further break down the Smatch score and report the performance for each component using the evaluation tool from Damonte et al. (2017). The evaluation tool examines different aspects of the AMR parsing result through different ablation tests that we summarize as follows. The detailed description of the ablation test can be found in Damonte et al. (2017). 5 Conclusion We present the first Chinese AMR parser trained on the Chinese AMR Bank. We show that a transition-based AMR parsing framework first proposed for English is general enough to handle the linguistic phenomena in Chinese and has produced a strong baseline that future research can build on. In addition, we perform a detailed comparative analysis of the transition di"
N18-2040,N15-1119,0,0.0149367,"AMR) (Banarescu et al., 2013) is a semantic representation where the meaning of a sentence is encoded as a rooted, directed and acyclic graph. AMR parsing has received a significant amount of attention in the NLP research community. Since the release of the AMR bank a number of AMR parsers have been developed in recent years (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016). The initial benefit of AMR parsing has also been demonstrated in various downstream applications such as Information Extraction (Pan et al., 2015; Huang et al., 2016), Machine Comprehension (Sachan and Xing, 2016), and Natural Language Generation (Flanigan et al., 2016; Butler, 2016). In this paper, we present the first AMR parser built using the Chinese AMR Bank (Li et al., 2016). We adopt the transition-based parsing framework first proposed for English (Wang et al., 2015b, 2016), where AMR parsing is modeled as a dependency tree to AMR graph transformation using a set of linguistically motivated actions. We briefly describe the Chinese AMR Bank in Section 2, present the transition-based Chinese AMR 3 Transition-based AMR Parsing In"
N18-2040,K15-1004,0,0.0763236,"Missing"
N18-2040,N16-1087,0,0.0291864,"cted and acyclic graph. AMR parsing has received a significant amount of attention in the NLP research community. Since the release of the AMR bank a number of AMR parsers have been developed in recent years (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016). The initial benefit of AMR parsing has also been demonstrated in various downstream applications such as Information Extraction (Pan et al., 2015; Huang et al., 2016), Machine Comprehension (Sachan and Xing, 2016), and Natural Language Generation (Flanigan et al., 2016; Butler, 2016). In this paper, we present the first AMR parser built using the Chinese AMR Bank (Li et al., 2016). We adopt the transition-based parsing framework first proposed for English (Wang et al., 2015b, 2016), where AMR parsing is modeled as a dependency tree to AMR graph transformation using a set of linguistically motivated actions. We briefly describe the Chinese AMR Bank in Section 2, present the transition-based Chinese AMR 3 Transition-based AMR Parsing In a transition-based AMR parsing framework an input sentence is first parsed into a dependency tree and then transformed into"
N18-2040,D15-1136,0,0.0474707,"Missing"
N18-2040,P14-1134,0,0.215977,"Missing"
N18-2040,P16-2079,0,0.0140987,"re the meaning of a sentence is encoded as a rooted, directed and acyclic graph. AMR parsing has received a significant amount of attention in the NLP research community. Since the release of the AMR bank a number of AMR parsers have been developed in recent years (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016). The initial benefit of AMR parsing has also been demonstrated in various downstream applications such as Information Extraction (Pan et al., 2015; Huang et al., 2016), Machine Comprehension (Sachan and Xing, 2016), and Natural Language Generation (Flanigan et al., 2016; Butler, 2016). In this paper, we present the first AMR parser built using the Chinese AMR Bank (Li et al., 2016). We adopt the transition-based parsing framework first proposed for English (Wang et al., 2015b, 2016), where AMR parsing is modeled as a dependency tree to AMR graph transformation using a set of linguistically motivated actions. We briefly describe the Chinese AMR Bank in Section 2, present the transition-based Chinese AMR 3 Transition-based AMR Parsing In a transition-based AMR parsing framework an input sentence is first"
N18-2040,P16-1001,0,0.0354107,"trancies, meaning that they have a graph structure that cannot be represented with a tree representation. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation where the meaning of a sentence is encoded as a rooted, directed and acyclic graph. AMR parsing has received a significant amount of attention in the NLP research community. Since the release of the AMR bank a number of AMR parsers have been developed in recent years (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016). The initial benefit of AMR parsing has also been demonstrated in various downstream applications such as Information Extraction (Pan et al., 2015; Huang et al., 2016), Machine Comprehension (Sachan and Xing, 2016), and Natural Language Generation (Flanigan et al., 2016; Butler, 2016). In this paper, we present the first AMR parser built using the Chinese AMR Bank (Li et al., 2016). We adopt the transition-based parsing framework first proposed for English (Wang et al., 2015b, 2016), where AMR parsing is modeled as a dependency tree to AMR graph transformation using a set of linguistically mo"
N18-2040,S16-1181,1,0.850264,"Missing"
N18-2040,P15-2141,1,0.914875,"Missing"
N18-2040,N15-1040,1,0.907244,"Missing"
N18-2040,P14-1069,1,0.781241,"rticles 5061-5558 are used as the training set, articles 5000-5030 are used as the development set and articles 5031-5060 are used as the test set. The train/development/test ratio in this dataset is 7608/1264/1277. As the data are drawn from the Chinese Treebank where words are manually segmented, we will simply use the gold segmentation in our experiments. We then process the whole Chinese dataset using the Stanford CoreNLP (Manning et al., 2014) toolkit to get the POS and Named Entity tags. To get the dependency parse for the Chinese data, we use the transition-based constituent parser in (Wang and Xue, 2014) to first parse the Chinese sentences into constituent trees, which are then transformed into dependency trees using the converter in the Stanford CoreNLP toolkit. Note that this Chinese constituent parser also uses the Chinese Treebank 8.0 to train its model. To avoid training on the parser on AMR test set, we train the constituent parser using a 10-fold cross-validation with each fold parsed using a model trained on the other 9 folds. In order to compare results between Chinese and English, we also train an English AMR parsing model on the LDC2015E86 dataset used in SemEval 2016 Task 8 with"
N18-2040,D16-1065,0,0.135284,"sentences have reentrancies, meaning that they have a graph structure that cannot be represented with a tree representation. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation where the meaning of a sentence is encoded as a rooted, directed and acyclic graph. AMR parsing has received a significant amount of attention in the NLP research community. Since the release of the AMR bank a number of AMR parsers have been developed in recent years (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016). The initial benefit of AMR parsing has also been demonstrated in various downstream applications such as Information Extraction (Pan et al., 2015; Huang et al., 2016), Machine Comprehension (Sachan and Xing, 2016), and Natural Language Generation (Flanigan et al., 2016; Butler, 2016). In this paper, we present the first AMR parser built using the Chinese AMR Bank (Li et al., 2016). We adopt the transition-based parsing framework first proposed for English (Wang et al., 2015b, 2016), where AMR parsing is modeled as a dependency tree to AMR graph transformation using a s"
N19-1095,D07-1057,0,\N,Missing
N19-1095,D10-1062,0,\N,Missing
N19-1095,P11-2037,0,\N,Missing
N19-1095,D10-1086,0,\N,Missing
N19-1095,P15-2051,1,\N,Missing
N19-1095,P07-1068,0,\N,Missing
N19-1095,N13-1125,1,\N,Missing
N19-1095,P15-2053,0,\N,Missing
N19-1095,N16-1174,0,\N,Missing
N19-1095,D17-1135,0,\N,Missing
O03-4002,J96-4004,0,0.0302375,"Missing"
O03-4002,J97-4004,0,0.0269066,"Missing"
O03-4002,P97-1041,0,0.0403494,"Missing"
O03-4002,W95-0107,0,0.067675,"Missing"
O03-4002,J96-3004,0,0.0190058,"Missing"
O03-4002,P98-2206,0,0.0197155,"Missing"
O03-4002,A94-1030,0,0.0365299,"Missing"
O03-4002,xia-etal-2000-developing,1,0.575318,"Missing"
O03-4002,C02-1145,1,0.38196,"Missing"
O03-4002,W96-0213,0,\N,Missing
O03-4002,O03-4001,0,\N,Missing
O03-4002,C98-2201,0,\N,Missing
P06-2118,W04-0807,0,0.0819859,"Missing"
P06-2118,P04-1038,1,0.849462,"segmented, POS-tagged and parsed version of the same text to facilitate the extraction of the different types of features. The extraction of the semantic role labels as features requires the use of a semantic role tagger, which we describe in greater detail in Section 2.2. In addition to using the semantic role labeling information, we also extract another type of semantic features from the verb’s NP arguments. These features are top-level semantic categories from a three-level general taxonomy for Chinese nouns, which was created semi-automatically based on two Chinese semantic dictionaries (Chen and Palmer, 2004). 2.1 A Comparison with Our English WSD System Similar to our English WSD system, which achieved the best published results on SENSEVAL2 English verbs for both finegrained and coarse-grained senses (Chen and Palmer, 2005), our Chinese WSD system uses the same smoothed MaxEnt machine learning model and linguistically motivated features for Chinese verb sense disambiguation. However, the features used in the two systems differ 922 somewhat due to the different properties of the two languages . For example, our English system uses the inflected form and the part-of-speech tag of the target verb a"
P06-2118,S01-1005,1,0.880072,"Missing"
P06-2118,J05-1004,1,0.415007,"勒 腰腰 集 tighten waistband collect 公公 (direct object)。 highway speech tags as collocation features. A further investigation on the different sizes of the context window (3,5,7,9,11) showed that increasing the window size decreased our system’s accuracy. 2.2 Features Based on Automatic Semantic Role Tagging In a recent paper on the WSD of English verbs, Dang and Palmer (2005) showed that semantic role information significantly improves the WSD accuracy of English verbs for both fine-grained and coarse-grained senses. However, this result assumes the human annotation of the Penn English Propbank (Palmer et al, 2005). It seems worthwhile to investigate whether the semantic role information produced by a fully automatic Semantic Role tagger can improve the WSD accuracy on verbs, and test the hypothesis that the senses of a verb have a high correlation to the arguments it takes. To that end, we assigned semantic role labels to the arguments of the target verb with a fully automatic semantic role tagger (Xue and Palmer, 2005) trained on the Chinese Propbank (CPB) (Xue and Palmer, 2003), a corpus annotated with semantic role labels that are similiar in style to the Penn English Propbank. In this annotation, c"
P06-2118,C02-1143,1,0.842499,"Missing"
P06-2118,P05-1006,1,0.846016,"ommon nouns that often require determiners such as the, a or an, Chinese common nouns can stand alone. Therefore, the direct object of a verb often occurs right after the verb in Chinese, as shown in (2). （2）动动 mobilize 资 funds 群群 people 开 build 勒勒 腰腰 集 tighten waistband collect 公公 (direct object)。 highway speech tags as collocation features. A further investigation on the different sizes of the context window (3,5,7,9,11) showed that increasing the window size decreased our system’s accuracy. 2.2 Features Based on Automatic Semantic Role Tagging In a recent paper on the WSD of English verbs, Dang and Palmer (2005) showed that semantic role information significantly improves the WSD accuracy of English verbs for both fine-grained and coarse-grained senses. However, this result assumes the human annotation of the Penn English Propbank (Palmer et al, 2005). It seems worthwhile to investigate whether the semantic role information produced by a fully automatic Semantic Role tagger can improve the WSD accuracy on verbs, and test the hypothesis that the senses of a verb have a high correlation to the arguments it takes. To that end, we assigned semantic role labels to the arguments of the target verb with a f"
P06-2118,W03-1707,1,0.835673,"r both fine-grained and coarse-grained senses. However, this result assumes the human annotation of the Penn English Propbank (Palmer et al, 2005). It seems worthwhile to investigate whether the semantic role information produced by a fully automatic Semantic Role tagger can improve the WSD accuracy on verbs, and test the hypothesis that the senses of a verb have a high correlation to the arguments it takes. To that end, we assigned semantic role labels to the arguments of the target verb with a fully automatic semantic role tagger (Xue and Palmer, 2005) trained on the Chinese Propbank (CPB) (Xue and Palmer, 2003), a corpus annotated with semantic role labels that are similiar in style to the Penn English Propbank. In this annotation, core arguments such as agent or theme are labeled with numbered arguments such as Arg0 and Arg1, up to Arg5 while adjunct-like elements are assigned functional tags such as TMP (for temporal), MNR, prefixed by ArgM. The Semantic Role tagger takes as input syntactic parses produced by the parser described above as input and produces a list of arguments for each of the sense-tagged target verbs and assigns argument labels to them. Features are extracted from both the core a"
P06-2118,N06-2015,1,0.814745,"Missing"
P06-2118,I05-3012,0,0.036656,"Missing"
P06-2118,I05-1081,1,\N,Missing
P06-2118,W04-0847,0,\N,Missing
P11-2111,H91-1060,0,0.0501973,"Missing"
P11-2111,W04-1101,0,0.215934,"able 3: Feature effectiveness 5 Related work There has been a fair amount of research on punctuation prediction or generation in the context of spoken 634 language processing (Lu and Ng, 2010; Guo et al., 2010). The task presented here is different in that the punctuation marks are already present in the text and we are only concerned with punctuation marks that are semantically ambiguous. Our specific focus is on the Chinese comma, which sometimes signals a sentence boundary and sometimes doesn’t. The Chinese comma has also been studied in the context of syntactic parsing for long sentences (Jin et al., 2004; Li et al., 2005), where the study of comma is seen as part of a “divide-and-conquer” strategy to syntactic parsing. Long sentences are split into shorter sentence segments on commas before they are parsed, and the syntactic parses for the shorter sentence segments are then assembled into the syntactic parse for the original sentence. We study comma disambiguation in its own right aimed at helping a wide range of NLP applications that include parsing and Machine Translation. 6 Conclusion The main goal of this short paper is to bring to the attention of the field a problem that has largely bee"
P11-2111,I05-2002,0,0.124404,"fectiveness 5 Related work There has been a fair amount of research on punctuation prediction or generation in the context of spoken 634 language processing (Lu and Ng, 2010; Guo et al., 2010). The task presented here is different in that the punctuation marks are already present in the text and we are only concerned with punctuation marks that are semantically ambiguous. Our specific focus is on the Chinese comma, which sometimes signals a sentence boundary and sometimes doesn’t. The Chinese comma has also been studied in the context of syntactic parsing for long sentences (Jin et al., 2004; Li et al., 2005), where the study of comma is seen as part of a “divide-and-conquer” strategy to syntactic parsing. Long sentences are split into shorter sentence segments on commas before they are parsed, and the syntactic parses for the shorter sentence segments are then assembled into the syntactic parse for the original sentence. We study comma disambiguation in its own right aimed at helping a wide range of NLP applications that include parsing and Machine Translation. 6 Conclusion The main goal of this short paper is to bring to the attention of the field a problem that has largely been taken for grante"
P11-2111,D10-1018,0,0.0168479,"lti-sentence group) first. The sentence segmentation can thus come before parsing in the processing pipeline even in a language like Chinese where sentences are not unambiguously marked. all - (1,2) -10 -11 -4 -5 -6 -7 -9 -8 -3 overall 89.2 87.5 87.8 88.6 89.0 89.1 89.1 89.1 89.1 89.2 89.4 f1 (EOS) 70.1 67.7 67.5 68.6 69.6 69.5 69.9 70.1 69.7 70.5 70.5 f1 (non-EOS) 93.4 92.3 92.5 93.1 93.3 93.3 93.4 93.4 93.3 93.4 93.5 Table 3: Feature effectiveness 5 Related work There has been a fair amount of research on punctuation prediction or generation in the context of spoken 634 language processing (Lu and Ng, 2010; Guo et al., 2010). The task presented here is different in that the punctuation marks are already present in the text and we are only concerned with punctuation marks that are semantically ambiguous. Our specific focus is on the Chinese comma, which sometimes signals a sentence boundary and sometimes doesn’t. The Chinese comma has also been studied in the context of syntactic parsing for long sentences (Jin et al., 2004; Li et al., 2005), where the study of comma is seen as part of a “divide-and-conquer” strategy to syntactic parsing. Long sentences are split into shorter sentence segments o"
P11-2111,J93-2004,0,0.0395946,"P ADVP VP VV VP NP ， PU 这些 法规性 文件 NP 专门 队伍 Obtaining data To our knowledge, there is no data in the public domain with commas explicitly annotated based on whether they mark sentence boundaries. One could imagine using parallel data where a Chinese sentence is word-aligned with multiple English sentences, but such data is generally noisy and commas are not disambiguated based on a uniform standard. We instead pursued a different path and derived our training and test data from the Chinese Treebank (CTB). The CTB does not disambiguate commas explicitly, and just like the Penn English Treebank (Marcus et al., 1993), the sentence boundaries in the CTB are identified by periods, exclamation and question marks. However, there are clear syntactic patterns that can be used to disambiguate the two types of commas. Commas that mark sentence boundaries delimit loosely coordinated toplevel IPs, as illustrated in Figure 1, and commas that don’t cover all other cases. One such example is Figure 2, where a PP is separated from the rest of the sentence with a comma. We devised a heuristic algorithm to detect loosely coordinated structures in the Chinese Treebank, and labeled each comma with either EOS (end of a sent"
P11-2111,N07-1051,0,0.00502977,"devised a heuristic algorithm to detect loosely coordinated structures in the Chinese Treebank, and labeled each comma with either EOS (end of a sentence) or Non-EOS (not the end of a sentence). 3 IP Learning After the commas are labeled, we have basically turned comma disambiguation into a binary classification problem. The syntactic structures are an obvious source of information for this classification task, so we parsed the entire CTB 6.0 in a roundrobin fashion. We divided CTB 6.0 into 10 portions, and parsed each portion with a model trained on other portions, using the Berkeley parser (Petrov and Klein, 2007). The labels for the commas are derived 632 VP VV NP 进行 监督 检查 Figure 1: Sentence-boundary denoting comma IP PP PU P NP 据 介绍 NP ， DNP NP 这 十四 个 城市 NP NP VP PU 步伐 VV 。 DEG 城市 建设 和 合作区 开发 建设 加快 的 Figure 2: Non-sentence boundary denoting comma from the gold-standard parses using the heuristics described in Section 2, as they obviously should be. We first established a baseline by applying the same heuristic algorithm to the automatic parses. This will give us a sense of how accurately commas can be disambiguated given imperfect parses. The research question we’re trying to address here basically i"
P11-2111,A97-1004,0,0.191385,"Missing"
P12-1008,W04-2703,0,0.11548,"jects insertion of a connective even if it expresses the underlying discourse relation exactly (or sometimes, maybe the wording itself is the reason for not having a connective). So to try to insert a connective expression may very well be too hard a task for annotators, with little to show for their effort in the end. Furthermore, the inter-annotator agreement for providing an explicit connective in place of an implicit one is computed based on the type of explicit connectives (e.g. cause-effect relations, temporal relations, contrastive relations, etc.), rather than based on their identity (Miltsakaki et al., 2004). This suggests that a reasonable degree of agreement for such a task may only be reached with a coarse classification scheme. Given the above two considerations, our solution is to annotate implicit discourse relations with their senses directly, bypassing the step of inserting a connective expression. It has been pointed out that to train annotators to reason about pre-defined abstract relations with high reliability might be too hard a task (Prasad et al., 2007). This difficulty can be overcome by associating each semantic type with one or two prototypical explicit connectives and asking an"
P12-1008,prasad-etal-2008-penn,0,0.567841,"Missing"
P12-1008,W05-0312,1,0.866439,"tical situation with its (possible) consequences);1 • Subtype: A third level of subtypes is defined for some, but not all, types. For instance, under the type “CONTINGENCY: Cause”, there are two subtypes: “reason” (for cases like because and since) and “result” (for cases like so and as a result). It is worth noting that a type of implicit relation, namely those labeled as “EntRel”, is not part of the sense hierarchy since it has no explicit counterpart. 3 Adapted scheme for Chinese 3.1 Key characteristics of Chinese text Despite similarities in discourse features between Chinese and English (Xue, 2005), there are differences that have a significant impact on how discourse relations could be best annotated. These differences can be illustrated with (5): (5) 据悉 ， [AO1 东莞 海关 according to reports , Dongguan Customs 共 接受 企业 合同 备案 in total accept company contract record 八千四百多 份 ] ，[AO2 比 试点 8400 plus CLASS , compare pilot 前 略 有 上升 ] ，[AO3 企业 before slight EXIST increase , company 1 There is another dimension to this level, i.e. literal or pragmatic use. If this dimension is taken into account, there could be said to be four types: “Cause”, “Pragmatic Cause”, “Condition”, and “Pragmatic Condition”"
P12-1008,W01-1605,0,\N,Missing
P12-1083,W04-0211,0,0.0408924,"Missing"
P12-1083,I11-1170,0,0.161926,"Missing"
P12-1083,P02-1047,0,0.117732,"Missing"
P12-1083,N03-1030,0,0.0375936,"Missing"
P12-1083,P11-2111,1,0.843714,"scourse units in a subordination relation (SUBORD). Based on the levels of embedding and the syntactic category of the coordinated structures, we define three different types of coordination (SB, IP COORD and VP COORD). We also define three types of subordination relations (ADJ, COMP, Sent SBJ), based on the syntactic structure. As we will show below, each of the six relations has a clear syntactic pattern that can be exploited for their automatic detection. ALL RELATION OTHER COORD SB COORD_IP COORD_VP ADJ SUBORD COMP Sent_SBJ Figure 1: Comma classification Sentence Boundary (SB): Following (Xue and Yang, 2011), we consider the loosely coordinated IPs that are the immediate children of the root IP to be independent sentences, and the commas separating them to be delimiters of sentence boundary. This is illustrated in (2), where a Chinese sentence can be split into two independent shorter sentences at the comma. We view this comma to be a marker of the sentence boundary and it serves the same function as the unambiguous sentence boundary delimitors (periods, question marks, exclamation marks) in Chinese. The syntactic pattern that is used to infer this relation is illustrated in (b). (2) 广东省 建立 了 自然"
P12-1083,N07-1051,0,0.172697,"c annotation of the CTB and attached to instances of the comma in the corpus. This makes it possible for us to train supervised models to automatically classify the commas in any Chinese text. 790 3 Two comma classification methods Given the gold standard parses, based on the syntactic patterns described in Section 2, we can map the POS tag of each comma instance in the CTB to one of the seven classes described in Section 2. Using this relabeled data as training data, we experimented with two automatic comma disambiguation methods. In the first method, we simply retrained the Berkeley parser (Petrov and Klein, 2007) on the relabeled data and computed how accurately the commas are labeled in a held-out test set. In the second method, we trained a Maximum Entropy classifier with the Mallet (McCallum et al., 2002) machine learning package to classify the commas. The features are extracted from the CTB data automatically parsed with the Berkeley parser. We implemented features described in (Xue and Yang, 2011), and also experimented with a set of new features as follows. In general, these new features are extracted from the two text spans surrounding the comma. Given a comma, we define the preceding text spa"
P12-1083,H91-1060,0,0.0276931,"Missing"
P12-1083,prasad-etal-2008-penn,0,0.130423,"ranslation, for example, it is very typical for “one” Chinese sentence to be translated into multiple English sentences, with each comma-separated segment corresponding to one English sentence. In the present work, we expand this view and propose to look at the Chinese comma in the context of discourse analysis. The Chinese comma is viewed as a delimiter of elementary discourse units (EDUs), in the sense of the Rhetorical Structure Theory (Carlson et al., 2002; Mann et al., 1988). It is also considered to be the anchor of discourse relations, in the sense of the Penn Discourse Treebank (PDT) (Prasad et al., 2008). Disambiguating the comma is thus necessary for the purpose of discourse segmentation, the identification of EDUs, a first step in building up the discourse structure of a Chinese text. Developing a supervised or semi-supervised model of discourse segmentation would require ground truth annotated based on a well-established representation scheme, but as of right now no such annotation exists for Chinese to the best of our knowledge. However, syntactically annotated treebanks often contain important clues that can be used to infer discourse-level information. We present 786 Proceedings of the"
P12-1083,W04-1101,0,0.0753649,"Missing"
P12-1083,I05-2002,0,0.244381,"Missing"
P12-1083,P89-1010,0,0.0967148,"ion of predi and sbjj 3. whether the conjunction of sbji and predj occurs more than 2 times in the auxiliary corpus when j does not have a subject. 4. whether the conjunction of obji and predj occurs more than 2 times in the auxiliary corpus when j does not have a subject 5. Whether the conjunction of predi and sbjj occurs more than 2 times in the auxiliary corpus when i does not have a subject. Mutual Information features: Mutual information is intended to capture the association strength between the subject of a previous span and the predicate of the current span. We use Mutual Information (Church and Hanks, 1989) as shown in Equation (1) and the frequency count computed based on the auxiliary corpus to measure such constraints. M I = log2 # co-occur of S and P * corpus size # S occur * # P occur (1) 1. The conjunction of sbji and predj when j does not have a subject if their M Ivalue is greater than -8.0, an empirically established threshold. 2. Whether obji and predj has an MI value greater than 5.0 if j does not have a subject. 3. Whether the MI value of sbji and predj is greater than 0.0, and they occur 2 times in the auxiliary corpus when j doesn’t have a subject. 4. Whether the MI value of obji a"
P12-1083,miltsakaki-etal-2004-penn,0,\N,Missing
P12-1083,H05-1033,0,\N,Missing
P13-2110,D12-1046,0,0.577613,"orithm adjusts the Lagrange multiplier values based on the differences between ( ) and ( ) (line 8). A crucial point is that the argmax problems in line 3 and line 4 can be solved efficiently using the original decoding algorithms, because the Lagrange multiplier can be regarded as adjustments for lexical rule probabilities and word probabilities. 4 Experiments We conduct experiments on the Chinese Treebank Version 5.0 and use the standard data split 625 (Petrov and Klein, 2007). The traditional evaluation metrics for POS tagging and parsing are not suitable for the joint task. Following with Qian and Liu (2012), we redefine precision and recall by computing the span of a constituent based on character offsets rather than word offsets. 4.1 from our word segmentation system as input and “Lattice-based Parser” represents the system taking the compacted word lattice as input. We find the lattice-based parser gets better performance than the pipeline system among all three subtasks. Performance of the Basic Sub-systems We train the word segmentation system with 100 iterations of the Maximum Entropy model using the OpenNLP toolkit. Table 1 shows the performance. It shows that our word segmentation system"
P13-2110,D10-1001,0,0.0231916,"timization problem by optimizing the dual problem. First, we introduce a vector of Lagrange multipliers ( , , ) for each equality constraint. Then, the Lagrangian is formulated as: ( , , )= + ( )+ , , ( ) ( , , )( ( , , ) − ( , , )) ( )− , , (, , ) (, , ) (, , ) (, , ) , , Then, the dual objective is Combined Optimization Between The Lattice-based POS Tagger and The Lattice-based Parser , ( )+ and , ( ) = max ( , , ) = max ( )+ max ( )− , , , , , (, , ) (, , ) + (, , ) (, , ) The dual problem is to find min ( ). We use the subgradient method (Boyd et al., 2003) to minimize the dual. Following Rush et al. (2010), we define the subgradient of ( ) as: ( , , ) = ( , , ) − ( , , ) for all ( , , ) Then, adjust ( , , ) as follows: ( , , ) = ( , , ) − ( ( , , ) − ( , , )) where &gt;0 is a step size. Algorithm 1: Combined Optimization 1: Set ( ) ( , , )=0, for all ( , , ) 2: For k=1 to K ( ) + ∑ , , ( )( , , ) ( , , ) 3: ( ) ← argmax ( ) ( ) − ∑ , , ( )( , , ) ( , , ) 4: ← argmax ( )( ( ) ( , , ) for all ( , , ) 5: If , , )= 6: Return ( ( ) , ( ) ) 7: Else ( ) (, , )= 8: ( )( , , ) − ( ( ) ( , , ) − ( ) ( , , )) Algorithm 1 presents the subgradient method to solve the dual problem. The algorithm initializes the"
P13-2110,P11-1139,0,0.0605215,"Missing"
P13-2110,A00-2018,0,0.323933,"Missing"
P13-2110,O03-4002,1,0.833333,"Missing"
P13-2110,W02-1001,0,0.0115278,"one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for al"
P13-2110,D10-1082,0,0.157435,"Missing"
P13-2110,P11-2124,0,0.0248862,"o each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calcula"
P13-2110,C08-1049,0,0.135876,"compact the N-best lists into a word lattice by collapsing all the identical words into one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a"
P13-2110,P09-1058,0,0.0593855,"Missing"
P13-2110,P05-1010,0,0.0149397,"word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calculated as: ( [ ]) = ( [ ] → [ ] [ ]) × × ( ) ( [ ]→ By grouping the terms that depend on we rewrit"
P13-2110,P06-1055,0,0.0363155,"S tagger with 20 iterations of the average perceptron algorithm. Table 2 presents the joint word segmentation and POS tagging performance and shows that our lattice-based POS tagger obtains results that are comparable with state-of-the-art systems. (Kruengkrai et al., 2009) (Zhang and Clark, 2010) (Qian and Liu, 2012) (Sun, 2011) Lattice-based POS tagger P 93.28 93.1 93.64 R 94.07 93.96 93.87 F 93.67 93.67 93.53 94.02 93.75 Table 2: POS tagging evaluation. We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al., 2006). Table 3 shows the performance, where the “Pipeline Parser” represents the system taking one-best segmentation result Pipeline Parser Lattice-based Parser P 96.97 92.01 80.86 97.73 93.24 81.83 Seg. POS Parse Seg. POS Parse R 98.06 93.04 81.47 97.66 93.18 81.71 F 97.52 92.52 81.17 97.70 93.21 81.77 Table 3: Parsing evaluation. 4.2 Performance of the Framework For the lattice-based framework, we set the maximum iteration in Algorithm 1 as K = 20. The step size is tuned on the development set and empirically set to be 0.8. Table 4 shows the parsing performance on the test set. It shows that the"
P13-2110,N07-1051,0,0.253157,"based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building. A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks. 1 Introduction Previous work on syntactic parsing generally assumes a processing pipeline where an input sentence is first tokenized, POS-tagged and then parsed (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). This approach works well for languages like English where automatic tokenization and POS tagging can be performed with high accuracy without the guidance of the highlevel syntactic structure. Such an approach, however, is not optimal for languages like Chinese where there are no natural delimiters for word boundaries, and word segmentation (or tokenization) is a non-trivial research problem by itself. Errors in word segmentation would propagate to later processing stages such as POS tagging and syntactic parsing. More importantly, Chinese is a language that lacks the morphological clues that"
P13-2110,J03-4003,0,\N,Missing
P13-2110,I11-1035,0,\N,Missing
P13-2110,P12-1026,0,\N,Missing
P14-1069,I11-1136,0,0.102266,"into P0 16: beami+1 ← k best states of P0 17: return the best state in beam2n−1 With such an action, POS tagging becomes a natural part of transition-based parsing. However, some feature templates in Table 1 become unavailable, because POS tags for the look-ahead words are not specified yet under the joint framework. For example, for the template q0 wt , the POS tag of the first word q0 in the queue β is required, but it is not specified yet at the present state. To overcome the lack of look-ahead POS tags, we borrow the concept of delayed features originally developed for dependency parsing (Hatori et al., 2011). Features that require look-ahead POS tags are defined as delayed features. In these features, look-ahead POS tags are taken as variables. During parsing, delayed features are extracted and passed from one state to the next state. When a sh-x action is performed, the look-ahead POS tag of some delayed features is specified, therefore these delayed features can be transformed into normal features (by replacing variable with the newly specified POS tag). The remaining delayed features will be transformed similarly when their look-ahead POS tags are specified during the following parsing steps."
P14-1069,D09-1087,0,0.0517862,"Missing"
P14-1069,P08-1067,0,0.0187858,"les. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based parser can use arbitrarily complex structural features. Therefore, we can concisely utilize non-local features in a singleTable 7 presents the statistics of frequent POS tagging error patterns. We can see that JointParsing system disambiguates {VV, NN} and {DEC, DEG} better than Pipeline system, but cannot deal with the NN→JJ pattern very well. StateAlign system got bette"
P14-1069,P08-1068,0,0.0229619,"eighbours NGramTree Heads Wproj Word Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005) and semi-supervised word cluster features (Koo et al., 2008). Table 2 lists all the non-local features we want to use. These features have been proved very helpful for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005). But almost all previous work considered non-local features only in parse reranking frameworks. Instead, we attempt to extract non-local features from newly constructed subtrees during the decoding process as they become incrementally available and score newly generated parser states with them. One difficulty is that the subtrees built by our baseline parser are binary trees (only the complete parse tree is debinariz"
P14-1069,P13-2018,0,0.0358112,"Missing"
P14-1069,P05-1022,0,0.130415,"y identified by its path from the root, and represented as a bit-string. By using various length of prefixes of the bit-string, we can produce word clusters of different granularities (Miller et al., 2004). Inspired from Koo et al. (2008), we employ two types of word clusters: (1) taking 4 bit-string prefixes of word clusters as replacements of POS tags, and (2) taking 8 bit-string prefixes as replacements of words. Using these two types of clusters, we construct semi-supervised word cluster features by mimicking the template structure of the original baseline features in Table 1. beams. 3.3 (Charniak and Johnson, 2005) CoPar HeadTree CoLenPar RightBranch Heavy Neighbours NGramTree Heads Wproj Word Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005)"
P14-1069,N04-1043,0,0.0427075,"Missing"
P14-1069,A00-2018,0,0.215926,"produced by the Pipeline system. The JointParsing system reduced errors of all types produced by the Pipeline system except for the coordination error type (Coord). The StateAlign system corrected a lot of the NP-internal errors (NP Int.). The Nonlocal system and the Cluster system produced similar numbers of errors for all error types. The Nonlocal&Cluster system produced the Best numbers for all the error types. NPinternal errors are still the most frequent error type in our parsing systems. Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features ha"
P14-1069,N07-1051,0,0.0632134,"Missing"
P14-1069,J05-1003,0,0.0243004,"iak and Johnson, 2005) CoPar HeadTree CoLenPar RightBranch Heavy Neighbours NGramTree Heads Wproj Word Feature Extension One advantage of transition-based constituent parsing is that it is capable of incorporating arbitrarily complex structural features from the already constructed subtrees in σ and unprocessed words in β. However, all the feature templates given in Table 1 are just some simple structural features. To further improve the performance of our transition-based constituent parser, we consider two group of complex structural features: non-local features (Charniak and Johnson, 2005; Collins and Koo, 2005) and semi-supervised word cluster features (Koo et al., 2008). Table 2 lists all the non-local features we want to use. These features have been proved very helpful for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005). But almost all previous work considered non-local features only in parse reranking frameworks. Instead, we attempt to extract non-local features from newly constructed subtrees during the decoding process as they become incrementally available and score newly generated parser states with them. One difficulty is that the subtrees built by our baseline parse"
P14-1069,P06-1055,0,0.0298089,"Pipeline system. The JointParsing system reduced errors of all types produced by the Pipeline system except for the coordination error type (Coord). The StateAlign system corrected a lot of the NP-internal errors (NP Int.). The Nonlocal system and the Cluster system produced similar numbers of errors for all error types. The Nonlocal&Cluster system produced the Best numbers for all the error types. NPinternal errors are still the most frequent error type in our parsing systems. Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully u"
P14-1069,P04-1015,0,0.216455,"Missing"
P14-1069,W05-1513,0,0.605273,"roduction Constituent parsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a se733 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733–742, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into"
P14-1069,I11-1140,1,0.908945,"Missing"
P14-1069,P06-1054,0,0.126292,"arsing is one of the most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a se733 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733–742, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into the same beam, ther"
P14-1069,W09-3825,0,0.839428,"e most fundamental tasks in Natural Language Processing (NLP). It seeks to uncover the underlying recursive phrase structure of sentences. Most of the state-of-theart parsers are based on the PCFG paradigm and chart-based decoding algorithms (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Chart-based parsers perform exhaustive search with dynamic programming, which contributes to their high accuracy, but they also suffer from higher runtime complexity and can only exploit simple local structural information. Transition-based constituent parsing (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009) is an attractive alternative. It utilizes a se733 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733–742, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics determination, the accuracy of POS tagging improves, and this will in turn improve parsing accuracy. Second, we propose a novel state alignment strategy to align candidate parses with different action sizes during beam-search decoding. With this strategy, parser states and their unary extensions are put into the same beam, therefore the parsing model"
P14-1069,P13-1013,0,0.0123913,". The Nonlocal&Cluster system produced the Best numbers for all the error types. NPinternal errors are still the most frequent error type in our parsing systems. Related Work Joint POS tagging with parsing is not a new idea. In PCFG-based parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006), POS tagging is considered as a natural step of parsing by employing lexical rules. For transition-based parsing, Hatori et al. (2011) proposed to integrate POS tagging with dependency parsing. Our joint approach can be seen as an adaption of Hatori et al. (2011)’s approach for constituent parsing. Zhang et al. (2013) proposed a transition-based constituent parser to process an input sentence from the character level. However, manual annotation of the word-internal structures need to be added to the original Treebank in order to train such a parser. Non-local features have been successfully used for constituent parsing (Charniak and Johnson, 2005; Collins and Koo, 2005; Huang, 2008). However, almost all of the previous work use nonlocal features at the parse reranking stage. The reason is that the single-stage chart-based parser cannot use non-local structural features. In contrast, the transition-based pa"
P14-1069,P13-1043,0,0.477547,"taken as variables. During parsing, delayed features are extracted and passed from one state to the next state. When a sh-x action is performed, the look-ahead POS tag of some delayed features is specified, therefore these delayed features can be transformed into normal features (by replacing variable with the newly specified POS tag). The remaining delayed features will be transformed similarly when their look-ahead POS tags are specified during the following parsing steps. 3.2 We propose a novel method to align states during the parsing process instead of just aligning terminal states like Zhu et al. (2013). We classify all the actions into two groups according to whether they consume items in σ or β. sh-x, rl-x, and rr-x belong to consuming actions, and ru-x belongs to non-consuming action. Algorithm 2 gives the details of our method. It is based on the beam search decoding algorithm described in Algorithm 1. Different from Algorithm 1, Algorithm 2 is guaranteed to perform 2n − 1 parsing steps for an input sentence containing n words (line 2), and divides each parsing step into two parsing phases. In the first phase (line 4-9), each of the k states in beami is extended by consuming actions. In"
P14-1069,J03-4003,0,\N,Missing
P14-2033,D11-1090,0,0.0927471,"cter is not &apos;¨&apos;, the value of the LNG feature is set to &apos;O&apos;. The P KL values are real numbers and are sparse. A common solution to sparsity reduction is binning. We rank the P KL values between two adjacent characters in each patent from low to high, and then divide all values into five bins. Each bin is assigned a unique ID and all P KL values in the same bin are replaced by this ID. This ID is then used as the PKL feature value for the target character Ci . 201 Pointwise Mutual information (PMI) Pointwise Mutual information has been widely used in previous work on Chinese word segmentation (Sun and Xu, 2011; Zhang et al., 2013b) and it is a measure of the mutual dependence of two strings and reflects the tendency of two strings appearing in one word. In previous work, PMI statistics are gathered on the entire data set, and here we gather PMI statistics for each patent in an attempt to capture character strings with high PMI in a particular patent. The procedure for calculating PMI is the same as that for computing pseudo KL divergence, but the functions (1) and (2) are replaced with the following functions: Table 1: Training, development and test data on Patent data Data set # of words # of pate"
P14-2033,C10-2139,0,0.013387,"5.72 96.42 96.48 93.98 94.89 89.04 87.88 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genres (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above genres as they contain scientific and technical terms that require some special training to understand. There has been very little work in this area,"
P14-2033,Y96-1018,0,0.320166,"Missing"
P14-2033,P11-1139,0,0.132899,"Missing"
P14-2033,W03-1722,0,0.0559876,"Missing"
P14-2033,O97-3004,0,0.020979,"Missing"
P14-2033,P13-1072,0,0.0120005,"so show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 P 95.34 95.58 96.32 95.62 95.65 95.72 96.42 96.48 93.98 94.89 89.04 87.88 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genr"
P14-2033,P09-1059,0,0.107748,"right for an invention granted by the government to the inventor, and many of the patents have a high concentration of scientific and technical terms. From a machine learning perspective, these terms are hard to detect and segment because they are often &quot;new words&quot; that are not seen in everyday language. These technical Character type N-gram features We classify the characters in Chinese text into 4 types: Chinese characters or hanzi, English letters, numbers and others. Ti is the character type of Ci . The character type has been used in the previous works in various forms (Ng and Low, 2004; Jiang et al., 2009), and the 4 features we use are as follows: 200 Algorithm 1 Longest n-gram sequence extraction. Input: Sentences {si } in patent Pi ; Output: Longest n-gram sequence list for Pi ; 1: For each sentence si in Pi do: n-gram sequence extraction (2≤n≤length(si )); 2: Count the frequency of each n-gram sequence; 3: Delete the sequence if its frequency&lt;2; 4: Delete sequence i if it is contained in a longer sequence j; 5: All the remaining sequences form a longest ngram sequence list for Pi ; 6: return Longest n-gram sequences list. Algorithm 2 Pseudo KL divergence. Input: Sentences {si } in patent Pi"
P14-2033,C12-2073,0,0.180448,"2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genres (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above genres as they contain scientific and technical terms that require some special training to understand. There has been very little work in this area, and the only work that is devoted to Chinese word segmentation is (Guo et al., 2012), which reports 5 Conclusion In this paper, we presented an accurate characterbased word segmentation model for Chinese patents. Our contributions are two-fold. Our first contribution is that we have annotated a significant amount of Chinese pat"
P14-2033,O03-4002,1,0.942858,"he evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). 199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 199–205, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Method • Character type unigrams: Tk (k = i) • Character type bigrams: Tk Tk+1 (i−2 &lt; k &lt; i + 1) and Tk−1 Tk+1 (k = i) We adopt the character-based sequence labeling approach, first proposed in (Xue, 2003), as our modeling technique for its simplicity and effectiveness. This approach treats each sentence as a sequence of characters and assigns to each character a label that indicates its position in the word. In this paper, we use the BMES tag set to indicate the character positions. The tag set has four labels that represent for possible positions a character can occupy within a word: B for beginning, M for middle, E for ending, and S for a single character as a word. After each character in a sentence is tagged with a BMES label, a sequence of words can be derived from this labeled character"
P14-2033,I05-3025,0,0.173315,"ment-level features designed to capture the characteristics of patent data. main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 P 95.34 95.58 96.32 95.62 95.65 95.72 96.42 96.48 93.98 94.89 89.04 87.88 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such a"
P14-2033,P13-2031,0,0.0125627,"tics of patent data. main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 P 95.34 95.58 96.32 95.62 95.65 95.72 96.42 96.48 93.98 94.89 89.04 87.88 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu"
P14-2033,W04-3236,0,0.0354683,"character Chinese words, each of which is assigned a single POS tag. When extracting features for the target character Ci , if Ci is in this list, the POS tag of Ci is used as a feature for this target character. Character features (CF) When predicting the position of a character within a word, features based on its surrounding characters and their types have shown to be the most effective features for this task (Xue, 2003). There are some variations of these features depending on the window size in terms of the number of characters to examine, and here we adopt the feature templates used in (Ng and Low, 2004). Character N-gram features The N-gram features are various combinations of the surrounding characters of the candidate character Ci . The 10 features we used are listed below: • Character unigrams: Ck (i − 3 &lt; k &lt; i + 3) • Character bigrams: Ck Ck+1 (i − 3 &lt; k &lt; i + 2) and Ck−1 Ck+1 (k = i) 2.3 Document-level features A patent is a property right for an invention granted by the government to the inventor, and many of the patents have a high concentration of scientific and technical terms. From a machine learning perspective, these terms are hard to detect and segment because they are often &quot;n"
P14-2033,P13-1076,0,0.0132239,"tics of patent data. main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 P 95.34 95.58 96.32 95.62 95.65 95.72 96.42 96.48 93.98 94.89 89.04 87.88 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu"
P14-2033,C04-1081,0,0.220235,"Missing"
P14-2033,P07-1106,0,0.0615792,"experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 P 95.34 95.58 96.32 95.62 95.65 95.72 96.42 96.48 93.98 94.89 89.04 87.88 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genres (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above g"
P14-2033,J96-3004,0,0.188515,"Missing"
P14-2033,P13-2032,0,0.0777607,"he value of the LNG feature is set to &apos;O&apos;. The P KL values are real numbers and are sparse. A common solution to sparsity reduction is binning. We rank the P KL values between two adjacent characters in each patent from low to high, and then divide all values into five bins. Each bin is assigned a unique ID and all P KL values in the same bin are replaced by this ID. This ID is then used as the PKL feature value for the target character Ci . 201 Pointwise Mutual information (PMI) Pointwise Mutual information has been widely used in previous work on Chinese word segmentation (Sun and Xu, 2011; Zhang et al., 2013b) and it is a measure of the mutual dependence of two strings and reflects the tendency of two strings appearing in one word. In previous work, PMI statistics are gathered on the entire data set, and here we gather PMI statistics for each patent in an attempt to capture character strings with high PMI in a particular patent. The procedure for calculating PMI is the same as that for computing pseudo KL divergence, but the functions (1) and (2) are replaced with the following functions: Table 1: Training, development and test data on Patent data Data set # of words # of patent Training 345336 1"
P14-2033,D13-1031,0,0.0584063,"he value of the LNG feature is set to &apos;O&apos;. The P KL values are real numbers and are sparse. A common solution to sparsity reduction is binning. We rank the P KL values between two adjacent characters in each patent from low to high, and then divide all values into five bins. Each bin is assigned a unique ID and all P KL values in the same bin are replaced by this ID. This ID is then used as the PKL feature value for the target character Ci . 201 Pointwise Mutual information (PMI) Pointwise Mutual information has been widely used in previous work on Chinese word segmentation (Sun and Xu, 2011; Zhang et al., 2013b) and it is a measure of the mutual dependence of two strings and reflects the tendency of two strings appearing in one word. In previous work, PMI statistics are gathered on the entire data set, and here we gather PMI statistics for each patent in an attempt to capture character strings with high PMI in a particular patent. The procedure for calculating PMI is the same as that for computing pseudo KL divergence, but the functions (1) and (2) are replaced with the following functions: Table 1: Training, development and test data on Patent data Data set # of words # of patent Training 345336 1"
P14-2033,W06-0127,0,0.026843,"s designed to capture the characteristics of patent data. main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 P 95.34 95.58 96.32 95.62 95.65 95.72 96.42 96.48 93.98 94.89 89.04 87.88 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word"
P14-2033,C92-1019,0,\N,Missing
P15-1110,C14-1076,0,0.0522234,"Missing"
P15-1110,P05-1022,0,0.0395646,"Missing"
P15-1110,A00-2018,0,0.0909756,"entences that can be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an eff"
P15-1110,D14-1082,0,0.145268,"Missing"
P15-1110,E03-1002,0,0.330686,"but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Con1138 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1138–1147, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics DT NN VP MD That debt would VB be V"
P15-1110,P04-1013,0,0.0531726,"Missing"
P15-1110,D10-1002,0,0.0703841,"Missing"
P15-1110,P08-1067,0,0.0590737,"Missing"
P15-1110,P14-1130,0,0.0385871,"the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Co"
P15-1110,J93-2004,0,0.0509943,"ining has demonstrated its effectiveness as a way of initializing neural network models (Erhan et al., 2010). Since our model requires many run-time primitive units (POS tags and constituent labels), we employ an in-house shift-reduce parser to parse a large amount of unlabeled sentences, and pre-train the model with the automatically parsed data. Third, we utilize the Dropout strategy to address the overfitting probExperimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005) and the Wall Street Journal (WSJ) portion of Penn English Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 301-325 as the development set, and Articles 271300 as the testing set. For English, we use sections 2-21 for training, section 22 for developing and section 23 for testing. We also utilized some unlabeled corpora and used the word2vec2 toolkit to train word embeddings. For Chinese, we used the unlabeled Chinese Gigaword (LDC2003T09) and performed Chinese word segmentation using our in-house segmenter. For English, we randomly selected"
P15-1110,N06-1020,0,0.0318095,"Missing"
P15-1110,C10-1093,0,0.0603562,"Missing"
P15-1110,N07-1051,0,0.0498802,"t conversation (bc), weblogs (wb) and discussion forums (df). Since all of the mz domain data is already included in our training set, we only selected sample sentences from the other five domains as the test sets 5 , and made sure these test sets had no overlap with our treebank training, development and test sets. Note that we did not use any data from these five domains for training or development. The models are still the ones described in the previous subsection. The results are presented in Table 7. Although our “Supervised” model got slightly worse performance than the Berkeley Parser (Petrov and Klein, 2007), as shown in Table 5, it outperformed the Berkeley Parser on the cross-domain data sets. This suggests that the learned features can better adapt to cross-domain situations. Compared with the Berkeley Parser, on average our “Pretrain-Finetune” model is 3.4 percentage points better in terms of parsing accuracy, and 3.2 percentage points better in terms of POS tagging accuracy. We also presented the performance of our pre-trained model (“Only-Pretrain”). We found the “Only-Pretrain” model performs poorly on this cross-domain data sets. But even pretraining based on this less than competitive mo"
P15-1110,P06-1055,0,0.0172632,"n be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features"
P15-1110,W96-0213,0,0.123723,"l we cannot say for sure that this is the optimal subset of features for the parsing task. To cope with this problem, we propose to simultaneously optimize feature representation and parsing accuracy via a neural network model. Figure 2 illustrates the architecture of our model. Our model consists of input, projection, hidden and output layers. First, in the input layer, all primitive units (shown in Table 1(a)) are imported to the network. We also import the suffixes and prefixes of the first word in the queue, because these units have been shown to be very effective for predicting POS tags (Ratnaparkhi, 1996). Then, in the projection layer, each primitive unit is projected into a vector. Specifically, word-type units are represented as word embeddings, and other units are transformed into one-hot representations. The p0 w, p0 t,p0 c, p1 w, p1 t,p1 c, p2 w, p2 t,p2 c, p3 w, p3 t,p3 c p0l w, p0l c, p0r w, p0r c,p0u w, p0u c, p1l w, p1l c, p1r w, p1r c,p1u w, p1u c q0 w, q1 w, q2 w, q3 w trigrams p0 tc, p0 wc, p1 tc, p1 wc, p2 tc p2 wc, p3 tc, p3 wc, q0 wt, q1 wt q2 wt, q3 wt, p0l wc, p0r wc p0u wc, p1l wc, p1r wc, p1u wc p0 wp1 w, p0 wp1 c, p0 cp1 w, p0 cp1 c p0 wq0 w, p0 wq0 t, p0 cq0 w, p0 cq0 t q"
P15-1110,W05-1513,0,0.0851226,"egies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had"
P15-1110,W09-3825,0,0.392801,"nd more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the t"
P15-1110,P13-1043,0,0.4563,"ithm of high computational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after th"
P15-1110,P13-1045,0,0.111934,"Missing"
P15-1110,P14-1069,1,0.936891,"tational complexity in order Nianwen Xue Brandeis University 415 South St Waltham, MA, USA xuen@brandeis.edu to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking proces"
P15-1110,I11-1140,1,0.901748,"Missing"
P15-1110,P06-1054,0,0.0708317,"rence algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the lea"
P15-1110,W08-2102,0,\N,Missing
P15-1110,J03-4003,0,\N,Missing
P15-2051,P13-1081,0,0.797512,"l. 2002) machine learning package to tag each word token with one of the pronouns or None in one run. None indicates that there is no dropped pronoun before this word. We leveraged a set of lexical features from previous work (Yang and Xue. 2010 ). To our knowledge, the work we report here represents the first effort on dropped pronoun recovery on Chinese SMS/Chat data. As described in Section 2, SMS data is different from newswire data which is commonly used in previous work (Converse. 2006; Zhao and Ng. 2007; Peng and Araki. 2007; Kong and Zhou. 2010; Chung and Gildea 2010; Cai et al. 2011; Xiang et al. 2013) in many aspects. The frequency of pronoun being dropped is much higher in SMS/Chat data compared to newswire data. The distribution of dropped pronoun types in SMS data is also very different from that of newswire data. In SMS/Chat data, the identities of the participants who send the messages are critical in identifying the dropped pronoun type, while there is no participant information in newswire data. Thus, we also design a new set of context based features to capture the stylistic properties of text messages. Dropped pronoun annotation We annotated 684 Chinese SMS/Chat files following th"
P15-2051,J93-2004,0,0.0528503,". 1 2 . . . . . . . . . . . . . . . . . 2 . . . . . . . . . . . . . . . . . . . . . 1 1 . 1 . . . . . . . . . . . . . 6 14 5 9 . 5 1 3 2 4 . . 1 . . 1 . . . . . . . . . . . . . . . . . . report here, we are more interested in detecting dropped pronouns and determining what types of pronoun they are. Dropped pronoun detection is also related to Empty Category (EC) detection and resolution (Chung and Gildea 2010; Cai et al. 2011; Xiang et al. 2013), the aim of which is to recover long-distance dependencies, discontinuous constituents, and certain dropped elements in phrase structure treebanks (Marcus et al. 1993; Xue et al. 2005). In previous work on EC detection (Chung and Gildea 2010; Cai et al. 2011; Xiang et al. 2013), ECs are recovered from newswire data by leveraging lexical and syntactic information from each sentence. Context information beyond the current sentence is typically not used. When recovering dropped pronouns in SMS/Chat messages, it is crucially important to make use of information beyond the current sentence. Error Analysis From Table 3, which is a confusion matrix generated from results on the test set, showing the classification errors among different types, we can see that the"
P15-2051,N07-1051,0,0.0272325,"Missing"
P15-2051,D10-1086,0,0.559517,"Missing"
P15-2051,D13-1028,0,0.0305658,"Missing"
P15-2051,P11-2037,0,0.396913,"et (McCallum et al. 2002) machine learning package to tag each word token with one of the pronouns or None in one run. None indicates that there is no dropped pronoun before this word. We leveraged a set of lexical features from previous work (Yang and Xue. 2010 ). To our knowledge, the work we report here represents the first effort on dropped pronoun recovery on Chinese SMS/Chat data. As described in Section 2, SMS data is different from newswire data which is commonly used in previous work (Converse. 2006; Zhao and Ng. 2007; Peng and Araki. 2007; Kong and Zhou. 2010; Chung and Gildea 2010; Cai et al. 2011; Xiang et al. 2013) in many aspects. The frequency of pronoun being dropped is much higher in SMS/Chat data compared to newswire data. The distribution of dropped pronoun types in SMS data is also very different from that of newswire data. In SMS/Chat data, the identities of the participants who send the messages are critical in identifying the dropped pronoun type, while there is no participant information in newswire data. Thus, we also design a new set of context based features to capture the stylistic properties of text messages. Dropped pronoun annotation We annotated 684 Chinese SMS/Cha"
P15-2051,D10-1062,0,0.608128,"lassifier with the Mallet (McCallum et al. 2002) machine learning package to tag each word token with one of the pronouns or None in one run. None indicates that there is no dropped pronoun before this word. We leveraged a set of lexical features from previous work (Yang and Xue. 2010 ). To our knowledge, the work we report here represents the first effort on dropped pronoun recovery on Chinese SMS/Chat data. As described in Section 2, SMS data is different from newswire data which is commonly used in previous work (Converse. 2006; Zhao and Ng. 2007; Peng and Araki. 2007; Kong and Zhou. 2010; Chung and Gildea 2010; Cai et al. 2011; Xiang et al. 2013) in many aspects. The frequency of pronoun being dropped is much higher in SMS/Chat data compared to newswire data. The distribution of dropped pronoun types in SMS data is also very different from that of newswire data. In SMS/Chat data, the identities of the participants who send the messages are critical in identifying the dropped pronoun type, while there is no participant information in newswire data. Thus, we also design a new set of context based features to capture the stylistic properties of text messages. Dropped pronoun annotation We annotated 68"
P15-2051,baran-etal-2012-annotating,1,0.908607,"messages, namely dropped pronouns. It is well-known that Chinese is a pro-drop language, meaning pronouns can be dropped from a sentence without causing the sentence to become ungrammatical or incomprehensible when the identity of the pronoun can be inferred from the context. Pronouns can be dropped even in formal text genres like newswire, but the extent to which this happens and the types of pronouns that are dropped in text messages and formal genres like newswire are very different. For example, the most frequently dropped pronouns in Chinese newswire is the third person singular 它(“it”) (Baran et al. 2012 ), and one reason is that first and second person pronouns are rarely used in newswire in the first place. In contrast, in text B (我) 步行 或 坐车 (I) walk or take the bus “(I) walk or take the bus.” A (pleonastic) 看来 交通业 还是 (it) look like transportation 比较 发达 的. relatively developed “(It) looks like you have a relatively developed transportation system.” B (pleonastic) 下雪 (我) 就 不 能 (it) snow (I) then not can 上班 了 go to work ASP “When (it) snows, (I) cannot go to work.” B (它) 还可以 (it) OK “(It) is OK.” Detecting dropped pronouns involves first of all determining where in the sentence pronouns are 3"
P15-2051,N13-1125,1,0.84742,"Missing"
P15-2051,C10-2158,1,0.906662,"Missing"
P15-2051,D07-1057,0,\N,Missing
P15-2141,W13-2322,0,0.506583,"Missing"
P15-2141,P13-2131,0,0.239952,"arger and more diverse dataset (C HARNIAK (ON)) yields the best overall AMR parsing performance. Subsequent experiments are all based on this version of the Charniak parser. We first tune and evaluate our system on the newswire section of LDC2013E117 dataset. Then we show our parser’s performance on the recent LDC2014T12 dataset. 4.1 Experiments on LDC2013E117 We first conduct our experiments on the newswire section of AMR annotation corpus (LDC2013E117). The train/dev/test split of dataset is 4.0K/2.1K/2.1K, which is identical to the settings of JAMR. We evaluate our parser with Smatch v2.0 (Cai and Knight, 2013) on all the experiments. System Charniak (ON) Charniak Stanford Malt Turbo P .67 .66 .64 .65 .65 R .64 .62 .62 .61 .61 4.1.2 In Table 2 we present results from extending the transition-based AMR parser. All experiments are conducted on the development set. From Table 2, we can see that the I NFER action yields a 4 point improvement in F1 score over the C HAR NIAK (ON) system. Adding Brown clusters improves the recall by 1 point, but the F1 score remains unchanged. Adding semantic role features on top of the Brown clusters leads to an improvement of another 2 points in F1 score, and gives us th"
P15-2141,P05-1022,0,0.037047,"aluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xi"
P15-2141,W02-1001,0,0.150911,"coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding the new action significantly improves the parser performance, and (iii) semantic role information is beneficial to AMR parsing when used as features, while the Brown clusters do not make a difference and coreference information slightly hurts the AMR parsing performance. The rest of the paper is"
P15-2141,P14-1134,0,0.446603,"atures actually slightly hurts the performance. F1 .65 .64 .63 .63 .63 Table 1: AMR parsing performance on development set using different syntactic parsers. System Charniak (ON) +I NFER +I NFER+BROWN +I NFER+BROWN+SRL +I NFER+BROWN+SRL+COREF P .67 .71 .71 .72 .72 R .64 .67 .68 .69 .69 Impact of parser extensions F1 .65 .69 .69 .71 .70 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trai"
P15-2141,N04-1030,1,0.857032,"er et al., 2005), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding"
P15-2141,N06-2015,0,0.0238792,"er (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is available at http://cemantix.org/ontonotes/ontonotesamr-document-overlap.txt 860 System Our system JAMR (GitHub)2 JAMR (Flanigan et al., 2014) Stanford SHRG-based Wang"
P15-2141,W13-3516,1,0.458104,"Missing"
P15-2141,P08-1068,0,0.0291106,"coreference feature and semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For action that predicts the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by the semantic role labeler. For example, for partial graph in Figure 4, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is equal to node wants’s frameset want-01 as predicted by the semantic role labeler, the value of feature EQ FRAMESET is set to true. 2) IS ARGUMENT ."
P15-2141,P10-1040,0,0.182545,"ure 4: An example of coreference feature and semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For action that predicts the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by the semantic role labeler. For example, for partial graph in Figure 4, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is equal to node wants’s frameset want-01 as predicted by the semantic role labeler, the value of feature EQ FRAMESET is set to true"
P15-2141,J13-4004,0,0.0491347,"mation produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding the new action significantly improves the"
P15-2141,N15-1040,1,0.806,"igan et al. (2014) treat concept identification as a sequence labeling task and utilize a semiMarkov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. A competitive alternative to the MSCG approach is transition-based AMR parsing. Our previous work (Wang et al., 2015) describes a transition-based system that also involves two stages. In the first step, an input sentence is country ARG0-of name have-org-role-91 name ARG1 1 ARG1 person ARG2 op1 “South” op2 “Korea” minister name mod op1 foreign “Israel” Figure 1: An example showing abstract concept have-org-role-91 for the sentence “Israel foreign minister visits South Korea.” Unlike a dependency parse where each leaf node corresponds to a word in a sentence and there is an inherent alignment between the words in a sentence and the leaf nodes in the parse tree, the alignment between the word tokens in a sente"
P15-2141,P14-5010,0,0.0057335,".71 .70 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We exc"
P15-2141,P15-1095,0,0.18983,"1 .65 .64 .63 .63 .63 Table 1: AMR parsing performance on development set using different syntactic parsers. System Charniak (ON) +I NFER +I NFER+BROWN +I NFER+BROWN+SRL +I NFER+BROWN+SRL+COREF P .67 .71 .71 .72 .72 R .64 .67 .68 .69 .69 Impact of parser extensions F1 .65 .69 .69 .71 .70 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank,"
P15-2141,P10-4014,0,0.0220356,"igure 3: I NFER -have-org-role-91 action 3.2 For action REENTRANCEbelieve -ARG1 SHARE DEPENDENT : true DEPENDENT LABEL : dobj Figure 4: An example of coreference feature and semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For action that predicts the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by the semantic role labeler. For example, for partial graph in Figure 4, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is"
P15-2141,P13-2109,0,0.010285,"f JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is"
P15-2141,H05-1066,0,0.0984606,"Missing"
P15-2141,P81-1022,0,0.772079,"Missing"
P15-2141,J05-1004,0,0.15216,"concepts, and existing AMR parsers do not have a systematic way of inferring such abstract concepts. 857 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 857–862, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Current AMR parsers are in their early stages of development, and their features are not yet fully developed. For example, the AMR makes heavy use of the framesets and semantic role labels used in the Proposition Bank (Palmer et al., 2005), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al.,"
P15-2141,nivre-etal-2006-maltparser,0,\N,Missing
P15-2141,K15-1004,0,\N,Missing
S16-1181,D15-1198,0,0.186545,"CG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of exi"
S16-1181,W13-2322,0,0.42263,"Missing"
S16-1181,P13-2131,0,0.176692,"ng Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator trained using a word sense disambiguation system— IMS (Zhong and Ng, 2010). All these components viz., the Charniak parser, Stanford named entity tagger, ASSERT, and IMS word sense disambiguator were retrained on the OntoNotes v5.0 training 1176 data2 (Pradhan et al., 2013)3 . We use the version of CAMR described in (Wang et al., 2015a) (without the feature extensions) as the baseline. We evaluate our parser with Smatch v2.0.2 (Cai and Knight, 2013) on all the experiments. It should be noted that all the rows in Table 2 except for the last one get implicitly penalized by the scorer for lack of wikification information. 5.1 SemEval Development Set As discussed in (Wang et al., 2015a), the performance of the syntactic parser in the first stage has a high impact on the AMR parsing accuracy. We first do a sanity check to choose the best first stage parser. Here we only consider two scenarios: the Charniak parser trained on WSJ and OntoNotes, as shown in Table 1. As using the Charniak parser trained on OntoNotes yields slightly better AMR par"
S16-1181,P05-1022,0,0.0786156,"(CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics labels produced by an automatic SRL system. The rest of the paper is organized as follows. In Section 2 we briefly describe CAMR, and in Section 3 we describ"
S16-1181,P14-1134,0,0.635756,"d step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Association for Computationa"
S16-1181,P14-5010,0,0.00352027,"ly on the Semeval release (c.). In Section 5.3 we also use the full test set of release (b.) to evaluate the performance improvement made to CAMR as part of the SemEval evaluations against previously reported performance. Syntactic Parser Charniak (ON) Charniak (WSJ) P 70.76 69.88 R 60.57 60.24 F1 65.27 64.70 Table 1: AMR parsing performance on the SemEval development set (LDC2015E86) across two Charniak parser models 5 Experiments We use the official release dataset and standard train/dev/test split of SemEval Task 8 for experiments. All the sentences are preprocessed using Stanford CoreNLP (Manning et al., 2014) to get tokenization, lemma, named entity tag, POS tag. And we use the aligner that comes with JAMR (Flanigan et al., 2014) to align the sentence with its AMR graph. We then parse the tokenized sentences using Charniak parser (Charniak and Johnson, 2005)(Its phrase structure output is converted to dependency structure using a slightly modified version of the Stanford CoreNLP converter). Rich named entity tags are generated using Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator tr"
S16-1181,H05-1066,0,0.057511,"Missing"
S16-1181,N15-1119,1,0.758601,"urrent head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it against the dependent. If the argument predicted by ASSERT matches the dependent, the value of the IS ARGUMENT is set to true. Word Clusters For the semi-supervised word cluster feature, we use Brown clusters, more specifically, the 1000-class word clusters trained by Turian et al. (2010). We use prefixes of lengths 4, 6, 10 and 20 of the word’s bit-string as features. 1175 3.2 Wikification We apply an AMR based wikification system (Pan et al., 2015) which utilizes AMR to represent semantic information about entity mentions expressed in their textual context. Given an entity mention m, this system first constructs a Knowledge Graph g(m) with m at the hub and leaf nodes obtained from entity mentions reachable by AMR graph traversal from m. A subset of the leaf nodes are selected as collaborators of m. Mentions connected by AMR conjunction relations are grouped into sets of coherent mentions. For each entity mention m, an initial ranked list of entity candidates E = (e1 , . . . , en ) is generated based on a salience measure (Medelyan and W"
S16-1181,K15-1004,0,0.347794,"Missing"
S16-1181,N04-1030,1,0.146339,"e input sentence is in the verbalization list. semantic role labeling: wants, want-01, ARG0: the boy, ARG1: the girl to believe him wants boy ARG1 girl believe him For action NEXT- NODE-want-01 EQ FRAMESET: true Figure 2: An example of semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For actions that predict the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by ASSERT (Pradhan et al., 2004). For example, in the partial graph in Figure 2, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is equal to node wants’s frameset want-01 as predicted by ASSERT, the value of feature EQ FRAMESET is set to true. 2) IS ARGUMENT. For actions that predict the edge label, we check whether ASSERT predicts that the current dependent is an argument of the current head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it agai"
S16-1181,W13-3516,1,0.868503,"Missing"
S16-1181,D15-1136,0,0.326487,"s used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Asso"
S16-1181,P10-1040,0,0.0357655,"EQ FRAMESET is set to true. 2) IS ARGUMENT. For actions that predict the edge label, we check whether ASSERT predicts that the current dependent is an argument of the current head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it against the dependent. If the argument predicted by ASSERT matches the dependent, the value of the IS ARGUMENT is set to true. Word Clusters For the semi-supervised word cluster feature, we use Brown clusters, more specifically, the 1000-class word clusters trained by Turian et al. (2010). We use prefixes of lengths 4, 6, 10 and 20 of the word’s bit-string as features. 1175 3.2 Wikification We apply an AMR based wikification system (Pan et al., 2015) which utilizes AMR to represent semantic information about entity mentions expressed in their textual context. Given an entity mention m, this system first constructs a Knowledge Graph g(m) with m at the hub and leaf nodes obtained from entity mentions reachable by AMR graph traversal from m. A subset of the leaf nodes are selected as collaborators of m. Mentions connected by AMR conjunction relations are grouped into sets of cohe"
S16-1181,P15-2141,1,0.766369,"014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial"
S16-1181,N15-1040,1,0.829287,"014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial"
S16-1181,P10-4014,0,0.0295827,"g. And we use the aligner that comes with JAMR (Flanigan et al., 2014) to align the sentence with its AMR graph. We then parse the tokenized sentences using Charniak parser (Charniak and Johnson, 2005)(Its phrase structure output is converted to dependency structure using a slightly modified version of the Stanford CoreNLP converter). Rich named entity tags are generated using Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator trained using a word sense disambiguation system— IMS (Zhong and Ng, 2010). All these components viz., the Charniak parser, Stanford named entity tagger, ASSERT, and IMS word sense disambiguator were retrained on the OntoNotes v5.0 training 1176 data2 (Pradhan et al., 2013)3 . We use the version of CAMR described in (Wang et al., 2015a) (without the feature extensions) as the baseline. We evaluate our parser with Smatch v2.0.2 (Cai and Knight, 2013) on all the experiments. It should be noted that all the rows in Table 2 except for the last one get implicitly penalized by the scorer for lack of wikification information. 5.1 SemEval Development Set As discussed in (Wa"
S19-1019,D17-1108,0,0.0658712,"ith high accuracy and agreement through crowdsourcing. We produce a corpus of temporal dependency trees, and present a baseline temporal dependency parser, trained and evaluated on this new corpus. 1 Introduction Temporal relation extraction is an important NLP task for a range of downstream applications, such as question answering, summarization, and storyline generation. This task has attracted a significant amount of research interest (Pustejovsky et al., 2003a; Verhagen et al., 2007, 2010; UzZaman et al., 2012; Bethard et al., 2016, 2017; Dligach et al., 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b; Zhang and Xue, 2018a,b). One practical challenge in temporal relation extraction is to represent the temporal relations in a text in a way that is feasible for manual annotation and producing training data for machine learning models. Given a text of n events n and time expressions, there are 2 possible relations if the temporal relation between all pairs of events and time expressions is annotated. This quickly becomes infeasible even for a text of modest length. One way to address this problem is to 178 Proceedings of the Eighth Joint Conference on Lexical and Computational Seman"
S19-1019,S16-1165,0,0.0226044,"otations, and show that this representation is intuitive and can be collected with high accuracy and agreement through crowdsourcing. We produce a corpus of temporal dependency trees, and present a baseline temporal dependency parser, trained and evaluated on this new corpus. 1 Introduction Temporal relation extraction is an important NLP task for a range of downstream applications, such as question answering, summarization, and storyline generation. This task has attracted a significant amount of research interest (Pustejovsky et al., 2003a; Verhagen et al., 2007, 2010; UzZaman et al., 2012; Bethard et al., 2016, 2017; Dligach et al., 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b; Zhang and Xue, 2018a,b). One practical challenge in temporal relation extraction is to represent the temporal relations in a text in a way that is feasible for manual annotation and producing training data for machine learning models. Given a text of n events n and time expressions, there are 2 possible relations if the temporal relation between all pairs of events and time expressions is annotated. This quickly becomes infeasible even for a text of modest length. One way to address this problem is to 178 P"
S19-1019,P18-1122,0,0.0669784,"either. 180 news websites. Their experiments show that the large crowdsourced data improved classifier performance significantly. However, both of these works focused on pair-wise temporal relations and didn’t experiment with crowdsourcing more complex temporal structures. Vempala and Blanco (2018) uses a crowdsourcing approach to collect temporal and spatial knowledge. However, they first automatically generated such knowledge and then used crowdsourcing to either validate or discard these automatically generated information, and crowdsourcing was not utilized to do annotation from scratch. Ning et al. (2018a) proposed a “multi-axis” representation of temporal relations in a text, and published the MATRES corpus by annotating “multiaxis” temporal structures on top of the TempEval3 data through crowdsourcing. In this representation, events are annotated on different “axes” according to their eventuality types, and for events on the same axis, pair-wise temporal relations are annotated. Their annotation task is broken down to two smaller subtasks too. In the first subtask, crowd workers annotate whether an event is on a given axis. In the second subtask, crowd workers annotate the temporal relation"
S19-1019,S17-2093,0,0.147921,"Missing"
S19-1019,S18-2018,0,0.0131264,"either. 180 news websites. Their experiments show that the large crowdsourced data improved classifier performance significantly. However, both of these works focused on pair-wise temporal relations and didn’t experiment with crowdsourcing more complex temporal structures. Vempala and Blanco (2018) uses a crowdsourcing approach to collect temporal and spatial knowledge. However, they first automatically generated such knowledge and then used crowdsourcing to either validate or discard these automatically generated information, and crowdsourcing was not utilized to do annotation from scratch. Ning et al. (2018a) proposed a “multi-axis” representation of temporal relations in a text, and published the MATRES corpus by annotating “multiaxis” temporal structures on top of the TempEval3 data through crowdsourcing. In this representation, events are annotated on different “axes” according to their eventuality types, and for events on the same axis, pair-wise temporal relations are annotated. Their annotation task is broken down to two smaller subtasks too. In the first subtask, crowd workers annotate whether an event is on a given axis. In the second subtask, crowd workers annotate the temporal relation"
S19-1019,P14-2082,0,0.291955,"(i.e. main verbs) in a sentence. In order to extract matrix verbs, we use the gold constituent trees for the part of TimeBank that overlaps with the Penn Treebank, and parse the rest of TimeBank with the Berkeley Neural Parser (Kitaev and Klein, 2018). All time expressions in TimeBank are kept. To facilitate quality control in crowdsourcing and agreement evaluation, we distinguish two subsets of the TimeBank dataset: (1) TB-small is a small subset of 10 short Wall Street Journal news documents with 59 matrix verbs. (2) TBdense consists of the same 36 documents as in the TimeBank-Dense corpus (Cassidy et al., 2014). It contains 654 matrix verbs. TB-small and TBdense are annotated by both crowd workers and experts. The rest of the paper is organized as follows. We first explain in detail how we set up this dependency tree crowdsourcing annotation task (§2). In (§3) we present experimental results that show that if temporal dependency structures are broken into smaller subtasks, high inter-annotator agreement can be achieved. In (§4), we show that crowdsource data can be used to successfully train temporal dependency parsers, including an attentionbased neural model (§4). We discuss related work (§5) and"
S19-1019,E17-2118,0,0.0176093,"representation is intuitive and can be collected with high accuracy and agreement through crowdsourcing. We produce a corpus of temporal dependency trees, and present a baseline temporal dependency parser, trained and evaluated on this new corpus. 1 Introduction Temporal relation extraction is an important NLP task for a range of downstream applications, such as question answering, summarization, and storyline generation. This task has attracted a significant amount of research interest (Pustejovsky et al., 2003a; Verhagen et al., 2007, 2010; UzZaman et al., 2012; Bethard et al., 2016, 2017; Dligach et al., 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b; Zhang and Xue, 2018a,b). One practical challenge in temporal relation extraction is to represent the temporal relations in a text in a way that is feasible for manual annotation and producing training data for machine learning models. Given a text of n events n and time expressions, there are 2 possible relations if the temporal relation between all pairs of events and time expressions is annotated. This quickly becomes infeasible even for a text of modest length. One way to address this problem is to 178 Proceedings of the Eighth Joi"
S19-1019,P18-2124,0,0.0294911,"Missing"
S19-1019,W10-0713,0,0.111058,"Missing"
S19-1019,D08-1027,0,0.189435,"Missing"
S19-1019,P18-1249,0,0.0148221,"gold temporal dependency trees and their job is just to label the temporal relation for each parent-child pair. Example text: depend-on Data Setup Our TDT annotations are performed on top of the TimeBank corpus (Pustejovsky et al., 2003b), with time expressions and events already extracted. Following (Zhang and Xue, 2018b), we focus only on events that are matrix verbs (i.e. main verbs) in a sentence. In order to extract matrix verbs, we use the gold constituent trees for the part of TimeBank that overlaps with the Penn Treebank, and parse the rest of TimeBank with the Berkeley Neural Parser (Kitaev and Klein, 2018). All time expressions in TimeBank are kept. To facilitate quality control in crowdsourcing and agreement evaluation, we distinguish two subsets of the TimeBank dataset: (1) TB-small is a small subset of 10 short Wall Street Journal news documents with 59 matrix verbs. (2) TBdense consists of the same 36 documents as in the TimeBank-Dense corpus (Cassidy et al., 2014). It contains 654 matrix verbs. TB-small and TBdense are annotated by both crowd workers and experts. The rest of the paper is organized as follows. We first explain in detail how we set up this dependency tree crowdsourcing annot"
S19-1019,L18-1052,0,0.0306672,"e second part is our expert-annotated TDTs on the TimeBankDense training set documents. The parser is tuned 2 https://github.com/yuchenz/tdp_ ranking 3 Standard TimeBank-Dense train/dev/test split can be found in Cassidy et al. (2014). 1 And for the same reason, Cohen’s kappa and Fleiss’ kappa scores are not applicable here either. 180 news websites. Their experiments show that the large crowdsourced data improved classifier performance significantly. However, both of these works focused on pair-wise temporal relations and didn’t experiment with crowdsourcing more complex temporal structures. Vempala and Blanco (2018) uses a crowdsourcing approach to collect temporal and spatial knowledge. However, they first automatically generated such knowledge and then used crowdsourcing to either validate or discard these automatically generated information, and crowdsourcing was not utilized to do annotation from scratch. Ning et al. (2018a) proposed a “multi-axis” representation of temporal relations in a text, and published the MATRES corpus by annotating “multiaxis” temporal structures on top of the TempEval3 data through crowdsourcing. In this representation, events are annotated on different “axes” according to"
S19-1019,E17-1108,0,0.0674282,"Missing"
S19-1019,S07-1014,0,0.0606129,"method to crowdsource temporal dependency tree annotations, and show that this representation is intuitive and can be collected with high accuracy and agreement through crowdsourcing. We produce a corpus of temporal dependency trees, and present a baseline temporal dependency parser, trained and evaluated on this new corpus. 1 Introduction Temporal relation extraction is an important NLP task for a range of downstream applications, such as question answering, summarization, and storyline generation. This task has attracted a significant amount of research interest (Pustejovsky et al., 2003a; Verhagen et al., 2007, 2010; UzZaman et al., 2012; Bethard et al., 2016, 2017; Dligach et al., 2017; Leeuwenberg and Moens, 2017; Ning et al., 2017, 2018a,b; Zhang and Xue, 2018a,b). One practical challenge in temporal relation extraction is to represent the temporal relations in a text in a way that is feasible for manual annotation and producing training data for machine learning models. Given a text of n events n and time expressions, there are 2 possible relations if the temporal relation between all pairs of events and time expressions is annotated. This quickly becomes infeasible even for a text of modest l"
S19-1019,C12-1129,0,0.469447,"del adapts better to new data sets than the LogReg model with manually-crafted languagespecific features. 5 Related Work Although crowdsourcing is widely used in other NLP tasks, there have been only a few temporal relation annotation tasks via crowdsourcing. The first attempt on crowdsourcing temporal relation annotations is described in Snow et al. (2008). They selected a restricted subset of verb events from TimeBank and performed strict before/after temporal relation annotation through crowdsourcing. They reported high agreements showing that simple temporal relations are crowdsourceable. Ng and Kan (2012) adopts the TimeML representation from the TimeBank, and crowdsourced temporal annotations on news articles crawled from 6 Conclusion and Future Work In this paper, we introduce a crowdsourcing approach for acquiring annotations on a relatively complex NLP concept – temporal dependency structures. We build the first English temporal dependency tree corpus through high quality crowdsourcing. Our system experiments show that competitive temporal dependency parsers can be trained on our newly collected data. In future work, we plan to crowdsource more TDT data across different domains. 181 Refere"
S19-1019,P11-1122,0,0.030719,"Missing"
S19-1019,D18-1371,1,0.411958,"n Figure 1 is annotated with a temporal relation. The number of temporal relations that need to be annotated in a text is therefore linear to the number of events and time expressions in a text, making the annotation task feasible. At the same time, additional temporal relations can be inferred as needed based on the TDT structure. For example, in Figure 1 since “1918” includes the “born” event and “1929” includes the “won” event, it can be inferred that the “born” event occurred before the “won” event. By providing annotators with detailed guidelines and training them in multiple iterations, Zhang and Xue (2018b) have shown that the TDT representation can be annotated with high interannotator agreement. Zhang and Xue (2018a) further show that a neural ranking model can be successfully trained on the corpus. However, this “traditional” approach to annotation is timeconsuming and expensive. The question we want to answer in this paper is whether TDT can be performed with crowdsourcing, a method that has gained popularity as a means to acquire linguistically annotated data quickly and cost-effectively for NLP research. Crowdsourcing has been used to annotate data for a wide range of NLP tasks that incl"
S19-1019,L18-1490,1,0.560568,"n Figure 1 is annotated with a temporal relation. The number of temporal relations that need to be annotated in a text is therefore linear to the number of events and time expressions in a text, making the annotation task feasible. At the same time, additional temporal relations can be inferred as needed based on the TDT structure. For example, in Figure 1 since “1918” includes the “born” event and “1929” includes the “won” event, it can be inferred that the “born” event occurred before the “won” event. By providing annotators with detailed guidelines and training them in multiple iterations, Zhang and Xue (2018b) have shown that the TDT representation can be annotated with high interannotator agreement. Zhang and Xue (2018a) further show that a neural ranking model can be successfully trained on the corpus. However, this “traditional” approach to annotation is timeconsuming and expensive. The question we want to answer in this paper is whether TDT can be performed with crowdsourcing, a method that has gained popularity as a means to acquire linguistically annotated data quickly and cost-effectively for NLP research. Crowdsourcing has been used to annotate data for a wide range of NLP tasks that incl"
W02-1815,J00-3004,0,0.086657,"Missing"
W02-1815,xia-etal-2000-developing,1,0.731633,"mentation accuracy drops sharply. In the second experiment, we applied the maximum entropy model to the problem of Chinese word segmentation. The results will show that this approach alone outperforms the state-of-the-art results reported in previous work in supervised machine-learning approaches. In the third experiment we combined the maximum entropy model with the error-driven transformationbased model. We used the error-driven transformation-based model to learn a set of rules to correct the errors produced by the maximum entropy model. The data we used are from the Penn Chinese Treebank (Xia et al. 2000, Xue et al. 2002) and they consist of Xinhua newswire articles. We took 250,389 words (426,292 characters or hanzi) worth of manually segmented data and divided them into two chunks. The first chunk has 237,791 words (404,680 Chinese characters) and is used as training data. The second chunk has 12,598 words (21,612 characters) and is used as testing data. These data are used in all three of our experiments. 3.1 Experiment One In this experiment, we conducted two subexperiments. In the first sub-experiment, we used a forward maximum matching algorithm to segment the testing data with a dictio"
W02-1815,C02-1145,1,0.862014,"y drops sharply. In the second experiment, we applied the maximum entropy model to the problem of Chinese word segmentation. The results will show that this approach alone outperforms the state-of-the-art results reported in previous work in supervised machine-learning approaches. In the third experiment we combined the maximum entropy model with the error-driven transformationbased model. We used the error-driven transformation-based model to learn a set of rules to correct the errors produced by the maximum entropy model. The data we used are from the Penn Chinese Treebank (Xia et al. 2000, Xue et al. 2002) and they consist of Xinhua newswire articles. We took 250,389 words (426,292 characters or hanzi) worth of manually segmented data and divided them into two chunks. The first chunk has 237,791 words (404,680 Chinese characters) and is used as training data. The second chunk has 12,598 words (21,612 characters) and is used as testing data. These data are used in all three of our experiments. 3.1 Experiment One In this experiment, we conducted two subexperiments. In the first sub-experiment, we used a forward maximum matching algorithm to segment the testing data with a dictionary compiled from"
W02-1815,W96-0213,0,\N,Missing
W03-1707,W00-1201,0,0.0258615,"al to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predicate-argument structure of lexical items are not captured. Recent effort in semantic annotation, the creation of the Penn Proposition Bank (Kingsbury and Palmer, 2002) on top of the Penn English Treebank is beginning to address this issue for English. In this new layer of annotation, the regularities of the predicates, mo"
W03-1707,A00-2018,0,0.00397732,"ions for this resource. 1 Introduction Linguistically interpreted corpora are instrumental in supervised machine learning paradigms of natural language processing. The information encoded in the corpora to a large extent determines what can be learned by supervised machine learning systems. Therefore, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predic"
W03-1707,P00-1058,0,0.012057,"e, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predicate-argument structure of lexical items are not captured. Recent effort in semantic annotation, the creation of the Penn Proposition Bank (Kingsbury and Palmer, 2002) on top of the Penn English Treebank is beginning to address this issue for English. In this new layer of annotation, the regularit"
W03-1707,P97-1003,0,0.0103606,", we discuss possible applications for this resource. 1 Introduction Linguistically interpreted corpora are instrumental in supervised machine learning paradigms of natural language processing. The information encoded in the corpora to a large extent determines what can be learned by supervised machine learning systems. Therefore, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, signif"
W03-1707,kingsbury-palmer-2002-treebank,1,0.867257,"USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predicate-argument structure of lexical items are not captured. Recent effort in semantic annotation, the creation of the Penn Proposition Bank (Kingsbury and Palmer, 2002) on top of the Penn English Treebank is beginning to address this issue for English. In this new layer of annotation, the regularities of the predicates, mostly verbs, are captured in the predicate-argument structure. For example, in the sentences “The Congress passed the bill” and “The bill passed”, it is intuitively clear that “the bill” plays the same role in the two occurrences of the verb “pass”. Similar regularities also exist in Chinese. For example, in “ /this /CL /bill /pass /AS” and “ /Congress /pass /AS /this /CL /bill”, “ /bill” also plays the same role for the verb “ /pass” even t"
W03-1707,J93-2004,0,0.0317252,"e then discuss how a lexical database with predicate-argument structure information can be used to ensure consistent annotation. Finally, we discuss possible applications for this resource. 1 Introduction Linguistically interpreted corpora are instrumental in supervised machine learning paradigms of natural language processing. The information encoded in the corpora to a large extent determines what can be learned by supervised machine learning systems. Therefore, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the inform"
W03-1707,J98-1001,0,\N,Missing
W03-1728,J95-4004,0,0.0372907,", pairwise voting (van Halteren et al., 1998) has 0.9148 HK 0.9146 0.9144 0.9142 0.914 X F-score been used to combine the results of two supertaggers that scan the input in the opposite directions. The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible. For example, an LM tag cannot immediately follow an MM. Pairwise voting does not use any contextual information, so it cannot prevent incompatible tags from occurring. Therefore, in our experiments described here, we use the Transformation-Based Learning (Brill, 1995) to combine the results of two MEMM taggers. The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). 0.9138 0.9136 0.9134 0.9132 0.913 0.9128 0.9126 100 150 200 250 300 iteration Figure 2: Learning curves on the development dataset of the HK City Univ. corpus. 3 Experiments 0.9391 PK 0.9389 0.9388 X F-score We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split the training data from each of t"
W03-1728,N01-1006,0,0.0277844,"at scan the input in the opposite directions. The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible. For example, an LM tag cannot immediately follow an MM. Pairwise voting does not use any contextual information, so it cannot prevent incompatible tags from occurring. Therefore, in our experiments described here, we use the Transformation-Based Learning (Brill, 1995) to combine the results of two MEMM taggers. The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). 0.9138 0.9136 0.9134 0.9132 0.913 0.9128 0.9126 100 150 200 250 300 iteration Figure 2: Learning curves on the development dataset of the HK City Univ. corpus. 3 Experiments 0.9391 PK 0.9389 0.9388 X F-score We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split the training data from each of the three sources into W ? of the official training data is two portions. @W used to train the MEMM taggers, and the other ? is held out as the development te"
W03-1728,P03-1064,1,0.729121,"ferty et al., 2001) that non-generative finite-state models, MEMM models included, share a weakness which they call the Label Bias Problem (LBP): a transition leaving a given state compete only against all other transitions in the model. They proposed Conditional Random Fields (CRFs) as a solution to address this problem. A partial solution to the LBP is to compute the probability of transitions in both directions. This way we can use two MEMM taggers, one of which scans the input from left to right and the other scans the input from right to left. This strategy has been successfully used in (Shen and Joshi, 2003). In that paper, pairwise voting (van Halteren et al., 1998) has 0.9148 HK 0.9146 0.9144 0.9142 0.914 X F-score been used to combine the results of two supertaggers that scan the input in the opposite directions. The pairwise voting is not suitable in this application because we must make sure that the LMR tags assigned to consecutive words are compatible. For example, an LM tag cannot immediately follow an MM. Pairwise voting does not use any contextual information, so it cannot prevent incompatible tags from occurring. Therefore, in our experiments described here, we use the Transformation-B"
W03-1728,J96-3004,0,0.017484,"ew hanzi. Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and a proper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word We represent the positions of a hanzi with four different tags (Table 1): LM for a hanzi that occurs on the left"
W03-1728,P98-1081,0,0.0114259,"Missing"
W03-1728,O03-4001,0,0.0147661,"ish text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and a proper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word We represent the positions of a hanzi with four different tags (Table 1): LM for a hanzi that occurs on the left periphery"
W03-1728,O03-4002,1,0.823395,"n which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters. Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places. This may sound simple enough but in reality identifying words in Chinese is a non-trivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003). The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and a proper way to handle out-of-vocabulary words. The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003). Given the proper context, generally provided by the sentence in which it occurs, the position of a hanzi can be determined. In this paper, we model the Chinese word We represent the positions of a hanzi with four different tags (Table 1): LM for a hanzi that occurs on the left periphery of a word,"
W03-1728,W96-0213,0,\N,Missing
W03-1728,J96-4004,0,\N,Missing
W03-1728,C98-1078,0,\N,Missing
W04-2704,S01-1001,0,0.0687376,"Missing"
W04-2704,W98-0315,1,0.884365,"Missing"
W04-2704,kingsbury-palmer-2002-treebank,1,0.773736,"ng or annotation. The Proposition Bank is designed as a broad-coverage resource to facilitate the development of more general systems. It focuses on the argument structure of verbs, and provides a complete corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming a component of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. 1 PropBank (Kingsbury & Palmer, 2002) is an annotation of the Wall Street Journal portion of the Penn Treebank II (Marcus, 1994) with `predicate-argument&apos; structures, using sense tags for highly polysemous words and semantic role labels for each argument. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG1 the window] and [ARG1 The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a shallow semantic analysis in th"
W04-2704,miltsakaki-etal-2004-penn,1,0.886331,"Missing"
W04-2704,W04-2807,1,\N,Missing
W04-2704,J00-4005,0,\N,Missing
W04-2704,W01-1605,0,\N,Missing
W04-2704,W03-1707,1,\N,Missing
W04-2704,kipper-etal-2004-extending,1,\N,Missing
W04-3212,kingsbury-palmer-2002-treebank,1,\N,Missing
W04-3212,J93-2004,0,\N,Missing
W04-3212,W03-1008,0,\N,Missing
W04-3212,W03-1707,1,\N,Missing
W04-3212,W03-1006,0,\N,Missing
W04-3212,J03-4003,0,\N,Missing
W04-3212,H94-1020,0,\N,Missing
W04-3212,P98-1013,0,\N,Missing
W04-3212,C98-1013,0,\N,Missing
W04-3212,J02-3001,0,\N,Missing
W04-3212,P02-1031,1,\N,Missing
W04-3212,P01-1017,0,\N,Missing
W04-3212,J05-1004,1,\N,Missing
W04-3212,N04-1030,0,\N,Missing
W04-3212,W04-2412,0,\N,Missing
W04-3212,N04-1032,0,\N,Missing
W05-0309,P98-1013,0,0.0755344,"Frameset. For instance, leave has both a DEPART Frameset ([ARG0 John] left [ARG1 the room]) and a GIVE Frameset, ([ARG0 I] left [ARG1 my pearls] [ARG2 to my daughter-inlaw] [ARGM-LOC in my will].) While most Framesets have three or four numbered roles, as many as six can appear, in particular for certain verbs of motion. Verbs can take any of a set of general, adjunct-like arguments (ARGMs), such as LOC (location), TMP (time), DIS (discourse connectives), PRP (purpose) or DIR (direction). Negations (NEG) and modals (MOD) are also marked. There are several other annotation projects, FrameNet (Baker et al., 1998), Salsa (Ellsworth et 62 al., 2004), and the Prague Tectogrammatics (Hajicova and Kucerova, 2002), that share similar goals. Berkeley.s FrameNet project, (Baker et al., 1998; Fillmore and Atkins, 1998; Johnson et al., 2002) is committed to producing rich semantic frames on which the annotation is based, but it is less concerned with annotating complete texts, concentrating instead on annotating a set of examples for each predicator (including verbs, nouns and adjectives), and attempting to describe the network of relations among the semantic frames. For instance, the buyer of a buy event and t"
W05-0309,P01-1017,0,0.0387506,"Missing"
W05-0309,hajicova-kucerova-2002-argument,0,0.0178125,"om]) and a GIVE Frameset, ([ARG0 I] left [ARG1 my pearls] [ARG2 to my daughter-inlaw] [ARGM-LOC in my will].) While most Framesets have three or four numbered roles, as many as six can appear, in particular for certain verbs of motion. Verbs can take any of a set of general, adjunct-like arguments (ARGMs), such as LOC (location), TMP (time), DIS (discourse connectives), PRP (purpose) or DIR (direction). Negations (NEG) and modals (MOD) are also marked. There are several other annotation projects, FrameNet (Baker et al., 1998), Salsa (Ellsworth et 62 al., 2004), and the Prague Tectogrammatics (Hajicova and Kucerova, 2002), that share similar goals. Berkeley.s FrameNet project, (Baker et al., 1998; Fillmore and Atkins, 1998; Johnson et al., 2002) is committed to producing rich semantic frames on which the annotation is based, but it is less concerned with annotating complete texts, concentrating instead on annotating a set of examples for each predicator (including verbs, nouns and adjectives), and attempting to describe the network of relations among the semantic frames. For instance, the buyer of a buy event and the seller of a sell event would both be Arg0.s (Agents) in PropBank, while in FrameNet one is the"
W05-0309,J93-2004,0,0.0248355,"Missing"
W05-0309,H94-1020,0,0.0374539,"nguistics icates has already begun at NYU. This paper describes the results of PropBank II, a project to provide richer semantic annotation to structures that have already been propbanked, specifically, eventuality ID.s, coreference, coarse-grained sense tags, and discourse connectives. Of special interest to the machine translation community is our finding, presented in this paper, that PropBank II annotation reconciles many of the surface differences of the two languages. 2 PropBank I PropBank (Palmer et al., 2005) is an annotation of the Wall Street Journal portion of the Penn Treebank II (Marcus et al., 1994) with ‘predicate-argument’ structures, using sense tags for highly polysemous words and semantic role labels for each argument. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG1 the window] and [ARG1 The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a shallow semantic analysis in that the annotation is close to the syntactic structure and each verb is its own predicate. I"
W05-0309,miltsakaki-etal-2004-penn,0,0.0153934,"e appropriateness of the grouped sense tags, and indicates potential for providing a useful level of granularity for MT. 3.3 Discourse connectives Another component of the Chinese / English Parallel Propbank II is the annotation of dis-course connectives for both Chinese corpus and its English translation. Like the other two components, the annotation is performed on the first 100K words of the Parallel Chinese English Treebank. The annotation of Chinese discourse connectives follows in large part the theoretic assumptions and annotation practices of the English Penn Discourse Project (PDTB) (Miltsakaki et al., 2004). Adaptations are made only when they are warranted by the linguistic facts of Chinese. While the English PTDB annotates both explicit and implicit discourse connectives, our iniNoun organization party investment development resolution English senses individuals working together event: putting things together state: the quality of being well-organization event: an occasion on which people can assemble for social interaction and entertainment political organization a band of people associated temporarily in some activity person or side in legal context time or money risked in hopes of profit th"
W05-0309,W04-2807,1,0.844354,"Missing"
W05-0309,J05-1004,1,0.556359,"ia Grant EIA02-05448 . event identifiers, and discourse and temporal relations, could provide the foundation for a major advance in our ability to automatically extract salient relationships from text. This will in turn facilitate breakthroughs in message understanding, machine translation, fact retrieval, and information retrieval. The Proposition Bank project is a major step towards providing this type of annotation. It takes a practical approach to semantic representation, adding a layer of predicate argument information, or semantic roles, to the syntactic structures of the Penn Treebank (Palmer et al., 2005). The Frame Files that provide guidance to the annotators constitute a rich English lexicon with explicit ties between syntactic realizations and coarse-grained senses, Framesets. PropBank Framesets are distinguished primarily by syntactic criteria such as differences in subcategorization frames, and can be seen as the toplevel of an hierarchy of sense distinctions. Groupings of fine-grained WordNet senses, such as those developed for Senseval2 (Palmer et al., to appear) provide an intermediate level, where groups are distinguished by either syntactic or semantic criteria. WordNet senses const"
W05-0309,W04-2704,1,\N,Missing
W05-0309,W05-0312,1,\N,Missing
W05-0309,W03-1707,1,\N,Missing
W05-0309,J03-4003,0,\N,Missing
W05-0309,C98-1013,0,\N,Missing
W05-0312,miltsakaki-etal-2004-penn,0,0.563479,"ind Johi and Martha Palmer for their comments. All errors are my own, of course. The goal of the Chinese Discourse Treebank (CDTB) Project is to add a layer of discourse annotation to the Penn Chinese Treebank (Xue et al., To appear), the bulk of which has also been annotated with predicate-argument structures. This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences. Like the Penn English Discourse Treebank (Miltsakaki et al., 2004a; Miltsakaki et al., 2004b), the CDTB project adopts the general idea presented in (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) where discourse connectives are considered to be predicates that take abstract objects such as propositions, events and situations as their arguments. This approach departs from the previous approaches to discourse analysis such as the Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2003) in that it does not start from a predefined inventory of abstract discourse relations. Instead, all discourse relations are lexically gro"
W05-0312,J05-1004,0,0.0414178,"Pie in the Sky, pages 84–91, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics rived anaphorically in the previous discourse. An advantage of this approach to discourse analysis is that discourse relations can be built up incrementally in a bottom-up manner and this advantage is magnified in large-scale annotation projects where interannotator agreement is crucial and has been verified in the construction of the Penn English Discourse Treebank (Miltsakaki et al., 2004a). This approach closely parallels the annotation of the the verbs in the English and Chinese Propbanks (Palmer et al., 2005; Xue and Palmer, 2003), where verbs are the anchors of predicate-argument structures. The difference is that the extents of the arguments to discourse connectives are far less certain, while the arity of the predcates is fixed for the discourse connectives. This paper outlines the issues that arise from the annotation of Chinese discourse connectives, with an initial focus on explicit discourse connectives. Section 2 gives an overview of the different kinds of discourse connectives that we plan to annotate for the CDTB Project. Section 3 surveys the distribution of the discourse connectives a"
W05-0312,W98-0315,0,0.0962228,"course Treebank (CDTB) Project is to add a layer of discourse annotation to the Penn Chinese Treebank (Xue et al., To appear), the bulk of which has also been annotated with predicate-argument structures. This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences. Like the Penn English Discourse Treebank (Miltsakaki et al., 2004a; Miltsakaki et al., 2004b), the CDTB project adopts the general idea presented in (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) where discourse connectives are considered to be predicates that take abstract objects such as propositions, events and situations as their arguments. This approach departs from the previous approaches to discourse analysis such as the Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2003) in that it does not start from a predefined inventory of abstract discourse relations. Instead, all discourse relations are lexically grounded and anchored by a discourse connective. The discourse relations so defined can be structural or anaph"
W05-0312,P99-1006,0,0.0793722,"roject is to add a layer of discourse annotation to the Penn Chinese Treebank (Xue et al., To appear), the bulk of which has also been annotated with predicate-argument structures. This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences. Like the Penn English Discourse Treebank (Miltsakaki et al., 2004a; Miltsakaki et al., 2004b), the CDTB project adopts the general idea presented in (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) where discourse connectives are considered to be predicates that take abstract objects such as propositions, events and situations as their arguments. This approach departs from the previous approaches to discourse analysis such as the Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2003) in that it does not start from a predefined inventory of abstract discourse relations. Instead, all discourse relations are lexically grounded and anchored by a discourse connective. The discourse relations so defined can be structural or anaphoric. Structural disc"
W05-0312,J03-4002,0,0.0226504,"yer of discourse annotation to the Penn Chinese Treebank (Xue et al., To appear), the bulk of which has also been annotated with predicate-argument structures. This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences. Like the Penn English Discourse Treebank (Miltsakaki et al., 2004a; Miltsakaki et al., 2004b), the CDTB project adopts the general idea presented in (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) where discourse connectives are considered to be predicates that take abstract objects such as propositions, events and situations as their arguments. This approach departs from the previous approaches to discourse analysis such as the Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2003) in that it does not start from a predefined inventory of abstract discourse relations. Instead, all discourse relations are lexically grounded and anchored by a discourse connective. The discourse relations so defined can be structural or anaphoric. Structural discourse relations, gener"
W05-0312,W03-1707,1,0.708502,"s 84–91, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics rived anaphorically in the previous discourse. An advantage of this approach to discourse analysis is that discourse relations can be built up incrementally in a bottom-up manner and this advantage is magnified in large-scale annotation projects where interannotator agreement is crucial and has been verified in the construction of the Penn English Discourse Treebank (Miltsakaki et al., 2004a). This approach closely parallels the annotation of the the verbs in the English and Chinese Propbanks (Palmer et al., 2005; Xue and Palmer, 2003), where verbs are the anchors of predicate-argument structures. The difference is that the extents of the arguments to discourse connectives are far less certain, while the arity of the predcates is fixed for the discourse connectives. This paper outlines the issues that arise from the annotation of Chinese discourse connectives, with an initial focus on explicit discourse connectives. Section 2 gives an overview of the different kinds of discourse connectives that we plan to annotate for the CDTB Project. Section 3 surveys the distribution of the discourse connectives and Section 4 describes"
W05-0312,W01-1605,0,\N,Missing
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-2423,P98-1013,0,0.0111712,"Missing"
W09-2423,P01-1017,0,0.22112,"Missing"
W09-2423,N06-1024,0,0.0607349,"Missing"
W09-2423,W04-3228,0,0.0260109,"Missing"
W09-2423,P06-2055,1,0.767512,"Missing"
W09-2423,C02-1122,0,0.0190732,"rse of eight years or so. In contrast, the Chinese and Japanese systems are newer and considerably less time was spent developing them. Thus they currently do not represent as many regularizations. One obstacle is that we do not currently use subcategorization dictionaries for either language, while we have several for English. In particular, these would be helpful in predicting and filling relative clause and others gaps. We are considering auto153 matically acquiring simple dictionaries by recording frequently occurring argument types of verbs over a larger corpus, e.g., along the lines of (Kawahara and Kurohashi, 2002). In addition, existing Japanese dictionaries such as the IPAL (monolingual) dictionary (technology Promotion Agency, 1987) or previously acquired case information reported in (Kawahara and Kurohashi, 2002). Finally, we are investigating several avenues for using this system output for Machine Translation (MT) including: (1) aiding word alignment for other MT system (Wang et al., 2007); and (2) aiding the creation various MT models involving analyzed text, e.g., (Gildea, 2004; Shen et al., 2008). Acknowledgments This work was supported by NSF Grant IIS0534700 Structure Alignment-based MT. Refe"
W09-2423,W04-2705,1,0.83884,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,meyers-etal-2004-cross,1,0.822383,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,W07-1529,1,0.799455,"Missing"
W09-2423,W04-2703,0,0.139121,"Missing"
W09-2423,I05-4002,0,0.0390298,"Missing"
W09-2423,J05-1004,0,0.21622,"Missing"
W09-2423,W05-0302,1,0.852675,"els; we have a different set of relational labels; and finally, our approach is designed to be compatible with the Penn Treebank framework and therefore, Penn-Treebankbased parsers. In addition, the expansion of our theory is governed more by available resources than by the underlying theory. As our main goal is to use our system to regularize data, we freely incorporate any analysis that fits this goal. Over time, we have found ways of incorporating Named Entities, PropBank, NomBank and the Penn Discourse Treebank. Our agenda also includes incorporating the results of other research efforts (Pustejovsky et al., 2005). For each sentence, we generate a feature structure (FS) representing our most complete analysis. We distill a subset of this information into a dependency structure governed by theoretical assumptions, e.g., about identifying functors of phrases. Each GLARF dependency is between a functor and an argument, where the functor is the head of a phrase, conjunction, complementizer, or other function word. We have built applications that use each of these two representations, e.g., the dependency representation is used in (Shinyama, 2007) and the FS representation is used in (K. Parton and K. R. Mc"
W09-2423,P08-1066,0,0.0264245,"Missing"
W09-2423,P03-1010,0,0.0528115,"Missing"
W09-2423,D07-1077,0,0.0600045,"Missing"
W09-2423,J08-2004,1,0.877787,"Missing"
W09-2423,J93-2004,0,\N,Missing
W09-2423,W08-2121,1,\N,Missing
W09-2423,J03-4003,0,\N,Missing
W09-2423,P09-1048,1,\N,Missing
W09-2423,C98-1013,0,\N,Missing
W09-2423,W09-1201,1,\N,Missing
W09-3019,P03-1010,0,0.0268446,"Missing"
W09-3019,P01-1017,0,0.0441748,"our grammars of Japanese and Chinese do not currently.; (5) a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automati"
W09-3019,J02-3001,0,0.015508,"a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generat"
W09-3019,J08-2004,1,0.84166,"inese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammati"
W09-3019,W09-1201,1,0.825301,"Missing"
W09-3019,D09-1087,1,0.855247,"Missing"
W09-3019,P06-2055,1,0.900802,"Missing"
W09-3019,W08-2121,1,\N,Missing
W09-3019,J03-4003,0,\N,Missing
W09-3019,W04-0413,1,\N,Missing
W09-3019,W09-2423,1,\N,Missing
W09-3020,J07-3002,0,0.027874,"d into two parts: the Xinhua Chinese newswire with literal English translations (4,363 parallel sentences) and the Sinorama Chinese news magazine with non-literal English translations (12,600 parallel sentences). We experimented with the two parts separately to see how literal and non-literal translations affect word-alignments. Introduction Since verbs tend to be the roots of dependency relations in a sentence (Palmer et al., 2005), when it comes down to translations, finding correct mappings between verbs in a source and a target language is very important. Many machine translation systems (Fraser and Marcu, 2007) use wordalignment tools such as GIZA++ (Och and Ney, 2003) to retrieve word mappings between a source and a target language. Although GIZA++ gives well-structured alignments, it has limitations in several ways. First, it is hard to verify if alignments generated by GIZA++ are correct. Second, GIZA++ may not find alignments for low-frequent words. Third, GIZA++ does not account for any semantic information. In this paper, we suggest a couple of ways to enhance word-alignments for predicating expressions such as verbs1 . We restricted the source and the target language to Chinese and English, r"
W09-3020,J03-1002,0,0.00432598,"sh translations (4,363 parallel sentences) and the Sinorama Chinese news magazine with non-literal English translations (12,600 parallel sentences). We experimented with the two parts separately to see how literal and non-literal translations affect word-alignments. Introduction Since verbs tend to be the roots of dependency relations in a sentence (Palmer et al., 2005), when it comes down to translations, finding correct mappings between verbs in a source and a target language is very important. Many machine translation systems (Fraser and Marcu, 2007) use wordalignment tools such as GIZA++ (Och and Ney, 2003) to retrieve word mappings between a source and a target language. Although GIZA++ gives well-structured alignments, it has limitations in several ways. First, it is hard to verify if alignments generated by GIZA++ are correct. Second, GIZA++ may not find alignments for low-frequent words. Third, GIZA++ does not account for any semantic information. In this paper, we suggest a couple of ways to enhance word-alignments for predicating expressions such as verbs1 . We restricted the source and the target language to Chinese and English, respectively. The goal is to use the linguistic annotation a"
W09-3020,J05-1004,1,0.35791,"Missing"
W09-3020,W04-2705,0,\N,Missing
W10-1810,J05-1004,1,0.108587,"Missing"
W10-1810,P98-1013,0,0.078782,"Missing"
W10-1810,N04-1030,0,0.0417353,"Missing"
W10-1810,J02-3001,0,0.0135769,"Missing"
W10-1810,W04-0401,0,0.0591779,"Missing"
W10-1810,W04-2705,0,0.119612,"Missing"
W10-1810,C98-1013,0,\N,Missing
W11-0420,harabagiu-bejan-2006-answer,0,0.339182,"Missing"
W11-0420,C10-2058,0,0.165048,"Missing"
W11-0420,prasad-etal-2008-penn,0,0.0154515,"our own. The 162 scheme for this step is described in Section 2.1. The second problem is that there is too much temporal vagueness in natural language with respect to the temporal classification scheme. Since we cannot change the way natural language works, we try to model the classification scheme after the data it is supposed to classify. The scheme for the temporal annotation is covered in Sections 2.2 and 2.3. 2.1 Discourse-constrained selection of main events and their pairs 2.1.1 Discourse annotation scheme The PDTB adopts a lexically grounded approach to discourse relation annotation (Prasad et al., 2008). Based on discourse connectives like “since”, “and”, and “however”, discourse relation is treated as a predicate taking two abstract objects (AO’s) (such as events, states, and propositions) as arguments. For example, in the sentence below, “since” is the lexical anchor of the relation between Arg1 and Arg2 (example from Prasad et al. (2007)). (1) Since [Arg2 McDonald’s menu prices rose this year], [Arg1 the actual decline may have been more]. This notion is generalized to cover discourse relations that do not have a lexical anchor, i.e. implicit discourse relations. For example, in the twose"
W11-0420,verhagen-2010-brandeis,0,0.0500761,"Missing"
W11-0420,C10-2156,1,0.80164,"Missing"
W11-0420,S10-1010,0,\N,Missing
W11-0420,P12-1008,1,\N,Missing
W11-1010,2010.amta-papers.10,0,0.0467879,"Missing"
W11-1010,P01-1017,0,0.0710179,"n MT to future research. In addition to these papers, there has also been some work on rule-based reordering preprocessors to word alignment based on shallower linguistic information. For example (Crego and Mari˜no, 2006) reorders based on patterns of POS tags. We hypothesize that this is similar to the above approaches in that patterns of POS tags are likely to simulate parsing or chunking. 3 Preparing the Data The two stage parsers of previous decades (Hobbs and Grishman, 1976) generated a syntactic repre89 sentation analogous to the (more accurate) output of current treebank-based parsers (Charniak, 2001) and an additional second stage output that regularized constructions (passive, active, relative clauses) to representations similar to active clauses with no gaps, e.g., The book was read by Mary was given a representation similar to that of Mary read the book. Treating the active clause as canonical provides a way to reduce variation in language and thus, making it easier to acquire and apply statistical information from corpora–there is more evidence for particular statistical patterns when applications learn patterns and patterns more readily match data. Two-stage parsers were influenced b"
W11-1010,P05-1066,0,0.1291,"Missing"
W11-1010,2007.mtsummit-tutorials.1,0,0.0242494,"root that can be bridged by a surface path. 4 Manual Reordering Rules We derived manual rules for making the English Word Order more like the Chinese by manually inspecting the data. We inspected the first 100-200 sentences of the DEV corpus by first transliterating the Chinese into English – replaced each Chinese word with the aligned English counterpart. Several patterns emerged which were easy to formalize into rules in the GLARF framework. These patterns were verified and sometimes generalized through discussions with native Chinese speakers and linguists. Our rules, similar to those of (Wang et al., 2007) are as follows (results are discussed in section 6): (1) Front a post-nominal PP headed by a preposition in the list {of, in, with, about)}. (2) Front post-nominal relative clause that begins with that or does not have any relative pronoun, such that the main predicate is not a copula plus adjective construction. (3) Front post-nominal relative clause that begins with that or has no relative pronoun if the main predicate is a copula+adjective construction which is not negated by a word from the set {no neither nor never not n’t}. (4) Front post-nominal reduced relative in the form of a passiv"
W11-1010,P01-1067,0,0.230526,"Missing"
W11-1010,N07-1051,0,\N,Missing
W11-1010,W04-2703,0,\N,Missing
W11-1010,W03-1707,1,\N,Missing
W11-1010,J07-3002,0,\N,Missing
W11-1010,W09-2423,1,\N,Missing
W11-1010,J08-2004,1,\N,Missing
W11-1010,2006.amta-papers.4,0,\N,Missing
W11-1010,P08-1064,0,\N,Missing
W11-1010,P07-2045,0,\N,Missing
W11-1010,P06-2055,0,\N,Missing
W11-1010,W07-0401,0,\N,Missing
W11-1010,C96-1078,1,\N,Missing
W11-1010,J02-3001,0,\N,Missing
W11-1010,J03-1002,0,\N,Missing
W11-1010,J05-1004,0,\N,Missing
W11-1010,P08-1066,0,\N,Missing
W11-1010,P98-2139,1,\N,Missing
W11-1010,C98-2134,1,\N,Missing
W11-1010,D07-1077,0,\N,Missing
W11-1010,meyers-etal-2004-annotating,1,\N,Missing
W11-1010,N09-2004,0,\N,Missing
W11-1010,P00-1056,0,\N,Missing
W11-1901,W06-0609,1,0.725549,"he Switchboard Treebank. Given the frequency of disfluencies and the performance with which one can identify them automatically,8 a probable processing pipeline would filter them out before parsing. Since we did not have a readily available tagger for tagging disfluencies, we decided to remove them using oracle information available in the Treebank. Propositions The propositions in OntoNotes constitute PropBank semantic roles. Most of the verb predicates in the corpus have been annotated with their arguments. Recent enhancements to the PropBank to make it synchronize better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify"
W11-1901,P06-1005,0,0.929236,"ers including the parses, semantic roles, word senses, and named entities. As is customary for CoNLL tasks, there were two tracks, closed and open. For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data. 4.2.1 Closed Track In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by Bergsma and Lin (2006). For the training and test data, in addition to the underlying text, predicted versions of all the supplementary layers of annotation were provided, where those predictions were derived using off-the-shelf tools (parsers, semantic role labelers, named entity taggers, etc.) as described in Section 4.4.2. For the training data, however, in addition to predicted values for the other layers, we also provided manual gold-standard annotations for all the layers. Participants were allowed to use either the gold-standard or predicted annotation for training their systems. They were also free to use t"
W11-1901,W10-4305,0,0.156408,"Missing"
W11-1901,N01-1016,0,0.0275817,"ze better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. 9 doc/propbank/english-propbank.pdf lease. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the Release 1.0 of the Proposition Bank. This functionality is part of the OntoNotes DB Tool.10 Word Sense Gold word sense annotation was supplied using sense numbers as specified in the O"
W11-1901,P05-1022,0,0.0274644,"coreference but that have been annotated for other layers. For training 10 11 http://cemantix.org/ontonotes.html It should be noted that word sense annotation in OntoNotes is note complete, so only some of the verbs and nouns have word sense tags specified. 10 Senses Lemmas 1 2 &gt;2 1,506 1,046 1,016 Table 6: Word sense polysemy over verb and noun lemmas in OntoNotes models for each of the layers, where feasible, we used all the data that we could for that layer from the training portion of the entire OntoNotes release. Parse Trees Predicted parse trees were produced using the Charniak parser (Charniak and Johnson, 2005).12 Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were appropriately extended. The parser was then re-trained on the training portion of the release 4.0 data using 10-fold crossvalidation. Table 5 shows the performance of the re-trained Charniak parser on the CoNLL-2011 test set. We did not get a chance to re-train the re-ranker, and since the stock re-ranker crashes when run on nbest parses containing NMLs, because it has no"
W11-1901,N07-1011,0,0.525918,"fined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In"
W11-1901,N07-1030,0,0.0811053,"to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress,"
W11-1901,N10-1061,0,0.393386,"y work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surfa"
W11-1901,N01-1008,0,0.21035,"Missing"
W11-1901,N06-2015,1,0.528696,"2009) devoted to joint learning of syntactic and semantic dependencies. A principle ingredient for joint learning is the presence of multiple layers of semantic information. One fundamental question still remains, and that is – what would it take to improve the state of the art in coreference resolution that has not been attempted so far? Many different algorithms have been tried in the past 15 years, but one thing that is still lacking is a corpus comprehensively tagged on a large scale with consistent, multiple layers of semantic information. One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) is to explore whether it can fill this void and help push the progress further – not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that cov1 2 http://projects.ldc.upenn.edu/ace/data/ http://www.bbn.com/nlp/ontonotes 2 ers entities and events not limited to noun phrases or a limited set of entity types. A small portion of this corpus from the newswire and broadcast news genres (∼120k) was recently used for a S EM E VAL task (Recasens et al., 2010)."
W11-1901,H05-1004,0,0.943059,"Missing"
W11-1901,J93-2004,1,0.0615149,"Missing"
W11-1901,P00-1023,0,0.0803844,"partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collabor"
W11-1901,P10-1142,0,0.220993,"and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC). These corpora were tagged with coreferring entities identified by noun phrases in the text. The de facto standard datasets for current coreference studies are the MUC (Hirschman and Chin1 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1–27, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics chor, 1997; Chinchor, 2001; Chinchor and Sun"
W11-1901,J05-1004,1,0.333011,"Missing"
W11-1901,passonneau-2004-computing,0,0.00517084,"ing and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step towards improving"
W11-1901,W05-0311,0,0.0114861,", but represent small training and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step"
W11-1901,P09-5006,0,0.0282076,"Missing"
W11-1901,N06-1025,0,0.730558,"d-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gende"
W11-1901,D09-1101,0,0.522965,"annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques st"
W11-1901,W09-2411,0,0.155304,"Missing"
W11-1901,J01-4004,0,0.993465,"stering them into equivalence classes, has been well recognized in the natural language processing community. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of"
W11-1901,P09-1074,0,0.237943,"ry of evaluations on coreference tasks, variation in the evaluation criteria and in the training data used have made it difficult for researchers to be clear about the state of the art or to determine which particular areas require further attention. There are many different parameters involved in defining a coreference task. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is somewhat easier (Culotta et al., 2007). Given the space constraints, we refer the reader to Stoyanov et al. (2009) for a detailed treatment of the issue. Limitations in the size and scope of the available datasets have also constrained research progress. The MUC and ACE corpora are the two that have been used most for reporting comparative results, but they differ in the types of entities and coreference annotated. The ACE corpus is also one that evolved over a period of almost five years, with different incarnations of the task definition and different corpus cross-sections on which performance numbers have been reported, making it hard to untangle and interpret the results. The availability of the OntoN"
W11-1901,D07-1052,0,0.0160767,"ic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entiti"
W11-1901,M95-1005,0,0.967872,"2011 coreference task are likely to be lower than for coref evaluations based on MUC, where the mention spans are specified in the input,17 or those based on ACE data, where an approximate match is often allowed based on the specified head of the NP mention. 4.5.1 Metrics As noted above, the choice of an evaluation metric for coreference has been a tricky issue and there does not appear to be any silver bullet approach that addresses all the concerns. Three metrics have been proposed for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric (Vilain et al., 1995), ii) The mention based B - CUBED metric (Bagga and Baldwin, 1998) and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric (Luo, 2005). Very recently BLANC (BiLateral Assessment of NounPhrase Coreference) measure (Recasens and Hovy, 17 2011) has been proposed as well. Each of the metric tries to address the shortcomings or biases of the earlier metrics. Given a set of key entities K, and a set of response entities R, with each entity comprising one or more mentions, each metric generates its variation of a precision and recall measure. The MUC measure if the oldest and mos"
W11-1901,W08-2121,0,\N,Missing
W11-1901,E06-2015,0,\N,Missing
W11-1901,D08-1067,0,\N,Missing
W11-1901,S10-1001,0,\N,Missing
W11-1901,doddington-etal-2004-automatic,1,\N,Missing
W11-1901,W04-2327,0,\N,Missing
W11-1919,J01-4004,0,0.611003,"xisting in the development data, thus we used it in our algorithm. 2.2 Generating Entity-Pairs From Individual Entities In the annotated training documents, an entity has been marked in a coreference chain that includes all coreferential entities. In our algorithm, we only detect the closest antecedent for each entity, instead of all coreferences, of each entity. Specifically, we define each training and testing instance as a pair of entities. During the training process, for each entity encountered by the system, we create a positive instance by pairing an entity with its closest antecedent (Soon et al., 2001). In addition, a set of negative instances are also created by pairing the entity with any preceding entities that exist between its closest antecedent and the entity itself (note that the antecedent must be a coreference of the current entity, whereas preceding entities may not be coreferential). For example, in the entity sequence “A, B, C, D, E”, let us assume that “A” is the closest antecedent of “D”. Then, for entity “D”, “A-D” is considered a positive instance, whereas “B-D” and “C-D” are two negative instances. To generate testing data, every entity-pair within the same sentence is cons"
W12-3616,harabagiu-bejan-2006-answer,0,0.0633441,"Missing"
W12-3616,C10-2058,0,0.0353446,"Missing"
W12-3616,C10-2156,1,0.838906,"s a simplified version of the TimeML (Verhagen et al., 2009). 125 clear in this respect. Other types of temporal modifiers that share this problem include since [1990], [three years] ago, until [now] etc. (square brackets delimit time expressions). 2.2.3 How to choose time∼event pairs for annotation? How to find annotation targets for different types of temporal relation has been a long-standing problem in temporal annotation, and the normal solution is to annotate all pairs that satisfy some technical constraints specified in syntactic, semantic and/or discourse terms (Verhagen et al., 2009; Xue and Zhou, 2010; Zhou and Xue, 2011). In the case of temporal relation between time and event, Xue and Zhou (2010) proposed to let annotators judge which event(s) a given time expression is intended to modify. There are at least three problems with this proposal as it stood. First, as alluded to in Section 2.2.2, time expressions usually do not modify predicates by themselves, unless they can stand alone as a temporal modifier (e.g. now, tomorrow, this week). To use the temporal modifier from June 6 to August 14 as an example again, neither June 6 nor August 14, but the whole prepositional phrase, has an int"
W12-3616,W11-0420,1,0.539319,"on of the TimeML (Verhagen et al., 2009). 125 clear in this respect. Other types of temporal modifiers that share this problem include since [1990], [three years] ago, until [now] etc. (square brackets delimit time expressions). 2.2.3 How to choose time∼event pairs for annotation? How to find annotation targets for different types of temporal relation has been a long-standing problem in temporal annotation, and the normal solution is to annotate all pairs that satisfy some technical constraints specified in syntactic, semantic and/or discourse terms (Verhagen et al., 2009; Xue and Zhou, 2010; Zhou and Xue, 2011). In the case of temporal relation between time and event, Xue and Zhou (2010) proposed to let annotators judge which event(s) a given time expression is intended to modify. There are at least three problems with this proposal as it stood. First, as alluded to in Section 2.2.2, time expressions usually do not modify predicates by themselves, unless they can stand alone as a temporal modifier (e.g. now, tomorrow, this week). To use the temporal modifier from June 6 to August 14 as an example again, neither June 6 nor August 14, but the whole prepositional phrase, has an intended modification ta"
W12-3616,S10-1010,0,\N,Missing
W12-4501,W06-0609,0,0.0197938,"ion of the corpus is all newswire, this had no impact on it. However, for both Chinese and Arabic, since we remove trace tokens corresponding to dropped pronouns, all the other layers of annotation had to be remapped to the remaining sequence of tree tokens. Propositions The propositions in OntoNotes are PropBank-style semantic roles for English, Chinese and Arabic. Most of the verb predicates in the corpus have been annotated with their arguments. As part of the OntoNotes effort, some enhancements were made to the English PropBank and Treebank to make them synchronize better with each other (Babko-Malaya et al., 2006). One of the outcomes of this effort was that two types of LINKs that represent pragmatic coreference (LINK - PCR) and selec14 There is another phrase type — EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases by different speakers, so we decided not to remove that from the data. 15 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recal"
W12-4501,2011.mtsummit-papers.22,1,0.579383,"s that of number and gender. There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by Bergsma and Lin (2006), for use during both training and testing. Unfortunately neither Arabic, nor Chinese have comparable resources available that we could allow participants to use. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. 4.1.2 Open Track In addition to resources available in the closed track, in the open track, systems were allowed to use 7 There are a few instances of novel senses introduced in OntoNotes which were not present in WordNet, and so lack a mapping back to the WordNet senses Algorithm 1 Procedure used to create OntoNotes training, development and test partitions. Procedure: Generate Partitions(OntoNotes) returns Train, Dev, Test 1: Train ← ∅ 2: Dev ← ∅ 3: Test ← ∅ 4: for all Source ∈ OntoNotes do 5: if Source = Wall Street Journal then 6: Train ← Train ∪ S"
W12-4501,P06-1005,0,0.473873,"f WordNet senses, systems could also map from the predicted or gold-standard word senses to the sets of underlying WordNet senses. Another significant piece of knowledge that is particularly useful for coreference but that is not available in the layers of OntoNotes is that of number and gender. There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by Bergsma and Lin (2006), for use during both training and testing. Unfortunately neither Arabic, nor Chinese have comparable resources available that we could allow participants to use. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. 4.1.2 Open Track In addition to resources available in the closed track, in the open track, systems were allowed to use 7 There are a few instances of novel senses introduced in OntoNotes which were not present in WordNet, and so lack a mapping back to the WordNet senses Algorithm 1 Procedur"
W12-4501,W10-4305,0,0.0238143,"distribution of the participants by country and the participation by language and task type. 4.5.3 Scoring Metrics Implementation We used the same core scorer implementation25 that was used for the S EM E VAL-2010 task, and which implemented all the different metrics. There were a couple of modifications done to this scorer since then. 6 1. Only exact matches were considered correct. Previously, for S EM E VAL-2010 nonexact matches were judged partially correct with a 0.5 score if the heads were the same and the mention extent did not exceed the gold mention. 2. The modifications suggested by Cai and Strube (2010) have been incorporated in the scorer. Since there are differences in the version used for CoNLL and the one available on the download site, and it is possible that the latter would be revised in the future, we have archived the version of the scorer on the CoNLL-2012 task webpage.26 5 Participants A total of 41 different groups demonstrated interest in the shared task by registering on the task 25 26 http://www.lsi.upc.edu/∼esapena/downloads/index.php?id=3 http://conll.bbn.com/download/scorer.v4.tar.gz 20 Country Participants Brazil China Germany Italy Switzerland USA 1 8 3 1 1 2 Table 13: Pa"
W12-4501,W11-1907,0,0.144398,"ghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution"
W12-4501,P11-2037,0,0.0414211,"ghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution"
W12-4501,N01-1016,0,0.027844,"ents. As part of the OntoNotes effort, some enhancements were made to the English PropBank and Treebank to make them synchronize better with each other (Babko-Malaya et al., 2006). One of the outcomes of this effort was that two types of LINKs that represent pragmatic coreference (LINK - PCR) and selec14 There is another phrase type — EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases by different speakers, so we decided not to remove that from the data. 15 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. tional preferences (LINK - SLC) were added to PropBank. More details can be found in the addendum to the PropBank guidelines16 in the OntoNotes v5.0 release. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the PropBank 1.0 release. This functionality is part of the OntoNotes DB"
W12-4501,P05-1022,0,0.00733886,"have been annotated for other layers. For training models for each of the layers, where feasible, we used all the data that we could 16 17 doc/propbank/english-propbank.pdf http://cemantix.org/ontonotes.html 14 Layer English Chinese Arabic Verb Noun All Verb Noun Sense Inventories 2702 Frames 5672 2194 1335 763 150 20134 2743 111 532 Table 7: Number of senses defined for English, Chinese and Arabic in the OntoNotes v5.0 corpus. for that layer from the training portion of the entire OntoNotes v5.0 release. Parse Trees Predicted parse trees for English were produced using the Charniak parser18 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags, to Penn-style part of speech tags. We used the mapping that is included with the Arabic treebank. The predicted parses for the training portion of the data wer"
W12-4501,N07-1011,0,0.0270615,"d multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high interannotator agreement possibly by restricting the coreference annotating to phenomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation scenario with an official evaluation setup, and possibly"
W12-4501,N07-1030,0,0.0061787,"/mt/ three languages. As we will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used"
W12-4501,doddington-etal-2004-automatic,0,0.253776,"Uryupina University of Trento, 38123 Povo (TN) Italy Yuchen Zhang Brandeis University, Waltham, MA 02453 USA uryupina@gmail.com yuchenz@brandeis.edu Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with decision trees and hand-written rules. Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC) (Hirschman and Chinchor, 1997; Chinchor, 2001; Chinchor and Sundheim, 2003). The de facto standard datasets for current coreference studies are the MUC and the ACE 1 (Doddington et al., 2004) corpora. These corpora were tagged with coreferring entities in the form of noun phrases in the text. The MUC corpora cover all noun phrases in text but are relatively small in size. The ACE corpora, on the other hand, cover much more data, but the annotation is restricted to a small subset of entities. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it is a problem that requires world knowledge to solve and word knowledge is hard to define, and partly owing to the lack of substantial annotated data. Aside from"
W12-4501,N10-1061,0,0.0387987,"liarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rath"
W12-4501,N01-1008,0,0.104449,"Missing"
W12-4501,N06-2015,0,0.586083,"enomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation scenario with an official evaluation setup, and possibly several ablation settings to capture the range of performance. This can then be used as a standard benchmark by the research community. (iii) Continue to improve learning algorithms that better incorporate world knowledge and jointly incorporate information from other layers of syntactic and semantic annotation to improve the state of the art. One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) 2 was to explore whether it could fill this void and help push the progress further — not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a subset of entity types. The coreference layer in OntoNotes constitutes just one part of a multilayered, integrated annotation of shallow semantic structures in text with high inter-annotator agreement. This addresses the first issue. In the language"
W12-4501,W11-1902,0,0.174907,"Missing"
W12-4501,H05-1004,0,0.90641,"Missing"
W12-4501,W04-1602,0,0.00799095,"Missing"
W12-4501,J93-2004,0,0.0513136,"Missing"
W12-4501,P00-1023,0,0.125028,"ist.gov/iad/mig/publications/ASRhistory/index.html http://www.itl.nist.gov/iad/mig/tests/mt/ three languages. As we will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaborative"
W12-4501,P10-1142,0,0.204917,"semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution techniques to the next level. The OntoNotes project has created a large-scale corpus of accurate and integrated annotation of multiple levels of"
W12-4501,J05-1004,0,0.387003,"Missing"
W12-4501,palmer-etal-2008-pilot,0,0.00787827,"Missing"
W12-4501,passonneau-2004-computing,0,0.00467497,"high agreement which likely lessened the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and 1 http://projects.ldc.upenn.edu/ace/data/ 1 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order to take language understanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has ma"
W12-4501,N07-1051,0,0.00750508,"Number of senses defined for English, Chinese and Arabic in the OntoNotes v5.0 corpus. for that layer from the training portion of the entire OntoNotes v5.0 release. Parse Trees Predicted parse trees for English were produced using the Charniak parser18 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags, to Penn-style part of speech tags. We used the mapping that is included with the Arabic treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-fold for Arabic) cross-validation. The development and test parses were generated using a model trained on the entire training portion. We used OntoNotes v5.0 training data for training the Chinese and Arabic parser models, but the OntoNotes v4.0 subset of OntoNotes v5.0 data was used for training the English m"
W12-4501,W05-0311,0,0.0332271,"t equally annotatable with high agreement which likely lessened the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and 1 http://projects.ldc.upenn.edu/ace/data/ 1 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order to take language understanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation sce"
W12-4501,P09-5006,0,0.0290366,"Missing"
W12-4501,N06-1025,0,0.0301668,"tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress,"
W12-4501,D10-1048,0,0.0903952,"tem uses feature templates defined on mention pairs. bj¨orkelund mentions that disallowing transitive closures gave performance improvement of 0.6 and 0.4 respectively for English and Chinese/Arabic. bj¨orkelund also mentions seeing a considerable increase in performance after adding features that correspond to the Shortest Edit Script (Myers, 1986) between surface forms and unvocalised Buckwalter forms, respectively. These could be better at capturing the differences in gender and number signaled by certain morphemes than hand-crafted rules. chen built upon the sieve architecture proposed in Raghunathan et al. (2010) and added one more sieve — head match — for Chinese and modified two sieves. Some participants tried to incorporate peculiarities of the corpus in their systems. For example, martschat excluded adjectival nation names. Unlike English, and especially in absence of an external resource, it is hard to make a gender distinction in Arabic and Chinese. martschat used the information 23 that 先 生(sir) and 女士(lady) often suggest gender information. bo and martschat used plurality markers 们 to identify plurals. For example, 同学 (student) is singular and 同学们 (students) is plural. bo also uses a heuristic"
W12-4501,D09-1101,0,0.142437,"will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorith"
W12-4501,W09-2411,0,0.101275,"Missing"
W12-4501,J01-4004,0,0.987173,"plex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high interannotator agreement possibly by restricting the coreference annotating to phenomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation sc"
W12-4501,P09-1074,0,0.0393009,"rstanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high"
W12-4501,P08-4003,1,0.780554,"P and based pruning; Genre specific selected NE in Arabic. Learning to prune non-referential mentions models NP, PRP and PRP $ in all languages; PN in Logistic Regression Chinese; all NE in English. Exclude pleonastic (LIBLINEAR) it in English. Prune smaller mentions with same head. Eight different mention types for English, and Directed multigraph adjectival use for nations and a few NEs are representation where the filtered as well as embedded mentions and weights are learned over the pleonastic pronouns. Four mention types in training data (on top of BART Chinese. Copulas are also handled (Versley et al., 2008)) appropriately. Latent Structure Learning All noun phrases, pronouns and name entities modification of BART using multi-objective optimization. Standard rules for English and Classifier to Domain specific classifiers for identify markable NPs in Chinese and Arabic. nw and bc genre. Memory based learning NP, PRP and PRP $ in English, and all NP in (T I MBL) Chinese and Arabic. Singleton classifier. All phrase types that are mentions in training MaxEnt are considered as mentions and a classifier is trained to identify potential mentions. C4.5 and deterministic rules All noun phrases, pronouns a"
W12-4501,D07-1052,0,0.0121409,". (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techni"
W12-4501,M95-1005,0,0.953302,"Missing"
W12-4501,J08-2004,1,0.444195,"agging where The NULL arguments are first filtered out, and the remaining NON - NULL arguments are classified into one of the argument types. The argument identification module used an ensemble of ten classifiers — each trained on a tenth of the training data and combined using unweighted voting. This should still give a close to state-of-theart performance given that the argument identification performance tends to start to be asymptotic around 10K training instances (Pradhan et al., 2005). The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on all the training portion of the OntoNotes v5.0 data. No propositional structures were provided for Arabic due to resource constraints. Table 9 shows the detailed performance numbers. The CoNLL-2005 scorer was used to compute the scores. At first glance, the performance on the English newswire genre is much lower than what has been reported for WSJ Section 23. This could be attributed to several factors: i) the fact that we had to compromise on the training method, ii) the newswire in OntoNotes not only contains WSJ data, but also Xinhua news, iii) The WSJ training and test porti"
W12-4501,C10-2158,1,0.696855,"Missing"
W12-4501,W10-1836,1,0.72614,"Missing"
W12-4501,P10-4014,0,0.0186234,"44 74.99 76.19 Table 6: Parser performance on the CoNLL-2012 test set. R English Broadcast Conversation [BC] Broadcast News [BN] Magazine [MZ] Newswire [NW] Weblogs and Newsgroups [WB] Overall 81.3 81.5 78.8 85.7 77.6 81.2 82.0 79.1 85.7 77.5 81.2 81.7 79.0 85.7 77.5 82.5 82.5 82.5 Chinese Broadcast Conversation [BC] Broadcast News [BN] Magazine [MZ] Newswire [NW] Overall Arabic Accuracy P F Newswire [NW]19 - - 80.5 85.4 82.4 89.1 - - 84.3 75.2 75.9 75.6 Table 8: Word sense performance over both verbs and nouns in the CoNLL-2012 test set. Word Sense This year we used the IMS (It Makes Sense) (Zhong and Ng, 2010) word sense tagger.20 Word sense information, unlike syntactic parse information is not central to approaches taken by current coreference systems and so we decided to use a better word sense tagger to get a good state of the art accuracy estimate, at the cost of a completely fair (but, still close enough) comparison with English CoNLL-2011 results. This will also allow potential future uses to benefit from it. IMS was trained on all the word sense data that is present in the training portion of the OntoNotes corpus using crossvalidated predictions on the input layers similar to the propositio"
W12-4501,W08-2121,0,\N,Missing
W12-4501,E06-2015,0,\N,Missing
W12-4501,D08-1067,0,\N,Missing
W12-4501,W09-1201,1,\N,Missing
W12-4501,S10-1001,0,\N,Missing
W12-6305,W03-1022,0,0.0255625,"can be used as a tagset to annotate words in a natural language corpus, which can then be used to train automatic lexical semantic classifiers. Compared with words sense disambiguation, where senses have to be defined for each word, classifying words based on their lexical classes is a more general task. The advantage is that there is no need to train classifiers for each individual word, as is typically the case for word sense disambiguation systems. Building lexical semantic resources and systems has attracted much interest in the NLP and lexical semantics communities. (Picca et al., 2007, Ciaramita & Johnson, 2003) described a corpus 2 Related Work There have been several past efforts to produce (Chinese) lexical taxonomies aimed to provide lexical knowledge for NLP tasks (Chen, 1998; Chen, 2001; Wang etc., 2003). (Wang et al, 2003) 18 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 18–26, Tianjin, China, 20-21 DEC. 2012 used lexical classes to describe word sense in SKCC (Semantic Knowledge Base of Contemporary Chinese), along with syntactic and argument structure features. WordNet (Fellbaum, 1998). Gather senses with similar lexical meaning according to one"
W12-6305,J08-2004,1,0.88746,"Missing"
W12-6306,W03-0804,0,0.0727506,"Missing"
W12-6306,W10-4343,0,0.0244556,"今天/today)) (SP 啊/Ah) (PU ?))) Chinese include “嗯/um, uh-huh” , “呃/Ugh”, “唔 /oh”, “啊/Ah”, “这个/Eh”, “那个/Eh”, etc. (5) (IP (NP-SBJ (PN 你/you)) (VP (ADVP (AD 多/more)) (FLR (INF 那个/that one)) (VP (VV 长/grow) (NP-OBJ (QP (CLP (M 个/CL))) (NP (NN 心眼儿/mind)))))) “You should be more mindful.” “Yesterday, (you mean) today?” (6c) Restart Disfluency (DFL) In conversational speech, a speaker often has to repeat what s/he has just said, or abandon what s/he just said and restart with revised content. This is a phenomenon called repair in speech literature. There is extensive literature on speech repairs [8][9][13]. Typically, a speech repair instance can be characterized as a template that consists of a reparandum and an alteration [13]. The reparandum is the speech sequence that is erroneous or inappropriate, while the alteration represents the correction of the problematic sequence. The alteration can delete from, add to, substitute for, or repeat the problematic sequence. Or it can be a fresh restart that has little resemblance to the problematic sequence. The alteration is essential to the completeness of the syntactic structure of a sentence, while the reparandum, like fillers, can be consider"
W12-6306,P11-1122,0,0.0323576,"Missing"
W12-6306,O03-4002,1,0.774546,"Missing"
W12-6306,J93-2004,0,0.0394163,"ds functional tags and empty categories to the bare-bone parses before they are corrected. 5 Some relevant statistics Our raw texts include newswire, magazine articles, broadcast news, broadcast conversations, and weblogs. As of this writing, we have annotated over 400,000 words in the informal genre based on the extended annotation guidelines. Here is some statistics based on an analysis of 461 files with 396,874 words: label DFL tags FLR tags INC tags TYPO tags SKIP tags MBD tags -DIS tags -UNF tags 6 occurrences 2819 1854 637 13 281 167 150 924 Related work The success of the Penn Treebank [15] has spurred the development of a large number of treebanks in many different languages, but most of the early treebanking efforts are directed at the formal genres. Specific to Chinese, there are number of significant treebanking efforts (Sinica Treebank and Tsinghua Treebank), but the Chinese Treebank is one of the early ones. There are relatively few efforts directed at annotating informal genres. The Switchboard Corpus is one notable exception [17]. It is a speech corpus annotated following guidelines that extend the Penn Treebank annotation guidelines. To the best of our knowledge, there"
W12-6306,C02-1145,1,0.735693,"the new phenomena are treated in our revised annotation scheme. In Section 4 we present our new workflow that decomposes our annotation task into smaller, self-contained tasks. We also discuss advantages of such an approach and problems that still exist. Section 5 presents some relevant statistics and Section 6 discusses related work. Section 7 concludes our paper. 2 An overview of the existing Chinese Treebank annotation framework The Chinese Treebank (CTB) is a fully segmented, part-of-speech (POS) tagged, and syntactically bracketed Chinese corpus annotated in a phrase structure framework [16]. The CTB adopts the same architectural and representation framework used by the Penn Treebank [13], as is natural given the success of the Penn Treebank annotation style and the affinity of the research groups. Just like the Penn Treebank, the CTB has three layers of annotation: word segmentation / tokenization, part-ofspeech (POS) tagging, and syntactic bracketing. There are three sets of guidelines [17,18,19], one for each layer, and the syntactic bracketing guidelines are by far the most complex among the three. At the part-of-speech tagging layer, each word token in the corpus is assigned"
W13-0307,li-etal-2012-parallel,1,0.842279,"Missing"
W13-0307,I11-1125,0,0.0549512,"Missing"
W13-0307,D10-1032,0,0.0265413,"Missing"
W13-0307,S07-1014,0,0.0244061,"Missing"
W13-0307,xue-etal-2008-annotating,1,0.850424,"e CSFs. We target similar distinctions in our annotation, but instead of treating these distinctions as unstructured finegrained senses, we classify events along three different dimensions that in conjunction can make similar distinctions in a more structured manner. On the Chinese side, there have been several past attempts to infer “tense” for Chinese automatically using statistical models and modest success has been reported. There are two general approaches to “tense” inference for Chinese. The first approach has been to manually annotate tense on Chinese verbs (Ye et al., 2006; Ye, 2007; Xue et al., 2008; Xue, 2008) and use the annotated data to train statistical models to predict tense in previously unseen text. (Xue, 2008) has shown that even though there are no morpho-syntactic clues for tense in Chinese, contextual information can be exploited to infer “tense”. Such contextual information includes explicit clues such as time expressions and aspect markers as well as implicit information such as verb types: bounded events (e.g., “explode”) tend to occur in the past while unbounded events tend to occur in the present (e.g., “believe, know, like”), a generalization first articulated in (Smit"
W13-0307,D08-1074,1,0.934316,"these text spans with tense and modality categories. Each text span is annotated along three dimensions to support the planned automatic inference of tense and modality on the Chinese side. The first dimension is the semantic tense, and the annotator must indicate whether the text span describes a past, present, or future event state. The second dimension is event type that indicates whether the text span represents a habitual event, an ongoing event, a completed event, an episodic event, or a state. The event type is annotated because it has been shown (Smith, 2001; Smith and Erbaugh, 2005; Xue, 2008) that in a language without explicit tense markers, event types are good indicators of tense. The third dimension is modality. The modality dimension is broadly construed and it classifies events or states as actual, intended (which encompasses expected, planned events), hypothetical (as in conditional clauses) or modalized. An event or state is modalized if it occurs with a modal verb that indicates possibility, necessity, or ability. These categories are very coarse-grained and we did not get into the finer distinctions of different types of modality. Each of these categories are described i"
W13-0307,W06-0107,0,0.0609709,"Missing"
W13-0307,S10-1010,0,\N,Missing
W13-3516,P05-1022,0,0.0426867,"umption. For the spoken genres – BC, BN and TC – we use the manual transcriptions rather than the output of a speech recognizer, as would be the case in real world. The performance on various layers for these genres would therefore be artificially inflated, and should be taken into account while analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-"
W13-3516,P08-1091,1,0.914176,"in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6 1208 2849 92.8 9,261 26,882 87.7 885 2,323 93.3 929 4,419 92.3 451 2,620 96.6 481 2,210 82.2 968 1,622 87.8 758 1,761 90.9 4,472 14,955 8"
W13-3516,W00-1322,0,0.0291123,"Missing"
W13-3516,W06-0609,0,0.0675644,"and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word senses are tagged for the mos"
W13-3516,2011.mtsummit-papers.22,1,0.659518,"reference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic informatio"
W13-3516,P06-1005,0,0.0274234,"able. 4.5 Coreference The task is to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains. The coreference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features i"
W13-3516,W12-4503,1,0.323243,"Missing"
W13-3516,W11-1905,1,0.8623,"Missing"
W13-3516,W10-4305,0,0.00957321,"thm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported in Recasens et al., (2013). As of this writing, the BCUBED metric has been fixed, and the correctness of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performa"
W13-3516,P11-2037,0,0.0435593,"tructure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This meant removing any and"
W13-3516,W05-0620,0,0.0226073,"Missing"
W13-3516,P05-1045,0,0.0391503,"of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performance of the system on the Table 6: Performance of the named entity recognizer on the CoNLL-2012 test set. 4.4 Named Entities We retrained the Stanford named entity recognizer20 (Finkel et al., 2005) on the OntoNotes data. Table 6 shows the performance details for all the languages across all 18 name types broken down by genre. In English, BN has the highest performance followed by the NW genre. There is a significant drop from those and the TC and WB genre. Somewhat similar trend is observed in the Chinese data, with Arabic having the lowest scores. Since the Pivot Text portion (PT) of OntoNotes was not tagged with names, we could not compute the accuracy for that cross-section of the data. Previously Finkel and Manning (2009) performed 17 The number of sentences in this table are a subs"
W13-3516,W01-0521,0,0.0213903,"ver the sense inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, develo"
W13-3516,hockenmaier-steedman-2002-acquiring,0,0.0152391,"g5 1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA 2 University of Trento, University of Trento, 38123 Povo (TN), Italy 3 QCRI, Qatar Foundation, 5825 Doha, Qatar 4 Brandeis University, Brandeis University, Waltham, MA 02453, USA 5 National University of Singapore, Singapore, 117417 6 University of Stuttgart, 70174 Stuttgart, Germany Abstract the Penn Discourse Treebank (Prasad et al., 2008), and many other annotation projects, all annotate the same underlying body of text. It was also converted to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the cu"
W13-3516,W04-1602,0,0.0421028,"d to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the current hurdles in language processing is the problem of domain, or genre adaptation. Although genre or domain are popular terms, their definitions are still vague. In OntoNotes, “genre” means a type of source – newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), telephone conversation (TC), web data (WB) or pivot text (PT). Changes in the entity and event profiles across source types, and even in the same source over a time duration, as explicitly expressed by surface lexical forms, usually"
W13-3516,J93-2004,0,0.0664902,"sitions for most verb and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word s"
W13-3516,N06-1020,0,0.0154191,"inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so a"
W13-3516,P02-1014,0,0.0344201,"ce annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported"
W13-3516,J05-1004,0,0.0531944,"rformance. 1 Introduction Roughly a million words of text from the Wall Street Journal newswire (WSJ), circa 1989, has had a significant impact on research in the language processing community — especially those in the area of syntax and (shallow) semantics, the reason for this being the seminal impact of the Penn Treebank project which first selected this text for annotation. Taking advantage of a solid syntactic foundation, later researchers who wanted to annotate semantic phenomena on a relatively large scale, also used it as the basis of their annotation. For example the Proposition Bank (Palmer et al., 2005), BBN Name Entity and Pronoun coreference corpus (Weischedel and Brunstein, 2005), 1 A portion of the English data in the OntoNotes corpus is a selected set of sentences that were annotated for parse and word sense information. These sentences are present in a document of their own, and so the documents for parse layers for English are inflated by about 3655 documents and for the word sense are inflated by about 8797 documents. 143 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143–152, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Comput"
W13-3516,palmer-etal-2008-pilot,0,0.0620251,"Missing"
W13-3516,N07-1051,0,0.0519693,"ile analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-validation. For testing, we used a model trained on the entire training portion. Table 3 shows the precision, recall and F1 -scores of the re-trained parsers on the CoNLL-2012 test along with the part of speech accuracies (POS) using the standard evalb scorer. The performance on the PT ge"
W13-3516,J08-2006,1,0.738119,"of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so as to facilitate future studies. 2.1.3 Proposition The propositions in OntoNo"
W13-3516,W11-1901,1,0.408033,"Missing"
W13-3516,W12-4501,1,0.81236,"he exact same phenomena have been annotated on a broad cross-section of the same language before OntoNotes. The OntoNotes corpus thus provides an opportunity for studying the genre effect on different syntactic, semantic and discourse analyzers. Parts of the OntoNotes Corpus have been used for various shared tasks organized by the language processing community. The word sense layer was the subject of prediction in two SemEval-2007 tasks, and the coreference layer was the subject of prediction in the SemEval-20102 (Recasens et al., 2010), CoNLL-2011 and 2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). The CoNLL-2012 shared task provided predicted information to the participants, however, that did not include a few layers such as the named entities for Chinese and Arabic, propositions for Arabic, and for better comparison of the English data with the CoNLL-2011 task, a smaller OntoNotes v4.0 portion of the English parse and propositions was used for training. This paper is a first attempt at presenting a coherent high-level picture of the performance of various publicly available state-of-the-art tools on all the layers of OntoNotes in all three languages, so as to pave the way for further"
W13-3516,prasad-etal-2008-penn,0,0.0477485,"Missing"
W13-3516,S10-1001,0,0.0530331,"Missing"
W13-3516,N13-1071,0,0.0213813,"Missing"
W13-3516,J08-2004,1,0.227032,"nd LINK - PCR. Since the community is not used to the new PropBank representation which (i) relies heavily on the trace structure in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6"
W13-3516,C10-2158,1,0.243983,"nd the proposition structure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This mean"
W13-3516,W10-1836,1,0.361229,"Missing"
W13-3516,P10-4014,1,0.579474,"Missing"
W13-3516,D08-1105,1,0.881098,"Missing"
W13-3516,N01-1016,0,\N,Missing
W13-3516,N07-1070,1,\N,Missing
W13-3516,N09-1037,0,\N,Missing
W14-4904,P10-1146,0,0.0655495,"show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel parse trees with some modiﬁcation of existing syntactic annotation guidelines. We believe this has implications for the use of parallel parse trees as an important resource for Machine Translation models. 1 Introduction Parallel treebanks have been proved to be a valuable resource in Machine Translation research (Gildea, 2003; Liu et al., 2009; Sun et al., 2010; Chiang, 2010; Xiao and Zhu, 2013), but one issue that hampers their utility is the incompatibility between the syntactic parse trees for a sentence pair (Chiang, 2010), as the trees are annotated based on independently developed monolingual syntactic annotation standards. For example, even though the Penn Chinese Treebank (Xue et al., 2005) and English TreeBank (Marcus et al., 1993) are often referred to collectively as the Penn series of treebanks and are both annotated with phrase structure trees in very similar annotation frameworks, different annotation decisions have led to divergent tree structures"
W14-4904,C14-1143,1,0.679452,"自己 . . 品评 . 方式 都 . . 可以 . . 利用 . .此 . . 国家 . 人民 . 的 . 所有 . . . . methods can . use People . in. all . . their . of. expression . in. on. this to. weigh . countries . own DT . . NP . . IN . . RP . .PP . VB NN . . JJ. . PRP . . PDT NNS . . IN . . . . NP . . PP . . . NP NNS . . . . TO . . VP . NP . . . . MD . NP . . PP . NP . . . VB . NP . . . NNS . . IN . VP . VP . S. Figure 1: A hierarchically aligned sentence pair 31 S. VP . string to 品评/weigh in since to weigh in is not even a constituent in the English parse tree. For a more comprehensive and detailed description of the HACEPT project, see (Deng and Xue, 2014). A natural question arises for our approach: cross-linguistic divergences between languages may cause parse tree incompatibilities to arise, which calls into question the possibility of doing phrase alignments to a useful extent. The fact is that we did ﬁnd incompatibilities between parse trees in our annotation. In the next section, we report three types of parse tree incompatibilities we have encountered. 3 Three types of parse tree incompatibilities During the annotation process, we encountered three types of parse tree incompatibilities that make some phrase alignments impossible. The thr"
W14-4904,P03-1011,0,0.0604157,"tibility patterns identiﬁed within VPs and NPs and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel parse trees with some modiﬁcation of existing syntactic annotation guidelines. We believe this has implications for the use of parallel parse trees as an important resource for Machine Translation models. 1 Introduction Parallel treebanks have been proved to be a valuable resource in Machine Translation research (Gildea, 2003; Liu et al., 2009; Sun et al., 2010; Chiang, 2010; Xiao and Zhu, 2013), but one issue that hampers their utility is the incompatibility between the syntactic parse trees for a sentence pair (Chiang, 2010), as the trees are annotated based on independently developed monolingual syntactic annotation standards. For example, even though the Penn Chinese Treebank (Xue et al., 2005) and English TreeBank (Marcus et al., 1993) are often referred to collectively as the Penn series of treebanks and are both annotated with phrase structure trees in very similar annotation frameworks, different annotatio"
W14-4904,P09-1063,0,0.221731,"rns identiﬁed within VPs and NPs and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel parse trees with some modiﬁcation of existing syntactic annotation guidelines. We believe this has implications for the use of parallel parse trees as an important resource for Machine Translation models. 1 Introduction Parallel treebanks have been proved to be a valuable resource in Machine Translation research (Gildea, 2003; Liu et al., 2009; Sun et al., 2010; Chiang, 2010; Xiao and Zhu, 2013), but one issue that hampers their utility is the incompatibility between the syntactic parse trees for a sentence pair (Chiang, 2010), as the trees are annotated based on independently developed monolingual syntactic annotation standards. For example, even though the Penn Chinese Treebank (Xue et al., 2005) and English TreeBank (Marcus et al., 1993) are often referred to collectively as the Penn series of treebanks and are both annotated with phrase structure trees in very similar annotation frameworks, different annotation decisions have l"
W14-4904,J93-2004,0,0.0462994,"allel parse trees as an important resource for Machine Translation models. 1 Introduction Parallel treebanks have been proved to be a valuable resource in Machine Translation research (Gildea, 2003; Liu et al., 2009; Sun et al., 2010; Chiang, 2010; Xiao and Zhu, 2013), but one issue that hampers their utility is the incompatibility between the syntactic parse trees for a sentence pair (Chiang, 2010), as the trees are annotated based on independently developed monolingual syntactic annotation standards. For example, even though the Penn Chinese Treebank (Xue et al., 2005) and English TreeBank (Marcus et al., 1993) are often referred to collectively as the Penn series of treebanks and are both annotated with phrase structure trees in very similar annotation frameworks, different annotation decisions have led to divergent tree structures (Chiang, 2010). The purpose of this study is to investigate to what extent the divergences between Chinese-English parallel parse trees are caused by different annotation styles (and therefore can be avoided by revising the annotation guidelines), and to what extent they are caused by cross-linguistic differences inherent in language. The answer to this question would sh"
W14-4904,P10-1032,0,0.137671,"in VPs and NPs and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel parse trees with some modiﬁcation of existing syntactic annotation guidelines. We believe this has implications for the use of parallel parse trees as an important resource for Machine Translation models. 1 Introduction Parallel treebanks have been proved to be a valuable resource in Machine Translation research (Gildea, 2003; Liu et al., 2009; Sun et al., 2010; Chiang, 2010; Xiao and Zhu, 2013), but one issue that hampers their utility is the incompatibility between the syntactic parse trees for a sentence pair (Chiang, 2010), as the trees are annotated based on independently developed monolingual syntactic annotation standards. For example, even though the Penn Chinese Treebank (Xue et al., 2005) and English TreeBank (Marcus et al., 1993) are often referred to collectively as the Penn series of treebanks and are both annotated with phrase structure trees in very similar annotation frameworks, different annotation decisions have led to divergent tr"
W15-1001,2008.amta-srw.1,0,0.782853,"ity by incorporating syntactic information into word alignments (May and Knight, 2007; Fossum, Knight, and Abney, 2008). Another research direction has been explored to conduct syntactic alignment between parse trees (Tinsley et al., 2007; Pauls et al., 2010; Sun, Zhang, and Tan, 2010b; Sun, Zhang, and Tan, 2010a), and implements syntactic rule extraction based on syntactic alignment instead of word alignment. Our work reported in Section 3 can be viewed as a combination of these two lines of research. There has also been reasearch done to automatically obtain phrasal translation equivalents (Ambati and Lavie, 2008; Hanneman, Burroughs, and Lavie, 2011; Lavie, Parlikar, and Ambati, 2008; Zhu, Li, and Xiao, 2015). This line of research is different from our work in two respects: First, word alignment as the foundation of phrasepair extraction is done differently in the two approaches. Automatic extraction of phrase pairs uses automatically generated word alignments, where there are lots of spurious word alignments, which, as pointed out by (Zhu, Li, and Xiao, 2015), are harmful to rule extraction and affect translation quality. By contrast, HACEPT is free of spurious word alignments. As already mentioned"
W15-1001,J93-2003,0,0.0829353,"ation. To address the problem, we propose a new annotation scheme where word alignment and the alignment of non-terminal nodes (i.e., phrases) are done simultaneously to avoid conﬂicts between word alignments and syntactic structures. Relying on this new alignment approach, we construct a Hierarchically Aligned Chinese-English Parallel Treebank (HACEPT), and show that all phrasal translation equivalents can be automatically extracted based on the phrase alignments in HACEPT. 1 Introduction During the past two decades since the emergence of the statistical paradigm of Machine Translation (MT) (Brown et al., 1993), the ﬁeld of Statistical Machine Translation (SMT) has attained consensus on the need for structural mappings between languages in MT. Accurately identifying structural mappings (i.e., phrasal translation equivalents) is critical to the performance of both phrase-based systems (Koehn, Och, and Marcu, 2003; Och and Ney, 2004) and syntax-based systems (Chiang, 2005; Chiang, 2007; Galley et al., 2004). The fact is that phrasal translation equivalents are identiﬁed based on word alignments, so how word alignments are done directly affects the identiﬁcation of phrasal translation equivTo address t"
W15-1001,P05-1033,0,0.189668,"l phrasal translation equivalents can be automatically extracted based on the phrase alignments in HACEPT. 1 Introduction During the past two decades since the emergence of the statistical paradigm of Machine Translation (MT) (Brown et al., 1993), the ﬁeld of Statistical Machine Translation (SMT) has attained consensus on the need for structural mappings between languages in MT. Accurately identifying structural mappings (i.e., phrasal translation equivalents) is critical to the performance of both phrase-based systems (Koehn, Och, and Marcu, 2003; Och and Ney, 2004) and syntax-based systems (Chiang, 2005; Chiang, 2007; Galley et al., 2004). The fact is that phrasal translation equivalents are identiﬁed based on word alignments, so how word alignments are done directly affects the identiﬁcation of phrasal translation equivTo address this shortcoming, we designed a hierarchical alignment scheme in which word-level alignment (namely alignment of terminal nodes) and phrase-level alignments (namely alignment of nonterminals) are done simultaneously in a coordinated manner so that conﬂicts between word alignments and syntactic structures are avoided. Based on this alignment scheme, we constructed a"
W15-1001,J07-2003,0,0.187779,"slation equivalents can be automatically extracted based on the phrase alignments in HACEPT. 1 Introduction During the past two decades since the emergence of the statistical paradigm of Machine Translation (MT) (Brown et al., 1993), the ﬁeld of Statistical Machine Translation (SMT) has attained consensus on the need for structural mappings between languages in MT. Accurately identifying structural mappings (i.e., phrasal translation equivalents) is critical to the performance of both phrase-based systems (Koehn, Och, and Marcu, 2003; Och and Ney, 2004) and syntax-based systems (Chiang, 2005; Chiang, 2007; Galley et al., 2004). The fact is that phrasal translation equivalents are identiﬁed based on word alignments, so how word alignments are done directly affects the identiﬁcation of phrasal translation equivTo address this shortcoming, we designed a hierarchical alignment scheme in which word-level alignment (namely alignment of terminal nodes) and phrase-level alignments (namely alignment of nonterminals) are done simultaneously in a coordinated manner so that conﬂicts between word alignments and syntactic structures are avoided. Based on this alignment scheme, we constructed a Hierarchicall"
W15-1001,W08-0306,0,0.023041,"will serve as the phrase alignment of VPe1 , which cannot be unaligned in the ﬁgure. With the phrase alignment between VPe1 and the hypothetical intermediate node (call it VPc9 ), the number of terminal nodes in (6) will be reduced to zero even without the creation of VPe0 in (7). The new rule looks like this: (8) IP0 ⇔ S0 NPc0 ADVP VPc9 <> NPe0 ADVP VPe1 (VPc9 ⇒ ADVP VPc1 ) 7 7 Related work To address the problem caused by spurious word alignments, there has been research done to improve word alignment quality by incorporating syntactic information into word alignments (May and Knight, 2007; Fossum, Knight, and Abney, 2008). Another research direction has been explored to conduct syntactic alignment between parse trees (Tinsley et al., 2007; Pauls et al., 2010; Sun, Zhang, and Tan, 2010b; Sun, Zhang, and Tan, 2010a), and implements syntactic rule extraction based on syntactic alignment instead of word alignment. Our work reported in Section 3 can be viewed as a combination of these two lines of research. There has also been reasearch done to automatically obtain phrasal translation equivalents (Ambati and Lavie, 2008; Hanneman, Burroughs, and Lavie, 2011; Lavie, Parlikar, and Ambati, 2008; Zhu, Li, and Xiao, 20"
W15-1001,N04-1035,0,0.309203,"lents can be automatically extracted based on the phrase alignments in HACEPT. 1 Introduction During the past two decades since the emergence of the statistical paradigm of Machine Translation (MT) (Brown et al., 1993), the ﬁeld of Statistical Machine Translation (SMT) has attained consensus on the need for structural mappings between languages in MT. Accurately identifying structural mappings (i.e., phrasal translation equivalents) is critical to the performance of both phrase-based systems (Koehn, Och, and Marcu, 2003; Och and Ney, 2004) and syntax-based systems (Chiang, 2005; Chiang, 2007; Galley et al., 2004). The fact is that phrasal translation equivalents are identiﬁed based on word alignments, so how word alignments are done directly affects the identiﬁcation of phrasal translation equivTo address this shortcoming, we designed a hierarchical alignment scheme in which word-level alignment (namely alignment of terminal nodes) and phrase-level alignments (namely alignment of nonterminals) are done simultaneously in a coordinated manner so that conﬂicts between word alignments and syntactic structures are avoided. Based on this alignment scheme, we constructed a Hierarchically Aligned Chinese-Engl"
W15-1001,W11-1015,0,0.0142174,"tactic information into word alignments (May and Knight, 2007; Fossum, Knight, and Abney, 2008). Another research direction has been explored to conduct syntactic alignment between parse trees (Tinsley et al., 2007; Pauls et al., 2010; Sun, Zhang, and Tan, 2010b; Sun, Zhang, and Tan, 2010a), and implements syntactic rule extraction based on syntactic alignment instead of word alignment. Our work reported in Section 3 can be viewed as a combination of these two lines of research. There has also been reasearch done to automatically obtain phrasal translation equivalents (Ambati and Lavie, 2008; Hanneman, Burroughs, and Lavie, 2011; Lavie, Parlikar, and Ambati, 2008; Zhu, Li, and Xiao, 2015). This line of research is different from our work in two respects: First, word alignment as the foundation of phrasepair extraction is done differently in the two approaches. Automatic extraction of phrase pairs uses automatically generated word alignments, where there are lots of spurious word alignments, which, as pointed out by (Zhu, Li, and Xiao, 2015), are harmful to rule extraction and affect translation quality. By contrast, HACEPT is free of spurious word alignments. As already mentioned in Section 3, all the word alignments"
W15-1001,N03-1017,0,0.0489242,"cally Aligned Chinese-English Parallel Treebank (HACEPT), and show that all phrasal translation equivalents can be automatically extracted based on the phrase alignments in HACEPT. 1 Introduction During the past two decades since the emergence of the statistical paradigm of Machine Translation (MT) (Brown et al., 1993), the ﬁeld of Statistical Machine Translation (SMT) has attained consensus on the need for structural mappings between languages in MT. Accurately identifying structural mappings (i.e., phrasal translation equivalents) is critical to the performance of both phrase-based systems (Koehn, Och, and Marcu, 2003; Och and Ney, 2004) and syntax-based systems (Chiang, 2005; Chiang, 2007; Galley et al., 2004). The fact is that phrasal translation equivalents are identiﬁed based on word alignments, so how word alignments are done directly affects the identiﬁcation of phrasal translation equivTo address this shortcoming, we designed a hierarchical alignment scheme in which word-level alignment (namely alignment of terminal nodes) and phrase-level alignments (namely alignment of nonterminals) are done simultaneously in a coordinated manner so that conﬂicts between word alignments and syntactic structures ar"
W15-1001,W08-0411,0,0.0950002,"s (May and Knight, 2007; Fossum, Knight, and Abney, 2008). Another research direction has been explored to conduct syntactic alignment between parse trees (Tinsley et al., 2007; Pauls et al., 2010; Sun, Zhang, and Tan, 2010b; Sun, Zhang, and Tan, 2010a), and implements syntactic rule extraction based on syntactic alignment instead of word alignment. Our work reported in Section 3 can be viewed as a combination of these two lines of research. There has also been reasearch done to automatically obtain phrasal translation equivalents (Ambati and Lavie, 2008; Hanneman, Burroughs, and Lavie, 2011; Lavie, Parlikar, and Ambati, 2008; Zhu, Li, and Xiao, 2015). This line of research is different from our work in two respects: First, word alignment as the foundation of phrasepair extraction is done differently in the two approaches. Automatic extraction of phrase pairs uses automatically generated word alignments, where there are lots of spurious word alignments, which, as pointed out by (Zhu, Li, and Xiao, 2015), are harmful to rule extraction and affect translation quality. By contrast, HACEPT is free of spurious word alignments. As already mentioned in Section 3, all the word alignments in HACEPT are compatible with the"
W15-1001,li-etal-2012-parallel,1,0.826485,"have an equivalent in terms of lexical meaning and grammatical function. For those words that do not have a translation counterpart, we leave them unaligned at word level and instead the appropriate phrases in which they appear. This strategy makes sure that both redundancies and incompatibilities between word alignments and syntactic structures are avoided. In addition, artiﬁcial ambiguities are also eliminated. These points will be illustrated in the discussion of the concrete example in Figure 1 below. We take the Chinese-English portion of the Parallel Aligned Treebank (PAT) described in (Li et al., 2012) for annotation. Our data have three batches: one batch is weblogs, one batch is postings from online discussion forums and one batch is news wire. The English sentences in the data set are annotated based on the original Penn TreeBank (PTB) annotation stylebook (Bies et al., 1995) as well as its extensions (Warner et al., 2004), while the Chinese sentences in the data set are annotated based on the Chinese TreeBank (CTB) annotation guidelines (Xue and Xia, 1998) and its extensions ZhangandXue12. The PAT has no phrase alignments and the word alignments in it are done under the requirement that"
W15-1001,J93-2004,0,0.0507919,"slation equivalents from being extracted. In this section, we will show how spurious word alignments in human annotated word alignments make the extraction of phrasal translation equivalents impossible. Consider the following example quoted from (Li, Ge, and Strassel, 2009), where the relevant word alignment in each sentence/phrase pair is highlighted by underlining. Note that the word alignments are done without taking syntactic structures into consideration, as can be told from the fact that all the underlined aligned multi-word strings do not correspond to a constituent in a Penn TreeBank (Marcus, Santorini, and Marcinkiewicz, 1993) or Chinese TreeBank (Xue et al., 2005) parse tree. 2b. gone to Beijing <> 去 北京 2c. solve the problem <> 解决 问题 2d. doing experiments <> 做 实验 2e. the Chinese position <> 中方 的 立场 The reason why the phrasal translation equivalents in (2) cannot be extracted is because a word in a phrase on one side is aligned to a word that is not part of the corresponding phrase on the other side. Take (2c) for instance. The Chinese verb 解决/solve in the phrase 解决 问题 is aligned to both solve and to in (1c), which is not part of the phrase solve the problem. As a result, the phrase pair in (2c) cannot be obtained"
W15-1001,D07-1038,0,0.16066,"his intermediate node will serve as the phrase alignment of VPe1 , which cannot be unaligned in the ﬁgure. With the phrase alignment between VPe1 and the hypothetical intermediate node (call it VPc9 ), the number of terminal nodes in (6) will be reduced to zero even without the creation of VPe0 in (7). The new rule looks like this: (8) IP0 ⇔ S0 NPc0 ADVP VPc9 <> NPe0 ADVP VPe1 (VPc9 ⇒ ADVP VPc1 ) 7 7 Related work To address the problem caused by spurious word alignments, there has been research done to improve word alignment quality by incorporating syntactic information into word alignments (May and Knight, 2007; Fossum, Knight, and Abney, 2008). Another research direction has been explored to conduct syntactic alignment between parse trees (Tinsley et al., 2007; Pauls et al., 2010; Sun, Zhang, and Tan, 2010b; Sun, Zhang, and Tan, 2010a), and implements syntactic rule extraction based on syntactic alignment instead of word alignment. Our work reported in Section 3 can be viewed as a combination of these two lines of research. There has also been reasearch done to automatically obtain phrasal translation equivalents (Ambati and Lavie, 2008; Hanneman, Burroughs, and Lavie, 2011; Lavie, Parlikar, and Am"
W15-1001,J03-1002,0,0.0668822,"er Science Department Brandeis University 415 South Street, Waltham, MA 02453 ddeng@brandeis.edu, xuen@brandeis.edu, shim@brandeis.edu Abstract alents. As reported by (Zhu, Li, and Xiao, 2015), even one spurious word alignment can prevent some desirable phrasal translation equivalents from being extracted. The unfortunate fact is that spurious word alignments abound in current word-aligned parallel texts used for extracting phrasal translation equivalents. This is because the word alignments in these parallel texts, whether they are induced in an unsupervised manner such as that described by (Och and Ney, 2003) or manually annotated based on existing word alignment standards such as (Li, Ge, and Strassel, 2009) and (Melamed, 1998), are generally done as an independent task without taking syntactic structures into consideration. As a result, conﬂicts between word alignments and syntactic structures are inevitable, and when such a conﬂict arises, the extraction of desirable phrasal translation equivalents will be impossible. Accurate identiﬁcation of phrasal translation equivalents is critical to both phrase-based and syntax-based machine translation systems. We show that the extraction of many phrasa"
W15-1001,J04-4002,0,0.130423,"Parallel Treebank (HACEPT), and show that all phrasal translation equivalents can be automatically extracted based on the phrase alignments in HACEPT. 1 Introduction During the past two decades since the emergence of the statistical paradigm of Machine Translation (MT) (Brown et al., 1993), the ﬁeld of Statistical Machine Translation (SMT) has attained consensus on the need for structural mappings between languages in MT. Accurately identifying structural mappings (i.e., phrasal translation equivalents) is critical to the performance of both phrase-based systems (Koehn, Och, and Marcu, 2003; Och and Ney, 2004) and syntax-based systems (Chiang, 2005; Chiang, 2007; Galley et al., 2004). The fact is that phrasal translation equivalents are identiﬁed based on word alignments, so how word alignments are done directly affects the identiﬁcation of phrasal translation equivTo address this shortcoming, we designed a hierarchical alignment scheme in which word-level alignment (namely alignment of terminal nodes) and phrase-level alignments (namely alignment of nonterminals) are done simultaneously in a coordinated manner so that conﬂicts between word alignments and syntactic structures are avoided. Based on"
W15-1001,N10-1014,0,0.0382789,"Missing"
W15-1001,C10-1118,0,0.0199189,"Pc9 ), the number of terminal nodes in (6) will be reduced to zero even without the creation of VPe0 in (7). The new rule looks like this: (8) IP0 ⇔ S0 NPc0 ADVP VPc9 <> NPe0 ADVP VPe1 (VPc9 ⇒ ADVP VPc1 ) 7 7 Related work To address the problem caused by spurious word alignments, there has been research done to improve word alignment quality by incorporating syntactic information into word alignments (May and Knight, 2007; Fossum, Knight, and Abney, 2008). Another research direction has been explored to conduct syntactic alignment between parse trees (Tinsley et al., 2007; Pauls et al., 2010; Sun, Zhang, and Tan, 2010b; Sun, Zhang, and Tan, 2010a), and implements syntactic rule extraction based on syntactic alignment instead of word alignment. Our work reported in Section 3 can be viewed as a combination of these two lines of research. There has also been reasearch done to automatically obtain phrasal translation equivalents (Ambati and Lavie, 2008; Hanneman, Burroughs, and Lavie, 2011; Lavie, Parlikar, and Ambati, 2008; Zhu, Li, and Xiao, 2015). This line of research is different from our work in two respects: First, word alignment as the foundation of phrasepair extraction is done differently in the two"
W15-1001,P10-1032,0,0.306749,"Pc9 ), the number of terminal nodes in (6) will be reduced to zero even without the creation of VPe0 in (7). The new rule looks like this: (8) IP0 ⇔ S0 NPc0 ADVP VPc9 <> NPe0 ADVP VPe1 (VPc9 ⇒ ADVP VPc1 ) 7 7 Related work To address the problem caused by spurious word alignments, there has been research done to improve word alignment quality by incorporating syntactic information into word alignments (May and Knight, 2007; Fossum, Knight, and Abney, 2008). Another research direction has been explored to conduct syntactic alignment between parse trees (Tinsley et al., 2007; Pauls et al., 2010; Sun, Zhang, and Tan, 2010b; Sun, Zhang, and Tan, 2010a), and implements syntactic rule extraction based on syntactic alignment instead of word alignment. Our work reported in Section 3 can be viewed as a combination of these two lines of research. There has also been reasearch done to automatically obtain phrasal translation equivalents (Ambati and Lavie, 2008; Hanneman, Burroughs, and Lavie, 2011; Lavie, Parlikar, and Ambati, 2008; Zhu, Li, and Xiao, 2015). This line of research is different from our work in two respects: First, word alignment as the foundation of phrasepair extraction is done differently in the two"
W15-1001,2007.mtsummit-papers.62,0,0.635863,"rase alignments and the word alignments in it are done under the requirement that all the words in a sentence should be aligned. Next we discuss our annotation procedure in detail. Our annotators are presented with sentence pairs that come with parallel parse trees. The task of the annotator is to decide, ﬁrst on the word level and then on the phrase level, if a word or phrase needs to be aligned at all, and if so, to which word or phrase it should be aligned. The decisions about word alignment and phrase alignment are not independent, and must obey well-formedness constraints as outlined in (Tinsley et al., 2007): a. A non-terminal node can only be aligned once. b. if Node nc is aligned to Node ne , then the descendants of nc can only be aligned to descendants of ne . 3 3 This means that once a word alignment is in place, it puts constraints on phrase alignments. A pair of non-terminal nodes (nc , ne ) cannot be aligned if a word that is a descendant of nc is aligned to a word that is not a descendant of ne on the word level. Let us use the concrete example in Figure 1 to illustrate the annotation process, which is guided by a set of detailed annotation guidelines. On the word level, only those words"
W16-1702,bonial-etal-2014-propbank,0,0.014421,"ror rate in alignment imposes a serious limitation on the overall AMR parsing accuracy. 2 AMR Overview In AMR annotation, each sentence is represented as a rooted, directed, acyclic graph in which the nodes are concepts and the edges are relations between the concepts. The backbone of an AMR graph is the predicate-argument structure of verbal or nominal predicates, though syntactic notions such as verbs and nouns are not part of the AMR vocabulary. AMR draws this aspect from the Proposition Bank (Palmer et al., 2005): the core argument roles (Arg0-Arg5) are defined in the PropBank frame files(Bonial et al., 2014), together with the senses of the predicates as different senses of a predicate have different argument structures. In addition, AMR also annotates named entities (person, location, company, etc.), relations between entities, time expressions, polarity and modality. In this paper we present the Chinese AMR (CAMR) annotation specifications that we use to annotate the Chinese translation of the Little Prince, which has 1,562 sentences. The CAMR annotation specifications are adapted from the AMR specification for English (Banarescu et al., 2015). We choose the Little Prince for our Chinese AMR an"
W16-1702,P13-2131,0,0.0655492,"that the alignment can be learned in an unsupervised manner with EM-based algorithms, just like word alignment between different languages can be learned automatically without the need for manual annotation. Although this expectation has been partially met in the work of Pourdamghani We annotated all of the 1562 sentences in the Chinese version of the Little Prince following the CAMR specifications. Two linguistic undergraduate students were trained to perform the annotation. Each completed the annotation for all of the 1562 sentences, and the inter-agreement is calculated by Smatch toolkit (Cai and Knight, 2013). The overall Smatch score between the two annotators is 0.83.2 We analyzed the annotated data from the two annotators to see to what extent the graph representation of the meaning of a sentence is necessary. Out of the 1,562 sentences of the two annotated files, 576 and 548 of them have non-tree CAMR graphs in Chinese version of the Little Prince. This is in comparison with the 663 sentences that have non-tree AMR graphs in the English version, which is sentence-aligned with the Chinese version. The Pearson correlation of the sentences having non-tree graphs is around 0.56, indicating the bil"
W16-1702,N04-1030,0,0.169311,"Missing"
W16-1702,P14-1134,0,0.144968,"Missing"
W16-1702,J02-3001,0,0.0658311,"Missing"
W16-1702,xue-etal-2014-interlingua,1,0.816689,"ect is such an attempt, along with other similar efforts such as the Universal Dependency annotation project (Nivre, 2015) and the Semantic Dependency Parsing effort (Oepen et al., 2014). One salient characteristic of AMR annotation is that it abstracts away from elements of surface syntactic structure such as word order and morpho-syntactic markers. Since word order and morpho-syntactic variations account for much of the cross-linguistic variations, this makes the AMR annotation framework more portable across languages, as the preliminary AMR annotation on Chinese and Czech has demonstrated (Xue et al., 2014). Another consequence of such “decoupling” from the syntactic structure of a sentence is that the AMR annotation framework gives us more freedom in how we handle cases of syntaxsemantic mismatch. Words that do not contribute to the meaning of a sentence (e.g., infinitive “to” in English) are left out of the AMR annotation. In light verb constructions such as “take a bath”, since the light verb “take” is semantically impoverished if not vacuous, it is also left out of the AMR annotation. Some discontinuous constructions such as “if. . . then” can be collapsed into a single relation “:condition”"
W16-1702,W04-2705,0,0.402674,"Missing"
W16-1702,W12-2511,0,0.057983,"Missing"
W16-1702,S14-2008,0,0.0260449,"jun Bu, Weiguang Qu School of Chinese Language and Literature Nanjing Normal University Nanjing, China libin.njnu@gmail.com Abstract argument structure annotation of other languages as well (Xue and Palmer, 2009; Zaghouani et al., 2012). As we gain more insights on the sentence meaning from annotating individual meaning components, annotating the meaning for the entire sentence becomes a logical next step. The AMR annotation project is such an attempt, along with other similar efforts such as the Universal Dependency annotation project (Nivre, 2015) and the Semantic Dependency Parsing effort (Oepen et al., 2014). One salient characteristic of AMR annotation is that it abstracts away from elements of surface syntactic structure such as word order and morpho-syntactic markers. Since word order and morpho-syntactic variations account for much of the cross-linguistic variations, this makes the AMR annotation framework more portable across languages, as the preliminary AMR annotation on Chinese and Czech has demonstrated (Xue et al., 2014). Another consequence of such “decoupling” from the syntactic structure of a sentence is that the AMR annotation framework gives us more freedom in how we handle cases o"
W16-1702,J05-1004,0,0.154851,"The interagreement smatch score between the two annotators is 0.83. We also propose to integrate alignment into Chinese AMR annotation. 1 Nianwen Xue Computer Science Department Brandeis University Waltham, MA, USA nwxue@brandeis.edu Introduction Abstract Meaning Representation (AMR) is an annotation framework designed to capture the “meaning” of a sentence with a single rooted, acyclic1 , directed graph (Banarescu et al., 2013), departing from previous practices of performing partial semantic annotation that focuses on one component of the sentential meaning at a time. For example, Propbank (Palmer et al., 2005; Xue and Palmer, 2009) and NomBank (Meyers et al., 2004) annotations focus on the predicate-argument structure of verbs and predicative or relational nouns. The annotation is done on a predicate basis and the resulting annotation may not necessarily form a fully connected structure for the entire sentence. The practice was necessary as a first attempt to annotate a key aspect of sentential meaning and contributed to a high-quality corpus that has spurred research in automatic Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Palmer et al., 20"
W16-1702,D14-1048,0,0.119897,"Missing"
W16-1702,W13-2322,0,\N,Missing
W16-1703,I11-1138,0,0.139561,"PS. Within the framework of Head-driven Phrase Structure Grammar (HPSG) the conversion implemented in this work outputs HPSG-conform PS trees via three steps: converting DS into pseudo PS by creating additional constituent nodes that immediately dominate head words and their dependents, annotating the branches of the pseudo PS with HPSG-oriented schemata, and binarizing the pseudo PS. This conversion process is specific to HPSG framework and cannot be straightforwardly manipulated for PTB style PS. Consequently, we follow a more universal DS-to-PS conversion procedure suggested in (Xia, 2008; Bhatt et al., 2011), including the following steps: 1) DS to DS+: removing non-projectivity 2) DS+ to PS+: simple and general conversion 3) PS+ to PS: handling subtleties In addition, we adopt the approaches of DS+ to PS+ conversion proposed in (Xia and Palmer, Introduction A treebank is usually created based on either dependency structure (DS) or phrase structure (PS) such that the selected formalism is optimally compatible with the language under consideration. From this perspective, DS formalism is suited for SynTagRus, the first general-purpose treebank (1M words) for Russian, a Slavic language with a relati"
W16-1703,boguslavsky-etal-2002-development,0,0.321107,"ing steps: 1) DS to DS+: removing non-projectivity 2) DS+ to PS+: simple and general conversion 3) PS+ to PS: handling subtleties In addition, we adopt the approaches of DS+ to PS+ conversion proposed in (Xia and Palmer, Introduction A treebank is usually created based on either dependency structure (DS) or phrase structure (PS) such that the selected formalism is optimally compatible with the language under consideration. From this perspective, DS formalism is suited for SynTagRus, the first general-purpose treebank (1M words) for Russian, a Slavic language with a relatively free word order (Boguslavsky et al., 2002). In contrast, existing gold standard corpora involving language variation and change such as Penn corpora of historical English (Kroch and Taylor, 2000; Kroch et al., 2004; Kroch et al., 2016) and the corpus of Appalachian English (Tortora et al., in progress) use PS formalism similar to English Penn Treebank (PTB) (Bies et al., 1995). To facilitate the creation of comparable corpora for less-configurational languages, and to enable the use of the wealth of NLP and theoretical 1 http://corpussearch.sourceforge.net/ index.html. 2 The code for this conversion is available at https:// github.com"
W16-1703,P99-1065,0,0.332266,"Missing"
W16-1703,H01-1014,0,\N,Missing
W16-1720,D15-1109,0,0.104024,"o be charming Figure 6: 4 Disagreement on connections Although determining which message is connected to which previous message is intuitive for the most part, disagreement does happen when a message has more than one possible and meaningful connection. For instance, message m0010 in Figure 6 can be a response to message m0009 or an extension of message m0008. This is one of the cases on which the two annotators disagree. Related work There has been relatively little work on annotating the discourse and dialogue structure of SMS conversations. The work that is most similar to ours is that of (Perret, 2015), where they annotated the discourse structure of multi-party dialogues using a corpus collected from an on-line version of the The Settlers of Catan game. They argue that multi-party dialogues need to be modeled with a graph structure and adopted an annotation scheme in the SDRT framework (Asher and Lascarides, 2003). In our annotation, since we are dealing with SMS dialogues that involve two participants, we did not find a graph structure to be necessary. We opted for a simpler (non-projective) dependency structure that is easier to model algorithmically. In fact, (Perret, 2015) developed an"
W16-1720,prasad-etal-2008-penn,0,0.427995,"S messages are usually short, and most of the messages consist of single sentences, but there are a small and yet significant proportion of messages that consist of multiple sentences. In our current round of annotation, we do not analyze relations between the sentences inside one message, but we leave that possibility open for future rounds of annotation. Compared 180 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 180–187, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics with discourse annotation of newswire text (Carlson et al., 2001; Prasad et al., 2008), determining the text units to perform annotation on is a relatively simple task, due to the fact that there is a natural boundary between text messages. 2.2 arcs are drawn one one side, there will be crossing edges. These properties are important in fashioning a strategy for parsing this structure automatically, a topic that is out of the scope of this paper. Linking each message to its antecedent message is the first step of our annotation project. Structure of the SMS message conversations 2.3 Due to the asynchronous nature of SMS message conversations, individual messages are often “out o"
W16-1720,J00-3003,0,0.504563,"Missing"
W16-1720,W01-1605,0,0.16002,"text messages. The SMS messages are usually short, and most of the messages consist of single sentences, but there are a small and yet significant proportion of messages that consist of multiple sentences. In our current round of annotation, we do not analyze relations between the sentences inside one message, but we leave that possibility open for future rounds of annotation. Compared 180 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 180–187, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics with discourse annotation of newswire text (Carlson et al., 2001; Prasad et al., 2008), determining the text units to perform annotation on is a relatively simple task, due to the fact that there is a natural boundary between text messages. 2.2 arcs are drawn one one side, there will be crossing edges. These properties are important in fashioning a strategy for parsing this structure automatically, a topic that is out of the scope of this paper. Linking each message to its antecedent message is the first step of our annotation project. Structure of the SMS message conversations 2.3 Due to the asynchronous nature of SMS message conversations, individual mes"
W19-3303,C18-1313,0,0.198296,"Missing"
W19-3303,E17-2039,0,0.0583464,"Missing"
W19-3303,J05-1004,0,0.351856,": by focusing on the predicative core of a sentence, it is an intuitive representation for both interpreting the semantics of a sentence, and perhaps more importantly, for use in annotation efforts. An AMR represents the meaning of a sentence with a single-rooted, directed, acyclic graph with nodes labeled with concepts and edges labeled with relations. The primary component of an AMR is the predicate-argument structure, with the predicate being a concept that takes a number of - ARG0 afford-01 time ARG0 person ARG1 moment car name name op “John” Propositions in an AMR are sensedisambiguated (Palmer et al., 2005). In the example above, “possible-01” refers to the first sense of “possible” while “afford-01” represents the first sense of “afford”. A predicate can take a number of core arguments (ARG0, ARG1, etc.) as well as adjunct arguments (e.g., time). The semantic roles for the core arguments are defined with respect to each sense of a predicate 28 Proceedings of the First International Workshop on Designing Meaning Representations, pages 28–33 c Florence, Italy, August 1st, 2019 2019 Association for Computational Linguistics and they are drawn from the PropBank frame files 1 . For example, the sema"
W19-3303,W13-2322,0,0.749115,"appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible. 1 (1) John can’t afford a car at the moment. (2) a. (p / possible-01 :ARG0 (a / afford-01 :ARG0 (p2 / person :name (n / name :op ""John"")) :ARG1 (c /car) :time (m / moment)) :polarity -) b. possible-01 Abstract Meaning Representations polarity Abstract Meaning Representations (AMRs) have recently become popular as a strategy for encoding a kind of canonical meaning for natural language sentences (Banarescu et al., 2013). They differ significantly from other encoding schemes used in NLP—e.g., minimal recursion semantics (MRS)—in terms of their expressiveness for several semantic phenomena in natural language (Copestake et al., 2005). Still, in spite of such shortcomings, there is a major attraction to the general philosophy of this approach: by focusing on the predicative core of a sentence, it is an intuitive representation for both interpreting the semantics of a sentence, and perhaps more importantly, for use in annotation efforts. An AMR represents the meaning of a sentence with a single-rooted, directed,"
W19-3303,W15-1841,0,0.0271802,"poral expressions and their relative scope over event variables, rather than quantified arguments to the verb. An example is that shown in (14). not ARG0 afford-01 time ARG0 person ARG1 moment car name name op “John” The graph-interpretation function continues walking down the tree, and expands the Skolemized form for ‘car’ into a quantified expression, inside the scope of the modal, as shown below. (14) a. John golfed every Sunday. b. ∀t[Sunday(t) → ∃e[golf(e, j) ∧ on(e, t)]] (16) ¬3[∃x[car(x)∧∃e[afford(e, j, x)∧@(e, N)]] This can be compared to the first-order modal expression generated by (Bos, 2015; Bos et al., 2017) for the sentence as shown below in (17). The strategy taken by (Bos et al., 2017), followed here as well, is to scope temporal expressions over the events they govern. Now let us see how the scope relation can be deployed to handle negation and modality in UMR. Consider first the treatment of modals in AMR. As seen in (2) above, modals are treated as predicative nodes. Hence, from (p / possible-01 :ARG0 phi), we can derive the equivalent propositional modal expression, 3φ. However, in (2) we need to translate the polarity over the modal appropriately: ¬3φ. (17) ¬∃x[car(x) ∧"
W19-3303,J16-3006,0,0.0510979,"(d / fail-01 :ARG0 (s / student :mod (a / all)) :polarity -) The attraction of AMR-style representations and annotations is the adoption of a predicative core element along with its arguments: e.g., an event and its participants. This, in turn, leads to an event-rooted graph that has many advantages for parsing and matching algorithms. As can be seen from the example, the predicate-argument structure is front and center in AMR, and we consider this to be one of its strengths. However, as it currently stands, AMR does not represent quantification or its interaction with modality and negation (Bos, 2016). The challenge is to maintain the focus on the predicate-argument structure while also adequately accounting for linguistic phenomena that operate above the level of the core AMR representation, in particular quantification and modality. 2 b. fail-01 ARG0 - polarity student mod all The sentence is ambiguous, however, between the readings “for every student, that student did not fail” and “it is not the case that every student failed”. While MRS and other flattened semantic representations provide a solution to these issues, giving faithful translations of scope with typed expressions, there a"
W19-3303,L18-1282,1,0.843096,"Missing"
W19-3303,E09-1001,0,0.0315359,"that student did not fail” and “it is not the case that every student failed”. While MRS and other flattened semantic representations provide a solution to these issues, giving faithful translations of scope with typed expressions, there are several drawbacks to these approaches. Flat representations reveal no semantic core. Hence, as annotations, the resulting structures are difficult to interpret and inspect. Furthermore, quantifier scope is often underspecified even when it can be disambiguated in context. Dependency MRS (DMRS) is one exception to this in the MRS family of representations (Copestake, 2009), where dependency relations link argument heads to the major predicator of the sentence. In our research, we propose to represent scope relationally, while maintaining both the centrality of the predicative core of the sentence (e.g., listen, drink), as well as the syntactic integrity of the quantified expression (e.g., every person). A relational interpretation for scope provides a first-order interpretation: it references two specific nodes in the graph, and orders one relative to the other. This operates over generalized quantifiers (some book, most people), negation (not, no), as well as"
xue-2006-annotating,W04-2705,0,\N,Missing
xue-2006-annotating,W03-1707,1,\N,Missing
xue-2006-annotating,J05-1004,0,\N,Missing
xue-etal-2008-annotating,W06-0107,0,\N,Missing
xue-etal-2014-interlingua,N07-1051,0,\N,Missing
xue-etal-2014-interlingua,W04-2705,0,\N,Missing
xue-etal-2014-interlingua,A00-2018,0,\N,Missing
xue-etal-2014-interlingua,J93-2004,0,\N,Missing
xue-etal-2014-interlingua,C12-1083,0,\N,Missing
xue-etal-2014-interlingua,W04-3212,1,\N,Missing
xue-etal-2014-interlingua,J03-4003,0,\N,Missing
xue-etal-2014-interlingua,Q13-1034,1,\N,Missing
xue-etal-2014-interlingua,W09-1201,1,\N,Missing
xue-etal-2014-interlingua,J02-3001,0,\N,Missing
xue-etal-2014-interlingua,J05-1004,1,\N,Missing
xue-etal-2014-interlingua,Q13-1019,0,\N,Missing
xue-etal-2014-interlingua,prasad-etal-2008-penn,0,\N,Missing
xue-etal-2014-interlingua,W13-2322,1,\N,Missing
xue-etal-2014-interlingua,N04-1030,0,\N,Missing
xue-zhang-2014-buy,D10-1032,0,\N,Missing
xue-zhang-2014-buy,J93-2004,0,\N,Missing
xue-zhang-2014-buy,S07-1014,0,\N,Missing
xue-zhang-2014-buy,I11-1125,0,\N,Missing
xue-zhang-2014-buy,D08-1074,1,\N,Missing
xue-zhang-2014-buy,S10-1010,0,\N,Missing
xue-zhang-2014-buy,li-etal-2012-parallel,1,\N,Missing
xue-zhang-2014-buy,W06-0107,0,\N,Missing
