2020.coling-main.454,J09-1005,1,0.711615,"matic meaning in our dataset. By contrast, the relatively high score assigned to ‘apples and oranges’ implies a significant ratio of literal contexts of this expression. We, therefore, imposed an (empirically determined) strict threshold of 0.25, filtering out idiomatic expressions exceeding this literality score from our analysis. 6 We used the pyinflect module to retrieve all verb tense forms. Removing the instances in the corpus of idiomatic expressions that are used literally would be a noisy alternative, since token-based identification of idioms is an open research problem. For example, Fazly et al. (2009) propose a method for identifying literal usages of idioms, but it does not generalize to the range of idiomatic forms in our list, since it depends on syntactic properties of idioms of a particular form. We take the more conservative approach of simply removing from consideration the idiomatic expressions that are likely to be commonly used in their literal interpretation. 7 5184 idiom wooden spoon on the bandwagon black and blue apples and oranges ball of fire score 0.600 0.593 0.498 0.441 0.414 idiom firing on all cylinders through thick and thin water under the bridge reading between the l"
2020.coling-main.454,2020.acl-main.51,1,0.637616,"ds within each embedding space provides a window into the subtle distinctions in usage patterns typical to the language of the two genders. We trained two sets of embeddings (using the gensim package), where all surface variants of an idiomatic expression (cf. Section 3.2.1) were treated as a single token in the training data. Because our goal is to identify expressions with the most distinct contextual usages across the genders, we determine for each idiom the similarity of their immediate neighbourhoods across the M and F sets of trained representations. Using a variation of the approach of Gonen et al. (2020), we compute the genderlanguage similarity of an idiom by finding the intersection of its neighbors in semantic space across the two genders, weighting close neighbors higher than more distant ones, in a ranked list of the 100 closest neighbours. Formally, we denote by Nik (M ) the ranked list of the k most similar words to idiom i in semantic space trained on male productions, and by Nik (F ) its counterpart list for embeddings trained on female language. A variation of the ranked-biased overlap (RBO) metric (Webber et al., 2010) is then applied to compute the similarity of the two lists. Ass"
2020.coling-main.454,D07-1039,0,0.0178575,"which we refer to by its canonical form; as an example, the cumulative counts of ‘pick a fight’ include instances of ‘pick a fight’, ‘picked a fight’, ‘picks a fight’, etc. 3.2.2 Filtering Out Expressions with a Common Literal Reading Because our goal is to analyze idiomatic language use, we aim to automatically remove from our list of idioms those with a high degree of literality – i.e., a high potential for literal interpretation.7 Relevant work has focused on the related property of compositionality – the extent to which an idiomatic interpretation is formed from the component words (e.g., McCarthy et al. (2007), Salehi et al. (2015)). While this property is related to literality (Spearman’s ρ=0.62 in human ratings, Nordmann et al. (2014)) it is not identical, since highly decomposable idioms may not actually occur frequently in their literal meaning. However, because the method of Salehi et al. (2015) uses word embeddings to detect compositionality (comparing the embedding for the entire idiom to the embeddings for its component words), it is a suitable approximation to literality: i.e., the frequent use of the idiomatic form in its literal meaning entails that the semantic representation of the idi"
2020.coling-main.454,W11-1709,0,0.0926002,"Missing"
2020.coling-main.454,P18-1017,0,0.0204518,"ntrol (although see Park et al. (2016) for an alternative finding). In this study we test if gender-related preferences in figurative language reveal differences along these emotional dimensions. Specifically, we test if the distributions of the three emotion variables across idiom usages differ between men and women, where the values of the emotional dimensions are based on the underlying meanings (definitions) of the idiomatic expressions. 4.2.1 Assessing the Emotional Dimensions of Idiom Definitions A large dataset of human rankings of VAD for 20, 000 English words was recently released by Mohammad (2018), where each word is assigned a value for each of the three dimensions on a 0-1 scale. Because we aim at estimating the affective variables of phrases (idiom definitions), rather than individual words, we must automatically infer the affective ratings of phrases using those of individual words, as follows. Word embedding spaces have been shown to capture variability in emotional dimensions closely corresponding to valence, arousal, and dominance (Hollis and Westbury, 2016), implying that such semantic representations carry over information useful for the task of emotional affect assessment. Th"
2020.coling-main.454,E17-1101,1,0.893648,"Missing"
2020.coling-main.454,D19-1410,0,0.0129868,"finitions), rather than individual words, we must automatically infer the affective ratings of phrases using those of individual words, as follows. Word embedding spaces have been shown to capture variability in emotional dimensions closely corresponding to valence, arousal, and dominance (Hollis and Westbury, 2016), implying that such semantic representations carry over information useful for the task of emotional affect assessment. Therefore, we exploit affective dimension ratings assigned to individual words for supervision in extracting ratings of sentences. We use the model introduced by Reimers and Gurevych (2019) for producing word- and sentence-embeddings using Siamese BERT-Networks, thereby obtaining semantic representations for the 20, 000 words in Mohammad (2018) as well as for the 527 idiom definitions in this study. Beta regression models8 were then trained to predict VAD scores from the embeddings for the 20K individual words, obtaining Pearson’s correlations of 0.85 (V), 0.78 (A), and 0.81 (D) with the human annotated ratings on a held-out set of 1000 words. The three trained models were then used to infer VAD measurements for the entire set of definitions (sentence embeddings). Table 4 presen"
2020.coling-main.454,N15-1099,0,0.0116629,"canonical form; as an example, the cumulative counts of ‘pick a fight’ include instances of ‘pick a fight’, ‘picked a fight’, ‘picks a fight’, etc. 3.2.2 Filtering Out Expressions with a Common Literal Reading Because our goal is to analyze idiomatic language use, we aim to automatically remove from our list of idioms those with a high degree of literality – i.e., a high potential for literal interpretation.7 Relevant work has focused on the related property of compositionality – the extent to which an idiomatic interpretation is formed from the component words (e.g., McCarthy et al. (2007), Salehi et al. (2015)). While this property is related to literality (Spearman’s ρ=0.62 in human ratings, Nordmann et al. (2014)) it is not identical, since highly decomposable idioms may not actually occur frequently in their literal meaning. However, because the method of Salehi et al. (2015) uses word embeddings to detect compositionality (comparing the embedding for the entire idiom to the embeddings for its component words), it is a suitable approximation to literality: i.e., the frequent use of the idiomatic form in its literal meaning entails that the semantic representation of the idiom will be closer to t"
2020.nlpcovid19-acl.13,L18-1269,0,0.0125845,"ngs assigned to individual words for supervision in extracting ratings of sentences. We use the model introduced by Reimers and Gurevych (2019) for producing word- and sentence-embeddings using Siamese BERT-Networks,6 thereby obtaining semantic representations for the 20, 000 words in Mohammad (2018) as well as for sentences in our datasets. This model performs significantly better than alternatives (such as averaging over a sentence’s individual word embeddings and using BERT encoding (Reimers and Gurevych, 2019)) on the SentEval toolkit, a popular evaluation toolkit for sentence embeddings (Conneau and Kiela, 2018). Next, we trained beta regression models7 (Zeileis et al., 2010) to predict VAD scores (dependent variables) of words from their embeddings (independent predictors), yielding Pearson’s correlations of 0.85, 0.78, and 0.81 on a 1000-word held-out set for V, A, and D, respectively. The trained models were then used to infer VAD values for each sentence within a post using the sentence embeddings.8 A post’s final score was computed as the average of the predicted scores for each of its constituent sentences. As an example, the post ‘most countries handled the covid-19 situation appropriately’ wa"
2020.nlpcovid19-acl.13,P18-1017,0,0.0580771,"address the above gaps in the literature, by performing a comprehensive analysis of the similarities and differences between male and female language collected from the Reddit discussion platform. Our main corpus is a large collection of spontaneous COVID-related utterances by (selfreported) M and F authors. Importantly, we also collect productions on a wide variety of topics by the same set of authors as a ‘baseline’ dataset. First, using a multidimensional affective framework from psychology (Bradley and Lang, 1994), we draw on a recently-released dataset of human affective ratings of words Mohammad (2018) to support the emotional assessment of male and female posts in our datasets. Through this approach, we corroborate existing assumptions on differences in the emotional aspects of linguistic productions of men and women in the COVID corpus. Moreover, our use of a baseline dataset enables us to further show that these distinctions are amplified in the emotionallyintensive setting of COVID discussions compared to productions on other topics. Second, we take a topic modeling approach to demonstrate detectable distinctions in the range of topics discussed by the two genders in our COVID corpus, r"
2020.nlpcovid19-acl.13,N10-1012,0,0.0349764,"topic modelling over each of the M and F subcorpora separately, and (2) comparison of the distribution of dominant topics in M vs. F posts as derived from a topic model over the entire M+F dataset. For each analysis, we used a publicly-available topic modeling tool (MALLET, McCallum, 2002). Each topic is represented by a probability distribution over the entire vocabulary, where terms more characteristic of a topic are assigned a higher probability.9 A common way to evaluate a topic learned from a set of documents is by computing its coherence score – a measure reflecting its overall quality (Newman et al., 2010). We assess the quality of a learned model by averaging the scores of its individual topics – the model coherence score. Analysis of Cross-gender Topics. Here we explore topical aspects of the productions of the two genders by comparing two topic models: one created using M posts, and another using F posts, in the COVID dataset. We selected the optimal number of topics for each set of posts by maximizing its model coherence score, resulting in 8 topics 9 Prior to topic modeling we applied a preprocessing step including lemmatization of a post’s text and filtering out stopwords (the 300 most fr"
2020.nlpcovid19-acl.13,D19-1410,0,0.0120845,"ple sentences), rather than individual words; we do so by inferring the affective ratings of sentences using those of individual words, as follows. Word embedding spaces have been shown to capture variability in emotional dimensions closely corresponding to valence, arousal, and dominance (Hollis and Westbury, 2016), implying that such semantic representations carry over information useful for the task of emotional affect assessment. Therefore, we exploit affective dimension ratings assigned to individual words for supervision in extracting ratings of sentences. We use the model introduced by Reimers and Gurevych (2019) for producing word- and sentence-embeddings using Siamese BERT-Networks,6 thereby obtaining semantic representations for the 20, 000 words in Mohammad (2018) as well as for sentences in our datasets. This model performs significantly better than alternatives (such as averaging over a sentence’s individual word embeddings and using BERT encoding (Reimers and Gurevych, 2019)) on the SentEval toolkit, a popular evaluation toolkit for sentence embeddings (Conneau and Kiela, 2018). Next, we trained beta regression models7 (Zeileis et al., 2010) to predict VAD scores (dependent variables) of words"
2021.findings-acl.360,D13-1169,0,0.0282121,"Missing"
2021.findings-acl.360,S17-1017,0,0.0234917,"luding, e.g., Chinese (NATIONALITY) or pulmonary (BODY- PART). Much work shows the ability of DSMs to match human knowledge of semantic properties within a domain (e.g., Baroni et al., 2014; Pereira et al., 2016; An et al., 2018; Grand et al., 2018), but there is little work, to our knowledge, on whether the similarity structure of a DSM is sensitive to commonalities of abstract properties that hold across a variety of semantic domains.2 Research on vector-based representations of analogy suggests that DSMs may be limited in their ability to represent crossdomain word relations: Rogers et al. (2017) show that cross-domain analogical relations like hypernymy (e.g., turtle:reptile::salmon:fish) are significantly harder to solve than within-domain ones (e.g., Paris:France::Ottawa:Canada). Lu et al. (2019) make significant progress towards representing such relations, showing that a DSM can form the basis for detecting the cross-domain word 1 The same distinction between schematic and content is applied in Paradis (2001); Cruse and Togia (1996). It is also worth noting explicitly that our use of the term ‘abstract’ in this sense is not to be interpreted as ‘not concrete’. 2 DSMs may, e.g., e"
2021.findings-acl.360,P15-2119,0,0.0308278,"ng in adverbs, demonstrating, in a separate task, the effectiveness of detecting this abstract semantic property. 1 Distributional Models and Abstract Semantic Classes Distributional semantic models (DSMs) are widely used as representations of word-level semantics. However, open questions remain as to precisely which aspects of human semantic knowledge DSMs effectively capture (e.g., Baroni et al., 2014; Hollis and Westbury, 2016; Schnabel et al., 2015; Utsumi, 2020). For example, popular DSMs such as word2vec and GloVe have been shown to predict human ratings of semantic features of objects (Rubinstein et al., 2015; Grand et al., 2018). However, performance is variable across features and object categories (Grand et al., 2018), and in particular, is better for taxonomic properties (‘is an animal’, ‘is a weapon’) than for general attributive properties (‘is yellow’, ‘is dangerous’) (Rubinstein et al., 2015). While people may or may not have semantic categories such as “all yellow things”, abstract semantic classes are an important part of human linguistic knowledge that should be captured in a computational system. Note that by abstract we mean the schematic properties of word meaning, rather than the co"
2021.findings-acl.360,E14-4023,0,0.161253,"his, their method requires additional learning of a relation-specific warping of the distributional semantic space. Our goal here is to see whether the similarity structure of the (original) DSM itself directly captures such cross-domain knowledge. We focus as a test case on the abstract class of extreme adjectives: scalar adjectives that express an extreme value of their scale, such as brilliant and freezing. Previous computational work on scalar adjectives has focused on assessing their relative ranking within a domain (e.g., learning that smart &lt; brilliant on the INTELLIGENCE scale) (e.g., Ruppenhofer et al., 2014; Cocos et al., 2018). However, as work in linguistics shows (Cruse, 1986; Paradis, 2001; Morzycki, 2012), extreme adjectives do not simply behave as if they are further along their scale, but rather (as a class) have distinguishing semantic properties. Our goal is to see whether DSMs can capture this cross-domain property of “extremeness”. In our first experiment, we demonstrate that we can successfully identify extreme adjectives, across a wide range of domains, on the basis of the information contained in their DSM representations alone. Our next goal was to show that this ability to detect"
2021.findings-acl.360,D15-1036,0,0.0252334,"s-domain property like extremeness can be captured in a word’s DSM representation. We then use the extremeness classifier to model the emergence of intensifier meaning in adverbs, demonstrating, in a separate task, the effectiveness of detecting this abstract semantic property. 1 Distributional Models and Abstract Semantic Classes Distributional semantic models (DSMs) are widely used as representations of word-level semantics. However, open questions remain as to precisely which aspects of human semantic knowledge DSMs effectively capture (e.g., Baroni et al., 2014; Hollis and Westbury, 2016; Schnabel et al., 2015; Utsumi, 2020). For example, popular DSMs such as word2vec and GloVe have been shown to predict human ratings of semantic features of objects (Rubinstein et al., 2015; Grand et al., 2018). However, performance is variable across features and object categories (Grand et al., 2018), and in particular, is better for taxonomic properties (‘is an animal’, ‘is a weapon’) than for general attributive properties (‘is yellow’, ‘is dangerous’) (Rubinstein et al., 2015). While people may or may not have semantic categories such as “all yellow things”, abstract semantic classes are an important part of h"
2021.findings-acl.360,D17-1058,0,0.0182182,"improvement over a more standard application of DSMs that assesses in-domain similarities. This pair of experiments thus provides evidence that a DSM can successfully capture an abstract class defined by cross-domain similarity.5 3 Classifying Extreme Adjectives Here we propose an approach for identifying extreme adjectives using a DSM. Other work in computational linguistics has considered automatic means for placing scalar adjectives in the appropriate relative position along their scale (e.g. Sheinman et al., 2013), including approaches using DSMs to do so (e.g. Kim and de Marneffe, 2013; Sharma et al., 2017). Such methods do not distinguish extreme adjectives as a special set across 5 All code and data are available https://github.com/smfsamir/ detect-adjectival-extremeness. at Repulsive (extreme) NN Unattractive (non-extreme) NN uninteresting 200 100 uncompetitive alluring unfeminineattractive undesirable 0 hideous repugnant reprehensible detestable revolting depraved 100 200 200 100 0 100 200 Figure 1: Nearest neighbours of repulsive (extreme) and unattractive (non-extreme), visualized with t-SNE (Maaten and Hinton, 2008) on word2vec embeddings (Mikolov et al., 2013). semantic domains, but rath"
2021.findings-acl.360,L16-1424,0,0.0160616,"ification will be successful to the extent that vectors of adjectives that share the abstract property of extremeness are sufficiently similar to serve as an informative prototype. 3.1 Precision 0.9 Dataset 0.8 0.7 0.6 We collect a dataset of extreme and non-extreme adjectives in English for training and evaluating a supervised classifier. We started with a set of 54 adjectives identified as extreme (Morzycki, 2012; Paradis, 2001; Huttenlocher et al., 1971; Cruse, 1986; Lassiter, 2017). We then added extreme adjectives from human-annotated datasets of adjectival intensity (Cocos et al., 2018; Wilkinson and Tim, 2016; de Melo and Bansal, 2013; Ruppenhofer et al., 2014). For each adjective, these datasets specify its scale and its human-rated range of intensity values. For each of the scales of the 54 previously-identified extreme adjectives, we gathered all further adjectives tied with or ranked above extreme adjectives in their intensity value (N = 17). After filtering out 3 adjectives with frequency less than 0.5 per million, we obtained a total of N = 68 extreme adjectives that cover a diverse set of adjectival scales, such as D ESIR ABILITY (sensational), I NTELLIGENCE (moronic), and SIZE (gigantic)."
C00-2118,J93-2002,0,0.0711494,"Missing"
C00-2118,A97-1052,0,0.125704,"Missing"
C00-2118,P97-1003,0,0.0370437,"e (ripped o ) in the corpus of the causative, and borne out in our corpus analysis. from the intended change of state usage. passive or active use (pass), in a past participle or simple past use (vbn), in a causative or noncausative use (caus), and with an animate subject or not (anim), as described below. The rst three counts (trans, pass, vbn) were performed on the LDC&apos;s 65-million word tagged ACL/DCI corpus (Brown, and Wall Street Journal 1987{1989). The last two counts (caus and anim) were performed on a 29-million word parsed corpus (Wall Street Journal 1988, provided by Michael Collins (Collins, 1997)). The features were counted as follows: trans: The closest noun following a verb was considered a potential object. A verb immediately followed by a potential object was counted as transitive, otherwise as intransitive. pass: A token tagged VBD (the tag for simple past) was counted as active. A token tagged VBN (the tag for past participle) was counted as active if the closest preceding auxiliary was have , and as passive if the closest preceding auxiliary was be . vbn: The counts for VBN/VBD were based on the POS label in the tagged corpus. Each of the above counts was normalized over all oc"
C00-2118,P98-1046,0,0.062086,"n the explicit goal, the text has been semantically annotated (Webster and Marcus, 1989), or external semantic resources have been consulted (Aone and McKee, 1996). We extend these results by showing that thematic information can be induced from corpus counts. The experimental results show that our method is powerful, and suited to the classi cation of lexical items. However, we have not yet addressed the problem of verbs that can have multiple classi cations. We think that many cases of ambiguous classi cation of verb types can be addressed with the notion of intersective sets introduced by (Dang et al., 1998). This is an important concept that proposes that egular&quot; ambiguity in classi cation|i.e., sets of verbs that have the same multi-way classi cations according to (Levin, 1993)|can be captured with a nergrained notion of lexical semantic classes. Extending our work to exploit this idea requires only to de ne the classes appropriately; the basic approach will remain the same. When we turn to consider ambiguity, we must also address the problem that individual instances of verbs may come from di erent classes. In future research we plan to extend our method to the classi cation of ambiguous tok"
C00-2118,C96-1055,0,0.0956638,"accuracy in our classi cation is due to argument structure information, as subcategorization is the same for all verbs, con rming that the content of thematic roles is crucial for classi cation. Secondly, our results further support the assumption that thematic di erences such as these are apparent not only in di erences in subcategorization frames, but also in di erences in their frequencies. We thus join the many recent results that all seem to converge in supporting the view that the relation between lexical syntax and semantics can be usefully exploited (Aone and McKee, 1996; Dorr, 1997; Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Siegel, 1998), especially in a statistical framework. Finally, we observe that this information is detectable in a corpus and can be learned automatically. Thus we view corpora, especially if annotated with currently available tools, as useful repositories of implicit grammars. Technically, our approach extends existing corpus-based learning techniques to a more complex learning problem, in several dimensions. Our statistical approach, which does not require explicit negative examples, extends approaches that encode Levin&apos;s alternations directly"
C00-2118,W99-0632,0,0.466322,"1 Introduction Detailed information about verbs is critical to a broad range of NLP and IR tasks, yet its manual determination for large numbers of verbs is dicult and resource intensive. Research on the automatic acquisition of verb-based knowledge has succeded in gleaning syntactic properties of verbs such as subcategorization frames from online resources (Brent, 1993; Briscoe and Carroll, 1997; Dorr, 1997; Manning, 1993). Recently, researchers have investigated statistical corpusbased methods for lexical semantic classi cation from syntactic properties of verb usage (Aone and McKee, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; McCarthy, 2000). Corpus-based approaches to lexical semantic classi cation in particular have drawn on Levin&apos;s hypothesis (Levin, 1993) that verbs can be classi ed according to the diathesis alternations (alternations in the syntactic expressions of arguments) in which they participate|for example, whether a This research was partly sponsored by US NSF grants #9702331 and #9818322, Swiss NSF fellowship 821046569, Information Sciences Council of Rutgers University and IRCS, U. of Pennsylvania. This research was conduct"
C00-2118,merlo-stevenson-2000-establishing,1,0.795113,"Missing"
C00-2118,E99-1007,1,0.833312,"is critical to a broad range of NLP and IR tasks, yet its manual determination for large numbers of verbs is dicult and resource intensive. Research on the automatic acquisition of verb-based knowledge has succeded in gleaning syntactic properties of verbs such as subcategorization frames from online resources (Brent, 1993; Briscoe and Carroll, 1997; Dorr, 1997; Manning, 1993). Recently, researchers have investigated statistical corpusbased methods for lexical semantic classi cation from syntactic properties of verb usage (Aone and McKee, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; McCarthy, 2000). Corpus-based approaches to lexical semantic classi cation in particular have drawn on Levin&apos;s hypothesis (Levin, 1993) that verbs can be classi ed according to the diathesis alternations (alternations in the syntactic expressions of arguments) in which they participate|for example, whether a This research was partly sponsored by US NSF grants #9702331 and #9818322, Swiss NSF fellowship 821046569, Information Sciences Council of Rutgers University and IRCS, U. of Pennsylvania. This research was conducted while the rst author was at Rutgers University."
C00-2118,W99-0503,1,0.525476,"ge of NLP and IR tasks, yet its manual determination for large numbers of verbs is dicult and resource intensive. Research on the automatic acquisition of verb-based knowledge has succeded in gleaning syntactic properties of verbs such as subcategorization frames from online resources (Brent, 1993; Briscoe and Carroll, 1997; Dorr, 1997; Manning, 1993). Recently, researchers have investigated statistical corpusbased methods for lexical semantic classi cation from syntactic properties of verb usage (Aone and McKee, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; McCarthy, 2000). Corpus-based approaches to lexical semantic classi cation in particular have drawn on Levin&apos;s hypothesis (Levin, 1993) that verbs can be classi ed according to the diathesis alternations (alternations in the syntactic expressions of arguments) in which they participate|for example, whether a This research was partly sponsored by US NSF grants #9702331 and #9818322, Swiss NSF fellowship 821046569, Information Sciences Council of Rutgers University and IRCS, U. of Pennsylvania. This research was conducted while the rst author was at Rutgers University.  Paola Merlo LATL { Dep"
C00-2118,P89-1022,0,0.0330042,"approach by learning argument structure properties, which, unlike grammatical functions, are not marked morphologically. Others have tackled the problem of lexical semantic classi cation, as we have, but using only subcategorization frequencies as input data (Lapata and Brew, 1999; Schulte im Walde, 1998). By contrast, we explicitly address the de nition of features that can tap directly into thematic role differences that are not re ected in subcategorization distinctions. Finally, when learning of thematic role assignment has been the explicit goal, the text has been semantically annotated (Webster and Marcus, 1989), or external semantic resources have been consulted (Aone and McKee, 1996). We extend these results by showing that thematic information can be induced from corpus counts. The experimental results show that our method is powerful, and suited to the classi cation of lexical items. However, we have not yet addressed the problem of verbs that can have multiple classi cations. We think that many cases of ambiguous classi cation of verb types can be addressed with the notion of intersective sets introduced by (Dang et al., 1998). This is an important concept that proposes that egular&quot; ambiguity"
C00-2118,A00-2034,0,\N,Missing
C00-2118,C98-1046,0,\N,Missing
C00-2118,P93-1032,0,\N,Missing
C02-1146,H01-1035,0,\N,Missing
C02-1146,C00-2108,0,\N,Missing
C02-1146,J94-4003,0,\N,Missing
C02-1146,J95-4004,0,\N,Missing
C02-1146,P98-2247,0,\N,Missing
C02-1146,C98-2242,0,\N,Missing
C02-1146,J01-3003,1,\N,Missing
C02-1146,W99-0632,0,\N,Missing
C02-1146,W01-0705,1,\N,Missing
cook-stevenson-2010-automatically,W09-0214,0,\N,Missing
cook-stevenson-2010-automatically,J98-1004,0,\N,Missing
cook-stevenson-2010-automatically,D09-1062,0,\N,Missing
cook-stevenson-2010-automatically,D09-1063,0,\N,Missing
cook-stevenson-2010-automatically,P97-1023,0,\N,Missing
cook-stevenson-2010-automatically,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
D14-1031,J06-1003,0,0.00863629,"form S(w) in Algorithm 1 by selecting a given number of words ns whose tokens are probabilistically chosen from the clusters according to how likely each cluster k is given w: the number of word tokens picked from each k is proportional to P (k|F ) and is equal to P (k|F ) × ns . 4 = Given the gold-standard similarity scores for each word pair, we evaluate the semantic connectivity of the network based on two performance measures: coefficient of correlation and the median rank of the first five gold-standard associates. Correlation is a standard way to compare two lists of similarity scores (Budanitsky and Hirst, 2006). We create two lists, one containing the gold-standard similarity scores for all word pairs, and the other containing their corresponding learned similarity scores. We calculate the Spearman’s rank correlation coefficient, ρ, between these two lists of similarity scores. Note that the learned similarity scores reflect the semantic distance among words whereas the WordNet scores reflect semantic closeness. Thus, a negative correlation is best in our evaluation, where the value of -1 corresponds to the maximum correlation. Evaluation We evaluate a semantic network in two regards: The semantic c"
D14-1031,P94-1019,0,\N,Missing
D15-1207,N01-1013,0,0.117785,"Missing"
D15-1207,P98-2124,0,0.227739,"Missing"
D15-1207,C98-2119,0,\N,Missing
D16-1010,W12-1701,1,0.843292,"and non-alternating (DOonly or PD-only) classes. The model of Parisien and Stevenson (2010) addresses this limitation by learning alternation classes from the data (including the dative), but it uses only syntactic slot features that can be gleaned automatically from a corpus. In addition, both models use batch processing, failing to address how learning to generalize across an alternation might be achieved incrementally. Alishahi and Stevenson (2008) presents an incremental Bayesian model shown to capture various aspects of verb argument structure acquisition (Alishahi and Pyykko¨nen, 2011; Barak et al., 2012, 2013b; Matusevych et al., 2016), but the model is unable to mimic alternation learning behaviour. Barak et al. (2014) extends this constructionlearning model to incrementally learn both constructions and classes of alternating verbs, and show the role of the classes in learning the dative. However, like Parisien and Stevenson (2010), the input to the model in this study is limited to syntactic properties, not allowing for a full analysis of the relevant factors that influence acquisition of alternations. Ambridge and Blything (2015) propose the first computational model of this phenomenon to"
D16-1010,W14-2005,1,0.482495,"and Brooks, 1995; Theakston, 2004; Ambridge et al., 2014). Recently, Ambridge and Blything (2015) propose a computational model designed to study the role of verb semantics and frequency in the acquisition of the dative alternation. However, they only evaluate their model preferences for one of the two constructions, which does not provide a full picture of the alternation behaviour; moreover, they incorporate certain assumptions about the input that may not match the properties of naturalistic data. In this paper, we compare the model of Ambridge and Blything (2015) to the Bayesian model of Barak et al. (2014) that offers a general framework of verb construction learning. We replicate the approach taken in Ambridge and Blything (2015) in order to provide appropriate comparisons, but we also extend the experimental settings and analysis to enable a more fulsome evaluation, on data with more naturalistic statistical properties. Our results show that the Bayesian model provides a better fit to the psycholinguistic data, which we suggest is due to its richer learning mechanism: its two-level clustering approach can exploit distributional properties of the input in a manner consistent with human general"
D16-1010,W13-3525,1,0.803916,"ng on the second-level (verb-class) clusters improves the results. Conclusions. The use of an artificial highfrequency non-dative construction (other), and the use of log frequencies, seem to mask the influence of the semantic and syntactic properties on learning the verb-bias for each verb. Previous psycholinguistic data and computational models have found that a skewed naturalistic distribution of the input is helpful in learning constructions, due to the high-frequency verbs establishing appropriate construction-meaning associations (Casenhiser and Goldberg, 2005; Borovsky and Elman, 2006; Barak et al., 2013b; Matusevych et al., 2014). To allow a more direct analysis of the role of statistical and semantic properties in learning and generalizing the dative, we adjust the input to the models in the next section. 5.2 Exp 2: Raw Freq Input; 2 Constructions Results. Here we perform the same type of experiments, but using input in proportion to the raw frequencies of the verbs (instead of log frequencies) over occurrences only in the two dative constructions (with no other construction). Since 98 of the 281 verbs do not occur with either dative construcAB (Connectionist) DO PD DO-PD [0.06] 0.33 0.39 B"
D19-1484,W18-3219,0,0.0350366,"Missing"
D19-1484,W18-3200,0,0.22418,"Missing"
D19-1484,W16-5801,0,0.0366793,"Missing"
D19-1484,W18-3211,0,0.0116396,"n forums (McLellan, 2005), English-Spanish bilingual blogging (Montes-Alcal´a, 2007), and English-Chinese instant messaging (Lam, 2009). However, these studies have analyzed productions of a small number of authors (even a single person), and each is restricted to a single language-pair. Most large-scale computational work on CS in social media has focused on essential prerequisites for various NLP tasks, such as POS tagging (Solorio and Liu, 2008; Soto et al., 2018), tokenlevel language identification (Soto et al., 2018; Rijhwani et al., 2017), NER (Aguilar et al., 2018a), language modeling (Chandu et al., 2018), automatic speech recognition (Yılmaz et al., 2018), Mean parse tree depth (tree depth) – the average depth of a parse tree of a sentence, using the constituency parser of Kitaev and Klein (2018). Mean number of clauses (num of clauses) – the average number of clauses in a sentence. Figure 2 presents these metrics for high and low code-switchers. We find that high codeswitchers exhibit statistically significant lower proficiency in the lexical metrics (with the exception of word concreteness level), but statistically significant higher proficiency for two of the grammatical metrics. Although"
D19-1484,1993.eamt-1.1,0,0.65454,"stic proficiency metrics (mean and standard error; lexical on the left, grammatical on the right) of high and low code-switchers. ‘*’ indicates significant difference in a Wilcoxon rank-sum test at the level of p &lt; .05. length in characters of a token. 4 Mean sentence length (sent. length) – the average length of a sentence (in tokens). Many studies have elucidated the communicative functions of oral code-switching. These include conveying nuanced attitudes through linguistic choices reflecting emotion and sentiment (Dewaele and Nakano, 2013), as well as establishing a level of (in)formality (Fishman, 1970; Genesee, 1982; Myers-Scotton, 1995). Other studies have focused more on the characteristics of code-switchers themselves: While CS has mostly been viewed as a deliberate choice of a proficient bilingual speaker, requiring adept control of two simultaneously-activated linguistic systems (Costa and Santesteban, 2004; Kootstra et al., 2012), mixing two languages may also serve as a strategy for bridging lexical inefficacy in a second language (Grosjean, 1982; Faerch and Kasper, 1983; Poulisse, 1989). As noted earlier, written CS is much less studied and the extent to which these findings hold a"
D19-1484,P19-1340,0,0.0649175,"Missing"
D19-1484,P11-1132,0,0.0279459,"Missing"
D19-1484,N10-1012,0,0.028047,"glish words; we removed all words ranked higher than 10, 000 in the wordrank list built from the English Wikipedia corpus.9 We used a publicly-available topic modeling tool (MALLET, McCallum, 2002) to identify the prevalent discussion topics in the CS and monolingual messages. Each topic is represented by a probability distribution over the entire vocabulary, where terms more characteristic of a topic are assigned a higher probability. A common way to evaluate a topic learned from a set of documents is by computing its coherence score – a statistical measure reflecting the quality of a topic (Newman et al., 2010). The quality of a learned model is then estimated by averaging the scores of its individual topics – the model coherence score. We selected the optimal number of topics for each set of posts by maximizing its model coherence score, resulting in 17 topics for CS posts (score of 0.58), and 21 topics for monolingual posts (score of 0.60). Qualitative analysis: We first examine similarities and differences across the two topical distributions by extracting the top 5 topics – those with the 9 https://dumps.wikimedia.org/enwiki highest individual coherence scores – in each of the code-switched and"
D19-1484,Q18-1024,1,0.893614,"Missing"
D19-1484,N18-1012,0,0.0322351,"es compared to monolingual texts in our datasets. 3.2 (In)formality of code-switched content Oral code-switching has been shown to be a marker of informality (Genesee, 1982; MyersScotton, 1995). While user-generated content on the Reddit discussion platform is by and large considered informal, in this section we test whether (presumably subtle) formality differences can be observed in CS posts compared to monolingual ones. In accord with findings in spoken language, we expect more informal context to stimulate more frequent language mixing. We use the formal-informal GYAFC parallel dataset by Rao and Tetreault (2018) to extract a set of markers indicative of informal style. Originally collected from the Yahoo Answers platform, the corpus comprises over 50K sentence-pairs in the domain of Entertainment&Music and Family&Relationships, where each original informal sentence is paired with a manually generated formal counterpart. The parallel nature of the corpus facilitates extraction of a clean set of stylistic markers characteristic of informal text, while abstracting away from content. We use the log-odds Entahlah, I’m self diagnosing, but maybe I’m on the spiral to depression... [ I’ve no idea, I’m self d"
D19-1484,P17-1180,0,0.0133179,"le in email (Hinrichs, 2006), Malay-English language in online discussion forums (McLellan, 2005), English-Spanish bilingual blogging (Montes-Alcal´a, 2007), and English-Chinese instant messaging (Lam, 2009). However, these studies have analyzed productions of a small number of authors (even a single person), and each is restricted to a single language-pair. Most large-scale computational work on CS in social media has focused on essential prerequisites for various NLP tasks, such as POS tagging (Solorio and Liu, 2008; Soto et al., 2018), tokenlevel language identification (Soto et al., 2018; Rijhwani et al., 2017), NER (Aguilar et al., 2018a), language modeling (Chandu et al., 2018), automatic speech recognition (Yılmaz et al., 2018), Mean parse tree depth (tree depth) – the average depth of a parse tree of a sentence, using the constituency parser of Kitaev and Klein (2018). Mean number of clauses (num of clauses) – the average number of clauses in a sentence. Figure 2 presents these metrics for high and low code-switchers. We find that high codeswitchers exhibit statistically significant lower proficiency in the lexical metrics (with the exception of word concreteness level), but statistically signif"
D19-1484,D16-1121,0,0.0213136,"cal capabilities in order to construct mixed sentences without distorting the ‘grammaticality’ of the target utterance. These results both demonstrate the value of studying code-switching at scale, and leave room for much further investigation. Our future plans include breaking down the data by individual native languages of the authors, in order to capture differences in proficiency stemming from various L1 backgrounds. 4783 Related Work etc. Almost no work has addressed theoretical and sociolinguistic aspects of written multilingual discourse computationally, at scale (for an exception, see Rudra et al., 2016). Our work here addresses various gaps noted in the research above. We create (and make available) a novel dataset that enables investigation of research questions on written code-switching at scale, across multiple language pairs and a large number of speakers. Moreover, we use this dataset for computational, large-scale investigation of subtle issues of linguistic content and style, as well as speaker proficiency, that have been extensively studied in the context of spoken language, and enjoyed very limited attention in written communication. References 5 B. Bullock and A. Toribio. 2009. The"
D19-1484,D08-1102,0,0.0206418,"linguistic perspective has considered CS in an array of online genres: e.g., English-Jamaican Creole in email (Hinrichs, 2006), Malay-English language in online discussion forums (McLellan, 2005), English-Spanish bilingual blogging (Montes-Alcal´a, 2007), and English-Chinese instant messaging (Lam, 2009). However, these studies have analyzed productions of a small number of authors (even a single person), and each is restricted to a single language-pair. Most large-scale computational work on CS in social media has focused on essential prerequisites for various NLP tasks, such as POS tagging (Solorio and Liu, 2008; Soto et al., 2018), tokenlevel language identification (Soto et al., 2018; Rijhwani et al., 2017), NER (Aguilar et al., 2018a), language modeling (Chandu et al., 2018), automatic speech recognition (Yılmaz et al., 2018), Mean parse tree depth (tree depth) – the average depth of a parse tree of a sentence, using the constituency parser of Kitaev and Klein (2018). Mean number of clauses (num of clauses) – the average number of clauses in a sentence. Figure 2 presents these metrics for high and low code-switchers. We find that high codeswitchers exhibit statistically significant lower proficien"
E03-1040,W99-0632,0,0.0516803,"ask. 1 Introduction Wide-coverage language processing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001) showed that verbs could be automatically classified into one of three lexical semantic classes on the basis of five simple statistical features. This work demonstrated the feasibility of verb classification from noisy, easily extractable corpus statistics. However, the approach was limited in scope by the need for manual determination of a discriminating set of features, t"
E03-1040,A00-2034,0,0.342342,"oss a wider range of verb class variability. Specifically, we drew on Part I of Levin, which elaborates the alternations she uses to characterize verb classes, in contrast to Part II, which enumerates the classes themselves. The feature space is summarized in Table 1, and described in detail in the rest of this section. All frequency counts are normalized by the total number of occurrences of the verb (or, in some cases, a relevant subset). 2.1 Features over Syntactic Slots Syntactic alternations play a central role in defining classes, but are difficult to detect automatically from a corpus (McCarthy, 2000). We have devised several types of features that tap into different aspects of alternation behaviour. One set of features encodes the frequency of the syntactic slots containing verbal arguments in Levin's classes (subject, direct and indirect object, and prepositional phrases), thus approximating the allowable syntac§ in Text 2.1 2.2 2.3 Feature Category #Features Syntactic slots Slot overlap ""Empty"" words Passive POS of the verb Aux, modal, Adv Derived forms Animacy of NPs 76 40 4 2 6 13 3 76 Table 1: Feature categories with number of features of each type. tic frames for a verb. 2 For prepo"
E03-1040,J01-3003,1,0.953197,"essing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001) showed that verbs could be automatically classified into one of three lexical semantic classes on the basis of five simple statistical features. This work demonstrated the feasibility of verb classification from noisy, easily extractable corpus statistics. However, the approach was limited in scope by the need for manual determination of a discriminating set of features, through a linguistic analysis of the target verb clas"
E03-1040,P02-1027,1,0.864961,"ernations. We also believe it will be straightforward to extend the feature space to cover additional alternations 169 not in the scope of Levin (such as those involving sentential arguments), through the same process of generalizing the existing features to new slots. Still, our investigation has been limited to English verb classes, and our feature space is limited in being motivated by alternations in English. It thus lacks grammatical features (e.g., Case or other rich morphological properties) used in other languages to mark arguments or indicate the relation between them. Interestingly, Merlo et al. (2002) showed that some of the hand-crafted features of MS01 were useful in Italian for classifying the same verb classes in that language. We think a more general feature space such as ours will have even more potential for crosslinguistic applicability: We have devised a mapping of Levin's analysis of the variation in expression of arguments across classes in English to a general set of features. To the extent that Levin's analysis is grounded in general principles concerning the linking of semantic arguments to their syntactic expression, our feature space is an initial step in capturing variatio"
E03-1040,W98-1106,0,0.0452233,"classification tasks. Our results show that the approach is generally applicable, and avoids the need for resource-intensive linguistic analysis for each new task. 1 Introduction Wide-coverage language processing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001) showed that verbs could be automatically classified into one of three lexical semantic classes on the basis of five simple statistical features. This work demonstrated the feasibility of verb classification from noisy, easily"
E03-1040,J93-2002,0,0.0607408,"nce to feature sets manually selected for the particular classification tasks. Our results show that the approach is generally applicable, and avoids the need for resource-intensive linguistic analysis for each new task. 1 Introduction Wide-coverage language processing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001) showed that verbs could be automatically classified into one of three lexical semantic classes on the basis of five simple statistical features. This"
E03-1040,A97-1052,0,0.0660813,"e sets manually selected for the particular classification tasks. Our results show that the approach is generally applicable, and avoids the need for resource-intensive linguistic analysis for each new task. 1 Introduction Wide-coverage language processing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001) showed that verbs could be automatically classified into one of three lexical semantic classes on the basis of five simple statistical features. This work demonstrated the feasi"
E03-1040,C96-1055,0,0.0993186,"nalysis for each new task. 1 Introduction Wide-coverage language processing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001) showed that verbs could be automatically classified into one of three lexical semantic classes on the basis of five simple statistical features. This work demonstrated the feasibility of verb classification from noisy, easily extractable corpus statistics. However, the approach was limited in scope by the need for manual determination of a discrimina"
E03-1040,C00-2108,0,0.113859,"language processing systems require large amounts of knowledge about individual words, leading to a lexical acquisition bottleneck. Because verbs play a central role in the syntactic and semantic interpretation of a sentence, much research has focused on automatically learning properties of verbs from text corpora, such as their subcategorization (Brent, 1993; Briscoe and Carroll, 1997), argument roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001) showed that verbs could be automatically classified into one of three lexical semantic classes on the basis of five simple statistical features. This work demonstrated the feasibility of verb classification from noisy, easily extractable corpus statistics. However, the approach was limited in scope by the need for manual determination of a discriminating set of features, through a linguistic anal"
E03-1040,P02-1029,0,0.0644243,"Missing"
E03-1040,E99-1007,1,0.889135,"hrases, we have a separate feature for each of 51 high frequency prepositions, as well as 19 groups of closely related prepositions (e.g., between, in between, among, amongst, amid, and amidst form one group). We also count verb uses where any prepositional phrase occurs. Using syntactic frame information alone misses critical properties of alternations, since two verbs may occur in the same frames but undergo different mappings of their arguments to the positions. Earlier work has used a measure of overlap over nouns in syntactic slots as a noisy indicator of participation in an alternation (Stevenson and Merlo, 1999; McCarthy, 2000). The idea is that since the same semantic argument may occur in two different slots in a pair of alternating frames, the degree to which those two slots contain the same entities is an indication of the verb's participation in the alternation. In our feature space, we consider the overlap between each pair of slots that corresponds to an alternation in Part I of Levin (i.e., §1-8). For each alternation in which a semantic argument occurs in one slot in one usage of the verb (such as subject in The sky cleared), and in a different slot in the alternant usage (such as object of"
E03-1040,W04-3213,1,\N,Missing
E03-1040,W99-0503,1,\N,Missing
E03-1040,W03-0410,1,\N,Missing
E03-1040,C02-1132,0,\N,Missing
E03-1040,C02-1040,0,\N,Missing
E03-1040,W02-0907,0,\N,Missing
E03-1040,E03-1037,0,\N,Missing
E03-1040,W04-2606,0,\N,Missing
E03-1040,P98-1013,0,\N,Missing
E03-1040,C98-1013,0,\N,Missing
E03-1040,P99-1014,0,\N,Missing
E03-1040,J93-1002,0,\N,Missing
E03-1040,J04-1003,0,\N,Missing
E03-1040,J02-3001,0,\N,Missing
E03-1040,W04-2411,1,\N,Missing
E06-1043,W03-1812,0,0.746614,"of syntactic variations that are tolerated by a VNIC and that should be included in its lexical representation. Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality. Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from l"
E06-1043,W03-1809,0,0.30522,"ns that are tolerated by a VNIC and that should be included in its lexical representation. Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality. Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from literal (compositional)"
E06-1043,P94-1038,0,0.0672566,"Missing"
E06-1043,Y05-1003,1,0.806975,"Missing"
E06-1043,P98-2127,0,0.0869181,"Missing"
E06-1043,P99-1041,0,0.9432,"its constituents by a semantically (and syntactically) similar word generally does not result in another VNIC, but in an invalid or a literal expression. One way of measuring lexical fixedness of a given verb+noun combination is thus to examine the idiomaticity of its variants, i.e., expressions generated by replacing one of the constituents by a similar word. This approach has two main challenges: (i) it requires prior knowledge about the idiomaticity of expressions (which is what we are developing our measure to determine); (ii) it needs information on “similarity” among words. Inspired by Lin (1999), we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity. We use the automatically-built thesaurus of Lin (1998) to find similar words to the noun of the target expression, in order to automatically generate variants. Only the noun constituent is varied, since replacing the verb constituent of a VNIC with a semantically related verb is more likely to yield another VNIC, as in keep/lose one’s cool (Nunberg et al., 1994).     be the set Let  of the most similar"
E06-1043,W03-1810,0,0.0820497,"lly discovering the set of syntactic variations that are tolerated by a VNIC and that should be included in its lexical representation. Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality. Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that d"
E06-1043,J93-1007,0,0.674593,"about the lexicosyntactic behaviour of idiomatic combinations. We put forward a means for automatically discovering the set of syntactic variations that are tolerated by a VNIC and that should be included in its lexical representation. Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality. O"
E06-1043,H05-1113,0,0.274127,"omaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality. Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from literal (compositional) combinations. Moreover, we suggest novel techniques for translating such characteristics into measures that predict the idiomaticity level of verb+noun combinations. More specifically, we propose sta"
E06-1043,W04-0411,0,0.0327517,"ions (MWEs). Nonetheless, most research on the topic has focused on compound nouns and verb particle constructions. Earlier work on idioms have only touched the surface of the problem, failing to propose explicit mechanisms for appropriately handling them. Here, we provide effective mechanisms for the treatment of a broadly documented and crosslinguistically frequent class of idioms, i.e., VNICs. Earlier research on the lexical encoding of idioms mainly relied on the existence of human annotations, especially for detecting which syntactic variations (e.g., passivization) an idiom can undergo (Villavicencio et al., 2004). We propose techniques for the automatic acquisition and encoding of knowledge about the lexicosyntactic behaviour of idiomatic combinations. We put forward a means for automatically discovering the set of syntactic variations that are tolerated by a VNIC and that should be included in its lexical representation. Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Mos"
E06-1043,H05-1106,0,0.0357151,"at should be included in its lexical representation. Moreover, we incorporate such information into statistical measures that effectively predict the idiomaticity level of a given expression. In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms. Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality. Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from literal (compositional) combinations. Moreover, we suggest novel"
E06-1043,W05-1006,0,0.0866036,"tistical measures that quantify the degree of lexical, syntactic, and overall fixedness of such combinations. We demonstrate 343 that these measures can be successfully applied to the task of automatically distinguishing idiomatic combinations from non-idiomatic ones. We also show that our syntactic and overall fixedness measures substantially outperform a widely used mea(*),+ , even when the latter sure of collocation, takes syntactic relations into account. Others have also drawn on the notion of syntactic fixedness for idiom detection, though specific to a highly constrained type of idiom (Widdows and Dorow, 2005). Our syntactic fixedness measure looks into a broader set of patterns associated with a large class of idiomatic expressions. Moreover, our approach is general and can be easily extended to other idiomatic combinations. Each measure we use to identify VNICs cap(I)J+ tures a different aspect of idiomaticity: reflects the statistical idiosyncrasy of VNICs, while the fixedness measures draw on their lexicosyntactic peculiarities. Our ongoing work focuses on combining these measures to distinguish VNICs from other idiosyncratic verb+noun combinations that are neither purely idiomatic nor complete"
E06-1043,J03-4003,0,\N,Missing
E06-1043,C98-2122,0,\N,Missing
E99-1007,P97-1003,0,0.0747996,"Missing"
E99-1007,P98-1046,0,0.157667,"Missing"
E99-1007,C96-1055,0,0.358533,"ave witnessed a shift in grammar development methodology, from crafting large grammars, to annotation of corpora. Correspondingly, there has been a change from developing rule-based parsers to developing statistical methods for inducing grammatical knowledge from annotated corpus data. The shift has mostly occurred because building wide-coverage grammars is time-consuming, error prone, and difficult. The same can be said for crafting the rich lexical representations that are a central component of linguistic knowledge, and research in automatic lexical acquisition has sought to address this ((Dorr and Jones, 1996; Dorr, 1997), among others). Yet there have been few attempts to learn finegrained lexical classifications from the statistical analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (Brent, 1993; Klavans and Chodorow, 1992; Resnik, 1992)). In this paper, we propose such an approach for the automatic classification of verbs 45 In exploring these questions, we focus on verb classification for several reasons. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major"
E99-1007,C92-4177,0,0.0611426,"otated corpus data. The shift has mostly occurred because building wide-coverage grammars is time-consuming, error prone, and difficult. The same can be said for crafting the rich lexical representations that are a central component of linguistic knowledge, and research in automatic lexical acquisition has sought to address this ((Dorr and Jones, 1996; Dorr, 1997), among others). Yet there have been few attempts to learn finegrained lexical classifications from the statistical analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (Brent, 1993; Klavans and Chodorow, 1992; Resnik, 1992)). In this paper, we propose such an approach for the automatic classification of verbs 45 In exploring these questions, we focus on verb classification for several reasons. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role in the organization and use of this knowledge: Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dorr, 1997), and document classification (Klavans and Kan, 1998). Manual classification of large"
E99-1007,J93-2002,0,0.173984,"edge from annotated corpus data. The shift has mostly occurred because building wide-coverage grammars is time-consuming, error prone, and difficult. The same can be said for crafting the rich lexical representations that are a central component of linguistic knowledge, and research in automatic lexical acquisition has sought to address this ((Dorr and Jones, 1996; Dorr, 1997), among others). Yet there have been few attempts to learn finegrained lexical classifications from the statistical analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (Brent, 1993; Klavans and Chodorow, 1992; Resnik, 1992)). In this paper, we propose such an approach for the automatic classification of verbs 45 In exploring these questions, we focus on verb classification for several reasons. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role in the organization and use of this knowledge: Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dorr, 1997), and document classification (Klavans and Kan, 1998). Man"
E99-1007,P93-1024,0,0.234604,"Missing"
E99-1007,P97-1008,0,0.0336484,"Missing"
E99-1007,W97-0301,0,0.0172238,"nsitive, causative form. These features correspond directly to the defining alternations of the three verb classes under study (intransitive/transitive, causative). Additionally, we see that other related features to these usages serve to distinguish the two resolutions of the ambiguity. The main verb form is active and a main verb part-of-speech (labeled as VBD by automatic POS taggers); by contrast, the reduced relative form is passive and a past participle (tagged as VBN). Although these properties are redundant with the intransitive/transitive distinction, recent work in machine learning (Ratnaparkhi, 1997; Ratnaparkhi, 1998) has shown that using overlapping features can be beneficial for learning in a maximum entropy framework, and we want to explore it in this setting to test H 3 above. 2 In the next section, 2These properties are redundant with the intransitive/transitive distinction, as passive implies transitive use, and necessarily entails the use of a past participle. We performed a correlation analysis that Proceedings of EACL &apos;99 we describe how we compile the corpus counts for each of the four properties, in order to approximate the distributional information of these alternations. 3"
E99-1007,P98-2177,0,0.0223226,"form. These features correspond directly to the defining alternations of the three verb classes under study (intransitive/transitive, causative). Additionally, we see that other related features to these usages serve to distinguish the two resolutions of the ambiguity. The main verb form is active and a main verb part-of-speech (labeled as VBD by automatic POS taggers); by contrast, the reduced relative form is passive and a past participle (tagged as VBN). Although these properties are redundant with the intransitive/transitive distinction, recent work in machine learning (Ratnaparkhi, 1997; Ratnaparkhi, 1998) has shown that using overlapping features can be beneficial for learning in a maximum entropy framework, and we want to explore it in this setting to test H 3 above. 2 In the next section, 2These properties are redundant with the intransitive/transitive distinction, as passive implies transitive use, and necessarily entails the use of a past participle. We performed a correlation analysis that Proceedings of EACL &apos;99 we describe how we compile the corpus counts for each of the four properties, in order to approximate the distributional information of these alternations. 3 Frequency Distributi"
E99-1007,P98-2184,0,0.0293719,"Missing"
E99-1007,C98-1046,0,\N,Missing
E99-1007,C98-2179,0,\N,Missing
E99-1007,P98-1112,0,\N,Missing
E99-1007,C98-1108,0,\N,Missing
E99-1007,P92-1053,0,\N,Missing
E99-1007,C98-2172,0,\N,Missing
H05-1111,P98-1013,0,0.0562367,"by which to demonstrate the utility of deriving more explicit knowledge from a predicate lexicon, which can be later extended through the use of additional probabilistic features. We face a methodological challenge arising from the particular choice of VerbNet for the prototyping of our method: the lexicon has no associated semantic role labelled corpus. While this underscores the need for approaches which do not rely on such a resource, it also means that we lack a labelled sample of data against which to evaluate our results. To address this, we use the existing labelled corpus of FrameNet (Baker et al., 1998), and develop a mapping for converting the FrameNet roles to corresponding VerbNet roles. Our mapping method demonstrates the possibility of leveraging existing resources to support the development of role labelling systems based on verb lexicons that do not have an associated hand-labelled corpus. 2 VerbNet Roles and the Role Mapping Before describing our labelling algorithm, we first briefly introduce the semantic role information available in VerbNet, and describe how we map FrameNet roles to VerbNet roles. 884 whisper Frames: Agent V Agent V Prep(+dest) Recipient Agent V Topic Verbs in sam"
H05-1111,C02-1132,0,0.0584901,"Missing"
H05-1111,J02-3001,0,0.283737,"he usefulness of predicate lexicons for role labelling, as well as the feasibility of modifying an existing role-labelled corpus for evaluating a different set of semantic roles. We achieve a substantial improvement over an informed baseline. 1 Introduction Intelligent language technologies capable of full semantic interpretation of domain-general text remain an elusive goal. However, statistical advances have made it possible to address core pieces of the problem. Recent years have seen a wealth of research on one important component of semantic interpretation—automatic role labelling (e.g., Gildea and Jurafsky, 2002; Pradhan et al., 2004; Hacioglu et al., 2004, and additional papers from Carreras and Marquez, 2004). Such work aims to annotate each constituent in a clause with a semantic tag indicating the role that the constituent plays with respect to the target predicate, as in (1): (1) [Yuka]Agent [whispered]Pred to [Dar]Recipient Semantic role labelling systems address a crucial first step in the automatic extraction of semantic relations from domain-general text, taking us closer to the goal of comprehensive semantic mark-up. Most work thus far on domain-general role labelling depends on supervised"
H05-1111,W04-2416,0,0.0150364,"lling, as well as the feasibility of modifying an existing role-labelled corpus for evaluating a different set of semantic roles. We achieve a substantial improvement over an informed baseline. 1 Introduction Intelligent language technologies capable of full semantic interpretation of domain-general text remain an elusive goal. However, statistical advances have made it possible to address core pieces of the problem. Recent years have seen a wealth of research on one important component of semantic interpretation—automatic role labelling (e.g., Gildea and Jurafsky, 2002; Pradhan et al., 2004; Hacioglu et al., 2004, and additional papers from Carreras and Marquez, 2004). Such work aims to annotate each constituent in a clause with a semantic tag indicating the role that the constituent plays with respect to the target predicate, as in (1): (1) [Yuka]Agent [whispered]Pred to [Dar]Recipient Semantic role labelling systems address a crucial first step in the automatic extraction of semantic relations from domain-general text, taking us closer to the goal of comprehensive semantic mark-up. Most work thus far on domain-general role labelling depends on supervised learning over statistical features extracted"
H05-1111,N04-1030,0,0.181087,"lexicons for role labelling, as well as the feasibility of modifying an existing role-labelled corpus for evaluating a different set of semantic roles. We achieve a substantial improvement over an informed baseline. 1 Introduction Intelligent language technologies capable of full semantic interpretation of domain-general text remain an elusive goal. However, statistical advances have made it possible to address core pieces of the problem. Recent years have seen a wealth of research on one important component of semantic interpretation—automatic role labelling (e.g., Gildea and Jurafsky, 2002; Pradhan et al., 2004; Hacioglu et al., 2004, and additional papers from Carreras and Marquez, 2004). Such work aims to annotate each constituent in a clause with a semantic tag indicating the role that the constituent plays with respect to the target predicate, as in (1): (1) [Yuka]Agent [whispered]Pred to [Dar]Recipient Semantic role labelling systems address a crucial first step in the automatic extraction of semantic relations from domain-general text, taking us closer to the goal of comprehensive semantic mark-up. Most work thus far on domain-general role labelling depends on supervised learning over statisti"
H05-1111,W98-1106,0,0.162588,"Missing"
H05-1111,W04-3213,1,\N,Missing
H05-1111,J03-4003,0,\N,Missing
H05-1111,C98-1013,0,\N,Missing
J01-3003,J93-2002,0,0.215201,"predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In facing the task of automatic acquisition of knowledge about verbs, two basic questions must be addressed: What information about verbs and their relational properties needs to be learned? What information can in practice be learned through automatic means? In answering these questions, some approaches to lexical acquisition have focused on learning syntactic information about verbs, by automatically extracting subcategorization frames from a corpus or machine-readable dictionary (Brent 1993; Briscoe and Carroll 1997; Dorr 1997; Lapata 1999; Manning 1993; McCarthy and Korhonen 1998). * Linguistics Department; University of Geneva; 2 rue de Candolle; 1211 Geneva 4, Switzerland; merlo@ lettres.unige.ch t Department of Computer Science; University of Toronto; 6 King&apos;s College Road; Toronto, ON M5S 3H5 Canada; suzanne@cs.toronto.edu Q 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 Table 1 Examples of verbs from the three optionally intransitive classes. Unergative The horse raced past the barn. The jockey raced the horse past the barn. Un"
J01-3003,A97-1052,0,0.666125,"Missing"
J01-3003,W98-1106,0,0.217437,"Science; University of Toronto; 6 King&apos;s College Road; Toronto, ON M5S 3H5 Canada; suzanne@cs.toronto.edu Q 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 Table 1 Examples of verbs from the three optionally intransitive classes. Unergative The horse raced past the barn. The jockey raced the horse past the barn. Unaccusative The butter melted in the pan. The cook melted the butter in the pan. Object-Drop The boy played. The boy played soccer. Other work has attempted to learn deeper semantic properties such as selectional restrictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans and Chodorow 1992; Siegel 1999), or lexical-semantic verb classes such as those proposed by Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulte im Walde 2000). In this paper, we focus on argument structure--the thematic roles assigned by a verb to its arguments--as the w a y in which the relational semantics of the verb is represented at the syntactic level. Specifically, our proposal is to automatically classify verbs based on argument structure properties, using statistical corpus-based methods. We address the problem of classification bec"
J01-3003,A92-1011,0,0.0346587,"Missing"
J01-3003,C00-2108,0,0.739729,"Missing"
J01-3003,P99-1015,0,0.124924,"Canada; suzanne@cs.toronto.edu Q 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 Table 1 Examples of verbs from the three optionally intransitive classes. Unergative The horse raced past the barn. The jockey raced the horse past the barn. Unaccusative The butter melted in the pan. The cook melted the butter in the pan. Object-Drop The boy played. The boy played soccer. Other work has attempted to learn deeper semantic properties such as selectional restrictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans and Chodorow 1992; Siegel 1999), or lexical-semantic verb classes such as those proposed by Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulte im Walde 2000). In this paper, we focus on argument structure--the thematic roles assigned by a verb to its arguments--as the w a y in which the relational semantics of the verb is represented at the syntactic level. Specifically, our proposal is to automatically classify verbs based on argument structure properties, using statistical corpus-based methods. We address the problem of classification because it provides a means for lexical organization which"
J01-3003,J99-2004,0,0.0429239,"Missing"
J01-3003,J98-3003,0,0.0428045,"Missing"
J01-3003,W99-0503,1,0.792109,"Missing"
J01-3003,P89-1022,0,0.0601827,"nnotated text (e.g., Brent [1993]; Sanfilippo and Poznanski [1992]; Manning [1993]; Collins [1997]). Others have tackled the problem of lexical semantic classification, but using only subcategorization frequencies as input data (Lapata and Brew 1999; Schulte im Walde 2000). Specifically, these researchers have not explicitly addressed the definition of features to tap directly into thematic role differences that are not reflected in subcategorization distinctions. On the other hand, when learning of thematic role assignment has been the explicit goal, the text has been semantically annotated (Webster and Marcus 1989), or external semantic resources have been consulted (Aone and McKee 1996; McCarthy 2000). We extend these results by showing that thematic information can be induced from linguistically-guided counts in a corpus, without the use of thematic role tagging or external resources such as WordNet. Finally, our results converge with the increasing agreement that corpus-based techniques are fruitful in the automatic construction of computational lexicons, providing machine readable dictionaries with complementary, reusable resources, such as frequencies of argument structures. Moreover, these techniq"
J01-3003,W98-1116,1,\N,Missing
J01-3003,merlo-stevenson-2000-establishing,1,\N,Missing
J01-3003,A00-2034,0,\N,Missing
J01-3003,W96-0213,0,\N,Missing
J01-3003,C96-1055,0,\N,Missing
J01-3003,P97-1003,0,\N,Missing
J01-3003,C92-4177,0,\N,Missing
J01-3003,P98-1046,0,\N,Missing
J01-3003,C98-1046,0,\N,Missing
J01-3003,P98-1112,0,\N,Missing
J01-3003,C98-1108,0,\N,Missing
J01-3003,J03-2004,0,\N,Missing
J01-3003,P93-1032,0,\N,Missing
J01-3003,P98-2247,0,\N,Missing
J01-3003,C98-2242,0,\N,Missing
J01-3003,J96-2004,0,\N,Missing
J01-3003,P99-1051,0,\N,Missing
J01-3003,W99-0632,0,\N,Missing
J08-2001,W04-2412,1,0.733851,"Missing"
J08-2001,W05-0620,1,0.919702,"Missing"
J08-2001,W04-2415,1,0.828554,"Missing"
J08-2001,W05-0622,0,0.0206004,"Missing"
J08-2001,copestake-flickinger-2000-open,0,0.00738653,"]Pred to [the boy beside her]Recipient Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities participating in an event, and Temporal and Manner for the characterization of other aspects of the event or participant relations. This type of role labeling thus yields a ﬁrstlevel semantic representation of the text that indicates the basic event properties and relations among relevant entities that are expressed in the sentence. Research has proceeded for decades on manually created lexicons, grammars, and other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000) in support of deep semantic analysis of language input, but such approaches have been labor-intensive and often restricted to narrow domains. The 1990s saw a growth in the development of statistical machine learning methods across the ﬁeld of computational linguistics, enabling systems to learn complex linguistic knowledge rather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe an"
J08-2001,P07-1028,0,0.0144889,"Missing"
J08-2001,S07-1048,0,0.143442,"Missing"
J08-2001,W05-0625,0,0.0189955,"Missing"
J08-2001,W04-0803,0,0.00817493,"Missing"
J08-2001,S07-1005,1,0.808742,"Missing"
J08-2001,W05-0628,1,0.603848,"Missing"
J08-2001,S07-1008,1,0.800462,"Missing"
J08-2001,J01-3003,1,0.757374,"been labor-intensive and often restricted to narrow domains. The 1990s saw a growth in the development of statistical machine learning methods across the ﬁeld of computational linguistics, enabling systems to learn complex linguistic knowledge rather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore, Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 an"
J08-2001,W04-2705,0,0.0796831,"ather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore, Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and"
J08-2001,W04-2609,0,0.0198714,"evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for acquiring and exploiting semantic relations among the different components of a text. This special issue of Computational Linguistics presents several articles representing the state-of-the-art in SRL, and this overview is intended to provide a broader c"
J08-2001,N06-2026,0,0.0821305,"Missing"
J08-2001,C04-1100,0,0.0222676,"s et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for acquiring and exploiting semantic relations among th"
J08-2001,W03-0411,0,0.0160444,"Missing"
J08-2001,J05-1004,0,0.471683,"Missing"
J08-2001,W05-0634,0,0.0213716,"Missing"
J08-2001,J06-2001,0,0.0246464,"Missing"
J08-2001,P03-1002,0,0.288914,"ea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for a"
J08-2001,H05-1111,1,0.811574,"Missing"
J08-2001,P05-1073,0,0.0404859,"Missing"
J08-2001,W04-3212,0,0.06057,"Missing"
J08-2001,S07-1077,1,0.814405,"Missing"
J08-2001,N07-1069,0,\N,Missing
J08-2001,W04-3213,1,\N,Missing
J08-2001,boas-2002-bilingual,0,\N,Missing
J08-2001,S07-1018,0,\N,Missing
J08-2001,S07-1016,0,\N,Missing
J08-2001,J13-3006,1,\N,Missing
J08-2001,A97-1052,0,\N,Missing
J08-2001,P04-1055,0,\N,Missing
J08-2001,J02-3001,0,\N,Missing
J08-2001,W06-2303,0,\N,Missing
J09-1005,W03-1812,0,0.710876,"Missing"
J09-1005,W07-1101,0,0.295213,"Missing"
J09-1005,W03-1809,0,0.219673,"Missing"
J09-1005,E06-1042,0,0.259961,"asks, such as semantic parsing and machine translation, which require the identiﬁcation of multiword semantic units. Most recent studies focusing on the identiﬁcation of idiomatic and non-idiomatic tokens either assume the existence of manually annotated data for a supervised classiﬁcation (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006), or even ignore the speciﬁc properties of nonliteral language and rely mainly on general purpose methods for the task (Birke and Sarkar 2006). We propose unsupervised methods that rely on automatically acquired knowledge about idiom types to identify their token occurrences as idiomatic or literal (Section 6). More speciﬁcally, we explore the hypothesis that the type-based knowledge we automatically acquire about an idiomatic expression can be used to determine whether an instance of the expression is used literally or idiomatically (token-based knowledge). Our experimental results show that the performance of the token-based idiom identiﬁcation methods proposed here is comparable to that of existing supervised techniques (Section"
J09-1005,W07-1106,1,0.861217,"Missing"
J09-1005,copestake-etal-2002-multiword,0,0.0168772,"y idiomatic expression as either idiomatic or literal in order to handle a given sequence of words appropriately. For example, a machine translation system must translate held ﬁre differently in The army held their ﬁre and The worshippers held the ﬁre up to the idol. Previous studies focusing on the automatic identiﬁcation of idiom types have often recognized the importance of drawing on their linguistic properties, such as their semantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic ex"
J09-1005,P94-1038,0,0.0471208,"Missing"
J09-1005,evert-etal-2004-identifying,0,0.0607398,"Missing"
J09-1005,P01-1025,0,0.0483953,"Missing"
J09-1005,E06-1043,1,0.723279,"ure reﬂecting the degree of lexical ﬁxedness for the target pair. We assume that the target pair is lexically ﬁxed to the extent that its PMI deviates from the average PMI of its variants. By our measure, the target pair is considered lexically ﬁxed (i.e., is given a high ﬁxedness score) only if the difference between its PMI value and that of most of its variants—not necessarily all, as in the method of Lin (1999)—is high.4 Our measure calculates this deviation, normalized using the sample’s standard deviation: . PMI(v, n) − PMI Fixednesslex (v, n) = s (2) 3 In an early version of this work (Fazly and Stevenson 2006), only the noun constituent was varied because we expected replacing the verb constituent with a related verb to be more likely to yield another VNIC, as in keep/lose one’s cool, give/get the bird, crack/break the ice (Nunberg, Sag, and Wasow 1994; Grant 2005). Later experiments on the development data showed that variants generated by replacing both constituents, one at a time, produce better results. 4 This way, even if an idiom has a few frequently used variants (e.g., break the ice and crack the ice), it may still be assigned a high ﬁxedness score if most other variants are uncommon. Note"
J09-1005,W07-1102,1,0.82404,"Missing"
J09-1005,P06-2046,0,0.0396623,"Missing"
J09-1005,W06-1203,0,0.72055,"Missing"
J09-1005,E03-1073,0,0.104902,"Missing"
J09-1005,P98-2127,0,0.133518,"easure to determine); (ii) it can only measure the lexical ﬁxedness of idiomatic combinations, and so could not apply to literal combinations. We thus interpret this property statistically in the following way: We expect a lexically ﬁxed verb+noun combination to appear much more frequently than its variants in general. Speciﬁcally, we examine the strength of association between the verb and the noun constituent of a combination (the target expression or its lexical variants) as an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the automatically built thesaurus of Lin (1998) to ﬁnd words similar to each constituent, in order to automatically generate variants.2 Variants are generated by replacing either 2 We also replicated our experiments with an automatically built thesaurus created from the British National Corpus (BNC) in a similar fashion, and kindly provided to us by Diana McCarthy. Results were similar, hence we do not report them here. 66 Fazly, Cook, and Stevenson Unsupervised Idiom Identiﬁcation the noun or the verb constituent of a pair with a semantically (and syntactically) similar word.3 Examples of automatically generated variants for the pair spi"
J09-1005,P99-1041,0,0.779836,"erties, such as their semantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic expressions. Speciﬁcally, we focus on a cross-linguistically prominent class of phrasal idioms which are commonly and productively formed from the combination of a frequent verb and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg, Sag, and Wasow 1994; Fellbaum 2002), for example, shoot the breeze, make a face, and push one’s luck. We refer to these as verb+noun idiomat"
J09-1005,W03-1810,0,0.533617,"Missing"
J09-1005,W97-0311,0,0.146044,"Missing"
J09-1005,W97-0207,0,0.186513,"Missing"
J09-1005,ritz-heid-2006-extraction,0,0.0284773,"Missing"
J09-1005,J93-1007,0,0.366692,"army held their ﬁre and The worshippers held the ﬁre up to the idol. Previous studies focusing on the automatic identiﬁcation of idiom types have often recognized the importance of drawing on their linguistic properties, such as their semantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic expressions. Speciﬁcally, we focus on a cross-linguistically prominent class of phrasal idioms which are commonly and productively formed from the combination of a frequent verb and a noun in"
J09-1005,W07-1104,0,0.0398115,"Missing"
J09-1005,H05-1113,0,0.108608,"Missing"
J09-1005,W06-2405,0,0.0263079,"Missing"
J09-1005,H05-1106,0,0.0466439,"Missing"
J09-1005,W05-1006,0,0.141505,"Missing"
J09-1005,J09-2001,0,\N,Missing
J09-1005,J03-4003,0,\N,Missing
J09-1005,C98-2122,0,\N,Missing
J10-1002,C96-1005,0,0.13629,"ss to distributional weights on concepts, and occupied area to the spread of the weighted concepts in the ontology. We call the resulting measure of proﬁle coherence proﬁle density. 7.3.1 The Proﬁle Density Measure. To adapt the deﬁnition of geographical density to our problem, we ﬁrst need to determine the analogs of population mass and occupied area in a semantic proﬁle. The proﬁle mass at each concept node is directly analogous to the population mass. Deﬁning the occupied area within an ontology is not as straightforward, as there is no simple deﬁnition of area within a graph. For example, Agirre and Rigau (1996) use the number of nodes within a subgraph as its area, but this fails to take into account how dispersed the nodes are throughout the ontology. We instead develop a deﬁnition of area that captures the actual spatial spread of the proﬁle mass through the ontology. To begin, we note that any subgraph of the WordNet hypernym hierarchy is hierarchical itself. Thus, any region of the ontology that contains some proﬁle mass is a hierarchy rooted at some common ancestor of those proﬁle nodes.10 As shown in Figure 8, the more dispersed (less closely clustered together) a set of nodes is, the further"
J10-1002,H05-1042,0,0.026331,"bgraph), which was found to be useful for WSD. Graphs in general are the obvious mathematical formalism to encode the relationships (represented as edges) between either words or longer units of text (represented as nodes). (The reverse is possible, using nodes to represent relations and edges for semantic entities. The choice of representation clearly depends on the NLP task itself.) Once we formulate a problem into a standard graph problem, there are existing efﬁcient graph-based algorithms that we can use to ﬁnd an optimal or near-optimal solution. For example, both Pang and Lee (2004) and Barzilay and Lapata (2005) use a minimumcut algorithm for two vastly different applications, document polarity classiﬁcation and content selection, respectively. In these approaches, the sentences are represented as nodes in a graph, and the edge connecting each pair of nodes is weighted with an association score between the sentences, reﬂecting, for example, the distance (number of sentences) between a pair of sentences. The minimum-cut method allows them to classify the nodes, and thus the sentences, into different categories. Another popular graph method is the random walk algorithm, which is successfully employed b"
J10-1002,A97-1052,0,0.0362616,"ow a causative alternation. (Speciﬁcally, none of the classes containing a ﬁller verb allows an alternation in which the same underlying argument appears in the Subj-Intrans slot as well as the Obj-Trans slot.) The full set of potential causative and ﬁller verbs were ﬁltered according to corpus counts, as described next. 4.1.2 Corpus Data and Argument Extraction. We used a randomly selected 35M-word portion of the British National Corpus (BNC; Burnard 2000). The text was parsed using the RASP parser of Briscoe and Carroll (2002), and subcategorization frames were extracted using the system of Briscoe and Carroll (1997). Each subcategorization frame entry for a verb includes a list of the observed argument heads per slot along with their frequencies. For each verb/slot pair, we thus extracted the set of nouns used in that slot along with their frequency of occurrence. Verbs were ﬁltered from the potential list of experimental items if they occurred less than 10 times in our corpus in either the transitive or intransitive frame. The verbs were then divided into multiple frequency bands: high (at least 450 instances), medium (between 150 and 400 instances), and low (between 10 and 100 instances). An equal numb"
J10-1002,briscoe-carroll-2002-robust,0,0.0125441,"n equal number of ﬁller verbs, subject to the constraint that their Levin classes do not allow a causative alternation. (Speciﬁcally, none of the classes containing a ﬁller verb allows an alternation in which the same underlying argument appears in the Subj-Intrans slot as well as the Obj-Trans slot.) The full set of potential causative and ﬁller verbs were ﬁltered according to corpus counts, as described next. 4.1.2 Corpus Data and Argument Extraction. We used a randomly selected 35M-word portion of the British National Corpus (BNC; Burnard 2000). The text was parsed using the RASP parser of Briscoe and Carroll (2002), and subcategorization frames were extracted using the system of Briscoe and Carroll (1997). Each subcategorization frame entry for a verb includes a list of the observed argument heads per slot along with their frequencies. For each verb/slot pair, we thus extracted the set of nouns used in that slot along with their frequency of occurrence. Verbs were ﬁltered from the potential list of experimental items if they occurred less than 10 times in our corpus in either the transitive or intransitive frame. The verbs were then divided into multiple frequency bands: high (at least 450 instances), m"
J10-1002,J02-2003,0,0.144684,"ts. For example, Ribas (1995) maps the word frequency to the most speciﬁc concept(s) for the word, including all of the possible synsets for the word, but not their hypernyms. Resnik (1993) also distributes the word frequency uniformly, but does so across the most speciﬁc concept(s) and all of their hypernyms. Other approaches, although still avoiding the difﬁculties of WSD, do try to capture the overall semantic “tendencies” of the set of words. Such methods estimate the appropriate probability distribution over a set of concepts to represent a given bag of nouns as a whole (Li and Abe 1998; Clark and Weir 2002). However, such techniques still start with a mapping of each word to all of its immediate concepts. 5 There is disagreement over the suitability of treating WordNet as an ontology, rather than as a lexical network (Gangemi, Guarino, and Oltramari 2001; Hirst 2009). However, the intention of the creators of WordNet is apparently that its synsets correspond to concepts, and the relations between them include both “conceptual-semantic and lexical relations” (http://wordnet.princeton.edu/), qualifying it, under some views, as a general domain ontology. Although recognizing the limitations and dif"
J10-1002,W05-1203,0,0.149868,") relations (Wu and Palmer 1994; Resnik 1995; Jiang and Conrath 1997, among others). Using such an ontological measure to compare two texts (collections of words instead of single words) might involve mapping each word of a text to its appropriate concept(s) in the ontology, and then calculating the aggregate distance between the two resulting sets of concepts across the ontological relations. For example, one might calculate the semantic distance between the two texts as the average, minimum, maximum, or summed ontological distance between the individual elements of the two sets of concepts (Corley and Mihalcea 2005). Observe that each of these approaches to text comparison—distributional and ontological—encodes information not contained in the other. Distributional distance captures important information about frequency of occurrence of the words that constitute the target text, whereas ontological distance captures essential semantic knowledge that has been encoded in the relations of an ontology. In response, previous work has attempted to combine distributional and ontological information in computing semantic distance. For example, researchers have developed measures of semantic distance between text"
J10-1002,N03-1012,0,0.0669282,"nalyze the results in detail to identify future directions for further illuminating when and to what extent the method might be useful. The analysis reveals that our method is not consistently successful across our sample tasks. We hypothesize that, because ontological relations play an integral role in our semantic distance measure, the measure is less effective when the semantic proﬁle for a text (the set of corresponding concepts) lacks semantic coherence. Other work has explored ways to measure the semantic coherence of a set of concepts in terms of their connectedness within an ontology (Gurevych et al. 2003). Because a semantic proﬁle in our work includes both ontological (relational) and distributional (frequency) knowledge, we require a measure of semantic coherence that takes both into account. We develop a novel measure of semantic coherence called proﬁle density that captures both the ontological and distributional coherence of a set of frequencyweighted concepts, and apply it to the data sets used in the different tasks to better understand the performance of our semantic distance measure. Our distance measure is cast as a graphical text comparison task within a network ﬂow framework as des"
J10-1002,D07-1061,0,0.0203369,"en the nodes (e.g., word co-occurrence). The random walk algorithm allows for the classiﬁcation of each node based on the relevance of its neighbors. For example, Mihalcea (2006) uses random walk for WSD by constructing a graph in the following way. Each node represents an ambiguous (test) word, or a (training) word labelled with one of its senses. Each edge indicates that the corresponding two words co-occur in some context. The sense of an ambiguous word is determined by the sense of its most relevant neighbor(s), by randomly traversing the graph until an equilibrium state has been reached. Hughes and Ramage (2007) also use a random walk method, with the goal of determining semantic relatedness between individual words (not sets of words, as in our work). In their work, the random walk method computes a probability distribution over WordNet concepts. Note that the probability distributions resulting from random walks centered at different concepts in WordNet are distinct. One can then measure the semantic relatedness between two concepts by calculating the divergence between their probability distributions over WordNet concepts as a result of the two random walks centered at them. In comparison to other"
J10-1002,O97-1002,0,0.903484,"in the set of words used in each. Semantic distance can also be measured more explicitly, by using the relations in an ontology as the direct encoding of semantic association. However, such approaches have generally been limited to calculating the distance between two individual concepts, rather than capturing the distance between two sets of concepts corresponding to two texts. Numerous measures have been proposed, for example, for capturing the distance between two concepts in WordNet, typically relying on the synonymy (synset) and hyponymy (is-a) relations (Wu and Palmer 1994; Resnik 1995; Jiang and Conrath 1997, among others). Using such an ontological measure to compare two texts (collections of words instead of single words) might involve mapping each word of a text to its appropriate concept(s) in the ontology, and then calculating the aggregate distance between the two resulting sets of concepts across the ontological relations. For example, one might calculate the semantic distance between the two texts as the average, minimum, maximum, or summed ontological distance between the individual elements of the two sets of concepts (Corley and Mihalcea 2005). Observe that each of these approaches to"
J10-1002,P98-2124,0,0.171986,"responding concepts. For example, Ribas (1995) maps the word frequency to the most speciﬁc concept(s) for the word, including all of the possible synsets for the word, but not their hypernyms. Resnik (1993) also distributes the word frequency uniformly, but does so across the most speciﬁc concept(s) and all of their hypernyms. Other approaches, although still avoiding the difﬁculties of WSD, do try to capture the overall semantic “tendencies” of the set of words. Such methods estimate the appropriate probability distribution over a set of concepts to represent a given bag of nouns as a whole (Li and Abe 1998; Clark and Weir 2002). However, such techniques still start with a mapping of each word to all of its immediate concepts. 5 There is disagreement over the suitability of treating WordNet as an ontology, rather than as a lexical network (Gangemi, Guarino, and Oltramari 2001; Hirst 2009). However, the intention of the creators of WordNet is apparently that its synsets correspond to concepts, and the relations between them include both “conceptual-semantic and lexical relations” (http://wordnet.princeton.edu/), qualifying it, under some views, as a general domain ontology. Although recognizing t"
J10-1002,A00-2034,0,0.123853,"ormation not contained in the other. Distributional distance captures important information about frequency of occurrence of the words that constitute the target text, whereas ontological distance captures essential semantic knowledge that has been encoded in the relations of an ontology. In response, previous work has attempted to combine distributional and ontological information in computing semantic distance. For example, researchers have developed measures of semantic distance between texts that apply distributional distances to concept vectors of frequencies rather than to word vectors (McCarthy 2000; Mohammad and Hirst 2006). However, these approaches only make pairwise comparisions between the elements of the concept vectors, and do not take into account the important ontological relations among the concepts. In order to capture such relations, other methods have instead integrated distributional information into an ontological method. However, such approaches have heretofore been limited to measuring distance between two individual concepts. For example, some ontological measures use corpus frequencies of words to yield concept weights that are taken into account in measuring the dista"
J10-1002,J01-3003,1,0.629295,"b and its relations to its arguments at least partially determine the syntactic expression of those arguments (see Pinker [1989], among others). Inﬂuential work by Levin (1993) showed that this relationship could be exploited “in reverse” by using alternation behavior as an indicator of the underlying semantics of a verb—speciﬁcally, that verbs undergoing the same sets of alternations form classes with similar semantics. Computational linguists have built on this work by demonstrating that statistical cues to alternation behavior can be used to automatically place verbs into semantic classes (Merlo and Stevenson 2001; Schulte im Walde 2006). Detection of verb alternation behavior can be cast as a text comparison problem (McCarthy 2000; Merlo and Stevenson 2001). Consider an alternation such as the causative illustrated in Example (1). The set of nouns appearing as the subject of the intransitive (such as chocolate) have the same relation to the verb as the set of nouns appearing as the object of the transitive. Because the verb places constraints on what kinds of entities can be in that relation (here, things that are meltable), the two sets of nouns should be similar. Hence, to identify a particular alte"
J10-1002,H05-1052,0,0.39488,"(Wu and Palmer 1994; Resnik 1995; Jiang and Conrath 1997, among others). Using such an ontological measure to compare two texts (collections of words instead of single words) might involve mapping each word of a text to its appropriate concept(s) in the ontology, and then calculating the aggregate distance between the two resulting sets of concepts across the ontological relations. For example, one might calculate the semantic distance between the two texts as the average, minimum, maximum, or summed ontological distance between the individual elements of the two sets of concepts (Corley and Mihalcea 2005). Observe that each of these approaches to text comparison—distributional and ontological—encodes information not contained in the other. Distributional distance captures important information about frequency of occurrence of the words that constitute the target text, whereas ontological distance captures essential semantic knowledge that has been encoded in the relations of an ontology. In response, previous work has attempted to combine distributional and ontological information in computing semantic distance. For example, researchers have developed measures of semantic distance between text"
J10-1002,W06-1605,0,0.146217,"d by Pedersen, Purandare, and Kulkarni (2005) to provide “annotated” experimental data (with each text indicating the correct name) without the need for expensive manual annotation. In Pedersen, Purandare, and Kulkarni (2005), an unsupervised method of name discrimination through text clustering was used to address this task. This is infeasible for a method like ours, in which each distance calculation requires access to an ontology. (The worst-case complexity of clustering with our method is quadratic in the size of the ontology used; a detailed discussion can be found in Tsang and Stevenson [2006].) Instead, we use a supervised methodology, but experiment with varying small amounts of data in a minimally supervised approach. Although our method requires extra manual effort in the form of data annotation for training, we ﬁnd that the amount of annotated data required is modest. 5.1 Experimental Methodology 5.1.1 Corpus Data. We use Pedersen, Purandare, and Kulkarni’s (2005) data set, which was taken from the Agence France-Press English Service portion of the GigaWord English corpus distributed by the Linguistic Data Consortium. They extracted the local context of six pairs of names of v"
J10-1002,W04-0844,0,0.0589291,"Missing"
J10-1002,P04-1035,0,0.00355917,"Lee 1993). In other words, words are grouped together based on their distributional properties instead of their explicit semantic/ontological properties. Furthermore, unlike in our method, once the words are collapsed into unnamed concepts, the individual elements (i.e., the unnamed concepts) across data points cannot be compared. As shown 63 Computational Linguistics Volume 36, Number 1 in our experiments, taking into account this extra piece of information is beneﬁcial for some applications. 8.2 Graph Approaches In recent years, we have seen an increasing use of graph-based methods in NLP (Pang and Lee 2004; Mihalcea 2005; Navigli and Velardi 2005). The graph-theoretic approach is popular due to the elegance of representing appropriate NLP problems and the availability of a number of efﬁcient algorithms. One of the most straightforward NLP examples is the use of WordNet. Besides our work here, much prior research has taken advantage of the graphical structure of WordNet. For example, Agirre and Rigau’s (1996) conceptual density uses WordNet as a graph and calculates the density within a subgraph (the number of relevant concepts within a subgraph), which was found to be useful for WSD. Graphs in"
J10-1002,P00-1014,0,0.0340586,"y comparing the text of a new document to the text of various documents whose topics are known. The new document is then labelled with the topic of the document whose text is most similar to it. In general, the texts to be compared may be full documents, as in this example, or may be portions of documents, or even collections of documents. Using text comparison to perform semantic classiﬁcation has been adopted in a variety of natural language processing (NLP) tasks, from document classiﬁcation (Scott and Matwin 1998; Rennie 2001; Al-Mubaid and Umair 2006), to prepositional phrase attachment (Pantel and Lin 2000), to spelling correction (Budanitsky and Hirst 2001). ∗ Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada. E-mail: vyctsang@cs.toronto.edu. ∗∗ Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada. E-mail: suzanne@cs.toronto.edu. Submission received: 16 December 2007; revised submission received: 18 June 2008; accepted for publication: 20 August 2008. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 Distributional methods for semant"
J10-1002,P93-1024,0,0.309606,"Missing"
J10-1002,E95-1016,0,0.0304931,"d in the text that appears in the noun hierarchy of WordNet is included in the bag of nouns. The bag of nouns with their associated frequencies must be mapped to the appropriate concepts in WordNet. Given the current state of unsupervised WSD, there is generally no attempt to disambiguate the words of a text when performing this kind of mapping—that is, there is no selection of the most appropriate concept or set of concepts to map the words to, given the context of their use. The simplest method is to distribute the frequency of each word uniformly to its corresponding concepts. For example, Ribas (1995) maps the word frequency to the most speciﬁc concept(s) for the word, including all of the possible synsets for the word, but not their hypernyms. Resnik (1993) also distributes the word frequency uniformly, but does so across the most speciﬁc concept(s) and all of their hypernyms. Other approaches, although still avoiding the difﬁculties of WSD, do try to capture the overall semantic “tendencies” of the set of words. Such methods estimate the appropriate probability distribution over a set of concepts to represent a given bag of nouns as a whole (Li and Abe 1998; Clark and Weir 2002). However"
J10-1002,W98-0706,0,0.272057,"nce. For example, given a suitable text distance measure, document classiﬁcation can be performed by comparing the text of a new document to the text of various documents whose topics are known. The new document is then labelled with the topic of the document whose text is most similar to it. In general, the texts to be compared may be full documents, as in this example, or may be portions of documents, or even collections of documents. Using text comparison to perform semantic classiﬁcation has been adopted in a variety of natural language processing (NLP) tasks, from document classiﬁcation (Scott and Matwin 1998; Rennie 2001; Al-Mubaid and Umair 2006), to prepositional phrase attachment (Pantel and Lin 2000), to spelling correction (Budanitsky and Hirst 2001). ∗ Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada. E-mail: vyctsang@cs.toronto.edu. ∗∗ Department of Computer Science, University of Toronto, 6 King’s College Road, Toronto, Ontario M5S 3G4, Canada. E-mail: suzanne@cs.toronto.edu. Submission received: 16 December 2007; revised submission received: 18 June 2008; accepted for publication: 20 August 2008. © 2010 Association for Computa"
J10-1002,W04-2411,1,0.8692,"Missing"
J10-1002,W06-3815,1,0.915302,"ngth of an arrow represents the ontological distance, c(i, j), and the width indicates the amount of ﬂow, x(i, j). Note that the mass at the rightmost square in the ﬁgure has to be distributed over the two triangles, and the mass at the leftmost square is transported over a path with one edge (as indicated by 3 Some semantic distances, such as those of Lin (1998) and Resnik (1995), do not directly use the underlying graph structure of the ontology in calculating the distance between two concepts. Using this type of distance in our MCF framework requires an extra graph transformation step; see Tsang and Stevenson (2006) for more details. 4 Earlier we made the simplifying assumption that square nodes were the supply proﬁle and triangle nodes the demand proﬁle. We have now seen that a node can belong to both proﬁles, and its characterization more accurately is stated in terms of net supply/demand. Thus, for example, a square node may belong to just the supply proﬁle or to both the supply and demand proﬁle; the deﬁning factor is that it has a net supply. 38 Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance Figure 4 An example of transporting the weights at the square nodes (supply nodes) to"
J10-1002,C04-1146,0,0.0848339,"Missing"
J10-1002,P94-1019,0,0.0596409,"texts that is implicitly encoded in the set of words used in each. Semantic distance can also be measured more explicitly, by using the relations in an ontology as the direct encoding of semantic association. However, such approaches have generally been limited to calculating the distance between two individual concepts, rather than capturing the distance between two sets of concepts corresponding to two texts. Numerous measures have been proposed, for example, for capturing the distance between two concepts in WordNet, typically relying on the synonymy (synset) and hyponymy (is-a) relations (Wu and Palmer 1994; Resnik 1995; Jiang and Conrath 1997, among others). Using such an ontological measure to compare two texts (collections of words instead of single words) might involve mapping each word of a text to its appropriate concept(s) in the ontology, and then calculating the aggregate distance between the two resulting sets of concepts across the ontological relations. For example, one might calculate the semantic distance between the two texts as the average, minimum, maximum, or summed ontological distance between the individual elements of the two sets of concepts (Corley and Mihalcea 2005). Obse"
J10-1002,J06-2001,0,\N,Missing
J10-1002,J06-1003,0,\N,Missing
J10-1002,C98-2119,0,\N,Missing
J10-1005,alex-2008-comparing,0,0.0258709,"at these studies rely on limit their applicability to general text. Techniques for inferring lexical properties of neologisms can make use of information that is typically not available in other lexical acquisition tasks—speciﬁcally, knowledge of the processes through which neologisms are formed. Computational work on neologisms has tended to focus on tasks pertaining to a speciﬁc type of neologism, such as identifying and inferring the long form of acronyms (Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example), recognizing loanwords (Baker and Brew 2008; Alex 2008, for example), and identifying and expanding clippings (Means 1988, for example). This study focuses on the tasks of identifying, and inferring the source words of, lexical blends, a common type of neologism, which have been previously unaddressed except for our preliminary work in Cook and Stevenson (2007). In addition to knowledge about a word’s formation process, for many types of neologism, information about its phonological and orthographic content can be used to 146 Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English infer aspects of its syntactic"
J10-1005,baker-brew-2008-statistical,0,0.0652231,"Missing"
J10-1005,P08-1065,0,0.0466999,"Missing"
J10-1005,P90-1034,0,0.0660105,"the blend, whereas Lehrer’s subjects were not. 145 Computational Linguistics Volume 36, Number 1 Figure 1 ROC curves for blend identiﬁcation. In future work, we plan to re-examine this task and develop methods speciﬁcally for identifying blends and other types of neologism. 7. Related Work As discussed in Section 1, techniques generally used in the automatic acquisition of syntactic and semantic properties of words are not applicable here, because they use corpus statistics that cannot be accurately estimated for low frequency items, such as the novel lexical blends considered in this study (Hindle 1990; Lapata and Brew 2004; Joanis, Stevenson, and James 2008, for example). Other work has used the context in which an unknown word occurs, along with domain-speciﬁc knowledge, to infer aspects of its meaning and syntax (Granger 1977; Cardie 1993; Hastings and Lytinen 1994, for example). These studies have been able to learn properties of an unknown word from just one usage, or a small number of usages; however, the domain-speciﬁc resources that these studies rely on limit their applicability to general text. Techniques for inferring lexical properties of neologisms can make use of information t"
J10-1005,O97-1002,0,0.0232708,"Missing"
J10-1005,A88-1013,0,0.67328,"ction into the language. Fortunately, linguistic observations regarding neologisms— namely, that they are formed through speciﬁc word formation processes—can give insights for automatically learning their lexical properties. New words come about through a variety of means, including derivational morphology, compounding, and borrowing from another language (Algeo 1980; Bauer 1983; Plag 2003). Computational work on neologisms has largely focused on particular word formation processes, and has exploited information about the formation process to learn aspects of the semantic properties of words (Means 1988; Nadeau and Turney 2005; Baker and Brew 2008, for example). Subtractive word formations—words formed from partial orthographic or phonological content from existing words—have received a fair amount of attention recently in computational linguistics, particularly under the heading of inferring the long form of acronyms, especially in the bio-medical domain (e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). Lexical blends—the focus of this study—also known as blends, are another common type of subtractive word formation. Most blends are formed by"
J10-1005,J97-3003,0,0.0680422,"al and orthographic content can be used to 146 Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English infer aspects of its syntactic and semantic properties. This is the case for neologisms that are composed of existing words or afﬁxes (e.g., compounds and derivations) or partial orthographic or phonological material from existing words or afﬁxes (e.g., acronyms, clippings, and blends). For example, in the case of part-of-speech tagging, information about the sufﬁx of an unknown word can be used to determine its part-of-speech (Brill 1994; Ratnaparkhi 1996; Mikheev 1997, for example). For the task of inferring the long form of an acronym, the letters which compose a given acronym can be used to determine the most likely long form (Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). The latter approach to acronyms is somewhat similar to the way in which we use knowledge of the letters that make up a blend to form candidate sets and determine the most likely source words. However, in the case of acronyms, each word in a long form typically contributes only one letter to the acronym, while for blends, a source word usuall"
J10-1005,W06-1605,0,0.0244025,"Missing"
J10-1005,P06-2083,0,0.0754098,"has largely focused on particular word formation processes, and has exploited information about the formation process to learn aspects of the semantic properties of words (Means 1988; Nadeau and Turney 2005; Baker and Brew 2008, for example). Subtractive word formations—words formed from partial orthographic or phonological content from existing words—have received a fair amount of attention recently in computational linguistics, particularly under the heading of inferring the long form of acronyms, especially in the bio-medical domain (e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). Lexical blends—the focus of this study—also known as blends, are another common type of subtractive word formation. Most blends are formed by combining a preﬁx of one source word with a sufﬁx of another source word, as in brunch (breakfast and lunch). There may be overlap in the contribution of the source words, as in fantabulous ( fantastic and fabulous). It is also possible that one or both source words are entirely present, for example, gaydar ( gay radar) and jetiquette ( jet etiquette). We refer to blends such as these as simple two-word sequential blends, and focus on thi"
J10-1005,W96-0213,0,0.22287,"out its phonological and orthographic content can be used to 146 Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English infer aspects of its syntactic and semantic properties. This is the case for neologisms that are composed of existing words or afﬁxes (e.g., compounds and derivations) or partial orthographic or phonological material from existing words or afﬁxes (e.g., acronyms, clippings, and blends). For example, in the case of part-of-speech tagging, information about the sufﬁx of an unknown word can be used to determine its part-of-speech (Brill 1994; Ratnaparkhi 1996; Mikheev 1997, for example). For the task of inferring the long form of an acronym, the letters which compose a given acronym can be used to determine the most likely long form (Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). The latter approach to acronyms is somewhat similar to the way in which we use knowledge of the letters that make up a blend to form candidate sets and determine the most likely source words. However, in the case of acronyms, each word in a long form typically contributes only one letter to the acronym, while for blends, a sour"
J10-1005,J04-1003,0,\N,Missing
J10-1005,N04-3012,0,\N,Missing
K19-1008,D11-1010,0,0.0173234,"ntroduction The ubiquity of English as an online lingua franca offers a rich opportunity for computational research on second language acquisition and on tools for aiding non-native speakers. Most computational research in second language (L2) has focused on spelling and grammar errors, and has been conducted on learners with beginner-tointermediate proficiency level (henceforth, “learners”) (e.g. Ji et al., 2017; Sakaguchi et al., 2017; Rozovskaya et al., 2017; Lo et al., 2018). Little empirical work has looked at semantic errors, with existing research mostly focusing on collocations (e.g., Dahlmeier and Ng, 2011; Vecchi et al., 2011; Kochmar and Briscoe, 2013). Also, highly proficient, advanced L2 speakers (henceforth, “advanced L2s”) have received little attention (though see Daudaravicius et al., 2016). In contrast to learners, these speakers rarely violate grammatical norms of the L2, but rather deviate from native usage in much more nuanced ways, often exhibiting mild infelicities rather than outright errors. 1 We refer to either less preferred or unacceptable occurrences of an IP, as in (2) and (3), as infelicitous usages. 77 Proceedings of the 23rd Conference on Computational Natural Language L"
K19-1008,W16-0506,0,0.0240238,"Most computational research in second language (L2) has focused on spelling and grammar errors, and has been conducted on learners with beginner-tointermediate proficiency level (henceforth, “learners”) (e.g. Ji et al., 2017; Sakaguchi et al., 2017; Rozovskaya et al., 2017; Lo et al., 2018). Little empirical work has looked at semantic errors, with existing research mostly focusing on collocations (e.g., Dahlmeier and Ng, 2011; Vecchi et al., 2011; Kochmar and Briscoe, 2013). Also, highly proficient, advanced L2 speakers (henceforth, “advanced L2s”) have received little attention (though see Daudaravicius et al., 2016). In contrast to learners, these speakers rarely violate grammatical norms of the L2, but rather deviate from native usage in much more nuanced ways, often exhibiting mild infelicities rather than outright errors. 1 We refer to either less preferred or unacceptable occurrences of an IP, as in (2) and (3), as infelicitous usages. 77 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 77–86 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Usage class specific (SP) non-specific (NS) question (QU) conditional (CD) indirect nega"
K19-1008,N19-1423,0,0.0381788,"Missing"
K19-1008,C18-2018,0,0.0164885,"Section 5 we consider the ability of deep learning language models (LMs) – shown to be adept at capturing grammatical phenomena (Ji Introduction The ubiquity of English as an online lingua franca offers a rich opportunity for computational research on second language acquisition and on tools for aiding non-native speakers. Most computational research in second language (L2) has focused on spelling and grammar errors, and has been conducted on learners with beginner-tointermediate proficiency level (henceforth, “learners”) (e.g. Ji et al., 2017; Sakaguchi et al., 2017; Rozovskaya et al., 2017; Lo et al., 2018). Little empirical work has looked at semantic errors, with existing research mostly focusing on collocations (e.g., Dahlmeier and Ng, 2011; Vecchi et al., 2011; Kochmar and Briscoe, 2013). Also, highly proficient, advanced L2 speakers (henceforth, “advanced L2s”) have received little attention (though see Daudaravicius et al., 2016). In contrast to learners, these speakers rarely violate grammatical norms of the L2, but rather deviate from native usage in much more nuanced ways, often exhibiting mild infelicities rather than outright errors. 1 We refer to either less preferred or unacceptable"
K19-1008,D18-1151,0,0.094058,"ngs when someone pointed that out. Someone please make me a GIF of that Wade dunk. Anyone know what the issue might be? I would love it if someone could explain it in a more precise way. I don’t understand how anyone can really hate on him. I don’t have anything to add other than to say thanks for typing this out. If you work harder you deserve to earn more than someone who doesn’t do so. ...they invite anyone on, including musicians sometimes. Table 1: Usage classes of IPs, an indication of those subsumed by some- and any-, and examples from our corpora. et al., 2017; Sakaguchi et al., 2017; Marvin and Linzen, 2018; Goldberg, 2019) – to identify the subtle infelicities that stem from the semantic confusion introduced by some- and any- IPs. We show that while state-of-the-art models obtain encouraging initial results on this task, they leave room for future improvement (possibly informed by our linguistic findings) in mastering the semantic nuances of the system of English IPs. The contribution of this work is thus three-fold: First, to our knowledge, we develop the first largescale empirical investigation of second-language acquisition of indefinite pronouns, constituting a case study of taking a comput"
K19-1008,W14-1701,0,0.0557443,"Missing"
K19-1008,N18-1108,0,0.0929604,"ronoun preferred by the model to the gold annotation. Table 8 presents statistics across usage classes, for learners and advanced L2s (taken from Table 5), as well as for BERT. The top panel refers to learner data; the bottom panel, to advanced L2 data. While (expectedly) outperforming the two non-native populations, the model exhibits similar distributional patterns, with more infelicities in the CD and QU classes. The model also has Gulordava et al.: A successful variant of RNNs, the long short-term memory model (LSTM, Hochreiter and Schmidhuber, 1997), used for syntactic error detection in Gulordava et al. (2018). We trained the model using a similar set of parameters to Gulordava et al. (2018),10 on 10M sentences by native English speakers of Reddit (see Section 3), using a 20K sentence validation set and a 50K sentence test set. This model allows us to test the benefits of using in-domain data (for advanced L2s), despite its significantly lower volume, compared to other models. Google 1B: A very large publicly available LM released by Jozefowicz et al. (2016). This finetuned language model, trained on a billion-word corpus (Chelba et al., 2013), requires a massive infrastructure for training. It ach"
K19-1008,Q18-1024,1,0.838807,"do so. To summarize, our linguistic analysis reveals two potential challenges of English some- and any-: their confusability across many classes, and the particular difficulty of any- in the QU/CD classes. We further find empirically that someIPs are more frequent than any- in native English 3 Materials and Methods 3.1 Datasets We expect that mastery of IPs will depend on a speaker’s command of English, and therefore consider language productions both of learners (largely beginner-to-intermediate), and of L2 speakers on Reddit (shown to be highly proficient, almost on par with Reddit natives; Rabinovich et al. 2018). Our learner dataset comprises several sub-corpora: EFCAMDAT (Geertzen et al., 2013), TOEFL11 (Blanchard et al., 2013), and the freely available part of the FCE corpus (Yannakoudakis et al., 2011). The advanced L2 dataset includes online posts by advanced non-native English speakers from the L2-Reddit corpus (released by Rabinovich et al., 2018, and comprising utterances by native as well as highlyproficient non-native speakers, published on the Reddit platform). We extended the L2-Reddit corpus (originally collected in 2017) with data published through September 2018; the final dataset inclu"
K19-1008,J17-4002,0,0.0155147,"ep in that direction, in Section 5 we consider the ability of deep learning language models (LMs) – shown to be adept at capturing grammatical phenomena (Ji Introduction The ubiquity of English as an online lingua franca offers a rich opportunity for computational research on second language acquisition and on tools for aiding non-native speakers. Most computational research in second language (L2) has focused on spelling and grammar errors, and has been conducted on learners with beginner-tointermediate proficiency level (henceforth, “learners”) (e.g. Ji et al., 2017; Sakaguchi et al., 2017; Rozovskaya et al., 2017; Lo et al., 2018). Little empirical work has looked at semantic errors, with existing research mostly focusing on collocations (e.g., Dahlmeier and Ng, 2011; Vecchi et al., 2011; Kochmar and Briscoe, 2013). Also, highly proficient, advanced L2 speakers (henceforth, “advanced L2s”) have received little attention (though see Daudaravicius et al., 2016). In contrast to learners, these speakers rarely violate grammatical norms of the L2, but rather deviate from native usage in much more nuanced ways, often exhibiting mild infelicities rather than outright errors. 1 We refer to either less preferr"
K19-1008,I17-2062,0,0.0510563,"Missing"
K19-1008,P17-1070,0,0.0430389,"ties that L2 speakers face. As a first step in that direction, in Section 5 we consider the ability of deep learning language models (LMs) – shown to be adept at capturing grammatical phenomena (Ji Introduction The ubiquity of English as an online lingua franca offers a rich opportunity for computational research on second language acquisition and on tools for aiding non-native speakers. Most computational research in second language (L2) has focused on spelling and grammar errors, and has been conducted on learners with beginner-tointermediate proficiency level (henceforth, “learners”) (e.g. Ji et al., 2017; Sakaguchi et al., 2017; Rozovskaya et al., 2017; Lo et al., 2018). Little empirical work has looked at semantic errors, with existing research mostly focusing on collocations (e.g., Dahlmeier and Ng, 2011; Vecchi et al., 2011; Kochmar and Briscoe, 2013). Also, highly proficient, advanced L2 speakers (henceforth, “advanced L2s”) have received little attention (though see Daudaravicius et al., 2016). In contrast to learners, these speakers rarely violate grammatical norms of the L2, but rather deviate from native usage in much more nuanced ways, often exhibiting mild infelicities rather than o"
K19-1008,W11-1301,0,0.0341666,"of English as an online lingua franca offers a rich opportunity for computational research on second language acquisition and on tools for aiding non-native speakers. Most computational research in second language (L2) has focused on spelling and grammar errors, and has been conducted on learners with beginner-tointermediate proficiency level (henceforth, “learners”) (e.g. Ji et al., 2017; Sakaguchi et al., 2017; Rozovskaya et al., 2017; Lo et al., 2018). Little empirical work has looked at semantic errors, with existing research mostly focusing on collocations (e.g., Dahlmeier and Ng, 2011; Vecchi et al., 2011; Kochmar and Briscoe, 2013). Also, highly proficient, advanced L2 speakers (henceforth, “advanced L2s”) have received little attention (though see Daudaravicius et al., 2016). In contrast to learners, these speakers rarely violate grammatical norms of the L2, but rather deviate from native usage in much more nuanced ways, often exhibiting mild infelicities rather than outright errors. 1 We refer to either less preferred or unacceptable occurrences of an IP, as in (2) and (3), as infelicitous usages. 77 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 77–86"
K19-1008,P19-1340,0,0.0491556,"Missing"
K19-1008,P11-1019,0,0.0296569,"/CD classes. We further find empirically that someIPs are more frequent than any- in native English 3 Materials and Methods 3.1 Datasets We expect that mastery of IPs will depend on a speaker’s command of English, and therefore consider language productions both of learners (largely beginner-to-intermediate), and of L2 speakers on Reddit (shown to be highly proficient, almost on par with Reddit natives; Rabinovich et al. 2018). Our learner dataset comprises several sub-corpora: EFCAMDAT (Geertzen et al., 2013), TOEFL11 (Blanchard et al., 2013), and the freely available part of the FCE corpus (Yannakoudakis et al., 2011). The advanced L2 dataset includes online posts by advanced non-native English speakers from the L2-Reddit corpus (released by Rabinovich et al., 2018, and comprising utterances by native as well as highlyproficient non-native speakers, published on the Reddit platform). We extended the L2-Reddit corpus (originally collected in 2017) with data published through September 2018; the final dataset includes over 320M native and L2 English sentences. Table 2 presents details of the two corpora. Dataset learners advanced L2s (Reddit) native (Reddit) Sentences 5.6M 177M 146M Tokens 72M 2.4B 2.1B L1s"
K19-1008,R13-1047,0,0.0161576,"ine lingua franca offers a rich opportunity for computational research on second language acquisition and on tools for aiding non-native speakers. Most computational research in second language (L2) has focused on spelling and grammar errors, and has been conducted on learners with beginner-tointermediate proficiency level (henceforth, “learners”) (e.g. Ji et al., 2017; Sakaguchi et al., 2017; Rozovskaya et al., 2017; Lo et al., 2018). Little empirical work has looked at semantic errors, with existing research mostly focusing on collocations (e.g., Dahlmeier and Ng, 2011; Vecchi et al., 2011; Kochmar and Briscoe, 2013). Also, highly proficient, advanced L2 speakers (henceforth, “advanced L2s”) have received little attention (though see Daudaravicius et al., 2016). In contrast to learners, these speakers rarely violate grammatical norms of the L2, but rather deviate from native usage in much more nuanced ways, often exhibiting mild infelicities rather than outright errors. 1 We refer to either less preferred or unacceptable occurrences of an IP, as in (2) and (3), as infelicitous usages. 77 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 77–86 c Hong Kong, China, November"
K19-1008,N16-1042,0,0.0159125,"the most successful LMs. While being easily applied to a wide range of tasks, they provide significant improvements over classic backoff ngram models. A common use of a pre-trained LM – typically trained on an extremely large corpus – is to predict the likelihood of an ‘unseen’ sample of text: The higher the score (or the lower the perplexity) a text is assigned, the more probable it is, given the model. In particular, a fluent, wellformed text is likely to be scored higher by an LM than a text containing linguistic anomalies. Encouraged by results on the task of grammatical error detection (Yuan and Briscoe, 2016; Ji et al., 2017), we adhere to a similar approach, casting the detection of infelicities as a binary classification scenario: An LM is applied on a sentence with an original pronoun (e.g., something) and on the same sentence where the pronoun is substituted with its alternative (e.g., anything); then the one predicted as more probable (scored highest) is chosen as a model decision. Automatic Detection of Infelicities Our motivation for the above analysis is to use these insights to drive development of tools for L2 82 5.1 Models 5.2 Aiming to test the effect of various factors, such as train"
K19-1008,Q16-1037,0,\N,Missing
merlo-stevenson-2000-establishing,W98-1106,0,\N,Missing
merlo-stevenson-2000-establishing,J99-2004,0,\N,Missing
merlo-stevenson-2000-establishing,J98-3003,0,\N,Missing
merlo-stevenson-2000-establishing,C96-1055,0,\N,Missing
merlo-stevenson-2000-establishing,C92-4177,0,\N,Missing
merlo-stevenson-2000-establishing,E99-1007,1,\N,Missing
merlo-stevenson-2000-establishing,C00-2118,1,\N,Missing
merlo-stevenson-2000-establishing,A97-1052,0,\N,Missing
merlo-stevenson-2000-establishing,J93-2002,0,\N,Missing
merlo-stevenson-2000-establishing,P98-1046,0,\N,Missing
merlo-stevenson-2000-establishing,C98-1046,0,\N,Missing
merlo-stevenson-2000-establishing,P98-1112,0,\N,Missing
merlo-stevenson-2000-establishing,C98-1108,0,\N,Missing
merlo-stevenson-2000-establishing,P93-1032,0,\N,Missing
merlo-stevenson-2000-establishing,P98-2247,0,\N,Missing
merlo-stevenson-2000-establishing,C98-2242,0,\N,Missing
merlo-stevenson-2000-establishing,J96-2004,0,\N,Missing
merlo-stevenson-2000-establishing,P99-1015,0,\N,Missing
merlo-stevenson-2000-establishing,P99-1051,0,\N,Missing
merlo-stevenson-2000-establishing,W99-0632,0,\N,Missing
P02-1027,W98-1106,0,\N,Missing
P02-1027,A00-2034,0,\N,Missing
P02-1027,C02-1146,1,\N,Missing
P02-1027,H01-1035,0,\N,Missing
P02-1027,C00-2108,0,\N,Missing
P02-1027,J94-4003,0,\N,Missing
P02-1027,J95-4004,0,\N,Missing
P02-1027,J00-4004,0,\N,Missing
P02-1027,J01-3003,1,\N,Missing
P93-1036,P83-1017,0,0.0636996,"Missing"
S12-1014,W03-0601,0,0.0673542,"Missing"
S12-1014,N10-1125,0,0.0293427,"se is much clearer. We develop an unsupervised WSD algorithm based on Yarowsky’s that uses words in a short caption along with “visual words” from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image. Language-vision integration is a quickly developing field, and a number of researchers have explored the possibility of combining text and visual features in various multimodal tasks. Leong and Mihalcea (2011) explored semantic relatedness between words and images to better exploit multimodal content. Jamieson et al. (2009) and Feng and Lapata (2010) combined text and vision to perform effective image annotation. Barnard and colleagues (2003; 2005) showed that supervised WSD by could be improved with visual features. Here we show that unsupervised WSD can similarly be improved. Loeff, Alm and Forsyth (2006) and Saenko and Darrell (2008) combined visual and textual information to solve a related task, image sense disambiguation, in 85 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85–89, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics an unsupervised fashion. In Loeff et al.’"
S12-1014,W11-0120,0,0.0271602,"then text-only disambiguation becomes much more difficult; a human could do little more than guess. But if an image is available, the intended sense is much clearer. We develop an unsupervised WSD algorithm based on Yarowsky’s that uses words in a short caption along with “visual words” from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image. Language-vision integration is a quickly developing field, and a number of researchers have explored the possibility of combining text and visual features in various multimodal tasks. Leong and Mihalcea (2011) explored semantic relatedness between words and images to better exploit multimodal content. Jamieson et al. (2009) and Feng and Lapata (2010) combined text and vision to perform effective image annotation. Barnard and colleagues (2003; 2005) showed that supervised WSD by could be improved with visual features. Here we show that unsupervised WSD can similarly be improved. Loeff, Alm and Forsyth (2006) and Saenko and Darrell (2008) combined visual and textual information to solve a related task, image sense disambiguation, in 85 First Joint Conference on Lexical and Computational Semantics (*S"
S12-1014,P06-2071,0,0.0164476,"s. We also require sense annotations for the keyword for each image to use for evaluation. Barnard and Johnson (2005) developed the “Music is an important means of expression for many teens.” “Keeping your office supplies organized is easy, with the right tools.” “The internet has opened up the world to people of all nationalities.” “When there is no cheese I will take over the world.” Figure 2: Example image-caption pairs from our dataset, for “band” (top) and “mouse” (bottom). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al., 1993). Loeff et al. (2006) and Saenko and Darrell (2008) used Yahoo!’s image search to gather images with their associated web pages. While these datasets contain images paired with text, the textual contexts are much larger than typical captions. 3.1 Captioning Images To develop a large set of sense-annotated image– caption pairs with a focus on caption-sized text, we turned to ImageNet (Deng et al., 2009). ImageNet is a database of images that are each associated with a synset from WordNet. Hundreds of images are available for each of a number of senses of a wide variety of common nouns. To gather captions, we used A"
S12-1014,H93-1061,0,0.444438,"ith associated captions. We also require sense annotations for the keyword for each image to use for evaluation. Barnard and Johnson (2005) developed the “Music is an important means of expression for many teens.” “Keeping your office supplies organized is easy, with the right tools.” “The internet has opened up the world to people of all nationalities.” “When there is no cheese I will take over the world.” Figure 2: Example image-caption pairs from our dataset, for “band” (top) and “mouse” (bottom). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al., 1993). Loeff et al. (2006) and Saenko and Darrell (2008) used Yahoo!’s image search to gather images with their associated web pages. While these datasets contain images paired with text, the textual contexts are much larger than typical captions. 3.1 Captioning Images To develop a large set of sense-annotated image– caption pairs with a focus on caption-sized text, we turned to ImageNet (Deng et al., 2009). ImageNet is a database of images that are each associated with a synset from WordNet. Hundreds of images are available for each of a number of senses of a wide variety of common nouns. To gathe"
S12-1014,S07-1086,0,0.0250066,".63 .62 vector, where the ith element is the proportion of word Vi in the document. We run K-means both with and without visual features. For LDA, we use the dictionary sense model from Saenko and Darrell (2008). A topic model is learned where the relatedness of a topic to a sense is based on the probabilities of that topic generating the seed words from its dictionary definitions. Analogously to k-means, we learn a model for text alone, and a model for text augmented with visual information. For unsupervised WSD (applied to text only), we use WordNet::SenseRelate::TargetWord, hereafter PBP (Patwardhan et al., 2007), the highest scoring unsupervised lexical sample word sense disambiguation algorithm at SemEval07 (Pradhan et al., 2007). PBP treats the nearby words around the target word as a bag, and uses the WordNet hierarchy to assign a similarity score between the possible senses of words in the context, and possible senses of the target word. As our captions are fairly short, we use the entire caption as context. The most important result is the gain in accuracy after adding visual features. While the average gain 88 across all words is slight, it is significant at p < 0.02 (using a paired t-test). Fo"
S12-1014,S07-1016,0,0.0284505,"Missing"
S12-1014,P95-1026,0,0.415928,"context is often no larger than a tweet. Unsupervised WSD has been shown to work very well when the target word is embedded in a large We thank NSERC and U. Toronto for financial support. Fidler and Dickinson were sponsored by the Army Research Laboratory and this research was accomplished in part under Cooperative Agreement Number W911NF-10-2-0060. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either express or implied, of the Army Research Laboratory or the U.S. Government. quantity of text (Yarowsky, 1995). However, if the only available text is “The crane was so massive it blocked the sun” (see Fig. 1), then text-only disambiguation becomes much more difficult; a human could do little more than guess. But if an image is available, the intended sense is much clearer. We develop an unsupervised WSD algorithm based on Yarowsky’s that uses words in a short caption along with “visual words” from the captioned image to choose the best of two possible senses of an ambiguous keyword describing the content of the image. Language-vision integration is a quickly developing field, and a number of research"
W01-0705,W99-0632,0,0.122791,"classes of English verbs. We hypothesize that some lexical semantic features that are dicult to detect super cially in English may manifest themselves as easily extractable surface syntactic features in another language. Our experimental results combining English and Chinese features show that a small bilingual corpus may provide a useful alternative to using a large monolingual corpus for verb classi cation. 1 Introduction Recently, a number of researchers have devised corpus-based approaches for automatically learning the lexical semantic class of verbs (e.g., (McCarthy and Korhonen, 1998; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001)). Automatic verb classi cation yields important potential bene ts for the creation of lexical resources. Lexical semantic classes incorporate both syntactic and semantic information about verbs, such as the general sense of the verb (e.g., change-of-state or manner-of-motion) and the allowable mapping of verbal arguments to syntactic positions (e.g., whether an experiencer argument can appear as the subject or the object of the verb) (Levin, 1993). By automatically learning the assignment of verbs to lexical semantic classes, each verb inher"
W01-0705,P98-2247,0,0.0986742,"ng of verb classi cation, though we are not the rst to utilized multilingual resources for lexical acquisition tasks generally. For example, (Siegel and McKeown, 2000) suggested the use of parallel corpora in learning the aspectual classi cation (i.e., state or event) of English verbs. (Ide, 2000) and (Resnik and Yarowsky, 2000) made use of parallel corpora for word sense disambiguation. That is, a parallel (Englishnon-English) corpus was used as a source for lexicalizing some ne-grained English senses. Other work using multilingual resources that is highly related to ours are studies by Fung (1998) and by Melamed et al. (1997; 1998), in which a bilingual corpus was used to extract bilingual lexical entries. An important assumption is that the bilingual corpus is sentence or segment alignable, which allows for the calculation some co-occurence score between any two possible translations. One common theme in these papers is that, given any arbitrary tokens and some text coordinate system, the closer the two tokens' coordinates are, the more likely they are translational equivalents. Although we did not use an automatic method to nd translations of verbs, our aligned data collection techni"
W01-0705,P97-1039,0,0.024111,"Missing"
W01-0705,J01-3003,1,0.90031,"ome lexical semantic features that are dicult to detect super cially in English may manifest themselves as easily extractable surface syntactic features in another language. Our experimental results combining English and Chinese features show that a small bilingual corpus may provide a useful alternative to using a large monolingual corpus for verb classi cation. 1 Introduction Recently, a number of researchers have devised corpus-based approaches for automatically learning the lexical semantic class of verbs (e.g., (McCarthy and Korhonen, 1998; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001)). Automatic verb classi cation yields important potential bene ts for the creation of lexical resources. Lexical semantic classes incorporate both syntactic and semantic information about verbs, such as the general sense of the verb (e.g., change-of-state or manner-of-motion) and the allowable mapping of verbal arguments to syntactic positions (e.g., whether an experiencer argument can appear as the subject or the object of the verb) (Levin, 1993). By automatically learning the assignment of verbs to lexical semantic classes, each verb inherits a great deal of information about its possible u"
W01-0705,C00-2108,0,0.0898464,"hesize that some lexical semantic features that are dicult to detect super cially in English may manifest themselves as easily extractable surface syntactic features in another language. Our experimental results combining English and Chinese features show that a small bilingual corpus may provide a useful alternative to using a large monolingual corpus for verb classi cation. 1 Introduction Recently, a number of researchers have devised corpus-based approaches for automatically learning the lexical semantic class of verbs (e.g., (McCarthy and Korhonen, 1998; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001)). Automatic verb classi cation yields important potential bene ts for the creation of lexical resources. Lexical semantic classes incorporate both syntactic and semantic information about verbs, such as the general sense of the verb (e.g., change-of-state or manner-of-motion) and the allowable mapping of verbal arguments to syntactic positions (e.g., whether an experiencer argument can appear as the subject or the object of the verb) (Levin, 1993). By automatically learning the assignment of verbs to lexical semantic classes, each verb inherits a great deal of info"
W01-0705,J00-4004,0,0.0697528,"e overall accuracy. Observe in the italicized sections in the table (the best overall performance), the F scores are larger than those in the monolingual section ( rst two lines of the table). We conclude that adding Chinese features to English features has a performance bene t over the monolingual features alone for both verb classes, as well as overall. 6 Related Work Our work is the rst use of a bilingual corpusbased technique for the automatic learning of verb classi cation, though we are not the rst to utilized multilingual resources for lexical acquisition tasks generally. For example, (Siegel and McKeown, 2000) suggested the use of parallel corpora in learning the aspectual classi cation (i.e., state or event) of English verbs. (Ide, 2000) and (Resnik and Yarowsky, 2000) made use of parallel corpora for word sense disambiguation. That is, a parallel (Englishnon-English) corpus was used as a source for lexicalizing some ne-grained English senses. Other work using multilingual resources that is highly related to ours are studies by Fung (1998) and by Melamed et al. (1997; 1998), in which a bilingual corpus was used to extract bilingual lexical entries. An important assumption is that the bilingual cor"
W01-0705,W96-0213,0,\N,Missing
W01-0705,C98-2242,0,\N,Missing
W03-0410,W98-1106,0,\N,Missing
W03-0410,C96-1055,0,\N,Missing
W03-0410,E03-1040,1,\N,Missing
W03-0410,E99-1007,1,\N,Missing
W03-0410,C02-1040,0,\N,Missing
W03-0410,E03-1037,0,\N,Missing
W03-0410,J02-3001,0,\N,Missing
W03-0410,P02-1029,0,\N,Missing
W03-0410,J01-3003,1,\N,Missing
W03-0410,W99-0632,0,\N,Missing
W03-0604,J93-2003,0,\N,Missing
W04-0401,E95-1014,0,\N,Missing
W04-0401,W03-1810,0,\N,Missing
W04-0401,W02-1030,0,\N,Missing
W04-0401,W03-1809,0,\N,Missing
W04-0401,P99-1041,0,\N,Missing
W04-2411,W98-1106,0,\N,Missing
W04-2411,A00-2034,0,\N,Missing
W04-2411,J98-2002,0,\N,Missing
W04-2411,C02-1146,1,\N,Missing
W04-2411,J02-2003,0,\N,Missing
W04-2411,A97-1052,0,\N,Missing
W04-2411,W01-1009,0,\N,Missing
W04-2411,O97-1002,0,\N,Missing
W04-2411,P99-1004,0,\N,Missing
W04-2411,briscoe-carroll-2002-robust,0,\N,Missing
W04-2411,P94-1019,0,\N,Missing
W04-2411,C00-2148,0,\N,Missing
W04-2411,P02-1029,0,\N,Missing
W04-2411,J01-3003,1,\N,Missing
W04-2411,W04-2605,1,\N,Missing
W04-2605,A00-2034,0,\N,Missing
W04-2605,J98-2002,0,\N,Missing
W04-2605,C02-1146,1,\N,Missing
W04-2605,J02-2003,0,\N,Missing
W04-2605,A97-1052,0,\N,Missing
W04-2605,briscoe-carroll-2002-robust,0,\N,Missing
W04-2605,P94-1019,0,\N,Missing
W04-2605,C00-2148,0,\N,Missing
W04-2605,P02-1029,0,\N,Missing
W04-2605,J01-3003,1,\N,Missing
W04-2605,P99-1051,0,\N,Missing
W04-2605,W99-0632,0,\N,Missing
W04-2605,W01-1009,0,\N,Missing
W04-3213,W98-1106,0,\N,Missing
W04-3213,W04-2416,0,\N,Missing
W04-3213,J93-1005,0,\N,Missing
W04-3213,W03-1007,0,\N,Missing
W04-3213,J02-2003,0,\N,Missing
W04-3213,W03-1006,0,\N,Missing
W04-3213,E03-1037,0,\N,Missing
W04-3213,P98-1013,0,\N,Missing
W04-3213,C98-1013,0,\N,Missing
W04-3213,J02-3001,0,\N,Missing
W04-3213,J05-1004,0,\N,Missing
W04-3213,J01-3003,1,\N,Missing
W05-1005,E95-1014,0,\N,Missing
W05-1005,W03-1808,0,\N,Missing
W05-1005,W04-0401,1,\N,Missing
W05-1005,W03-1810,0,\N,Missing
W05-1005,J03-4003,0,\N,Missing
W05-1005,W03-1812,0,\N,Missing
W05-1005,W03-1809,0,\N,Missing
W05-1005,moiron-2004-discarding,0,\N,Missing
W05-1005,P99-1041,0,\N,Missing
W06-1207,W04-2608,0,0.0560858,"se compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambiguation of specific prepositions (e.g., Alam, 2004), while other work has classified preposition tokens according to their semantic role (O’Hara and Wiebe, 2003). Moreover, two large lexical resources of preposition senses are currently under construction, The Preposition Project (Litkowski, 2005) and PrepNet (SaintDizier, 2005). These resources were not suitable as the basis for our sense classes because they do not address the range of metaphorical extensions that a preposition/particle can take on, but future work may enable larger scale studies of the type 7 Conclusions While progress has recently been made in techniques for assessing the"
W06-1207,W03-1809,0,0.362002,"Missing"
W06-1207,W05-1005,1,0.867879,"n a goal-oriented sense as in A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof ). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, c Sydney, July 2006. 2006 Association for Computational Linguistics 2003). The features are motivated by the fact that semantic properties of a verb are reflected in the syntactic expression of"
W06-1207,E03-1040,1,0.900401,"Missing"
W06-1207,J04-1003,0,0.0241316,"trick and Fletcher (2005) classify VPC tokens, considering each as compositional, non-compositional or not a VPC. Again, however, it is important to recognize which of the possible meaning components is being contributed. In this vein, Uchiyama et al. (2005) tackle token classification of Japanese compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambiguation of specific prepositions (e.g., Alam, 2004), while other work has classified preposition tokens according to their semantic role (O’Hara and Wiebe, 2003). Moreover, two large lexical resources of preposition senses are currently under construction, The Preposition Project (Litkowski, 2005) and PrepNet (SaintDizier, 2005). These resources were not suitable"
W06-1207,P99-1041,0,0.0612925,"han one sense; in contrast to (1a), come up may use up in a goal-oriented sense as in A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof ). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, c Sydney, July 2006. 2006 Association for Computational Linguistics 2003). The features are motivated by the fact that semantic"
W06-1207,W03-1810,0,0.753485,"), come up may use up in a goal-oriented sense as in A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof ). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, c Sydney, July 2006. 2006 Association for Computational Linguistics 2003). The features are motivated by the fact that semantic properties of a verb are reflected in the syn"
W06-1207,P04-1036,0,0.0117226,"ure its semantics when used in VPCs in general). The deadline is coming up. While our long-term goal is token classification (disambiguation) of a VPC in context, following other work on VPCs (e.g., Bannard et al., 2003; McCarthy et al., 2003), we begin here with the task of type classification. Given our use of features which capture the statistical behaviour relevant to a VPC across a corpus, we assume that the outcome of type classification yields the predominant sense of the particle in the VPC. Predominant sense identification is a useful component of sense disambiguation of word tokens (McCarthy et al., 2004), and we presume our VPC type classification work will form the basis for later token disambiguation. Section 2 continues the paper with a discussion of the features we developed for particle sense classification. Section 3 first presents some brief cognitive linguistic background, followed by the sense classes of up used in our experiments. Sections 4 and 5 discuss our experimental set-up and results, Section 6 related work, and Section 7 our conclusions. 2.1.2 Particle Features Two types of features are motivated by properties specific to the semantics and syntax of particles and VPCs. First"
W06-1207,W03-0411,0,0.174508,"Missing"
W06-3815,J02-2003,0,\N,Missing
W06-3815,O97-1002,0,\N,Missing
W06-3815,J98-1004,0,\N,Missing
W06-3815,J06-1003,0,\N,Missing
W06-3815,P98-2124,0,\N,Missing
W06-3815,C98-2119,0,\N,Missing
W07-0606,W99-0901,0,0.368371,"bution over them. They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a χ2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Ciaramita and Johnson (2000) use a Bayesian network with the same topology as WordNet to estimate the probability distribution of the relevant set of nodes in the hierarchy. Abney and Light (1999) use a different representational approach: they train a separate hidden Markov model for each verb, and the selectional preference is represented as a probability distribution over words instead of semantic classes. Overview of the Model Our model learns the set of argument structure frames for each verb, and their grouping across verbs into constructions. An argument structure frame is a set of features of a verb usage that are both syntactic (the number of arguments, the syntactic pattern of the usage) and semantic (the semantic properties of the verb, the semantic properties of each argume"
W07-0606,E03-1034,0,0.0194936,"ew version of our model for the task of learning selectional preferences, we need a wide selection of verbs and their arguments that is impractical to compile by hand. The training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2 To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3 Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4 http://www.natcorp.ox.ac.uk 5 http://tedlab.mit.edu/∼ dr/Tgrep2 6 The software was provided to us by Eric Joanis, and Af45 each verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage"
W07-0606,C00-1028,0,0.0709696,"in a corpus. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a χ2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Ciaramita and Johnson (2000) use a Bayesian network with the same topology as WordNet to estimate the probability distribution of the relevant set of nodes in the hierarchy. Abney and Light (1999) use a different representational approach: they train a separate hidden Markov model for each verb, and the selectional preference is represented as a probability distribution over words instead of semantic classes. Overview of the Model Our model learns the set of argument structure frames for each verb, and their grouping across verbs into constructions. An argument structure frame is a set of features of a verb usage that ar"
W07-0606,J02-2003,0,0.10234,"association of a verb with a class is also defined as the contribution of that class to the total selectional preference strength. Resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its relevant argument in a corpus. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a χ2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Ciaramita and Johnson (2000) use a Bayesian network with the same topology as WordNet to estimate the probability distribution of the relevant set of nodes in the hierarchy. Abney and Light (1999) use a different representational approach: they train a separate hidden Markov model for each verb, and the selectional preference is represented as a probability distribution"
W07-0606,J98-2002,0,0.18844,"preferences is, therefore, modeled as using a set of training data to estimate that number. Resnik (1996) defines the selectional preference strength of a verb as the divergence between two probability distributions: the prior probabilities of the classes, and the posterior probabilities of the classes given that verb. The selectional association of a verb with a class is also defined as the contribution of that class to the total selectional preference strength. Resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its relevant argument in a corpus. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a χ2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Ciaramita and Johnson (2000) us"
W07-0606,J03-4003,0,\N,Missing
W07-1102,W03-1812,0,0.378809,"up is notably higher than ALL). On the other hand, INST features have a very poor performance on these classes, reinforcing that IDMs and LVCs may not necessarily appear with significantly high frequency of occurrence in a given corpus. Fixedness features thus prove to be 8 Since this is a binary feature, it can only distinguish two classes. In the future, we need to include more semantic classes. 15 Much recent work on classifying MWEs focuses on determining different levels of compositionality in verb+particle combinations using a measure of distributional similarity (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Another group of research attempts to classify a particular MWE subtype, such as verb-particle constructions (VPCs) or LVCs, according to some fine-grained semantic criteria (Wanner, 2004; Uchiyama et al., 2005; Cook and Stevenson, 2006). Here, we distinguish subtypes of MWEs that are defined according to coarse-grained distinctions in their degree of semantic idiosyncrasy. Wermter and Hahn (2004) recognize the importance of distinguishing MWE subtypes that are similar to our four classes, but only focus on separating MWEs as one single class from literal combinations."
W07-1102,W03-1809,0,0.183065,"han ALL). On the other hand, INST features have a very poor performance on these classes, reinforcing that IDMs and LVCs may not necessarily appear with significantly high frequency of occurrence in a given corpus. Fixedness features thus prove to be 8 Since this is a binary feature, it can only distinguish two classes. In the future, we need to include more semantic classes. 15 Much recent work on classifying MWEs focuses on determining different levels of compositionality in verb+particle combinations using a measure of distributional similarity (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Another group of research attempts to classify a particular MWE subtype, such as verb-particle constructions (VPCs) or LVCs, according to some fine-grained semantic criteria (Wanner, 2004; Uchiyama et al., 2005; Cook and Stevenson, 2006). Here, we distinguish subtypes of MWEs that are defined according to coarse-grained distinctions in their degree of semantic idiosyncrasy. Wermter and Hahn (2004) recognize the importance of distinguishing MWE subtypes that are similar to our four classes, but only focus on separating MWEs as one single class from literal combinations. For this, they use a m"
W07-1102,W06-1207,1,0.853784,"hus prove to be 8 Since this is a binary feature, it can only distinguish two classes. In the future, we need to include more semantic classes. 15 Much recent work on classifying MWEs focuses on determining different levels of compositionality in verb+particle combinations using a measure of distributional similarity (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Another group of research attempts to classify a particular MWE subtype, such as verb-particle constructions (VPCs) or LVCs, according to some fine-grained semantic criteria (Wanner, 2004; Uchiyama et al., 2005; Cook and Stevenson, 2006). Here, we distinguish subtypes of MWEs that are defined according to coarse-grained distinctions in their degree of semantic idiosyncrasy. Wermter and Hahn (2004) recognize the importance of distinguishing MWE subtypes that are similar to our four classes, but only focus on separating MWEs as one single class from literal combinations. For this, they use a measure that draws on the limited modifiability of MWEs, in addition to their expected high frequency. Krenn and Evert (2001) attempt to separate German idioms, LVCs, and literal phrases (of the form verb+prepositional phrase). They treat L"
W07-1102,E06-1043,1,0.726073,"PMI) are also used in many NLP applications (Church et al., 1991). PMI of a verb+noun combination ≺v , n≻ is defined as: P (v , n) . PMI (v , n) = log P (v ) P (n) f (∗, ∗)f (v , n) (1) ≈ log f (v , ∗) f (∗, n) where all frequency counts are calculated over verb–object pairs in a corpus. We use both frequency and PMI of a verb+noun combination to measure its degree of institutionalization. We refer to this group of measures as INST. 3.2 Measuring Fixedness To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness that we have developed in a previous study (Fazly and Stevenson, 2006), as well as some new measures we introduce here. The following paragraphs give a brief description of each. Fixednesslex quantifies the degree of lexical fixedness of the target combination, ≺v , n≻, by comparing its strength of association (measured by PMI) with those of its lexical variants. Like Lin (1999), we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of Lin (1998). We then use a standard statistic, the z -score, to calculate Fixednesslex : . Fixednesslex ("
W07-1102,P98-2127,0,0.00748376,"syntactic, and overall fixedness that we have developed in a previous study (Fazly and Stevenson, 2006), as well as some new measures we introduce here. The following paragraphs give a brief description of each. Fixednesslex quantifies the degree of lexical fixedness of the target combination, ≺v , n≻, by comparing its strength of association (measured by PMI) with those of its lexical variants. Like Lin (1999), we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of Lin (1998). We then use a standard statistic, the z -score, to calculate Fixednesslex : . Fixednesslex (v , n) = PMI(v , n) − PMI std (2) where PMI is the mean and std the standard deviation over the PMI of the target and all its variants. Fixednesssyn quantifies the degree of syntactic fixedness of the target combination, by comparing its behaviour in text with the behaviour of a typical verb–object, both defined as probability distributions over a predefined set of patterns. We use a standard information-theoretic measure, relative entropy, v v v v v v det:NULL det:a/an det:the det:DEM det:POSS det:OT"
W07-1102,P99-1041,0,0.395785,"b+noun combination to measure its degree of institutionalization. We refer to this group of measures as INST. 3.2 Measuring Fixedness To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness that we have developed in a previous study (Fazly and Stevenson, 2006), as well as some new measures we introduce here. The following paragraphs give a brief description of each. Fixednesslex quantifies the degree of lexical fixedness of the target combination, ≺v , n≻, by comparing its strength of association (measured by PMI) with those of its lexical variants. Like Lin (1999), we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of Lin (1998). We then use a standard statistic, the z -score, to calculate Fixednesslex : . Fixednesslex (v , n) = PMI(v , n) − PMI std (2) where PMI is the mean and std the standard deviation over the PMI of the target and all its variants. Fixednesssyn quantifies the degree of syntactic fixedness of the target combination, by comparing its behaviour in text with the behaviour of a typical verb–object, both defin"
W07-1102,W03-1810,0,0.113105,"performance of this group is notably higher than ALL). On the other hand, INST features have a very poor performance on these classes, reinforcing that IDMs and LVCs may not necessarily appear with significantly high frequency of occurrence in a given corpus. Fixedness features thus prove to be 8 Since this is a binary feature, it can only distinguish two classes. In the future, we need to include more semantic classes. 15 Much recent work on classifying MWEs focuses on determining different levels of compositionality in verb+particle combinations using a measure of distributional similarity (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Another group of research attempts to classify a particular MWE subtype, such as verb-particle constructions (VPCs) or LVCs, according to some fine-grained semantic criteria (Wanner, 2004; Uchiyama et al., 2005; Cook and Stevenson, 2006). Here, we distinguish subtypes of MWEs that are defined according to coarse-grained distinctions in their degree of semantic idiosyncrasy. Wermter and Hahn (2004) recognize the importance of distinguishing MWE subtypes that are similar to our four classes, but only focus on separating MWEs as one single class from"
W07-1102,H05-1113,0,0.0706402,"to their expected high frequency. Krenn and Evert (2001) attempt to separate German idioms, LVCs, and literal phrases (of the form verb+prepositional phrase). They treat LVCs and idioms as institutionalized expressions, and use frequency and several association measures, such as PMI, for the task. The main goal of their work is to find which association measures are particularly suited for identifying which of these classes. Here, we look at properties of MWEs other than their institutionalization (the latter we quantify using an association measure). The work most similar to ours is that of Venkatapathy and Joshi (2005). They propose a minimallysupervised classification schema that incorporates a variety of features to group verb+noun combinations according to their level of compositionality. Their work has the advantage of requiring only a small amount of manually-labeled training data. However, their classes are defined on the basis of compositionality only. Here, we consider classes that are linguistically salient, and moreover need special treatment within a computational system. Our work is also different in that it brings in a new group of features, the fixedness measures, which prove to be very effect"
W07-1102,C04-1141,0,0.0121735,"n classifying MWEs focuses on determining different levels of compositionality in verb+particle combinations using a measure of distributional similarity (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Another group of research attempts to classify a particular MWE subtype, such as verb-particle constructions (VPCs) or LVCs, according to some fine-grained semantic criteria (Wanner, 2004; Uchiyama et al., 2005; Cook and Stevenson, 2006). Here, we distinguish subtypes of MWEs that are defined according to coarse-grained distinctions in their degree of semantic idiosyncrasy. Wermter and Hahn (2004) recognize the importance of distinguishing MWE subtypes that are similar to our four classes, but only focus on separating MWEs as one single class from literal combinations. For this, they use a measure that draws on the limited modifiability of MWEs, in addition to their expected high frequency. Krenn and Evert (2001) attempt to separate German idioms, LVCs, and literal phrases (of the form verb+prepositional phrase). They treat LVCs and idioms as institutionalized expressions, and use frequency and several association measures, such as PMI, for the task. The main goal of their work is to f"
W07-1102,J03-4003,0,\N,Missing
W07-1102,C98-2122,0,\N,Missing
W07-1106,W03-1812,0,0.345058,"s. 1 Introduction Identification of multiword expressions (MWEs), such as car park, make a decision, and kick the bucket, is extremely important for accurate natural language processing (NLP) (Sag et al., 2002). Most MWEs need to be treated as single units of meaning, e.g., make a decision roughly means “decide”. Nonetheless, the components of an MWE can be separated, making it hard for an NLP system to identify the expression as a whole. Many researchers have recently developed methods for the automatic acquisition of various properties of MWEs from corpora (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Villada Moir´on and Tiedemann, 2006; Most of these methods have been aimed at recognizing MWE types; less attention has been paid to the identification of instances (tokens) of MWEs in context. For example, most such techniques (if successful) would identify make a face as a potential MWE. This expression is, however, ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons. Despite the common perception that phrases tha"
W07-1106,E06-1042,0,0.147767,"zly and Stevenson, 2006). Much research has addressed the non-compositionality of MWEs as an important property related to their idiomaticity, and has used it in the classification of both MWE types and tokens (Baldwin et al., 2003; McCarthy et al., 2003; Katz and Giesbrecht, 2006). We also make use of this property in an MWE token classification task, but in addition, we draw on other salient characteristics of MWEs which have been previously shown to be useful for their type classification (Evert et al., 2004; Fazly and Stevenson, 2006). The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. Our results suggest that such properties are often more informative than the local 47 context, in determining the class of an MWE token. The supervised classifier of Patrick and Fletcher (2005) distinguishes between compositional and non-compositional English verb-particle construction tokens. Their classifier incorporates linguistically-motivated features, such as the degree of separation between the verb and particle. Here, we focus on a"
W07-1106,evert-etal-2004-identifying,0,0.220131,"tion, and draw on the local context of an expression to disambiguate it. Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). Pre41 Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41–48, c Prague, June 2007. 2007 Association for Computational Linguistics vious work on the identification of MWE types, however, has found other properties of MWEs, such as their syntactic fixedness, to be relevant to their identification (Evert et al., 2004; Fazly and Stevenson, 2006). In this paper, we propose techniques that draw on this property to classify individual tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward classification techniques that combine such information with evidence from the local context of an MWE. We explore the hypothesis that informative prior knowledge about the overall syntactic behaviour of an idiomatic expression (type-based knowledge) can be used to determine whether an instance of the expression is used literally or idiomatically (tokenbased knowledge). Based on this hypothesis"
W07-1106,E06-1043,1,0.917289,"e local context of an expression to disambiguate it. Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). Pre41 Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41–48, c Prague, June 2007. 2007 Association for Computational Linguistics vious work on the identification of MWE types, however, has found other properties of MWEs, such as their syntactic fixedness, to be relevant to their identification (Evert et al., 2004; Fazly and Stevenson, 2006). In this paper, we propose techniques that draw on this property to classify individual tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward classification techniques that combine such information with evidence from the local context of an MWE. We explore the hypothesis that informative prior knowledge about the overall syntactic behaviour of an idiomatic expression (type-based knowledge) can be used to determine whether an instance of the expression is used literally or idiomatically (tokenbased knowledge). Based on this hypothesis, we develop unsupervised me"
W07-1106,P06-2046,0,0.119525,"ocal 47 context, in determining the class of an MWE token. The supervised classifier of Patrick and Fletcher (2005) distinguishes between compositional and non-compositional English verb-particle construction tokens. Their classifier incorporates linguistically-motivated features, such as the degree of separation between the verb and particle. Here, we focus on a different class of English MWEs, verb+noun combinations. Moreover, by making a more direct use of their syntactic behaviour, we develop unsupervised token classification methods that perform well. The unsupervised token classifier of Hashimoto et al. (2006) uses manually-encoded information about allowable and non-allowable syntactic transformations of Japanese idioms—that are roughly equivalent to our notions of canonical and non-canonical forms. The rule-based classifier of Uchiyama et al. (2005) incorporates syntactic information about Japanese compound verbs (JCVs), a type of MWE composed of two verbs. In both cases, although the classifiers incorporate syntactic information about MWEs, their manual development limits the scalability of the approaches. Uchiyama et al. (2005) also propose a statistical token classification method for JCVs. Th"
W07-1106,W06-1203,0,0.751428,"as a word frequency vector ~ve where each dimension i of ~ve is the frequency with which e co-occurs with word i across the usages of e. We similarly estimate the meaning of a single token of an expression t as a vector ~vt capturing that usage. To determine if an instance of an expression is literal or idiomatic, we compare its co-occurrence vector to the co-occurrence vectors representing each of the literal and idiomatic meanings of the expression. We use a standard measure of distributional similarity, 43 cosine, to compare co-occurrence vectors. In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manuallyannotated training data. Here, we propose unsupervised methods for estimating these vectors. We use one way of estimating the idiomatic meaning of an expression, and two ways for estimating its literal meaning, yielding two methods for token classification. Our first Diff method draws further on our expectation that canonical forms are more likely idiomatic usages, and non-canonical forms are more likely literal usages. We estimate the idiomatic meaning of an expression by building a co-occurrence vector, ~ vI -"
W07-1106,P99-1041,0,0.846778,"te-of-the-art supervised techniques. 1 Introduction Identification of multiword expressions (MWEs), such as car park, make a decision, and kick the bucket, is extremely important for accurate natural language processing (NLP) (Sag et al., 2002). Most MWEs need to be treated as single units of meaning, e.g., make a decision roughly means “decide”. Nonetheless, the components of an MWE can be separated, making it hard for an NLP system to identify the expression as a whole. Many researchers have recently developed methods for the automatic acquisition of various properties of MWEs from corpora (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Villada Moir´on and Tiedemann, 2006; Most of these methods have been aimed at recognizing MWE types; less attention has been paid to the identification of instances (tokens) of MWEs in context. For example, most such techniques (if successful) would identify make a face as a potential MWE. This expression is, however, ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons. D"
W07-1106,W03-1810,0,0.196615,"tification of multiword expressions (MWEs), such as car park, make a decision, and kick the bucket, is extremely important for accurate natural language processing (NLP) (Sag et al., 2002). Most MWEs need to be treated as single units of meaning, e.g., make a decision roughly means “decide”. Nonetheless, the components of an MWE can be separated, making it hard for an NLP system to identify the expression as a whole. Many researchers have recently developed methods for the automatic acquisition of various properties of MWEs from corpora (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Villada Moir´on and Tiedemann, 2006; Most of these methods have been aimed at recognizing MWE types; less attention has been paid to the identification of instances (tokens) of MWEs in context. For example, most such techniques (if successful) would identify make a face as a potential MWE. This expression is, however, ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons. Despite the common perception that phrases that can be idioms are mai"
W07-1106,H05-1113,0,0.0624968,"Missing"
W07-1106,W06-2405,0,0.133843,"Missing"
W07-1106,J03-4003,0,\N,Missing
W08-2108,J93-2003,0,0.0111514,"ed in choosing the referent of a familiar as opposed to a novel target word (as noted by Halberda, 2006). Moreover, the overall behaviour of our model confirms that the probabilistic bootstrapping approach to word learning naturally leads to the onset of fast mapping in the course of lexical development, without hard-coding any specialized learning mechanism into the model to account for this phenomenon. 2 Overview of the Computational Model This section summarizes the model presented in Fazly et al. (2008). Our word learning algorithm is an adaptation of the IBM translation model proposed by Brown et al. (1993). However, our model is incremental, and does not require a batch process over the entire data. 2.1 Utterance and Meaning Representations The input to our word learning model consists of a set of utterance–scene pairs that link an observed scene (what the child perceives) to the utterance that describes it (what the child hears). We represent each utterance as a sequence of words, and the correspond58 ing scene as a set of meaning symbols. To simulate referential uncertainty (i.e., the case where the child perceives aspects of the scene that are unrelated to the perceived utterance), we includ"
W08-2112,W00-0717,0,0.95218,"/creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as on the distributional information about the contexts in which they appear. Several computational models have been proposed that draw on one or more of the above-mentioned properties in order to group words into discrete unlabeled categories. Most existing models only intend to show the relevance of such properties to the acquisition of adult-like syntactic categories such as nouns and verbs; hence, they do not necessarily incorporate the types of learning mechanisms used by children (Sch¨utze, 1993; Redington et al., 1998; Clark, 2000; Mintz, 2003; Onnis and Christiansen, 2005). For example, in contrast to the above models, children acquire their knowledge of syntactic categories incrementally, processing the utterances they hear one at a time. Moreover, children appear to be sensitive to the fact that syntactic categories are partially defined in terms of other categories, e.g., nouns tend to follow determiners, and can be modified by adjectives. We thus argue that a computational model should be incremental, and should use more abstract category knowledge to help better identify syntactic categories. Incremental processi"
W08-2112,P07-1094,0,0.0320359,"artwright and Brent (1997) propose 95 an incremental model of syntactic category acquisition that uses a series of linguistic preferences to find common patterns across sentence-length templates. Their model presents an important incremental algorithm which is very effective for discovering categories in artificial languages. However, the model’s reliance on templates limits its applicability to transcripts of actual spoken language data, which contain high variability and noise. Recent models that apply Bayesian approaches to PoS tagging are not incremental and assume a fixed number of tags (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008). In syntactic category acquisition, the true number of categories is unknown, and must be inferred from the input. 8 Conclusions and Future Directions We have developed a computational model of syntactic category acquisition in children, and demonstrated its behaviour on a corpus of naturalistic childdirected data. The model is based on domain-general properties of feature similarity, in contrast to earlier, more linguistically-specific methods. The incremental nature of the algorithm contributes to a substantial improvement in psychological plausibility over pre"
W08-2112,P93-1034,0,0.0997936,"Missing"
W09-2010,P06-2005,0,0.62746,"tivity, and hence many novel lexical items, in the language of text messaging, or texting language. Normalization of non-standard forms— converting non-standard forms to their standard forms—is a challenge that must be tackled before other types of natural language processing can take place (Sproat et al., 2001). In the case of text messages, text-to-speech synthesis may be 1 The number of characters in a text message may also be limited to 160 characters, although this is not always the case. particularly useful for the visually impaired; automatic translation has also been considered (e.g., Aw et al., 2006). For texting language, given the abundance of creative forms, and the wide-ranging possibilities for creating new forms, normalization is a particularly important problem, and has indeed received some attention in computational linguistics (e.g., Aw et al., 2006; Choudhury et al., 2007; Kobus et al., 2008). In this paper we propose an unsupervised noisy channel method for texting language normalization, that gives performance on par with that of a supervised system. We pursue unsupervised approaches to this problem, as large collections of text messages, and their corresponding standard forms"
W09-2010,fairon-paumier-2006-translated,0,0.031005,"ve phenomena similar to text messaging, although at a lower frequency (Ling and Baron, 2007). Moreover, technological changes, such as new input devices, are likely to have an impact on the language of such media (Thurlow, 2003).3 An unsupervised approach, drawing on linguistic properties of creative word formations, has the potential to be adapted for normalization of text in other similar genres—such as Internet discussion forums—without the cost of developing a large training corpus. Moreover, normalization may be particularly important for such genres, given the 2 One notable exception is Fairon and Paumier (2006), although this resource is in French. The resource used in our study, Choudhury et al. (2007), is quite small in comparison. 3 The rise of other technology, such as word prediction, could reduce the use of abbreviations, although it’s not clear such technology is widely used (Grinter and Eldridge, 2001). 71 Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 71–78, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Formation type Stylistic variation Subseq. abbrev. Prefix clipping Syll. letter/digit G-clipping Phonetic"
W09-2010,N07-1047,0,0.035286,"d of the word; P (ht |ps , hs , pos) is the probability of texting form graphemes ht given the standard form phonemes ps and graphemes hs at position pos. ht , ps , and hs can be a single grapheme or phoneme, or a bigram. We compute edit-probability between the graphemes of si and ti . When filling each cell in the chart, we consider edit operations between segments of si and ti of length 0–2, referred to as a and b, respectively. If a aligns with phonemes in si , we also consider those phonemes, p. In our lexicon, the graphemes and phonemes of each word are aligned according to the method of Jiampojamarn et al. (2007). For example, the alignment for without is given in Table 2. The probability of each edit operation is then determined by three properties—the length of a, whether a aligns with any phonemes in si , and if so, p—as shown below: |a|= 0 or 1, not aligned w/ si phonemes: P (b|a, pos ) |a|= 2, not aligned w/ si phonemes: 0 |a|= 1 or 2, aligned w/ si phonemes: P (b|p, a, pos ) 3.1.2 Subsequence Abbreviations We model subsequence abbreviations according to the equation below: ( c if ti is a subseq of si P (ti |si , subseq abrv) = 0 otherwise where c is a constant. Note that this is similar to the e"
W09-2010,C08-1056,0,0.458132,"., 2001). In the case of text messages, text-to-speech synthesis may be 1 The number of characters in a text message may also be limited to 160 characters, although this is not always the case. particularly useful for the visually impaired; automatic translation has also been considered (e.g., Aw et al., 2006). For texting language, given the abundance of creative forms, and the wide-ranging possibilities for creating new forms, normalization is a particularly important problem, and has indeed received some attention in computational linguistics (e.g., Aw et al., 2006; Choudhury et al., 2007; Kobus et al., 2008). In this paper we propose an unsupervised noisy channel method for texting language normalization, that gives performance on par with that of a supervised system. We pursue unsupervised approaches to this problem, as large collections of text messages, and their corresponding standard forms, are not readily available.2 Furthermore, other forms of computer-mediated communication, such as Internet messaging, exhibit creative phenomena similar to text messaging, although at a lower frequency (Ling and Baron, 2007). Moreover, technological changes, such as new input devices, are likely to have an"
W09-2010,P02-1019,0,0.501736,"th machine translation and automatic speech recognition for text message normalization. However, both of these approaches are supervised, and have only limited means for normalizing texting forms that do not occur in the training data. Our work, like that of Choudhury et al. (2007), can be viewed as a noisy-channel model for spelling error correction (e.g., Mays et al., 1991; Brill and Moore, 2000), in which texting forms are seen as a kind of spelling error. Furthermore, like our approach to text message normalization, approaches to spelling correction have incorporated phonemic information (Toutanova and Moore, 2002). The word model of the supervised approach of Choudhury et al. consists of hidden Markov models, which capture properties of texting language similar to those of our stylistic variation model. We propose multiple word models—corresponding to frequent texting language formation processes—and an unsupervised method for parameter estimation. 7 Conclusions We analyze a sample of texting forms to determine frequent word formation processes in creative texting language. Drawing on these observations, we construct an unsupervised noisy-channel model for text message normalization. On an unseen test"
W09-2010,P00-1037,0,\N,Missing
W10-2109,P07-2009,0,0.018145,"date the factors influencing interpretation of such sentences across a collection of actual usages. The second reason for our interest in this construction is that it illustrates a complex ambiguity that can cause difficulty for natural language processing applications that seek to semantically interpret text. Faced with the above two sentences, a parsing system (in the absence of specific knowledge of this construction) will presumably find the exact same structure for each, giving no basis on which to determine the correct meaning from the parse. (Unsurprisingly, when we run the C&C Parser (Curran et al., 2007) on (1) and (2) it assigns the same structure to each sentence.) Our second goal in this work is thus to explore whether increased linguistic understanding of this phenomenon could be used to disambiguate such examples automatically. Specifically, we use this construction as an example of the kind of difficulties faced in semantic interpretation when meaning may be determined by pragmatic or other extra-syntactic factors, in order to explore whether We consider sentences of the form No X is too Y to Z, in which X is a noun phrase, Y is an adjective phrase, and Z is a verb phrase. Such construc"
W10-2109,D08-1027,0,0.011597,"Missing"
W10-2109,H05-1044,0,0.0208223,"ebatable paths we’ve taken in our war against terror? (7) No neighborhood is too remote to diminish Mr. Levine’s determination to discover and announce some previously unheralded treat. Polarity Because of the observation that the verb in the target construction, in particular, has some property of negativity in the “no” interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance. The features are tertiary, representing positive, neutral, or negative polarity. We obtain polarity information from the subjectivity lexicon provided by Wilson et al. (2005), and consider words to be neutral if they have both positive and negative polarity, or are not in the lexicon. (8) No one is too remote anymore to be concerned about style, Ms. Hansen suggested. In example (6) the author is using the target construction to express somebody else’s viewpoint that “any amount should be spent on the war against terror”. Therefore the literal reading of the target construction appears to be the “every” interpretation. However, this construction is being used rhetorically (as part of the overall sentence) to express the author’s belief that “too much money is being"
W10-2109,C00-1028,0,0.0390032,"Missing"
W10-2109,J02-2003,0,0.0250003,"Missing"
W10-2109,J03-4003,0,0.00650375,"ated to a verb, as in No interest is too narrow for attention. (We would only extract the latter if there were an infinitive verb embedded in or following the NP.) In the present study we limit our consideration to sentences of the form discussed by Wason and Reich (1979), but intend to consider related constructions such as these—which appear to exhibit the same ambiguity as the target construction—in the future. We next manually identify the noun, adjective, and verb that participate in the target construction in each sentence. Although this could be done automatically using a parser (e.g., Collins, 2003) or chunker (e.g., Abney, 1991), here we want to ensure error-free identification. We also note a number of sentences containing co-ordination, such as in the following example. 3.2 Annotation We used Amazon Mechanical Turk (AMT, https://www.mturk.com/) to obtain judgements as to the correct interpretation of each instance of the target construction in both the development and testing datasets. For each instance, we generated two paraphrases, one corresponding to each of the interpretations discussed in Section 1. We then presented the given instance of the target construction along with its t"
W11-0910,P98-1013,0,0.229848,"of the construction across verb classes in the lexicon and evaluate against VerbNet. We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages. 1 Introduction Automatic semantic analysis has been very successful when taking a supervised learning approach on data labeled with sense tags and semantic roles (e.g., see Màrquez et al., 2008). Underlying these recent successes are lexical resources, such as PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998; Fillmore et al., 2002), which encode the relational semantics of numerous lexical items, especially verbs. However, because authors and speakers use verbs productively in previously unseen ways, semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources (cf. Pustejovsky & Jezek, 2008). To achieve more accurate semantic analyses, we must augment such resources with knowledge of the extensibility of verbs. Central to verb extensibility is the process of semantic and syntactic coercion. Coercion allows a verb to be used"
W11-0910,P98-1046,1,0.549854,"that coerce the meaning of blink to fit with a CM event would currently be misanalysed. One option might be to augment the Hiccup class with the CM frame from the Pour class, which would ensure that such sentences would be analyzed more accurately. However, given the productive nature of constructional coercion and its widespread applicability, the approach of adding any possible pattern to each class is not appropriate: this would undermine the definitional distinctions between classes and greatly lessen their usefulness. Complicating the issue is the phenomenon of regular sense extensions (Dang et al., 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb. For example, the verbs in the Push class express the general meaning of exerting force on an object, such as She pushed on the wall. Often, the exertion of force moves the object, which can be expressed in a CM construction such as She pushed the box across the room. VerbNet accounts for this regular sense extension by including most of the Push verbs in the Carry class as well, which has the CM construction as one of its frames. Deciding when to include a verb in another class base"
W11-0910,P10-1160,0,0.0310982,"project the syntactic structures that encode that information. Verbs are also highly variable, displaying a rich range of semantic and syntactic behavior. Verb classifications help NLP systems to deal with this complexity by organizing verbs into groups that share core semantic and syntactic properties. For example, VerbNet (derived from Levin‟s [1993] work, Kipper et al., 2008) is widely used for a number of semantic processing tasks, including semantic role labeling (Swier and Stevenson, 2004), the creation of semantic parse trees (Shi and Mihalcea, 2005), and implicit argument resolution (Gerber and Chai, 2010). The detailed semantic predicates listed with each VerbNet class also have the potential to contribute to textspecific semantic representations and, thereby, to tasks requiring inferencing (Zaenen et al., 2008; Palmer et al., 2009). VerbNet identifies semantic roles and syntactic patterns characteristic of the verbs in each class makes explicit the connections between the syntactic patterns and the underlying semantic relations that can be inferred for all members of the class. Each syntactic frame in a class has a corresponding semantic representation that details the semantic relations betw"
W11-0910,J08-2001,1,0.827612,"bNet fails to accurately capture all usages of the construction. We use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against VerbNet. We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages. 1 Introduction Automatic semantic analysis has been very successful when taking a supervised learning approach on data labeled with sense tags and semantic roles (e.g., see Màrquez et al., 2008). Underlying these recent successes are lexical resources, such as PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998; Fillmore et al., 2002), which encode the relational semantics of numerous lexical items, especially verbs. However, because authors and speakers use verbs productively in previously unseen ways, semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources (cf. Pustejovsky & Jezek, 2008). To achieve more accurate semantic analyses, we must augment such resource"
W11-0910,J05-1004,1,0.546474,"Missing"
W11-0910,W10-0801,1,0.87385,"Missing"
W11-0910,E03-1073,0,0.0325187,"nt knowledge about verbs in a lexical resource. Importantly, constructional coercion is not an all-ornothing process – a word must be semantically and syntactically compatible in some respects with a context in order for its use to be extended to that context, but the restrictions on compatibility are not hard-and-fast rules (Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 2006; Goldberg, to appear). Gradience of compatibility plays an important role in coercion, suggesting that a probabilistic approach may be necessary for encoding knowledge of constructional coercion in a verb lexicon (cf. Lapata & Lascarides, 2003). Our hypothesis here is that, due to this gradient process of productivity, existing verb lexicons do not adequately capture the actual patterns of use of extensible constructions. In this paper, we focus on the CAUSEDMOTION (CM) construction as an initial test case. We first annotate the classes of an extensive verb lexicon, VerbNet, as to whether the CM construction is allowed for all, some, or none of the verbs in the class, noting additionally whether it is a typical or coerced usage. We find that many of the classes that allow the construction for at least some verbs do not include the C"
W11-0910,W04-3213,1,\N,Missing
W11-0910,zaenen-etal-2008-encoding,0,\N,Missing
W11-0910,C98-1046,1,\N,Missing
W11-0910,C98-1013,0,\N,Missing
W13-3525,W12-1701,1,0.498157,"An example input frame. The Syntactic features We require an incremental model in which we can examine developmental patterns as it gradually learns relevant aspects of argument structures. This task calls for an ability to represent the semantic and syntactic properties of verb usages, including those containing MSVs and other kinds of verbs taking sentential complements (SCs). Most computational models of verb argument structure acquisition have largely focused on physical action verbs (Alishahi and Stevenson, 2008; Chang, 2009; Perfors et al., 2010; Parisien and Stevenson, 2011). Recently, Barak et al. (2012) extended the incremental Bayesian model of Alishahi and Stevenson (2008) to include the syntactic and semantic features required for the processing of MSVs and other verbs that take SCs. While Barak et al. (2012) modeled some developmental patterns of MSVs overall, their work did not account for the difference between Desire and Belief verbs. In this section, we present their model, which we adopt for our experiments. In Section 3, we describe how we modify the representation of the input in Barak et al. (2012) to enable our investigation of the differences among the MSV classes. 2.1 reflect"
W14-2005,P02-1029,0,0.0337973,"es as constructions, and to the groupings of verbs given their distribution over those constructions as verb classes. 3.1 Overview of the Model The model learns from a sequence of frames, where each frame is a collection of features representing what the learner might extract from an utterance s/he has heard. Similarly to previous computational studies (e.g., Parisien and Stevenson, 2010), here we focus on syntactic features since our goal is to understand the acquisition of acceptable syntactic structures of verbs indepen38 for verb classes (Levin, 1993; Merlo and Stevenson, 2000; Schulte im Walde and Brew, 2002). To capture this, our model learns groupings of verbs that have similar distributions across the learned constructions. These groupings form verb classes that provide a higher-level of abstraction over the input; see the top level in Figure 1. Consider the dative alternation: the classes capture the fact that some verbs may occur only in prepositional dative (PD) forms, such as sing, while others occur only in double object (DO) forms (call), while still others alternate – i.e., they occur in both (bring). Our model simultaneously learns both of these types of knowledge: constructions are clu"
W14-2005,W13-3525,1,0.39444,"P (Ftest |v) for each of these, as in: X P (Ftest |v) = P (Ftest |k)P (k|v) Table 1: Number of non-alternating (non-ALT) and alternating (ALT) verbs in our lexicon, as well as the relative frequency of each construction in our generated input corpora. 4 4.1 Experimental Setup Generation of the Input Corpora We follow the input generation method of AS08 to create naturalistic corpora that are based on the distributional properties of verbs over various constructions, as observed in child-directed speech (CDS). Our input-generation lexicon contains 71 verbs drawn from AS08 (11 action verbs) and Barak et al. (2013) (31 verbs of varying syntactic patterns), plus an additional 40 of the most frequent verbs in CDS, in order to have a range of verbs that occur with the PD and DO constructions. Table 4.1 shows the number of verbs that appear in the DO or PD construction only (nonalternating), as well as those that alternate across the two. (The table also gives the relative frequency of each dative construction in our generated input corpora.) Each verb lexical entry includes its overall frequency, and its relative frequency with each of a number of observed syntactic constructions. The frequencies are extra"
W14-2005,J01-3003,1,\N,Missing
W14-2006,E12-1024,0,0.0322867,"ed. Starting from initially small and concrete representations, it learns incrementally long representations (syntagmatic growth) as well as more abstract ones (paradigmatic growth). Several models address either paradigmatic (Alishahi and Stevenson, 2008; Chang, 2008; Bannard et al., 2009) or syntagmatic (Freudenthal et al., 2010) growth. This model aims to explain both, thereby contributing to the understanding of how different learning mechanisms interact. As opposed to other models involving grammars with semantic representations (Alishahi and Stevenson, 2008; Chang, 2008), but similar to Kwiatkowski et al. (2012), the model starts without an inventory of mappings of single words to meanings. The representations and processes yielding the limited length and telegraphic style of language production early on in acquisition have received little attention in acquisitional modeling. In this paper, we present a model, starting with minimal linguistic representations, that incrementally builds up an inventory of increasingly long and abstract grammatical representations (form+meaning pairings), in line with the usage-based conception of language acquisition. We explore its performance on a comprehension and a"
W15-2412,W12-1708,1,0.837467,"as frequency. We are currently planning to extend this research to a variety of error data sets, both in English and other languages, to see if similar results are found and to further evaluate the role of the various perceptual, typological, and frequency factors. Another issue we plan to work on is the fact that the model performs ‘too well’: It predicts no overextensions for 6 out of the 8 color stimuli, despite children displaying a few errors on 4 of these colors. Using our typologically-derived semantic space within a fuller model of word learning, such as that of Fazly et al. (2010) or Nematzadeh et al. (2012), rather than using a simple categorization model as we do here, might further our insight into potential sources of overextensions. Given our general methodological approach, reviewers noted other interesting possibilities and suggested that alternative design choices are possible as well for the dimensionality reduction technique, the alignment method between predicted model data and observed experimental data, and the statistical evaluation procedure. We plan to follow up on these suggestions in future research, in addition to the exploration of a wider set of crosslinguistic error patterns"
W18-0105,D14-1031,1,0.931396,"”,“ FROG ”) have in common, the more similar their learned representations. The model learns not only the features associated with that particular word, however, but also features that often occur in the same context as the word. For example, in the above utterance–scene pair, the model may come to associate a non-zero probability with the feature FRUIT and the word monkey. Hence, the learned meanings of words capture not only a conceptual hierarchy for that word but also information learned from the context of their usage. Pt (ai j |fi ) = Incremental Network Creation We use the approach of Nematzadeh et al. (2014) to incrementally build a semantic network, which draws on the probabilistic cross-situational word learning model developed by Fazly et al. (2010). 2.1 Incremental Word Learning Model The semantic network is generated from word meanings (representations) learned by the model of Fazly et al. (2010), trained on the Manchester corpus (Theakston et al., 2001) of the CHILDES database (MacWhinney, 2000). Each input to the model consists of an utterance from the corpus, labelled with a scene consisting of semantic features for each word. For example, consider the following utterance (U) and selected"
W18-0106,R13-1019,0,0.0145188,"as we did not observe it in bilinguals’ L1 Dutch. 6 All words were lemmatized, and hapax legomena and multiword responses were removed. 7 http://freedict.org 8 http://www.dict.cc 9 http://www.opensubtitles.org 5 We use B1-1 ∪ B2 , as this combined data set provides more responses for the comparison; using B1-1 or B2 instead gives very similar results. 49 forms. Cognates are believed to enjoy a special status in bilinguals (van Hell and de Groot, 1998; Voga and Grainger, 2007). These edges are defined using a similarity measure S, which is complementary to the normalized Levenshtein distance (Ciobanu and Dinu, 2013). Given two words wi and wj , S is computed as: S(wi , wj ) = 1 − L(wi , wj ) max(|wi |, |wj |) (4) where L(wi , wj ) is the Levenshtein distance between the words, and |w |is the number of characters in w. We consider wi and wj to be cognates when they are translation equivalents in our dictionary, and S(w1 , w2 ) ≥ 0.5. This rather low threshold was chosen to capture cognates that are spelled differently due to morphological or etymological reasons, yet are similar in their pronunciation: swell–zwellen, photography–fotografie, etc. Finally, we consider two extra types of edges, which connect"
W18-0106,N06-1014,0,0.0871496,"way, using the English association norms. The two resulting sub-networks are then connected to each other with two following types of edges. Translation equivalent edges (TE) connect nodes that are translations of each other. Translations are obtained from two dictionaries: FreeDict7 and dict.cc.8 In many cases a node n has more than one translation (e.g., a and b). To determine which one is more frequent, we use OpenSubtitles,9 a bilingual corpus of Dutch–English subtitles (Lison and Tiedemann, 2016). Word alignment was performed on a random sample of 50 million sentences using the method of Liang et al. (2006), and conditional probabilities of each Dutch–English and English–Dutch translation were extracted. If a and b are translations of node n, edges Ena and Enb are weighted proportionally to the conditional probabilities p(a|n) and p(b|n). Cognate edges (CG) are placed between translation equivalents that have similar orthographic To summarize our human data analyses, we have shown quantitatively that Dutch–English bilinguals give systematically different responses in English (their L2) from English monolinguals. While such a difference has long been observed, to our knowledge we are the first to"
W18-0106,W14-0509,0,0.0153149,"at any between-group difference we find is due to more than the variation in responses among bilinguals. Existing computational models 3.1 Graph-based models (or semantic networks) have been widely used in research on semantic memory (see an overview by Beckage and Colunga, 2016). Despite their ‘localist’ approach in which a word is simply represented by a node (rather than using distributed representations), such models are a useful tool in the study of lexical access and acquisition. In particular, they have successfully replicated patterns of human verbal behavior in free word association (Enguix et al., 2014; Gruenenfelder et al., 2015), semantic fluency tasks (Abbott et al., 2015; Nematzadeh et al., 2016), lexical growth/acquisition (Stella et al., 2017; Bilson et al., 2015), assessment of semantic similarity (Jackson and Bolger, 2014; De Deyne et al., 2016), etc. Naturally, a graph is only a static representation of the lexicon, although its structure presumably reflects lexical processing (Beckage and Colunga, 2016). To simulate the actual processing dynamDistance measures Our goal is to compare two sets of responses to a particular cue word against each other. For this, we use two measures. T"
W18-0106,L16-1147,0,0.033718,"al to conditional probabilities p(response|cue) obtained from the norms. L2 associative edges (EA) are created the same way, using the English association norms. The two resulting sub-networks are then connected to each other with two following types of edges. Translation equivalent edges (TE) connect nodes that are translations of each other. Translations are obtained from two dictionaries: FreeDict7 and dict.cc.8 In many cases a node n has more than one translation (e.g., a and b). To determine which one is more frequent, we use OpenSubtitles,9 a bilingual corpus of Dutch–English subtitles (Lison and Tiedemann, 2016). Word alignment was performed on a random sample of 50 million sentences using the method of Liang et al. (2006), and conditional probabilities of each Dutch–English and English–Dutch translation were extracted. If a and b are translations of node n, edges Ena and Enb are weighted proportionally to the conditional probabilities p(a|n) and p(b|n). Cognate edges (CG) are placed between translation equivalents that have similar orthographic To summarize our human data analyses, we have shown quantitatively that Dutch–English bilinguals give systematically different responses in English (their L2"
W98-1116,P97-1003,0,0.153094,"etain structural knowledge, such as phrase structween a main clause and reduced relative conture, subcategorization and long distance destruction. We measure the probability distripendencies. So they are equally capable of butions of several linguistic features (transitivmodelling the fine lexical idiosyncrasies and tile ity. tense, voice) over a sample of optionally inmore general syntactic regularities. transitive verbs. In agreement with recent reGiven an annotated training corpus, such suits on parsing with lexicalised probabilistic methods learn its distributions (the lexical cogrammars (Collins, 1997; Srinivas, 1997), we occurrences), which requires being given the find that statistics over lexical, as opposed to correct space of events in the m o d e l - - t h a t is, structural, features best correspond to human the g r a m m a r - - a c c u r a t e l y enough that they can intuitive .judgments and to experimental findparse new instances of the same c o r p u s . The ings. These results are enlightening to invessuccess of such models suggests that a statistitigate novel uses of corpora, by assessing the cal model nmst have access to tile appropriate portability of statistics across task"
W98-1116,C94-2149,0,0.0329316,"d abow,, correspond to a different elementary tree. im:luding the unergative and unaccusative distinction, encoded by different labels referring to theinatic roles. Current LTAG part-of-speech taggers, called supertaggets (Joshi and Srinivas, 1994; Srinivas, 1997) assign a set of elementary trees to each word, in effect chunking the text. The counts performed in the study reported here would have required simply counting the occurrences of the labels assigned to the words in the text by such a supertagger. Refinements in this direction of the annotation of the grammar used by the XTAG system (Doran et al., 1994) are actually tinder way. We also can see, from the raw frequencies obtained, that when collecting counts about syntactic phenomena, corpora must be in the order of hundreds of millions of words for the statistics to be reliable. 4 Conclusions Our main result in this paper is that statistics over lezical features best correspond to independently established human intuitive judgments. We have argued that, methodologically, this result casts light on the relationship between different data collection methods, and shows that some apparently contradictory results can be reconciled by defining prob"
W98-1116,P95-1034,0,0.0241127,"Missing"
W98-1116,W97-0301,0,0.0226157,"Missing"
W98-1116,P98-2177,0,0.0240221,"dings is coherent and in accordance with current dew~lopments in statistical parsing and grammati,:al theory in two important respects. First, the discrepancy between the frequencies of each of the lexical features and the frequencies of the m:tual construction suggests that the frequency of a construction is a composition fimction of (at least some of) its lexical features, even if such t};atures are not-independent. Models that can handle non-independent lexical features have given very good results both for part-of-speech and structural disambiguation (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Ratnaparkhi, 1998). Second, we observe that the lexical and sublexical features we counted are not sufficient to identify all the relevant linguistic classes: statistical tests fail to differentiate between unaccusatives and object-drop verbs. In order to distinguish between these two classes of verbs one needs to look at some of the surrounding context. This result is expected. Performance measures of statistical parsers show that statistics based on one word give poor results, but that statistics on bigrams have much better per139 formance (Charniak, 1997). 3 General Discussion 3.1 Relationship b e t w e e n"
W98-1116,P98-2184,0,0.0189322,"ethod to collect data: tbr example: frequencybased preferences are not used by hmnans; the wi'ong frequencies had been ,:omltcd: experimental results are not representative of natural linguistic behaviour: or corpora are not representative of natural linguistic behaviom'. The findings in this study show a way of reconciling results obtained by different data collection methods: if we count at the level of lexical and sublexical features, we find that differences in native speakers' preferences do correspond to significant differences in distributions. Similar conclusions are being reached in (Roland and Jurafsky, 1998), who compare different corpora. 3.2 Classification Properties of Lexical Features and C o n s e q u e n c e s Looking at the frequencies of the Iexical features in Table 2, we can observe that P12T, PASS and TRANS have counts that can be used to directly predict the difficulty of the 1212.construction. This observation can be used beneficially in a task different fl'om parsing, for instance in a generation system. Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructic,ns are generated and then filtered based on a sl~atistical model. If th"
W98-1116,E91-1006,0,0.0238761,"Missing"
W98-1116,C98-2179,0,\N,Missing
Y05-1003,W03-1812,0,0.0495243,"iate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moir´on, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of multiword expressions either treats them as collocations (Smadja, 1993), or examines the distributional similarity between an expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of"
Y05-1003,W02-2001,0,0.013689,"he two sets of numbers, averaged across all light verbs and semantic classes. The results show that our linguistically-motivated measure of acceptability has smaller divergence when compared with the collocation-based PMI measures. 5. General Discussion Recently there has been a growing understanding both of the need for the appropriate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moir´on, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of m"
Y05-1003,W03-1809,0,0.068336,"e need for the appropriate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moir´on, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of multiword expressions either treats them as collocations (Smadja, 1993), or examines the distributional similarity between an expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a"
Y05-1003,J91-1003,0,0.0997224,"ductive MWPs, such as verb-particle constructions. The significance of the role metaphor plays in language has long been recognized. However, due to the peculiarities in the behaviour of metaphorical and idiomatic expressions, they have been mostly overlooked by researchers in computational linguistics. Previous studies recognize the challenges these constructions impose on NLP systems (see, e.g., Fellbaum, 2005), but often lack proposals for robust and wide-coverage mechanisms to handle them. Some work has relied on the existence of expensive resources such as manually-built knowledge bases (Fass, 1991; Villavicencio et al., 2004). Mason (2004) incorporates automatically-induced knowledge about the domain of use of a verb to help identify different metaphorical meanings. However, highly polysemous verbs cannot be easily associated with particular domains. Hence such an approach overlooks the great potential of basic verbs in forming metaphorical and idiomatic MWPs. Our work demonstrates that combining statistical approaches with linguistic information is beneficial in devising reliable techniques, both for the acquisition of knowledge about metaphorical and idiomatic MWPs, and for the organ"
Y05-1003,W05-1005,1,0.753669,"ve a groan just now. (b) ?? Azin gave the groan just now. (c) ? Azin gave a couple of groans last night. (d) ?? A groan was given by Azin just now. (e) ?? The groan that Azin gave was very long. In general, the degree to which the light verb retains aspects of its literal meaning—and contributes them compositionally to the LVC—is reflected in the degree of freedom exhibited by the noun component. We have proposed a statistical measure that uses this insight to situate an LV+N combination (a potential LVC) on a scale of literal to metaphorical usage of the light verb (metaphoricity continuum) (Fazly et al., 2005). Our measure assigns a score to each potential LVC, reflecting its syntactic fixedness as determined by the degree of freedom of the noun component. This is approximated as the difference between the strength of association of the potential LVC with two types of syntactic patterns: those that are preferred by LVCs (as in 5(a)), and those that are less preferred by more metaphorical LVCs (as in 5(b–e)). We evaluate our measure of metaphoricity by comparing the ratings it assigns to a set of candidate LV+N combinations with those assigned by human judges. Using the Spearman rank correlation coe"
Y05-1003,E95-1014,0,0.0344983,"latter is estimated by calculating the sum of squared errors between the two sets of numbers, averaged across all light verbs and semantic classes. The results show that our linguistically-motivated measure of acceptability has smaller divergence when compared with the collocation-based PMI measures. 5. General Discussion Recently there has been a growing understanding both of the need for the appropriate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moir´on, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computat"
Y05-1003,P99-1041,0,0.694293,"her aspects of the above-mentioned linguistic properties of idioms and their distributional behaviour in a corpus. We assume that a target VNC is lexically fixed, and hence idiomatic, if it is not open to lexical substitution. This means that we expect to see a notable difference between the idiomaticity level of the target VNC and that of the variants generated by replacing one of its constituents (the verb or noun) with a semantically (and syntactically) similar word. The paradox of this assumption is that it requires knowledge about the idiomaticity of the variant combinations. Inspired by Lin (1999), we moderate this assumption by examining the association strengths of the target combination and its variants, as an indirect cue to their idiomaticity. Target VNCs that are significantly greater in association strength than their variants are assumed to be more lexically fixed, and therefore more idiomatic. Our contribution is a novel technique for incorporating these association strengths into a single measure of lexical fixedness for the target expression. We assume a target VNC is syntactically fixed, and hence idiomatic, if it mainly appears in restricted syntactic constructions. We thu"
Y05-1003,J04-1002,0,0.021837,"ructions. The significance of the role metaphor plays in language has long been recognized. However, due to the peculiarities in the behaviour of metaphorical and idiomatic expressions, they have been mostly overlooked by researchers in computational linguistics. Previous studies recognize the challenges these constructions impose on NLP systems (see, e.g., Fellbaum, 2005), but often lack proposals for robust and wide-coverage mechanisms to handle them. Some work has relied on the existence of expensive resources such as manually-built knowledge bases (Fass, 1991; Villavicencio et al., 2004). Mason (2004) incorporates automatically-induced knowledge about the domain of use of a verb to help identify different metaphorical meanings. However, highly polysemous verbs cannot be easily associated with particular domains. Hence such an approach overlooks the great potential of basic verbs in forming metaphorical and idiomatic MWPs. Our work demonstrates that combining statistical approaches with linguistic information is beneficial in devising reliable techniques, both for the acquisition of knowledge about metaphorical and idiomatic MWPs, and for the organization of such knowledge in a computationa"
Y05-1003,W03-1810,0,0.0476352,"nderstanding both of the need for the appropriate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moir´on, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of multiword expressions either treats them as collocations (Smadja, 1993), or examines the distributional similarity between an expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step fu"
Y05-1003,W97-0311,0,0.0305661,"rrors between the two sets of numbers, averaged across all light verbs and semantic classes. The results show that our linguistically-motivated measure of acceptability has smaller divergence when compared with the collocation-based PMI measures. 5. General Discussion Recently there has been a growing understanding both of the need for the appropriate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moir´on, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previ"
Y05-1003,moiron-2004-discarding,0,0.027862,"Missing"
Y05-1003,J93-1007,0,0.249016,"that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of multiword expressions either treats them as collocations (Smadja, 1993), or examines the distributional similarity between an expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations (VNCs) according to their compositional"
Y05-1003,W04-0401,1,0.89097,"with respect to a semantic class of potential complements. 4.1. Individual LVC Acceptability First, we focus on the individual acceptability of potential LVCs. Although light verbs tend to have similar patterns of cooccurrence with semantically similar complements, they fail to exhibit full generality in their combinations with a semantic class of nouns. Hence, deciding precisely which light verbs combine with which nouns to form acceptable LVCs is a difficult task. Our first solution to this problem treated LV+N combinations as syntactically-dependent collocations, using a PMI-based measure (Stevenson et al., 2004). This measure outperformed the standard PMI measure because it incorporated some knowledge of the preferred LVC pattern (cf. the examples in 5 above). However, while common LVCs typically appear as good collocations, our collocation-based measure failed to take into account other important properties of LVCs. We have devised an improved acceptability measure that brings together some of the linguistic properties of LVCs into a probability formula to determine the likelihood of a given light verb and noun forming an acceptable LVC (Fazly et al., 2005, 2006). The measure captures the general te"
Y05-1003,H05-1113,0,0.0670186,"at involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of multiword expressions either treats them as collocations (Smadja, 1993), or examines the distributional similarity between an expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003). Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds—their lexical fixedness—to identify them. Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work by incorporating lexical fixedness, collocation-based, and distributional similarity measures into a set of features which are used to rank verb+noun combinations (VNCs) according to their compositionality. Our work differs from such approaches in presenting an alternative view on compositionality. More specifically, we carefully examine several linguistic properties of metaphorical and idiomatic MWPs, those that distinguish them from literal (compositional) combinations. These Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language,"
Y05-1003,W03-1808,0,0.0133569,"t that combines with the light verb. Not only does a light verb tend to combine with semantically similar complements, it tends to contribute similar metaphorical meaning to the resulting LVC. Semantic class knowledge may thus enable us to further refine the semantic space of a light verb by elucidating its relation with complements from different classes. Wanner (2004) attempts to perform a similar task by classifying VNCs into predefined groups, each corresponding to a particular semantic relation between the verb and the noun. However, his approach requires manually labelled training data. Villavicencio (2003) uses class-based knowledge to extend a lexicon of verb-particle constructions, but assumes that an unobserved expression is not acceptable. We instead propose that more robust application of class-based knowledge can be achieved with a better estimate of the acceptability of various expressions. While we focus on light verb constructions, we believe that similar techniques can be useful in dealing with other semi-productive MWPs, such as verb-particle constructions. The significance of the role metaphor plays in language has long been recognized. However, due to the peculiarities in the behav"
Y05-1003,villavicencio-etal-2004-multilingual,0,0.0250129,", such as verb-particle constructions. The significance of the role metaphor plays in language has long been recognized. However, due to the peculiarities in the behaviour of metaphorical and idiomatic expressions, they have been mostly overlooked by researchers in computational linguistics. Previous studies recognize the challenges these constructions impose on NLP systems (see, e.g., Fellbaum, 2005), but often lack proposals for robust and wide-coverage mechanisms to handle them. Some work has relied on the existence of expensive resources such as manually-built knowledge bases (Fass, 1991; Villavicencio et al., 2004). Mason (2004) incorporates automatically-induced knowledge about the domain of use of a verb to help identify different metaphorical meanings. However, highly polysemous verbs cannot be easily associated with particular domains. Hence such an approach overlooks the great potential of basic verbs in forming metaphorical and idiomatic MWPs. Our work demonstrates that combining statistical approaches with linguistic information is beneficial in devising reliable techniques, both for the acquisition of knowledge about metaphorical and idiomatic MWPs, and for the organization of such knowledge in"
Y05-1003,H05-1106,0,0.0965581,"5. General Discussion Recently there has been a growing understanding both of the need for the appropriate handling of multiword expressions, and of the complexities involved in the task (Sag et al., 2002). Most research, however, has concentrated on the automatic extraction of these expressions (Grefenstette and Teufel, 1995; Dras and Johnson, 1996; Melamed, 1997; Baldwin and Villavicencio, 2002; Seretan et al., 2003; Moir´on, 2004). Previous studies that focus on learning about semantic properties of multiword expressions, such as their compositionality, have mainly covered compound nouns (Wermter and Hahn, 2005), and verb-particle constructions (McCarthy et al., 2003; Bannard et al., 2003; Baldwin et al., 2003). Our work differs in focusing on those multiword predicates (MWPs) that involve a “basic” verb, a broadly-documented class of expressions that has received relatively little attention within the computational linguistics community. Most previous work on compositionality of multiword expressions either treats them as collocations (Smadja, 1993), or examines the distributional similarity between an expression and its constituents (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003"
Y05-1003,W05-1006,0,0.154831,"ty measures into a set of features which are used to rank verb+noun combinations (VNCs) according to their compositionality. Our work differs from such approaches in presenting an alternative view on compositionality. More specifically, we carefully examine several linguistic properties of metaphorical and idiomatic MWPs, those that distinguish them from literal (compositional) combinations. These Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation include characteristics related not only to lexical fixedness, but to syntactic fixedness as well. Widdows and Dorow (2005) also draw on the notion of syntactic fixedness for idiom detection, though their method is specific to a highly constrained type of idiom. Our work examines a broader range of syntactic patterns associated with a large class of MWPs. We then suggest novel techniques for translating these lexical and syntactic characteristics into measures that predict the level of metaphoricity or idiomaticity of MWPs. Work indicating acceptability of multiword expressions is largely limited to collocational analysis using PMI-based measures (Lin, 1999; Stevenson et al., 2004). In our recent work, we have pro"
Y05-1003,E06-1043,1,\N,Missing
