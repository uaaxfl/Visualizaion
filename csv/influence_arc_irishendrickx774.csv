2010.jeptalnrecital-court.14,2002.jeptalnrecital-long.13,0,0.0206932,"Missing"
2010.jeptalnrecital-court.14,J97-1003,0,0.343444,"Missing"
2010.jeptalnrecital-court.14,sitbon-bellot-2006-tools,0,0.0295025,"Missing"
2016.lilt-14.5,W13-0301,0,0.47917,"Missing"
2016.lilt-14.5,baker-etal-2010-modality,0,0.288867,"lications, especially with the recent attention to opinion mining and social networks. As the vast amount of digitally available data keeps growing, so does the demand to automatically extract relevant information. We see a clear trend in information extraction applications to go beyond the extraction of pure facts, to focus on personal opinions in sentiment analysis and opinion mining, and to distinguish between factual and 4 / LiLT volume 14, issue 5 August 2016 probable information (Saur´ı et al., 2006), to detect uncertainty, speculation and negation, especially in biomedical text mining (Baker et al., 2010, Matsuyoshi et al., 2010, Szarvas et al., 2008). Modality detection is therefore also clearly linked to the current trend in NLP on sentiment analysis and opinion mining. We will report on the two modality annotation schemes designed for Portuguese, each addressing a specific national variety and a specific text type. The European Portuguese (EP) scheme was envisaged as genre independent and tested on written data, while the Brazilian Portuguese (BP) scheme was designed for spontaneous speech, and modality is taken as the evaluation or the point of view of a conceptualizer towards the locutor"
2016.lilt-14.5,W09-3012,0,0.0706552,"Missing"
2016.lilt-14.5,W10-3001,0,0.0185478,"sentence. The trigger receives an attribute modal value, while both trigger and target are marked for polarity. This feature describes the positive or negative polarity of the trigger and not the polarity of full sentences. A trigger may be marked with negative polarity by a negation adverb, or the negative polarity can be expressed morphologically by an affix (improbable) or by the lexical verbal form itself (proibir ‘forbid’). The choice of the size of the components to be annotated is a challenge. To achieve some level of consistency, we were inspired by the “min-max strategy” presented by Farkas et al. (2010). The trigger is annotated as the smallest possible unit (for example only the head noun in a noun phrase), while the target is annotated maximally and include all relevant parts. Adverbial adjuncts are included only when they are structurally inside the scope of the target, in order to avoid a proliferation of discontinuous targets. It is nevertheless difficult to establish exactly what is to be considered relevant and annotators still differ in this regard. For the sources, we annotate full noun phrases or verbs. In fact, as Portuguese is a null-subject language, the source of the modality i"
2016.lilt-14.5,genereux-etal-2012-introducing,1,0.888037,"Missing"
2016.lilt-14.5,hendrickx-etal-2012-modality,1,0.892921,"Missing"
2016.lilt-14.5,D15-1189,0,0.025241,"Missing"
2016.lilt-14.5,martin-2004-winpitch,0,0.025617,"glia, 2005), whereby diaphasic variation is privileged in order for a large diversity of illocutions and informational structuring to be documented. C-ORAL-BRASIL I comprises 200 texts of approximately 1,500 words each, proportionally distributed into dialogues, conversations and monologues. The corpus follows the CHILDES-CLAN3 transcription format to which prosodic 3 CHILDES - Child Language Data Exchange System, at: 14 / LiLT volume 14, issue 5 August 2016 annotation is added, marking tone unit and utterance boundaries. The entire corpus is speech to text aligned with the WinPitch software (Martin, 2004). In this particular study an annotated sample from the C-ORALBRASIL I was analyzed. It covers 20 texts, totaling 31,318 words, 5,484 utterances and 9,825 tone units. Firstly, the identification and classification of modal markers was undertaken by three annotators working independently; the codification was then qualitatively validated through group discussions involving the research group coordinator and her students. The search for modal markers was performed manually, through qualitative transcription examination, supported by the WinPitch textto-audio aligned files and their concomitant e"
2016.lilt-14.5,matsuyoshi-etal-2010-annotating,0,0.372179,"y with the recent attention to opinion mining and social networks. As the vast amount of digitally available data keeps growing, so does the demand to automatically extract relevant information. We see a clear trend in information extraction applications to go beyond the extraction of pure facts, to focus on personal opinions in sentiment analysis and opinion mining, and to distinguish between factual and 4 / LiLT volume 14, issue 5 August 2016 probable information (Saur´ı et al., 2006), to detect uncertainty, speculation and negation, especially in biomedical text mining (Baker et al., 2010, Matsuyoshi et al., 2010, Szarvas et al., 2008). Modality detection is therefore also clearly linked to the current trend in NLP on sentiment analysis and opinion mining. We will report on the two modality annotation schemes designed for Portuguese, each addressing a specific national variety and a specific text type. The European Portuguese (EP) scheme was envisaged as genre independent and tested on written data, while the Brazilian Portuguese (BP) scheme was designed for spontaneous speech, and modality is taken as the evaluation or the point of view of a conceptualizer towards the locutory material in a given utt"
2016.lilt-14.5,W05-0310,0,0.0663869,"Missing"
2016.lilt-14.5,W13-2328,1,0.682399,"ed globally to the sentence/event or it can be encoded on specific lexical items. The applied nature of these studies leads to detailed description of the textual elements involved in the expression of modality and the roles they have: most schemes identify a modal trigger, the subject of the modality (source) and the elements in the scope of the modal trigger (target/scope/focus). The availability of large sets of data annotated with modality provides important insights on the interaction between modality and other linguistic systems, such as negation (Morante and Sporleder, 2012) and focus (Mendes et al., 2013, Moreira, 2005). Another application of these data sets are experiments in the automatic annotation of modal values for information extraction and data mining. In BioNLP, Miwa et al. (2012) annotated pre-recognized events with the epistemic value “level of certainty” and attain F-measures of 74,9 for “low confidence” and 66,5 for “high but not complete confidence”. The factuality-oriented scheme presented by Saur´ı et al. (2006) has been applied in an experiment of automatic identification of events and their modal features in text, and attains 97.04 accuracy with the EviTA tool. A specific t"
2016.lilt-14.5,J12-2001,0,0.127907,"Missing"
2016.lilt-14.5,W13-0501,0,0.17733,"lity has been based on constructed examples, although the importance of looking at modal items in context is increasingly acknowledged, on a par with a clear trend in Corpus Linguistics and Natural Language Processing (NLP) to go beyond the part-of-speech (POS) and syntactic levels and to include semantics, pragmatics and supra-sentential information. This interest gave rise to some proposals for the annotation of modality in corpora, mainly for the English language. The annotation schemes covering modality differ greatly in their objectives and in the nature of the concepts that are labeled (Nissim et al., 2013). According to 6 / LiLT volume 14, issue 5 August 2016 Nissim and Pietandrea (2015), computationally, the automatic identification and interpretation of modalized statements is a prime concern in a large number of applications, especially with the recent attention to opinion mining and social networks (pp. vii) but the computational linguistics community is still far from having developed working, shared standards for converting modality-related issues into annotation categories. (pp. vii) Modality may be one aspect of the semantic information encoded in the properties of events (cf. (Baker et"
2016.lilt-14.5,W12-3807,0,0.038245,"Missing"
2016.lilt-14.5,W13-0306,0,0.0837766,"Missing"
2016.lilt-14.5,ruppenhofer-rehbein-2012-yes,0,0.179578,"tructure-based tagger over 249 modality-tagged sentences from the English side of the NIST 09 MTEval training sentences. The approach of Diab et al. (2009) covers modality but is essentially geared towards the identification of belief. Contrary to our own approach, the authors do not take into consideration the polysemy of the auxiliary verbs and only encode the epistemic value, although they do report that the verbs may be deontic in some contexts. This experiment has been extended to other modality values (ability, effort, intention, success and want) (Prabhakaran et al., 2012). The work of Ruppenhofer and Rehbein (2012) focuses on English modal verbs. The five English modal verbs (can/could, may/might, must, ought, shall/should) were first identified in texts and their modal values were predicted by training a maximum entropy classifier on features extracted from the training set. The classifier achieved an improvement of the most frequent sense baseline for all verbs but must, and accuracy numbers between 68.7 and 93.5. Our experiments presented in Section 4 are closely related to this type of automatic modal verb labeling. This brief overview of related work has shown the diversity of objectives, annotatio"
2016.lilt-14.5,W15-0812,0,0.0251436,"Missing"
2016.lilt-14.5,W08-0606,0,0.247999,"on to opinion mining and social networks. As the vast amount of digitally available data keeps growing, so does the demand to automatically extract relevant information. We see a clear trend in information extraction applications to go beyond the extraction of pure facts, to focus on personal opinions in sentiment analysis and opinion mining, and to distinguish between factual and 4 / LiLT volume 14, issue 5 August 2016 probable information (Saur´ı et al., 2006), to detect uncertainty, speculation and negation, especially in biomedical text mining (Baker et al., 2010, Matsuyoshi et al., 2010, Szarvas et al., 2008). Modality detection is therefore also clearly linked to the current trend in NLP on sentiment analysis and opinion mining. We will report on the two modality annotation schemes designed for Portuguese, each addressing a specific national variety and a specific text type. The European Portuguese (EP) scheme was envisaged as genre independent and tested on written data, while the Brazilian Portuguese (BP) scheme was designed for spontaneous speech, and modality is taken as the evaluation or the point of view of a conceptualizer towards the locutory material in a given utterance in a communicati"
2020.lrec-1.57,bunt-etal-2010-towards,0,0.01178,"onnected to a PostgreSQL database, where we store all of the dialogue information per user. intent example keyword(s) question inform confirm disconfirm salutation valediction stalling auto-feedback what do you mean yes no hello goodbye ehm uh-huh Table 1: List of user intents that can be recognized by the prototype with examples (translated to English). The default intent is an inform. The NLU component in our first prototype, used to collect the data described in Section 4., uses keyword-spotting for intent recognition. We selected relevant intents from the DIT++ taxonomy for our prototype (Bunt et al., 2010). User intents can be classified as question, salutation, inform, valediction, confirm, disconfirm, stalling and autofeedback (see Table 1). Additionally, we use the Dutch Pattern9 library to extract emotion from the transcripts of the ASR and for retrieving verbs and nouns from user responses (De Smedt and Daelemans, 2012). Stopwords are filtered with spaCy’s10 default stopword list for Dutch. In our prototype, the nouns and verbs represent the activities of a particular user. The NLG component is rule- and template-based. In our first prototype, the agent follows a script of small-talk after"
2020.lrec-1.57,W12-1620,0,0.0264007,"eune et al., 1997). The OVIS system was developed using a bootstrapping method. As a follow-up to OVIS, the IMIX project (van den Bosch and Bouma, 2011) developed a multimodal question answering system for Dutch, combining speech and visual modalities. One of its use cases was answering questions about repetitive strain injury; however, this was only for demonstration purposes. A look at the healthcare applications employing spoken dialogues, reveal that they have been developed mainly for limited domains such as breast cancer screening (Beveridge and Fox, 2006) or military mental healthcare (Morbini et al., 2012). An example from the field of persuasive technology is (Meschtscherjakov et al., 2016), focusing on support for speedy recovery or taking up regular exercise or medication. The Council of Coaches (COUCH) project uses multiple virtual agents to provide support for users who have for example diabetes or COPD (op den Akker et al., 2018). In the health domain we do not want to give the wrong information to patients. Therefore instead of dealing with free speech as input, Bickmore and Picard (2005) suggest to use a menu of options or limited text input, to both make the dialogue smoother and preve"
2020.lrec-1.57,W19-5941,0,0.0276437,"Pobil, 2018). All of these systems are designed to answer domainspecific user questions. Our focus in BLISS is on long-term interaction, asking engaging questions (instead of answering questions) and learning a user happiness model through normal spoken conversation. ELIZA, one of the first chatbots, was rule-based and designed as a therapeutic chatbot that could ask questions to users (Weizenbaum, 1966). Users talking to ELIZA disclosed personal information and were engaged with her. This kind of interaction is very different from how people interact with smart devices nowadays. As noted by Radlinski et al. (2019), communication with smart devices is often very command-like in style and not similar to human-human communication. They set up a Wizard-of-Oz experiment to collect a dataset of more spoken natural conversations in the context of movie recommendation and found indeed that these conversations contain far more complex information than what smart devices are capable of now. Similarly, with BLISS we want to have people speaking with the agent in a natural conversational way. Specifically for obtaining natural conversation data about personal topics, Zhang et al. (2018) collected PERSONA-CHAT, a d"
2020.lrec-1.57,W97-1206,0,0.233681,"Missing"
2020.lrec-1.57,P18-1205,0,0.0217629,"s nowadays. As noted by Radlinski et al. (2019), communication with smart devices is often very command-like in style and not similar to human-human communication. They set up a Wizard-of-Oz experiment to collect a dataset of more spoken natural conversations in the context of movie recommendation and found indeed that these conversations contain far more complex information than what smart devices are capable of now. Similarly, with BLISS we want to have people speaking with the agent in a natural conversational way. Specifically for obtaining natural conversation data about personal topics, Zhang et al. (2018) collected PERSONA-CHAT, a dataset containing text-based chitchat between two people recruited via crowdsourcing. They trained a chatbot on the dataset and indeed found that the chatbot was more engaging to talk to for people than chatbots trained on other resources such as Twitter. More importantly for our research, the profiles the chatbots generated from the conversations with users contained valuable information about the users’ personal lives. 3. Architecture In BLISS, we use the classical spoken dialogue system architecture for our agent, consisting of five main components: the Automatic"
genereux-etal-2012-introducing,W96-0102,0,\N,Missing
genereux-etal-2012-introducing,P99-1037,0,\N,Missing
genereux-etal-2012-introducing,P03-1004,0,\N,Missing
genereux-etal-2012-introducing,aluisio-etal-2004-lacio,0,\N,Missing
genereux-etal-2012-introducing,do-nascimento-etal-2000-portuguese,0,\N,Missing
genereux-etal-2012-introducing,branco-silva-2004-evaluating,0,\N,Missing
genereux-etal-2012-introducing,barreto-etal-2006-open,1,\N,Missing
genereux-etal-2012-introducing,hendrickx-etal-2012-modality,1,\N,Missing
genereux-etal-2012-introducing,P01-1058,0,\N,Missing
genereux-etal-2012-introducing,evert-2008-lightweight,0,\N,Missing
hagemeijer-etal-2014-gulf,gimenez-marquez-2004-svmtool,0,\N,Missing
hendrickx-etal-2008-coreference,J98-2001,0,\N,Missing
hendrickx-etal-2008-coreference,W96-0102,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1005,0,\N,Missing
hendrickx-etal-2008-coreference,N01-1008,0,\N,Missing
hendrickx-etal-2008-coreference,A88-1003,0,\N,Missing
hendrickx-etal-2008-coreference,W02-1008,0,\N,Missing
hendrickx-etal-2008-coreference,W05-0303,0,\N,Missing
hendrickx-etal-2008-coreference,C02-1139,0,\N,Missing
hendrickx-etal-2008-coreference,N06-1025,0,\N,Missing
hendrickx-etal-2008-coreference,J01-4004,0,\N,Missing
hendrickx-etal-2008-coreference,P98-2143,0,\N,Missing
hendrickx-etal-2008-coreference,C98-2138,0,\N,Missing
hendrickx-etal-2008-coreference,hoste-de-pauw-2006-knack,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1006,0,\N,Missing
hendrickx-etal-2008-coreference,M95-1011,0,\N,Missing
hendrickx-etal-2008-coreference,P03-1023,0,\N,Missing
hendrickx-etal-2012-modality,morante-2010-descriptive,0,\N,Missing
hendrickx-etal-2012-modality,matsuyoshi-etal-2010-annotating,0,\N,Missing
hendrickx-etal-2012-modality,baker-etal-2010-modality,0,\N,Missing
hendrickx-etal-2012-modality,W05-0310,0,\N,Missing
hendrickx-etal-2012-modality,W10-3001,0,\N,Missing
L16-1003,W09-1904,0,0.0383851,"in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation and ranking process the"
L16-1003,2015.mtsummit-papers.19,1,0.892142,"i et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-g"
L16-1003,2005.mtsummit-papers.11,0,0.0298517,"onsist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are ta"
L16-1003,P02-1040,0,0.0956189,"EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (millio"
L16-1003,P11-1138,0,0.0602764,"Missing"
L16-1003,steinberger-etal-2006-jrc,0,0.0272335,"experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz da"
L16-1003,tiedemann-2012-parallel,0,0.0663706,"known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz data that are far from straightforward to"
L16-1003,abdelali-etal-2014-amara,0,0.284209,"Missing"
L16-1003,D09-1030,0,0.252764,"Missing"
L16-1003,2012.eamt-1.60,0,0.0756545,"king field. The targeted crowds consist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality a"
L16-1003,N12-1047,0,0.0685374,"Missing"
L16-1003,N13-1001,0,0.0129109,"TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 20"
L16-1003,W10-0713,0,0.0292656,", and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation an"
L16-1003,D08-1089,0,0.0114402,"ments, slides, and other course materials. The forum data of TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the hi"
L16-1003,N12-1006,0,0.0606257,"Missing"
L16-1003,ambati-etal-2010-active,0,\N,Missing
L18-1073,abdelali-etal-2014-amara,0,0.0313475,"has the main benefit of access to people speaking the different languages. 3. Data Selection Our aim was to collect wikification annotations for 500 to 1,000 sentences from parallel educational texts for each language pair. We used three existing parallel text resources as the basis for the sentence selection so as to cover a broad range of online courses and to cover all eleven language pairs. In particular, we use parallel texts from course material of the Coursera MOOC platform, the Iversity MOOC platform, and the QCRI Educational Domain (QED) Corpus (formerly known as QCRI AMARA Corpus) (Abdelali et al., 2014). The Iversity data consists of (i) manually translated MOOC data, and (ii) MT output of English MOOC data produced by the first MT software prototype that was developed in the first year of the TraMOOC project. Both Coursera and QED material consist of MOOC subtitles that were translated using crowdsourcing in other projects unrelated to TraMOOC. The QED corpus consists of a large collection of files and each file contains a number of subtitles from MOOC video lectures in a particular language. The aligned files (parallel corpus) share the same first part of the file name. However, not all fi"
L18-1073,S16-1081,0,0.0258532,"We use the data set for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit"
L18-1073,S17-2001,0,0.03374,"for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evalua"
L18-1073,W08-0336,0,0.024088,"of speech (such as “mmm”, [NOISE], or [MUSIC]). Also, particularly long paragraphs were not selected, so as to maintain the microtasking nature of the activity. The selected source and target sentences were automatically tokenized using the multilingual tokenizer Ucto1 (van Gompel et al., 2017). Ucto has language-specific rules for the tokenization of several languages including Dutch, English, German, Italian, Portuguese, and Russian. For the other languages, generic language-independent settings of Ucto were used except for the Chinese language where we applied the Stanford Word Segmenter (Chang et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016"
L18-1073,D13-1184,0,0.0441826,"Missing"
L18-1073,N16-1104,0,0.0316436,"ng et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task"
L18-1073,S15-2049,0,0.0331012,"e also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data set is intended as both tuning material for the developed implicit machine translation system evaluation tool and for testing the final machine translation systems. The data set also gives us insights into the coverage of"
L18-1073,P11-1138,0,0.042925,"2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data"
L18-1073,sabou-etal-2014-corpus,0,0.0251481,"ared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task design, task completion time, quality control, task monitoring and crowd evaluation. Given pairs of aligned texts, the entity annotation (wikification) task consists of identifying and annotating"
L18-1073,N16-1072,0,0.0225831,"an be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those"
L18-1161,baker-etal-2010-modality,0,0.0360698,"s obtained and Section 4 discusses some conclusions and future work aiming at improving the system. 2. Related work As Portuguese is one of the 10 most spoken languages in the world, with more than 260 millions of speakers (da L´ıngua Portuguesa, 2015), the development of natural language processing tools and linguistically annotated resources for Portuguese are crucial to keep up with the current information society (Branco et al., 2012). However, most studies related to modality still focus on the English language, and besides our own work, not much tools have been developed for Portuguese. Baker et al. (2010), Matsuyoshi et al. (2010), Nirenburg and McShane (2008) and Sauri et al. (2006) present modality annotation schemes for the English language; for Portuguese we can identify the work from Hendrickx et al. (2012) for writ´ ten European Portuguese, Avila and Melo (2013) for spoken Brazilian Portuguese, and the updated proposal of both ´ teams in Avila et al. (2015). Thompson et al. (2008) addressed the identification of ex1000 pressions linked to modality in biomedical texts using three dimensions: the kind of knowledge, level of certainty and point of view. Their approach uses a list of words a"
L18-1161,hendrickx-etal-2012-modality,1,0.902881,"nt of the speaker to the truth of the proposition (whether the event is perceived as possible, probable or certain), but also deontic modality (obligation or permission), capacity and volition (Sequeira et al., 2016). Information about the modality of a text is crucial for the above mentioned trends on automatic fact finding and information extraction. This work extends the experiments done previously (Sequeira et al., 2016) in the pursuit of creating a semiautomatic modality tagging system for the Portuguese language from a manually annotated corpus that uses the modality scheme described by Hendrickx et al. (2012) and Mendes et al. (2016). In this study we focus on machine learning optimization and feature selection for modality detection and labeling. We compare two different system architectures, namely one classifier trained on all modal verbs and one architecture where we train a classifier for each modal verb separately. Such ‘word expert’ approach is known to work well in word sense disambiguation (a closely related task) (Hoste et al., 2002). We also investigate whether the complex feature representation based on parse information as applied in our previous work (Sequeira et al., 2016) is indeed"
L18-1161,matsuyoshi-etal-2010-annotating,0,0.0149533,"n 4 discusses some conclusions and future work aiming at improving the system. 2. Related work As Portuguese is one of the 10 most spoken languages in the world, with more than 260 millions of speakers (da L´ıngua Portuguesa, 2015), the development of natural language processing tools and linguistically annotated resources for Portuguese are crucial to keep up with the current information society (Branco et al., 2012). However, most studies related to modality still focus on the English language, and besides our own work, not much tools have been developed for Portuguese. Baker et al. (2010), Matsuyoshi et al. (2010), Nirenburg and McShane (2008) and Sauri et al. (2006) present modality annotation schemes for the English language; for Portuguese we can identify the work from Hendrickx et al. (2012) for writ´ ten European Portuguese, Avila and Melo (2013) for spoken Brazilian Portuguese, and the updated proposal of both ´ teams in Avila et al. (2015). Thompson et al. (2008) addressed the identification of ex1000 pressions linked to modality in biomedical texts using three dimensions: the kind of knowledge, level of certainty and point of view. Their approach uses a list of words and phrases with modal char"
L18-1161,2016.lilt-14.5,1,0.848086,"Missing"
L18-1161,ruppenhofer-rehbein-2012-yes,0,0.0451107,"Missing"
L18-1161,W13-0301,0,0.0161172,"ent of natural language processing tools and linguistically annotated resources for Portuguese are crucial to keep up with the current information society (Branco et al., 2012). However, most studies related to modality still focus on the English language, and besides our own work, not much tools have been developed for Portuguese. Baker et al. (2010), Matsuyoshi et al. (2010), Nirenburg and McShane (2008) and Sauri et al. (2006) present modality annotation schemes for the English language; for Portuguese we can identify the work from Hendrickx et al. (2012) for writ´ ten European Portuguese, Avila and Melo (2013) for spoken Brazilian Portuguese, and the updated proposal of both ´ teams in Avila et al. (2015). Thompson et al. (2008) addressed the identification of ex1000 pressions linked to modality in biomedical texts using three dimensions: the kind of knowledge, level of certainty and point of view. Their approach uses a list of words and phrases with modal characteristics specific for the biomedical domain. Baker et al. (2010) tested two rule-based modality taggers that identify both the modal trigger (word or word list where modality is expressed, usually by the use of modal verbs) and its target"
L18-1161,W15-0301,1,0.827662,"Missing"
L18-1521,P16-2050,1,0.627018,"Missing"
L18-1521,P14-5010,0,0.00299399,"grape varieties to be predicted. Much variation can be seen in the number of training instances per grape, ranging from 5,706 reviews for chardonnay to 222 reviews for carmen`ere. The third classifier aims at predicting among 47 different countries of origin. Again, the class distribution is unbalanced, with some countries represented very well (e.g., US: 25,104 reviews, Italy: 9,912 reviews, France: 8,568 reviews) to countries only occurring once (Tunisia, South Korea, Montenegro, India) in the training set. All wine reviews were linguistically preprocessed by means of the Stanford toolkit (Manning et al., 2014) involving tokenization, lemmatization and Part-of-Speech tagging. From the preprocessed review text, three different feature types were extracted to model the three classification tasks: lexical, semantic and terminology features. 4.1.1. Lexical features We extracted a list of bag-of-words (BoW) unigram features from the review text containing lowercased lemmas. These BoW features were filtered on Part-of-Speech category to filter out function words and only keep content words (nouns, adjectives, verbs, and adverbs). The BoW features were incorporated as binary features, meaning that each BoW"
P16-2050,P14-5010,0,0.0072621,"Missing"
R11-1026,doddington-etal-2004-automatic,0,0.0216555,"Missing"
R11-1026,C08-1033,0,0.0154312,"aria, 12-14 September 2011. they use genre-specific features such as average length of the coreferential chain and average distance separating several mentions of the same referent. An exception to this observation of small datasets is the new OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each"
R11-1026,hendrickx-etal-2008-coreference,1,0.926636,"Missing"
R11-1026,W07-1503,0,0.036824,"in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or worse and whether it is possible to determine a priori which training data to add to our resolver so as to obtain better results. The results are presented using four of the more frequently used evaluation metrics for coreference research, namely MUC (Vilain et al., 1995), Bcubed (Bagga"
R11-1026,W09-2411,0,0.0520498,"Missing"
R11-1026,H05-1083,0,0.158255,"delines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or worse and whether it is possible to determine a priori which training data to add to our resolver so as to obtain better results. The results are presented using four of the more frequently used evaluation metrics for coreference research, namely MUC (Vilain et al., 1995), Bcubed (Bagga and Baldwin, 1998), CEAF (Luo and Zitouni, 2005) and BLANC (Recasens and Hovy, 2011). We show that adding more data to training proves mostly beneficial, especially when genrespecific information is included. Moreover, training a resolver on each genre separately allows us to classify each genre as having good or bad generalization power when applied to other genres. This led us to conduct experiments in which we train on all genres while progressively leaving out the worst-performing cross-domain genres as an attempt to boost overall performance. Although the 3 4 results are sometimes better, performance does not rise nor drop dramatically"
R11-1026,rodriguez-etal-2010-anaphoric,0,0.0411628,"Missing"
R11-1026,magnini-etal-2006-cab,0,0.0302547,"Dutch Coreference Resolution Orph´ee De Clercq, V´eronique Hoste LT3, Language and Translation Technology Team University College Ghent Groot-Brittannielaan 45 B - 9000 Gent, Belgium orphee.declercq@hogent.be veronique.hoste@hogent.be Abstract have been put in annotating corpora with coreferential relations. Not only a widespread language such as English (e.g. ACE-2 (Doddington et al., 2004), ARRAU (Poesio and Artstein, 2008), OntoNotes 3.0 (Weischedel et al., 2009)), but also Czech (PDT 2.0 (Kuˇcov´a and Hajiˇcov´a, 2004)), Catalan (AnCora-Ca (Recasens and Mart´ı, 2010)) and Italian (I-CAB (Magnini et al., 2006))2 can now rely on substantial resources for coreference research. One of the challenges in many current NLP tasks is to test their portability across different domains and languages. This portability to other languages was the main objective of the SemEval 2010 Task on Coreference Resolution in Multiple Languages (Recasens et al., 2010). The issue of domain portability was the focus of the ACL 2010 Workshop on Domain Adaptation for NLP (Daum´e III et al., 2010). In this paper we investigate the performance of an existing mention-pair coreference resolver for Dutch (Hoste, 2005; Hendrickx et a"
R11-1026,schuurman-etal-2010-interacting,1,0.831589,"OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or w"
R11-1026,J01-4004,0,0.0499172,"nce shifts, datasets of equal size (about 30K) were randomly selected. The focus of the current experiments was on resolving identity and predicative relations. Table 1 gives some statistics about each dataset, such as the average sentence length and the number of coreferring NPs. For all experiments we used an existing coreference resolver for Dutch, developed by Hoste (2005) and Hendrickx et al. (2008b). The system Website from CoNLL 2011: http://conll.bbn.com SoNaR is currently still under development. 5 187 http://mmax2.net follows a machine learning approach6 based on the seminal work of Soon et al. (2001) and represents a mention-pair model. First, a classifier is trained to decide whether a pair of NPs is coreferential or not, after which coreference chains are built for the pairs of NPs that were classified as coreferential. #docs ADM AUTO EXT INST MED JOUR WIKI DUO 21 15 29 18 213 52 15 56 #tokens 30,215 30,058 29,940 29,994 30,001 30,002 30,340 29,740 avg. senl 18.1 14.6 15.9 17.5 14.4 18.2 18.9 19.7 6.3 (Daelemans et al., 2010) with default parameter settings. Our experimental results are evaluated using the four scoring metrics as implemented in the scoring script from the coreference re"
R11-1026,W02-2024,0,0.0521019,"recall by counting how many elements are in the true coreferential cluster and how many in the predicted coreferential cluster. All datasets were preprocessed in the same way. Tokenisation, lemmatisation, Part-of-Speech tagging and grammatical relations were based on the manually verified output of the Alpino parser (Bouma et al., 2001), i.e. gold standard dependency structures. For the DuOMAn data, however, no gold standard dependency trees were available. Named entity recognition was performed using MBT (Daelemans et al., 2003), trained on the 2002 CoNNL shared task Dutch dataset (Tjong Kim Sang, 2002) and an additional gazetteer lookup. As features we employ string matching, distance between sentences and NPs, grammatical role and named entity overlap, synonym/hypernym lookup using Cornetto (a Dutch database combining Dutch Wordnet (Vossen, 1998) and the Referentie Bestand Nederlands (Martin and Ploeger, 1999)) and local context. All instances were built between NP pairs going 20 sentences back in context. NPs that are not part of a coreferential chain (singletons) are included as negative examples. For more information we refer to Hoste (2005) and Hendrickx et al. (2008a). Since the focus"
R11-1026,P10-1142,0,0.0157525,"oped to overcome problems with the other scoring methods. This measure is a variant of the Rand Index (Rand, 1971) adapted for coreference resolution and it averages over a score for correctly detecting singletons, and a score for detecting the correct cluster for coreferential elements. An important remark to make here is that our system does not take into account chains of only one element. As a consequence, contrary to the SemEval-2010 competition, when we compute 6 For an extensive overview of the different machine learning approaches for coreference resolution, we refer to the surveys of Ng (2010) and Poesio et al. (forthcoming) 7 Hoste (2005) built a separate learning module for each of these NP types based on the motivation that the impact of different information sources varies per NP type. 188 1. 2. 3. TRAIN one genre all genres but one all genres one genre all LOO outliers TEST that genre left out genre one genre other genres one genre 3 Results The results of the first round of experiments are presented in Figure 1. The dots marked as individual present the experiments in which each classifier was trained and tested on the same material. The scores for All-individual present expe"
R11-1026,M95-1005,0,0.416969,"ures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or worse and whether it is possible to determine a priori which training data to add to our resolver so as to obtain better results. The results are presented using four of the more frequently used evaluation metrics for coreference research, namely MUC (Vilain et al., 1995), Bcubed (Bagga and Baldwin, 1998), CEAF (Luo and Zitouni, 2005) and BLANC (Recasens and Hovy, 2011). We show that adding more data to training proves mostly beneficial, especially when genrespecific information is included. Moreover, training a resolver on each genre separately allows us to classify each genre as having good or bad generalization power when applied to other genres. This led us to conduct experiments in which we train on all genres while progressively leaving out the worst-performing cross-domain genres as an attempt to boost overall performance. Although the 3 4 results are s"
R11-1026,nguyen-etal-2008-challenges,0,0.0237694,"se genre-specific features such as average length of the coreferential chain and average distance separating several mentions of the same referent. An exception to this observation of small datasets is the new OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis i"
R11-1026,poesio-artstein-2008-anaphoric,0,0.0234744,"the performance of an existing mention-pair coreference resolver for Dutch (Hoste, 2005; Hendrickx et al., 2008b) across various text genres. More specifically we want to know whether training on out-of-domain data can be done without performance loss. The above-mentioned corpora designed for coreference resolution consist almost exclusively of text from the same genre, i.e. newspaper texts, and as a consequence resulting coreference resolvers are mostly trained on this particular genre. Moreover, when other genres are included, the acquired data are rather scarce: 25K of dialogues in ARRAU (Poesio and Artstein, 2008), 23K manuals in AnATar (Hammami et al., 2009) or 50K of annotated blogs in LiveMemories (Rodr´ıguez et al., 2010). Another related study is the work of Longo and Todirascu (2010). They analyzed a French corpus (50K) consisting of 5 different text genres to develop genre-specific features; in their study This article explores the portability of a coreference resolver across a variety of eight text genres. Besides newspaper text, we also include administrative texts, autocues, texts used for external communication, instructive texts, wikipedia texts, medical texts and unedited new media texts."
R11-1026,C04-1033,0,0.0119811,"86–193, Hissar, Bulgaria, 12-14 September 2011. they use genre-specific features such as average length of the coreferential chain and average distance separating several mentions of the same referent. An exception to this observation of small datasets is the new OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et a"
R11-1026,S10-1001,1,\N,Missing
R11-1026,C69-7001,0,\N,Missing
R11-1026,C69-6902,0,\N,Missing
R11-1026,W10-2600,0,\N,Missing
S01-1003,W96-0102,0,0.0517625,"n information can be found immediately adjacent to the ambiguous word.It has been found that a four-word window, two words before the target word and two words after gives good results; cf. (Veenstra et al., 2000). Second, information about the grammatical category of the target word and its direct context words can also be valuable. Consequently, each sentence of the Dutch corpus was tagged and the part-of-speech (POS) tags of the word and its direct context (two left, two right) are included in the representation of the sentence. Part-of-speech tagging was done with the Memory Based Tagger (Daelemans et al., 1996). Third, informative words in the context ('keywords') are detected based on the statistical chi-squared test. Chi-square estimates the significance, or degree of surprise, of the number of keyword occurrences with respect to the expected number of occurrences (apriori probability): 4.9~9 9319 6.702 54 Table 1: Basic corpus statistics metjmeLprepositie geweld/= opgelostjoplossen_probleem wordenjworden_hww ,"" /= zeiden/zeggen_praten de/= koningenjkoning .J= toenjtoen_adv verklaardenjverklaren_oorlog zej= elkaar/=de/= oorlog/= .J= The dataset needed some adaptations to make it fully usable for c"
S07-1039,W96-0102,0,0.0707357,"ous way to model their lexical semantics was by utilizing WordNet3.0 (Fellbaum, 1998) (WN). One of the systems followed this route. We also entered a second system, which did not rely on WN but instead made use of automatically 2 System Description The development of the system consists of a preprocessing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computatio"
S07-1039,J93-2004,0,0.0321182,"ing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computational Linguistics The features extracted are of three types: semantic, lexical, and morpho-syntactic. The features that apply to the entities in a relation (e1,e2) are extracted for term 1 (t1) and term 2 (t2) of the relation, where t1 is the first term in the relation name, and t2 is the second term."
S10-1006,W09-1401,0,0.0287746,"for each semantic relation. Here, we describe the general guidelines, which delineate the scope of the data to be collected and state general principles relevant to the annotation of all relations.1 Our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to “aboutness” annotation such as semantic roles (Carreras and M`arquez, 2004) or the BioNLP 2009 task (Kim et al., 2009) where negated relations are also labelled as positive. Similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios. In practice, this means disallowing annotations within the scope of modals or negations, e.g., “Smoking may/may not have caused cancer in this case.” We accept as relation arguments only noun phrases with common-noun heads. This distinguishes our task from much work in Information Extraction, which tends to focus on specific classes of named entities and on considerably more finegrained relations than we do. Named entities ar"
S10-1006,W05-0620,0,\N,Missing
S13-2025,W04-0404,0,0.0532624,"Missing"
S13-2025,C08-1011,1,0.662917,"ages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings."
S13-2025,W09-2416,1,0.899683,"Missing"
S13-2025,P07-1072,0,0.203668,"Missing"
S13-2025,P06-2064,0,0.395087,"Missing"
S13-2025,W04-2609,0,0.0835979,"located in Geneva. 138 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE)"
S13-2025,P08-1052,1,0.483189,"uation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve r"
S13-2025,E09-1071,1,0.900906,"Missing"
S13-2025,C08-1082,1,0.890902,"Missing"
S13-2025,W03-1803,0,0.161511,"Missing"
S13-2025,P10-1070,0,0.829879,"Missing"
S14-2005,S13-2029,1,0.709155,"y, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number of nouns. Our task emphasizes a correct meaning-preserving choice 37 site1 . We crea"
S14-2005,S10-1002,0,0.0325185,"k to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number o"
S14-2005,S14-2030,0,0.0733812,"nd Nanyang Technological University (Singapore) – all language pairs 6. TeamZ - Anubhav Gupta - Universit´e de Franche-Comt´e (France) – English-Spanish, English-German Participants implemented distinct methodologies and implementations. One obvious avenue of tackling the problem is through standard Statistical Machine Translation (SMT). The CNRC team takes a pure SMT approach with few modifications. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-l"
S14-2005,S14-2110,0,0.0209095,"uesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results, building on word and phrase alignment data as does SMT, yet not usin"
S14-2005,S14-2060,0,0.037367,"Missing"
S14-2005,S14-2123,0,0.0228464,"tions. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve t"
S14-2005,S14-2132,0,0.0206542,"IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve this (Silva-Schlenker et al., 2014). The two systems on the lower end of the result spectrum use different techniques altogether. The Sensible team approaches the problem • Best - The system may only output one, its best, translation; • Out of Five - The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. C"
S14-2005,P07-2045,0,0.0150004,"Missing"
S14-2005,S14-2094,0,0.0289288,"- The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results,"
S14-2005,P14-1082,1,0.682767,"Missing"
S14-2005,2005.mtsummit-papers.11,0,0.00919859,"d in a simple XML format that explicitly marks the fragments. System output of participants adheres to the same format. The trial set, released early on in the task, was used by participants to develop and tune their systems on. The test set corresponds to the final data released for the evaluation period; the final evaluation was conducted on this data. The trial data was constructed in an automated fashion in the way described in our pilot study (van Gompel and van den Bosch, 2014). First a phrase-translation table is constructed from a parallel corpus. We used the Europarl parallel corpus (Koehn, 2005) and the Moses tools (Koehn et al., 2007), which in turn makes use of GIZA++ (Och and Ney, 2000). Only strong phrase pairs (exceeding a set threshold) were retained and weaker ones were pruned. This phrase-translation table was then used to create input sentences in which the L2 fragments are swapped for their L1 counterparts, effectively mimicking a fall-back to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.”"
W02-0809,S01-1003,1,\N,Missing
W02-0809,J01-3001,0,\N,Missing
W02-0809,P97-1056,1,\N,Missing
W02-0809,W02-0814,1,\N,Missing
W02-0809,P96-1006,0,\N,Missing
W02-0814,P99-1037,1,0.884605,"Missing"
W02-0814,P01-1005,0,0.0641157,"Missing"
W02-0814,W96-0102,1,0.832954,"Missing"
W02-0814,S01-1020,1,0.844583,"Missing"
W02-0814,J98-1001,0,0.0345256,"Missing"
W02-0814,P96-1006,0,0.0730626,"polysemy and sense distributions. Iris Hendrickx and Antal van den Bosch ILK Computational Linguistics Tilburg University, The Netherlands I.H.E.Hendrickx,antalb @kub.nl  1 Introduction The task of word sense disambiguation (WSD) is to assign a sense label to a word in context. Both knowledge-based and statistical methods have been applied to the problem. See (Ide and V´eronis, 1998) for an introduction to the area. Recently (both S ENSEVAL competitions), various machine learning (ML) approaches have been demonstrated to produce relatively successful WSD systems, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), decision lists (Yarowsky, 2000), boosting (Escudero et al., 2000). In this paper, we evaluate the results of a memorybased learning approach to WSD. We ask ourselves whether we can learn lessons from the errors made in the S ENSEVAL -2 competition. More particularly, we are interested whether there are words or categories of words which are more difficult to predict than other words. If so, do these words have certain characteristic features? We furthermore investigate the interaction between the use of different information sources and the part-of-speech categories o"
W02-0814,J01-3001,0,0.0788235,"Missing"
W02-0814,1995.iwpt-1.8,0,\N,Missing
W02-0814,J95-4002,0,\N,Missing
W02-0814,C00-2098,0,\N,Missing
W02-0814,H94-1020,0,\N,Missing
W02-0814,P00-1058,0,\N,Missing
W02-0814,P90-1035,0,\N,Missing
W02-0814,P83-1017,0,\N,Missing
W02-0814,P98-2156,0,\N,Missing
W02-0814,C98-2151,0,\N,Missing
W03-0427,buchholz-van-den-bosch-2000-integrating,1,0.898249,"Missing"
W03-0427,W02-2004,0,0.0192722,"the final test set of 78.20 on English and 63.02 on German. 2 Data and features The CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) supplied datasets in two languages, English and German, using four named entity categories: persons, organisations, locations, and “miscellany names”. Manual annotation has been performed at the University of Antwerp. Apart from tokenized wordforms, the data provides predicted PoS-tags and chunks. Additionally we computed the following features with each wordform, largely following those used by the bestperforming submission of the 2002 shared task (Carreras et al., 2002): • Orthographic features represented as binary features: Begin cap, All caps, Internal cap, Contains digit, Contains digit en alpha, Initial, Lower case, First word • The wordform’s first letter and last three letters (as three separate features) • The direct output of the memory-based lemmatizer (Van den Bosch and Daelemans, 1999), providing PoS tag, morphological features, and spelling change information • PoS tag from a slow but accurate version of the memory-based tagger trained on a portion of the British National Corpus, according to the CLAWS-5 tagset (for English data only) For exampl"
W03-0427,W99-0612,0,0.015855,"imilarities between instances using these extra features, it is able, in principle, to recognise and correct reoccurring patterns of errors within sub-sentential sequences. This could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first extension. 4.3 Unannotated data For both languages a large unannotated dataset was made available for extracting data or information. Alternative to using this data to expand or bootstrap seed lists (Cucerzan and Yarowsky, 1999; Buchholz and Van den Bosch, 2000), we use the unannotated corpus to select useful instances to be added directly to the training set. Not unlike (Yarowsky, 1995) we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory. We make the simple assumption that entropy in the class distribution in the nearest neighbour set computed in the classification of a new instance is correlated with the reliability of the classification, when k &gt; 1. When k nearest neighbours all vote for the same class, the entropy of that cla"
W03-0427,W03-0419,0,0.0772698,"Missing"
W03-0427,P99-1037,1,0.85853,"Missing"
W03-0427,P95-1026,0,0.0280531,"is could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first extension. 4.3 Unannotated data For both languages a large unannotated dataset was made available for extracting data or information. Alternative to using this data to expand or bootstrap seed lists (Cucerzan and Yarowsky, 1999; Buchholz and Van den Bosch, 2000), we use the unannotated corpus to select useful instances to be added directly to the training set. Not unlike (Yarowsky, 1995) we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory. We make the simple assumption that entropy in the class distribution in the nearest neighbour set computed in the classification of a new instance is correlated with the reliability of the classification, when k &gt; 1. When k nearest neighbours all vote for the same class, the entropy of that class vote is 0.0. Alternatively, when the votes tie, the entropy is maximal. A secondary heuristic assumption is that it is probably not useful to add (almost) exact"
W04-2414,W99-0629,1,0.820692,"role labeling: Optimizing features, algorithm, and output Antal van den Bosch, Sander Canisius, Walter Daelemans, Iris Hendrickx Erik Tjong Kim Sang ILK / Computational Linguistics CNTS / Department of Linguistics Tilburg University, P.O. Box 90153, University of Antwerp, Universiteitsplein 1, NL-5000 LE Tilburg, The Netherlands B-2610 Antwerpen, Belgium {Antal.vdnBosch,S.V.M.Canisius, {Walter.Daelemans, I.H.E.Hendrickx}@uvt.nl 1 Introduction In this paper we interpret the semantic role labeling problem as a classification task, and apply memory-based learning to it in an approach similar to Buchholz et al. (1999) and Buchholz (2002) for grammatical relation labeling. We apply feature selection and algorithm parameter optimization strategies to our learner. In addition, we investigate the effect of two innovations: (i) the use of sequences of classes as classification output, combined with a simple voting mechanism, and (ii) the use of iterative classifier stacking which takes as input the original features and a pattern of outputs of a first-stage classifier. Our claim is that both methods avoid errors in sequences of predictions typically made by simple classifiers that are unaware of their previous"
W04-2414,W04-2412,0,0.0763194,"Missing"
W08-1129,W07-2302,0,0.0688957,"irstname.lastname@ua.ac.be Abstract In this paper we describe our machine learning approach to the generation of referring expressions. As our algorithm we use memory-based learning. Our results show that in case of predicting the TYPE of the expression, having one general classifier gives the best results. On the contrary, when predicting the full set of properties of an expression, a combined set of specialized classifiers for each subdomain gives the best performance. 1 Introduction In this paper we describe the systems with which we participated in the GREC task of the REG 2008 challenge (Belz and Varges, 2007). The GREC task concerns predicting which expression is appropriate to refer to a particular discourse referent in a certain position in a text, given a set of alternative referring expressions for selection. The organizers provided the GREC corpus that consists of 2000 texts collected from Wikipedia, from 5 different subdomains (people, cities, countries, mountains and rivers) . One of the main goals of the task is to discover what kind of information is useful in the input to make the decision between candidate referring expressions. We experimented with a pool of features and several machin"
W08-1138,2007.mtsummit-ucnlg.14,0,0.0267791,"s a graph search problem, which outputs the cheapest distinguishing graph (if one exists) given a particular cost function. For the second step, realisation, we use a simple template-based realiser written by Irene Langkilde-Geary from Brighton University that was made available to all REG 2008 participants. A version of the Graph-based algorithm was submitted for the ASGRE 2007 Challenge (Theune et al. 2007). For us, one of the most striking, general outcomes was the observed “trend for the mean DICE score obtained by a system to decrease as the proportion of minimal descriptions increases” (Belz and Gatt 2007).2 Thus, while REG systems have a tendency to produce minimal descriptions, human speakers tend to include redundant properties in their descriptions, which is in line with recent findings in psycholinguistics on the production of referring expressions (e.g., Engelhardt et al. 2006). In principle, the graph-based approach has the potential to deal with redundancy by allowing some attributes to have zero costs. Viethen et al. (2008), however, show that merely assigning zero costs to an attribute is not a sufficient condition for inclusion; if the search terminates before the free properties are"
W08-1138,J03-1003,1,0.772741,"Missing"
W08-1138,viethen-etal-2008-controlling,1,\N,Missing
W08-1138,W09-0629,0,\N,Missing
W09-0604,C04-1051,0,0.105585,"Missing"
W09-0604,W03-1608,0,0.0699754,"Missing"
W09-0604,A00-2024,0,0.110693,"Missing"
W09-0604,N03-1003,0,0.280604,"Missing"
W09-0604,W03-1101,0,0.0331291,"the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Proceedings of the 12th European Workshop on Natural Language Generation, pages 25–32, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 25"
W09-0604,E06-1040,0,0.0771484,"Missing"
W09-0604,P06-1048,0,0.0449688,"Missing"
W09-0604,P05-1036,0,0.305391,"Missing"
W09-0604,W04-1015,0,0.450146,"crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Proceedings of the 12th European Workshop on Natural Language Generation, pages 25–32, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 25 Atranos and Musa – on automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004). All"
W09-0604,vandeghinste-tjong-kim-sang-2004-using,0,0.0675821,"Missing"
W09-0604,daelemans-etal-2004-automatic,1,\N,Missing
W09-0604,Y03-1033,0,\N,Missing
W09-2415,W04-3205,0,0.0639941,"unrelated semantic roles. There is a rudimentary frame hierarchy that defines mappings between roles of individual frames,5 but it is far from complete. The situation is similar in PropBank. PropBank does use a small number of semantic roles, but these are again to be interpreted at the level of individual predicates, with little cross-predicate generalization. In contrast, all of the semantic relation inventories discussed in Section 1 contain fewer than 50 types of semantic relations. More generally, semantic relation inventories attempt to generalize relations across wide groups of verbs (Chklovski and Pantel, 2004) and include relations that are not verbcentered (Nastase and Szpakowicz, 2003; Moldovan et al., 2004). Using the same labels for similar semantic relations facilitates supervised learning. For example, a model trained with examples of sell relations should be able to transfer what it has learned to give relations. This has the potential of adding 5 For example, it relates the B UYER role of the C OM frame (verb sell ) to the R ECIPIENT role of the G IVING frame (verb give). MERCE SELL 97 1. People in Hawaii might be feeling &lt;e1>aftershocks&lt;/e1> from that powerful &lt;e2>earthquake&lt;/e2> for weeks"
W09-2415,P08-1027,0,0.0924452,"tical NLP settings, where any relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, som"
W09-2415,J02-3001,0,0.0386032,"This is motivated by modelling considerations. Presumably, the data for OTHER will be very nonhomogeneous. By including it, we force any model of the complete data set to correctly identify the decision boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of mu"
W09-2415,S07-1003,1,0.384989,"Missing"
W09-2415,C92-2082,0,0.060649,"tion will take place in two rounds. In the first round, we will do a coarse-grained search for positive examples for each relation. We will collect data from the Web using a semi-automatic, pattern-based search procedure. In order to ensure a wide variety of example sentences, we will use several dozen patterns per relation. We will also ensure that patterns retrieve both positive and negative example sentences; the latter will help populate the OTHER relation with realistic near-miss negative examples of the other relations. The patterns will be manually constructed following the approach of Hearst (1992) and Nakov and Hearst (2008).6 The example collection for each relation R will be passed to two independent annotators. In order to maintain exclusivity of relations, only examples that are negative for all relations but R will be included as positive and only examples that are negative for all nine relations will be included as OTHER. Next, the annotators will compare their decisions and assess inter-annotator agreement. Consensus will be sought; if the annotators cannot agree on an example it will not be included in the data set, but it will be recorded for future analysis. Finally, two othe"
W09-2415,P08-2047,0,0.0143881,"relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, some subsequent publications tri"
W09-2415,I05-1082,1,0.187851,"nds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relat"
W09-2415,W04-2609,0,0.0615549,"fy noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes c"
W09-2415,P08-1052,1,0.611835,"medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is wh"
W09-2415,C08-1082,1,0.339838,"Missing"
W09-2415,J05-1004,0,0.0280101,"boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of multiple participants and props, while semantic relations are in practice (although not necessarily) binary. The second major difference is the syntactic context. Theories of semantic roles usually d"
W09-2415,P06-1015,1,0.178213,"m of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes clear that context does indeed play a role. Consider, for example, the noun compound wood shed : it may refer either to a shed made of wood, or to a shed of any material used to store wood. This ambiguity is likely to be resolved in particular contexts. In fact, most NLP appli"
W09-2415,D07-1075,0,0.0368294,"annotation, we define a nominal as a noun or a base noun phrase. A base noun phrase is a noun and its pre-modifiers (e.g., nouns, adjectives, determiners). We do not include complex noun phrases (e.g., noun phrases with attached prepositional phrases or relative clauses). For example, lawn is a noun, lawn mower is a base noun phrase, and the engine of the lawn mower is a complex noun phrase. We focus on heads that are common nouns. This emphasis distinguishes our task from much work in IE, which focuses on named entities and on considerably more fine-grained relations than we do. For example, Patwardhan and Riloff (2007) identify categories like Terrorist organization as participants in terror-related semantic relations, which consists predominantly of named entities. We feel that named entities are a specific category of nominal expressions best dealt with using techniques which do not apply to common nouns; for example, they do not lend themselves well to semantic generalization. Figure 1 shows two examples of annotated sentences. The XML tags &lt;e1> and &lt;e2> mark the target nominals. Since all nine proper semantic relations in this task are asymmetric, the ordering of the two nominals must be taken into acco"
W09-2415,W01-0511,0,0.0250487,"CL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one"
W09-2415,P02-1032,0,0.00907258,"stics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and dat"
W09-2415,W09-1401,0,\N,Missing
W09-2415,J02-3004,0,\N,Missing
W09-2415,S10-1006,1,\N,Missing
W09-2415,W04-2412,0,\N,Missing
W09-2812,J05-3002,0,0.0716088,"Missing"
W09-2812,N03-1020,0,0.181494,"the maximal margin relevance criterion (Carbonell and Goldstein, 1998). MMR models the trade-off between a focused summary and a summary with a wide scope. The novelty-reranker is an extension of the cosine-reranker and boosts sentences occurring after an important sentence by multiplying with 1.2. The reranker tries to mimic human behavior as people tend to pick clusters of sentences when summarizing. 4 For the experiments on the development set, we compare each of the automatically produced extracts with five manually written summaries and report macro-average Rouge-2 and Rouge-SU4 scores (Lin and Hovy, 2003). For the experiments on the test set, we also perform a manual evaluation. We follow the DUC 2006 guidelines for manual evaluation of responsiveness and the linguistic quality of the produced summaries. The responsiveness scores express the information content of the summary with respect to the query. The linguistic quality is evaluated on five different objectives: grammaticality, non-redundancy, coherence, referential clarity and focus. The annotators can choose a value on a five point scale where 1 means ‘very poor’ and 5 means ‘very good’. We use two independent annotators to evaluate the"
W09-2812,radev-etal-2004-mead,0,0.100917,"Missing"
W09-2812,vossen-etal-2008-integrating,0,0.0604442,"Missing"
W10-1812,P03-1068,0,0.0858629,"Missing"
W10-1812,E06-2013,0,0.0292689,"ic type of relations. FrameNet uses frame semantics theory to represent such predicate-argument structures which also includes handling complex predicates (e.g. (Johnson and Fillmore, 2000)). For German, there exists a fully annotated corpus with semantic frames (Erk et al., 2003). The basis of the Framenet semantic annotation are conceptual frames expressing an event or object and the semantic arguments (frame elements) that are (obligatory or optional) parts of the frames. They also specifically address support verbs and observe that support verbs often occur with nouns expressing an event (Johansson and Nugues, 2006). In a Framenet semantic annotation, support verbs are not considered as parts of frames or as part of the frame elements, they are annotated with a specific ‘support verb’ label. We, on the contrary, view CP as one semantic and syntactic unit. In Nombank, a distinction is made between idioms (which in principle are not marked) and light verb plus noun combinations, which are to be annotated, and criteria are given to make such distinction (English (Meyers, 2007), Chinese (Xue, 2006)). In (1) we show a NomBank annotation example of the sentence with a complex predicate. Usually, CPs of the typ"
W10-1812,A00-2008,0,0.039882,"r approach, on constructions where the subject of the verb is a participant in the event denoted by the noun. Their objective is however not corpus annotation, but the creation of a computational lexicon of MWEs with both syntactic and semantic information. Also the field of semantic or thematic role labeling investigates constructions of verb+noun, but it focuses on predicate-argument structures in general, while we focus on a specific type of relations. FrameNet uses frame semantics theory to represent such predicate-argument structures which also includes handling complex predicates (e.g. (Johnson and Fillmore, 2000)). For German, there exists a fully annotated corpus with semantic frames (Erk et al., 2003). The basis of the Framenet semantic annotation are conceptual frames expressing an event or object and the semantic arguments (frame elements) that are (obligatory or optional) parts of the frames. They also specifically address support verbs and observe that support verbs often occur with nouns expressing an event (Johansson and Nugues, 2006). In a Framenet semantic annotation, support verbs are not considered as parts of frames or as part of the frame elements, they are annotated with a specific ‘sup"
W10-1812,barreto-etal-2006-open,1,0.893864,"seem to contribute to the properties of complex predicates, in such a way that the argument structure and the attribution of thematic roles are determined by both constituents through the combination of their thematic structures (Grimshaw, 1988). One has to address several important questions: is there a systematic relationship between the syntactic and semantic selection properties of the two elements? How do the argument structure of the light verb and the derived noun combine and contribute to define the complex predicate? To study these questions we annotated the Portuguese CINTIL corpus (Barreto et al., 2006) with a new layer on CPs. By taking into consideration different types of CPs and by using corpus data for our analysis of their properties, the objective is to present a unified approach to CP formation, along which the CP constructions available in Portuguese may be accounted for, namely in what concerns their lexico-syntactic properties and their interpretation. Here we focus on the corpus annotation of complex predicates. This paper is structured as follows. In section 2 we discuss related work on the annotation of CPs in other languages. In section 3 we present a typology of complex predi"
W10-1812,J93-2004,0,0.0343929,"n the construction, or as an auxiliary verb with aspectual properties (Abeill´e et al., 1998). 2 Related Work For other languages, people have proposed different representations for CPs and for some languages there are corpora available enhanced with CP labeling. The Prague TreeBank for Czech, which is based on a dependency grammar, labels CPs explicitly. A complex predicate is represented 100 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 100–108, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics tated as CPs (cf. the Penn Treebank (Marcus et al., 1993) and the Portuguese treebank CordialSIN (Carrilho and Magro, 2009)). by two nodes: the verb node is assigned a functor according to the function of the entire complex predicate in the sentence structure; the nominal node is assigned the CPHR functor, which signals that it is a part of a multi-word predicate, and is represented as an immediate daughter of the node for the verbal component (Mikulov´a et al., 2006; Cinkov´a and Kol´aˆrov´a, 2005). For German there is an example corpus annotated with verb phrases and light verbs (Fellbaum et al., 2006). However, only idiomatic expressions are labe"
W10-1812,xue-2006-annotating,0,0.0126587,"dress support verbs and observe that support verbs often occur with nouns expressing an event (Johansson and Nugues, 2006). In a Framenet semantic annotation, support verbs are not considered as parts of frames or as part of the frame elements, they are annotated with a specific ‘support verb’ label. We, on the contrary, view CP as one semantic and syntactic unit. In Nombank, a distinction is made between idioms (which in principle are not marked) and light verb plus noun combinations, which are to be annotated, and criteria are given to make such distinction (English (Meyers, 2007), Chinese (Xue, 2006)). In (1) we show a NomBank annotation example of the sentence with a complex predicate. Usually, CPs of the type verb+verb are treated as infinitive dependent clauses and are not anno(1) ‘The campaign takes advantage of the eye-catching photography.’ SUPPORT = takes REL = advantage ARG0 = the campaign ARG1 = of the eye-catching photography 3 Typology of complex predicates We consider CPs as constructions sharing certain properties defined in Butt (1995). A complex predicate has: a multi-headed and complex argument structure; more than one lexical unit, each contributing part of the informatio"
W10-1822,E09-1086,0,0.028714,"some work about the creation and representation of MWE lexicons (Baldwin and Kim, 2010). Most of the currently available corpora annotated with MWE information consist of a collection of extracted sentences containing a MWE (for example the data sets in the MWE 2008 shared task1 ). Fellbaum et al. (2006) report on a larger German example corpus consisting of MWEs with their surrounding sentences. There are also data sets specifically designed for automatic MWE identification, in which part of the sentences contains an idiomatic expression and the other part expresses a literal meaning (e.g. (Sporleder and Li, 2009)). An example of a balanced corpus fully annotated with MWEs is the Prague Treebank which is enriched with a diverse set of MWE annotations (B¨ohmov´a et al., 2005). Introduction Given the widespread studies of co-occurring words phenomenon, the term ‘multi-word expression’ (MWE) usually refers to a sequence of words that act as a single unit, embracing all different types of word combinations. Their study is of extreme importance for computational linguistics, where applications find notorious difficulties when dealing with them (Sag et al., 2002). Having a well-balanced corpus annotated with"
W10-1822,barreto-etal-2006-open,1,0.824565,"a single unit, embracing all different types of word combinations. Their study is of extreme importance for computational linguistics, where applications find notorious difficulties when dealing with them (Sag et al., 2002). Having a well-balanced corpus annotated with multi-word expressions offers the possibility to analyze the behavior of MWEs as they appear in running text. Such corpus will contain a rich and diversified set of MWE and also be an excellent resource to evaluate automatic MWE identification systems. Here we propose our approach to the manual annotation of the CINTIL corpus (Barreto et al., 2006) with MWE information. This Portuguese corpus of 1M tokens is a balanced corpus of both spoken and written data from different sources and has been previously annotated with linguistic information such as part-of-speech and lemma and inflection. As the starting point for our annotation project, we want to use a Portuguese MWE lexicon containing approximately 14,000 entries. The lexicon contains besides idiomatic expressions, also 3 MWE Lexicon Our annotation proposal uses information from a lexicon of MWE for Portuguese (available online2 ). This lexicon is implemented on a MySQL relational da"
W10-1822,calzolari-etal-2002-towards,0,0.0928003,"Missing"
W10-1822,J90-1003,0,0.342044,"For each of those several examples are collected from the corpus. Each MWE entry is also assigned 1 More infomation at: http://multiword.sourceforge.net/ MWE lexicon: http://www.clul.ul.pt/sectores/linguistica de corpus/manual combinatorias online.php 2 152 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 152–156, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 4.2 to one or multiple word lemmas, of a total number of 1180 single word lemmas. The MWE were selected from a sorted list of n-grams based on the mutual information measure (Church and Hanks, 1990) and validated manually (Mendes et al., 2006; Antunes et al., 2006; Bacelar do Nascimento et al., 2006). 4 When studying the MWE lexicon, we noticed different properties of MWEs according to their syntactic patterns. Consequently, we propose to divide our annotation guidelines according to each syntactic pattern and to establish different properties that enables us to distinguish literal from idiomatic usage. At the sentence level, MWEs such as proverbs or aphorisms (e.g. a´ gua mole em pedra dura tanto bate at´e que fura lit. ‘water in hard rock beats so long that it finally breaks’) have spe"
W10-1822,mendes-etal-2006-combina,1,0.829462,"d from the corpus. Each MWE entry is also assigned 1 More infomation at: http://multiword.sourceforge.net/ MWE lexicon: http://www.clul.ul.pt/sectores/linguistica de corpus/manual combinatorias online.php 2 152 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 152–156, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 4.2 to one or multiple word lemmas, of a total number of 1180 single word lemmas. The MWE were selected from a sorted list of n-grams based on the mutual information measure (Church and Hanks, 1990) and validated manually (Mendes et al., 2006; Antunes et al., 2006; Bacelar do Nascimento et al., 2006). 4 When studying the MWE lexicon, we noticed different properties of MWEs according to their syntactic patterns. Consequently, we propose to divide our annotation guidelines according to each syntactic pattern and to establish different properties that enables us to distinguish literal from idiomatic usage. At the sentence level, MWEs such as proverbs or aphorisms (e.g. a´ gua mole em pedra dura tanto bate at´e que fura lit. ‘water in hard rock beats so long that it finally breaks’) have specific properties: they do not accept any pos"
W13-2328,W13-0301,1,0.646996,"goal is to study closely how exclusive particles affect and alter the modal meaning of the sentence. By performing a systematic annotation of these interactions in examples drawn from a large corpus we better comprehend the role that these particles play and the different type of effects that exclusive particles can have. Most annotation schemes for modality focus on English but resources are now being developed for other languages including Portuguese. Hendrickx et al. (2012b) have previously developed an annotation scheme for European Portuguese and applied it to a corpus of 2000 sentences. Ávila & Mello (2013) presented an annotation scheme for Brazilian Portuguese speech data, applied to information units. Here we look at the interaction between focussensitive adverbs and modality and discuss how to integrate these findings in the annotation scheme of Hendrickx et al. (2012a). 228 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 228–237, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics The structure of the paper is as follows. In section 2, we review related work in the field of modality, its annotation in texts, focu"
W13-2328,baker-etal-2010-modality,0,0.395701,"is annotation scheme to a sample of 100 sentences. 1 Introduction Modality in language has been studied extensively (see Portner (2009) for an overview). In recent years, the study of modality has been associated with a trend in Information Extraction applications that aim to identify personal opinions in sentiment analysis and opinion mining (Wiebe et al., 2005), to identify events which are factual, probable or uncertain, as well as speculation and negation. This trend has lead to the development of several practical annotation schemes for modality (Sauri et al., 2006; Szarvas et al., 2008; Baker et al., 2010, Matsuyoshi et al., 2010). Most of these annotation schemes focus on the annotation of modal elements like modal verbs or adverbs, but in the present study we go one step deeper and discuss the complex interaction between modality and focus in Portuguese. Our notion of modality focuses on the expression of the opinion and attitude of the speaker or the agent towards the proposition (Palmer, 1986). This attitude or opinion towards a state or event can assume diverse values. For example, the speaker (or subject) may consider something to be possible, probable or certain (epistemic modality), he"
W13-2328,W10-3001,0,0.214612,"Missing"
W13-2328,genereux-etal-2012-introducing,1,0.355379,"ny of its variants. This is an attempt to put the two notions together and propose a scheme that describes the scope of exclusive particles and its impact on the meaning of the expressed modality. 3 Interaction between modal value adverbs and In this section, we discuss in detail the possible interactions between the adverb só and modal expressions in the text. We extracted our examples from the online search platform of the Corpus de Referência do Português Contemporâneo (CRPC)1, a highly diverse corpus of 312 million words covering a large variety of textual genres and Portuguese varieties (Généreux et al., 2012). The adverb só is considered a focus-sensitive particle (Beaver & Clark, 2008; Aloni et al., 1999), defined as a word which semantics “involves essential reference to the information structure of the sentence containing it” (Aloni et al., 1999:1). The meaning of only consists of asserting that no proposition from the set of relevant contrasts other than the one expressed is true (von Fintel, 1994). The standard views on exclusive particles consider that “the position of focal accent identi1 229 http://alfclul.clul.ul.pt/CQPweb/crpcweb23/index.php fies the constituent associated with only” (Dr"
W13-2328,hendrickx-etal-2012-modality,1,0.894962,"exclusive particles (Beaver and Clark, 2008), and, for the purposes of this paper, we will center our discussion on the adverb só ‘only’. Our goal is to study closely how exclusive particles affect and alter the modal meaning of the sentence. By performing a systematic annotation of these interactions in examples drawn from a large corpus we better comprehend the role that these particles play and the different type of effects that exclusive particles can have. Most annotation schemes for modality focus on English but resources are now being developed for other languages including Portuguese. Hendrickx et al. (2012b) have previously developed an annotation scheme for European Portuguese and applied it to a corpus of 2000 sentences. Ávila & Mello (2013) presented an annotation scheme for Brazilian Portuguese speech data, applied to information units. Here we look at the interaction between focussensitive adverbs and modality and discuss how to integrate these findings in the annotation scheme of Hendrickx et al. (2012a). 228 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 228–237, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Lingui"
W13-2328,W05-0310,0,0.268877,"Missing"
W13-2328,morante-2010-descriptive,0,0.018758,"he proposition (Palmer, 1986). This attitude or opinion towards a state or event can assume diverse values. For example, the speaker (or subject) may consider something to be possible, probable or certain (epistemic modality), he might be obliged or allowed to do it (deontic modality), or he wants or fears it (volitive modality). Frequently, several modal expressions interact to compose the overall modal meaning of the sentence. Non modal elements can also directly influence the modality type and alter the meaning of the sentence. One such element, rather well studied, is the negation marker (Morante, 2010; Morante and Sporleder, 2012). In this paper however we discuss the element focus, taken as a means to “give prominence to meaning-bearing elements in an expression.” (Krifka, 1995:240). The prominent constituent is called the focus, while the complement notion is called the background. We are especially concerned with the interaction between modality and a subtype of focus-sensitive expressions named exclusive particles (Beaver and Clark, 2008), and, for the purposes of this paper, we will center our discussion on the adverb só ‘only’. Our goal is to study closely how exclusive particles aff"
W13-2328,J12-2001,0,0.129504,"(Palmer, 1986). This attitude or opinion towards a state or event can assume diverse values. For example, the speaker (or subject) may consider something to be possible, probable or certain (epistemic modality), he might be obliged or allowed to do it (deontic modality), or he wants or fears it (volitive modality). Frequently, several modal expressions interact to compose the overall modal meaning of the sentence. Non modal elements can also directly influence the modality type and alter the meaning of the sentence. One such element, rather well studied, is the negation marker (Morante, 2010; Morante and Sporleder, 2012). In this paper however we discuss the element focus, taken as a means to “give prominence to meaning-bearing elements in an expression.” (Krifka, 1995:240). The prominent constituent is called the focus, while the complement notion is called the background. We are especially concerned with the interaction between modality and a subtype of focus-sensitive expressions named exclusive particles (Beaver and Clark, 2008), and, for the purposes of this paper, we will center our discussion on the adverb só ‘only’. Our goal is to study closely how exclusive particles affect and alter the modal meanin"
W13-2328,W08-0606,0,0.0800666,"the application of this annotation scheme to a sample of 100 sentences. 1 Introduction Modality in language has been studied extensively (see Portner (2009) for an overview). In recent years, the study of modality has been associated with a trend in Information Extraction applications that aim to identify personal opinions in sentiment analysis and opinion mining (Wiebe et al., 2005), to identify events which are factual, probable or uncertain, as well as speculation and negation. This trend has lead to the development of several practical annotation schemes for modality (Sauri et al., 2006; Szarvas et al., 2008; Baker et al., 2010, Matsuyoshi et al., 2010). Most of these annotation schemes focus on the annotation of modal elements like modal verbs or adverbs, but in the present study we go one step deeper and discuss the complex interaction between modality and focus in Portuguese. Our notion of modality focuses on the expression of the opinion and attitude of the speaker or the agent towards the proposition (Palmer, 1986). This attitude or opinion towards a state or event can assume diverse values. For example, the speaker (or subject) may consider something to be possible, probable or certain (epi"
W13-2328,matsuyoshi-etal-2010-annotating,0,0.340293,"to a sample of 100 sentences. 1 Introduction Modality in language has been studied extensively (see Portner (2009) for an overview). In recent years, the study of modality has been associated with a trend in Information Extraction applications that aim to identify personal opinions in sentiment analysis and opinion mining (Wiebe et al., 2005), to identify events which are factual, probable or uncertain, as well as speculation and negation. This trend has lead to the development of several practical annotation schemes for modality (Sauri et al., 2006; Szarvas et al., 2008; Baker et al., 2010, Matsuyoshi et al., 2010). Most of these annotation schemes focus on the annotation of modal elements like modal verbs or adverbs, but in the present study we go one step deeper and discuss the complex interaction between modality and focus in Portuguese. Our notion of modality focuses on the expression of the opinion and attitude of the speaker or the agent towards the proposition (Palmer, 1986). This attitude or opinion towards a state or event can assume diverse values. For example, the speaker (or subject) may consider something to be possible, probable or certain (epistemic modality), he might be obliged or allow"
W13-2328,J12-2006,0,\N,Missing
W14-0704,N03-1020,0,0.0335552,"30 million tokens; on the other hand, we want to use n-gram analyses to investigate the semantic connection between the segments in a want versus omdat connection. The use of n-grams to measure semantic overlap is a well known method, which has been applied in the standard evaluation metrics for tasks like machine translation and automatic summarization. In these tasks automatic systems aim to produce a text as similar as possible to a manually constructed gold standard text. To evaluate the quality of these automatically produced text, measures such as BLUE (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003) measure n-gram overlap between the system text and the gold standard text. Furthermore, in other types of research like in the field of literary studies n-grams have been applied, for example to discriminate between genres (Louwerse et al., 2008) or for author discrimination (Hirst and Feiguina, 2007). Backward causal connectives denotes a cause relation. The connective is positioned in a sentence between the consequence (denoted as Q) and the cause (denoted as P). For the sentence in example 1 Q is the text segment before the connective, and P contains all words after the connective as follo"
W14-0704,P02-1040,0,0.12844,"l. using a larger corpus of about 30 million tokens; on the other hand, we want to use n-gram analyses to investigate the semantic connection between the segments in a want versus omdat connection. The use of n-grams to measure semantic overlap is a well known method, which has been applied in the standard evaluation metrics for tasks like machine translation and automatic summarization. In these tasks automatic systems aim to produce a text as similar as possible to a manually constructed gold standard text. To evaluate the quality of these automatically produced text, measures such as BLUE (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003) measure n-gram overlap between the system text and the gold standard text. Furthermore, in other types of research like in the field of literary studies n-grams have been applied, for example to discriminate between genres (Louwerse et al., 2008) or for author discrimination (Hirst and Feiguina, 2007). Backward causal connectives denotes a cause relation. The connective is positioned in a sentence between the consequence (denoted as Q) and the cause (denoted as P). For the sentence in example 1 Q is the text segment before the connective, and P contains all word"
W15-0301,W13-0301,1,0.845334,"ities), given its broad scope to written and oral data. Indeed, although more modality schemes have become available in the recent years, their contrastive study is hampered by the diversity of modal values that are included, and so is the evaluation of tools for the automatic annotation of modality. The two schemes for Portuguese differ mainly in what concerns the text type that they apply to: the scheme proposed for European Portuguese (Hendrickx et al., 2012a; 2012b) has been designed and applied over written texts, while the scheme for Brazilian Portuguese targets spontaneous speech data (Ávila and Mello, 2013; Ávila, 2014). We will revise and compare the two schemes in section 3, report on their application to corpora in section 4 and attempt a unified perspective in section 5. 2. Related work The growing interest on separating facts from speculations results from the importance of this task to NLP applications, as information extraction (Kartunnen and Zaennen, 2005); uncertainty modelling of clinical texts (Mowery et al., 2012); question answering (Saurí et al., 2006); classification of hedges (Medlock and Briscoe, 2007; Morante and Daelemans, 2009); and sentiment analysis (Wiebe et al., 2005). A"
W15-0301,baker-etal-2010-modality,0,0.369482,"al., 2005). Annotating modality, in order to allow its automatic recognition, includes identifying modal indexes, classifying them in a given typology (e.g. in epistemic and non-epistemic meanings), defining its source and its semantic scope. Many projects that have been developed for the annotation of modal expressions focus mostly on English and on modal auxiliaries. Some highlight the relationship between modality and negation (Morante and Sporleder, 2012; Baker et al., 2012), the annotation of modal verbs meanings (Ruppenhofer and Rehbein, 2012), or the construction of automatic taggers (Baker et al., 2010). There are also annotation efforts undertaken for other languages, such as the work on Chinese (Cui and Chi, 2013), European Portuguese (Hendrickx et al., 2012; Mendes et al., 2013), and Brazilian Portuguese spoken data (Ávila and Mello, 2013; Ávila, 2014). Opposed to classical linguistic typologies of modality, these schemes describe in detail which elements in the text are actually involved in the expression of modality and their roles. These are the subject of the modality (source) and the elements in its scope (target/scope/focus). Other schemes (Baker et al., 2010; Matsuyoshi et al., 201"
W15-0301,J12-2006,0,0.141211,"aurí et al., 2006); classification of hedges (Medlock and Briscoe, 2007; Morante and Daelemans, 2009); and sentiment analysis (Wiebe et al., 2005). Annotating modality, in order to allow its automatic recognition, includes identifying modal indexes, classifying them in a given typology (e.g. in epistemic and non-epistemic meanings), defining its source and its semantic scope. Many projects that have been developed for the annotation of modal expressions focus mostly on English and on modal auxiliaries. Some highlight the relationship between modality and negation (Morante and Sporleder, 2012; Baker et al., 2012), the annotation of modal verbs meanings (Ruppenhofer and Rehbein, 2012), or the construction of automatic taggers (Baker et al., 2010). There are also annotation efforts undertaken for other languages, such as the work on Chinese (Cui and Chi, 2013), European Portuguese (Hendrickx et al., 2012; Mendes et al., 2013), and Brazilian Portuguese spoken data (Ávila and Mello, 2013; Ávila, 2014). Opposed to classical linguistic typologies of modality, these schemes describe in detail which elements in the text are actually involved in the expression of modality and their roles. These are the subject"
W15-0301,W10-3001,0,0.0225433,"t-dependent component to encompass the cases in which the target, in a given utterance, is not explicit, but it can be recoverable in the referential chain of the text. The two different types of sources are marked up to capture cases where the conceptualizer of the modality is not the producer of the text or speech. While the components of both schemes are practically the same, their conceptualization and application differ according to options in the delimitation of Trigger and Target and, mainly, to the text type which is annotated. For instance, the EP scheme follows a “min-max strategy” (Farkas et al., 2010) in which the Trigger is tagged as a single element whenever possible and the Target is tagged maximally (covering possible discontinuous sequences), while the BP scheme frequently selects multiword triggers. But the most significant difference falls on the Target component. The identification of the limits of the target is always a challenge, especially in what concerns consistency between annotators. In written texts the scope of the target is of a syntactic nature and the EP scheme specifies that syntactic boundaries should be respected. In spoken data, the target is in the scope of an info"
W15-0301,genereux-etal-2012-introducing,1,0.861308,"polarity, that interacts with the modality scheme (e.g. Morante, 2010) or to deal with both in a unified scheme (e.g. Baker et al., 2012). The approach taken leans towards the second option, although very tentatively. A specific study in the interaction between modal triggers and focus (the exclusive particle só ‘only’) was also addressed by the EP scheme (Mendes et al., 2013). 4. Application to corpora The EP scheme has been applied to a corpus of 158.553 tokens, composed of 2000 sentences of written texts extracted from the written subpart of the Reference Corpus of Contemporary Portuguese (Généreux et al., 2012), a highly diverse corpus of 312 million words covering a large variety of textual genres and Portuguese varieties. A list of 40 Portuguese lemma verbs covering each modal value was the starting point for the extraction of the corpus sample and equal sets of single sentences for each modal type were randomly selected. Subsequently, the annotation covered all modal triggers found in the sentences. The BP scheme was applied to a sample from the C-ORAL-BRASIL I, an informal corpus of 139 texts, already published (Raso and Mello, 2012). The sample for modality annotation covers a sub-corpus of 20"
W15-0301,hendrickx-etal-2012-modality,1,0.939763,"se two proposals, to be soon implemented. We also consider that this proposal could be applied to other languages than Portuguese (regarding their particularities), given its broad scope to written and oral data. Indeed, although more modality schemes have become available in the recent years, their contrastive study is hampered by the diversity of modal values that are included, and so is the evaluation of tools for the automatic annotation of modality. The two schemes for Portuguese differ mainly in what concerns the text type that they apply to: the scheme proposed for European Portuguese (Hendrickx et al., 2012a; 2012b) has been designed and applied over written texts, while the scheme for Brazilian Portuguese targets spontaneous speech data (Ávila and Mello, 2013; Ávila, 2014). We will revise and compare the two schemes in section 3, report on their application to corpora in section 4 and attempt a unified perspective in section 5. 2. Related work The growing interest on separating facts from speculations results from the importance of this task to NLP applications, as information extraction (Kartunnen and Zaennen, 2005); uncertainty modelling of clinical texts (Mowery et al., 2012); question answe"
W15-0301,matsuyoshi-etal-2010-annotating,0,0.376739,"s (Baker et al., 2010). There are also annotation efforts undertaken for other languages, such as the work on Chinese (Cui and Chi, 2013), European Portuguese (Hendrickx et al., 2012; Mendes et al., 2013), and Brazilian Portuguese spoken data (Ávila and Mello, 2013; Ávila, 2014). Opposed to classical linguistic typologies of modality, these schemes describe in detail which elements in the text are actually involved in the expression of modality and their roles. These are the subject of the modality (source) and the elements in its scope (target/scope/focus). Other schemes (Baker et al., 2010; Matsuyoshi et al., 2010; Sauri et al., 2006) also determine the relation between sentences in text, identifying temporal and conditional relations between events or the evaluation of the degree of relevance of some information within a text, rather than classifying modal values. For more contrastive information on the existing annotation schemes, see an overview in Nissim et al. (2013). 3. Annotation schemes for Portuguese While the modal scheme for EP has been designed and applied to written texts, the modal scheme for BP is designed for spontaneous speech, and it is more theoretically-oriented. Modality is taken i"
W15-0301,W05-0310,0,0.08028,"Missing"
W15-0301,W13-2328,1,0.434475,"istemic meanings), defining its source and its semantic scope. Many projects that have been developed for the annotation of modal expressions focus mostly on English and on modal auxiliaries. Some highlight the relationship between modality and negation (Morante and Sporleder, 2012; Baker et al., 2012), the annotation of modal verbs meanings (Ruppenhofer and Rehbein, 2012), or the construction of automatic taggers (Baker et al., 2010). There are also annotation efforts undertaken for other languages, such as the work on Chinese (Cui and Chi, 2013), European Portuguese (Hendrickx et al., 2012; Mendes et al., 2013), and Brazilian Portuguese spoken data (Ávila and Mello, 2013; Ávila, 2014). Opposed to classical linguistic typologies of modality, these schemes describe in detail which elements in the text are actually involved in the expression of modality and their roles. These are the subject of the modality (source) and the elements in its scope (target/scope/focus). Other schemes (Baker et al., 2010; Matsuyoshi et al., 2010; Sauri et al., 2006) also determine the relation between sentences in text, identifying temporal and conditional relations between events or the evaluation of the degree of relevan"
W15-0301,J12-2001,0,0.0217484,"2012); question answering (Saurí et al., 2006); classification of hedges (Medlock and Briscoe, 2007; Morante and Daelemans, 2009); and sentiment analysis (Wiebe et al., 2005). Annotating modality, in order to allow its automatic recognition, includes identifying modal indexes, classifying them in a given typology (e.g. in epistemic and non-epistemic meanings), defining its source and its semantic scope. Many projects that have been developed for the annotation of modal expressions focus mostly on English and on modal auxiliaries. Some highlight the relationship between modality and negation (Morante and Sporleder, 2012; Baker et al., 2012), the annotation of modal verbs meanings (Ruppenhofer and Rehbein, 2012), or the construction of automatic taggers (Baker et al., 2010). There are also annotation efforts undertaken for other languages, such as the work on Chinese (Cui and Chi, 2013), European Portuguese (Hendrickx et al., 2012; Mendes et al., 2013), and Brazilian Portuguese spoken data (Ávila and Mello, 2013; Ávila, 2014). Opposed to classical linguistic typologies of modality, these schemes describe in detail which elements in the text are actually involved in the expression of modality and their roles."
W15-0301,W12-2407,0,0.028963,"n Portuguese (Hendrickx et al., 2012a; 2012b) has been designed and applied over written texts, while the scheme for Brazilian Portuguese targets spontaneous speech data (Ávila and Mello, 2013; Ávila, 2014). We will revise and compare the two schemes in section 3, report on their application to corpora in section 4 and attempt a unified perspective in section 5. 2. Related work The growing interest on separating facts from speculations results from the importance of this task to NLP applications, as information extraction (Kartunnen and Zaennen, 2005); uncertainty modelling of clinical texts (Mowery et al., 2012); question answering (Saurí et al., 2006); classification of hedges (Medlock and Briscoe, 2007; Morante and Daelemans, 2009); and sentiment analysis (Wiebe et al., 2005). Annotating modality, in order to allow its automatic recognition, includes identifying modal indexes, classifying them in a given typology (e.g. in epistemic and non-epistemic meanings), defining its source and its semantic scope. Many projects that have been developed for the annotation of modal expressions focus mostly on English and on modal auxiliaries. Some highlight the relationship between modality and negation (Morant"
W15-0301,W13-0501,0,0.0133949,"l which elements in the text are actually involved in the expression of modality and their roles. These are the subject of the modality (source) and the elements in its scope (target/scope/focus). Other schemes (Baker et al., 2010; Matsuyoshi et al., 2010; Sauri et al., 2006) also determine the relation between sentences in text, identifying temporal and conditional relations between events or the evaluation of the degree of relevance of some information within a text, rather than classifying modal values. For more contrastive information on the existing annotation schemes, see an overview in Nissim et al. (2013). 3. Annotation schemes for Portuguese While the modal scheme for EP has been designed and applied to written texts, the modal scheme for BP is designed for spontaneous speech, and it is more theoretically-oriented. Modality is taken in enunciative terms (Bally, 1932), that is, it stands for the point of view of a subject who evaluates the locutory material in a given utterance in a communicative act. The scheme follows the Language Into Act Theory (Cresti, 2000), which takes the utterance as its reference unit, and considers the scope of the modality to be the information unit (Tucci, 2007)."
W15-0301,ruppenhofer-rehbein-2012-yes,0,0.179649,"scoe, 2007; Morante and Daelemans, 2009); and sentiment analysis (Wiebe et al., 2005). Annotating modality, in order to allow its automatic recognition, includes identifying modal indexes, classifying them in a given typology (e.g. in epistemic and non-epistemic meanings), defining its source and its semantic scope. Many projects that have been developed for the annotation of modal expressions focus mostly on English and on modal auxiliaries. Some highlight the relationship between modality and negation (Morante and Sporleder, 2012; Baker et al., 2012), the annotation of modal verbs meanings (Ruppenhofer and Rehbein, 2012), or the construction of automatic taggers (Baker et al., 2010). There are also annotation efforts undertaken for other languages, such as the work on Chinese (Cui and Chi, 2013), European Portuguese (Hendrickx et al., 2012; Mendes et al., 2013), and Brazilian Portuguese spoken data (Ávila and Mello, 2013; Ávila, 2014). Opposed to classical linguistic typologies of modality, these schemes describe in detail which elements in the text are actually involved in the expression of modality and their roles. These are the subject of the modality (source) and the elements in its scope (target/scope/fo"
W15-0301,P07-1125,0,\N,Missing
W17-0806,W15-1621,0,0.0172827,"attitude/ speech/ perception embedding entity, like confessed in (2), and the report complement, here that he was in love. rapid annotation tool BRAT is an open source web-based tool for text annotation (Stenetorp et al., 2012)3 and is an extension of stav, a visualization tool that was designed initially for complex semantic annotations for information extraction in the bio-medical domain including entities, events and their relations (Ohta et al., 2012; Neves et al., 2012). BRAT has been used in many different linguistic annotation projects that require complex annotation such as ellipsis (Anand and McCloskey, 2015), co-reference resolution (Kilicoglu and Demner-Fushman, 2016), and syntactic chunks (Savkov et al., 2016). As BRAT uses a server-based web interface, annotators can access it in a web browser on their own computer without the need for further installation of software. All annotations are conveniently stored on the server. We considered several other possible annotation tools for our project, such as MMAX2 (M¨uller and Strube, 2006), GATE Teamware (Bontcheva et al., 2013) and Arethusa4 . The main reasons for se(2) John confessed that he was in love. The attitude/speech/perception embedding ent"
W17-0806,hendrickx-etal-2012-modality,1,0.824693,"e ANNIS search tool (Krause and Zeldes, 2016) in a later stage to carry out more complex search queries. Previous attempts at corpus annotation for related topics include the annotation of committed belief for English (Diab et al., 2009) and the annotation of direct and indirect speech in Portuguese (Freitas et al., 2016). Our project differs from the former in its focus on complementation (rather than information retrieval) and from the latter in its broader scope (reports in general rather than only speech). Also related are the annotation schemes for modality such as (McShane et al., 2004; Hendrickx et al., 2012). These schemes aim to grasp the attitude of the actual speaker towards the proposition and label such attitudes as for example belief or obligation. In contrast to modality annotation, which focuses on the attitude of the actual speaker, we are interested in speech, attitude and perception acriptions in general, including ascriptions to other people than the actual speaker. Another difference is our focus on the linguistic constructions used. In that respect our scheme also differs from (Wiebe et al., 2005), which, like RAG, annotates what we call reports, but without differentiating between"
W17-0806,W09-3012,0,0.0310053,"he CoNNL shared task format, MALT XML5 for parsing and the BIO format (Ramshaw and Marcus, 1995). BRAT stores the annotation in a rather simple plain text standoff format that is merely a list of character spans and their assigned labels and relations, but that can easily be converted to other formats for further exploitation or search. We plan to port the annotated corpus to the ANNIS search tool (Krause and Zeldes, 2016) in a later stage to carry out more complex search queries. Previous attempts at corpus annotation for related topics include the annotation of committed belief for English (Diab et al., 2009) and the annotation of direct and indirect speech in Portuguese (Freitas et al., 2016). Our project differs from the former in its focus on complementation (rather than information retrieval) and from the latter in its broader scope (reports in general rather than only speech). Also related are the annotation schemes for modality such as (McShane et al., 2004; Hendrickx et al., 2012). These schemes aim to grasp the attitude of the actual speaker towards the proposition and label such attitudes as for example belief or obligation. In contrast to modality annotation, which focuses on the attitud"
W17-0806,W12-4304,0,0.0197308,"cheme REPORTS consists of entities, events and attributes of both. Entities are (possibly discontinuous) spans of text. Let’s start with two central ones, the attitude/ speech/ perception embedding entity, like confessed in (2), and the report complement, here that he was in love. rapid annotation tool BRAT is an open source web-based tool for text annotation (Stenetorp et al., 2012)3 and is an extension of stav, a visualization tool that was designed initially for complex semantic annotations for information extraction in the bio-medical domain including entities, events and their relations (Ohta et al., 2012; Neves et al., 2012). BRAT has been used in many different linguistic annotation projects that require complex annotation such as ellipsis (Anand and McCloskey, 2015), co-reference resolution (Kilicoglu and Demner-Fushman, 2016), and syntactic chunks (Savkov et al., 2016). As BRAT uses a server-based web interface, annotators can access it in a web browser on their own computer without the need for further installation of software. All annotations are conveniently stored on the server. We considered several other possible annotation tools for our project, such as MMAX2 (M¨uller and Strube, 20"
W17-0806,W95-0107,0,0.40402,"arks. https://github.com/GreekPerspective 47 2 Related work lecting BRAT as tool for the implementation of our annotation scheme were its web interface and its flexibility: BRAT accommodates the annotation of discontinuous spans as one entity and supports different types of relations and attributes. Furthermore, BRAT offers a simple search interface and contains a tool for comparison of different versions of annotations on the same source text. BRAT also includes conversion scripts to convert several input formats such as the CoNNL shared task format, MALT XML5 for parsing and the BIO format (Ramshaw and Marcus, 1995). BRAT stores the annotation in a rather simple plain text standoff format that is merely a list of character spans and their assigned labels and relations, but that can easily be converted to other formats for further exploitation or search. We plan to port the annotated corpus to the ANNIS search tool (Krause and Zeldes, 2016) in a later stage to carry out more complex search queries. Previous attempts at corpus annotation for related topics include the annotation of committed belief for English (Diab et al., 2009) and the annotation of direct and indirect speech in Portuguese (Freitas et al"
W17-0806,E12-2021,0,0.0450962,"which, like RAG, annotates what we call reports, but without differentiating between e.g. different kinds of complements. 3 BRAT 4 REPORTS: an annotation scheme for speech, attitude and perception reports 4.1 The scheme The annotation scheme REPORTS consists of entities, events and attributes of both. Entities are (possibly discontinuous) spans of text. Let’s start with two central ones, the attitude/ speech/ perception embedding entity, like confessed in (2), and the report complement, here that he was in love. rapid annotation tool BRAT is an open source web-based tool for text annotation (Stenetorp et al., 2012)3 and is an extension of stav, a visualization tool that was designed initially for complex semantic annotations for information extraction in the bio-medical domain including entities, events and their relations (Ohta et al., 2012; Neves et al., 2012). BRAT has been used in many different linguistic annotation projects that require complex annotation such as ellipsis (Anand and McCloskey, 2015), co-reference resolution (Kilicoglu and Demner-Fushman, 2016), and syntactic chunks (Savkov et al., 2016). As BRAT uses a server-based web interface, annotators can access it in a web browser on their"
