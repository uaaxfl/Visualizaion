2021.naacl-main.240,Controlling Dialogue Generation with Semantic Exemplars,2021,-1,-1,3,0,3963,prakhar gupta,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Dialogue systems pretrained with large language models generate locally coherent responses, but lack fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide response generation. We show that controlling dialogue generation based on the semantic frames of exemplars improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses."
2021.naacl-main.383,Understanding Factuality in Abstractive Summarization with {FRANK}: A Benchmark for Factuality Metrics,2021,-1,-1,3,0,4382,artidoro pagnoni,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses."
2021.mrqa-1.16,Simple and Efficient ways to Improve {REALM},2021,-1,-1,3,0,4383,vidhisha balachandran,Proceedings of the 3rd Workshop on Machine Reading for Question Answering,0,"Dense retrieval has been shown to be effective for Open Domain Question Answering, surpassing sparse retrieval methods like BM25. One such model, REALM, (Guu et al., 2020) is an end-to-end dense retrieval system that uses MLM based pretraining for improved downstream QA performance. However, the current REALM setup uses limited resources and is not comparable in scale to more recent systems, contributing to its lower performance. Additionally, it relies on noisy supervision for retrieval during fine-tuning. We propose REALM++, where we improve upon the training and inference setups and introduce better supervision signal for improving performance, without any architectural changes. REALM++ achieves {\textasciitilde}5.5{\%} absolute accuracy gains over the baseline while being faster to train. It also matches the performance of large models which have 3x more parameters demonstrating the efficiency of our setup."
2021.mrl-1.15,Improving the Diversity of Unsupervised Paraphrasing with Embedding Outputs,2021,-1,-1,4,0,5227,monisha jegadeesan,Proceedings of the 1st Workshop on Multilingual Representation Learning,0,"We present a novel technique for zero-shot paraphrase generation. The key contribution is an end-to-end multilingual paraphrasing model that is trained using translated parallel corpora to generate paraphrases into {``}meaning spaces{''} {--} replacing the final softmax layer with word embeddings. This architectural modification, plus a training procedure that incorporates an autoencoding objective, enables effective parameter sharing across languages for more fluent monolingual rewriting, and facilitates fluency and diversity in the generated outputs. Our continuous-output paraphrase generation models outperform zero-shot paraphrasing baselines when evaluated on two languages using a battery of computational metrics as well as in human assessment."
2021.findings-emnlp.63,Efficient Test Time Adapter Ensembling for Low-resource Language Varieties,2021,-1,-1,2,0.869565,3348,xinyi wang,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires training a separate language adapter for every language one wishes to support, which can be impractical for languages with limited data. An intuitive solution is to use a related language adapter for the new language variety, but we observe that this solution can lead to sub-optimal performance. In this paper, we aim to improve the robustness of language adapters to uncovered languages without training new adapters. We find that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters. Building upon this observation, we propose Entropy Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. Experiments on three diverse groups of language varieties show that our method leads to significant improvements on both named entity recognition and part-of-speech tagging across all languages."
2021.findings-emnlp.288,Detecting Community Sensitive Norm Violations in Online Conversations,2021,-1,-1,7,1,7117,chan park,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Online platforms and communities establish their own norms that govern what behavior is acceptable within the community. Substantial effort in NLP has focused on identifying unacceptable behaviors and, recently, on forecasting them before they occur. However, these efforts have largely focused on toxicity as the sole form of community norm violation. Such focus has overlooked the much larger set of rules that moderators enforce. Here, we introduce a new dataset focusing on a more complete spectrum of community norms and their violations in the local conversational and global community contexts. We introduce a series of models that use this data to develop context- and community-sensitive norm violation detection, showing that these changes give high performance."
2021.findings-emnlp.374,Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates,2021,-1,-1,2,1,7341,xiaochuang han,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Among the most critical limitations of deep learning NLP models are their lack of interpretability, and their reliance on spurious correlations. Prior work proposed various approaches to interpreting the black-box models to unveil the spurious correlations, but the research was primarily used in human-computer interaction scenarios. It still remains underexplored whether or how such model interpretations can be used to automatically {``}unlearn{''} confounding features. In this work, we propose influence tuning{---}a procedure that leverages model interpretations to update the model parameters towards a plausible interpretation (rather than an interpretation that relies on spurious patterns in the data) in addition to learning to predict the task labels. We show that in a controlled setup, influence tuning can help deconfounding the model from spurious patterns in data, significantly outperforming baseline methods that use adversarial training."
2021.findings-acl.338,Synthesizing Adversarial Negative Responses for Robust Response Ranking and Evaluation,2021,-1,-1,2,0,3963,prakhar gupta,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.64,{SELFEXPLAIN}: A Self-Explaining Architecture for Neural Text Classifiers,2021,-1,-1,4,0,8551,dheeraj rajagopal,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We introduce SelfExplain, a novel self-explaining model that explains a text classifier{'}s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SelfExplain facilitates interpretability without sacrificing performance. Most importantly, explanations from SelfExplain show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines."
2021.emnlp-main.570,Evaluating the Morphosyntactic Well-formedness of Generated Texts,2021,-1,-1,7,0.45977,153,adithya pratapa,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L{'}AMBRE {--} a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages."
2021.eacl-main.204,Cross-Cultural Similarity Features for Cross-Lingual Transfer Learning of Pragmatically Motivated Tasks,2021,-1,-1,4,0,10818,jimin sun,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Much work in cross-lingual transfer learning explored how to select better transfer languages for multilingual tasks, primarily focusing on typological and genealogical similarities between languages. We hypothesize that these measures of linguistic proximity are not enough when working with pragmatically-motivated tasks, such as sentiment analysis. As an alternative, we introduce three linguistic features that capture cross-cultural similarities that manifest in linguistic patterns and quantify distinct aspects of language pragmatics: language context-level, figurative language, and the lexification of emotion concepts. Our analyses show that the proposed pragmatic features do capture cross-cultural similarities and align well with existing work in sociolinguistics and linguistic anthropology. We further corroborate the effectiveness of pragmatically-driven transfer in the downstream task of choosing transfer languages for cross-lingual sentiment analysis."
2021.eacl-main.220,{S}truct{S}um: Summarization via Structured Representations,2021,-1,-1,6,0,4383,vidhisha balachandran,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines."
2021.crac-1.13,Improving Span Representation for Domain-adapted Coreference Resolution,2021,-1,-1,3,0,11319,nupoor gandhi,"Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference",0,"Recent work has shown fine-tuning neural coreference models can produce strong performance when adapting to different domains. However, at the same time, this can require a large amount of annotated target examples. In this work, we focus on supervised domain adaptation for clinical notes, proposing the use of concept knowledge to more efficiently adapt coreference models to a new domain. We develop methods to improve the span representations via (1) a retrofitting loss to incentivize span representations to satisfy a knowledge-based distance function and (2) a scaffolding loss to guide the recovery of knowledge from the span representation. By integrating these losses, our model is able to improve our baseline precision and F-1 score. In particular, we show that incorporating knowledge with end-to-end coreference models results in better performance on the most challenging, domain-specific spans."
2021.acl-short.16,Machine Translation into Low-resource Language Varieties,2021,-1,-1,4,1,5228,sachin kumar,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"State-of-the-art machine translation (MT) systems are typically trained to generate {``}standard{''} target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (source{--}variety) data. This also includes adaptation of MT systems to low-resource typologically-related target languages. We experiment with adapting an English{--}Russian MT system to generate Ukrainian and Belarusian, an English{--}Norwegian Bokm{\aa}l system to generate Nynorsk, and an English{--}Arabic system to generate four Arabic dialects, obtaining significant improvements over competitive baselines."
2021.acl-long.149,"A Survey of Race, Racism, and Anti-Racism in {NLP}",2021,-1,-1,4,1,11320,anjalie field,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices."
2020.socialnlp-1.2,Demoting Racial Bias in Hate Speech Detection,2020,25,0,3,0,3358,mengzhou xia,Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,0,"In the task of hate speech detection, there exists a high correlation between African American English (AAE) and annotators{'} perceptions of toxicity in current datasets. This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech (high false positive rate) by current hate speech classifiers. Here, we use adversarial training to mitigate this bias. Experimental results on one hate speech dataset and one AAE dataset suggest that our method is able to reduce the false positive rate for AAE text with only a minimal compromise on the performance of hate speech classification."
2020.semeval-1.230,{LTI}at{CMU} at {S}em{E}val-2020 Task 11: Incorporating Multi-Level Features for Multi-Granular Propaganda Span Identification,2020,-1,-1,5,0,2579,sopan khosla,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The {''}multi-granular{''} model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge."
2020.scil-1.32,What Code-Switching Strategies are Effective in Dialog Systems?,2020,0,0,3,1,11447,emily ahn,Proceedings of the Society for Computation in Linguistics 2020,0,None
2020.scil-1.43,Where New Words Are Born: Distributional Semantic Analysis of Neologisms and Their Semantic Neighborhoods,2020,32,0,5,0,1349,maria ryskina,Proceedings of the Society for Computation in Linguistics 2020,0,"We perform statistical analysis of the phenomenon of neology, the process by which new words emerge in a language, using large diachronic corpora of English. We investigate the importance of two factors, semantic sparsity and frequency growth rates of semantic neighbors, formalized in the distributional semantics paradigm. We show that both factors are predictive of word emergence although we find more support for the latter hypothesis. Besides presenting a new linguistic application of distributional semantics, this study tackles the linguistic question of the role of language-internal factors (in our case, sparsity) in language change motivated by language-external factors (reflected in frequency growth)."
2020.ngt-1.7,A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with Bilingual Semantic Similarity Rewards,2020,-1,-1,3,0,3585,ziyi dou,Proceedings of the Fourth Workshop on Neural Generation and Translation,0,"Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English{--}Chinese and English{--}German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines."
2020.emnlp-main.44,Unsupervised Discovery of Implicit Gender Bias,2020,47,0,2,1,11320,anjalie field,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements."
2020.emnlp-main.359,On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment,2020,-1,-1,3,0,20104,zirui wang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers{'} generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations."
2020.emnlp-main.422,Automatic Extraction of Rules Governing Morphological Agreement,2020,-1,-1,6,0.869565,831,aditi chaudhary,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world{'}s languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78{\%}. We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/"
2020.emnlp-main.622,Fortifying Toxic Speech Detectors Against Veiled Toxicity,2020,-1,-1,2,1,7341,xiaochuang han,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector{'}s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity."
2020.conll-1.46,Understanding Linguistic Accommodation in Code-Switched Human-Machine Dialogues,2020,-1,-1,3,0,20995,tanmay parekh,Proceedings of the 24th Conference on Computational Natural Language Learning,0,"Code-switching is a ubiquitous phenomenon in multilingual communities. Natural language technologies that wish to communicate like humans must therefore adaptively incorporate code-switching techniques when they are deployed in multilingual settings. To this end, we propose a Hindi-English human-machine dialogue system that elicits code-switching conversations in a controlled setting. It uses different code-switching agent strategies to understand how users respond and accommodate to the agent{'}s language choice. Through this system, we collect and release a new dataset CommonDost, comprising of 439 human-machine multilingual conversations. We adapt pre-defined metrics to discover linguistic accommodation from users to agents. Finally, we compare these dialogues with Spanish-English dialogues collected in a similar setting, and analyze the impact of linguistic and socio-cultural factors on code-switching patterns across the two language pairs."
2020.acl-main.492,Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions,2020,38,0,3,1,7341,xiaochuang han,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which {`}saliency maps{'} may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data."
2020.acl-main.754,Balancing Training for Multilingual Neural Machine Translation,2020,38,1,2,0.869565,3348,xinyi wang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized."
W19-5943,A Dynamic Strategy Coach for Effective Negotiation,2019,25,0,4,0,23783,yiheng zhou,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"Negotiation is a complex activity involving strategic reasoning, persuasion, and psychology. An average person is often far from an expert in negotiation. Our goal is to assist humans to become better negotiators through a machine-in-the-loop approach that combines machine{'}s advantage at data-driven decision-making and human{'}s language generation ability. We consider a bargaining scenario where a seller and a buyer negotiate the price of an item for sale through a text-based dialogue. Our negotiation coach monitors messages between them and recommends strategies in real time to the seller to get a better deal (e.g., {``}reject the proposal and propose a price{''}, {``}talk about your personal experience with the product{''}). The best strategy largely depends on the context (e.g., the current price, the buyer{'}s attitude). Therefore, we first identify a set of negotiation strategies, then learn to predict the best strategy in a given dialogue context from a set of human-human bargaining dialogues. Evaluation on human-human dialogues shows that our coach increases the profits of the seller by almost 60{\%}."
W19-4208,{CMU}-01 at the {SIGMORPHON} 2019 Shared Task on Crosslinguality and Context in Morphology,2019,21,0,6,0.869565,831,aditi chaudhary,"Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and Lemmatization in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 treebanks. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, Case, etc.) independently. However, most treebanks are under-resourced, thus making it challenging to train deep neural models for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology."
W19-3823,Measuring Bias in Contextualized Word Representations,2019,25,4,5,0,22746,keita kurita,Proceedings of the First Workshop on Gender Bias in Natural Language Processing,0,"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases."
P19-1243,Entity-Centric Contextual Affective Analysis,2019,30,0,2,1,11320,anjalie field,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women."
N19-1062,Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings,2019,25,10,4,0,26090,thomas manzini,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Online texts - across genres, registers, domains, and styles - are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting, such as binary gender. Next, we propose a novel methodology for the evaluation of multiclass debiasing. We demonstrate that our multiclass debiasing is robust and maintains the efficacy in standard NLP tasks."
D19-5621,A Margin-based Loss with Synthetic Negative Samples for Continuous-output Machine Translation,2019,0,0,3,0,2574,gayatri bhat,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"Neural models that eliminate the softmax bottleneck by generating word embeddings (rather than multinomial distributions over a vocabulary) attain faster training with fewer learnable parameters. These models are currently trained by maximizing densities of pretrained target embeddings under von Mises-Fisher distributions parameterized by corresponding model-predicted embeddings. This work explores the utility of margin-based loss functions in optimizing such models. We present syn-margin loss, a novel margin-based loss that uses a synthetic negative sample constructed from only the predicted and target embeddings at every step. The loss is efficient to compute, and we use a geometric analysis to argue that it is more consistent and interpretable than other margin-based losses. Empirically, we find that syn-margin provides small but significant improvements over both vMF and standard margin-based losses in continuous-output neural machine translation."
D19-5626,Learning to Generate Word- and Phrase-Embeddings for Efficient Phrase-Based Neural Machine Translation,2019,0,0,2,1,7117,chan park,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"Neural machine translation (NMT) often fails in one-to-many translation, e.g., in the translation of multi-word expressions, compounds, and collocations. To improve the translation of phrases, phrase-based NMT systems have been proposed; these typically combine word-based NMT with external phrase dictionaries or with phrase tables from phrase-based statistical MT systems. These solutions introduce a significant overhead of additional resources and computational costs. In this paper, we introduce a phrase-based NMT model built upon continuous-output NMT, in which the decoder generates embeddings of words or phrases. The model uses a fertility module, which guides the decoder to generate embeddings of sequences of varying lengths. We show that our model learns to translate phrases better, performing on par with state of the art phrase-based NMT. Since our model does not resort to softmax computation over a huge vocabulary of phrases, its training time is about 112x faster than the baseline."
D19-1176,Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts,2019,0,4,4,0,23766,luke breitfeller,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Microaggressions are subtle, often veiled, manifestations of human biases. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of microaggressions in communication has made it difficult for researchers to analyze their exact nature, and to quantify and extract microaggressions automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of microaggressions based on a small subset of data that we have. We then create two datasets: one with examples of diverse types of microaggressions recollected by their targets, and another with gender-based microaggressions in public conversations on social media. We introduce a new, more objective, criterion for annotation and an active-learning based procedure that increases the likelihood of surfacing posts containing microaggressions. Finally, we analyze the trends that emerge from these new datasets."
D19-1425,Topics to Avoid: Demoting Latent Confounds in Text Classification,2019,0,2,4,1,5228,sachin kumar,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Despite impressive performance on many text classification tasks, deep neural networks tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of \textit{native language identification}. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author{'}s native language is Swedish). We propose a method that represents the latent topical confounds and a model which {``}unlearns{''} confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content."
Q18-1024,Native Language Cognate Effects on Second Language Lexical Choice,2018,20,0,2,0.31895,8815,ella rabinovich,Transactions of the Association for Computational Linguistics,0,"We present a computational analysis of cognate effects on the spontaneous linguistic productions of advanced non-native speakers. Introducing a large corpus of highly competent non-native English speakers, and using a set of carefully selected lexical items, we show that the lexical choices of non-natives are affected by cognates in their native language. This effect is so powerful that we are able to reconstruct the phylogenetic language tree of the Indo-European language family solely from the frequencies of specific lexical items in the English of authors with various native languages. We quantitatively analyze non-native lexical choice, highlighting cognate facilitation as one of the important phenomena shaping the language of non-native speakers."
P18-1080,Style Transfer Through Back-Translation,2018,42,40,2,0,4128,shrimai prabhumoye,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency."
N18-6005,Socially Responsible {NLP},2018,0,0,1,1,3965,yulia tsvetkov,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,"As language technologies have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of language technologies and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as methods to adequately mitigate these issues (e.g., adjusting sampling rates or de-biasing through regularization). The tutorial is based on a new course on Ethics and NLP developed at Carnegie Mellon University."
L18-1445,{R}t{G}ender: A Corpus for Studying Differential Responses to Gender,2018,0,11,5,0,26230,rob voigt,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1393,Framing and Agenda-setting in {R}ussian News: a Computational Analysis of Intricate Political Strategies,2018,0,9,6,1,11320,anjalie field,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and {``}fake news{''}. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing."
P17-2009,Incorporating Dialectal Variability for Socially Equitable Language Identification,2017,15,18,2,0.199344,222,david jurgens,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Language identification (LID) is a critical first step for processing multilingual text. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like Twitter, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our model achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using Twitter for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of {``}socially inclusive{''} NLP tools."
W16-2506,Problems With Evaluation of Word Embeddings Using Word Similarity Tasks,2016,38,52,2,0.449235,8204,manaal faruqui,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,"Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of semantic similarity is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods."
W16-2520,Correlation-based Intrinsic Evaluation of Word Vector Representations,2016,21,0,1,1,3965,yulia tsvetkov,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,"We introduce QVEC-CCA--an intrinsic evaluation metric for word vector representations based on correlations of learned vectors with features extracted from linguistic resources. We show that QVEC-CCA scores are an effective proxy for a range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity."
P16-1013,Learning the Curriculum with {B}ayesian Optimization for Task-Specific Word Representation Learning,2016,42,4,1,1,3965,yulia tsvetkov,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order."
N16-1077,Morphological Inflection Generation Using Character Sequence to Sequence Learning,2016,51,22,2,0.449235,8204,manaal faruqui,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation.
N16-1161,Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning,2016,36,25,1,1,3965,yulia tsvetkov,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences---a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually."
P15-2021,Lexicon Stratification for Translating Out-of-Vocabulary Words,2015,30,5,1,1,3965,yulia tsvetkov,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"A language lexicon can be divided into four main strata, depending on origin of words: core vocabulary words, fullyand partiallyassimilated foreign words, and unassimilated foreign words (or transliterations). This paper focuses on translation of fullyand partially-assimilated foreign words, called xe2x80x9cborrowed wordsxe2x80x9d. Borrowed words (or loanwords) are content words found in nearly all languages, occupying up to 70% of the vocabulary. We use models of lexical borrowing in machine translation as a pivoting mechanism to obtain translations of out-of-vocabulary loanwords in a lowresource language. Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines."
P15-1144,Sparse Overcomplete Word Vector Representations,2015,58,84,2,0.449235,8204,manaal faruqui,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they outperform the original vectors on benchmark tasks."
N15-1062,Constraint-Based Models of Lexical Borrowing,2015,44,9,1,1,3965,yulia tsvetkov,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a xe2x80x9cdonorxe2x80x9d language to a xe2x80x9crecipientxe2x80x9d language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, andxe2x80x94in contrast to cognate relationshipsxe2x80x94borrowing relationships may exist across unrelated languages (for example, about 40% of Swahilixe2x80x99s vocabulary is borrowed from Arabic). In this paper, we develop a model of morpho-phonological transformations across languages with features based on universal constraints from Optimality Theory (OT). Compared to several standardxe2x80x94 but linguistically naivexe2x80x94baselines, our OTinspired model obtains good performance with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages."
D15-1161,Not All Contexts Are Created Equal: Better Word Representations with Variable Attention,2015,24,93,2,0,13846,wang ling,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We introduce an extension to the bag-ofwords model for learning words representations that take into account both syntactic and semantic properties within language. This is done by employing an attention model that finds within the contextual words, the words that are relevant for each prediction. The general intuition of our model is that some words are only relevant for predicting local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model."
D15-1243,Evaluation of Word Vector Representations by Subspace Alignment,2015,27,85,1,1,3965,yulia tsvetkov,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks. Most common intrinsic evaluations of vector quality measure correlation with similarity judgments. However, these often correlate poorly with how well the learned representations perform as features in downstream evaluation tasks. We present QVECxe2x80x94a computationally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resourcesxe2x80x94that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1"
W14-3315,The {CMU} Machine Translation Systems at {WMT} 2014,2014,24,7,8,0,26314,austin matthews,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, Germanxe2x80x93English and Hindixe2x80x93English. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create xe2x80x9csynthetic translation optionsxe2x80x9d that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules."
P14-1024,Metaphor Detection with Cross-Lingual Model Transfer,2014,42,67,1,1,3965,yulia tsvetkov,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets in Spanish, Farsi, and Russian. The results support the hypothesis that metaphors are conceptual, rather than lexical, in nature."
tsvetkov-etal-2014-augmenting-english,Augmenting {E}nglish Adjective Senses with Supersenses,2014,19,22,1,1,3965,yulia tsvetkov,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We develop a supersense taxonomy for adjectives, based on that of GermaNet, and apply it to English adjectives in WordNet using human annotation and supervised classification. Results show that accuracy for automatic adjective type classification is high, but synsets are considerably more difficult to classify, even for trained human annotators. We release the manually annotated data, the classifier, and the induced supersense labeling of 12,304 WordNet adjective synsets."
bhatia-etal-2014-unified,A Unified Annotation Scheme for the Semantic/Pragmatic Components of Definiteness,2014,19,4,4,0,11651,archna bhatia,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a definiteness annotation scheme that captures the semantic, pragmatic, and discourse information, which we call communicative functions, associated with linguistic descriptions such as {``}a story about my speech{''}, {``}the story{''}, {``}every time I give it{''}, {``}this slideshow{''}. A survey of the literature suggests that definiteness does not express a single communicative function but is a grammaticalization of many such functions, for example, identifiability, familiarity, uniqueness, specificity. Our annotation scheme unifies ideas from previous research on definiteness while attempting to remove redundancy and make it easily annotatable. This annotation scheme encodes the communicative functions of definiteness rather than the grammatical forms of definiteness. We assume that the communicative functions are largely maintained across languages while the grammaticalization of this information may vary. One of the final goals is to use our semantically annotated corpora to discover how definiteness is grammaticalized in different languages. We release our annotated corpora for English and Hindi, and sample annotations for Hebrew and Russian, together with an annotation manual."
J14-2007,Identification of Multiword Expressions by Combining Multiple Linguistic Information Sources,2014,-1,-1,1,1,3965,yulia tsvetkov,Computational Linguistics,0,None
E14-1065,Augmenting Translation Models with Simulated Acoustic Confusions for Improved Spoken Language Translation,2014,41,12,1,1,3965,yulia tsvetkov,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using only a source language pronunciation dictionary and language model (i.e., without an acoustic model), and use these to augment the phrase table of a standard MT system. The augmented system can thus recover from recognition errors during decoding using synthesized phrases. Using the outputs of five different English ASR systems as input, we find consistent and significant improvements in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements."
C14-1100,Automatic Classification of Communicative Functions of Definiteness,2014,45,2,4,0,11651,archna bhatia,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Definiteness expresses a constellation of semantic, pragmatic, and discourse propertiesxe2x80x94the communicative functionsxe2x80x94of an NP. We present a supervised classifier for English NPs that uses lexical, morphological, and syntactic features to predict an NPxe2x80x99s communicative function in terms of a language-universal classification scheme. Our classifiers establish strong baselines for future work in this neglected area of computational semantic analysis. In addition, analysis of the features and learned parameters in the model provides insight into the grammaticalization of definiteness in English, not all of which is obvious a priori."
W13-2234,Generating {E}nglish Determiners in Phrase-Based Translation with Synthetic Translation Options,2013,34,14,1,1,3965,yulia tsvetkov,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation optionsxe2x80x94phrasal translations that are generated by auxiliary translation and postediting processesxe2x80x94to augment the default phrase inventory learned from parallel data. We apply our technique to the problem of producing English determiners when translating from Russian and Czech, languages that lack definiteness morphemes. Our approach augments the English side of the phrase table using a classifier to predict where English articles might plausibly be added or removed, and then we decode as usual. Doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier."
W13-1736,Identifying the {L}1 of non-native writers: the {CMU}-Haifa system,2013,29,13,1,1,3965,yulia tsvetkov,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We show that it is possible to learn to identify, with high accuracy, the native language of English test takers from the content of the essays they write. Our method uses standard text classification techniques based on multiclass logistic regression, combining individually weak indicators to predict the most probable native language from a set of 11 possibilities. We describe the various features used for classification, as well as the settings of the classifier that yielded the highest accuracy."
W13-0906,Cross-Lingual Metaphor Detection Using Common Semantic Features,2013,13,34,1,1,3965,yulia tsvetkov,Proceedings of the First Workshop on Metaphor in {NLP},0,"We present the CSF - Common Semantic Features method for metaphor detection. This method has two distinguishing characteristics: it is cross-lingual and it does not rely on the availability of extensive manually-compiled lexical resources in target languages other than English. A metaphor detecting classifier is trained on English samples and then applied to the target language. The method includes procedures for obtaining semantic features from sentences in the target language. Our experiments with Russian and English sentences show comparable results, supporting our hypothesis that a CSF-based classifier can be applied across languages. We obtain state-ofthe-art performance in both languages."
D11-1077,Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources,2011,56,25,1,1,3965,yulia tsvetkov,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. The architecture combines various linguistically-motivated classification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost entirely unsupervised and completely language-independent; it relies on few language resources and is thus suitable for a large number of languages. Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic constructions. We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines."
tsvetkov-wintner-2010-automatic,Automatic Acquisition of Parallel Corpora from Websites with Dynamic Content,2010,12,18,1,1,3965,yulia tsvetkov,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Parallel corpora are indispensable resources for a variety of multilingual natural language processing tasks. This paper presents a technique for fully automatic construction of constantly growing parallel corpora. We propose a simple and effective dictionary-based algorithm to extract parallel document pairs from a large collection of articles retrieved from the Internet, potentially containing manually translated texts. This algorithm was implemented and tested on Hebrew-English parallel texts. With properly selected thresholds, precision of 100{\%} can be obtained."
C10-2144,Extraction of Multi-word Expressions from Small Parallel Corpora,2010,49,25,1,1,3965,yulia tsvetkov,Coling 2010: Posters,0,"We present a general methodology for extracting multi-word expressions (of various types), along with their translations, from small parallel corpora. We automatically align the parallel corpus and focus on misalignments; these typically indicate expressions in the source language that are translated to the target in a non-compositional way. We then use a large monolingual corpus to rank and filter the results. Evaluation of the quality of the extraction algorithm reveals significant improvements over naive alignment-based methods. External evaluation shows an improvement in the performance of machine translation that uses the extracted dictionary."
