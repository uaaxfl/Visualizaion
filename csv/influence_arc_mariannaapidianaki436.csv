2006.jeptalnrecital-long.2,P91-1017,0,0.0387696,"Missing"
2006.jeptalnrecital-long.2,W04-2213,0,0.0591445,"Missing"
2006.jeptalnrecital-long.2,W03-0313,0,0.0595477,"Missing"
2006.jeptalnrecital-long.2,2000.bcs-1.3,0,0.072038,"Missing"
2006.jeptalnrecital-long.2,2003.jeptalnrecital-long.25,0,0.463254,"Missing"
2007.jeptalnrecital-long.19,2006.jeptalnrecital-long.2,1,0.719339,"Missing"
2007.jeptalnrecital-long.19,P91-1034,0,0.198336,"Missing"
2007.jeptalnrecital-long.19,N03-1015,0,0.0337067,"Missing"
2007.jeptalnrecital-long.19,J98-1004,0,0.0977195,"Missing"
2007.jeptalnrecital-long.19,W03-0304,0,0.0629874,"Missing"
2007.jeptalnrecital-long.19,2006.eamt-1.28,0,0.0420684,"Missing"
2007.jeptalnrecital-long.19,2003.jeptalnrecital-long.25,0,0.0753682,"Missing"
2007.jeptalnrecital-long.19,H05-1097,0,0.0714033,"Missing"
2009.jeptalnrecital-position.2,C88-1016,0,0.265923,"Missing"
2009.jeptalnrecital-position.2,P91-1034,0,0.0679462,"Missing"
2009.jeptalnrecital-position.2,E06-1032,0,0.0714272,"Missing"
2009.jeptalnrecital-position.2,2006.iwslt-evaluation.5,0,0.0352554,"Missing"
2009.jeptalnrecital-position.2,P07-1005,0,0.0419765,"Missing"
2009.jeptalnrecital-position.2,P05-1033,0,0.0705124,"Missing"
2009.jeptalnrecital-position.2,W04-0802,0,0.0413124,"Missing"
2009.jeptalnrecital-position.2,koen-2004-pharaoh,0,0.0834462,"Missing"
2009.jeptalnrecital-position.2,N03-1017,0,0.0093973,"Missing"
2009.jeptalnrecital-position.2,W07-0734,0,0.0487786,"Missing"
2009.jeptalnrecital-position.2,J04-4002,0,0.0420475,"Missing"
2009.jeptalnrecital-position.2,P02-1040,0,0.0774323,"Missing"
2009.jeptalnrecital-position.2,W97-0213,0,0.0623088,"Missing"
2009.jeptalnrecital-position.2,2007.tmi-papers.28,0,0.0352868,"Missing"
2010.iwslt-papers.2,W97-0322,0,\N,Missing
2010.iwslt-papers.2,apidianaki-2008-translation,1,\N,Missing
2010.iwslt-papers.2,S07-1004,0,\N,Missing
2010.iwslt-papers.2,E09-1010,1,\N,Missing
2010.iwslt-papers.2,Y09-1007,1,\N,Missing
2010.iwslt-papers.2,W09-0441,0,\N,Missing
2010.iwslt-papers.2,W07-0714,0,\N,Missing
2010.iwslt-papers.2,W02-0808,0,\N,Missing
2010.iwslt-papers.2,P02-1040,0,\N,Missing
2010.iwslt-papers.2,D08-1021,0,\N,Missing
2010.iwslt-papers.2,W06-1610,0,\N,Missing
2010.iwslt-papers.2,W05-0909,0,\N,Missing
2010.iwslt-papers.2,P09-1034,0,\N,Missing
2010.iwslt-papers.2,J98-1004,0,\N,Missing
2010.iwslt-papers.2,W07-0734,0,\N,Missing
2010.iwslt-papers.2,D07-1007,0,\N,Missing
2010.iwslt-papers.2,J03-1002,0,\N,Missing
2010.iwslt-papers.2,W08-0309,0,\N,Missing
2010.iwslt-papers.2,2005.mtsummit-papers.11,0,\N,Missing
2016.jeptalnrecital-demo.12,L16-1099,1,0.848406,"Missing"
2018.jeptalnrecital-court.34,W12-2205,0,0.061664,"Missing"
2018.jeptalnrecital-court.34,W12-2202,0,0.0480299,"Missing"
2018.jeptalnrecital-court.34,francois-etal-2014-flelex,0,0.0715058,"Missing"
2018.jeptalnrecital-court.34,F14-1009,0,0.0722236,"Missing"
2018.jeptalnrecital-court.34,S12-1066,0,0.0540236,"Missing"
2018.jeptalnrecital-court.34,D09-1094,0,0.0868457,"Missing"
2018.jeptalnrecital-court.34,P13-3015,0,0.0322297,"Missing"
2018.jeptalnrecital-court.34,S12-1046,0,0.0613916,"Missing"
2020.blackboxnlp-1.13,marelli-etal-2014-sick,0,0.0607422,"16; Hewitt and Manning, 2019; Rogers et al., 2020; Tenney et al., 2019) as well as variation in their context of use. We propose to explore the impact of context variation on word representations. We specifically address representations generated by the BERT model (Devlin et al., 2019), trained using a language modeling objective, and translation models involving one or more language pairs (Artetxe and Schwenk, 2019; V´azquez et al., 2020). We run a series of controlled experiments using sentences illustrating both meaning preserving and meaning altering transformations from the SICK dataset (Marelli et al., 2014b), and examples automatically generated using a template-based method (Prasad et al., 2019). We explore the impact of specific alternations on the representations, namely passivization and negation. Examples in our datasets consist of sentences that only differ in terms of the specific alternation addressed. In order to detect the imprint of these transformations on the representations, we employ methodology inspired by work on linguistic bias detection in embedding representations (Bolukbasi et al., 2016; Lauscher et al., 2019; Ravfogel et al., 2020). Furthermore, we investigate the impact o"
2020.blackboxnlp-1.13,2020.cl-2.5,1,0.86502,"Missing"
2020.blackboxnlp-1.13,D14-1162,0,0.0953117,"Missing"
2020.blackboxnlp-1.13,N18-1202,0,0.0526859,"er agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019), or semantic phenomena such as semantic role labeling and coreference (Tenney et al., 2019; Kovaleva et al., 2019). In our work, we shift the focus from interpreting the knowledge about language encoded in the representations, to exploring the imprint of two specific transformations, passivization and negation, on word representations. The majority of the above mentioned works address representations generated by models trained with a language modeling objective, such as LSTM RNNs (Linzen et al., 2016), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Voita et al. (2019a) propose to study the representations obtained from models trained with a different objective. We take the same stance and investigate the impact of context on representations generated by BERT, and by the encoder of neural machine translation (NMT) models involving one or more language pairs. In order to detect the information related to the two studied transformations that is encoded in the representations, we employ methodology initially proposed for identifying and removing linguistic and other kinds of biases from representations. Such"
2020.blackboxnlp-1.13,K19-1007,0,0.0615277,"Missing"
2020.blackboxnlp-1.13,2020.acl-demos.14,0,0.0902401,"ing and illustrates lexical, syntactic and semantic phenomena that compositional distributional semantic models are expected to account for. PAS is one of the meaning preserving alternations in SICK, where a sentence S2 results from the passivization of an active sentence S1. We use all the 276 sentence 4 Our code and data are available at https://github. com/Helsinki-NLP/Syntactic_Debiasing 138 5 The code is available at https://github.com/ grushaprasad/RNN-Priming. 6 The complexity of the sentences also resulted in numerous syntactic analysis errors when we tried to parse them using Stanza (Qi et al., 2020). 7 The dataset was used in SemEval 2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment (Marelli et al., 2014a). zi cannot be predicted from g(xi ). The method is based on iteratively (1) training a linear classifier to predict zi from xi , followed by (2) projecting xi on the null-space of the classifier, using a projection matrix PN (W ) such that W (PN (W ) x) = 0 ∀x, where W is the weight matrix of the classifier, and N (W ) is its null-space. Through the projection step in each iteration, the informa"
2020.blackboxnlp-1.13,D18-1521,0,0.0187822,"lation (NMT) models involving one or more language pairs. In order to detect the information related to the two studied transformations that is encoded in the representations, we employ methodology initially proposed for identifying and removing linguistic and other kinds of biases from representations. Such methods fall in two main paradigms: projection and adversarial methods. Projection methods identify specific directions in word embedding space that correspond to the protected attribute, and remove them. Bolukbasi et al. (2016) identify a gender subspace by exploring gendered word lists. Zhao et al. (2018) propose to train debiased word embeddings from scratch by altering the loss of the GloVe model (Pennington et al., 2014) to concentrate specific information (e.g., about gender) in a dedicated coordinate of each vector. Dev and Phillips (2019) propose a simple linear projection method to reduce the bias in word embed137 dings. Lauscher et al. (2019) develop a variation of this method that introduces more flexibility in the formation of the debiasing vector used in the projection. Adversial methods extend the main task objective with a component that competes with the encoder trying to extract"
2020.blackboxnlp-1.13,D19-1448,0,0.0373628,"Missing"
2020.blackboxnlp-1.13,P19-1580,0,0.0294334,"heme in direct object position. In , 2 the semantic relationship of the mafia and the millionaire to the kidnapping event is the same but their syntactic roles have changed. 3 These two transformations were preferred on the basis that they do not change the words in the sentence, as opposed to other possible translations, which involve reformulations, eg. “a sewing machine” vs. “a machine made for sewing”. Related Work The analysis and interpretation of the linguistic knowledge present in contextualized representations has recently been the focus of a large amount of work (Clark et al., 2019; Voita et al., 2019b; Tenney et al., 2019; Talmor et al., 2019). The bulk of this interpretation work relies on probing tasks which serve to predict linguistic properties from the representations generated by the models (Linzen, 2018; Rogers et al., 2020). These might involve structural aspects of language, such as syntax, word order, or number agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019), or semantic phenomena such as semantic role labeling and coreference (Tenney et al., 2019; Kovaleva et al., 2019). In our work, we shift the focus from interpreting the knowledge about lang"
2020.emnlp-main.598,D19-1445,0,0.0225498,"falls in the neural network interpretation paradigm which explores the knowledge about language encoded in the representations of deep learning models (Voita et al., 2019a; Clark et al., 2019; Voita et al., 2019b; Tenney et al., 2019; Talmor et al., 2019). The bulk of this interpretation work addresses structural aspects of language such as syntax, word order, or number agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019; Rogers et al., 2020); shallow semantic phenomena closely related to syntax such as semantic role labelling and coreference (Tenney et al., 2019; Kovaleva et al., 2019); or the symbolic reasoning potential of language model representations (Talmor et al., 2019). Our 7371 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7371–7385, c November 16–20, 2020. 2020 Association for Computational Linguistics work makes a contribution towards the study of the knowledge pre-trained LMs encode about word meaning, generally overlooked until now in interpretation work. We evaluate the representations generated by BERT against gold standard adjective intensity estimates (de Melo and Bansal, 2013; Wilkinson, 2017; Cocos et al., 2"
2020.emnlp-main.598,E14-1057,0,0.0573263,"Missing"
2020.emnlp-main.598,Q16-1037,0,0.0198321,"for detecting the intensity relationship of two adjectives on the fly. We view intensity as a direction in the semantic space which, once identified, can serve to determine the intensity of new adjectives. Our work falls in the neural network interpretation paradigm which explores the knowledge about language encoded in the representations of deep learning models (Voita et al., 2019a; Clark et al., 2019; Voita et al., 2019b; Tenney et al., 2019; Talmor et al., 2019). The bulk of this interpretation work addresses structural aspects of language such as syntax, word order, or number agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019; Rogers et al., 2020); shallow semantic phenomena closely related to syntax such as semantic role labelling and coreference (Tenney et al., 2019; Kovaleva et al., 2019); or the symbolic reasoning potential of language model representations (Talmor et al., 2019). Our 7371 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7371–7385, c November 16–20, 2020. 2020 Association for Computational Linguistics work makes a contribution towards the study of the knowledge pre-trained LMs encode about word meanin"
2020.emnlp-main.598,D15-1300,0,0.0271103,"man et al., 2013; Shivade et al., 2015). For example, the patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjective less intense than Y. Another common approach is lexicon-based and draws upon a resource that maps adjectives to scores encoding sentiment polarity (positive or negative) and intensity. Such resources can be manually created, like the SOCAL lexicon (Taboada et al., 2011), or automatically compiled by mining adjective orderings from star-valued product reviews where people’s comments have associated ratings (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014). Cocos et al. (2018) combine knowledge from lexico-syntactic patterns and the SO-CAL lexicon with paraphrases in the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015). Our approach is novel in that it does not need specified patterns or access to lexicographic resources. It, instead, relies on the knowledge about intensity encoded in scalar adjectives’ contextualised representations. Our best performing method is inspired by work on gender bias which relies on simple vector arithmetic to uncover gender-related stereotypes. A gender directi"
2020.emnlp-main.598,N15-1051,0,0.0631974,"Missing"
2020.emnlp-main.598,J11-2001,0,0.0359153,"nsity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources. 1 Figure 1: Full scale of adjectives describing positive and negative sentiment at different degrees from the SO-CAL dataset (Taboada et al., 2011). Introduction Scalar adjectives describe a property of a noun at different degrees of intensity. Identifying the scalar relationship that exists between their meaning (for example, the increasing intensity between pretty, beautiful and gorgeous) is useful for text understanding, for both humans and automatic systems. It can serve to define the sentiment and subjectivity of a text, perform inference and textual entailment (Van Tiel et al., 2016; McNally, 2016), build question answering and recommendation systems (de Marneffe et al., 2010), and assist language learners in distinguishing between"
2020.emnlp-main.598,P19-1452,0,0.156202,"ed representations produced by BERT to be a good fit for this task. We also propose a method inspired by gender bias work (Bolukbasi et al., 2016; Dev and Phillips, 2019) for detecting the intensity relationship of two adjectives on the fly. We view intensity as a direction in the semantic space which, once identified, can serve to determine the intensity of new adjectives. Our work falls in the neural network interpretation paradigm which explores the knowledge about language encoded in the representations of deep learning models (Voita et al., 2019a; Clark et al., 2019; Voita et al., 2019b; Tenney et al., 2019; Talmor et al., 2019). The bulk of this interpretation work addresses structural aspects of language such as syntax, word order, or number agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019; Rogers et al., 2020); shallow semantic phenomena closely related to syntax such as semantic role labelling and coreference (Tenney et al., 2019; Kovaleva et al., 2019); or the symbolic reasoning potential of language model representations (Talmor et al., 2019). Our 7371 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7371–7385, c"
2020.emnlp-main.598,L16-1424,0,0.0213486,"annotated for intensity relations (&lt;, &gt;, and =). C ROWD (Cocos et al., 2018).3 The dataset consists of a set of adjective scales with high coverage of the PPDB vocabulary. It was constructed by a threestep process: Crowd workers were first asked to determine whether pairs of adjectives describe the same attribute (e.g., TEMPERATURE) and should, therefore, belong to the same scale. Sets of samescale adjectives were then refined over multiple rounds. Finally, workers ranked the adjectives in each set by intensity. The final dataset includes 330 adjective pairs along 79 half-scales. W ILKINSON (Wilkinson and Oates, 2016).4 This dataset was generated through crowdsourcing. Crowd workers were presented with small seed sets (e.g., huge, small, microscopic) and were asked to propose similar adjectives, resulting in twelve adjective sets. Sets were automatically cleaned for consistency, and then annotated for intensity by the crowd workers. The original dataset contains full scales. We use its division in 21 half-scales (with 61 adjective pairs) proposed by Cocos et al. (2018). In the rest of the paper, we use the term “scale” to refer to the half-scales contained in these datasets. Table 1 shows examples from eac"
2020.emnlp-main.598,Q14-1006,0,0.0430418,"rrible to bad, and from good to awesome) based on pattern-based evidence in the Google N-Grams corDE M ELO 1 Our code and data are available at https://github. com/ainagari/scalar_adjs 7372 2 http://demelo.org/gdm/intensity/ Dataset Adjective scale DE M ELO [soft → quiet → inaudible → silent] [thick → dense → impenetrable] C ROWD [fine → remarkable → spectacular] [scary ||frightening → terrifying] W ILKINSON [damp → moist → wet] [dumb → stupid → idiotic] Table 1: Examples of scales in each dataset. ‘||’ denotes a tie between adjectives of the same intensity. 2009)5 and the Flickr 30K dataset (Young et al., 2014).6 For every s ∈ D, a dataset from Section 3, and for each a ∈ s, we collect 1,000 instances (sentences) from each corpus.7 We substitute each instance i of a ∈ s, with each b ∈ s where b 6= a, creating |s |− 1 new sentences.8 For example, for an instance of thick from the scale [thick → dense → impenetrable] in Table 1, we generate two new sentences where thick is substituted by each of the other adjectives in the same context. 4.2 pus (Brants and Franz, 2006). The dataset contains 87 half-scales with 548 adjective pairs, manually annotated for intensity relations (&lt;, &gt;, and =). C ROWD (Cocos"
2020.emnlp-main.598,D18-1521,0,0.02689,"2015). Our approach is novel in that it does not need specified patterns or access to lexicographic resources. It, instead, relies on the knowledge about intensity encoded in scalar adjectives’ contextualised representations. Our best performing method is inspired by work on gender bias which relies on simple vector arithmetic to uncover gender-related stereotypes. A gender direction is determined (for example, by comparing the embeddings of she and he, or woman and man) and the projection of the vector of a potentially biased word on this direction is then calculated (Bolukbasi et al., 2016; Zhao et al., 2018). We extend this method to scalar adjectives and BERT representations. Kim and de Marneffe (2013) also consider vector distance in the semantic space to encode scalar relationships between adjectives. They specifically examine a small set of word pairs, and observe that the middle point in space between the word2vec (Mikolov et al., 2013) embeddings of two antonyms (e.g., furious and happy) falls close to the embedding of a mid-ranked word in their scale (e.g., unhappy). Their experiments rely on antonym pairs extracted from WordNet. We show that contextualised representations are a better fit"
2020.emnlp-main.598,D19-1448,0,0.039636,"Missing"
2020.emnlp-main.598,P19-1580,0,0.0229941,"ay vary from context to context), we consider the contextualised representations produced by BERT to be a good fit for this task. We also propose a method inspired by gender bias work (Bolukbasi et al., 2016; Dev and Phillips, 2019) for detecting the intensity relationship of two adjectives on the fly. We view intensity as a direction in the semantic space which, once identified, can serve to determine the intensity of new adjectives. Our work falls in the neural network interpretation paradigm which explores the knowledge about language encoded in the representations of deep learning models (Voita et al., 2019a; Clark et al., 2019; Voita et al., 2019b; Tenney et al., 2019; Talmor et al., 2019). The bulk of this interpretation work addresses structural aspects of language such as syntax, word order, or number agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019; Rogers et al., 2020); shallow semantic phenomena closely related to syntax such as semantic role labelling and coreference (Tenney et al., 2019; Kovaleva et al., 2019); or the symbolic reasoning potential of language model representations (Talmor et al., 2019). Our 7371 Proceedings of the 2020 Conference on Empiri"
2020.semeval-1.18,D19-1542,0,0.018566,"at https://github.com/ainagari/semeval2020-task3-multisem 158 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 158–165 Barcelona, Spain (Online), December 12, 2020. Lauscher et al. (2019) opt for the first, adding an additional lexical task to BERT’s two training objectives (language modelling and next sentence prediction) (Devlin et al., 2019). The semantic knowledge used in this additional task comes from pre-defined lexicographic resources (like WordNet (Miller, 1995)), and is shown to be beneficial on almost all tasks in the GLUE benchmark (Wang et al., 2018). Arase and Tsujii (2019) inject semantic knowledge into BERT by fine-tuning the pre-trained model on paraphrase data. They subsequently fine-tune the model again for the related tasks of paraphrase identification and semantic equivalence assessment, and report results that demonstrate improved performance over a model that has not been exposed to paraphrase data. We follow their approach and fine-tune BERT models for English and Finnish on a set of semantic tasks that are closely related to the GWSC task, since no training data is available for GWSC. One of our tasks is inspired by the retrofitting approach of Shi et"
2020.semeval-1.18,2020.lrec-1.720,0,0.396385,"Missing"
2020.semeval-1.18,L18-1218,0,0.360306,"al., 2004) that share a word and which are paraphrases of each other (T) or not (F). Shi et al. propose an orthogonal transformation for ELMo (Peters et al., 2018) that is trained to bring representations of word instances closer when they appear in meaning-equivalent contexts. They show that this retrofitting approach improves ELMo’s performance in a wide range of semantic tasks at the sentence level (sentiment analysis, inference and sentence relatedness). We follow their data collection method to obtain word instances for fine-tuning BERT. We replace the MRPC with the Opusparcus resource (Creutz, 2018) since it covers two of the languages addressed in GWSC, English and Finnish. 3 System Overview 3.1 Datasets We fine-tune pre-trained BERT models on semantic tasks that are related to GWSC. We specifically select tasks that address the similarity of word meaning in context, and use the corresponding datasets to make BERT more sensitive to this specific aspect of meaning. Table 1 contains annotated instances from each dataset used in our experiments. Usim The Usim dataset contains 10 sentences for each of 56 words of different parts of speech, manually annotated with pairwise usage similarity s"
2020.semeval-1.18,N19-1423,0,0.0397446,"at two stages: during model pre-training or during fine-tuning. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 Our code will be made available at https://github.com/ainagari/semeval2020-task3-multisem 158 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 158–165 Barcelona, Spain (Online), December 12, 2020. Lauscher et al. (2019) opt for the first, adding an additional lexical task to BERT’s two training objectives (language modelling and next sentence prediction) (Devlin et al., 2019). The semantic knowledge used in this additional task comes from pre-defined lexicographic resources (like WordNet (Miller, 1995)), and is shown to be beneficial on almost all tasks in the GLUE benchmark (Wang et al., 2018). Arase and Tsujii (2019) inject semantic knowledge into BERT by fine-tuning the pre-trained model on paraphrase data. They subsequently fine-tune the model again for the related tasks of paraphrase identification and semantic equivalence assessment, and report results that demonstrate improved performance over a model that has not been exposed to paraphrase data. We follow"
2020.semeval-1.18,C04-1051,0,0.292369,"ly fine-tune the model again for the related tasks of paraphrase identification and semantic equivalence assessment, and report results that demonstrate improved performance over a model that has not been exposed to paraphrase data. We follow their approach and fine-tune BERT models for English and Finnish on a set of semantic tasks that are closely related to the GWSC task, since no training data is available for GWSC. One of our tasks is inspired by the retrofitting approach of Shi et al. (2019). This consists in gathering sentence pairs from the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) that share a word and which are paraphrases of each other (T) or not (F). Shi et al. propose an orthogonal transformation for ELMo (Peters et al., 2018) that is trained to bring representations of word instances closer when they appear in meaning-equivalent contexts. They show that this retrofitting approach improves ELMo’s performance in a wide range of semantic tasks at the sentence level (sentiment analysis, inference and sentence relatedness). We follow their data collection method to obtain word instances for fine-tuning BERT. We replace the MRPC with the Opusparcus resource (Creutz, 201"
2020.semeval-1.18,P09-1002,0,0.090909,"Missing"
2020.semeval-1.18,J13-3003,0,0.633871,"Missing"
2020.semeval-1.18,N13-1092,0,0.194555,"Missing"
2020.semeval-1.18,S19-1002,1,0.821803,"Missing"
2020.semeval-1.18,E14-1057,0,0.0471603,"Missing"
2020.semeval-1.18,S07-1009,0,0.133534,"Missing"
2020.semeval-1.18,K16-1006,0,0.0255851,"sentences from the ukWaC corpus (Baroni et al., 2009) and automatically annotating them with lexical substitutes. We identify the content words in a sentence and use as their candidate substitutes their paraphrases in the Paraphrase Database (PPDB) lexical XXL package (Ganitkevitch et al., 2013; Pavlick et al., 2015).4 The PPDB resource was automatically constructed by a bilingual pivoting method. Every paraphrase pair has a PPDB 2.0 score indicating its quality. We only consider as candidates for substitution pairs with a score above 2. We then use the context2vec lexical substitution model (Melamud et al., 2016) to rank the candidates according to how 4 http://paraphrase.org/ 160 well they fit in a context. context2vec is a biLSTM model that jointly learns static representations of words and dynamic context representations. We rank candidate substitutes using the following formula: c2v score = cos(s, t) + 1 cos(s, C) + 1 × 2 2 (1) where s is the static representation of the candidate substitute, C is the context embedding of the sentence and t is the static embedding of a word instance i we want to replace. Using this formula, we obtain an ordered ranking R of substitutes for an instance i in context"
2020.semeval-1.18,P15-2070,0,0.0486128,"Missing"
2020.semeval-1.18,N18-1202,0,0.0302832,"improved performance over a model that has not been exposed to paraphrase data. We follow their approach and fine-tune BERT models for English and Finnish on a set of semantic tasks that are closely related to the GWSC task, since no training data is available for GWSC. One of our tasks is inspired by the retrofitting approach of Shi et al. (2019). This consists in gathering sentence pairs from the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) that share a word and which are paraphrases of each other (T) or not (F). Shi et al. propose an orthogonal transformation for ELMo (Peters et al., 2018) that is trained to bring representations of word instances closer when they appear in meaning-equivalent contexts. They show that this retrofitting approach improves ELMo’s performance in a wide range of semantic tasks at the sentence level (sentiment analysis, inference and sentence relatedness). We follow their data collection method to obtain word instances for fine-tuning BERT. We replace the MRPC with the Opusparcus resource (Creutz, 2018) since it covers two of the languages addressed in GWSC, English and Finnish. 3 System Overview 3.1 Datasets We fine-tune pre-trained BERT models on se"
2020.semeval-1.18,D19-1113,0,0.100003,"(2019) inject semantic knowledge into BERT by fine-tuning the pre-trained model on paraphrase data. They subsequently fine-tune the model again for the related tasks of paraphrase identification and semantic equivalence assessment, and report results that demonstrate improved performance over a model that has not been exposed to paraphrase data. We follow their approach and fine-tune BERT models for English and Finnish on a set of semantic tasks that are closely related to the GWSC task, since no training data is available for GWSC. One of our tasks is inspired by the retrofitting approach of Shi et al. (2019). This consists in gathering sentence pairs from the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) that share a word and which are paraphrases of each other (T) or not (F). Shi et al. propose an orthogonal transformation for ELMo (Peters et al., 2018) that is trained to bring representations of word instances closer when they appear in meaning-equivalent contexts. They show that this retrofitting approach improves ELMo’s performance in a wide range of semantic tasks at the sentence level (sentiment analysis, inference and sentence relatedness). We follow their data collectio"
2020.semeval-1.18,W18-5446,0,0.0252654,"ll be made available at https://github.com/ainagari/semeval2020-task3-multisem 158 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 158–165 Barcelona, Spain (Online), December 12, 2020. Lauscher et al. (2019) opt for the first, adding an additional lexical task to BERT’s two training objectives (language modelling and next sentence prediction) (Devlin et al., 2019). The semantic knowledge used in this additional task comes from pre-defined lexicographic resources (like WordNet (Miller, 1995)), and is shown to be beneficial on almost all tasks in the GLUE benchmark (Wang et al., 2018). Arase and Tsujii (2019) inject semantic knowledge into BERT by fine-tuning the pre-trained model on paraphrase data. They subsequently fine-tune the model again for the related tasks of paraphrase identification and semantic equivalence assessment, and report results that demonstrate improved performance over a model that has not been exposed to paraphrase data. We follow their approach and fine-tune BERT models for English and Finnish on a set of semantic tasks that are closely related to the GWSC task, since no training data is available for GWSC. One of our tasks is inspired by the retrof"
2021.blackboxnlp-1.7,2020.lrec-1.716,0,0.0421269,"ing physical (perceptual), functional and other properties. Among the collected 7,258 concept-feature pairs, we find that a dolphin is intelligent, friendly, and lives in oceans; and that a chandelier is hanging from ceilings and is made of crystal. The number of annotators who proposed each feature is also provided. The dataset has been extensively used to investigate and improve the knowledge about object properties encoded by distributional models (Rubinstein et al., 2015), word embeddings (Lucy and Gauthier, 2017; Yang et al., 2018) and, more recently, contextual LMs (Forbes et al., 2019; Hasegawa et al., 2020). These studies do not focus on adjectival attributes but rather consider all proposed properties, or specific subsets such as visual properties. In our experiments, we explore noun properties through the “IS_ADJ” features of noun concepts present in MRD. 4 Cloze Task Experiments We use the bert-base-uncased and bert-large-uncased models pre-trained on the BookCorpus (Zhu et al., 2015) and on English Wikipedia (Devlin et al., 2019). The models are trained using a cloze task where tokens of the input sequence are masked and the models learn to fill the slots, and a binary classification objecti"
2021.blackboxnlp-1.7,D15-1075,0,0.0465984,"rties, in order to see how BERT can predict these properties when it has access to images alongside text. Another goal is to collect evaluation data using cloze task queries in specifically designed crowdsourcing tasks. Table 6: Results on the Addone test set. Best results for each metric are highlighted in boldface. ENTAILMENT ) and the majority class proposed for each adjective in the training set, respectively. We also report the human performance on this task as an upper bound, and compare to the bestperforming model in Pavlick and Callison-Burch (2016) which relies on a RNN architecture (Bowman et al., 2015). BERT-CLS fails to learn the information needed for the task and predicts the ENTAILMENT label for all instances. This explains the low scores obtained with this model, since the majority label in this dataset is NON - ENTAILMENT. The default fine-tuning strategy used for textual entailment with BERT is, thus, not suitable for addressing cases of compositional entailment in the Addone dataset. It is much more effective to use the representations of the specific words that de−−−→ termine sentence entailment: BERT-TOK (N sN , −−−→ AsAN ) obtains higher results than the previous best model (RNN)"
2021.blackboxnlp-1.7,2020.tacl-1.28,0,0.0905404,"using algebraic operations. We also investigate the extent to which the representations of A and N in an AN phrase capture its meaning, since token-level BERT embeddings encode information from the surrounding context. 2 Relations between entities stored in Wikidata, common sense relations between concepts from ConceptNet (Speer and Havasi, 2012), and knowledge aimed for answering natural language questions in SQuAD (Rajpurkar et al., 2016). 80 cally extracted from Open Mind Common Sense (OMCS)3 sentences and are often too long, including irrelevant information that might confuse the model.4 Jiang et al. (2020) demonstrate the impact of prompt quality on LM probing but focus on relations involving encyclopedic knowledge (e.g., born/died in, profession, subclass). Bouraoui et al. (2020) also explore the knowledge BERT has about lexical, morphological and commonsense relations (e.g., hypernymy, meronymy, plural, cause-effect) through fine-tuning, but neither they address noun properties. 3 C instances having feature f (e.g., ALL guitars are musical instruments, but SOME guitars are electric). Quantification is important for semantic inference; it can serve to understand set relations (such as synonymy"
2021.blackboxnlp-1.7,2021.eacl-main.89,0,0.0270892,"tune BERT for entailment and test it in a task that involves AN constructions (Pavlick and Callison-Burch, 2016). Our cloze task results show that BERT has only marginal knowledge of noun properties and their prevalence, but can still successfully detect cases where the addition of an adjective does not alter the meaning of a sentence and where entailment is preserved. 2 We furthermore address the entailment relationship between N and ANs (N |= AN). In the opposite direction (AN |= N) entailment generally holds, i.e. almost all ANs entail their head noun (red car |= car) (Baroni et al., 2012; Kober et al., 2021). Determining whether N |= AN holds, however, depends on the semantic properties described by the adjective. We base this analysis on the AddOne dataset proposed by Pavlick and Callison-Burch (2016). AddOne consists of sentence pairs that contain AN phrases annotated for entailment through crowdsourcing. This simplified entailment task differs from the classical RTE task (Dagan et al., 2005) in that the premise and hypothesis differ by only one atomic edit (insertion of A). We use this task as a proxy for prototypicality based on the assumption that adjectives describing typical properties of"
2021.blackboxnlp-1.7,2020.acl-demos.14,0,0.0195568,"that the frequency-based filtering helps to retain good negative examples. The majority of the collected pairs do not describe prototypical properties (e.g., useless pistol, organic celery), with only a few (∼10) exceptions (e.g., silvery minnow). The final dataset contains 1,566 instances in total, 783 for each class (positive and negative).17 Representations For each AN in pos and neg, we obtain a BERT representation from a sentence (sAN ) in ukWaC where A modifies N. We pair sAN with a sentence sN where A has been automatically 16 We obtain the dependency parse of a sentence using stanza (Qi et al., 2020). 17 We omitted five positive AN pairs because not enough negative instances were found for the noun in Set (B) or in ukWaC. Appendix B.6 contains the quantifier precedence results. More details on determiners are given in Appendix B.6. 84 Model BERT BERT (ISO) fastText word2vec ALL - PROTO deleted (e.g., “Then shape into balls about the size of a small tangerine” vs. “Then shape into balls about the size of a tangerine”). We choose sentences where A is not modified by an adverb (e.g., very small ant, where removing small would result in an ungrammatical sentence). When no sentences are found"
2021.blackboxnlp-1.7,D16-1264,0,0.0295985,"learned from corpus-harvested phrase vectors. In our work, we represent AN phrases by combining the contextualised BERT representations of A and N in sentences where they occur, using algebraic operations. We also investigate the extent to which the representations of A and N in an AN phrase capture its meaning, since token-level BERT embeddings encode information from the surrounding context. 2 Relations between entities stored in Wikidata, common sense relations between concepts from ConceptNet (Speer and Havasi, 2012), and knowledge aimed for answering natural language questions in SQuAD (Rajpurkar et al., 2016). 80 cally extracted from Open Mind Common Sense (OMCS)3 sentences and are often too long, including irrelevant information that might confuse the model.4 Jiang et al. (2020) demonstrate the impact of prompt quality on LM probing but focus on relations involving encyclopedic knowledge (e.g., born/died in, profession, subclass). Bouraoui et al. (2020) also explore the knowledge BERT has about lexical, morphological and commonsense relations (e.g., hypernymy, meronymy, plural, cause-effect) through fine-tuning, but neither they address noun properties. 3 C instances having feature f (e.g., ALL g"
2021.naacl-main.370,D18-1202,1,0.855257,"n FR ES EL EN FR ES EL D E M ELO dim < gloomy < dark < black terne < sombre < foncé < noir sombrío < tenebroso < oscuro < negro αμυδρός ||αχνός < μουντός < σκοτεινός< μαύρος W ILKINSON bad < awful < terrible < horrible mauvais < affreux < terrible < horrible malo < terrible < horrible < horroroso κακός < απαίσιος < τρομερός < φρικτός Table 1: Example translations from each dataset. “||” indicates adjectives at the same intensity level (ties). Work on scalar adjectives has until now evolved around pre-compiled datasets (de Melo and Bansal, 2013; Taboada et al., 2011; Wilkinson and Oates, 2016; Cocos et al., 2018). Reliance on external resources has also restricted research to English, and has led to the prevalence of pattern-based and lexicon-based approaches. Recently, Garí Soler and Apidianaki (2020) showed that BERT representations (Devlin et al., 2019) encode intensity relationships between English scalar adjectives, paving the way for applying contextualised representations to intensity detection in other languages.1 In our work, we explicitly address the scalar adjective identification task, overlooked until now due to the focus on pre-compiled resources. We furthermore propose to extend scalar"
2021.naacl-main.370,N19-1423,0,0.0163692,"< horrible malo < terrible < horrible < horroroso κακός < απαίσιος < τρομερός < φρικτός Table 1: Example translations from each dataset. “||” indicates adjectives at the same intensity level (ties). Work on scalar adjectives has until now evolved around pre-compiled datasets (de Melo and Bansal, 2013; Taboada et al., 2011; Wilkinson and Oates, 2016; Cocos et al., 2018). Reliance on external resources has also restricted research to English, and has led to the prevalence of pattern-based and lexicon-based approaches. Recently, Garí Soler and Apidianaki (2020) showed that BERT representations (Devlin et al., 2019) encode intensity relationships between English scalar adjectives, paving the way for applying contextualised representations to intensity detection in other languages.1 In our work, we explicitly address the scalar adjective identification task, overlooked until now due to the focus on pre-compiled resources. We furthermore propose to extend scalar adjective ranking to new languages. We make available two new benchmark datasets for scalar adjective identification and multilingual ranking: (a) SCAL - REL, a balanced dataset of relational and scalar adjectives which can serve to probe model rep"
2021.naacl-main.370,L18-1550,0,0.0292533,"onduct experiments with state-ofthe-art contextual language models and several baselines on the MULTI - SCALE dataset. We use the pre-trained cased and uncased multilingual BERT model (Devlin et al., 2019) and report results of the best variant for each language. We also report results obtained with four monolingual models: bert-base-uncased (Devlin et al., 2019), flaubert_base_uncased (Le et al., 2020), bert-base-spanish-wwmuncased (Cañete et al., 2020), and bert-basegreek-uncased-v1 (Koutsikakis et al., 2020). We compare to results obtained using fastText static embeddings in each language (Grave et al., 2018). For a scale s, we feed the corresponding set of sentences to a model and extract the contextualised representations for ∀ a ∈ s from every layer. When an adjective is split into multiple BPE units, we average the representations of all wordpieces (we call this approach “WP”) or all pieces but the last one (“WP-1”). The intuition behind excluding the last WP is that the ending of a word often corresponds to a suffix with morphological information. The DIFFVEC method We apply the adjective ranking method proposed by Garí Soler and Apidianaki (2020) to our dataset, which relies on an intensity"
2021.naacl-main.370,P10-1018,0,0.0588701,"Missing"
2021.naacl-main.370,Q13-1023,0,0.0609654,"Missing"
2021.naacl-main.370,L16-1424,0,0.0996325,"on this task. 1 Introduction FR ES EL EN FR ES EL D E M ELO dim < gloomy < dark < black terne < sombre < foncé < noir sombrío < tenebroso < oscuro < negro αμυδρός ||αχνός < μουντός < σκοτεινός< μαύρος W ILKINSON bad < awful < terrible < horrible mauvais < affreux < terrible < horrible malo < terrible < horrible < horroroso κακός < απαίσιος < τρομερός < φρικτός Table 1: Example translations from each dataset. “||” indicates adjectives at the same intensity level (ties). Work on scalar adjectives has until now evolved around pre-compiled datasets (de Melo and Bansal, 2013; Taboada et al., 2011; Wilkinson and Oates, 2016; Cocos et al., 2018). Reliance on external resources has also restricted research to English, and has led to the prevalence of pattern-based and lexicon-based approaches. Recently, Garí Soler and Apidianaki (2020) showed that BERT representations (Devlin et al., 2019) encode intensity relationships between English scalar adjectives, paving the way for applying contextualised representations to intensity detection in other languages.1 In our work, we explicitly address the scalar adjective identification task, overlooked until now due to the focus on pre-compiled resources. We furthermore prop"
2021.nodalida-main.28,P18-2103,0,0.0283125,"Missing"
2021.nodalida-main.28,N18-2017,0,0.037114,"Missing"
2021.nodalida-main.28,D19-1275,0,0.0269615,"ccuracy for models fine-tuned on the ANLI hypothesis-only dataset. Although there still seems to be space for improvement (accuracy is around 0.5, i.e. well above chance), the reported findings are promising. Specifically, the performance is lower than on the hypothesis-only SNLI/MNLI datasets, showing that the dataset contains less artefacts that can guide prediction. ANLI is thus a natural candidate to further test our hypotheses, as it claims to remedy for a number of the shortcomings of earlier NLI datasets. Lessons learnt from previous work on designing reliable linguistic probing tasks (Hewitt and Liang, 2019) and the overfitting problems of NLI models discussed above, demonstrate the importance of systematic sanity checks like the ones we propose in this paper. Our dedicated control tasks specifically allow to determine whether a dataset triggers the models’ reasoning capabilities or, instead, allows them to rely on statistical biases and annotation artefacts for prediction. We use the quality of the predictions made by models finetuned and tested on corrupted data as a proxy to evaluate data quality. 3 3.1 Datasets The Multi-Genre NLI (MNLI) Corpus We carry out our experiments on the MultiGenre N"
2021.nodalida-main.28,S14-2055,0,0.043277,"Missing"
2021.nodalida-main.28,2021.ccl-1.108,0,0.0865513,"Missing"
2021.nodalida-main.28,marelli-etal-2014-sick,0,0.0927551,"Missing"
2021.nodalida-main.28,S18-2023,0,0.0449508,"Missing"
2021.nodalida-main.28,W19-4810,1,0.877356,"Missing"
2021.nodalida-main.28,N18-1101,0,0.13604,"prediction (Pham et al., 2020). To the contrary, small tweaks or perturbations in the data, such as replacing words with mutually exclusive cohyponyms and antonyms (Glockner et al., 2018) or changing the order of the two sentences (Wang et al., 2019b), has been shown to hurt the performance of NLI models. Motivated by this situation, our goal is to contribute a new suite of diagnostic tests that can be used to assess the quality of an NLU benchmark. In particular, we conduct a series of controlled experiments where a set of data corruption transformations are applied to the widely used MNLI (Williams et al., 2018) and ANLI (Nie et al., 2020) datasets, and explore their impact on fine-tuned BERT and ROBERTa (Liu et al., 2019) model performance. The obtained results provide evidence that can reveal the quality of a dataset: Given that the transformations seriously affect the quality of NLI sentences, going as far as making them unintelligible (cf. examples in Table 1), a decrease in performance for models fine-tuned on the corrupted dataset would be expected. High performance would, instead, indicate the presence of biases and other artefacts in the dataset which guide models’ predictions. This situation"
2021.nodalida-main.28,2020.acl-main.773,0,0.0313274,"Missing"
apidianaki-2008-translation,W04-2406,0,\N,Missing
apidianaki-2008-translation,W04-0811,0,\N,Missing
apidianaki-2008-translation,W04-0802,0,\N,Missing
apidianaki-2008-translation,S07-1002,0,\N,Missing
apidianaki-2008-translation,C02-1058,0,\N,Missing
apidianaki-2008-translation,C94-2113,0,\N,Missing
apidianaki-2008-translation,H91-1025,0,\N,Missing
apidianaki-2008-translation,C04-1192,0,\N,Missing
apidianaki-2008-translation,W03-0304,0,\N,Missing
apidianaki-2008-translation,W04-2213,0,\N,Missing
apidianaki-2008-translation,H05-1097,0,\N,Missing
apidianaki-2008-translation,J98-1004,0,\N,Missing
apidianaki-2008-translation,J04-2003,0,\N,Missing
apidianaki-2008-translation,P06-1014,0,\N,Missing
apidianaki-2008-translation,P03-1058,0,\N,Missing
apidianaki-2008-translation,P06-2111,0,\N,Missing
apidianaki-2008-translation,2005.mtsummit-papers.11,0,\N,Missing
apidianaki-2008-translation,W06-2505,0,\N,Missing
apidianaki-etal-2014-semantic,J90-1003,0,\N,Missing
apidianaki-etal-2014-semantic,N12-1095,0,\N,Missing
apidianaki-etal-2014-semantic,D09-1056,0,\N,Missing
apidianaki-etal-2014-semantic,S07-1002,0,\N,Missing
apidianaki-etal-2014-semantic,J13-3008,0,\N,Missing
apidianaki-etal-2014-semantic,P01-1008,0,\N,Missing
apidianaki-etal-2014-semantic,D08-1021,0,\N,Missing
apidianaki-etal-2014-semantic,W06-1610,0,\N,Missing
apidianaki-etal-2014-semantic,S13-2049,0,\N,Missing
apidianaki-etal-2014-semantic,S07-1009,1,\N,Missing
apidianaki-etal-2014-semantic,N03-1003,0,\N,Missing
apidianaki-etal-2014-semantic,P05-1074,0,\N,Missing
apidianaki-etal-2014-semantic,N06-1003,0,\N,Missing
apidianaki-etal-2014-semantic,N10-1031,0,\N,Missing
apidianaki-etal-2014-semantic,2005.mtsummit-papers.11,0,\N,Missing
apidianaki-etal-2014-semantic,S10-1011,0,\N,Missing
apidianaki-etal-2014-semantic,2010.iwslt-papers.2,1,\N,Missing
apidianaki-etal-2014-semantic,W07-0716,0,\N,Missing
apidianaki-etal-2014-semantic,D07-1043,0,\N,Missing
apidianaki-sagot-2012-applying,apidianaki-2008-translation,1,\N,Missing
apidianaki-sagot-2012-applying,steinberger-etal-2006-jrc,0,\N,Missing
apidianaki-sagot-2012-applying,E09-1010,1,\N,Missing
apidianaki-sagot-2012-applying,W02-0808,0,\N,Missing
apidianaki-sagot-2012-applying,P10-1023,0,\N,Missing
apidianaki-sagot-2012-applying,P02-1033,0,\N,Missing
apidianaki-sagot-2012-applying,P06-2111,0,\N,Missing
apidianaki-sagot-2012-applying,J03-1002,0,\N,Missing
apidianaki-sagot-2012-applying,2005.mtsummit-papers.11,0,\N,Missing
apidianaki-sagot-2012-applying,2010.jeptalnrecital-court.19,0,\N,Missing
apidianaki-sagot-2012-applying,2010.iwslt-papers.2,1,\N,Missing
C12-2007,apidianaki-2008-translation,1,0.855607,"o help human translators in their work and assist language learners. See (Madnani and Dorr, 2010) for a comprehensive survey of data-driven methods for paraphrase generation. 64 language pairs. CLLS addresses words of all open-class parts of speech in one language pair (English-Spanish) while CL-WSD focuses on the translation of English nouns in five languages (French, Spanish, German, Dutch and Italian).3 Another point of variation concerns the definition of senses. In CL-WSD, target word senses were described by means of clusters of their semantically similar translations (Ide et al., 2002; Apidianaki, 2008). More precisely, the translations of the target words in the Europarl corpus (Koehn, 2005) were manually clustered and the obtained clusters served for tagging. On the contrary, CLLS did not involve a clustering step and the annotators could propose translations found in any external resource. The CLLS test set was built from the English Internet Corpus (Sharoff, 2005) while CL-WSD test sentences were extracted from the BNC4 and the JRC -ACQUIS corpus (Steinberger et al., 2006). 2.2 Translation context: a neglected parameter Although the CL SemEval tasks are clearly oriented towards MT, annot"
C12-2007,E09-1010,1,0.852371,"geable on the basis of formal criteria, such as distributional similarity, might not be substitutable in real texts because of other parameters preventing the substitution (e.g. syntactic structure, collocations). In a translation setting where the substitution is done cross-lingually, it is important that the paraphrases preserve both the sense of the original word (or phrase) and the fluency of the translated text. However, clustered translations are usually near-synonyms translating the same sense, but almost never absolute synonyms interchangeable in translations (Edmonds and Hirst, 2002; Apidianaki, 2009). Consequently, although CLLS and CL-WSD could greatly contribute in MT by enhancing the semantic relevance of translations, the existing evaluations do not provide a fair estimate of the systems’ capacity to propose translations that would also fit well in the translated texts. We conduct a series of experiments to assess the adequacy of CL paraphrases in translations by exploiting the CLLS and CL-WSD test sets. As the two test sets were mainly built from monolingual corpora, no reference translations are available against which the quality of the CL paraphrases could be measured using standa"
C12-2007,P05-1074,0,0.338048,"evaluations do not provide a fair estimate of the systems’ capacity to propose translations that would also fit well in the translated texts. We conduct a series of experiments to assess the adequacy of CL paraphrases in translations by exploiting the CLLS and CL-WSD test sets. As the two test sets were mainly built from monolingual corpora, no reference translations are available against which the quality of the CL paraphrases could be measured using standard MT evaluation metrics (BLEU, METEOR, etc.). So, we adopt a variation of the substitution-based approach used in works on paraphrasing (Bannard and Callison-Burch, 2005) for validating candidate paraphrases, based on the assumption that items deemed to be paraphrases may behave as such only in some contexts and not in others. We translate the CLLS and CL-WSD test sets with a state-of-the-art MT system by exploiting the manually-defined GS paraphrases. Once the set of translations for each test sentence is produced, we measure the substitutability of the GS paraphrases using an automatic and a human ranking, as explained in the next section. 3 The CLLS lexical sample is composed of 300 noun, 310 verb, 280 adjective and 110 adverb instances with approximately 5"
C12-2007,D08-1021,0,0.113159,"onsider that it is not reliable enough to lead to safe conclusions as to the adequacy of CL paraphrases in translations. So, we also conduct a human evaluation. The annotators are asked to rank the set of Moses translations produced for each target word instance on a 3-point scale, according to the adequacy of the paraphrases and the fluency of the translated text.8 Good quality paraphrases (i.e. the highest ranked ones, assigned a ‘1’ value) should preserve both the meaning of the source word and the grammaticality of the target sentence. This experiment can be viewed as a substitution test (Callison-Burch, 2008) with the difference that the paraphrases are not just substituted in the translated sentences but fed into the MT system which exploits them during translation. Consequently, the context surrounding the paraphrase might be altered as well, as shown in the examples given in Table 1. The human ranking covers 538 instances of the CL-WSD test set with an average of 4.17 French paraphrases per instance. The 538 translation sets produced by Moses contain a total of 1821 7 8 Normalized scores are rounded to two decimal places. Translations with lower scores are considered as more fluent. The annotat"
C12-2007,N06-1003,0,0.031384,"f the participating systems to provide semantically correct translations for words in context that could, among others, constitute the input of Machine Translation (MT) systems (Mihalcea et al., 2010; Lefever and Hoste, 2010).1 The underlying assumption is that the closer the output of a CLLS / CL -WSD system is to a manually built gold standard of cross-lingual paraphrases, the higher its contribution in a real application will be. Paraphrasing is highly useful in MT as is shown by the substantial amount of research undertaken on the subject.2 It permits to deal with out-of-vocabulary words (Callison-Burch et al., 2006; Marton et al., 2009), capture lexical variation during evaluation (Zhou et al., 2006; Owczarzak et al., 2006), expand the set of reference translations for minimum error rate training (Madnani et al., 2007) and improve the general performance of MT systems (Max, 2010). It is however interesting that in spite of the MT orientation of the CL SemEval-2 tasks, translation selection and evaluation are carried out by reference solely to the source language. The target language context which plays an important role in lexical selection in statistical MT systems, as highlighted by the strong influen"
C12-2007,D07-1007,0,0.0314005,"spite of the MT orientation of the CL SemEval-2 tasks, translation selection and evaluation are carried out by reference solely to the source language. The target language context which plays an important role in lexical selection in statistical MT systems, as highlighted by the strong influence of the language model on word choice, is not considered. In this work, we explore the role of the target language in CLLS and CL-WSD by measuring the adequacy of CL paraphrases in translations. Our goal is not to estimate the impact of semantics in MT, as was the case in previous works on the subject (Carpuat and Wu, 2007; Chan et al., 2007), but to empirically test the adequacy of the sense descriptions provided in the CL evaluation tasks in an MT setting. The paper is organized as follows. The CL SemEval-2 tasks are described in Section 2. The adopted experimental methodology and evaluation setup are presented in Section 3. The analysis of the obtained results, in Section 4, highlights the importance of the target language context for CLLS and CL-WSD, and the implications of these findings for CL semantic evaluations. 2 Translation context in cross-lingual semantic evaluations 2.1 The SemEval-2 Cross-Lingual"
C12-2007,P07-1005,0,0.0198087,"ation of the CL SemEval-2 tasks, translation selection and evaluation are carried out by reference solely to the source language. The target language context which plays an important role in lexical selection in statistical MT systems, as highlighted by the strong influence of the language model on word choice, is not considered. In this work, we explore the role of the target language in CLLS and CL-WSD by measuring the adequacy of CL paraphrases in translations. Our goal is not to estimate the impact of semantics in MT, as was the case in previous works on the subject (Carpuat and Wu, 2007; Chan et al., 2007), but to empirically test the adequacy of the sense descriptions provided in the CL evaluation tasks in an MT setting. The paper is organized as follows. The CL SemEval-2 tasks are described in Section 2. The adopted experimental methodology and evaluation setup are presented in Section 3. The analysis of the obtained results, in Section 4, highlights the importance of the target language context for CLLS and CL-WSD, and the implications of these findings for CL semantic evaluations. 2 Translation context in cross-lingual semantic evaluations 2.1 The SemEval-2 Cross-Lingual tasks In the CLLS a"
C12-2007,J02-2001,0,0.0137323,"Words that seem interchangeable on the basis of formal criteria, such as distributional similarity, might not be substitutable in real texts because of other parameters preventing the substitution (e.g. syntactic structure, collocations). In a translation setting where the substitution is done cross-lingually, it is important that the paraphrases preserve both the sense of the original word (or phrase) and the fluency of the translated text. However, clustered translations are usually near-synonyms translating the same sense, but almost never absolute synonyms interchangeable in translations (Edmonds and Hirst, 2002; Apidianaki, 2009). Consequently, although CLLS and CL-WSD could greatly contribute in MT by enhancing the semantic relevance of translations, the existing evaluations do not provide a fair estimate of the systems’ capacity to propose translations that would also fit well in the translated texts. We conduct a series of experiments to assess the adequacy of CL paraphrases in translations by exploiting the CLLS and CL-WSD test sets. As the two test sets were mainly built from monolingual corpora, no reference translations are available against which the quality of the CL paraphrases could be me"
C12-2007,W02-0808,0,0.0865615,"Missing"
C12-2007,2005.mtsummit-papers.11,0,0.0892329,") for a comprehensive survey of data-driven methods for paraphrase generation. 64 language pairs. CLLS addresses words of all open-class parts of speech in one language pair (English-Spanish) while CL-WSD focuses on the translation of English nouns in five languages (French, Spanish, German, Dutch and Italian).3 Another point of variation concerns the definition of senses. In CL-WSD, target word senses were described by means of clusters of their semantically similar translations (Ide et al., 2002; Apidianaki, 2008). More precisely, the translations of the target words in the Europarl corpus (Koehn, 2005) were manually clustered and the obtained clusters served for tagging. On the contrary, CLLS did not involve a clustering step and the annotators could propose translations found in any external resource. The CLLS test set was built from the English Internet Corpus (Sharoff, 2005) while CL-WSD test sentences were extracted from the BNC4 and the JRC -ACQUIS corpus (Steinberger et al., 2006). 2.2 Translation context: a neglected parameter Although the CL SemEval tasks are clearly oriented towards MT, annotator judgments and system suggestions are made on the basis of source language information."
C12-2007,P07-2045,0,0.00269724,"automatic and a human ranking, as explained in the next section. 3 The CLLS lexical sample is composed of 300 noun, 310 verb, 280 adjective and 110 adverb instances with approximately 5 Spanish substitutes per target word and a pairwise inter-annotator agreement of 0.2777. The CL-WSD test data contains 50 instances of 20 target nouns and their substitutes in five languages. 4 http://www.natcorp.ox.ac.uk/ 65 3 Experimental setup 3.1 Systems and data The CLLS and CL-WSD test sets are translated into Spanish and French, respectively, using the baseline system of the WMT-2011 shared task (Moses) (Koehn et al., 2007). The two MT systems are trained on the data released for WMT-2011 for the two language pairs, namely the FrenchEnglish and Spanish-English parts of Europarl (version 6) (Koehn, 2005). The language models used during decoding are trained on the monolingual Spanish and French parts of Europarl. For each test sentence, we constrain the decoder to produce translations by using all GS paraphrases. These are plugged into Moses using its ‘XML Markup’ feature which allows to specify translations for parts of the input sentence. The ‘exclusive’ mode is activated which forces the decoder to use the XML"
C12-2007,S10-1003,0,0.10713,"ecember 2012. 63 1 Introduction An important trend in computational semantics in recent years is the adaptation of inventories, models and evaluations to specific applications. In this vein, the Cross-Lingual Lexical Substitution (CLLS) and Word Sense Disambiguation (CL-WSD) tasks of SemEval-2 address the disambiguation needs of multilingual applications: what is being evaluated is the capacity of the participating systems to provide semantically correct translations for words in context that could, among others, constitute the input of Machine Translation (MT) systems (Mihalcea et al., 2010; Lefever and Hoste, 2010).1 The underlying assumption is that the closer the output of a CLLS / CL -WSD system is to a manually built gold standard of cross-lingual paraphrases, the higher its contribution in a real application will be. Paraphrasing is highly useful in MT as is shown by the substantial amount of research undertaken on the subject.2 It permits to deal with out-of-vocabulary words (Callison-Burch et al., 2006; Marton et al., 2009), capture lexical variation during evaluation (Zhou et al., 2006; Owczarzak et al., 2006), expand the set of reference translations for minimum error rate training (Madnani et"
C12-2007,W07-0716,0,0.019795,"oste, 2010).1 The underlying assumption is that the closer the output of a CLLS / CL -WSD system is to a manually built gold standard of cross-lingual paraphrases, the higher its contribution in a real application will be. Paraphrasing is highly useful in MT as is shown by the substantial amount of research undertaken on the subject.2 It permits to deal with out-of-vocabulary words (Callison-Burch et al., 2006; Marton et al., 2009), capture lexical variation during evaluation (Zhou et al., 2006; Owczarzak et al., 2006), expand the set of reference translations for minimum error rate training (Madnani et al., 2007) and improve the general performance of MT systems (Max, 2010). It is however interesting that in spite of the MT orientation of the CL SemEval-2 tasks, translation selection and evaluation are carried out by reference solely to the source language. The target language context which plays an important role in lexical selection in statistical MT systems, as highlighted by the strong influence of the language model on word choice, is not considered. In this work, we explore the role of the target language in CLLS and CL-WSD by measuring the adequacy of CL paraphrases in translations. Our goal is"
C12-2007,J10-3003,0,0.0237554,"CLLS test set: &quot;At first the user is impressed by the fresh clean smell coming out of the machine and how nice it makes their home smell.&quot;, was tagged by the following set of translations which express the sense of fresh in Spanish: fresco 4; puro 1; flamante 1; limpio 1; nuevo 1. GS translations are lemmatized and the frequency counts indicate the number of annotators that proposed each substitute. The differences between the two tasks mainly lie in the targeted lexical samples and the involved 1 2 These systems can also help human translators in their work and assist language learners. See (Madnani and Dorr, 2010) for a comprehensive survey of data-driven methods for paraphrase generation. 64 language pairs. CLLS addresses words of all open-class parts of speech in one language pair (English-Spanish) while CL-WSD focuses on the translation of English nouns in five languages (French, Spanish, German, Dutch and Italian).3 Another point of variation concerns the definition of senses. In CL-WSD, target word senses were described by means of clusters of their semantically similar translations (Ide et al., 2002; Apidianaki, 2008). More precisely, the translations of the target words in the Europarl corpus (K"
C12-2007,D09-1040,0,0.0204836,"o provide semantically correct translations for words in context that could, among others, constitute the input of Machine Translation (MT) systems (Mihalcea et al., 2010; Lefever and Hoste, 2010).1 The underlying assumption is that the closer the output of a CLLS / CL -WSD system is to a manually built gold standard of cross-lingual paraphrases, the higher its contribution in a real application will be. Paraphrasing is highly useful in MT as is shown by the substantial amount of research undertaken on the subject.2 It permits to deal with out-of-vocabulary words (Callison-Burch et al., 2006; Marton et al., 2009), capture lexical variation during evaluation (Zhou et al., 2006; Owczarzak et al., 2006), expand the set of reference translations for minimum error rate training (Madnani et al., 2007) and improve the general performance of MT systems (Max, 2010). It is however interesting that in spite of the MT orientation of the CL SemEval-2 tasks, translation selection and evaluation are carried out by reference solely to the source language. The target language context which plays an important role in lexical selection in statistical MT systems, as highlighted by the strong influence of the language mod"
C12-2007,D10-1064,0,0.0129896,"a CLLS / CL -WSD system is to a manually built gold standard of cross-lingual paraphrases, the higher its contribution in a real application will be. Paraphrasing is highly useful in MT as is shown by the substantial amount of research undertaken on the subject.2 It permits to deal with out-of-vocabulary words (Callison-Burch et al., 2006; Marton et al., 2009), capture lexical variation during evaluation (Zhou et al., 2006; Owczarzak et al., 2006), expand the set of reference translations for minimum error rate training (Madnani et al., 2007) and improve the general performance of MT systems (Max, 2010). It is however interesting that in spite of the MT orientation of the CL SemEval-2 tasks, translation selection and evaluation are carried out by reference solely to the source language. The target language context which plays an important role in lexical selection in statistical MT systems, as highlighted by the strong influence of the language model on word choice, is not considered. In this work, we explore the role of the target language in CLLS and CL-WSD by measuring the adequacy of CL paraphrases in translations. Our goal is not to estimate the impact of semantics in MT, as was the cas"
C12-2007,S10-1002,0,0.0882457,"COLING 2012, Mumbai, December 2012. 63 1 Introduction An important trend in computational semantics in recent years is the adaptation of inventories, models and evaluations to specific applications. In this vein, the Cross-Lingual Lexical Substitution (CLLS) and Word Sense Disambiguation (CL-WSD) tasks of SemEval-2 address the disambiguation needs of multilingual applications: what is being evaluated is the capacity of the participating systems to provide semantically correct translations for words in context that could, among others, constitute the input of Machine Translation (MT) systems (Mihalcea et al., 2010; Lefever and Hoste, 2010).1 The underlying assumption is that the closer the output of a CLLS / CL -WSD system is to a manually built gold standard of cross-lingual paraphrases, the higher its contribution in a real application will be. Paraphrasing is highly useful in MT as is shown by the substantial amount of research undertaken on the subject.2 It permits to deal with out-of-vocabulary words (Callison-Burch et al., 2006; Marton et al., 2009), capture lexical variation during evaluation (Zhou et al., 2006; Owczarzak et al., 2006), expand the set of reference translations for minimum error"
C12-2007,W06-3112,0,0.041512,"Missing"
C12-2007,steinberger-etal-2006-jrc,0,0.0577961,"Missing"
C12-2007,H05-1097,0,0.0313922,"anking and the human ranking of translation adequacy, which is highly significant. Conclusion The findings of this study reveal that the results of the CL SemEval-2 tasks are not indicative of the contribution that the participating systems would have in MT. It has been shown that although the proposed evaluation metrics address the semantic relevance of CL paraphrases, they do not account for their suitability in translations. These empirical results highlight the importance of integrating translation information in CL semantic evaluations by resorting either to simplified translation tasks (Vickrey et al., 2005) or to full-fledged MT systems. Evaluation metrics capable of rewarding semantically correct translations that do not distort the fluency of the translations are much needed in the field of MT for evaluating the output of MT systems and the contribution of disambiguation modules. Another perspective worth exploring is the set up of all-words CL evaluation tasks, in addition to the lexical sample ones, allowing to assess the global capacities of CLLS and CL-WSD systems and the coverage they can attain in real-life applications. This setting would also permit to explore the potential of collabor"
C12-2007,W06-1610,0,0.022131,"at could, among others, constitute the input of Machine Translation (MT) systems (Mihalcea et al., 2010; Lefever and Hoste, 2010).1 The underlying assumption is that the closer the output of a CLLS / CL -WSD system is to a manually built gold standard of cross-lingual paraphrases, the higher its contribution in a real application will be. Paraphrasing is highly useful in MT as is shown by the substantial amount of research undertaken on the subject.2 It permits to deal with out-of-vocabulary words (Callison-Burch et al., 2006; Marton et al., 2009), capture lexical variation during evaluation (Zhou et al., 2006; Owczarzak et al., 2006), expand the set of reference translations for minimum error rate training (Madnani et al., 2007) and improve the general performance of MT systems (Max, 2010). It is however interesting that in spite of the MT orientation of the CL SemEval-2 tasks, translation selection and evaluation are carried out by reference solely to the source language. The target language context which plays an important role in lexical selection in statistical MT systems, as highlighted by the strong influence of the language model on word choice, is not considered. In this work, we explore t"
C12-2007,W09-2413,0,\N,Missing
C14-1121,W12-4201,1,0.858221,"mation was retained during training and not only for aligned ones, in contrast to direct transfer. We expect to augment the recall when using global estimates and hope that the effect on precision is not too negative. Learning For each French verb (v) in the lexicon built as described in Section 3, we want to be able to identify its correct predicate label in a new context by choosing one among its candidate labels (L) retained from the training corpus. A feature vector is built for each candidate label Li (1 ≤ i ≤ |L|) found for the verb v in the lexicon, following the procedure described in Apidianaki et al. (2012). For each candidate label, we extract the content word co-occurrences of the verb v in the French sentences where it translates an English verb tagged with this label in the training corpus. The retained French words constitute the features of the vector built for that label. Let N be the number of features retained for each label Li of the verb v from the corresponding French contexts. Each feature Fj (1 ≤ j ≤ N ) receives a total weight with the label (tw(Fj , Li )) which is learned from the data and defined as the product of the feature’s global weight (gw(Fj )) and its local weight with t"
C14-1121,E09-1010,1,0.835832,"its sense. For example, “give.01” stands for the first sense of the verb give. As the predicate label contains a lot of lexical information, putting the correct English predicate label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the candidate senses are the words’ translations in other languages and WSD aims at predicting semantically correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual transfer of predicate labels is that we do not search for correct translations of French words but for the most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense). The global predicate labelling method consists of a learning step and a labelling step. During learning, we compute estimates for annotation transfer on the basis of the word alignments between English and French predicates over the entire parallel training corpus. At labelling time, we label French verbs with English predica"
C14-1121,2009.jeptalnrecital-long.4,0,0.11988,"Missing"
C14-1121,D07-1007,0,0.0257633,"n the English verb and its sense. For example, “give.01” stands for the first sense of the verb give. As the predicate label contains a lot of lexical information, putting the correct English predicate label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the candidate senses are the words’ translations in other languages and WSD aims at predicting semantically correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual transfer of predicate labels is that we do not search for correct translations of French words but for the most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense). The global predicate labelling method consists of a learning step and a labelling step. During learning, we compute estimates for annotation transfer on the basis of the word alignments between English and French predicates over the entire parallel training corpus. At labelling time, we label French verbs w"
C14-1121,P11-1061,0,0.0277476,"The two global methods proposed in this paper are presented in Section 5. We report and discuss our results in Section 6, before concluding. 2 Related work Transferring annotation from one language to another in order to train monolingual tools for new languages was first introduced by Yarowsky and Ngai (2001). In their approach, token-level part-of-speech (PoS) and noun phrase bracketing information was projected across word-aligned bitext and this partial annotation served to estimate the parameters of a model that generalized from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed 1 Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010) or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider a"
C14-1121,J94-4004,0,0.0387831,"the previous section and exploit them for role labelling. For a given predicate, diathesis alternations are the major source of variation in propositions. They give rise to different syntactic structures, while the semantic roles remain stable. For example, the sentence “I gave the book to Jean” is syntactically different from “I gave Jean the book”, but semantic roles on the three arguments stay the same. We will show in a feasibility study that the effect of diathesis alternations on the correlation between syntax and semantics is limited. In a cross-lingual setting, structural divergences (Dorr, 1994) are expected to reduce the correlation between syntax and semantics. An example is the difference in syntactic structure between the sentences “Tu me manques” vs. “I miss you”, which are translations of each other, however the semantic roles are the same across languages. As our global method is not restricted to alignments at labelling time, we are able to classify all given arguments3 and not just those that are aligned in a parallel corpus. In this way, we believe that the negative effect of structural divergences and diathesis alternations is limited. Moreover, we show how mild supervisio"
C14-1121,W06-1601,0,0.029663,"us section. P|CF | j=1 tw(CFj , Li ) assoc score(Vi , C) = (4) |CF | The label that receives the highest association score with the new context is returned and serves to annotate the corresponding French verb. 5.2 Global cross-lingual role labelling For role labelling, we adopt a different strategy. Whereas predicate labels include a lot of lexical information, role labels do not. However, for role labels there is another source of information that helps to define global estimates: the correlation between syntax and semantics. Previous work in monolingual unsupervised semantic role induction (Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011) showed that mapping rules that assign semantic roles to arguments of a verb based on the syntactic functions of these arguments, represent a baseline that is very hard to beat. This strong correlation between syntactic labels and semantic role labels in the PropBank annotation has been shown in detail by Merlo and Van der Plas (2009). In contrast to previous work on monolingual unsupervised semantic role induction, we add the predicate label as a predictor. The core arguments of the verb, that are the numbered labels in PropBank, are known to be"
C14-1121,W08-2122,0,0.052169,"Missing"
C14-1121,2005.mtsummit-papers.11,0,0.0276186,"nd lexical) to adapt a source model to a targetlanguage model. The ideas behind their cross-lingual model adaptation resemble the ideas behind our global method for semantic role labelling. However, in contrast to their work we do not consider the predicate labelling as given because, as manual annotations show (van der Plas et al., 2010), this task is not trivial. We first build a tailored global model for cross-lingual predicate labelling and then use the predicted predicate labels for semantic role labelling. 3 Data In our experiments, we use the English-French part of the Europarl corpus (Koehn, 2005). The dataset is tokenised and lowercased and only sentence pairs corresponding to a one-to-one sentence alignment with lengths ranging from one to 40 tokens on both French and English sides are considered. Furthermore, because translation shifts are known to pose problems for the automatic projection of semantic roles across languages (Pad´o, 2007), we select only those parallel sentences in Europarl that are direct 1281 translations from English to French or vice versa. In the end, we have a parallel corpus of 276-thousand sentence pairs. The English part of the parallel corpus is annotated"
C14-1121,P13-1117,0,0.355914,"across multiple examples. In their work, transfer of predicate labels and semantic role labels is done in one step. The model needs an aggressive filter to compensate for missing annotations on the predicate level after direct transfer. This filter successively leads to drops in performance for the role labellings. Here, we build two separate global models that complement direct transfer instead of relying on it. The same emphasis on learning is found in cross-lingual model transfer where source language models are adapted to work on the target language directly. For semantic role labelling, Kozhevnikov and Titov (2013) use shared feature representations (syntactic and lexical) to adapt a source model to a targetlanguage model. The ideas behind their cross-lingual model adaptation resemble the ideas behind our global method for semantic role labelling. However, in contrast to their work we do not consider the predicate labelling as given because, as manual annotations show (van der Plas et al., 2010), this task is not trivial. We first build a tailored global model for cross-lingual predicate labelling and then use the predicted predicate labels for semantic role labelling. 3 Data In our experiments, we use"
C14-1121,N10-1137,0,0.0716458,"o estimate the parameters of a model that generalized from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed 1 Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010) or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider argument identification as given. 1280 EN: English FR: French Parallel corpus direct transfer FR EN semantic annotations (predicates + roles) FR annotation transfer PoS tags semantic annotations (predicates + roles) Parallel corpus global predicate labelling EN semantic annotations (predicates) FR PoS tags learning model for predicate labelling FR labelling semantic annotations (predicates) FR Parallel corpus EN syntactic annotations EN learning model for role labelling FR syntac"
C14-1121,P11-1112,0,0.113379,"ed from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed 1 Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010) or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider argument identification as given. 1280 EN: English FR: French Parallel corpus direct transfer FR EN semantic annotations (predicates + roles) FR annotation transfer PoS tags semantic annotations (predicates + roles) Parallel corpus global predicate labelling EN semantic annotations (predicates) FR PoS tags learning model for predicate labelling FR labelling semantic annotations (predicates) FR Parallel corpus EN syntactic annotations EN learning model for role labelling FR syntactic annotations FR model transfer OR global semanti"
C14-1121,J93-2004,0,0.0488719,"Missing"
C14-1121,P09-1033,1,0.929936,"Missing"
C14-1121,P03-1058,0,0.0207565,"ate labels contain the English verb and its sense. For example, “give.01” stands for the first sense of the verb give. As the predicate label contains a lot of lexical information, putting the correct English predicate label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the candidate senses are the words’ translations in other languages and WSD aims at predicting semantically correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual transfer of predicate labels is that we do not search for correct translations of French words but for the most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense). The global predicate labelling method consists of a learning step and a labelling step. During learning, we compute estimates for annotation transfer on the basis of the word alignments between English and French predicates over the entire parallel training corpus. At labelling time, w"
C14-1121,J03-1002,0,0.0104729,"Missing"
C14-1121,J05-1004,0,0.919839,", Ireland, August 23-29 2014. or constituents in those sentences. We will refer to these traditional methods as direct transfer because the semantic annotations are transferred directly from token to token. Although direct transfer methods are straightforward and easy to implement, they are vulnerable to missing or incorrect alignments which lead to missing and erroneous annotations in the target language. Consequently, non-literal translations and translation shifts present major problems for these methods. In this paper we propose a global approach to the cross-lingual transfer of PropBank (Palmer et al., 2005) semantic annotations that aggregates information at the corpus level and, as a consequence, is more robust to non-literal translations and alignment errors. Our global approach involves two steps: in the learning step, two global models are learned on the basis of role and predicate annotations in the source language (English). In the labelling step, these models assign labels to verbs and their arguments in the target language (French) without consulting any parallel data. Contrary to previous work, we build separate models for the transfer of semantic role and predicate annotation because p"
C14-1121,J08-2005,0,0.0479713,"to A1 if neither the dependency label nor the predicate has been seen in training. To treat the R-suffix, which takes care of anaphoric arguments, we use the following simple rule: for the monolingual setting all arguments with PoS-tags “WDT”, “WP”, and “WRB” receive the R-suffix. In the cross-lingual setting, we translate the PoS tags to the single French PoS tag “PROREL”. We do not treat the C-prefix, which takes care of discontinuous arguments, because there were only a few examples. We do not accept duplicate semantic roles, a constraint that leads to valid role configurations in general (Punyakanok et al., 2008). We expect the more prominent semantic roles, such as A0 and A1, to appear earlier in the sentence than semantic roles with higher numbers. We therefore attribute semantic roles of a predicate from left to right. 5.3 Combining direct and global cross-lingual transfer Direct transfer methods generally have low recall, we however expect them to be more precise than the global methods. In our combined method, we use the annotations assigned by direct transfer as the backbone and fill missing labels by the global methods. The annotations from direct transfer restrict the possible roles the global"
C14-1121,Q13-1001,0,0.0823028,"Missing"
C14-1121,W07-2218,0,0.0603147,"Missing"
C14-1121,W10-1814,1,0.935459,"Missing"
C14-1121,N01-1026,0,0.0849576,"our method adapts relatively easily to other language pairs without requiring semantic lexicons in the target language. In the next section, we present related work on cross-lingual annotation transfer. In Section 3 we present the data used in our experiments and in Section 4 we briefly discuss direct transfer. The two global methods proposed in this paper are presented in Section 5. We report and discuss our results in Section 6, before concluding. 2 Related work Transferring annotation from one language to another in order to train monolingual tools for new languages was first introduced by Yarowsky and Ngai (2001). In their approach, token-level part-of-speech (PoS) and noun phrase bracketing information was projected across word-aligned bitext and this partial annotation served to estimate the parameters of a model that generalized from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distrib"
C14-1121,W09-1201,0,\N,Missing
D16-1215,apidianaki-etal-2014-semantic,1,0.862913,"ackages contain high-precision paraphrases while larger ones aim for high coverage. Until now, pivot paraphrases have been used as equivalence sets (i.e. all paraphrases available for a word are viewed as semantically equivalent) and their substitutability in context has not yet been addressed. Substitutability might be restrained by several factors which make choosing the appropriate paraphrase for a word or phrase in different contexts a non-trivial task. In case of polysemous words, paraphrases describe different meanings and can lead to erroneous semantic mappings if substituted in texts (Apidianaki et al., 2014; Cocos and CallisonBurch, 2016). Even when paraphrases capture the same general sense, they are hardly ever equivalent synonyms and generally display subtle differences in meaning, connotation or usage (Edmonds and Hirst, 2002). Stylistic variation might also be present within paraphrase sets and substituting paraphrases that differ in terms of complexity and formality can result in a change in style (Pavlick and Nenkova, 2015). To increase paraphrase applicability in context, Pavlick et al. (2015a) propose to extract domain-specific pivot paraphrases by biasing the parallel training data use"
D16-1215,N15-1059,0,0.0356583,"asses which can lead to erroneous sense mappings due to semantic distinctions present in the paraphrase sets. We have recently showed that the context-based filtering of semantic variants improves METEOR’s correlation with human judgments of translation quality (Marie and Apidianaki, 2015). We believe that a contextbased paraphrase ranking mechanism will enhance correct substitutions and further improve the metric. Last but not least, the paraphrase vectors can be used for mapping the contents of the PPDB resource to other multilingual resources for which vector representations are available (Camacho-Collados et al., 2015a; Camacho-Collados et al., 2015b). The interest of mapping paraphrases in the vector space to concepts found in existing semantic resources is twofold: it would permit to analyse the semantics of the paraphrases by putting them into correspondence with explicit concept representations and would serve to enrich other semantic resources (e.g. BabelNet synsets) with semantically similar paraphrases. Handling phrasal paraphrases is another natural extension of this work. We consider using a vector space model of semantic composition to calculate the meaning of longer candidate paraphrases (Dinu e"
D16-1215,P15-1072,0,0.0184764,"asses which can lead to erroneous sense mappings due to semantic distinctions present in the paraphrase sets. We have recently showed that the context-based filtering of semantic variants improves METEOR’s correlation with human judgments of translation quality (Marie and Apidianaki, 2015). We believe that a contextbased paraphrase ranking mechanism will enhance correct substitutions and further improve the metric. Last but not least, the paraphrase vectors can be used for mapping the contents of the PPDB resource to other multilingual resources for which vector representations are available (Camacho-Collados et al., 2015a; Camacho-Collados et al., 2015b). The interest of mapping paraphrases in the vector space to concepts found in existing semantic resources is twofold: it would permit to analyse the semantics of the paraphrases by putting them into correspondence with explicit concept representations and would serve to enrich other semantic resources (e.g. BabelNet synsets) with semantically similar paraphrases. Handling phrasal paraphrases is another natural extension of this work. We consider using a vector space model of semantic composition to calculate the meaning of longer candidate paraphrases (Dinu e"
D16-1215,N16-1172,0,0.199309,"Missing"
D16-1215,de-marneffe-etal-2006-generating,0,0.174326,"Missing"
D16-1215,W10-1751,0,0.171548,"rve the semantics of specific text fragments. We evaluate the vector-based ranking models on data hand-annotated with lexical variants and compare the obtained ranking to confidence estimates available in the PPDB, highlighting the importance of context filtering for paraphrase selection. 2 Context-based paraphrase ranking 2.1 Paraphrase substitutability The PPDB1 provides millions of lexical, phrasal and syntactic paraphrases in 21 languages – acquired by applying bi- and multi-lingual pivoting on parallel corpora (Bannard and Callison-Burch, 2005) – and is largely exploited in applications (Denkowski and Lavie, 2010; Sultan et al., 2014; Faruqui et al., 1 http://paraphrase.org/#/download 2028 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2028–2034, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2015). PPDB paraphrases come into packages of different sizes (going from S to XXXL): smaller packages contain high-precision paraphrases while larger ones aim for high coverage. Until now, pivot paraphrases have been used as equivalence sets (i.e. all paraphrases available for a word are viewed as semantically equivalent) and the"
D16-1215,D10-1113,0,0.152238,"tional and word embedding similarities, formality and complexity scores, and scores assigned by a supervised ranking model (Pavlick et al., 2015b). These features serve to identify good quality paraphrases but do not say much about their substitutability in context. To judge the adequacy of paraphrases for specific instances of words or phrases, the surrounding context needs to be considered. This can be done using vector-space models of semantics which calculate the meaning of word occurrences in context based on distributional representations (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). These models capture the influence of the context on the meaning of a target word through vector composition. More precisely, they represent the contextualised meaning of a target word w in context c by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition (Thater et al., 2011). We use this kind of representations to rank the PPDB paraphrases in context and retain the ones that preserve the semantics of specific text fragments. We evaluate the vector-based ranking models on data hand-annotated with l"
D16-1215,P13-4006,0,0.0464403,"Missing"
D16-1215,D08-1094,0,0.11659,"Missing"
D16-1215,N15-1184,0,0.0868108,"Missing"
D16-1215,N13-1092,0,0.325269,"vector-space semantic models for selecting PPDB paraphrases that preserve the meaning of specific text fragments. This is the first work that addresses the substitutability of PPDB paraphrases in context. We show that vector-space models of meaning can be successfully applied to this task and increase the benefit brought by the use of the PPDB resource in applications. 1 Introduction Paraphrases are alternative ways to convey the same information and can improve natural language processing by making systems more robust to language variability and unseen words. The paraphrase database (PPDB) (Ganitkevitch et al., 2013) contains millions of automatically acquired paraphrases in 21 languages associated with features that serve to their ranking. In PPDB’s most recent release (2.0), such features include natural logic entailment relations, distributional and word embedding similarities, formality and complexity scores, and scores assigned by a supervised ranking model (Pavlick et al., 2015b). These features serve to identify good quality paraphrases but do not say much about their substitutability in context. To judge the adequacy of paraphrases for specific instances of words or phrases, the surrounding contex"
D16-1215,P10-2013,0,0.0286954,"cterising the specific meaning of a target word in its sentential context. When used for paraphrase ranking, these models derive a contextualised vector for a target word by reweighting the components of its basic meaning vector on the basis of the context of occurrence.3 Paraphrase candidates for a target word are then ranked according to the cosine similarity of their basic vector representation to the contextualised vector of the target.4 3 Experimental Set-up Data In our experiments, we use the C O I N C O corpus (Kremer et al., 2014), a subset of the “Manually Annotated Sub-Corpus” MASC (Ide et al., 2010) which comprises more than 15K word in2 The latent senses are induced using non-negative matrix factorization (NMF) (Lee and Seung, 2001) and latent Dirichlet allocation (LDA) (Blei et al., 2003). 3 Depending on the model, the vector combination function might be addition or multiplication of vector elements. 4 Thater et al.’s (2011) models delivered best results in paraphrase ranking on the CoInCo corpus (Kremer et al., 2014) and the S EM E VAL -2007 Lexical Substitution dataset (McCarthy and Navigli, 2007). PPDB S M L XL XXL # Instances 2146 3716 6228 13344 14507 |P |&gt; 1 # Lemmas 560 855 139"
D16-1215,E14-1057,0,0.250128,"Missing"
D16-1215,W15-3048,1,0.850059,"plication and on multiple languages is offered by MT evaluation. The M ETEOR - NEXT metric (Denkowski and Lavie, 2010) provides a straightforward framework for testing as it already exploits PPDB paraphrases for capturing sense correspondences between text fragments. In its current version, the metric views paraphrases as equivalent classes which can lead to erroneous sense mappings due to semantic distinctions present in the paraphrase sets. We have recently showed that the context-based filtering of semantic variants improves METEOR’s correlation with human judgments of translation quality (Marie and Apidianaki, 2015). We believe that a contextbased paraphrase ranking mechanism will enhance correct substitutions and further improve the metric. Last but not least, the paraphrase vectors can be used for mapping the contents of the PPDB resource to other multilingual resources for which vector representations are available (Camacho-Collados et al., 2015a; Camacho-Collados et al., 2015b). The interest of mapping paraphrases in the vector space to concepts found in existing semantic resources is twofold: it would permit to analyse the semantics of the paraphrases by putting them into correspondence with explici"
D16-1215,S07-1009,0,0.147813,"C O I N C O corpus (Kremer et al., 2014), a subset of the “Manually Annotated Sub-Corpus” MASC (Ide et al., 2010) which comprises more than 15K word in2 The latent senses are induced using non-negative matrix factorization (NMF) (Lee and Seung, 2001) and latent Dirichlet allocation (LDA) (Blei et al., 2003). 3 Depending on the model, the vector combination function might be addition or multiplication of vector elements. 4 Thater et al.’s (2011) models delivered best results in paraphrase ranking on the CoInCo corpus (Kremer et al., 2014) and the S EM E VAL -2007 Lexical Substitution dataset (McCarthy and Navigli, 2007). PPDB S M L XL XXL # Instances 2146 3716 6228 13344 14507 |P |&gt; 1 # Lemmas 560 855 1394 2822 3308 Avg |P| 2.67 2.92 3.57 10.33 185.09 |P |≥ 1 # Instances 5573 7771 10100 14060 14593 Table 1: Number of C O I N C O instances and distinct lemmas covered by each PPDB package. stances manually annotated with single and multiword substitutes. The manual annotations serve to evaluate the performance of the vector-space models on the task of ranking PPDB paraphrases. For each annotated English target word (noun, verb, adjective or adverb) in C O I N C O, we collect the lexical paraphrases (P = {p1 ,"
D16-1215,P08-1028,0,0.358036,"ude natural logic entailment relations, distributional and word embedding similarities, formality and complexity scores, and scores assigned by a supervised ranking model (Pavlick et al., 2015b). These features serve to identify good quality paraphrases but do not say much about their substitutability in context. To judge the adequacy of paraphrases for specific instances of words or phrases, the surrounding context needs to be considered. This can be done using vector-space models of semantics which calculate the meaning of word occurrences in context based on distributional representations (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). These models capture the influence of the context on the meaning of a target word through vector composition. More precisely, they represent the contextualised meaning of a target word w in context c by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition (Thater et al., 2011). We use this kind of representations to rank the PPDB paraphrases in context and retain the ones that preserve the semantics of specific text fragments. We evaluate the vector-based"
D16-1215,W11-1610,0,0.0646301,"Missing"
D16-1215,J07-2002,0,0.066497,"Missing"
D16-1215,P14-1009,0,0.0612078,"Missing"
D16-1215,N15-1023,0,0.0146041,"ent contexts a non-trivial task. In case of polysemous words, paraphrases describe different meanings and can lead to erroneous semantic mappings if substituted in texts (Apidianaki et al., 2014; Cocos and CallisonBurch, 2016). Even when paraphrases capture the same general sense, they are hardly ever equivalent synonyms and generally display subtle differences in meaning, connotation or usage (Edmonds and Hirst, 2002). Stylistic variation might also be present within paraphrase sets and substituting paraphrases that differ in terms of complexity and formality can result in a change in style (Pavlick and Nenkova, 2015). To increase paraphrase applicability in context, Pavlick et al. (2015a) propose to extract domain-specific pivot paraphrases by biasing the parallel training data used by the pivot method towards a specific domain. This customised model greatly improves paraphrase quality for the target domain but does not allow to rank and filter the paraphrases already in the PPDB according to specific contexts. To our knowledge, this is the first work that addresses the question of in-context substitutability of PPDB paraphrases. We show how existing substitutability models can be applied to this task in"
D16-1215,P15-2010,0,0.0230459,"Missing"
D16-1215,P15-2070,0,0.223956,"Missing"
D16-1215,N10-1013,0,0.109787,"tor composition meth2029 ods build representations that go beyond individual words to obtain word meanings in context. Some models use explicit sense representations while others modify the basic meaning vector of a target word with information from the vectors of the words in its context. In the framework proposed by Dinu and Lapata (2010), for example, word meaning is represented as a probability distribution over a set of latent senses reflecting the out-of-context likelihood of each sense, and the contextualised meaning of a word is modeled as a change in the original sense distribution.2 Reisinger and Mooney (2010) propose a multi-prototype vector-space model of meaning which produces multiple “sense-specific” vectors for each word, determined by clustering the contexts in which the word appears (Sch¨utze, 1998). The cluster centroids serve as prototype vectors describing a word’s senses and the meaning of a specific occurrence is determined by choosing the vector that minimizes the distance to the vector representing the current context. On the contrary, Thater et al. (2011) use no explicit sense representation. Their models allow the computation of vector representations for individual uses of words,"
D16-1215,S14-2039,0,0.0266729,"ic text fragments. We evaluate the vector-based ranking models on data hand-annotated with lexical variants and compare the obtained ranking to confidence estimates available in the PPDB, highlighting the importance of context filtering for paraphrase selection. 2 Context-based paraphrase ranking 2.1 Paraphrase substitutability The PPDB1 provides millions of lexical, phrasal and syntactic paraphrases in 21 languages – acquired by applying bi- and multi-lingual pivoting on parallel corpora (Bannard and Callison-Burch, 2005) – and is largely exploited in applications (Denkowski and Lavie, 2010; Sultan et al., 2014; Faruqui et al., 1 http://paraphrase.org/#/download 2028 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2028–2034, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2015). PPDB paraphrases come into packages of different sizes (going from S to XXXL): smaller packages contain high-precision paraphrases while larger ones aim for high coverage. Until now, pivot paraphrases have been used as equivalence sets (i.e. all paraphrases available for a word are viewed as semantically equivalent) and their substitutability i"
D16-1215,I11-1127,0,0.236261,"Missing"
D16-1215,P05-1074,0,\N,Missing
D17-1152,N06-1003,1,0.701386,"there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supe"
D17-1152,P11-2071,0,0.053725,"Missing"
D17-1152,P14-1079,0,0.0228582,"r low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix comple"
D17-1152,E14-1049,0,0.0756012,"nsla1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The mapping relies on seed translations which can be from existing dictiona"
D17-1152,P04-1064,0,0.0228215,"languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matri"
D17-1152,P16-1014,0,0.0134515,"coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with diff"
D17-1152,N13-1056,1,0.819043,"experiments on both low and high resource languages show the effectiveness of our model, outperforming the current stateof-the-art. • We make our code, datasets, and output translations publicly available.1 2 Related Work Bilingual Lexicon Induction Previous research has used different sources for estimating transla1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Simila"
D17-1152,J17-2001,1,0.854232,"y reported accuracies (Irvine and CallisonBurch, 2017) on test sets constructed from the same crowdsourced dictionaries (Pavlick et al., 2014)8 . The accuracies across languages appear to improve consistently with the amount of signals being input to the model. In the following experiments, we investigate how sensitive these improvements are with varying training size. In Figure 6, we show accuracies obtained by 7 Actual improvement per language depends on the coverage of the Wikipedia interlanguage links for that language 8 The comparison however, cannot be made apples-toapples since the way Irvine and Callison-Burch (2017) select test sets from the crowdsourced dictionaries maybe different and they do not release the test sets BPR NN kesadaran consciousness empathy awareness perceptions perception BPR WE kesadaran conscience awareness understanding consciousness acquaintance BPR WE with varying sizes of seed translation lexicons used to train its mapping. The results show that a seed lexicon size of 5K is enough across languages to achieve optimum performance. This finding is consistent with the finding of Vuli´c and Korhonen (2016) that accuracies peak at about 5K seed translations across all their models and"
D17-1152,D15-1015,0,0.252663,"Missing"
D17-1152,E12-1014,1,0.929334,"ata to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF"
D17-1152,P06-1103,0,0.411889,"an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF"
D17-1152,C12-1089,0,0.34901,"ata to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF"
D17-1152,N03-1017,0,0.052479,"f which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages. house EN koki Abstract Figure 1: Our framework allows us to use a diverse range of signals to learn translations, including incomplete bilingual dictionaries, information from related languages (like Indonesian loan words from Dutch shown here), word embeddings, and even visual similarity cues. Introduction Machine translation (MT) models typically require large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to"
D17-1152,N15-1028,0,0.0613798,"Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The mapping relies on seed translations which can be from existing dictionaries or be reliably chosen from pseudo-bilingual corpora of comparable texts e.g.,"
D17-1152,W15-1521,0,0.0222246,"onolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The mapping relies on seed translations which can be from existing dictionaries or be reliably chosen from pseudo-bilingual corpora of compar"
D17-1152,P16-1009,0,0.0345859,"olingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages. house EN koki Abstract Figure 1: Our framework allows us to use a diverse range of signals to learn translations, including incomplete bilingual dictionaries, information from related languages (like Indonesian loan words from Dutch shown here), word embeddings, and even visual similarity cues. Introduction Machine translation (MT) models typically require large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this p"
D17-1152,P16-1162,0,0.0205078,"olingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages. house EN koki Abstract Figure 1: Our framework allows us to use a diverse range of signals to learn translations, including incomplete bilingual dictionaries, information from related languages (like Indonesian loan words from Dutch shown here), word embeddings, and even visual similarity cues. Introduction Machine translation (MT) models typically require large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this p"
D17-1152,P15-2093,0,0.0438987,"ng space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matrix using MF with a Bayesian Personalized Ranking (BPR) objective (Rendle et al."
D17-1152,P95-1050,0,0.544339,"uire large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating tr"
D17-1152,P16-1157,0,0.139163,"and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Method"
D17-1152,N16-1103,0,0.062977,"been shown to outperform traditional supervised methods in the presence of positive-only data (Riedel et al., 2013), which is true in our case since we only observe positive translations. (2) BPR is easily extendable to incorporate additional signals for inferring missing values in the matrix (He and McAuley, 2016). Since observed translations may be sparse, i.e. the “cold start” problem in the matrix completion task, incorporating additional signals of translation equivalence estimated on monolingual corpora is useful. (3) BPR is also shown to be effective for multilingual transfer learning (Verga et al., 2016). For low resource source languages, there may be related, higher resource languages from which we can project available translations (e.g., translations of loan words) to the target language (Figure 1). We conduct large scale experiments to learn translations from both low and high resource languages to English and achieve state-of-the-art performance on these languages. Our main contributions are as follows: • We introduce a MF framework that learns translations by integrating diverse bilingual and monolingual signals of translation, each potentially noisy/incomplete. • The framework is easi"
D17-1152,N13-1008,0,0.0217984,"2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matrix using MF with a Bayesian Personalized Ranking (BPR) objective (Rendle et al., 2009). We select BPR for a number of reasons: (1) BPR has been shown to outperform traditional supervised methods in the presence of positive-only data (Riedel et al., 2013), which is true in our case since we only observe positive translations. (2) BPR is easily extendable to incorporate additional signals for inferring missing values in the matrix (He and McAuley, 2016). Since observed translations may be sparse, i.e. the “cold start” problem in the matrix completion task, incorporating additional signals of translation equivalence estimated on monolingual corpora is useful. (3) BPR is also shown to be effective for multilingual transfer learning (Verga et al., 2016). For low resource source languages, there may be related, higher resource languages from which"
D17-1152,N15-1118,0,0.0449847,"Missing"
D17-1152,W02-2026,0,0.639555,"l or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We u"
D17-1152,P16-2031,0,0.0888149,"Missing"
D17-1152,P16-1024,0,0.138033,"Missing"
D17-1152,D13-1168,0,0.0615986,"Missing"
D17-1152,P15-2118,0,0.110968,"Missing"
D17-1152,P13-1084,0,0.0298198,"ximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matrix using MF with a Bayesi"
D17-1152,D13-1141,0,0.050486,"n Previous research has used different sources for estimating transla1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The"
D17-2007,N13-1092,1,0.779387,"Missing"
D17-2007,Q14-1035,0,0.0284059,"ed by KnowYourNyms? players (with frequency counts). standing applications and may provide useful insights for psycholinguistics research. Go to www.know-your-nyms.com to play KnowYourNyms?. 2 Related Work Several games with a purpose (GWAPs) have been developed for gathering linguistic annotations for building resources and training systems (Chamberlain et al., 2013). Lafourcade (2007) and Fort et al. (2014) developed games for defining semantic relations and dependency relations in French. Chamberlain et al. (2008) created Phrase Detectives to annotate and validate things like co-reference. Jurgens and Navigli (2014) recently proposed using video games to link Word37 Proceedings of the 2017 EMNLP System Demonstrations, pages 37–42 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics in their appropriate distribution. Once completed, another round begins. The rounds are short (5-20 seconds, depending on the relation type), which makes the game fun and easy to play in short periods of time. 4 System Implementation 4.1 The web application was built with the Django framework, using Python for all backend and database interaction and standard JavaScript, HTML, and CSS for"
D17-2007,S15-1021,0,0.0144274,"he scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations to play. All are selected by default. System Overview KnowYourNyms? is modeled after GWAPs like th"
D17-2007,L16-1722,0,0.0134093,"his example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations to play. All are selected by default. System Overview KnowYour"
D17-2007,W15-4208,0,0.0223658,"three main views. Figure 1: This example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations to play. All are selected by defa"
D17-2007,W16-5310,0,0.0570328,"erver. The application has multiple components that are important to the user experience, which are separated into three main views. Figure 1: This example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, anto"
D17-2007,W11-2501,0,0.0197132,"xperience, which are separated into three main views. Figure 1: This example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations"
D17-2007,J17-4004,0,\N,Missing
D18-1202,J84-3009,0,0.153249,"Missing"
D18-1202,P18-1128,0,0.0192846,"* {quick} < {fast} < {speedy}* †† : p ≤ .01 †: p ≤ .05 Table 2: Pairwise relation prediction and global ranking results for each score type in isolation, and for the bestscoring combinations of 2 and 3 score types on each dataset. For the global ranking accuracy and average τb results, we denote with the † symbol scores for metrics incorporating paraphrase-based evidence that significantly out-perform both scorepat and scoresocal under the paired Student’s t-test, using the Anderson-Darling test to confirm that scores conform to a normal distribution (Fisher, 1935; Anderson and Darling, 1954; Dror et al., 2018). Example output is also given, with correct rankings starred. or positive correlation, and a value of 0 indicating no correlation between predicted and gold rankings. We report τb as a weighted average over scales in each dataset, where weights correspond to the number of adjective pairs in each scale. Spearman’s rho (ρ). We report the Spearman’s ρ rank correlation coefficient between predicted (rP (J)) and gold-standard (rG (J)) ranking permutations. For each dataset, we calculate this metric just once by treating each adjective in a particular scale as a single data point, and calculating a"
D18-1202,N13-1092,1,0.86228,"Missing"
D18-1202,P93-1023,0,0.785283,"xtracting sets of same-attribute adjectives from WordNet ‘dumbbells’ – consisting of two direct antonyms at the poles and satellites of synonymous/related adjectives incident to each antonym (Gross and Miller, 1990) – and ordering them by intensity. The annotations, however, are not in WordNet as of its latest version (3.1). Work on adjective intensity generally focuses on two related tasks: clustering adjectives based on the attributes they modify, and ranking sameattribute adjectives by intensity. With respect to the former, common approaches involve clustering adjectives by their contexts (Hatzivassiloglou and McKeown, 1993; Shivade et al., 2015). We do not focus on the clustering task in this paper, but concentrate on the ranking task. Approaches to the task of ranking scalar adjectives by their intensity mostly fall under the paradigms of pattern-based or lexicon-based approaches. Pattern-based approaches work by extracting lexical (Sheinman and Tokunaga, 2009; de Melo and Bansal, 2013; Sheinman et al., 2013) or syntactic (Shivade et al., 2015) patterns indicative of an intensity relationship from large corpora. For example, the patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjec"
D18-1202,C92-2082,0,0.163772,"approaches rely particularly pleased quite limited rather odd so silly completely mad ↔ ↔ ↔ ↔ ↔ ecstatic restricted crazy dumb crazy Figure 1: Examples of paraphrases from PPDB of the form RB JJu ↔ JJv which can be used to infer pairwise intensity relationships (JJu < JJv ). on pattern-based or lexicon-based methods to predict the intensity ranking of adjectives. Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y. As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014). Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3). They can also be precise, but may not cover all adjectives of interest. We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives. A paraphrase is a pair of words or phrases wi"
D18-1202,D13-1169,0,0.230759,"Missing"
D18-1202,P10-1018,0,0.34145,"Missing"
D18-1202,Q13-1023,0,0.647048,"Missing"
D18-1202,P15-2070,1,0.904063,"Missing"
D18-1202,R15-1071,0,0.0652019,"adjectives ju and jv in JJG RAPH provides evidence about the relative intensity relationship between them. However, it has just been noted that JJG RAPH is noisy, containing both contradictory/cyclic edges and adverbs that are not uniformly intensifying. Rather than try to eliminate cycles, or manually annotate each adverb with a weight corresponding to its intensity and polarity 4 Other Intensity Evidence Our experiments compare the proposed paraphrase approach with existing pattern- and lexicon-based approaches. 4.1 Figure 3: A subgraph of JJG RAPH, depicting its directed graph structure. (Ruppenhofer et al., 2015; Taboada et al., 2011), we aim to learn these weights automatically in the process of predicting pairwise intensity. Given adjective pair (ju , jv ), we build a classifier that outputs a score from 0 to 1 indicating the predicted likelihood that ju < jv . Its binary features correspond to adverb edges from ju to jv and from jv to ju in JJG RAPH. The feature space includes only adverbs from R that appear at least 10 times in JJG RAPH, resulting in features for m = 259 unique adverbs in each direction (i.e. from ju to jv and vice versa) for 2m = 518 binary features total. Note that while all ad"
D18-1202,E14-4023,0,0.0822145,"paraphrases from PPDB of the form RB JJu ↔ JJv which can be used to infer pairwise intensity relationships (JJu < JJv ). on pattern-based or lexicon-based methods to predict the intensity ranking of adjectives. Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y. As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014). Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3). They can also be precise, but may not cover all adjectives of interest. We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives. A paraphrase is a pair of words or phrases with approximately similar meaning, such as really great ↔ phenomenal. Adjectival paraphrases can be exploited to uncover intensity relationships. A paraphrase pair of t"
D18-1202,D15-1300,0,0.0141514,"patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjective less intense than Y. Lexicon-based approaches are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993). These methods draw upon a 1 www.paraphrase.org lexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity. The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually. In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011). Our paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers. A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant"
D18-1202,N15-1051,0,0.413677,"Missing"
D18-1202,J11-2001,0,0.101837,"hes are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993). These methods draw upon a 1 www.paraphrase.org lexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity. The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually. In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011). Our paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers. A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant than are moderate adverbs like very. Unlike Collex, which requires predetermined sets of ‘end-of-scale’ and ‘normal’ adverbial modi"
D18-1202,L16-1424,0,0.507498,"ng a full scale (e.g. freezing to sweltering), or a half scale (warm to sweltering); all three test sets group adjectives into half scales. The three datasets are described here, and their characteristics are given in Table 1. deMelo (de Melo and Bansal, 2013)4 . 87 adjective 4 http://demelo.org/gdm/intensity/ sets are extracted from WordNet ‘dumbbell’ structures (Gross and Miller, 1990), and partitioned into half-scale sets based on their pattern-based evidence in the Google N-Grams corpus (Brants and Franz, 2009). Sets are manually annotated for intensity relations (<, >, and =). Wilkinson (Wilkinson and Oates, 2016). Twelve adjective sets are generated by presenting crowd workers with small seed sets (e.g. huge, small, microscopic), and eliciting similar adjectives. Sets are automatically cleaned for consistency, and then annotated for intensity by crowd workers. While the original dataset contains full scales, we manually sub-divide these into 21 half-scales for use in this study. Details on the modification from full- to half-scales are in the Supplemental Material. Crowd. We also crowdsourced a new set of adjective scales with high coverage of the PPDB vocabulary. In a three-step process, we first ask"
D18-2021,P18-1239,1,0.882792,"Missing"
D18-2021,W14-1618,0,0.0378344,"eried and will be positioned in the vector space close to other OOV words based on their string similarity: from pymagnitude import ∗ vectors = Magnitude (&quot;w2v. magnitude &quot;) k = vectors . query (&quot;king&quot;) q = vectors . query (&quot; queen &quot;) vectors . similarity (k,q) # 0.6510958 Magnitude queries return almost instantly and are memory efficient. It uses lazy loading directly from disk, instead of having to load the entire model into memory. Additionally, Magnitude supports nearest neighbors operations, finding all words that are closer to a key than another key, and analogy solving (optionally with Levy and Goldberg (2014)’s 3CosMul function): &quot;uber&quot; in vectors # True &quot; uberx &quot; in vectors # False &quot; uberxl &quot; in vectors # False vectors . query (&quot; uberx &quot;) # Returns : [ 0.0507 , − 0.0708, ...] vectors . query (&quot; uberxl &quot;) # Returns : [ 0.0473 , − 0.08237 , ...] vectors . similarity (&quot; uberx &quot;, &quot; uberxl &quot;) # Returns : 0.955 vectors . most similar (k, topn =5) #[(‘ king ’, 1.0) , (‘ kings ’, 0.71) , # (‘ queen ’, 0.65) , (‘ monarch ’, 0.64) , # (‘ crown prince ’, 0.62)] vectors . most similar (q, topn =5) #[(‘ queen ’, 1.0) , (‘ queens ’, 0.74) , #(‘ princess ’, 0.71) , (‘king ’, 0.65) , # (’ monarch ’, 0.64)] vecto"
D18-2021,D14-1162,0,0.101877,"ntation8 for more information about conversion configuration options. Other matching nuances We employ other techniques when computing the string similarity metric, such as shrinking repeated character sequences of three or more to two (hiiiiiiii → hii), ranking strings of a similar length higher, and ranking strings that share the same first or last character higher for shorter words. 6 File Format To provide efficiency at runtime, Magnitude uses a custom “.magnitude” file format instead of “.bin”, “.txt”, “.vec”, or “.hdf5” that word2vec, GloVe, fastText, and ELMo use (Mikolov et al., 2013; Pennington et al., 2014; Joulin et al., 2016; Peters et al., 2018). The “.magnitude” file is a SQLite database file. There are 3 variants of the file format: Light, Medium, Heavy. Heavy models have the largest file size but support all of the Magnitude library’s features. Medium models support all features except approximate similarity search. Light models do not support approximate similarity searches or interpolated OOV lookups, but they still support basic OOV lookups. See Figure 1 for more information about the structure and layout of the “.magnitude” format. Quantization The converter utility accepts a -p <PREC"
D18-2021,N18-1202,0,0.0336746,"configuration options. Other matching nuances We employ other techniques when computing the string similarity metric, such as shrinking repeated character sequences of three or more to two (hiiiiiiii → hii), ranking strings of a similar length higher, and ranking strings that share the same first or last character higher for shorter words. 6 File Format To provide efficiency at runtime, Magnitude uses a custom “.magnitude” file format instead of “.bin”, “.txt”, “.vec”, or “.hdf5” that word2vec, GloVe, fastText, and ELMo use (Mikolov et al., 2013; Pennington et al., 2014; Joulin et al., 2016; Peters et al., 2018). The “.magnitude” file is a SQLite database file. There are 3 variants of the file format: Light, Medium, Heavy. Heavy models have the largest file size but support all of the Magnitude library’s features. Medium models support all features except approximate similarity search. Light models do not support approximate similarity searches or interpolated OOV lookups, but they still support basic OOV lookups. See Figure 1 for more information about the structure and layout of the “.magnitude” format. Quantization The converter utility accepts a -p <PRECISION&gt; flag to specify the decimal precisio"
D19-1618,W12-3102,0,0.045691,"Missing"
D19-1618,W06-0707,0,0.260888,"aries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evaluation on three publicly available summarization datasets from NIST shared Figure 1: S UM -QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories. tasks, and compare the performance of our model to a wide variety of baseline methods capturing different aspects of linguistic quality. S UM -QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form.1 2 Related Work Summarization evaluation metrics like Pyramid (Nenkova and Passonneau, 2004) and ROUGE (Lin and Hovy, 2003; Lin, 2004) are recalloriented; they basically measure"
D19-1618,W14-3348,0,0.0121202,"d but poorly with linguistic qualities of summaries. Louis and Nenkova (2013) proposed a regression model for measuring summary quality without references. The scores of their model correlate well with Pyramid and Responsiveness, but text quality is only addressed indirectly.2 Quality Estimation is well established in MT (Callison-Burch et al., 2012; Bojar et al., 2016, 2017; Martins et al., 2017; Specia et al., 2018). QE methods provide a quality indicator for translation output at run-time without relying on human references, typically needed by MT evaluation metrics (Papineni et al., 2002; Denkowski and Lavie, 2014). QE models for MT make use of large postedited datasets, and apply machine learning methods to predict post-editing effort scores and quality (good/bad) labels. We apply QE to summarization, focusing on linguistic qualities that reflect the readability and fluency of the generated texts. Since no postedited datasets – like the ones used in MT – are available for summarization, we use instead the ratings assigned by human annotators with respect to a set of linguistic quality criteria. Our proposed models achieve high correlation with human judgments, showing that it is possible to estimate su"
D19-1618,N19-1423,0,0.0368882,"man references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evaluation on three publicly available summarization datasets from NIST shared Figure 1: S UM -QE rates summaries with respect to five linguistic qualities (Dang, 2006a). The datasets we use for tuning and evaluation contain human assigned scores (from 1 to 5) for each of these categories. tasks, and compare the performance of our model to a"
D19-1618,W04-1013,0,0.632253,"tic aspects. Predictions of the S UM -QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text. 1 Introduction Quality Estimation (QE) is a term used in machine translation (MT) to refer to methods that measure the quality of automatically translated text without relying on human references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are give"
D19-1618,N03-1020,0,0.485946,"Missing"
D19-1618,J13-2002,0,0.24798,"ence on Natural Language Processing, pages 6005–6011, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2019). ROUGE is the most commonly used evaluation metric (Nenkova and McKeown, 2012; Allahyari et al., 2017; Gambhir and Gupta, 2017). Inspired by BLEU (Papineni et al., 2002), it relies on common n-grams or subsequences between peer and model summaries. Many ROUGE versions are available, but it remains hard to decide which one to use (Graham, 2015). Being recall-based, ROUGE correlates well with Pyramid but poorly with linguistic qualities of summaries. Louis and Nenkova (2013) proposed a regression model for measuring summary quality without references. The scores of their model correlate well with Pyramid and Responsiveness, but text quality is only addressed indirectly.2 Quality Estimation is well established in MT (Callison-Burch et al., 2012; Bojar et al., 2016, 2017; Martins et al., 2017; Specia et al., 2018). QE methods provide a quality indicator for translation output at run-time without relying on human references, typically needed by MT evaluation metrics (Papineni et al., 2002; Denkowski and Lavie, 2014). QE models for MT make use of large postedited dat"
D19-1618,N04-1019,0,0.875899,"used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text. 1 Introduction Quality Estimation (QE) is a term used in machine translation (MT) to refer to methods that measure the quality of automatically translated text without relying on human references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evaluation on three publicly availabl"
D19-1618,P02-1040,0,0.104385,"S UM -QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text. 1 Introduction Quality Estimation (QE) is a term used in machine translation (MT) to refer to methods that measure the quality of automatically translated text without relying on human references (Bojar et al., 2016, 2017). In this study, we address QE for summarization. Our proposed model, S UM -QE, successfully predicts linguistic qualities of summaries that traditional evaluation metrics fail to capture (Lin, 2004; Lin and Hovy, 2003; Papineni et al., 2002; Nenkova and Passonneau, 2004). S UM -QE predictions can be used for system development, to inform users of the quality of automatically produced summaries and other types of generated text, and to select the best among summaries output by multiple systems. S UM -QE relies on the BERT language representation model (Devlin et al., 2019). We use a pre-trained BERT model adding just a taskspecific layer, and fine-tune the entire model on the task of predicting linguistic quality scores manually assigned to summaries. The five criteria addressed are given in Figure 1. We provide a thorough evalua"
D19-1618,P13-2026,0,0.0194252,"different aspects of linguistic quality. S UM -QE achieves very high correlations with human ratings, showing the ability of BERT to model linguistic qualities that relate to both text content and form.1 2 Related Work Summarization evaluation metrics like Pyramid (Nenkova and Passonneau, 2004) and ROUGE (Lin and Hovy, 2003; Lin, 2004) are recalloriented; they basically measure the content from a model (reference) summary that is preserved in peer (system generated) summaries. Pyramid requires substantial human effort, even in its more recent versions that involve the use of word embeddings (Passonneau et al., 2013) and a lightweight crowdsourcing scheme (Shapira et al., 1 Our code is available at https://github.com/ nlpaueb/SumQE 6005 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6005–6011, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2019). ROUGE is the most commonly used evaluation metric (Nenkova and McKeown, 2012; Allahyari et al., 2017; Gambhir and Gupta, 2017). Inspired by BLEU (Papineni et al., 2002), it relies on common n-grams or sub"
D19-1618,N19-1072,0,0.0876233,"Missing"
E09-1010,E06-1032,0,\N,Missing
E09-1010,W04-0802,0,\N,Missing
E09-1010,apidianaki-2008-translation,1,\N,Missing
E09-1010,S07-1004,0,\N,Missing
E09-1010,W04-2213,0,\N,Missing
E09-1010,W07-0714,0,\N,Missing
E09-1010,P02-1040,0,\N,Missing
E09-1010,H05-1097,0,\N,Missing
E09-1010,W05-0909,0,\N,Missing
E09-1010,P91-1034,0,\N,Missing
E09-1010,W07-0734,0,\N,Missing
E09-1010,P05-1048,0,\N,Missing
E09-1010,D07-1007,0,\N,Missing
E09-1010,P07-1005,0,\N,Missing
E09-1010,J08-4004,0,\N,Missing
E09-1010,2005.mtsummit-papers.11,0,\N,Missing
F14-1005,D07-1007,0,0.0574797,"Missing"
F14-1005,P11-1061,0,0.135028,"Missing"
F14-1005,W09-1201,0,0.0598514,"Missing"
F14-1005,W08-2122,0,0.0476428,"Missing"
F14-1005,2005.mtsummit-papers.11,0,0.0814041,"Missing"
F14-1005,P13-1117,0,0.0330466,"Missing"
F14-1005,J93-2004,0,0.0487423,"Missing"
F14-1005,P03-1058,0,0.0649263,"Missing"
F14-1005,J03-1002,0,0.00610544,"Missing"
F14-1005,J05-1004,0,0.260464,"Missing"
F14-1005,Q13-1001,0,0.106742,"Missing"
F14-1005,W10-1814,1,0.907122,"Missing"
F14-1005,N01-1026,0,0.334581,"Missing"
F14-1005,E09-1010,1,\N,Missing
F14-1005,P11-2052,1,\N,Missing
F14-1005,W12-4201,1,\N,Missing
gabor-etal-2012-boosting,vilnat-etal-2010-passage,1,\N,Missing
gabor-etal-2012-boosting,S07-1002,0,\N,Missing
gabor-etal-2012-boosting,C92-2082,0,\N,Missing
gabor-etal-2012-boosting,C08-1084,0,\N,Missing
gabor-etal-2012-boosting,J02-3004,0,\N,Missing
gabor-etal-2012-boosting,2010.jeptalnrecital-court.19,0,\N,Missing
J16-2003,apidianaki-2008-translation,1,0.788727,"different, but somewhat related. We want to know to what extent measures of clusterability of instances can predict the partitionability of a lemma. As our focus in this article is to test the predictive power of clusterability measures in the best possible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built f"
J16-2003,E09-1010,1,0.794351,"d for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefever and Hoste 2010). McCarthy (2011) shows that overlap of translations compared to overlap of paraphrases on sentence pairs for a given lemma are correlated with interannotator agreement of graded lemma usage similarity judgments (Erk, McCarthy, and Gaylord 2009) but does not attempt to cluster the translation or paraphrase data or examine the findings in terms of clusterability. In this initial study of the clusteribility phenomenon, we represent instances through translation and paraphrase annotations; in the future, we will move to automatic"
J16-2003,apidianaki-etal-2014-semantic,1,0.867919,"Missing"
J16-2003,D09-1056,0,0.0820066,"Missing"
J16-2003,P05-1074,0,0.0289879,"ses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefever and Hoste 2010). McCarthy (2011) shows that overlap of translations compared to overlap of paraphrases on sentence pairs for a given lemma are correlated with interannotator agreement of graded lemma usage similarity judgments (Erk, McCarthy, and Gaylord 2009) but does not attempt to cluster the translation or paraphrase data or examine the findings in terms of clusterability. In this initial study of the clusteribility phenomenon, we represent instances through translation and paraphrase annotations; in the future, we will"
J16-2003,N12-1095,0,0.0584039,"Missing"
J16-2003,D07-1007,0,0.030657,"Missing"
J16-2003,J13-3008,0,0.0609858,"Missing"
J16-2003,eom-etal-2012-using,0,0.0612982,"Missing"
J16-2003,P09-1002,1,0.923919,"Missing"
J16-2003,J13-3003,1,0.892716,"Missing"
J16-2003,N06-2015,0,0.0425995,"Missing"
J16-2003,W02-0808,0,0.182124,"Missing"
J16-2003,S13-2049,0,0.0416348,"Missing"
J16-2003,W09-2413,0,0.0378648,"Missing"
J16-2003,S10-1011,0,0.0219161,"pted for publication: 25 January 2016. doi:10.1162/COLI a 00247 © 2016 Association for Computational Linguistics Computational Linguistics Volume 42, Number 2 1. Introduction In computational linguistics, the field of word sense disambiguation (WSD)—where a computer selects the appropriate sense from an inventory for a word in a given context—has received considerable attention.1 Initially, most work focused on manually constructed inventories such as WordNet (Fellbaum 1998) but there has subsequently been a great deal of work on the related field of word sense induction (WSI) (Pedersen 2006; Manandhar et al. 2010; Jurgens and Klapaftis 2013) prior to disambiguation. This article concerns the phenomenon of word meaning and current practice in the fields of WSD and WSI . Computational approaches to determining word meaning in context have traditionally relied on a fixed sense inventory produced by humans or by a WSI system that groups token instances into hard clusters. Either sense inventory can then be applied to tag sentences on the premise that there will be one best-fitting sense for each token instance. However, word meanings do not always take the form of discrete senses but vary on a continuum b"
J16-2003,S07-1009,1,0.800354,"Missing"
J16-2003,S10-1002,1,0.886946,"Missing"
J16-2003,P08-1028,0,0.0875116,"Missing"
J16-2003,palmer-etal-2000-semantic,0,0.22024,"Missing"
J16-2003,passonneau-etal-2010-word,0,0.168918,"s (Tuggy 1993). For example, the noun crane is a clear-cut case of ambiguity between lifting device and bird, whereas the exact meaning of the noun thing can only be retrieved via the context of use rather than via a representation in the mental lexicon of speakers. Cases of polysemy such as the verb paint, which can mean painting a picture, decorating a room, or painting a mural on a house, lie somewhere between these two poles. Tuggy highlights the fact that boundaries between these different categories are blurred. Although specific context clearly plays a role (Copestake and Briscoe 1995; Passonneau et al. 2010) some lemmas are inherently much harder to partition than others (Kilgarriff 1998; Cruse 2000). There are recent attempts to address some of these issues by using alternative characterizations of word meaning that do not involve creating a partition of usages into senses (McCarthy and Navigli 2009; Erk, McCarthy, and Gaylord 2013), and by asking WSI systems to produce soft or graded clusterings (Jurgens and Klapaftis 2013) where tokens can belong to a mixture of the clusters. However, these approaches do not overtly consider the location of a lemma on the continuum, but doing so should help in"
J16-2003,W97-0213,0,0.220678,"ssible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent works focus on discovering the relationships between the translations and grouping them into clusters either automatically (Bannard and Callison-Burch 2005; Apidianaki 2009; Bansal, DeNero, and Lin 2012) or manually (Lefeve"
J16-2003,D07-1043,0,0.159065,"Missing"
J16-2003,J06-2001,0,0.0881139,"Missing"
J16-2003,D09-1067,0,0.028907,"Missing"
J16-2003,S07-1044,0,0.0306505,"now to what extent measures of clusterability of instances can predict the partitionability of a lemma. As our focus in this article is to test the predictive power of clusterability measures in the best possible case, we want the representations of the instances that we cluster to be as informative and “clean” as possible. For this reason, we represent instances through manually annotated translations (Mihalcea, Sinha, and McCarthy 2010) and paraphrases (McCarthy and Navigli 2007). Both translations (Resnik and Yarowsky 2000; Carpuat and Wu 2007; Apidianaki 2008) and monolingual paraphrases (Yuret 2007; Biemann and Nygaard 2010; Apidianaki, Verzeni, and McCarthy 2014) have previously been used as a way of inducing word senses, so they should be well suited for the task. Since the suggestion by Resnik and Yarowsky (1997) to limit WSD to senses lexicalized in other languages, numerous works have exploited translations for semantic analysis. Dyvik (1998) discovers word senses and their relationships through translations in a parallel corpus and Ide, Erjavec, and Tufis¸ (2002) group the occurrences of words into senses by using translation vectors built from a multilingual corpus. More recent w"
J16-2003,W09-2419,0,\N,Missing
J16-2003,passonneau-etal-2012-masc,0,\N,Missing
L16-1179,piperidis-2012-meta,0,0.0231461,"useum reviews in French. All but the last dataset were released for Subtask 1 (Sentence-level ABSA) and part of the data was annotated at the text level for Subtask 2 (Textlevel ABSA). The French Museum reviews dataset was released for Subtask 3 “Out-of-domain ABSA” where only test and no training data was provided.1 In what follows, we describe the data collection procedure and the annotation guidelines that were developed for the two domains addressed in French. The data and the annotation guidelines are publicly available under a noncommercial, no redistribution license2 through METASHARE (Piperidis, 2012),3 a repository devoted to the sharing and dissemination of language resources, and on the SemEval-2016 ABSA task website.4 2. 2.1. Datasets and Annotation Data Collection French datasets were developed for two of the SemEval2016 ABSA subtasks. For in-domain sentence-level ABSA (Subtask 1) the dataset comprises annotated restaurant reviews while for out-of-domain ABSA (Subtask 3) annotated museum reviews were released. For the first subtask, the restaurant domain, both training and test data was provided. In Subtask 3, the participating teams had the opportunity to test their systems in a prev"
L16-1179,S14-2004,0,0.109863,"Missing"
L16-1179,S15-2082,0,0.162236,"provided. The second dataset is a smaller set of museum reviews for which no training data is available, dedicated to out-of-domain ABSA evaluation. The ABSA task was first introduced for English in the SemEval 2014 evaluation campaign (Pontiki et al., 2014), where restaurant and laptop reviews annotated with aspect terms, categories and their polarity were provided for training and testing ABSA systems. The task was repeated in SemEval 2015 with a different, more unified, framework where aspect categories were defined as combinations of an entity type, an attribute type and a polarity value (Pontiki et al., 2015): (1) The fajitas were delicious, but expensive. {FOOD#QUALITY, TARGET: fajitas}→POSITIVE {FOOD#PRICES, TARGET: fajitas}→NEGATIVE (2) Great for a romantic evening. {AMBIENCE#GENERAL, TARGET: NULL }→ POSITIVE An out-of-domain subtask was also proposed where annotated hotel reviews were provided for testing but no training data was released. In 2016, the SemEval ABSA task became multilingual (Pontiki et al., 2016). New datasets were released for English allowing systems to be tested on the same domains as in previous years (laptops, restaurants and hotels), but datasets were also developed in ne"
L16-1179,S16-1002,1,0.901634,"Missing"
N16-3006,S15-2050,1,0.849792,"s could help (Moore, 2005). Additional functionalities in reading are also envisioned, such as an enhanced and non-distracting access to dictionary information for difficult words. Currently, Web Readers and mobile reading devices offer such functionality through a pop-up window presenting the complete dictionary entry. No assistance is however offered to access the right sense in context, which would be especially helpful for polysemous words or when language proficiency is low. In T RAN S R EAD , we propose to perform this selection automatically. Our word sense disambiguation (WSD) method (Apidianaki and Gong, 2015) exploits wordlevel alignments to annotate words on both sides of the bitext with the correct senses extracted from BabelNet (Navigli and Ponzetto, 2012). By integrating WSD information in the reader, we will be able to propose definitions, usage examples and Wikipedia entries, as well as synonymous words and semantically correct translations. Our WSD system embeds an alignment-based multi-word expression (MWE) identification mechanism (Marie and Apidianaki, 2015). Such information will serve as part 30 of a smart selection mechanism (Pantel et al., 2014), enabling the system to select appropr"
N16-3006,aziz-etal-2012-pet,0,0.0568317,"Missing"
N16-3006,P91-1022,0,0.324818,"Missing"
N16-3006,J93-2003,0,0.160503,"Missing"
N16-3006,P05-3025,0,0.0441529,"nd to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full translation context are much rarer. In DoppelText1 , DuoLir2 and Parallel Text Reader on iOS, the selec"
N16-3006,P08-2007,0,0.0230647,"omputing high quality word alignments for literary texts might be significantly more difficult than for other text genres. This also calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): bo"
N16-3006,P91-1023,0,0.735615,"Missing"
N16-3006,W08-0509,0,0.0892199,"Missing"
N16-3006,P08-4006,0,0.0225559,"es difficult challenges: it first requires to push existing MT technologies to the limit and to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full transl"
N16-3006,gilmanov-etal-2014-swift,0,0.0263098,"Missing"
N16-3006,C14-2026,0,0.0220746,"many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full translation context are much rarer. In DoppelText1 , DuoLir2 and Parallel Text Reader on iOS, the selection is performed at the sentence level, using alignments. Whatever level is used, this kind of switch-on-demand inter"
N16-3006,P09-1105,0,0.0233122,"in both directions. Alignments in the intersection were checked and corrected following the recommandations of Och and Ney (2003). Even for such simple texts, alignment errors were numerous, with an AER close to 0.17 (‘The Promise’), and to 0.19 (‘The Verger’). This confirms the intuition that computing high quality word alignments for literary texts might be significantly more difficult than for other text genres. This also calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignment"
N16-3006,P07-2045,0,0.00445779,"so calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): both techniques require to evaluate the parallelism of arbitrary chunks. We follow the latter here, also using punctuation marks"
N16-3006,2012.eamt-1.62,1,0.851957,"002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): both techniques require to evaluate the parallelism of arbitrary chunks. We follow the latter here, also using punctuation marks to select segmentation points. The resulting alignments are deliberately pretty coarse and primarily meant to be used in a contrastive condition for the human tests. 4 4.1 A Bilingual Reader Design The current version of the T RANS R EAD bilingual reader displays paginated versions of the bitext in parallel views. In Figure 1, the source text is displayed on the right side of the screen and its translation on the left. The user has selected a word in the source ver"
N16-3006,W02-1018,0,0.0654582,"s the intuition that computing high quality word alignments for literary texts might be significantly more difficult than for other text genres. This also calls for improved 7 ‘The verger’ and ‘The promise’, totalling slightly more that 160 sentences each. 28 techniques for computing confidence measures for word alignments (Huang, 2009): depending on the intended reading context, it might be better to avoid displaying erroneous alignment links. 3.3 Subsentential alignments The task of designing sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lar"
N16-3006,W15-3048,1,0.846898,"oficiency is low. In T RAN S R EAD , we propose to perform this selection automatically. Our word sense disambiguation (WSD) method (Apidianaki and Gong, 2015) exploits wordlevel alignments to annotate words on both sides of the bitext with the correct senses extracted from BabelNet (Navigli and Ponzetto, 2012). By integrating WSD information in the reader, we will be able to propose definitions, usage examples and Wikipedia entries, as well as synonymous words and semantically correct translations. Our WSD system embeds an alignment-based multi-word expression (MWE) identification mechanism (Marie and Apidianaki, 2015). Such information will serve as part 30 of a smart selection mechanism (Pantel et al., 2014), enabling the system to select appropriate spans and dictionary entries for MWEs found in texts. An experimental evaluation of the interface general design is currently being conducted. We study, notably, the effect of the depth of the alignment structure on human readers behavior. As short term future work, we shall also investigate other interaction techniques for focus management, such as distortion and 3D views for page turning (Cubaud, 2008). The graphic composition engine developed for the curre"
N16-3006,moore-2002-fast,0,0.20677,"Missing"
N16-3006,H05-1011,0,0.0500251,"e software in order to investigate a large design space of interaction for tablets. We have selected the Kivy framework for Python, which en8 As described in http://defoe.sourceforge. net/folio/knuth-plass.html 9 Provided by http://tug.org/tex-hyphen/ Figure 1: The T RANS R EAD bilingual reader application running on tablet ables cross-platform development for Android or iOS, and GPU-based graphics with OpenGL ES. 5 Perspectives As reflected in this paper, a top priority is to pursue our efforts towards high-precision alignments, an application where supervised learning techniques could help (Moore, 2005). Additional functionalities in reading are also envisioned, such as an enhanced and non-distracting access to dictionary information for difficult words. Currently, Web Readers and mobile reading devices offer such functionality through a pop-up window presenting the complete dictionary entry. No assistance is however offered to access the right sense in context, which would be especially helpful for polysemous words or when language proficiency is low. In T RAN S R EAD , we propose to perform this selection automatically. Our word sense disambiguation (WSD) method (Apidianaki and Gong, 2015)"
N16-3006,J03-1002,0,0.00851268,"Missing"
N16-3006,P14-1143,0,0.025515,"nse disambiguation (WSD) method (Apidianaki and Gong, 2015) exploits wordlevel alignments to annotate words on both sides of the bitext with the correct senses extracted from BabelNet (Navigli and Ponzetto, 2012). By integrating WSD information in the reader, we will be able to propose definitions, usage examples and Wikipedia entries, as well as synonymous words and semantically correct translations. Our WSD system embeds an alignment-based multi-word expression (MWE) identification mechanism (Marie and Apidianaki, 2015). Such information will serve as part 30 of a smart selection mechanism (Pantel et al., 2014), enabling the system to select appropriate spans and dictionary entries for MWEs found in texts. An experimental evaluation of the interface general design is currently being conducted. We study, notably, the effect of the depth of the alignment structure on human readers behavior. As short term future work, we shall also investigate other interaction techniques for focus management, such as distortion and 3D views for page turning (Cubaud, 2008). The graphic composition engine developed for the current application already allows such effects. A research agenda should also include long term e"
N16-3006,smith-jahr-2000-cairo,0,0.0372623,"age. Such endeavour poses difficult challenges: it first requires to push existing MT technologies to the limit and to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit"
N16-3006,P15-4021,0,0.0136487,"ires to push existing MT technologies to the limit and to revisit assumptions that are rarely questioned, such as the need to deliver fully aligned bitexts, including 27 many-to-many sentence links, and to output highprecision word and phrase alignments, even for rare words or gappy multi-word units. A second challenge is visualisation and interaction design. In fact, most existing interfaces for bilingual reading/writing have targeted specialists of the MT industry, serving purposes such as manual alignment input and visualisation (Smith and Jahry, 2000; Germann, 2008; Gilmanov et al., 2014; Steele and Specia, 2015), MT tracing and debugging (DeNeefe et al., 2005; Weese and CallisonBurch, 2010), MT quality assessment (Federmann, 2012; Chatzitheodorou, 2013; Girardi et al., 2014) or MT post-edition (Aziz et al., 2012). By contrast, our aim is not just to visualize the translation or bilingual correspondences, but rather to enable a smooth and seamless reading experience for the general public. Ebook reading applications typically allow the reader to select a word and to access the corresponding dictionary entry, but applications that exploit the full translation context are much rarer. In DoppelText1 , Du"
N16-3006,J97-3002,0,0.348397,"ning sound and tractable alignment models is notoriously much harder for groups of words than for words (Marcu and Wong, 2002; DeNero and Klein, 2008). Two main strategies have been explored in the literature: the most common, employed in most SMT systems (Koehn et al., 2007) starts with alignments for isolated words, which are incrementally grown subject to consistency constraints. The alternative way is to start with sentential alignments and adopt a divisive strategy, which yields progressive refinements of an initially holistic pairing; this can be performed exactly under ITG constraints (Wu, 1997); heuristic approaches, capable of handling alignments for arbitrarily long segments have also been proposed in (Lardilleux et al., 2012): both techniques require to evaluate the parallelism of arbitrary chunks. We follow the latter here, also using punctuation marks to select segmentation points. The resulting alignments are deliberately pretty coarse and primarily meant to be used in a contrastive condition for the human tests. 4 4.1 A Bilingual Reader Design The current version of the T RANS R EAD bilingual reader displays paginated versions of the bitext in parallel views. In Figure 1, the"
N16-3006,W12-2505,1,0.896833,"Missing"
N18-1019,P13-3021,0,0.0220536,"llel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformations involved in text simplification. Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Søgaard, 2013).1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking. To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their de"
N18-1019,C14-1188,0,0.0144012,"hine Translation system extended to support phrase deletion, and Wubben et al. (2012) augment a phrase-based system with a re-ranking heuristic. Woodsend and Lapata (2011) view simplification as a monolingual text generation task. They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input. In contrast to these methods, Narayan and Gardent (2016)’s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformat"
N18-1019,E14-1057,0,0.0577141,"Missing"
N18-1019,D16-1215,1,0.926775,"re lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sense inventories or to directly perform in-context substitution. In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step. Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015). Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases available in the 2 Datasets for system training and evaluation have been made available in the SemEval 2016 Complex Word Identification task (Paetzold and Specia, 2016) but present several issues that make system comparison problematic. We explain the drawbacks of the proposed datasets that led to their exclusion from this work in Section 5. 1 For a detailed overview of syntactic simplification works, see (Shardlow, 2014). 208 Annotators Prevalence Example Words 0 0.617 heard, sat, feet, shops, town 1 0.118"
N18-1019,P14-5010,0,0.0119287,"four different reading levels. Xu et al. (2015) also aligned sentences from these texts, extracting 141,582 complex/simple sentence pairs. We use the Newsela corpus to create a goldstandard dataset of complex and simple words for training and testing our models. We do this by hiring crowdsourced annotators through Amazon Mechanical Turk, and asking them to identify complex words in the context of given texts. We randomly select 200 texts from the Newsela corpus, and take the first 200 tokens from each to be labeled by nine annotators. We preprocess the texts using the Stanford CoreNLP suite (Manning et al., 2014) for tokenization, lemmatization, part-of-speech (POS) tagging, and named entity recognition. The annotators are instructed to label at least 10 complex words they deem worth sim3.2 Methods Following Shardlow (2013a), we use a Support Vector Machine classifier. We also conduct experiments with a Random Forest Classifier. Shardlow (2013a) identified several features that help to determine whether or not a word is complex, including word length, number of syllables, word frequency, number of unique WordNet synsets, and number of WordNet synonyms. Shardlow used word frequencies extracted from SUB"
N18-1019,P05-1074,1,0.635779,"elines, we consider candidate substitutions from three datasets. The first is WordNet (Miller, 1995), a lexical network encoding manually identified semantic relationships between words, such as synonymy, hypernymy and hyponymy. This resource has been widely used in substitution tasks (McCarthy and Navigli, 2007). We also use paraphrases extracted from the Paraphrase Database (PPDB) and the Simple Paraphrase Database (SimplePPDB). PPDB is a collection of more than 100 million English paraphrase pairs (Ganitkevitch et al., 2013). These pairs were extracted using a bilingual pivoting technique (Bannard and Callison-Burch, 2005), which assumes that two English phrases that translate to the same foreign phrase have the same meaning. PPDB was updated by Pavlick et al. (2015) to assign labels stating the precise entailment relationship between paraphrase pairs (e.g. forward/backward entailment), and new confidence scores (PPDB 2.0 scores) reflecting the strength of paraphrase relations. SimplePPDB is a subset of PPDB which contains 4.5 million simplification rules, linking a complex word or phrase with a simpler paraphrase with the same meaning. Simplification rules come with both a PPDB 2.0 score and a simplification c"
N18-1019,S07-1009,0,0.294471,"uce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context. This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sense inventories or to directly perform in-context substitution. In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step. Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015). Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases available in the 2 Datasets for system training and evaluation have been made avail"
N18-1019,P11-2087,0,0.0221431,"been shown that frequent words increase a text’s readability (Devlin and Tait, 1998; Kauchak, 2013). Models that include a semantic processing step for substitute selection aim to ensure that the selected substitutes express the correct meaning of words in specific contexts. WSD is often carried out by selecting the correct synset (i.e. set of synonyms describing a sense) for a target word in WordNet (Miller, 1995) and retrieving the synonyms describing that sense. Thomas and Anderson (2012) use WordNet’s tree structure (hypernymy relations) to reduce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context. This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sen"
N18-1019,W15-1501,0,0.256079,"o contextunaware models. 1 Figure 1: An example sentence with complex words identified by our classifier, and their substitutes proposed by the embedding-based substitution model. previous work, our classifier takes into account both lexical and context features. We extract candidate substitutes for the identified complex words from SimplePPDB (Pavlick and Callison-Burch, 2016), a database of 4.5 million English simplification rules linking English complex words to simpler paraphrases. We select the substitutes that best fit each context using a word embeddingbased lexical substitution model (Melamud et al., 2015). An example sentence, along with the complex words identified by our model and the proposed replacements, is shown in Figure 1. We show that our complex word identification classifier and substitution model improve over several baselines which exploit other types of information and do not account for context. Our approach proposes highly accurate substitutes that are simpler than the target words and preserve the meaning of the corresponding sentences. Introduction Automated text simplification is the process that involves transforming a complex text into one with the same meaning, but can be"
N18-1019,W17-1914,1,0.903861,"1 For a detailed overview of syntactic simplification works, see (Shardlow, 2014). 208 Annotators Prevalence Example Words 0 0.617 heard, sat, feet, shops, town 1 0.118 protests, pump, trial 2 0.062 sentenced, fraction, primary 3 0.047 measures, involved, elite 4 0.035 fore, pact, collapsed 5 0.031 slew, enrolled, widespread 6 0.029 edible, seize, dwindled 7 0.023 perilous, activist, remorse 8 0.023 vintners, adherents, amassed 9 0.015 abdicate, detained, liaison Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to select the ones that are adequate in specific contexts. In the same line, Cocos et al. (2017) used a word embedding-based substitution model (Melamud et al., 2015) for ranking PPDB paraphrases in context. We extend this work and adapt the Melamud et al. (2015) model to the simplification setting by using candidate paraphrases extracted from the Simple PPDB resource (Pavlick and Callison-Burch, 2016), a subset of the PPDB that contains complex words and phrases, and their simpler counterparts that can be used for incontext simplification. 3 Table 1: Examples of words identified as difficult to understand within a text by n annotators, where 0 ≤ n ≤ 9. Column 2 (Prevalence) shows the pr"
N18-1019,W11-1601,0,0.131133,"simplification, and sentence splitting. In this paper, we focus on lexical simplification, the task of replacing difficult words in a text with words that are easier to understand. Lexical simplification involves two main processes: identifying complex words within a text, and suggesting simpler paraphrases for these words that preserve their meaning in this context. To identify complex words, we train a model on data manually annotated for complexity. Unlike 2 Related Work Prior approaches to text simplification have addressed the task as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). The proposed models are trained on aligned sentences extracted from Wikipedia and Simple Wikipedia, a corpus that 207 Proceedings of NAACL-HLT 2018, pages 207–217 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion. Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution. Coster and Kauchak (2011) employ a phrase-based Machine Translatio"
N18-1019,W12-3018,0,0.0217572,"Missing"
N18-1019,N13-1092,1,0.904854,"Missing"
N18-1019,W16-6620,0,0.0544251,"The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input. In contrast to these methods, Narayan and Gardent (2016)’s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformations involved in text simplification. Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice an"
N18-1019,P13-1151,0,0.0188796,"ures, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used. Ranking is often addressed in terms of word frequency in a large corpus since it has been shown that frequent words increase a text’s readability (Devlin and Tait, 1998; Kauchak, 2013). Models that include a semantic processing step for substitute selection aim to ensure that the selected substitutes express the correct meaning of words in specific contexts. WSD is often carried out by selecting the correct synset (i.e. set of synonyms describing a sense) for a target word in WordNet (Miller, 1995) and retrieving the synonyms describing that sense. Thomas and Anderson (2012) use WordNet’s tree structure (hypernymy relations) to reduce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rul"
N18-1019,P16-2024,1,0.940117,"ct complex words with higher accuracy than other commonly used methods, and propose good simplification substitutes in context. They also highlight the limited contribution of context features for CWI, which nonetheless improve simplification compared to contextunaware models. 1 Figure 1: An example sentence with complex words identified by our classifier, and their substitutes proposed by the embedding-based substitution model. previous work, our classifier takes into account both lexical and context features. We extract candidate substitutes for the identified complex words from SimplePPDB (Pavlick and Callison-Burch, 2016), a database of 4.5 million English simplification rules linking English complex words to simpler paraphrases. We select the substitutes that best fit each context using a word embeddingbased lexical substitution model (Melamud et al., 2015). An example sentence, along with the complex words identified by our model and the proposed replacements, is shown in Figure 1. We show that our complex word identification classifier and substitution model improve over several baselines which exploit other types of information and do not account for context. Our approach proposes highly accurate substitut"
N18-1019,D11-1038,0,0.0387281,"us that 207 Proceedings of NAACL-HLT 2018, pages 207–217 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion. Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution. Coster and Kauchak (2011) employ a phrase-based Machine Translation system extended to support phrase deletion, and Wubben et al. (2012) augment a phrase-based system with a re-ranking heuristic. Woodsend and Lapata (2011) view simplification as a monolingual text generation task. They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zha"
N18-1019,P15-2070,1,0.920774,"Missing"
N18-1019,P12-1107,0,0.0775015,"Missing"
N18-1019,Q15-1021,1,0.934259,"Missing"
N18-1019,D17-1062,0,0.0421641,"11) view simplification as a monolingual text generation task. They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input. In contrast to these methods, Narayan and Gardent (2016)’s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformations involved in text simplification. Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical"
N18-1019,P13-3015,0,0.288672,"works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Søgaard, 2013).1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking. To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used. Ranking is often addressed in terms of word frequency in a large corpus since it has b"
N18-1019,C10-1152,0,0.10565,"ication, syntactic simplification, and sentence splitting. In this paper, we focus on lexical simplification, the task of replacing difficult words in a text with words that are easier to understand. Lexical simplification involves two main processes: identifying complex words within a text, and suggesting simpler paraphrases for these words that preserve their meaning in this context. To identify complex words, we train a model on data manually annotated for complexity. Unlike 2 Related Work Prior approaches to text simplification have addressed the task as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). The proposed models are trained on aligned sentences extracted from Wikipedia and Simple Wikipedia, a corpus that 207 Proceedings of NAACL-HLT 2018, pages 207–217 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion. Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution. Coster and Kauchak (2011) employ a phras"
N18-1019,W13-2908,0,0.44462,"works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Søgaard, 2013).1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking. To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used. Ranking is often addressed in terms of word frequency in a large corpus since it has b"
N18-1019,S12-1046,0,0.0205931,"nd Anderson (2012) use WordNet’s tree structure (hypernymy relations) to reduce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context. This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sense inventories or to directly perform in-context substitution. In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step. Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015). Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases"
N18-1019,Q14-1018,0,0.0257101,". The intuition for including context-specific features is that if a target word is surrounded by simple words, a reader is likely better able to understand the meaning of the target word, which would thus not need it simplified. To evaluate the performance of our lexical simplification model, we create a test set from the Newsela corpus. We extract lexical simplification rules from these parallel sentences using two methods. First, we find sentence pairs with only one lexical replacement and use these word pairs as simplification instances. Next, we use a monolingual word alignment software (Sultan et al., 2014) to extract all non-identical aligned word pairs. We only consider word pairs corresponding to different lemmas (i.e. words with different base forms). From this process, we collect a test set of 14,436 word pairs. 4 4.2 4.1 Lexical Simplification In-context Ranking and Substitution To accurately replace words in texts with simpler paraphrases and ensure the generated sentences preserve the meaning of the original, we need to take into account the surrounding context. To do this, we adapt the word embedding-based lexical substitution model of Melamud et al. (2015) to the simplification task. V"
N18-1019,I11-1127,0,0.0561484,"Missing"
N18-1030,E17-1056,0,0.0162954,"omic Organization: Select a subset of ˆ the predicted edges, R P ⊆ R, that produces a high sum of scores, r∈Rˆ s(rij ), subject to structural constraints. The final output is the ˆ ˆ graph G(E, R). Structural constraints dictate what can be considered a valid or invalid combination of edges in a taxonomic graph (Do and Roth, 2010). Two structural constraints frequently imposed are that the final graph be a DAG, or that the final graph be a tree/forest.1 Examples of algorithms that produce DAG structures are the longest-path algorithm of Kozareva and Hovy (2010), the ContrastMedium approach of Faralli et al. (2017), and the random cycle-breaking method used in (Panchenko et al., 2016) and Faralli et al. (2015). We experiment with a variation of the last one here, which we call N O C YC. To produce tree-structured taxonomies, most researchers (including us) use algorithms for finding the maximally-weighted rooted tree spanning a directed graph (DMST). Examples of prior work following this approach are Navigli et al. (2011) and Bansal et al. (2014). Another dimension along which taxonomy organization approaches differ is whether they explicitly require the set of chosen relational inˆ to be fully transiti"
N18-1030,P14-1098,0,0.036834,"Missing"
N18-1030,W11-2501,0,0.0144971,"th human judgements of paraphrase quality (Pavlick et al., 2015). We extract sets of entities from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appea"
N18-1030,N13-1092,1,0.720357,"Missing"
N18-1030,J15-2003,0,0.0185141,"insect) is seˆ and (insect IS - A organism) is lected as part of R, General Framework for Taxonomy Induction The problem of taxonomy induction can be summarized via three core sub-tasks. While all systems that build taxonomies automatically must ad1 WLOG, the tree and forest constraints are identical, as a dummy root node can be attached to the root of each component in a forest to produce a tree. 324 ˆ then (beetle IS - A organism) selected as part of R, must also be selected. Two methods that impose such transitivity constraints are the M AX T RANS G RAPH and M AX T RANS F OREST methods of Berant et al. (2015), both of which we experiment with here. A final consideration when choosing a taxonomy organization algorithm is whether the method should enable the consolidation of synonyms into a single taxonomic entity. Synonym sets, or synsets, are present as nodes in the WordNet graph (Miller, 1995). Potential advantages to using synonym sets, rather than individual terms, as nodes include the ability to model polysemy (horse means one thing when grouped with its synonym cavalry and another entirely when grouped with sawhorse), and the ability to be more precise in defining relations. A few early taxon"
N18-1030,C92-2082,0,0.59337,"Missing"
N18-1030,J12-1003,0,0.0200049,"in 3b. Finally, its transitive reduction is in 3c. Because flipping edges in the result produces a tree rooted at organism, the graph in 3a is called forest-reducible. The purpose of modifying scores this way is efficiency; M AX T RANS G RAPH solves its optimization on each connected component of the graph independently, where components are constructed by considering only positively-weighted edges in the graph. Increasing λ increases sparsity and decreases runtime. The objective of the ILP is to maximize the weights of selected relations, while requiring that the graph respects transitivity. Berant et al. (2012) proved this problem is NP-hard and provided an ILP formulation for it as follows. Let xij be a binary variable that indicates whether edge (ei , ej ) ˆ is in the subset of selected edges, R. The algorithm works by adding a dummy root node eROOT to E, and an edge from eROOT to every other node ei in the graph. We then use Chu-Liu-Edmonds to find the directed tree rooted at eROOT that spans all nodes in E and has the maximal sum of scores. Note that until now we have considered edges in taxonomy graphs to point from hyponyms to hypernyms; in this case we must switch the order, so that the spann"
N18-1030,D10-1108,0,0.0524198,"Missing"
N18-1030,N15-1098,0,0.0537651,"Missing"
N18-1030,S15-2151,0,0.0217095,"pairwise relation prediction as a common initialization; we then (b) feed the resulting graphs as identical input to six taxonomic organization algorithms. We evaluate the impact of varied structural constraints between algorithms. Introduction Many words and phrases fit within a natural semantic hierarchy: a mobile is a type of telephone, which in turn is a communications device and an object. Taxonomies, which encode this knowledge, are important resources for natural language understanding systems. There is ongoing interest in developing methods to build taxonomic resources automatically (Bordea et al., 2015, 2016). Although several widelyused general ontologies (e.g. WordNet (Miller, 1995)) and domain-specific ontologies (e.g. Unified Medical Language System (UMLS) (Bodenreider, 2004)) exist, these resources are handcrafted and therefore expensive to update or expand. Automatic taxonomy induction enables the construction of taxonomic resources at scale in new languages and domains. Further, there is evidence that it is useful to build dynamic or contextspecific taxonomies extemporaneously for some applications (Do and Roth, 2010). 323 Proceedings of NAACL-HLT 2018, pages 323–333 c New Orleans, L"
N18-1030,S16-1168,0,0.0294462,"Missing"
N18-1030,C02-1144,0,0.0786533,"when choosing a taxonomy organization algorithm is whether the method should enable the consolidation of synonyms into a single taxonomic entity. Synonym sets, or synsets, are present as nodes in the WordNet graph (Miller, 1995). Potential advantages to using synonym sets, rather than individual terms, as nodes include the ability to model polysemy (horse means one thing when grouped with its synonym cavalry and another entirely when grouped with sawhorse), and the ability to be more precise in defining relations. A few early taxonomy induction approaches incorporated synonym clustering (e.g. Lin and Pantel (2002) and Pantel and Ravichandran (2004)). The two transitive algorithms that we analyze here, M AX T RANS G RAPH and M AX T RANS F OREST, also consolidate equivalent terms into a single node. 3 3.1 The no-cycles method, which we abbreviate as N O C YC, is a simple method for constructing a DAG with high score from a set of predicted relational edges. It is not transitive. The algorithm works as follows. From the set R of all predicted hypernym relations, we first filter out of the graph G(E, R) any edges with score s(rij ) less than a tunable threshold τ . Next, we break any cycles by finding stro"
N18-1030,D10-1107,0,0.0817601,"in developing methods to build taxonomic resources automatically (Bordea et al., 2015, 2016). Although several widelyused general ontologies (e.g. WordNet (Miller, 1995)) and domain-specific ontologies (e.g. Unified Medical Language System (UMLS) (Bodenreider, 2004)) exist, these resources are handcrafted and therefore expensive to update or expand. Automatic taxonomy induction enables the construction of taxonomic resources at scale in new languages and domains. Further, there is evidence that it is useful to build dynamic or contextspecific taxonomies extemporaneously for some applications (Do and Roth, 2010). 323 Proceedings of NAACL-HLT 2018, pages 323–333 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dress each of these tasks, the sequence and manner in which they are addressed varies. In the most straightforward case, the core tasks are viewed as orthogonal and carried out sequentially. They are: Taxonomy induction involves three sub-tasks: entity extraction, relation prediction, and taxonomic organization. In many cases these subtasks are undertaken sequentially to build a taxonomy from the ground up. While many works directly compare methods for r"
N18-1030,D12-1104,0,0.0846298,"Missing"
N18-1030,E17-1007,0,0.012244,"New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dress each of these tasks, the sequence and manner in which they are addressed varies. In the most straightforward case, the core tasks are viewed as orthogonal and carried out sequentially. They are: Taxonomy induction involves three sub-tasks: entity extraction, relation prediction, and taxonomic organization. In many cases these subtasks are undertaken sequentially to build a taxonomy from the ground up. While many works directly compare methods for relation prediction (e.g. Turney and Mohammad (2015), Shwartz et al. (2017) and others), none directly compare methods for the final taxonomic organization step with varying constraints. Each paper that proposes a taxonomic organization method starts with its own set of predicted relations, making it impossible to determine – even with benchmark datasets – the extent to which improvements in identifying ground-truth relations are due to (a) better relation prediction, or (b) better taxonomic organization. In this work, we present an empirical applesto-apples comparison of six algorithms for unsupervised taxonomic organization. The algorithms vary along three axes: wh"
N18-1030,S15-1021,0,0.0128219,"the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appear in any of the local taxonomies. We train HypeNET using our 76K-pair test set, and provide the resu"
N18-1030,S16-1206,0,0.0461464,"Missing"
N18-1030,N04-1041,0,0.100186,"rganization algorithm is whether the method should enable the consolidation of synonyms into a single taxonomic entity. Synonym sets, or synsets, are present as nodes in the WordNet graph (Miller, 1995). Potential advantages to using synonym sets, rather than individual terms, as nodes include the ability to model polysemy (horse means one thing when grouped with its synonym cavalry and another entirely when grouped with sawhorse), and the ability to be more precise in defining relations. A few early taxonomy induction approaches incorporated synonym clustering (e.g. Lin and Pantel (2002) and Pantel and Ravichandran (2004)). The two transitive algorithms that we analyze here, M AX T RANS G RAPH and M AX T RANS F OREST, also consolidate equivalent terms into a single node. 3 3.1 The no-cycles method, which we abbreviate as N O C YC, is a simple method for constructing a DAG with high score from a set of predicted relational edges. It is not transitive. The algorithm works as follows. From the set R of all predicted hypernym relations, we first filter out of the graph G(E, R) any edges with score s(rij ) less than a tunable threshold τ . Next, we break any cycles by finding strongly connected components (SCC) in"
N18-1030,Q15-1025,0,0.0259428,"Missing"
N18-1030,P15-2070,1,0.861283,"Missing"
N18-1030,C14-1097,0,0.0305415,"Missing"
N18-1030,L16-1722,0,0.0121738,"quality (Pavlick et al., 2015). We extract sets of entities from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appear in any of the local taxonomi"
N18-1030,W15-4208,0,0.0129284,"We extract sets of entities from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appear in any of the local taxonomies. We train HypeNET using our 76"
N18-1030,P16-1226,0,0.0298547,"Missing"
N18-2077,W16-2302,0,0.0245056,"Missing"
N18-2077,D16-1025,0,0.024631,"Missing"
N18-2077,N10-1031,0,0.0239866,"etrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical su"
N18-2077,N12-1017,0,0.408623,"large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type"
N18-2077,P15-2070,1,0.921182,"Missing"
N18-2077,W16-2339,0,0.0316299,"Missing"
N18-2077,N13-1092,1,0.918735,"Missing"
N18-2077,2006.amta-papers.25,0,0.407861,"selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the metric for tuning and evaluation of current MT systems. 1 Damon undermines richness variability belittles pluralism diminishes divergence in cinematography cinema film movie Figure 1: An English reference sentence enriched with substitutes selected by the embedding-based lexical substitution model. metric with automatically generated lattices (hereafter HyTERA). We show that HyTERA strongly correlates with HyTER with hand-crafted lattices, and approximates the hTER score (Snover et al., 2006) as measured using post-edits made by human annotators. Furthermore, we generate lattices for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent tran"
N18-2077,H05-1021,0,0.0419773,"TERA to hTER scores. In Section § 5.2, we explore whether H y TERA can reliably predict human translation quality scores from the WMT16 Metrics Shared Task. The vectors s and t are word embeddings of the substitute and target generated by the skip-gram with negative sampling model (Mikolov et al., 2013b,a).4 The context C is the set of context embeddings for words appearing within a fixedwidth window of the target t in a sentence (we use 1 The code is available at https://bitbucket. org/gwisniewski/hytera/ 2 Note that as permutations of interest can be compactly encoded in a fine-state graph (Kumar and Byrne, 2005), the MOVE operation can be easily considered in our code by applying the substitutions to the permutation lattice rather than to the sentence. 3 PPDB paraphrases come into packages of different sizes (going from S to XXXL): small packages contain highprecision paraphrases while larger ones have high coverage. All are available from paraphrase.org 4 For the moment, we focus on individual content words. In future work, we plan to also annotate longer text segments in the references with multi-word PPDB paraphrases. 5 In the original implementation, Melamud et al. (2015) use syntactic dependenci"
N18-2077,W09-0441,0,0.0362055,"l substitution method described in Section 3 to each of the four references associated with a sentence, and considering the union of the resulting lattices. We report results for two kinds of lattices: lattices encoding all lexical substitutes available for a word in PPDB (allPars) and lattices of substitutes with PPDBSc&gt;2.3 (allParsFiltered) and AddCosSc≥0. As expected, the allPars lattices are much larger than the manual and the filtered lattices (cf. Table 1). In all our experiments, all corpora are down-cased and tokenized using standard Moses scripts. hTER scores are computed using TERp (Snover et al., 2009). 5.2 WMT Metrics Evaluation In our second set of experiments, we explore the ability of HyTERA to predict direct human judgments at the sentence level using the setting of the WMT16 Metrics Shared Task (Bojar et al., 2016). We measure the correlation between adSentence Level Evaluation Table 2 reports the correlation between HyTER, HyTERA and hTER at the sentence level. We also include as a baseline the correlation with the sentence-level B LEU 9 More precisely: S B LEU = 1 · 4 p where p is the i 4 ∑i=1 i number of i-grams that appears both in the reference and in the hypothesis divided by th"
N18-2077,W12-3129,0,0.0244022,"orm the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model ("
N18-2077,W15-3050,0,0.0514759,"Missing"
N18-2077,W15-1501,0,0.178423,". The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type of reference networks and test, for the first time, the 2 The Original HyTER Metric The HyTER metric (Dreyer and Marcu, 2012) computes the similarity between a translation hypothesis and a reference lattice that compactly encodes millions of meaning-equivalent translations. Formally HyTER is defined as: H y TER (x, Y ) = arg min y∈Y LS(x, y) len(y) (1) where Y is a set of references that can be encoded as a finite state automaton such as the one represented in Figure 1, x is a translation hypothesis and LS is the standard Levenshtein distance, defined as the minimum num"
N18-2077,W15-3053,0,0.053028,"Missing"
N18-2077,W12-3018,0,0.0656812,"Missing"
N18-2077,P02-1040,0,0.115047,"for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We"
N19-1317,C96-2183,0,0.667434,"of a simplification generated by a standard Seq2Seq model vs. our model. Introduction Automatic text simplification aims to reduce the complexity of texts and preserve their meaning, making their content more accessible to a broader audience (Saggion, 2017). This process can benefit people with reading disabilities, foreign language learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state"
N19-1317,W11-1601,0,0.0511063,"matic and human evaluation settings, and show that the generated simple sentences are shorter and simpler, while remaining competitive with respect to fluency and adequacy. We also include a detailed error analysis to explain where the model currently falls short and provide suggestions for addressing these issues. 2 Related Work Text simplification has often been addressed as a monolingual translation process, which generates a simplified version of a complex text. Zhu et al. (2010) employ a tree-based translation model and consider sentence splitting, deletion, reordering, and substitution. Coster and Kauchak (2011) use a Phrase-Based Machine Translation (PBMT) system with support for deleting phrases, while Wubben et al. (2012) extend a PBMT system with a reranking heuristic (PBMT-R). Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017). In recent"
N19-1317,P18-1082,0,0.0392393,"erate relatively long sentences, our model is able to generate shorter and simpler sentences, while remaining competitive regarding humanevaluated fluency and adequacy. Finally, we provide a qualitative analysis of where our different contributions improve performance, the effect of length on human-evaluated meaning preservation, and the current shortcomings of our model as insights for future research. Generating diverse outputs from Seq2Seq models could be used in a variety of NLP tasks, such as chatbots (Shao et al., 2017), image captioning (Vijayakumar et al., 2018), and story generation (Fan et al., 2018). In addition, the proposed techniques can also be extremely helpful in leveled and personalized text simplification, where the goal is to generate different sentences based on who is requesting the simplification. Acknowledgments We would like to thank the anonymous reviewers for their helpful feedback on this work. We would also like to thank Devanshu Jain, Shyam Upadhyay, and Dan Roth for their feedback on the postdecoding aspect of this work, as well as Anne Cocos and Daphne Ippolito for their insightful comments during proofreading. This material is based in part on research sponsored by"
N19-1317,W11-2123,0,0.0443189,"3.3 MSE 3.72 1.13 Table 2: Pearson Correlation and Overall Mean Squared Error (MSE) for the sentence-level complexity prediction model (CNN), compared to a length-based baseline. SCE ← CE · w return SCE 0 Correlation 0.503 0.650 Reranking Diverse Candidates Generating diverse sentences is helpful only if we are able to effectively rerank them in a way that promotes simpler sentences while preserving fluency and adequacy. To do this, we propose three • Fluency (fi ): We calculate the perplexity based on a 5-gram language model trained on English Gigaword v.5 (Parker et al., 2011) using KenLM (Heafield, 2011). • Adequacy (ai ): We generate Paragraph Vector representations (Le and Mikolov, 2014) for the input sentence and each candidate and calculate the cosine similarity. • Simplicity (si ): We develop a sentence complexity prediction model to predict the overall complexity of each sentence we generate. To calculate sentence complexity, we modify a Convolutional Neural Network (CNN) for sentence classification (Kim, 2014) to make continuous predictions. We use aligned sentences from the Newsela corpus (Xu et al., 2015) as training data, labeling each with the complexity level from which it came.6"
N19-1317,E17-3017,0,0.160826,"duction Automatic text simplification aims to reduce the complexity of texts and preserve their meaning, making their content more accessible to a broader audience (Saggion, 2017). This process can benefit people with reading disabilities, foreign language learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine trans"
N19-1317,D14-1181,0,0.00379492,"To do this, we propose three • Fluency (fi ): We calculate the perplexity based on a 5-gram language model trained on English Gigaword v.5 (Parker et al., 2011) using KenLM (Heafield, 2011). • Adequacy (ai ): We generate Paragraph Vector representations (Le and Mikolov, 2014) for the input sentence and each candidate and calculate the cosine similarity. • Simplicity (si ): We develop a sentence complexity prediction model to predict the overall complexity of each sentence we generate. To calculate sentence complexity, we modify a Convolutional Neural Network (CNN) for sentence classification (Kim, 2014) to make continuous predictions. We use aligned sentences from the Newsela corpus (Xu et al., 2015) as training data, labeling each with the complexity level from which it came.6 As with the word complexity prediction model, we report MSE and Pearson correlation on a held-out test set in Table 2.7 We normalize each individual score between 0 and 1, and calculate a final score as follows: scorei = βf fi + βa ai + βs si (2) We tune these weights (β) on our validation data during experimentation to find the most appropriate combinations of reranking metrics. Examples of improvements resulting fro"
N19-1317,W11-1611,1,0.672136,"Missing"
N19-1317,P14-1041,0,0.398723,"relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement learning (Zhang and Lapata,"
N19-1317,N18-1019,1,0.841356,"ion of Seq2Seq models, please see (Sutskever et al., 2014). 3138 3 Model Frequency Baseline Length Baseline LinReg Seq2Seq Approach 3.1 Complexity-Weighted Loss Function Standard Seq2Seq models use cross entropy as the loss function at training time. This only takes into account how similar our generated tokens are to those in the reference simple sentence, and not the complexity of said tokens. Therefore, we first develop a model to predict word complexities, and incorporate these into a custom loss function. 3.1.1 Word Complexity Prediction Extending the complex word identification model of Kriz et al. (2018), we train a linear regression model using length, number of syllables, and word frequency; we also include Word2Vec embeddings (Mikolov et al., 2013). To collect data for this task, we consider the Newsela corpus, a collection of 1,840 news articles written by professional editors at 5 reading levels (Xu et al., 2015).3 We extract word counts in each of the five levels; in this dataset, we denote 4 as the original complex document, 3 as the least simplified re-write, and 0 as the most simplified re-write. We propose using Algorithm 1 to obtain the complexity label for each word w, where lw re"
N19-1317,P17-2014,0,0.175761,"inyals and Le, 2015), summarization (Nallapati et al., 2016), etc. Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017).2 Nisioi et al. (2017) was the first major application of Seq2Seq models to text simplification, applying a standard encoder-decoder approach with attention and beam search. Vu et al. (2018) extended this framework to incorporate memory augmentation, which simultaneously performs lexical and syntactic simplification, allowing them to outperform standard Seq2Seq models. There are two main Seq2Seq models we will compare to in this work, along with the statistical model from Narayan and Gardent (2014). Zhang and Lapata (2017) proposed DRESS (Deep REinforcement Sentence Simplification), a Seq2Seq model that uses a rein"
N19-1317,P02-1040,0,0.104313,"nt. Model Hybrid DRESS DMASS S2S S2S-Loss S2S-FA S2S-Cluster-FA S2S-Diverse-FA S2S-All-FAS S2S-All-FA SARI 33.27 36.00 34.35 36.32 36.03 36.47 37.22 35.36 36.30 37.11 Model Complex Hybrid DRESS DMASS S2S S2S-Loss S2S-FA Oracle – – – – – 54.01 50.36 52.65 50.40 50.40 S2S-Cluster-FA S2S-Diverse-FA S2S-All-FAS S2S-All-FA Reference Table 4: Comparison of our models to baselines and state-of-the-art models using SARI. We also include oracle SARI scores (Oracle), given a perfect reranker. S2S-All-FA is significantly better than the DMASS and Hybrid baselines using a student t-test (p < 0.05). BLEU (Papineni et al., 2002) for evaluation; even though it correlates better with fluency than SARI, Sulem et al. (2018) recently showed that BLEU often negatively correlates with simplicity on the task of sentence splitting. We also calculate oracle SARI, where appropriate, to show the score we could achieve if we had a perfect reranking model. Our results are reported in Table 4. Our best models outperform previous state-ofthe-art systems, as measured by SARI. Table 4 also shows that, when used separately, reranking and clustering result in improvements on this metric. Our loss and diverse beam search methods have mor"
N19-1317,D15-1166,0,0.0218597,"a PBMT system with a reranking heuristic (PBMT-R). Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017). In recent work, Seq2Seq models are widely used for sequence transduction tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), conversation agents (Vinyals and Le, 2015), summarization (Nallapati et al., 2016), etc. Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks ("
N19-1317,P14-5010,0,0.00953077,"Missing"
N19-1317,K16-1028,0,0.0363035,"ropose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017). In recent work, Seq2Seq models are widely used for sequence transduction tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), conversation agents (Vinyals and Le, 2015), summarization (Nallapati et al., 2016), etc. Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017).2 Nisioi et al. (2017) was the first major application of Seq2"
N19-1317,D14-1162,0,0.0829513,"its are the same as Zhang and Lapata (2017). We preprocess our data by tokenizing and replacing named entities using CoreNLP (Manning et al., 2014). 4.2 Training Details For our experiments, we use Sockeye, an open source Seq2Seq framework built on Apache MXNet (Hieber et al., 2017; Chen et al., 2015). In this model, we use LSTMs with attention for both our encoder and decoder models with 256 hidden units, and two hidden layers. We attempt to match the hyperparameters described in Zhang and Lapata (2017) as closely as possible; as such, we use 300-dimensional pretrained GloVe word embeddings (Pennington et al., 2014), and Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. We ran our models for 30 epochs.8 During training, we use our complexityweighted loss function, with α = 2; for our baseline models, we use cross-entropy loss. At inference time, where appropriate, we set the beam size b = 100, and the similarity penalty δ = 1.0. After inference, we set the number of clusters to 20, and we compare two separate reranking weightings: one which uses fluency, adequacy, and simplicity (FAS), where βf = βa = βs = 13 ; and one which uses only fluency and adequacy (FA), where βf = βa = 12 and βs"
N19-1317,D17-1235,0,0.0276333,"e standard metric for simplification. More importantly, while other previous models generate relatively long sentences, our model is able to generate shorter and simpler sentences, while remaining competitive regarding humanevaluated fluency and adequacy. Finally, we provide a qualitative analysis of where our different contributions improve performance, the effect of length on human-evaluated meaning preservation, and the current shortcomings of our model as insights for future research. Generating diverse outputs from Seq2Seq models could be used in a variety of NLP tasks, such as chatbots (Shao et al., 2017), image captioning (Vijayakumar et al., 2018), and story generation (Fan et al., 2018). In addition, the proposed techniques can also be extremely helpful in leveled and personalized text simplification, where the goal is to generate different sentences based on who is requesting the simplification. Acknowledgments We would like to thank the anonymous reviewers for their helpful feedback on this work. We would also like to thank Devanshu Jain, Shyam Upadhyay, and Dan Roth for their feedback on the postdecoding aspect of this work, as well as Anne Cocos and Daphne Ippolito for their insightful"
N19-1317,2006.amta-papers.25,0,0.24737,"Missing"
N19-1317,W16-3411,0,0.240817,"age learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is th"
N19-1317,D18-1081,0,0.158457,"Missing"
N19-1317,P08-1040,0,0.0941426,"ir meaning, making their content more accessible to a broader audience (Saggion, 2017). This process can benefit people with reading disabilities, foreign language learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limita"
N19-1317,Q15-1021,1,0.956109,"ose in the reference simple sentence, and not the complexity of said tokens. Therefore, we first develop a model to predict word complexities, and incorporate these into a custom loss function. 3.1.1 Word Complexity Prediction Extending the complex word identification model of Kriz et al. (2018), we train a linear regression model using length, number of syllables, and word frequency; we also include Word2Vec embeddings (Mikolov et al., 2013). To collect data for this task, we consider the Newsela corpus, a collection of 1,840 news articles written by professional editors at 5 reading levels (Xu et al., 2015).3 We extract word counts in each of the five levels; in this dataset, we denote 4 as the original complex document, 3 as the least simplified re-write, and 0 as the most simplified re-write. We propose using Algorithm 1 to obtain the complexity label for each word w, where lw represents the level given to the word, and cwi represents the number of times that word occurs in level i. Algorithm 1 Word Complexity Data Collection 1: procedure DATA C OLLECTION 2: lw ← 4 3: for i ∈ {3, 0} do 4: if cwi ≥ 0.7 ∗ cwi+1 then 5: if cwi ≥ 0.4 ∗ cw4 then 6: lw ← i return lw Here, we initially label the word"
N19-1317,Q16-1029,1,0.870827,"utions, reranking using fluency, adequacy, and simplicity (FAS weights). Finally, S2S-All-FA integrates all modifications we propose, and reranks using FA weights. 5 Results In this section, we compare the baseline models and various configurations of our model with both standard automatic simplification metrics and a human evaluation. We show qualitative examples where each of our contributions improves the generated simplification in Table 3. 5.1 Automatic Evaluation Following previous work (Zhang and Lapata, 2017; Zhao et al., 2018), we use SARI as our main automatic metric for evaluation (Xu et al., 2016).11 Specifically, SARI calculates how often a generated sentence correctly keeps, inserts, and deletes n-grams from the complex sentence, using the reference simple standard as the gold-standard, where 1 ≤ n ≤ 4. Note that we do not use 9 For Hybrid and DRESS, we use the generated outputs provided in Zhang and Lapata (2017). We made a significant effort to rerun the code for DRESS, but were unable to do so. 10 For DMASS, we ran the authors’ code on our data splits from Newsela, in collaboration with the first author to ensure an accurate comparison. 11 To calculate SARI, we use the original sc"
N19-1317,D17-1062,0,0.387635,"and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement learning (Zhang and Lapata, 2017) and memory augmentation (Zhao et al., 2018), but these systems often still produce outputs that are longer than the reference sentences. To avoid this problem, we propose to extend the generic Seq2Seq framework at both training and inference time by encouraging the model to choose simpler content words, and by effectively choosing an output based on a large set of can3137 Proceedings of NAACL-HLT 2019, pages 3137–3147 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics didate simplifications. The main contributions of this paper can be summarized as"
N19-1317,D18-1355,0,0.549236,"l approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement learning (Zhang and Lapata, 2017) and memory augmentation (Zhao et al., 2018), but these systems often still produce outputs that are longer than the reference sentences. To avoid this problem, we propose to extend the generic Seq2Seq framework at both training and inference time by encouraging the model to choose simpler content words, and by effectively choosing an output based on a large set of can3137 Proceedings of NAACL-HLT 2019, pages 3137–3147 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics didate simplifications. The main contributions of this paper can be summarized as follows: • We propose a custom loss functio"
N19-1317,C10-1152,0,0.452302,"about the largest benefit for the simplification system. We compare our model to several state-of-the-art systems in both an automatic and human evaluation settings, and show that the generated simple sentences are shorter and simpler, while remaining competitive with respect to fluency and adequacy. We also include a detailed error analysis to explain where the model currently falls short and provide suggestions for addressing these issues. 2 Related Work Text simplification has often been addressed as a monolingual translation process, which generates a simplified version of a complex text. Zhu et al. (2010) employ a tree-based translation model and consider sentence splitting, deletion, reordering, and substitution. Coster and Kauchak (2011) use a Phrase-Based Machine Translation (PBMT) system with support for deleting phrases, while Wubben et al. (2012) extend a PBMT system with a reranking heuristic (PBMT-R). Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform co"
N19-1317,N18-2013,0,0.118978,"o a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017).2 Nisioi et al. (2017) was the first major application of Seq2Seq models to text simplification, applying a standard encoder-decoder approach with attention and beam search. Vu et al. (2018) extended this framework to incorporate memory augmentation, which simultaneously performs lexical and syntactic simplification, allowing them to outperform standard Seq2Seq models. There are two main Seq2Seq models we will compare to in this work, along with the statistical model from Narayan and Gardent (2014). Zhang and Lapata (2017) proposed DRESS (Deep REinforcement Sentence Simplification), a Seq2Seq model that uses a reinforcement learning framework at training time to reward the model for producing sentences that score high on fluency, adequacy, and simplicity. This work showed stateof"
N19-1317,D11-1038,0,0.515178,"in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement"
N19-1317,P12-1107,0,0.565081,"Missing"
P11-1148,S07-1002,0,0.0869888,"s. The model for verbs was constructed analogously, using 3K verbs, and the same number of dependency relations and context words. For our initial k-means clustering, we set k = 600 for nouns, and k = 400 for verbs. For the underlying interleaved NMF model, we used 50 iterations, and factored the model to 50 dimensions. 4.3 Evaluation measures The results of the systems participating in the SEMEVAL -2010 WSI / WSD task are evaluated both in a supervised and in an unsupervised manner. The supervised evaluation in the SEMEVAL -2010 WSI / WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. One part of the test set is used as a mapping corpus, which maps the automatically induced clusters to gold standard senses; the other part acts as an evaluation corpus. The mapping between clusters and gold standard senses is used to tag the evaluation corpus with gold standard tags. The systems are then evaluated as in a standard WSD task, using recall. In the unsupervised evaluation, the induced senses are evaluated as clusters of instances which are compared to the sets of instances tagged with the gold standard senses (corresponding to classes). Two partitions a"
P11-1148,W06-1669,0,0.678229,"Missing"
P11-1148,D09-1056,0,0.0528375,"Missing"
P11-1148,E06-1018,0,0.259547,"r). The first one to propose this idea of context-group discrimination was Sch¨utze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). In the graph-based approach, on the other hand, a co-occurrence graph is created, in which nodes represent words, and edges connect words that appear in the same context (dependency relation or context window). The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (V´eronis, 2004) or Pagerank (Agirre et al., 2006). Finally, Bordag (2006) recently proposed an approach that uses word triplets to perform word sense induction. The underlying idea is the ‘one sense per collocation’ assumption, and co-occurrence triplets are clustered based on the words they have in common. Global algorithms take an approach in which the different senses of a particular word are determined by comparing them to, and demarcating them from, the senses of other words in a full-blown word space model. The best known global approach is the one by Pantel and Lin (2002). They present a global clustering algorithm – coined clustering by committee (CBC) – th"
P11-1148,N06-2015,0,0.0269336,"uced without using any other resources. The training set for a target word consists of a set of target word instances in context (sentences or paragraphs). The complete training set contains 879,807 instances, viz. 716,945 noun and 162,862 verb instances. The senses induced during training are used for disambiguation in the testing phase. In this phase, the system is provided with a test set that consists of unseen instances of the target words. The test set contains 8,915 instances in total, of which 5,285 nouns and 3,630 verbs. The instances in the test set are tagged with OntoNotes senses (Hovy et al., 2006). The system needs to disambiguate these instances using the senses acquired during training. 4.2 Implementational details The SEMEVAL training set has been part of speech tagged and lemmatized with the Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003) and parsed with MaltParser (Nivre et al., 2006), trained on sections 221 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank6 in order to extract dependency triples. The SEMEVAL test set has only been tagged and lemmatized, as our disambiguation model"
P11-1148,P98-2127,0,0.143181,"ding to which nouns and documents can be represented more efficiently. Our model also applies a factorization technique (albeit a different one) in order to find a reduced semantic space. Context is a determining factor in the nature of the semantic similarity that is induced. A broad context window (e.g. a paragraph or document) yields broad, topical similarity, whereas a small context yields tight, synonym-like similarity. This has lead a number of researchers to use the dependency relations that a particular word takes part in as contextual features. One of the most important approaches is Lin (1998). An overview of dependency-based semantic space models is given in Pad´o and Lapata (2007). 2.2 Word sense induction The following paragraphs provide a succinct overview of word sense induction research. A thorough survey on word sense disambiguation (including unsupervised induction algorithms) is presented in Navigli (2009). Algorithms for word sense induction can roughly 1477 be divided into local and global ones. Local WSI algorithms extract the different senses of a word on a per-word basis, i.e. the different senses for each word are determined separately. They can be further subdivided"
P11-1148,nivre-etal-2006-maltparser,0,0.00598089,"phase. In this phase, the system is provided with a test set that consists of unseen instances of the target words. The test set contains 8,915 instances in total, of which 5,285 nouns and 3,630 verbs. The instances in the test set are tagged with OntoNotes senses (Hovy et al., 2006). The system needs to disambiguate these instances using the senses acquired during training. 4.2 Implementational details The SEMEVAL training set has been part of speech tagged and lemmatized with the Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003) and parsed with MaltParser (Nivre et al., 2006), trained on sections 221 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank6 in order to extract dependency triples. The SEMEVAL test set has only been tagged and lemmatized, as our disambiguation model does not use dependency triples as features (contrary to the induction model). 6 http://maltparser.org/mco/english_ parser/engmalt.html 1481 We constructed two different models – one for nouns and one for verbs. For each model, the matrices needed for our interleaved NMF factorization are extracted from the corpus. The noun model wa"
P11-1148,J07-2002,0,0.0888418,"Missing"
P11-1148,S10-1081,0,0.471581,"Missing"
P11-1148,W04-2406,0,0.151771,"ng algorithms and graph-based algorithms. In the context-clustering approach, context vectors are created for the different instances of a particular word, and those contexts are grouped into a number of clusters, representing the different senses of the word. The context vectors may be represented as first or secondorder co-occurrences (i.e. the contexts of the target word are similar if the words they in turn co-occur with are similar). The first one to propose this idea of context-group discrimination was Sch¨utze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). In the graph-based approach, on the other hand, a co-occurrence graph is created, in which nodes represent words, and edges connect words that appear in the same context (dependency relation or context window). The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (V´eronis, 2004) or Pagerank (Agirre et al., 2006). Finally, Bordag (2006) recently proposed an approach that uses word triplets to perform word sense induction. The underlying idea is the ‘one sense per collocation’ assumption, and co-occurrence trip"
P11-1148,D07-1043,0,0.0314269,"of instances tagged with the gold standard senses (corresponding to classes). Two partitions are thus created over the test set of a target word: a set of automatically generated clusters and a set of gold standard classes. A number of these instances will be members of both one gold standard class and one cluster. Consequently, the quality of the proposed clustering solution is evaluated by comparing the two groupings and measuring their similarity. Two evaluation metrics are used during the unsupervised evaluation in order to estimate the quality of the clustering solutions, the V-Measure (Rosenberg and Hirschberg, 2007) and the paired FScore (Artiles et al., 2009). V-Measure assesses the quality of a clustering by measuring its homogeneity (h) and its completeness (c). Homogeneity refers to the degree that each cluster consists of data points primarily belonging to a single gold standard class, while completeness refers to the degree that each gold standard class consists of data points primarily assigned to a single cluster. V-Measure is the harmonic mean of h and c. VM = 2·h·c h+c (7) In the paired F-Score (Artiles et al., 2009) evaluation, the clustering problem is transformed into a classification proble"
P11-1148,J98-1004,0,0.904193,"Missing"
P11-1148,W00-1308,0,0.107456,"Missing"
P11-1148,N03-1033,0,0.0105646,"training are used for disambiguation in the testing phase. In this phase, the system is provided with a test set that consists of unseen instances of the target words. The test set contains 8,915 instances in total, of which 5,285 nouns and 3,630 verbs. The instances in the test set are tagged with OntoNotes senses (Hovy et al., 2006). The system needs to disambiguate these instances using the senses acquired during training. 4.2 Implementational details The SEMEVAL training set has been part of speech tagged and lemmatized with the Stanford Part-OfSpeech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003) and parsed with MaltParser (Nivre et al., 2006), trained on sections 221 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank6 in order to extract dependency triples. The SEMEVAL test set has only been tagged and lemmatized, as our disambiguation model does not use dependency triples as features (contrary to the induction model). 6 http://maltparser.org/mco/english_ parser/engmalt.html 1481 We constructed two different models – one for nouns and one for verbs. For each model, the matrices needed for our interleaved NMF factorization"
P11-1148,C08-1117,1,0.884934,"Missing"
P11-1148,C02-1114,0,0.0953311,"er co-occurrences (i.e. the contexts of the target word are similar if the words they in turn co-occur with are similar). The first one to propose this idea of context-group discrimination was Sch¨utze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). In the graph-based approach, on the other hand, a co-occurrence graph is created, in which nodes represent words, and edges connect words that appear in the same context (dependency relation or context window). The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (V´eronis, 2004) or Pagerank (Agirre et al., 2006). Finally, Bordag (2006) recently proposed an approach that uses word triplets to perform word sense induction. The underlying idea is the ‘one sense per collocation’ assumption, and co-occurrence triplets are clustered based on the words they have in common. Global algorithms take an approach in which the different senses of a particular word are determined by comparing them to, and demarcating them from, the senses of other words in a full-blown word space model. The best known global approach is the one by Pa"
P11-1148,C98-2122,0,\N,Missing
P11-1148,S10-1011,0,\N,Missing
S13-2032,2010.iwslt-papers.2,1,0.884676,"ustering so translations might be found in different clusters. Final clusters are characterized by global connectivity, meaning that all their elements are linked by pertinent relations. Table 1 gives examples of clusters generated for CLWSD target words in the three languages. The clusters group translations carrying the same sense and their overlaps describe relations between senses. The translation clusters serve as the target words’ candidate senses from which one has to be selected during disambiguation. 1 The thresholding procedure and the clustering algorithm are described in detail in Apidianaki and He (2010). Spanish Subtask Best OOF OOF (dupl) Metric 23,23 27,48 53,07 57,34 - - LIMSI Baseline P/R Mode P/R P/R Mode P/R P/R Mode P/R 24,7 32,09 49,01 51,41 98,6 51,41 Italian French Best system 32,16 37,11 61,69 64,65 LIMSI Baseline 24,56 22,16 45,37 39,54 101,75 39,54 25,73 20,19 51,35 47,42 Best system 30,11 26,62 59,8 57,57 - - 20,21 19,88 42,62 41,68 Best system 25,66 31,61 53,57 56,61 - - LIMSI Baseline 21,2 23,06 40,25 47,21 90,23 47,21 Table 2: Results at the SemEval 2013 CLWSD task. 2.2 Word Sense Disambiguation The vectors used for clustering the translations also serve for disambiguating n"
S13-2032,E09-1010,1,0.903509,"e represented by means of translation clusters in different languages built by a cross-lingual Word Sense Induction (WSI) method. Our CLWSD classifier exploits the WSI output for selecting appropriate translations for target words in context. We present the design of the system and the obtained results. 1 Introduction This paper describes the LIMSI system that participated in the Cross-Lingual Word Sense Disambiguation (CLWSD) task of SemEval-2013. The goal of CLWSD is to predict semantically correct translations for ambiguous words in context (Resnik and Yarowsky, 2000; Carpuat and Wu, 2007; Apidianaki, 2009). The CLWSD task of the SemEval-2013 evaluation campaign is a lexical sample task for English nouns and is divided into two subtasks: the best subtask where systems are asked to provide a unique good translation for words in context; the out-of-five (oof) subtask where systems can propose up to five semantically related translations for each target word instance (Lefever and Hoste, 2013). The CLWSD lexical sample contains 20 nouns and the test set is composed of 50 instances per noun. System performance is evaluated by comparing the system output to a set of gold standard annotations in five l"
S13-2032,W11-2203,1,0.790027,"ions that were most frequently selected by the annotators for each instance and are thus considered as the most plausible ones. Our system outperforms the mode best baselines for all languages. In the oof task, the system has been penalized by the elimination of duplicate translations from the output after submission. In previous work, the CLWSD system gave very good results when applied, with some slight variations, to the out-of-ten subtask of the SemEval-2010 Cross-Lingual Lexical Substitution task where duplicates served to promote translations with high confidence (Mihalcea et al., 2010; Apidianaki, 2011). Here, after the post-processing step, oof suggestions contain in many cases less than five translations which explains the low scores. In Table 2 we provide oof results before and after postprocessing the output and show how the system was affected by this change in evaluation. By boosting plausible translations, precision and recall scores get higher while mode scores are naturally not affected.2 As the other systems might have been impacted to different extents by this change, we cannot estimate 2 Precision scores might be inflated, as in the case of French, because the credit for each ite"
S13-2032,D07-1007,0,0.0611269,") task. Word senses are represented by means of translation clusters in different languages built by a cross-lingual Word Sense Induction (WSI) method. Our CLWSD classifier exploits the WSI output for selecting appropriate translations for target words in context. We present the design of the system and the obtained results. 1 Introduction This paper describes the LIMSI system that participated in the Cross-Lingual Word Sense Disambiguation (CLWSD) task of SemEval-2013. The goal of CLWSD is to predict semantically correct translations for ambiguous words in context (Resnik and Yarowsky, 2000; Carpuat and Wu, 2007; Apidianaki, 2009). The CLWSD task of the SemEval-2013 evaluation campaign is a lexical sample task for English nouns and is divided into two subtasks: the best subtask where systems are asked to provide a unique good translation for words in context; the out-of-five (oof) subtask where systems can propose up to five semantically related translations for each target word instance (Lefever and Hoste, 2013). The CLWSD lexical sample contains 20 nouns and the test set is composed of 50 instances per noun. System performance is evaluated by comparing the system output to a set of gold standard an"
S13-2032,2005.mtsummit-papers.11,0,0.0136007,"test set is composed of 50 instances per noun. System performance is evaluated by comparing the system output to a set of gold standard annotations in five languages: French, Spanish, Italian, Dutch and German. Participating systems have to provide con2 2.1 System Description Translation clustering Contrary to monolingual WSI methods which group the instances of the words into clusters describing their senses, the cross-lingual WSI method used here clusters the translations of words in a parallel corpus. The corpus used for French consists of the English-French parts of Europarl (version 7) (Koehn, 2005) and of the JRC-Acquis corpus (Steinberger et al., 2006), joined together. For EnglishSpanish and English-Italian we only use the corresponding parts of Europarl. The corpora are first tokenized and lowercased using the Moses scripts, then lemmatized and tagged by part-of-speech (PoS) using the TreeTagger (Schmid, 1994). Words in the corpus are replaced by a lemma and PoS tag pair before word alignment, to resolve categorical ambiguities in context. The corpus is aligned in both translation directions with GIZA++ (Och and Ney, 2000) 178 Second Joint Conference on Lexical and Computational Sema"
S13-2032,W09-2412,0,0.0318151,"Missing"
S13-2032,P00-1056,0,0.119039,"rench consists of the English-French parts of Europarl (version 7) (Koehn, 2005) and of the JRC-Acquis corpus (Steinberger et al., 2006), joined together. For EnglishSpanish and English-Italian we only use the corresponding parts of Europarl. The corpora are first tokenized and lowercased using the Moses scripts, then lemmatized and tagged by part-of-speech (PoS) using the TreeTagger (Schmid, 1994). Words in the corpus are replaced by a lemma and PoS tag pair before word alignment, to resolve categorical ambiguities in context. The corpus is aligned in both translation directions with GIZA++ (Och and Ney, 2000) 178 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 178–182, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Target word Spanish French range {ensemble, diversit´e, palette, nombre} {domaine} {port´ee} {´eventail, nombre, gamme, s´erie, ensemble} mood {climat, atmosph`ere}, {esprit, atmosph`ere, ambiance, humeur} {opinion} {volont´e} {attitude} mission {op´eration, mandat} {d´el´egation, commission} {d´el´egation, tˆache, voyage, op´eration} {gama,"
S13-2032,steinberger-etal-2006-jrc,0,0.0350375,"Missing"
S13-2032,W09-2413,0,\N,Missing
S13-2032,S10-1002,0,\N,Missing
S15-2050,E09-1010,1,0.841978,"no recourse to context information. The system needs no training and can be applied directly to parallel data. The evaluation results show that the LIMSI system outperforms all systems in all domains in EnTask Description The SemEval-2015 Multilingual WSD and EL task (Moro and Navigli, 2015) aims to promote joint research in these two closely-related topics. WSD refers to the task of assigning meanings to occurrences of words in texts (Navigli, 2009) and its multilingual counterpart involves the identification of semantically adequate translations (Resnik and Yarowsky, 1997; Ide et al., 2002; Apidianaki, 2009). EL, on the other side, aims at linking entities in a text to the most suitable entry in a knowledge base. The systems participating in the Multilingual WSD and EL task can make a choice between different options (WSD, EL or both) and one or several WSD settings (all-words or specific part-of-speech disambiguation). Contrary to previous tasks (Navigli et al., 2013), the SemEval-2015 task addresses the disambiguation of words of all content parts of speech. No training data is provided and the test set consists of parallel texts in three languages (English, Italian and Spanish) pertaining to b"
S15-2050,P02-1033,0,0.158425,"and retain only intersecting alignments to rule out spurious correspondences. For each instance of an English content word in the test set we identify its Spanish translation in context and, alternatively, the English translations of Spanish and Italian words. We use the lemma and part-of-speech information provided by the task organizers. 3.2 Sense Selection The established alignment correspondences serve as constraints to retrieve the BabelSynsets that are relevant for words in the test set, based on the assumption of a semantic correspondence between a word and its translation in context (Diab and Resnik, 2002). BabelSynsets group synonymous English words and their translations in different languages. Polysemous words are found in different synsets, as in WordNet (Miller et al., 1990), and are associated to different translations. The procedure for selecting the most adequate BabelSynset for an occurrence of a word (w) in context is described in Figure 1. First, we find the synsets of 1 2 The resource is available at http://babelnet.org/ WordNet, wiki resources and automatic translations. 299 Notation: Sw : the set of BabelSynsets for w t: a translation of w in context Swt : the set of synsets in wh"
S15-2050,W02-0808,0,0.145521,"Missing"
S15-2050,H93-1061,0,0.0583217,"by our system compared to the baseline show that the alignment-based filtering remains beneficial in spite of the problematic sense ranking, as the aligned translation might occur in only one BabelSynset. Table 3 provides a detailed analysis of the results. The top part of the table shows the accuracy of the alignment-based predictions, which might coincide with the BFS or not. Our system improves over the BFS in 37 cases in English, 136 in Spanish and 142 in Italian. On the contrary, the BFS does better only 7 Sense numbers in WordNet reflect the frequency of the senses in the SemCor corpus (Miller et al., 1993). 8 An additional criterion applies to Wikipedia senses according to which pages that contain a parenthetical explanation, as in disambiguation pages, are ranked lower than ones that do not. 9 For exemple, in cases of unaligned words or where the aligned translation is not found in some synset. 301 is aligned to ventana in the Spanish text, which translates both the “opening” and the “computer” sense of the word. Although the Spanish translation helps to rule out 11 of the 15 BabelSynsets of window, ranking the remaining four synsets puts forward the more frequent “opening” sense (00081285n) w"
S15-2050,S15-2049,0,0.14281,"ambiguation and Entity Linking task of SemEval-2015. The system exploits the parallelism of the multilingual test data and uses translations as source of indirect supervision for sense selection. The LIMSI system gets best results in English in all domains and shows that alignment information can successfully guide disambiguation. This simple but effective method can serve to generate high quality sense annotated data for WSD system training. 1 2 Introduction This paper describes the LIMSI system at the Multilingual Word Sense Disambiguation (WSD) and Entity Linking (EL) task of SemEval-2015 (Moro and Navigli, 2015). The system performs sense selection by combining translation information obtained through alignment of the multilingual test set with sense ranking. It can thus be described as semisupervised given the indirect supervision provided by the translations. The alignment correspondences serve as constraints for reducing the search space for each word to BabelNet synsets (hereafter, BabelSynsets) containing the translation and the retained synsets are sorted according to the BabelNet sense ranking. Our goal is to test the contribution of translations in multilingual WSD with no recourse to context"
S15-2050,S13-2040,0,0.19706,"refers to the task of assigning meanings to occurrences of words in texts (Navigli, 2009) and its multilingual counterpart involves the identification of semantically adequate translations (Resnik and Yarowsky, 1997; Ide et al., 2002; Apidianaki, 2009). EL, on the other side, aims at linking entities in a text to the most suitable entry in a knowledge base. The systems participating in the Multilingual WSD and EL task can make a choice between different options (WSD, EL or both) and one or several WSD settings (all-words or specific part-of-speech disambiguation). Contrary to previous tasks (Navigli et al., 2013), the SemEval-2015 task addresses the disambiguation of words of all content parts of speech. No training data is provided and the test set consists of parallel texts in three languages (English, Italian and Spanish) pertaining to both open and closed domains (biomedical, math and computer, and a broader (social issues) domain). For evaluation, the data is manually annotated with senses from BabelNet (version 2.5.1), a wide-coverage multilin298 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 298–302, c Denver, Colorado, June 4-5, 2015. 2015 Associatio"
S15-2050,J03-1002,0,0.00558585,"WSD is addressed explicitly, the system is also assigned EL scores as it manages to annotate several Named Entities with the correct synset. 3 System Description 3.1 Alignment of the Evaluation Dataset The test data contains four parallel documents in English, Spanish and Italian. Our system exploits the parallelism of the test set, a feature overlooked by previous systems (Navigli et al., 2013). In order to avoid some discrepancies observed at the level of sentence correspondences, we first align the texts pairwise using the Hunalign sentence aligner (Varga et al., 2005). Then we run GIZA++ (Och and Ney, 2003) in both directions at the lemma level and retain only intersecting alignments to rule out spurious correspondences. For each instance of an English content word in the test set we identify its Spanish translation in context and, alternatively, the English translations of Spanish and Italian words. We use the lemma and part-of-speech information provided by the task organizers. 3.2 Sense Selection The established alignment correspondences serve as constraints to retrieve the BabelSynsets that are relevant for words in the test set, based on the assumption of a semantic correspondence between a"
S15-2050,W97-0213,0,0.130438,"ion of translations in multilingual WSD with no recourse to context information. The system needs no training and can be applied directly to parallel data. The evaluation results show that the LIMSI system outperforms all systems in all domains in EnTask Description The SemEval-2015 Multilingual WSD and EL task (Moro and Navigli, 2015) aims to promote joint research in these two closely-related topics. WSD refers to the task of assigning meanings to occurrences of words in texts (Navigli, 2009) and its multilingual counterpart involves the identification of semantically adequate translations (Resnik and Yarowsky, 1997; Ide et al., 2002; Apidianaki, 2009). EL, on the other side, aims at linking entities in a text to the most suitable entry in a knowledge base. The systems participating in the Multilingual WSD and EL task can make a choice between different options (WSD, EL or both) and one or several WSD settings (all-words or specific part-of-speech disambiguation). Contrary to previous tasks (Navigli et al., 2013), the SemEval-2015 task addresses the disambiguation of words of all content parts of speech. No training data is provided and the test set consists of parallel texts in three languages (English,"
S16-1002,L16-1465,1,0.768602,"Missing"
S16-1002,S15-2080,0,0.0503377,"Missing"
S16-1002,klinger-cimiano-2014-usage,0,0.0621363,"Missing"
S16-1002,P15-2128,0,0.0333833,"Missing"
S16-1002,S16-1003,0,0.0786167,"Missing"
S16-1002,S13-2052,0,0.0105895,"Missing"
S16-1002,piperidis-2012-meta,0,0.0160887,"Missing"
S16-1002,S14-2004,1,0.673256,"Missing"
S16-1002,S15-2082,1,0.813624,"Missing"
S16-1002,S14-2009,0,0.0111835,"Missing"
S16-1002,S15-2078,0,0.0105712,"Missing"
S16-1002,D13-1170,0,0.0173861,"Missing"
S16-1002,E12-2021,0,0.0937892,"Missing"
S17-1002,chrupala-etal-2008-learning,0,0.0948476,"Missing"
S17-1002,N13-1092,1,0.877486,"Missing"
S17-1002,P82-1020,0,0.820605,"Missing"
S17-1002,P05-1074,1,0.629065,"from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b) in the paraphrasebased method as training data for our neural network model. The main contributions of this paper are: tion can provide substantial complementary information to the distributional signal for distinguishing between different semantic relations. While antonymy applies to expressions that represent contrasting meanings, paraphrases are phrases expressing the same meaning, which usually occur in similar textual contexts (Barzilay and McKeown, 2001) or have common translations in other languages (Bannard and Callison-Burch, 2005). Specifically, if two words or phrases are paraphrases, they are unlikely to be antonyms of each other. Our first approach to antonym detection exploits this fact and uses paraphrases for detecting and generating antonyms (The dementors caught Sirius Black/ Black could not escape the dementors). We start by focusing on phrase pairs that are most salient for deriving antonyms. Our assumption is that phrases (or words) containing negating words (or prefixes) are more helpful for identifying opposing relationships between term-pairs. For example, from the paraphrase pair (caught/not escape), we"
S17-1002,P01-1008,0,0.208388,"prior path-based methods on this task. We used the antonym pairs extracted from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b) in the paraphrasebased method as training data for our neural network model. The main contributions of this paper are: tion can provide substantial complementary information to the distributional signal for distinguishing between different semantic relations. While antonymy applies to expressions that represent contrasting meanings, paraphrases are phrases expressing the same meaning, which usually occur in similar textual contexts (Barzilay and McKeown, 2001) or have common translations in other languages (Bannard and Callison-Burch, 2005). Specifically, if two words or phrases are paraphrases, they are unlikely to be antonyms of each other. Our first approach to antonym detection exploits this fact and uses paraphrases for detecting and generating antonyms (The dementors caught Sirius Black/ Black could not escape the dementors). We start by focusing on phrase pairs that are most salient for deriving antonyms. Our assumption is that phrases (or words) containing negating words (or prefixes) are more helpful for identifying opposing relationships"
S17-1002,E17-1008,0,0.521346,"e terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speakers consider antonyms, they have limited coverage. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that are commonly considered"
S17-1002,P15-1146,1,0.899335,"Missing"
S17-1002,P15-2070,1,0.888814,"Missing"
S17-1002,D14-1162,0,0.0859183,"n sources are used to recognize semantic relations: pathbased and distributional. Path-based methods consider the joint occurrences of the two terms in a given sentence and use the dependency paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speaker"
S17-1002,P14-2086,0,0.268465,"Missing"
S17-1002,K15-1026,0,0.209936,"other relationships has proven to be difficult. Approaches to antonym detection have exploited distributional vector representations relying on the distributional hypothesis of semantic similarity (Harris, 1954; Firth, 1957) that words co-occurring in similar contexts tend to be semantically close. Two main information sources are used to recognize semantic relations: pathbased and distributional. Path-based methods consider the joint occurrences of the two terms in a given sentence and use the dependency paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying anto"
S17-1002,W16-5310,1,0.91968,"irs from paraphrases in the PPDB, the largest paraphrase resource currently available. • We demonstrate improvements to an integrated path-based and distributional model, showing that our morphology-aware neural network model, AntNET, performs better than state-of-the-art methods for antonym detection. Our second method is inspired by the recent success of deep learning methods for relation detection. Shwartz et al. (2016) proposed an integrated path-based and distributional model to improve hypernymy detection between term-pairs, and later extended it to classify multiple semantic relations (Shwartz and Dagan, 2016) (LexNET). Although LexNET was the best performing system in the semantic relation classification task of the CogALex 2016 shared task, the model performed poorly on synonyms and antonyms compared to other relations. The path-based component is weak in recognizing synonyms, which do not tend to co-occur, and the distributional information caused confusion between synonyms and antonyms, since both tend to occur in the same contexts. We propose AntNET, a novel extension of LexNET that integrates information about negating prefixes as a new morphological pattern feature and is able to distinguish"
S17-1002,P16-1226,1,0.906259,"paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speakers consider antonyms, they have limited coverage. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that ar"
S17-1002,C92-2082,0,\N,Missing
S17-1009,D16-1215,1,0.895014,"Missing"
S17-1009,apidianaki-etal-2014-semantic,1,0.901008,"Missing"
S17-1009,W15-1501,0,0.0379515,"Missing"
S17-1009,W04-0807,0,0.0983441,"Missing"
S17-1009,W17-1914,1,0.830874,"Missing"
S17-1009,N16-1172,1,0.851795,"the extended synset s+i p . We calculate features that correspond to the average and maximum PPDB scores bewteen wp and lemmas in s+i p : There has been considerable research directed at expanding WordNet’s coverage either by integrating WordNet with additional semantic resources, as in Navigli and Ponzetto (2012), or by automatically adding new words and senses. In the second case, there have been several efforts specifically focused on hyponym/hypernym detection and attachment (Snow et al., 2006; Shwartz et al., 2016). There is also previous work aimed at adding semantic structure to PPDB. Cocos and Callison-Burch (2016) clustered paraphrases by word sense, effectively forming synsets within PPDB. By mapping individual paraphrases to WordNet synsets, our work could be used in coordination with these previous results in order to extend WordNet relations to the automaticallyinduced PPDB sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representati"
S17-1009,I05-5002,0,0.167329,"Missing"
S17-1009,W12-3018,0,0.0503996,"Missing"
S17-1009,N16-1163,0,0.0165124,", w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the cosine similarity between the word’s word2vec vector and the synset’s NA"
S17-1009,P16-1191,0,0.0209427,"sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the cosine similarity between t"
S17-1009,P15-1146,1,0.93301,"1998) is one of the most important resources for natural language processing research. Despite its utility, WordNet1 is manually compiled and therefore relatively small. It contains roughly 155k words, which does not approach web scale, and very few informal or colloquial words, domain-specific terms, new word uses, or named entities. Researchers have compiled several larger, automatically-generated thesaurus-like resources (Lin and Pantel, 2001; Dolan and Brockett, 2005; Navigli and Ponzetto, 2012; Vila et al., 2015). One of these is the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b). With over 100 million paraphrase pairs, PPDB dwarfs WordNet in size but it lacks WordNet’s semantic structure. Paraphrases for a given word are indistinguishable by sense, and PPDB’s only inherent semantic relational information is predicted entailment relations between word types (Pavlick et al., 2015a). Several earlier studies attempted to incorporate se1 Our overall objective in this work is to map PPDB paraphrases for a target word to the WordNet synsets of the target. This work has two parts. In the first part (Section 4), we train and evaluate a binary lemma-synset membership classif"
S17-1009,N13-1092,1,0.832294,"Missing"
S17-1009,P15-1010,0,0.0305768,"tomaticallyinduced PPDB sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the"
S17-1009,P15-2070,1,0.905439,"Missing"
S17-1009,N15-1070,0,0.0208517,"= max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the cosine similarity between the word’s word2vec ve"
S17-1009,D16-1234,0,0.0374014,"Missing"
S17-1009,P16-1226,0,0.0495732,"Missing"
S17-1009,P06-1101,0,0.0453391,"o synset sip as follows. We call the set of all lemmas belonging to sip and any of its hypernym or hyponym synsets the extended synset s+i p . We calculate features that correspond to the average and maximum PPDB scores bewteen wp and lemmas in s+i p : There has been considerable research directed at expanding WordNet’s coverage either by integrating WordNet with additional semantic resources, as in Navigli and Ponzetto (2012), or by automatically adding new words and senses. In the second case, there have been several efforts specifically focused on hyponym/hypernym detection and attachment (Snow et al., 2006; Shwartz et al., 2016). There is also previous work aimed at adding semantic structure to PPDB. Cocos and Callison-Burch (2016) clustered paraphrases by word sense, effectively forming synsets within PPDB. By mapping individual paraphrases to WordNet synsets, our work could be used in coordination with these previous results in order to extend WordNet relations to the automaticallyinduced PPDB sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which"
S17-1009,vasilescu-etal-2004-evaluating,0,0.0635538,"h˚ To compute the lexical substitutability score between a word wp and synset sip , we first retrieve example sentences e ∈ E containing t in sense sip from BabelNet v3.0 (Navigli and Ponzetto, 2012). Then, for each example e, we compute the AddCos lexical substitutability between wp and the target word in context Ce . We compute two types of this feature: The average AddCos score over all synset examples, and the maximum AddCos score over all synset examples. Lesk Similarity Among the information contained in WordNet for each synset is its definition, or gloss. The simplified Lesk algorithm (Vasilescu et al., 2004) identifies the most likely sense of a target word in context by measuring the overlap between the given context and the definition of each target sense. We use a slightly modified version of the algorithm to compute features that measure the overlap between the PPDB paraphrases for the target and the gloss of a synset. For calculating these Lesk-based features, we find synset glosses from WordNet 3.0 and from BabelNet v3.0 (Navigli and Ponzetto, 2012). First, we find D, the set of content words of the gloss for synset sip , by taking all nouns, verbs, adjectives, and adverbs that appear withi"
S19-1002,S16-1081,0,0.0135791,"on similarity judgments. ison of existing static and dynamic embeddingWe combine direct Usim assessments, made by based meaning representation methods on the usthe embedding-based methods, with a substituteage similarity (Usim) task, which involves estibased Usim approach. Building up on previous mating the semantic proximity of word instances work that used manually selected in-context subin different contexts (Erk et al., 2009). Usim stitutes as a proxy for Usim (Erk et al., 2013; Mcdiffers from a classical Semantic Textual SimiCarthy et al., 2016), we propose to automatize the larity task (Agirre et al., 2016) by the focus on annotation collection step in order to scale up the a particular word in the sentence. We evalumethod and make it operational on unrestricted ate on this task word and context representations text. We exploit annotations assigned to words obtained using pre-trained uncontextualized word 9 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 9–21 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Due to its high reliance on context, Usim can be viewed as a semantic textual similarity (STS) (Agirre et al., 201"
S19-1002,ide-etal-2008-masc,0,0.015982,"Missing"
S19-1002,P09-1002,0,0.636984,"g adapted to every new context of ity. The best representations are used as features use. in supervised models for Usim prediction, trained In this work, we perform an extensive comparon similarity judgments. ison of existing static and dynamic embeddingWe combine direct Usim assessments, made by based meaning representation methods on the usthe embedding-based methods, with a substituteage similarity (Usim) task, which involves estibased Usim approach. Building up on previous mating the semantic proximity of word instances work that used manually selected in-context subin different contexts (Erk et al., 2009). Usim stitutes as a proxy for Usim (Erk et al., 2013; Mcdiffers from a classical Semantic Textual SimiCarthy et al., 2016), we propose to automatize the larity task (Agirre et al., 2016) by the focus on annotation collection step in order to scale up the a particular word in the sentence. We evalumethod and make it operational on unrestricted ate on this task word and context representations text. We exploit annotations assigned to words obtained using pre-trained uncontextualized word 9 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 9–21 c Min"
S19-1002,U12-1006,0,0.225826,"sical word-averaging approach with traditional word embeddings (Pennington et al., 2014), to more recent contextualized word representations (Peters et al., 2018; Devlin et al., 2018). We explore the contribution of each separate method for Usim prediction, and use the best performing ones as features in supervised models. These are trained on sentence pairs labelled with Usim judgments (Erk et al., 2009) to predict the similarity of new word instances. Previous attempts to automatic Usim prediction involved obtaining vectors encoding a distribution of topics for every target word in context (Lui et al., 2012). In this work, Usim was approximated by the cosine similarity of the resulting topic vectors. We show how contextualized representations, and the supervised model that uses them as features, outperform topic-based methods on the graded Usim task. We combine the embedding-based direct Usim assessment methods with substitute-based representations obtained using an unsupervised lexical substitution model. McCarthy et al. (2016) showed it is possible to model usage similarity using manual substitute annotations for words in context. In this setting, the set of substitutes proposed for a word inst"
S19-1002,J13-3003,0,0.634806,"entations are used as features use. in supervised models for Usim prediction, trained In this work, we perform an extensive comparon similarity judgments. ison of existing static and dynamic embeddingWe combine direct Usim assessments, made by based meaning representation methods on the usthe embedding-based methods, with a substituteage similarity (Usim) task, which involves estibased Usim approach. Building up on previous mating the semantic proximity of word instances work that used manually selected in-context subin different contexts (Erk et al., 2009). Usim stitutes as a proxy for Usim (Erk et al., 2013; Mcdiffers from a classical Semantic Textual SimiCarthy et al., 2016), we propose to automatize the larity task (Agirre et al., 2016) by the focus on annotation collection step in order to scale up the a particular word in the sentence. We evalumethod and make it operational on unrestricted ate on this task word and context representations text. We exploit annotations assigned to words obtained using pre-trained uncontextualized word 9 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 9–21 c Minneapolis, June 6–7, 2019. 2019 Association for Comput"
S19-1002,D08-1094,0,0.144398,"Missing"
S19-1002,J16-2003,1,0.905227,"similarity of new word instances. Previous attempts to automatic Usim prediction involved obtaining vectors encoding a distribution of topics for every target word in context (Lui et al., 2012). In this work, Usim was approximated by the cosine similarity of the resulting topic vectors. We show how contextualized representations, and the supervised model that uses them as features, outperform topic-based methods on the graded Usim task. We combine the embedding-based direct Usim assessment methods with substitute-based representations obtained using an unsupervised lexical substitution model. McCarthy et al. (2016) showed it is possible to model usage similarity using manual substitute annotations for words in context. In this setting, the set of substitutes proposed for a word instance describe its specific meaning, while similarity of substitute annotations for different instances points to their semantic proximity.1 We follow up on this work and propose a way to use substitutes for Usim prediction on unrestricted text, bypassing the need for manual annotations. Our method relies on substitute annotations proposed by the context2vec model (Melamud et al., 2016), which uses word and context representat"
S19-1002,S07-1009,0,0.532242,"013) with the GOLD substitutes and Usim scores assigned by the annotators. The Usim score is high for similar instances, and decreases for instances that describe different meanings. The semantic proximity of two instances is also reflected in the similarity of their substitutes sets. For comparison, we also give in the Table the substitutes selected for these instances by the automatic context2vec substitution method used in our experiments (more details in Section 4.2). The LexSub and Usim Datasets We use the training and test datasets of the SemEval-2007 Lexical Substitution (LexSub) task (McCarthy and Navigli, 2007), which contain instances of target words in sentential context handlabelled with meaning-preserving substitutes. A subset of the LexSub data (10 instances x 56 lemmas) has additionally been annotated with graded pairwise Usim judgments (Erk et al., 2013). Each sentence pair received a rating (on a scale of 15) by multiple annotators, and the average judgment for each pair was retained. McCarthy et al. (2016) derive two additional scores from Usim annotations that denote how easy it is to partition a lemma’s usages into sets describing distinct senses: Uiaa, the inter-annotator agreement for a"
S19-1002,P08-1028,0,0.0581255,"on par with BERT on the graded and binary Usim tasks, when using embedding-based representations and clean lexical substitutes. 2 Related Work Usage similarity is a means for representing word meaning which involves assessing in-context semantic similarity, rather than mapping to word senses from external inventories (Erk et al., 2009, 2013). This methodology followed from the gradual shift from word sense disambiguation models that would select the best sense in context from a dictionary, to models that reason about meaning by solely relying on distributional similarity (Erk and Pad´o, 2008; Mitchell and Lapata, 2008), or allow multiple sense interpretations (Jurgens, 2014). In Erk et al. (2009), the idea is to model meaning in context in a way that captures different degrees of similarity to a word sense, or between word instances. 1 McCarthy et al. use the substitute annotations as features for predicting Usim, clustering instances and estimating the partitionability of words into senses. This offers a way to distinguish between lemmas with distinct senses and others with fuzzy semantics, which would be more challenging in annotation tasks and automatic processing. 10 Sentences The local papers took phot"
S19-1002,P15-2070,0,0.124288,"Missing"
S19-1002,D14-1162,0,0.0941852,"e for no charge and may not ] be placed in city cans ] the tag consists of a tiny chip , [about the [size of a match head that serves ] as a ] portable database . this is at least 26 weeks by the [week in [which the approved match with the child ] is made ]. Figure 1: We use contextualized word representations built from the whole sentence or smaller windows around the target word for usage similarity estimation, combined with automatic substitute annotations. grass clippings can be brought out to the landfill at anytime for no *charge* and may not be placed in city cans . embeddings (GloVe) (Pennington et al., 2014), with and without dimensionality reduction (SIF) Traditional word embeddings, like Word2Vec and al., 2017); context obGloVe, merge different meanings of a word in astarted(Arora So what out as a et perfectly lovely stroll ended [representations up as [a public from agrandma bidirectional LSTM (context2vec) wrestlingtained match between and] baby girl], with single vector representation (Mikolov et al., 2013; So what started (Melamud et al., 2016); word em-out as a perfectly lovely s baby girl loving very secondcontextualized of it . Pennington et al., 2014). These pre-trained emas [a public w"
S19-1002,N18-1202,0,0.719609,"a So what out as a et perfectly lovely stroll ended [representations up as [a public from agrandma bidirectional LSTM (context2vec) wrestlingtained match between and] baby girl], with single vector representation (Mikolov et al., 2013; So what started (Melamud et al., 2016); word em-out as a perfectly lovely s baby girl loving very secondcontextualized of it . Pennington et al., 2014). These pre-trained emas [a public wrestling match between g beddings derived from a LSTM bidirectional lanbeddings are fixed, and stay the same indepenbaby girl], with baby girl loving very s guage model (ELMo) (Peters et al., 2018) and gendently of the context of use. Current contextualerated by a Transformer (BERT) (Devlin et al., ized sense representations, like ELMo and BERT, 2018); doc2vec (Le and Mikolov, 2014) and go to the other extreme and model meaning as Universal Sentence Encoder representations (Cer word usage (Peters et al., 2018; Devlin et al., et al., 2018). All these embedding-based meth2018). They provide a dynamic representation of ods provide direct assessments of usage similarword meaning adapted to every new context of ity. The best representations are used as features use. in supervised models for"
S19-1002,jurgens-2014-analysis,0,\N,Missing
S19-1002,E14-1057,0,\N,Missing
S19-1002,N13-1092,0,\N,Missing
S19-1002,W15-1501,0,\N,Missing
S19-1002,K16-1006,0,\N,Missing
S19-1002,D18-2029,0,\N,Missing
W11-2203,2010.iwslt-papers.2,1,0.91007,"roved to be useful in various application settings. When exploited in cross-lingual WSD, it permits to assign ’sensetags’ containing several semantically correct translations to new instances of words in context (Apidianaki, 2009). Moreover, the use of clustering information during evaluation allows for a differing penalization of WSD errors. In an MT evaluation setting, sense clusters have been integrated into an MT evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric’s correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). The use of sense clusters in this setting permits to identify semantic correspondences between translations and hypotheses, and to circumvent the strict requirement for exact surface correspondences, one of the main critics addressed to MT evaluation metrics. The same notion of sense clusters has been adopted in the most recent SemEval Cross-Lingual WSD task (Lefever and Hoste, 2010). Instead of considering translations as indicators of distinct senses, as was the case in previous tasks, the senses of a small number of ambiguous words were described by manually created clusters of translatio"
W11-2203,apidianaki-2008-translation,1,0.929133,"ction of a translation different from the reference is considered as wrong even if it is semantically correct. So, this conception of senses does not per14 mit to penalize WSD errors relatively to their importance (Resnik and Yarowsky, 2000), unless semantic resources are used to identify semantic correspondences. 2.2 Cross-lingual sense clustering Instead of using translations as straightforward sense indicators, it is possible to perform a more thorough semantic analysis during cross-lingual WSI by combining distributional and translation information. The sense clustering method proposed by Apidianaki (2008) identifies complex semantic relations between word senses and their translations. The method is based on the contextual hypotheses of meaning and of semantic similarity (Harris, 1954; Miller and Charles, 1991), which underlie monolingual WSI methods, and is combined to the assumption of a semantic correspondence between words and their translations in real texts (Chesterman, 1998). Following these hypotheses, information coming from the source contexts of a target word when translated with a precise translation in a parallel corpus, is used to reveal the senses carried by the translation. Fur"
W11-2203,E09-1010,1,0.944723,"sense). In the automat1 This set of translations was extracted from the word aligned Europarl corpus (Koehn, 2005) after applying a set of filters that will be described in section 3. ically built bilingual inventories, the senses of the words in one language are thus described by clusters of their translations in another language. 2.3 Applications This type of sense clustering has proved to be useful in various application settings. When exploited in cross-lingual WSD, it permits to assign ’sensetags’ containing several semantically correct translations to new instances of words in context (Apidianaki, 2009). Moreover, the use of clustering information during evaluation allows for a differing penalization of WSD errors. In an MT evaluation setting, sense clusters have been integrated into an MT evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric’s correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). The use of sense clusters in this setting permits to identify semantic correspondences between translations and hypotheses, and to circumvent the strict requirement for exact surface correspondences, one"
W11-2203,S10-1024,0,0.0125619,"ORP R P Mode R Mode P 27.15 26.81 25.27 19.73 19.68 18.87 15.38 8.39 24.34 15.09 27.15 26.81 25.27 19.93 19.68 18.87 22.16 8.62 24.34 15.09 57.20 58.85 52.81 41.29 39.09 36.63 33.47 14.95 50.34 29.22 57.20 58.85 52.81 41.75 39.09 36.63 45.95 15.31 50.34 29.22 target language context (for instance, by using a language model) in order to retain the most adequate translation. It is interesting to note that the systems that perform better in the best subtask get relatively low results in the oot subtask, and the inverse. This is the case, for instance, for UBA-T (Basile and Semeraro, 2010), while Aziz and Specia (2010) clearly specify that their main goal is to maximize the accuracy of their system (USPwlv) in choosing the best translation. A conclusion that can be drawn is that each subtask has different requirements, which may be satisfied by different types of methods. In order to investigate other possible reasons behind the different behavior of the WSD method in the two evaluation subtasks, we performed the evaluation separately for each POS. The results are presented in Tables 5 and 6. POS Adjs Nouns Verbs Advs R P Mode R Mode P 287.94 127.01 115.94 111.46 296.41 141.65 121.43 111.46 72.44 37.78 53.1"
W11-2203,S10-1054,0,0.0868305,"UBA-W SWAT-S IRST-1 TYO DICT DICTCORP R P Mode R Mode P 27.15 26.81 25.27 19.73 19.68 18.87 15.38 8.39 24.34 15.09 27.15 26.81 25.27 19.93 19.68 18.87 22.16 8.62 24.34 15.09 57.20 58.85 52.81 41.29 39.09 36.63 33.47 14.95 50.34 29.22 57.20 58.85 52.81 41.75 39.09 36.63 45.95 15.31 50.34 29.22 target language context (for instance, by using a language model) in order to retain the most adequate translation. It is interesting to note that the systems that perform better in the best subtask get relatively low results in the oot subtask, and the inverse. This is the case, for instance, for UBA-T (Basile and Semeraro, 2010), while Aziz and Specia (2010) clearly specify that their main goal is to maximize the accuracy of their system (USPwlv) in choosing the best translation. A conclusion that can be drawn is that each subtask has different requirements, which may be satisfied by different types of methods. In order to investigate other possible reasons behind the different behavior of the WSD method in the two evaluation subtasks, we performed the evaluation separately for each POS. The results are presented in Tables 5 and 6. POS Adjs Nouns Verbs Advs R P Mode R Mode P 287.94 127.01 115.94 111.46 296.41 141.65"
W11-2203,D07-1007,0,0.359134,"formed permits to override some issues common to monolingual semantic processing tasks, such as the selection of an adequate sense inventory and the definition of the granularity of the semantic descriptions. In a multilingual context, word senses can be easily identified using their translations in other languages (Resnik and Yarowsky, 2000). Although this conception of senses presents some theoretical and practical drawbacks, it provides a standard criterion for sense delimitation which explains its wide adoption in recent works on multilingual Word Sense Disambiguation (WSD) and WSD in MT (Carpuat and Wu, 2007; Ng and Chan, 2007). In this paper, we explain how semantic clustering may provide answers to some of the issues posed by the traditional cross-lingual sense induction approach, and how it can be efficiently exploited for CLLS. Given that existing CLLS systems rely on predefined semantic resources, we show, for the first time, that CLLS can be performed in a fully unsupervised manner. The paper is organized as follows: in the next section, we present some arguments towards unsupervised clustering for crosslingual sense induction. The clustering method used is presented in section 3. Section 4"
W11-2203,P07-1005,0,0.0337334,"a parallel corpus (Resnik and Yarowsky, 2000). This empirical approach to sense induction offers a standard criterion for sense delimitation and, consequently, dissociates WSD from semantic theories and predefined semantic inventories. Moreover, by establishing semantic distinctions pertinent for translation between the implicated languages, it allows to tune sense induction to the needs of multilingual applications. It has thus been widely adopted in works on multilingual WSD and WSD in MT, where senses are derived from parallel data (Diab, 2003; Ide, 1999; Ide et al., 2002; Ng et al., 2003; Chan et al., 2007; Carpuat and Wu, 2007). By linking WSD and its evaluation to translation, this hypothesis also offers a solution to the problem of non-conformity of monolingual WSD methods in this setting. Nevertheless, the assumption of biunivocal (’oneto-one’) correspondences between senses and translations is rather simplistic. One word sense may be translated by different synonymous words in another language, whose relatedness should be considered during sense induction. Furthermore, this approach does not permit to account for cases of parallel ambiguities (Resnik, 2007), and cases where the senses of a"
W11-2203,W02-0808,0,0.0247918,"another language, usually found in a parallel corpus (Resnik and Yarowsky, 2000). This empirical approach to sense induction offers a standard criterion for sense delimitation and, consequently, dissociates WSD from semantic theories and predefined semantic inventories. Moreover, by establishing semantic distinctions pertinent for translation between the implicated languages, it allows to tune sense induction to the needs of multilingual applications. It has thus been widely adopted in works on multilingual WSD and WSD in MT, where senses are derived from parallel data (Diab, 2003; Ide, 1999; Ide et al., 2002; Ng et al., 2003; Chan et al., 2007; Carpuat and Wu, 2007). By linking WSD and its evaluation to translation, this hypothesis also offers a solution to the problem of non-conformity of monolingual WSD methods in this setting. Nevertheless, the assumption of biunivocal (’oneto-one’) correspondences between senses and translations is rather simplistic. One word sense may be translated by different synonymous words in another language, whose relatedness should be considered during sense induction. Furthermore, this approach does not permit to account for cases of parallel ambiguities (Resnik, 20"
W11-2203,2005.mtsummit-papers.11,0,0.103161,"tocar and autob´us are semantically related and do not lexicalize distinct senses of the English word, as is the case with entrenador. Sense clustering permits to estimate the semantic similarity of the translations and to not consider synonymous translations as indicators of distinct senses. Consequently, the English word coach has two senses after sense clustering: one described by the cluster {autocar, autob´us} (the ”bus” sense) and one described by the cluster {entrenador} (the ”trainer” sense). In the automat1 This set of translations was extracted from the word aligned Europarl corpus (Koehn, 2005) after applying a set of filters that will be described in section 3. ically built bilingual inventories, the senses of the words in one language are thus described by clusters of their translations in another language. 2.3 Applications This type of sense clustering has proved to be useful in various application settings. When exploited in cross-lingual WSD, it permits to assign ’sensetags’ containing several semantically correct translations to new instances of words in context (Apidianaki, 2009). Moreover, the use of clustering information during evaluation allows for a differing penalizatio"
W11-2203,W07-0734,0,0.0187041,"he words in one language are thus described by clusters of their translations in another language. 2.3 Applications This type of sense clustering has proved to be useful in various application settings. When exploited in cross-lingual WSD, it permits to assign ’sensetags’ containing several semantically correct translations to new instances of words in context (Apidianaki, 2009). Moreover, the use of clustering information during evaluation allows for a differing penalization of WSD errors. In an MT evaluation setting, sense clusters have been integrated into an MT evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric’s correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). The use of sense clusters in this setting permits to identify semantic correspondences between translations and hypotheses, and to circumvent the strict requirement for exact surface correspondences, one of the main critics addressed to MT evaluation metrics. The same notion of sense clusters has been adopted in the most recent SemEval Cross-Lingual WSD task (Lefever and Hoste, 2010). Instead of considering translations as indicators of distin"
W11-2203,W09-2412,0,0.396101,"Missing"
W11-2203,S07-1010,0,0.0201387,"ride some issues common to monolingual semantic processing tasks, such as the selection of an adequate sense inventory and the definition of the granularity of the semantic descriptions. In a multilingual context, word senses can be easily identified using their translations in other languages (Resnik and Yarowsky, 2000). Although this conception of senses presents some theoretical and practical drawbacks, it provides a standard criterion for sense delimitation which explains its wide adoption in recent works on multilingual Word Sense Disambiguation (WSD) and WSD in MT (Carpuat and Wu, 2007; Ng and Chan, 2007). In this paper, we explain how semantic clustering may provide answers to some of the issues posed by the traditional cross-lingual sense induction approach, and how it can be efficiently exploited for CLLS. Given that existing CLLS systems rely on predefined semantic resources, we show, for the first time, that CLLS can be performed in a fully unsupervised manner. The paper is organized as follows: in the next section, we present some arguments towards unsupervised clustering for crosslingual sense induction. The clustering method used is presented in section 3. Section 4 describes the SemEv"
W11-2203,P03-1058,0,0.114141,"usually found in a parallel corpus (Resnik and Yarowsky, 2000). This empirical approach to sense induction offers a standard criterion for sense delimitation and, consequently, dissociates WSD from semantic theories and predefined semantic inventories. Moreover, by establishing semantic distinctions pertinent for translation between the implicated languages, it allows to tune sense induction to the needs of multilingual applications. It has thus been widely adopted in works on multilingual WSD and WSD in MT, where senses are derived from parallel data (Diab, 2003; Ide, 1999; Ide et al., 2002; Ng et al., 2003; Chan et al., 2007; Carpuat and Wu, 2007). By linking WSD and its evaluation to translation, this hypothesis also offers a solution to the problem of non-conformity of monolingual WSD methods in this setting. Nevertheless, the assumption of biunivocal (’oneto-one’) correspondences between senses and translations is rather simplistic. One word sense may be translated by different synonymous words in another language, whose relatedness should be considered during sense induction. Furthermore, this approach does not permit to account for cases of parallel ambiguities (Resnik, 2007), and cases wh"
W11-2203,J03-1002,0,0.00388687,"e the senses of English words would be described by clusters of their Spanish translations. The training corpus used for building the sense cluster inventory is the SP-EN part of Europarl (release v5), which contains 1,689,850 aligned sentence pairs (Koehn, 2005). Before clustering, some preprocessing steps are performed. First, the corpus is lemmatized and tagged by POS (Schmid, 1994). Then sentence pairs presenting a great difference in length (i.e cases where one sentence is three times longer than the other) are eliminated and the corpus is aligned at the level of word types using Giza++ (Och and Ney, 2003). Two bilingual lexicons of content words are built from the alignment results, one for each translation direction (EN-SP/SP-EN). In the entries of these lexicons, source words are associated with the translations to which they are aligned. As these lexicons are automatically created, they contain some noise mainly due to spurious word alignments. In order to eliminate erroneous translation correspondences, we first apply a filter which discards translations with a probability below 0.001 (according to the scores assigned during word alignment). Then an intersection filter is applied which dis"
W11-2203,H05-1097,0,0.0648774,"Missing"
W11-2203,S10-1025,0,0.275089,"Missing"
W11-2203,W09-2413,0,\N,Missing
W11-2203,S10-1002,0,\N,Missing
W12-3141,W10-1704,1,0.815478,"eriments have demonstrated that better normalization tools provide better BLEU scores: all systems are thus built in “true-case”. Compared to last year, the pre-processing of utf-8 characters was significantly improved. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel"
W12-3141,E09-1010,1,0.834206,"lpful. As the IBM1 model is asymmetric, two models are estimated, one in both directions. Contrary to the reported results, these additional features do not yield significant improvements over the baseline system. We assume that the difficulty is to add information to an already extensively optimized system. Moreover, the IBM1 models are estimated on the same training corpora as the translation system, a fact that may explain the redundancy of these additional features. In a separate series of experiments, we also add WSD features calculated according to a variation of the method proposed in (Apidianaki, 2009). For each word of a subset of the input (source language) vocabulary, a simple WSD classifier produces a probability distribution over a set of translations8 . During reranking, each translation hypothesis is scanned and the word translations that match one of the proposed variant are rewarded using an additional score. While this method had given some Conclusion In this paper, we described our submissions to WMT’12 in the French-English and GermanEnglish shared translation tasks, in both directions. As for our last year’s participation, our main systems are built with n-code, the open source"
W12-3141,J93-2003,0,0.0934316,"ize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will a"
W12-3141,P05-1032,0,0.0155644,"e hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the s"
W12-3141,J04-2004,0,0.03102,"code, an open source in-house Statistical Machine Translation (SMT) system based on bilingual n-grams1 . The main novelty of this year’s participation is the use, in a large scale system, of the continuous space translation models described in (Hai-Son et al., 2012). These models estimate the n-gram probabilities of bilingual translation units using neural networks. We also investigate an alternative approach where the translation probabilities of a phrase based system are estimated “on-the-fly” 1 http://ncode.limsi.fr/ 2 System overview n-code implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006). In this framework, translation is divided in two steps: a source reordering step and a (monotonic) translation step. Source reordering is based on a set of learned rewrite rules that non-deterministically reorder the input words. Applying these rules result in a finite-state graph of possible source reorderings, which is then searched for the best possible candidate translation. 2.1 Features Given a source sentence s of I words, the best translation hypothesis ˆt is defined as the sequence of J words that maximizes a linear combination of fea33"
W12-3141,2010.iwslt-papers.6,0,0.024222,"Missing"
W12-3141,W08-0310,1,0.883725,"Missing"
W12-3141,N12-1005,1,0.885285,"Missing"
W12-3141,P07-2045,0,0.00674765,"lt (31.7 BLEU point) that is slightly worst than the n-code baseline (32.0) and slightly better than the equivalent Moses baseline (31.5), but does it much faster. Model estimation for the test file is reduced to 2 hours and 50 minutes, with an additional overhead for loading and writing files of one and a half hours, compared to roughly 210 hours for our baseline systems under comparable hardware conditions. 7 Experimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that th"
W12-3141,C08-1064,0,0.0749035,"rimental results 7.1 n-code with SOUL We also developped an alternative approach implementing “on-the-fly” estimation of the parameter of a standard phase-based model, using Moses (Koehn et al., 2007) as the decoder. Implementing on-thefly estimation for n-code, while possible in theory, is less appealing due to the computational cost of estimating a smoothed language model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on onthe-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we compute a suffix array for the source corpus. This further enables to consider only a subset of translation examples, which we select by deterministic random sampling, meaning that the sample is chosen randomly with respect to the full corpus but that the same sample is always returned for a given value of sample size, hereafter denoted N . In our experiments, we used N = 1, 000 and computed from the sample and the word alignments (we used the same tokenization and word alignments as in all other submitted systems) the same translation6 and lexical reordering models as the standard traini"
W12-3141,J06-4004,0,0.217743,"Missing"
W12-3141,P03-1021,0,0.0736745,"x lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, sour"
W12-3141,P02-1040,0,0.0874389,"ation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT)) using the newstest2009 as development set and BLEU (Papineni et al., 2002) as the optimization criteria. 2.2 P (s, t) = Standard n-gram translation models During the training phase (Mari˜no et al., 2006), tuples are extracted from a word-aligned corpus (using MGIZA++3 with default settings) in such a way that a unique segmentation of the bilingual corpus is achieved. A baseline n-gram translation model is then estimated over a training corpus composed of tuple sequences using modified KnesserNey Smoothing (Chen and Goodman, 1998). 2.3 During decoding, source sentences are represented in the form of word lattices containing the most promising reordering hypotheses, s"
W12-3141,C08-1098,0,0.0308747,"ing (alignment) and decoding (due to unknown forms). When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 333 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008). 5.2 Bilingual corpora As for last year’s evaluation, we used all the available parallel data for the German-English language pair, while only a subpart of the French-English parallel data was selected. Word alignment models were trained using all the data, whereas the translation models were estimated on a subpart of the parallel data: the UN corpus was discarded for this step and about half of the French-English Giga corpus was filtered based on a perplexity criterion as in (Allauzen et al., 2011)). For French-English, we mainly upgraded the training material from last year by extracting th"
W12-3141,N04-4026,0,0.0279162,"max t,a M X ) λm hm (a, s, t) (1) L Y P (ui |ui−1 , ..., ui−n+1 ) (2) i=1 m=1 where λm is the weight associated with feature function hm and a denotes an alignment between source and target phrases. Among the feature functions, the peculiar form of the translation model constitute one of the main difference between the n-gram approach and standard phrase-based systems. This will be further detailled in section 2.2 and 3. In addition to the translation model, fourteen feature functions are combined: a target-language model (Section 5.3); four lexicon models; six lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aiming at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in standard phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights are estimated from the automatic word alignments. The weights vector λ is learned using a discriminative training framework (Och, 2003) (Minimum Error Rate Training (MERT))"
W12-3141,2002.tmi-tutorials.2,0,0.0391067,"French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . in two parts (source and target), and by taking words as the basic units of the n-gram TM. This may seem to be a regression with respect to current state-ofthe-art SMT systems, as the shift from the wordbased model of (Brown et al., 1993) to the phrasebased models of (Zens et al., 2002) is usually considered as a major breakthrough of the recent years. Indeed, one important motivation for considering phrases was to capture local context in translation and reordering. It should however be emphasized that the decomposition of phrases into words is only re-introduced here as a way to mitigate the parameter estimation problems. Translation units are still pairs of phrases, derived from a bilingual segmentation in tuples synchronizing the source and target n-gram streams. In fact, the estimation policy described in section 4 will actually allow us to take into account larger cont"
W12-3141,D08-1039,0,\N,Missing
W12-3141,W11-2135,1,\N,Missing
W12-3141,N04-1021,0,\N,Missing
W12-4201,apidianaki-2008-translation,1,0.852565,"ained (because the two variants of the adjective are reduced to the same lemma). All lexicon entries satisfying the above criteria are retained and used for disambiguation. In these initial experiments, we disambiguate English words having less than 20 French translations in the lexicon. Each French translation of an English word that appears more than once in the training corpus4 is characterized by a weighted English feature vector built from the training data. Vector building The feature vectors corresponding to the translations are built by exploiting information from the source contexts (Apidianaki, 2008; Grefenstette, 1994). For each translation of an EN word w, we extract the content words that co-occur with w in the corresponding source sentences of the parallel corpus (i.e. the content words that occur in the same sentence as w whenever it is translated by this translation). The extracted source language words constitute the features of the vector built for the translation. For each translation Ti of w, let N be the number of features retained from the corresponding source context. Each feature Fj (1 ≤ j ≤ N) receives a total weight tw(Fj , Ti ) defined as the product of the feature’s glo"
W12-4201,E09-1010,1,0.961121,"at and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source word; the quantity and type of data used for training the classifier (e.g., use of raw data or"
W12-4201,P05-1048,0,0.0813922,"ing some avenues for future work. 2 Related work Word sense disambiguation systems generally work at the word level: given an input word and its context, they predict its (most likely) meaning. At the same time, state-of-the-art translation systems all consider groups of words (phrases, tuples, etc.) rather than single words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambig"
W12-4201,D07-1007,0,0.397955,"improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT. 1 Introduction Word Sense Disambiguation (WSD) is the task of identifying the sense of words in texts by reference to some pre-existing sense inventory. The selection of the appropriate inventory and WSD method strongly depends on the goal WSD intends to serve: recent methods are increasingly oriented towards the disambiguation needs of specific end applications, and explicitly aim at improving the overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009"
W12-4201,P07-1005,0,0.209625,"overall performance of complex Natural Language Processing systems (Ide and Wilks, 2007; Carpuat and Wu, 2007). This task-oriented conception of WSD is manifested in the area of multilingual semantic processing: supervised methods, which were previously shown to give the best results, are being abandoned in favor of unsupervised ones that do not rely on preannotated training data. Accordingly, pre-defined In a multilingual setting, the sense inventories needed for disambiguation are generally built from all possible translations of words or phrases in a parallel corpus (Carpuat and Wu, 2007; Chan et al., 2007), or by using more complex representations of the semantics of translations (Apidianaki, 2009; Mihalcea et al., 2010; Lefever and Hoste, 2010). However, integrating this semantic knowledge into Statistical Machine Translation (SMT) raises several challenges: the way in which the predictions of the WSD classifier have to be taken into account; the type of context exploited for disambiguation; the target words to be disambiguated (“all-words” WSD vs. WSD restricted to target words satisfying specific criteria); the use of a single classifier versus building separate classifiers for each source w"
W12-4201,C10-1027,1,0.871747,"If this is the case, the corresponding probabilities are additively accumulated for the current hypothesis. At the end, two features are appended to each hypothesis in the n-best list: the total score accumulated for the hypothesis and 5 the same score normalized by the number of words in the hypothesis. Two MERT initialization schemes were considered: (1) all model weights are initialized to zero, and (2) all the weights of “standard” features are initialized to the values found by MERT and the new WSD features to zero. 4.2 Local Language Models We propose to adapt the approach introduced in Crego et al. (2010) as an alternative way to integrate the WSD predictions within the decoder: for each sentence to be translated, an additional language model (LM) is estimated and taken into account during decoding. As this additional “local” model depends on the source sentence, it can be used as an external source of knowledge to reinforce translation hypotheses complying with criteria predicted from the whole source sentence. For instance, the unigram probabilities of the additional LM can be derived from the (word) predictions of a WSD system, bigram probabilities from the prediction of phrases and so on a"
W12-4201,2009.eamt-1.32,0,0.0163645,"er, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers t"
W12-4201,2010.amta-papers.23,0,0.0316251,"Missing"
W12-4201,P07-2045,0,0.00398791,". 5 Evaluation 5.3 5.1 Table 2 reports the results of our experiments. It appears that, for the considered task, sense disambiguation improves translation performance: n-best rescoring results in a 0.37 BLEU improvement and using an additional language model brings about an improvement of up to a 0.88 BLEU. In both cases, MERT assigns a large weight to the additional feaExperimental Setting In all our experiments, we considered the TEDtalk English to French data set provided by the IWSLT’11 evaluation campaign, a collection of public speeches on a variety of topics. We used the Moses decoder (Koehn et al., 2007). The TED-talk corpus is a small data set made of a monolingual corpus (111, 431 sentences) used 6 Results 7 http://statmt.org/wmt08/scripts.tgz method baseline rescoring additional LM — WSD (zero init) WSD (reinit) oracle 3-gram oracle 2-gram oracle 1-gram IBM 1 WSD BLEU 29.63 30.00 29.58 43.56 39.36 42.92 30.18 30.51 METEOR 53.78 54.26 53.96 64.64 62.92 69.39 54.36 54.38 Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions. baseline 67.57 45.97 51.79 52.17 PoS Nouns Verbs Adjectives Adverbs WSD 69.06 47.76 53.94 56.25 Table 3: Contrastive lexical e"
W12-4201,D09-1022,0,0.0449313,"n system to reflect the predictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of t"
W12-4201,max-etal-2010-contrastive,1,0.859696,"to the WSD method introduced in Section 3, these oracle experiments rely on sense predictions for all source words and not only content words. Surprisingly enough, predicting phrases instead of words results only in a small improvement. Additional experiments are required to explain why 2-gram oracle achieved such a low performance. 7 5.4 Contrastive lexical evaluation All the measures used for evaluating the impact of WSD information on translation show improvements, as discussed in the previous section. We complement these results with another measure of translation performance, proposed by Max et al. (2010), which allows for a more fine-grained contrastive evaluation of the translations produced by different systems. The method permits to compare the results produced by the systems on different word classes and to take into account the source words that were actually translated. We focus this evaluation on the classes of content words (nouns, adjectives, verbs and adverbs) on which WSD had an important coverage. Our aim is, first, to explore how these words are handled by a WSDinformed SMT system (the system using the local language models) compared to the baseline system that does not exploit a"
W12-4201,W09-2412,0,0.034286,"Missing"
W12-4201,J03-1002,0,0.00530526,"tions of a word and to assign a probability to each translation for new instances of the word in context. Each translation is represented by a source language feature vector that the classifier uses for disambiguation. All experiments carried out in this study are for the English (EN) - French (FR) language pair. 3.1 Source Language Feature Vectors Preprocessing The information needed by the classifier is gathered from the EN-FR training data provided for the IWSLT’11 evaluation task.1 The dataset consists of 107,268 parallel sentences, wordaligned in both translation directions using GIZA++ (Och and Ney, 2003). We disambiguate EN words found in the parallel corpus that satisfy the set of criteria described below. Two bilingual lexicons are built from the alignment results and filtered to eliminate spurious alignments. First, translation correspondences with a probability lower than a threshold are discarded;2 then translations are filtered by part-of-speech (PoS), keeping for each word only translations pertaining to the same grammatical category;3 finally, only intersecting alignments (i.e., correspondences found in the lexicons of both directions) are retained. Given that the lexicons contain wor"
W12-4201,I11-1074,0,0.589525,"redictions of their WSD system. While their approach and ours are built on the same intuition (an adaptation of a model to incorporate word predictions) their work is specific to hierarchical systems, while ours can be applied to any decoder that uses a language model. Haque et al. (2009) et Haque et al. (2010) introduce lexico-syntactic descriptions in the form of supertags as source language context-informed features in a phrase-based SMT and a state-of-the-art hierarchical model, respectively, and report significant gains in translation quality. Closer to our work, Mauser et al. (2009) and Patry and Langlais (2011) train a global lexicon model that predicts the bag of output words from the bag of input words. As no explicit alignment between input and output words is used, words are chosen based on the (global) input context. For each input sentence, the decoder considers these word predictions as an additional feature that it uses to define a new model score which favors translation hypotheses containing words predicted by the global lexicon model. A difference between this approach and our work is that instead of using a global lexicon model, we disambiguate a subset of the words in the input sentence"
W12-4201,P06-3010,0,0.142762,"slation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical translation system to"
W12-4201,H05-1097,0,0.178488,"ngle words in the translation process. This discrepancy between the units used in MT and those used in WSD is one of the major difficulties in integrating word predictions into the decoder. This was, for instance, one of the reasons for the somewhat disappointing results obtained by Carpuat and Wu (2005) when the output of a WSD system was directly incorporated into a Chinese-English SMT system. Because of this difficulty, other crosslingual semantics works have considered only simplified tasks, like blank-filling, without addressing the integration of the WSD models in full-scale MT systems (Vickrey et al., 2005; Specia, 2006). Since the pioneering work of Carpuat and Wu (2005), several more successful ways to take WSD predictions into account have been proposed. For instance, Carpuat and Wu (2007) proposed to generalize the WSD system so that it performs a fully 2 phrasal multiword disambiguation. However, given that the number of phrases is far larger than the number of words, this approach suffers from sparsity and computational problems, as it requires training a classifier for each entry of the phrase table. Chan et al. (2007) introduced a way to modify the rule weights of a hierarchical transla"
W12-4201,W09-2413,0,\N,Missing
W12-4201,S10-1002,0,\N,Missing
W12-4201,N04-1021,0,\N,Missing
W13-2501,fiser-etal-2012-addressing,1,0.850739,"(Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing dictionaries has been addressed"
W13-2501,N03-1015,0,0.0355119,"howed that the results with an automatically created seed lexicon, based on language similarity, can be as good as with a pre-existing dictionary. But all these approaches work on closely-related languages and cannot be used as successfully for language pairs with little lexical overlap, such as English and Slovene, which is the case in this experiment. Regarding the translation of the source vectors, we use contextual information to disambiguate their features and translate them using clusters of semantically similar translations in the target language. A similar idea has been implemented by Kaji (2003) who performed sense-based word 3 3.1 Resources Comparable corpus The comparable corpus from which the bilingual lexicon will be extracted is a collection of English (EN) and Slovene (SL) texts extracted from Wikipedia. The February 2013 dumps of Wikipedia articles were downloaded and cleaned for both languages after which the English corpus was tokenized, part-of-speech (PoS) tagged and lemmatized with the TreeTagger (Schmid, 1994). The same pre-processing was applied to the Slovene corpus with the ToTaLe analyzer (Erjavec et al., 2010) which uses the TnT tagger (Brants, 2000) and was trained"
W13-2501,apidianaki-2008-translation,1,0.80435,"rd We evaluate the quality of the bilingual lexicons extracted from the comparable corpus by comparing them to a gold standard lexicon, which was built from the aligned English (Fellbaum, 1998) and Slovene wordnets (Fiˇser and Sagot, 2008). We extracted all English synsets from the Base Concept sets that belong to the Factotum domain and contain literals with polysemy levels 1-5 and their 4.2 Translation clustering The translations of the English words in the lexicon built as described in 3.2 are clustered according to their semantic proximity using a crosslingual Word Sense Induction method (Apidianaki, 2008). For each translation Ti of a word w, a vector is built from the content word co3 Language POS Nouns Source word sphere address {obravnava, reˇsevanje, obravnavanje} (dealing with) {naslov} (postal address) portion {kos} (piece) {obrok, porcija} (serving) {deleˇz} (share) figure {ˇstevilka, podatek, znesek} (amount) {slika} (image) {osebnost} (person) seal EN–SL weigh Verbs Slovene sense clusters {krogla} (geometrical shape) {sfera, podroˇcje} (area) {tesniti} (to be water-/airtight) {zapreti, zapeˇcatiti} (to close an envelope or some other container) {pretehtati} (consider possibilities) {t"
W13-2501,W02-0902,0,0.0403888,"e high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing dictionaries has been addressed by Koehn and Knight (2002) who built the initial seed dictionary automatically, based on identical spelling features between English and German. Cognate detection has also been used by Saralegi et al. (2008) for extracting word translations from English-Basque comparable corpora. The cognate and seed lexicon approaches have been successfully combined by Fiˇser and Ljubeˇsi´c (2011) who showed that the results with an automatically created seed lexicon, based on language similarity, can be as good as with a pre-existing dictionary. But all these approaches work on closely-related languages and cannot be used as successf"
W13-2501,E09-1010,1,0.937001,"r that contains the feature. The translations found in the disambiguation output convey the sense of the features in the source vector, while the use of translation clusters permits to expand their translation with several variants. As a consequence, the translated vectors are less noisy and richer, and allow for the extraction of higher quality lexicons compared to simpler methods. 1 In this paper, we show how source vectors can be translated into the target language by a cross-lingual Word Sense Disambiguation (WSD) method which exploits the output of data-driven Word Sense Induction (WSI) (Apidianaki, 2009), and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus. This study extends our previous work on the topic (Apidianaki et al., 2012) by applying the proposed methods to a comparable corpus of general language (built from Wikipedia) and optimizing various parameters that affect the quality of the extracted translations. We expect the disambiguation to have a beneficial impact on the results given that polysemy is a frequent phenomenon in a general, mixed-domain corpus. Our experiments are carried out on the English-Slovene langu"
W13-2501,2005.mtsummit-papers.11,0,0.00864014,"approach to a specialized comparable corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in which polysemy is more prominent. 3.2 Parallel corpus The parallel corpus used for clustering and word sense induction consists of the Slovene-English parts of Europarl (release v6) (Koehn, 2005) and of JRC-Acquis (Steinberger et al., 2006) and amounts to approximately 35M words per language. A number of pre-processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeT"
W13-2501,C10-1085,0,0.0140729,"ranslation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewe"
W13-2501,A00-1031,0,0.0109396,"en implemented by Kaji (2003) who performed sense-based word 3 3.1 Resources Comparable corpus The comparable corpus from which the bilingual lexicon will be extracted is a collection of English (EN) and Slovene (SL) texts extracted from Wikipedia. The February 2013 dumps of Wikipedia articles were downloaded and cleaned for both languages after which the English corpus was tokenized, part-of-speech (PoS) tagged and lemmatized with the TreeTagger (Schmid, 1994). The same pre-processing was applied to the Slovene corpus with the ToTaLe analyzer (Erjavec et al., 2010) which uses the TnT tagger (Brants, 2000) and was trained on MultextEast corpora. The Wikipedia corpus contains about 1.5 billion tokens for English and almost 24 million tokens for Slovene. In previous work, we applied our approach to a specialized comparable corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in"
W13-2501,J05-4003,0,0.0420579,"polysemy is a frequent phenomenon in a general, mixed-domain corpus. Our experiments are carried out on the English-Slovene language pair but as the methods are totally data-driven, the approach can be easily applied to other languages. Introduction Large-scale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications. Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for low-resourced languages, and to complement and reduce the sparseness of existing resources (Munteanu and Marcu, 2005; Snover et al., 2008). The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). So, in order to identify a translation correspondence between the two languages, the contexts of the source word and the candidate translation have to be compared. For this comparison to take place, the same vector space has to be produced, which means that the vectors of the one language have to be translated The paper is organized as follows: In the next section, we present some related work on bilingu"
W13-2501,J03-1002,0,0.00375309,"processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeTagger (for English) and ToTaLe (for Slovene) (Erjavec et al., 2010). Next, the corpus is word-aligned with GIZA++ (Och and Ney, 2003) and two bilingual lexicons are extracted, one for each translation direction (EN–SL/SL– EN). To clean the lexicons from noisy alignments, the translations are filtered on the basis of their alignment score and PoS, keeping only translations that pertain to the same grammatical category as the source word. We retain only intersecting alignments and use for clustering translations that translate a source word more than 10 times in the training corpus. This threshold reduces data sparseness issues that affect the clustering and eliminates erroneous word alignments. The filtered EN-SL lexicon con"
W13-2501,erjavec-etal-2010-jos,1,0.876362,"Missing"
W13-2501,2007.mtsummit-papers.26,0,0.0099246,"usters permits to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploit"
W13-2501,2009.mtsummit-posters.14,0,0.0248424,"d most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing diction"
W13-2501,R11-1018,1,0.902304,"Missing"
W13-2501,P99-1067,0,0.343625,"sily applied to other languages. Introduction Large-scale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications. Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for low-resourced languages, and to complement and reduce the sparseness of existing resources (Munteanu and Marcu, 2005; Snover et al., 2008). The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). So, in order to identify a translation correspondence between the two languages, the contexts of the source word and the candidate translation have to be compared. For this comparison to take place, the same vector space has to be produced, which means that the vectors of the one language have to be translated The paper is organized as follows: In the next section, we present some related work on bilingual lexicon extraction from comparable corpora. Section 3 presents the data used in our experiments and Section 4 provides details on the approach and the experimental setup. In Section 5, we"
W13-2501,C04-1089,0,0.0319701,"sing translation clusters permits to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality o"
W13-2501,N09-2031,0,0.0211288,"s to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resour"
W13-2501,D08-1090,0,0.033079,"Missing"
W13-2501,steinberger-etal-2006-jrc,0,0.0167182,"able corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in which polysemy is more prominent. 3.2 Parallel corpus The parallel corpus used for clustering and word sense induction consists of the Slovene-English parts of Europarl (release v6) (Koehn, 2005) and of JRC-Acquis (Steinberger et al., 2006) and amounts to approximately 35M words per language. A number of pre-processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeTagger (for English) and ToTaLe (for Slovene)"
W15-1006,S15-2050,1,0.768768,"Missing"
W15-1006,apidianaki-etal-2014-semantic,1,0.897891,"Missing"
W15-1006,W05-0909,0,0.33414,"present an initial experiment on the integration of a disambiguation step in the M ETEOR metric and show how it helps increase correlation with human judgments of translation quality. We present an initial experiment in integrating a disambiguation step in MT evaluation. We show that accounting for sense distinctions helps M ETEOR establish better sense correspondences and improves its correlation with human judgments of translation quality. 1 Introduction Synonym and paraphrase support are useful means for capturing lexical variation in Machine Translation evaluation. In the M ETEOR metric (Banerjee and Lavie, 2005), some level of abstraction from the surface forms of words is achieved through the “stem” and “synonymy” modules which map words with the same stem or belonging to the same WordNet synset (Fellbaum, 1998). M ETEOR NEXT (Denkowski and Lavie, 2010) extends semantic mapping to languages other than English and to longer text segments, using the paraphrase tables constructed by the pivot method (Bannard and Callison-Burch, 2005). Although both metrics yield improvements regarding correlation with human judgments of translation quality compared to the standard M ETEOR configuration for English, the"
W15-1006,P05-1074,0,0.125229,"ts of translation quality. 1 Introduction Synonym and paraphrase support are useful means for capturing lexical variation in Machine Translation evaluation. In the M ETEOR metric (Banerjee and Lavie, 2005), some level of abstraction from the surface forms of words is achieved through the “stem” and “synonymy” modules which map words with the same stem or belonging to the same WordNet synset (Fellbaum, 1998). M ETEOR NEXT (Denkowski and Lavie, 2010) extends semantic mapping to languages other than English and to longer text segments, using the paraphrase tables constructed by the pivot method (Bannard and Callison-Burch, 2005). Although both metrics yield improvements regarding correlation with human judgments of translation quality compared to the standard M ETEOR configuration for English, they integrate semantic information in a rather simplistic way: matching is performed without disambiguation, which means that all the variants available for a particular text fragment are treated as semantically equivalent. This is however not always the case, as synonyms found in different WordNet synsets correspond to different senses. Similarly, paraphrase sets obtained by the pivot method of2 Disambiguation in M ETEOR We a"
W15-1006,N10-1031,0,0.586589,"ion step in MT evaluation. We show that accounting for sense distinctions helps M ETEOR establish better sense correspondences and improves its correlation with human judgments of translation quality. 1 Introduction Synonym and paraphrase support are useful means for capturing lexical variation in Machine Translation evaluation. In the M ETEOR metric (Banerjee and Lavie, 2005), some level of abstraction from the surface forms of words is achieved through the “stem” and “synonymy” modules which map words with the same stem or belonging to the same WordNet synset (Fellbaum, 1998). M ETEOR NEXT (Denkowski and Lavie, 2010) extends semantic mapping to languages other than English and to longer text segments, using the paraphrase tables constructed by the pivot method (Bannard and Callison-Burch, 2005). Although both metrics yield improvements regarding correlation with human judgments of translation quality compared to the standard M ETEOR configuration for English, they integrate semantic information in a rather simplistic way: matching is performed without disambiguation, which means that all the variants available for a particular text fragment are treated as semantically equivalent. This is however not alway"
W15-1006,W14-3336,0,0.0675827,"ared to the standard M ETEOR configuration for English, they integrate semantic information in a rather simplistic way: matching is performed without disambiguation, which means that all the variants available for a particular text fragment are treated as semantically equivalent. This is however not always the case, as synonyms found in different WordNet synsets correspond to different senses. Similarly, paraphrase sets obtained by the pivot method of2 Disambiguation in M ETEOR We apply the metric to translations of news texts from the five languages involved in the WMT14 Metrics Shared Task (Machacek and Bojar, 2014) (French, Hindi, German, Czech, Russian) into English. We disambiguate the English references – different for each language pair – using the Babelfy tool (Moro et al., 2014), which performs graphbased WSD by exploiting the structure of the multilingual network BabelNet (Navigli and Ponzetto, 2012). The assigned annotations are multilingual synsets grouping word and phrase variants in different languages coming from various sources (WordNet, Wikipedia, etc.) and carrying the same sense. We use the WordNet literals found in the sense selected by Babelfy to filter the WordNet synonym sets used in"
W15-1006,Q14-1019,0,0.0974565,"at all the variants available for a particular text fragment are treated as semantically equivalent. This is however not always the case, as synonyms found in different WordNet synsets correspond to different senses. Similarly, paraphrase sets obtained by the pivot method of2 Disambiguation in M ETEOR We apply the metric to translations of news texts from the five languages involved in the WMT14 Metrics Shared Task (Machacek and Bojar, 2014) (French, Hindi, German, Czech, Russian) into English. We disambiguate the English references – different for each language pair – using the Babelfy tool (Moro et al., 2014), which performs graphbased WSD by exploiting the structure of the multilingual network BabelNet (Navigli and Ponzetto, 2012). The assigned annotations are multilingual synsets grouping word and phrase variants in different languages coming from various sources (WordNet, Wikipedia, etc.) and carrying the same sense. We use the WordNet literals found in the sense selected by Babelfy to filter the WordNet synonym sets used in M ETEOR and prevent M ETEOR from considering erroneous matchings as correct.1 As a result, only the synonyms found in the proposed BabelNet synset are kept and considered a"
W15-3048,S15-2050,1,0.728125,"lter the WordNet synonym set used by the basic Meteor configuration in order to keep only variants that were good in this specific context and discard the ones corresponding to other senses. The reported MT evaluation results showed the beneficial impact of disambiguation which improved the correlation of the metric to human judgments from almost all languages involved in the WMT14 evaluation into English (except for Czech-English). Naturally, performance strongly depends on the quality of the WSD annotations. In this work, we use a recent version of the alignment-based WSD method proposed by Apidianaki and Gong (2015) which gives better disambiguation results than Babelfy on the WMT14 data. Disambiguation is now applied to references of all languages in the data, not only in English. The WSD method used in our experiments still relies on alignments but implements a mechanism that improves WSD in languages other than English compared to the previous version. More precisely, Apidianaki and Gong (2015) showed that the problematic sorting performed by the default BabelNet sense ranking mechanism in languages other than English has a strong negative impact on WSD. 2 In our experiments, we implement an alternati"
W15-3048,W15-1006,1,0.859905,"on quality but they fail to account for important semantics-related aspects. For example, Meteor and Meteor-NEXT treat all the variants available for a particular text fragment in WordNet (Fellbaum, 1998) or a pivot paraphrase database (Bannard and Callison-Burch, 2005) as semantically equivalent. Consequently, erroneous matches can be made by mapping synonyms found in different WordNet synsets and describing different senses. Similarly, pivot paraphrase sets 2 2.1 Meteor-WSD Context-dependent sense selection A first attempt to integrate context-based sense selection in Meteor is described in Apidianaki and Marie (2015). Word sense disambiguation (WSD) was performed using the Babelfy tool (Moro et al., 2014) which relies on the multilingual resource BabelNet (Navigli and Ponzetto, 2012). BabelNet is a wide coverage semantic network where senses are described by synsets (synonym and paraphrase sets) containing lexicographic and encyclopedic knowledge extracted from various sources in many languages and are linked between them by different types of relations. Depending on the language, the lexical and phrase variants available in the synsets come from different sources such as WordNet, Wikipedia, Wiktionary, O"
W15-3048,apidianaki-etal-2014-semantic,1,0.882686,"Missing"
W15-3048,W05-0909,0,0.108785,"n alignments and gives credit to semantically adequate translations in context. We show that context-sensitive synonym selection increases the correlation of the Meteor metric with human judgments of translation quality on the WMT14 data. RATATOUILLE combines MeteorWSD with nine other metrics for evaluation and outperforms the best metric (BEER) involved in its computation. 1 Marianna Apidianaki LIMSI-CNRS, Orsay, France marianna@limsi.fr Introduction The Meteor metric evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). The space of possible alignments for a hypothesis-reference pair is constructed by identifying all possible matches between the sentences according to different matchers mapping words with identical surface forms or having the same stem, WordNet synonyms and paraphrases. These modules add flexibility to the metric and improve its correlation with human judgments of translation quality but they fail to account for important semantics-related aspects. For example, Meteor and Meteor-NEXT treat all the variants available for a particular text fragment in WordNet (Fell"
W15-3048,P05-1074,0,0.126635,"ossible alignments for a hypothesis-reference pair is constructed by identifying all possible matches between the sentences according to different matchers mapping words with identical surface forms or having the same stem, WordNet synonyms and paraphrases. These modules add flexibility to the metric and improve its correlation with human judgments of translation quality but they fail to account for important semantics-related aspects. For example, Meteor and Meteor-NEXT treat all the variants available for a particular text fragment in WordNet (Fellbaum, 1998) or a pivot paraphrase database (Bannard and Callison-Burch, 2005) as semantically equivalent. Consequently, erroneous matches can be made by mapping synonyms found in different WordNet synsets and describing different senses. Similarly, pivot paraphrase sets 2 2.1 Meteor-WSD Context-dependent sense selection A first attempt to integrate context-based sense selection in Meteor is described in Apidianaki and Marie (2015). Word sense disambiguation (WSD) was performed using the Babelfy tool (Moro et al., 2014) which relies on the multilingual resource BabelNet (Navigli and Ponzetto, 2012). BabelNet is a wide coverage semantic network where senses are described"
W15-3048,N10-1031,0,0.0190254,"dit to semantically adequate translations in context. We show that context-sensitive synonym selection increases the correlation of the Meteor metric with human judgments of translation quality on the WMT14 data. RATATOUILLE combines MeteorWSD with nine other metrics for evaluation and outperforms the best metric (BEER) involved in its computation. 1 Marianna Apidianaki LIMSI-CNRS, Orsay, France marianna@limsi.fr Introduction The Meteor metric evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). The space of possible alignments for a hypothesis-reference pair is constructed by identifying all possible matches between the sentences according to different matchers mapping words with identical surface forms or having the same stem, WordNet synonyms and paraphrases. These modules add flexibility to the metric and improve its correlation with human judgments of translation quality but they fail to account for important semantics-related aspects. For example, Meteor and Meteor-NEXT treat all the variants available for a particular text fragment in WordNet (Fellbaum, 1998) or a pivot parap"
W15-3048,R13-1030,0,0.0510605,"Missing"
W15-3048,P14-1065,0,0.168028,"Missing"
W15-3048,D11-1125,0,0.0125191,"eft after disambiguation in languages other than English are useful and help to improve the correlation with human judgments. Table 2 presents our results at the system-level. As for the segment-level task, Meteor-WSD performs better than Meteor for almost all language pairs, with a significant improvement of .023 for the ru-en language pair. 3 3.2 Each metric of the combination gives a score for the evaluated segment. The score computed by RATATOUILLE is the result of the log-linear combination of each metric’s score. The weight for each metric score is tuned using a similar approach to PRO (Hopkins and May, 2011), already used by Guzm´an et al. (2014) in the context of metric combination evaluation. In this pairwise approach, candidate translation pairs are classified into two categories: correctly or incorrectly ordered, reducing the tuning to a binary classification problem. We studied two configurations, retaining all possible translation pairs or only pairs including translations separated by at least three ranks in the human judgments. We follow PRO which uses only pairs of translations of significant different quality and does not learn to tease apart translations of similar quality. Translation"
W15-3048,2006.amta-papers.25,0,0.138868,"Missing"
W15-3048,W14-3352,0,0.0259585,"Missing"
W15-3048,W14-3354,0,0.041145,"Missing"
W15-3048,P14-5003,0,0.0434718,"Missing"
W15-3048,P07-2045,0,0.00428872,"metric.11 The classifier used is a MaxEnt from the scikit-learn python library (Pedregosa et al., 2011). A Metric Combination: RATATOUILLE 3.1 Tuning The Metrics RATATOUILLE is a metric combination involving ten metrics mainly dedicated to segmentlevel evaluation: PER, WER, CDER (Leusch et al., 2006), TER (Snover et al., 2006), GTM 1.3 (Melamed et al., 2003), sentencelevel BLEU, Meteor 1.5, Meteor-WSD, RIBES 1.03.1 (Echizen’ya et al., 2013) and BEER 1.0 (Stanojevi´c and Sima’an, 2014). For the metrics PER, WER, CDER, TER and sentence-level BLEU we used the implementations available in M OSES (Koehn et al., 2007). For the metrics RIBES8 and BEER9 we used the implementations published by their authors, and the implementa7 As the synonymy module has no pre-defined weight for such translation directions, we tuned its weight on the WMT13 human judgments for each translation direction, searching empirically for the best weight between 0 and 1 with a 0.2 step size. 8 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html 9 https://github.com/stanojevic/beer/ 10 http://nlp.lsi.upc.edu/asiya/ For the fi-en language pair in the WMT15 metrics task, we used translation pairs from xx-en to tune the metric for fi-en"
W15-3048,E06-1031,0,0.0836172,"Missing"
W15-3048,N03-2021,0,0.0993474,"Missing"
W15-3048,Q14-1019,0,0.0394834,"d Meteor-NEXT treat all the variants available for a particular text fragment in WordNet (Fellbaum, 1998) or a pivot paraphrase database (Bannard and Callison-Burch, 2005) as semantically equivalent. Consequently, erroneous matches can be made by mapping synonyms found in different WordNet synsets and describing different senses. Similarly, pivot paraphrase sets 2 2.1 Meteor-WSD Context-dependent sense selection A first attempt to integrate context-based sense selection in Meteor is described in Apidianaki and Marie (2015). Word sense disambiguation (WSD) was performed using the Babelfy tool (Moro et al., 2014) which relies on the multilingual resource BabelNet (Navigli and Ponzetto, 2012). BabelNet is a wide coverage semantic network where senses are described by synsets (synonym and paraphrase sets) containing lexicographic and encyclopedic knowledge extracted from various sources in many languages and are linked between them by different types of relations. Depending on the language, the lexical and phrase variants available in the synsets come from different sources such as WordNet, Wikipedia, Wiktionary, OmegaWiki as well as Machine Translation output. The Babelfy tool jointly performs WSD and"
W15-3048,J03-1002,0,0.0135328,"ous version. More precisely, Apidianaki and Gong (2015) showed that the problematic sorting performed by the default BabelNet sense ranking mechanism in languages other than English has a strong negative impact on WSD. 2 In our experiments, we implement an alternative solution that eliminates the need for sense ranking. Furthermore, the currently used version integrates a multiword expression (MWE) identification step prior to disambiguation. (Schmid, 1994), except for Czech where the MorphoDiTa tool (Strakov´a et al., 2014) is used. The texts are then aligned at the lemma level using GIZA++ (Och and Ney, 2003). 2.3 Alignment-based MWE extraction We identify candidate multiword expressions in the reference texts prior to disambiguation using word alignments and filter them using information in the BabelNet resource (version 2.5).3 We consider as a candidate MWE a sequence of words in one language that is aligned to a single word in the other language (a n : 1 alignment).4 For example, t´el´ephone portable is considered as a candidate French MWE because both its parts are aligned to cellphone. We validate a candidate MWE if it constitutes a separate entry in the BabelNet resource either in its lemmat"
W17-1914,D16-1215,1,0.589816,"he rankings provided by two sense-agnostic, vector-based lexical substitution models. Lexical substitution requires systems to predict substitutes for target word instances that preserve their meaning in context (McCarthy and Navigli, 2007). We consider a sense inventory with high substitutability to be one which groups synonyms or paraphrases that are mutually-interchangeable in the same contexts. In contrast, sense inventories with low substitutability might group words linked by different types of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further d"
W17-1914,N15-1059,0,0.0611884,"Missing"
W17-1914,N16-1172,1,0.914477,"Applications, pages 110–119, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics 2 2.1 Sentence In this world, one’s word is a promise. Silverplate: code word for the historic mission that would end World War II. I think she only heard the last words of my speech. A sense inventory for substitution Paraphrase substitutability The candidate substitutes used by our ranking models come from the Paraphrase Database (PPDB) XXL package (Ganitkevitch et al., 2013).1 Paraphrase relations in the PPDB are defined between words and phrases which might carry different senses. Cocos and Callison-Burch (2016) used a spectral clustering algorithm to cluster PPDB XXL into senses, but the clusters contain noisy paraphrases and paraphrases linked by different types of relations (e.g. hypernyms, antonyms) which are not always substitutable. We use a slightly modified version of their method to cluster paraphrases where both the number of clusters (senses) and their contents are optimized for substitutability. 2.2 Annotated Substitutes (Count) vow (1), utterance (1), tongue (1), speech (1) phrase (3), term (2), verbiage(1), utterance (1), signal (1), name (1), dictate (1), designation (1), decree (1) bi"
W17-1914,J13-3003,0,0.0210365,"pes of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further doubt on the usefulness of word sense. Why bother to identify senses if even humans cannot agree upon their nature and number, and if simple word-embedding models yield good results without using any explicit sense representation? Word-based models are successful in various semantic tasks even though they conflate multiple word meanings into a single representation. Based on the hypothesis that capturing polysemy could further improve their performance, several works have focused on creating sen"
W17-1914,N15-1184,0,0.027754,"tances that preserve their meaning in context (McCarthy and Navigli, 2007). We consider a sense inventory with high substitutability to be one which groups synonyms or paraphrases that are mutually-interchangeable in the same contexts. In contrast, sense inventories with low substitutability might group words linked by different types of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further doubt on the usefulness of word sense. Why bother to identify senses if even humans cannot agree upon their nature and number, and if simple word-embedding models yield go"
W17-1914,N13-1092,1,0.871474,"with the clustered sense before learning embed110 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 110–119, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics 2 2.1 Sentence In this world, one’s word is a promise. Silverplate: code word for the historic mission that would end World War II. I think she only heard the last words of my speech. A sense inventory for substitution Paraphrase substitutability The candidate substitutes used by our ranking models come from the Paraphrase Database (PPDB) XXL package (Ganitkevitch et al., 2013).1 Paraphrase relations in the PPDB are defined between words and phrases which might carry different senses. Cocos and Callison-Burch (2016) used a spectral clustering algorithm to cluster PPDB XXL into senses, but the clusters contain noisy paraphrases and paraphrases linked by different types of relations (e.g. hypernyms, antonyms) which are not always substitutable. We use a slightly modified version of their method to cluster paraphrases where both the number of clusters (senses) and their contents are optimized for substitutability. 2.2 Annotated Substitutes (Count) vow (1), utterance (1"
W17-1914,P12-1092,0,0.14302,"Missing"
W17-1914,P15-1010,0,0.107934,"Missing"
W17-1914,N10-1013,0,0.105026,"Missing"
W17-1914,E14-1057,0,0.132413,"Missing"
W17-1914,I11-1127,0,0.218111,"Missing"
W17-1914,D15-1200,0,0.0216592,"uter and Information Science Department, University of Pennsylvania † LIMSI, CNRS, Universit´e Paris-Saclay, 91403 Orsay {acocos,marapi,ccb}@seas.upenn.edu Abstract dings (Reisinger and Mooney, 2010; Huang et al., 2012). Iacobacci et al. (2015) disambiguate the words in a corpus using a state-of-the-art WSD system and then produce continuous representations of word senses based on distributional information obtained from the annotated corpus. Moving from word to sense embeddings generally improves their performance in word and relational similarity tasks but is not beneficial in all settings. Li and Jurafsky (2015) show that although multisense embeddings give improved performance in tasks such as semantic similarity, semantic relation identification and part-of-speech tagging, they fail to help in others, like sentiment analysis and named entity extraction (Li and Jurafsky, 2015). The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for sense. We show that a filtering mechanism based on a sense inventory optimized for substitutability can improve the results of th"
W17-1914,de-marneffe-etal-2006-generating,0,0.0126458,"Missing"
W17-1914,S07-1009,0,0.212128,"Missing"
W17-1914,W15-1501,0,0.489592,"tution models. Lexical substitution requires systems to predict substitutes for target word instances that preserve their meaning in context (McCarthy and Navigli, 2007). We consider a sense inventory with high substitutability to be one which groups synonyms or paraphrases that are mutually-interchangeable in the same contexts. In contrast, sense inventories with low substitutability might group words linked by different types of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further doubt on the usefulness of word sense. Why bother to identify senses if even"
W17-1914,W12-3018,0,0.200101,"Missing"
W17-1914,P15-2070,1,0.884108,"Missing"
W19-0423,N18-2077,1,0.831184,"o. The Add measure (equation (1), hereafter called AddCos because of the Cosine function applied to the vector representations of words and contexts) estimates the substitutability of a candidate substitute s of the target word t in context C, where C corresponds to the set of the target word’s context elements in the sentence, and c corresponds to an individual context element. P cos(s, t) + c∈C cos(s, c) AddCos(t, s, C) = (1) |C |+ 1 The vectors used by the original method are syntax-based embeddings created with word2vecf (Levy and Goldberg, 2014). We use the lighter adaptation proposed by Apidianaki et al. (2018) which circumvents the need for syntactic analysis, and use 300-dimensional skip-gram word and context embeddings trained on the 4B words of the Annotated Gigaword corpus (Napoles et al., 2012). We apply the AddCos method to ELMo as well as to FC-ELMo embeddings. When using standard ELMo embeddings, the target and context word representations of a sentence are their corresponding ELMo vector, and the vector of a candidate substitute is obtained by substituting the target word by the candidate in the sentence, as described in Section 4.1. To adapt this to FC-ELMo embeddings, substitute represen"
W19-0423,P05-1074,1,0.601944,"ormation into our LexSub models, where senses are defined at the level of substitute paraphrases, can improve performance. For this purpose, we generate a dataset of “focused contexts” (hereafter abbreviated FC) for each target word which are specifically chosen to represent the specific sense that target word shares with each of its potential substitutes. The starting point for our focused contexts dataset is the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015), a collection of over 80M English paraphrase pairs. PPDB was automatically built using the pivot method (Bannard and Callison-Burch, 2005), which discovers same-language paraphrases by ‘pivoting’ over bilingual parallel corpora. Specifically, if two English phrases such as “under control” and “in check” are each translated to the same German phrase “unter kontrolle” in some contexts, then this is taken as evidence that “under control” and “in check” have approximately similar meaning. Because PPDB was constructed using the pivot method, it follows that each paraphrase pair x ↔ y in PPDB has a set of shared foreign translations. This idea is core to the method for extracting substitute-focused sentences. The sentences for paraphr"
W19-0423,P16-1191,0,0.0223826,"stitution task and compare their performance to the contextsensitive models of Melamud et al. (2015) and Melamud et al. (2016). We also propose a way to tune the ELMo representations to the LexSub task, by using a dataset containing a high number of sentences for words in context that represent meanings close to that of their possible substitutes. 3 Substitute-focused Contexts Contextualized word embeddings for a given target word vary based on the sense of a target word instance. Unlike the variation in discrete sense-level embeddings (e.g. Iacobacci et al. (2015); Rothe and Sch¨utze (2015); Flekova and Gurevych (2016), and others), this variation is continuous. One of our experiments aims to see whether incorporating discrete fine-grained sense information into our LexSub models, where senses are defined at the level of substitute paraphrases, can improve performance. For this purpose, we generate a dataset of “focused contexts” (hereafter abbreviated FC) for each target word which are specifically chosen to represent the specific sense that target word shares with each of its potential substitutes. The starting point for our focused contexts dataset is the Paraphrase Database (PPDB) (Ganitkevitch et al.,"
W19-0423,N13-1092,1,0.863493,"Missing"
W19-0423,P15-1010,0,0.0239021,"e ELMo vectors for the first time to the lexical substitution task and compare their performance to the contextsensitive models of Melamud et al. (2015) and Melamud et al. (2016). We also propose a way to tune the ELMo representations to the LexSub task, by using a dataset containing a high number of sentences for words in context that represent meanings close to that of their possible substitutes. 3 Substitute-focused Contexts Contextualized word embeddings for a given target word vary based on the sense of a target word instance. Unlike the variation in discrete sense-level embeddings (e.g. Iacobacci et al. (2015); Rothe and Sch¨utze (2015); Flekova and Gurevych (2016), and others), this variation is continuous. One of our experiments aims to see whether incorporating discrete fine-grained sense information into our LexSub models, where senses are defined at the level of substitute paraphrases, can improve performance. For this purpose, we generate a dataset of “focused contexts” (hereafter abbreviated FC) for each target word which are specifically chosen to represent the specific sense that target word shares with each of its potential substitutes. The starting point for our focused contexts dataset"
W19-0423,E14-1057,0,0.269398,"Missing"
W19-0423,P14-2050,0,0.0459,"can yield a high score even if one of the elements in the sum is zero. The Add measure (equation (1), hereafter called AddCos because of the Cosine function applied to the vector representations of words and contexts) estimates the substitutability of a candidate substitute s of the target word t in context C, where C corresponds to the set of the target word’s context elements in the sentence, and c corresponds to an individual context element. P cos(s, t) + c∈C cos(s, c) AddCos(t, s, C) = (1) |C |+ 1 The vectors used by the original method are syntax-based embeddings created with word2vecf (Levy and Goldberg, 2014). We use the lighter adaptation proposed by Apidianaki et al. (2018) which circumvents the need for syntactic analysis, and use 300-dimensional skip-gram word and context embeddings trained on the 4B words of the Annotated Gigaword corpus (Napoles et al., 2012). We apply the AddCos method to ELMo as well as to FC-ELMo embeddings. When using standard ELMo embeddings, the target and context word representations of a sentence are their corresponding ELMo vector, and the vector of a candidate substitute is obtained by substituting the target word by the candidate in the sentence, as described in S"
W19-0423,S07-1009,0,0.536058,"esentations is explicitly modeled during training. 1 Introduction Contextualized word representations model complex characteristics of word usage, and give state-of-theart performance in a variety of NLP tasks involving syntactic and semantic processing. Each proposed model accounts for context in a different way depending on the underlying architecture, and might account for local or long-distance phenomena. In this work, we compare different word representations on the lexical substitution (LexSub) task, which involves proposing meaning-preserving substitutes for words in specific contexts (McCarthy and Navigli, 2007). The importance of context in defining the meaning of word instances, and selecting the substitutes that best fit specific sentences, makes of the LexSub task an ideal testbed for a direct comparison of the contextualized representations built by different models. We compare representations that model context in different ways: they exploit context embeddings generated within the skip-gram model (Melamud et al., 2015), learn a generic context embedding function using a bidirectional Long Short-Term Memory (LSTM) network (Melamud et al., 2016), or use vectors that are learned functions of the"
W19-0423,K16-1006,0,0.107648,"substitutes for words in specific contexts (McCarthy and Navigli, 2007). The importance of context in defining the meaning of word instances, and selecting the substitutes that best fit specific sentences, makes of the LexSub task an ideal testbed for a direct comparison of the contextualized representations built by different models. We compare representations that model context in different ways: they exploit context embeddings generated within the skip-gram model (Melamud et al., 2015), learn a generic context embedding function using a bidirectional Long Short-Term Memory (LSTM) network (Melamud et al., 2016), or use vectors that are learned functions of the internal states of a deep bidirectional language model (biLM) (Peters et al., 2018a). Additionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub task. Finally, we compare the performance of contextualized models to baseline models that exploit standard word embedding represe"
W19-0423,W15-1501,0,0.46586,"e compare different word representations on the lexical substitution (LexSub) task, which involves proposing meaning-preserving substitutes for words in specific contexts (McCarthy and Navigli, 2007). The importance of context in defining the meaning of word instances, and selecting the substitutes that best fit specific sentences, makes of the LexSub task an ideal testbed for a direct comparison of the contextualized representations built by different models. We compare representations that model context in different ways: they exploit context embeddings generated within the skip-gram model (Melamud et al., 2015), learn a generic context embedding function using a bidirectional Long Short-Term Memory (LSTM) network (Melamud et al., 2016), or use vectors that are learned functions of the internal states of a deep bidirectional language model (biLM) (Peters et al., 2018a). Additionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub tas"
W19-0423,L18-1008,0,0.380285,"dditionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub task. Finally, we compare the performance of contextualized models to baseline models that exploit standard word embedding representations for measuring semantic similarity without directly accounting for context, such as Glove (Pennington et al., 2014) and FastText (Mikolov et al., 2018). The results of this study highlight the importance of the architecture used for model training in capturing information relevant for lexical substitution. We show that contextualized representations that Substitutes Sentences shoot (5) The panther fired at the bridge and hit a truck. While both he and the White House deny he was fired, Frum is so insistent on the fact that he quit on his own that it really makes you wonder. As a coach, we speak and listen with the intent of helping people surface, question and reframe assumptions. We hopped back onto the coach - now for the boulangerie! sack"
W19-0423,W12-3018,0,0.027377,"Missing"
W19-0423,P15-2070,1,0.90961,"Missing"
W19-0423,D14-1162,0,0.1156,"model (biLM) (Peters et al., 2018a). Additionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub task. Finally, we compare the performance of contextualized models to baseline models that exploit standard word embedding representations for measuring semantic similarity without directly accounting for context, such as Glove (Pennington et al., 2014) and FastText (Mikolov et al., 2018). The results of this study highlight the importance of the architecture used for model training in capturing information relevant for lexical substitution. We show that contextualized representations that Substitutes Sentences shoot (5) The panther fired at the bridge and hit a truck. While both he and the White House deny he was fired, Frum is so insistent on the fact that he quit on his own that it really makes you wonder. As a coach, we speak and listen with the intent of helping people surface, question and reframe assumptions. We hopped back onto the c"
W19-0423,N18-1202,0,0.141718,"Missing"
W19-0423,D18-1179,0,0.0528649,"Missing"
W19-0423,P15-1173,0,0.0489999,"Missing"
W19-0423,P10-1097,0,0.0706147,"Missing"
W19-5032,D14-1179,0,0.0078856,"Missing"
W19-5032,D17-1070,0,0.0136659,". . . , wn i, a bidirectional GRU (BIGRU) computes two sets of n hidden state vectors, one for each direction. These two sets are then added to form the output H of the BIGRU: Hf = GRUf (e(w1 ), . . . , e(wn )) (4) Hb = GRUb (e(w1 ), . . . , e(wn )) (5) H = Hf + Hb (6) where f , b denote the forward and backward directions, and + indicates component-wise addition. We add residual connections (He et al., 2015) from each word embedding e(wt ) to the corresponding hidden state ht of H. Instead of using the final forward and backward states of H, we apply max-pooling (Collobert and Weston, 2008; Conneau et al., 2017) over the state vectors ht of H. The output of the max pooling is the node embedding f (v). Figure 3 illustrates this method. Additional experiments were conducted with several variants of the last encoder. A unidirectional GRU instead of a BIGRU, and a BIGRU with 301 Node Embedding f(v): X X ..... X ..... X Max-Pooling X X X X + + + + h1 h2 h3 h4 h1 h2 h3 h4 e1 e2 e3 e4 lumen of arterial trunk Node: Statistics Nodes Edges Training true positive edges Training true negative edges Test true positive edges Test true negative edges Avg. descriptor length Max. descriptor length 16,894 19,436 16,89"
W19-5032,D18-1211,1,0.879754,"Missing"
W19-5032,N19-1221,0,0.0218948,"embeddings from texts and network structure; and unlike WANE, we do not align the descriptors of different nodes. We generate the embedding of each node from the word embeddings of its descriptor via the RNN (Fig. 1), but the parameters of the RNN, the word embeddings, hence also the node embeddings are updated during training to predict NODE 2 VEC’s neighborhoods. Although we use NODE 2 VEC to incorporate network context in the node embeddings, other neighborhood embedding methods, such as GCNs, could easily be used too. Similarly, text encoders other than RNNs could be applied. For example, Mishra et al. (2019) try to detect abusive language in tweets with a semi-supervised learning approach based on GCNs. They exploit the network structure and also the labels associated with the tweets, taking into account the linguistic behavior of the authors. 3 Proposed Node Embedding Approach Consider a network (graph) G = hV, E, Si, where V is the set of nodes (vertices); E ⊆ V × V is the set of edges (links) between nodes; and S is a function that maps each node v ∈ V to its textual descriptor S(v) = hw1 , w2 , . . . , wn i, where n is the word length of the descriptor, and each word wi comes from a vocabular"
W19-5032,D18-1209,0,0.0655153,"inoma acute lymphocytic leukemia Figure 1: Example network with nodes associated with textual descriptors. a) A model where each node is represented by a vector (node embedding) from a look-up table. b) A model where each node embedding is generated compositionally from the word embeddings of its descriptor via an RNN. The latter model can learn node embeddings from both the network structure and the word sequences of the textual descriptors. as textual descriptors (labels) or other meta-data associated with the nodes. More recent NE methods, e.g., CANE (Tu et al., 1 Introduction 2017), WANE (Shen et al., 2018), produce embeddings by combining the network structure and the Network Embedding (NE) methods map each text associated with the nodes. These contentnode of a network to an embedding, meaning a oriented methods embed networks whose nodes low-dimensional feature vector. They are highly are rich textual objects (often whole documents). effective in network analysis tasks involving preThey aim to capture the compositionality and sedictions over nodes and edges, for example link mantic similarities in the text, encoding them with prediction (Lu and Zhou, 2010), and node classideep learning methods"
W19-5032,P17-1158,0,0.0516818,"al. (2003) strengthen Content-Aware-N2V 299 the argument of compositionality by observing that many GO terms contain other GO terms. Also, they argue that substrings that are not GO terms appear frequently and often indicate semantic relationships. Ogren et al. (2004) use finite state automata to represent GO terms and demonstrate how small conceptual changes can create biologically meaningful candidate terms. In other work on NE methods, CENE (Sun et al., 2016) treats textual descriptors as a special kind of node, and uses bidirectional recurrent neural networks (RNNs) to encode them. CANE (Tu et al., 2017) learns two embeddings per node, a textbased one and an embedding based on network structure. The text-based one changes when interacting with different neighbors, using a mutual attention mechanism. WANE (Shen et al., 2018) also uses two types of node embeddings, text-based and structure-based. For the text-based embeddings, it matches important words across the textual descriptors of different nodes, and aggregates the resulting alignment features. In spite of performance improvements over structure-oriented approaches, these content-aware methods do not thoroughly explore the network struct"
W19-5802,D18-2029,0,0.0701647,"Missing"
W19-5802,P09-1002,0,0.175208,"learning, USE improves performance on different NLP tasks at the sentence and phrase level (e.g. sentiment analysis). We use the Deep Averaging Network (DAN) encoder,5 where input word and bigram embeddings are averaged and fed through a feedforward neural network, to create embeddings for WiC sentences. 3 http://u.cs.biu.ac.il/ ˜nlp/resources/ downloads/context2vec/ 4 The medium-sized model at https://allennlp. org/elmo. 5 https://tfhub.dev/google/ universal-sentence-encoder/2 4 Automatic Substitution Manual substitute annotations have been useful for in-context usage similarity estimation (Erk et al., 2009; McCarthy et al., 2016). The idea is that a high proportion of shared substitutes between two word instances reflects their semantic similarity.6 Extending previous work where manual substitute annotations were used to estimate usage similarity (Erk et al., 2009), we automatically annotate WiC instances with substitutes, and use features based on their overlap for our classifier. We use the context2vec method for automatic lexical substitution (Melamud et al., 2016). Given a sentence with a new instance of a target word t, and a set of candidate substitutes for the word (S = s1 , s2 , ..., sn"
W19-5802,ide-etal-2008-masc,0,0.0175976,"the verb drink (gold label: F) with automatic substitute annotations assigned by context2vec. Substitutes in italics were discarded after filtering. • GAP score: GAP (Generalized Average Precision) considers the order of ranked elements and their weights (Kishida, 2005). GAP score ranges from 0 to 1 (for perfect disagreement/agreement). We take the average score between the rankings produced for a sentence pair in both directions (GAP(R1 , R2 ) and GAP(R2 , R1 )). Weights are the scores assigned to the substitutes by context2vec. We use the GAP implementation shared by Melamud et al. (2015). (Ide et al., 2008) which contains manual substitute annotations for all content words in a sentence. We use a balanced collection of similar (T ) and dissimilar (F) sentence pairs from CoInCo, with labels automatically assigned based on substitute overlap (Gar´ı Soler et al., 2019).10 We apply the automatic substitution method described in Section 4, and extract substitute- and embeddingbased features to be used by our models. • Substitute cosine similarity. We form pairs of substitutes from R1 and R2 , and calculate the average of their GloVe cosine similarities. This feature accounts for the semantic similari"
W19-5802,E14-1057,0,0.0569321,"Missing"
W19-5802,J16-2003,1,0.936362,"roves performance on different NLP tasks at the sentence and phrase level (e.g. sentiment analysis). We use the Deep Averaging Network (DAN) encoder,5 where input word and bigram embeddings are averaged and fed through a feedforward neural network, to create embeddings for WiC sentences. 3 http://u.cs.biu.ac.il/ ˜nlp/resources/ downloads/context2vec/ 4 The medium-sized model at https://allennlp. org/elmo. 5 https://tfhub.dev/google/ universal-sentence-encoder/2 4 Automatic Substitution Manual substitute annotations have been useful for in-context usage similarity estimation (Erk et al., 2009; McCarthy et al., 2016). The idea is that a high proportion of shared substitutes between two word instances reflects their semantic similarity.6 Extending previous work where manual substitute annotations were used to estimate usage similarity (Erk et al., 2009), we automatically annotate WiC instances with substitutes, and use features based on their overlap for our classifier. We use the context2vec method for automatic lexical substitution (Melamud et al., 2016). Given a sentence with a new instance of a target word t, and a set of candidate substitutes for the word (S = s1 , s2 , ..., sn ), context2vec ranks al"
W19-5802,K16-1006,0,0.234407,"btained vector representations is used as a feature for our classifier. We use the following types of embeddings: SIF (Smooth Inverse Frequency): Simple method for deriving sentence representations from uncontextualized embeddings (Arora et al., 2017). Dimensionality reduction is applied to a weighted average of the vectors of words in a sentence. Weighting is based on word frequency in Common Crawl. We use SIF in combination with 300d GloVe vectors trained on Common Crawl (Pennington et al., 2014).2 Context2vec: Neural model that learns embeddings for words and their contexts simultaneously (Melamud et al., 2016). It is based on word2vec’s 1 http://www.wiktionary.org/ 2 https://nlp.stanford.edu/projects/ glove/ CBOW (Mikolov et al., 2013), but replaces the averaging of context word embeddings with a biLSTM that learns a representation of a sentence excluding the target word. We use a 600-d model pre-trained on the UkWac corpus (Baroni et al., 2009).3 ELMo (Embeddings from Language Models): Contextualized word representations obtained from the internal states of a deep bidirectional LSTM trained with a language model objective (Peters et al., 2018). Instead of learning the best linear combination of la"
W19-5802,W15-1501,0,0.0250447,"way (gold label: T) and the verb drink (gold label: F) with automatic substitute annotations assigned by context2vec. Substitutes in italics were discarded after filtering. • GAP score: GAP (Generalized Average Precision) considers the order of ranked elements and their weights (Kishida, 2005). GAP score ranges from 0 to 1 (for perfect disagreement/agreement). We take the average score between the rankings produced for a sentence pair in both directions (GAP(R1 , R2 ) and GAP(R2 , R1 )). Weights are the scores assigned to the substitutes by context2vec. We use the GAP implementation shared by Melamud et al. (2015). (Ide et al., 2008) which contains manual substitute annotations for all content words in a sentence. We use a balanced collection of similar (T ) and dissimilar (F) sentence pairs from CoInCo, with labels automatically assigned based on substitute overlap (Gar´ı Soler et al., 2019).10 We apply the automatic substitution method described in Section 4, and extract substitute- and embeddingbased features to be used by our models. • Substitute cosine similarity. We form pairs of substitutes from R1 and R2 , and calculate the average of their GloVe cosine similarities. This feature accounts for t"
W19-5802,N13-1092,0,0.088872,"Missing"
W19-5802,P15-2070,0,0.0653881,"Missing"
W19-5802,N18-1202,0,0.0379724,"beddings for words and their contexts simultaneously (Melamud et al., 2016). It is based on word2vec’s 1 http://www.wiktionary.org/ 2 https://nlp.stanford.edu/projects/ glove/ CBOW (Mikolov et al., 2013), but replaces the averaging of context word embeddings with a biLSTM that learns a representation of a sentence excluding the target word. We use a 600-d model pre-trained on the UkWac corpus (Baroni et al., 2009).3 ELMo (Embeddings from Language Models): Contextualized word representations obtained from the internal states of a deep bidirectional LSTM trained with a language model objective (Peters et al., 2018). Instead of learning the best linear combination of layer representations for a task – a common way of using ELMo – we use out-of-the-box 512-d embeddings.4 We experiment with the top layer, and the average of the three hidden layers. We represent each WiC sentence in two ways: a) with the ELMo embedding corresponding to the target word, and b) with the average of ELMo embeddings of all words in the sentence. We also average the embeddings at a context window of size two, as this was shown to work better for word usage similarity with ELMo (Gar´ı Soler et al., 2019). BERT (Bidirectional Encod"
W19-5802,N19-1128,0,0.0454543,"ent types, and with features based on in-context substitute annotations automatically assigned to WiC sentence pairs. The model with the highest performance on the WiC development set uses a combination of cosine similarities from different embedding types. It obtains an accuracy of 66.7 on the shared task test set and is ranked third among the participating systems. 1 Introduction The SemDeep-5 WiC shared task proposes to identify the intended meaning of words in context. It is framed as a binary classification task that addresses whether two instances of a target word have the same meaning (Pilehvar and CamachoCollados, 2019). The WiC dataset contains 7,466 sentence pairs and is proposed as a new evaluation benchmark for context-sensitive word representations. We apply to this task the method from Gar´ı Soler et al. (2019) which addresses the usage similarity of contextualized instances of words. The method integrates cosine similarities from different types of context-sensitive embeddings and in-context automatic substitutes. Our best system combines cosine similarities from three embedding types. It obtains an accuracy of 66.7 on the WiC test set, and is ranked third among all systems that participated in the ta"
Y09-1007,apidianaki-2008-translation,1,0.842285,"semantically distant ones. We explain how the semantic information acquired by this method can be exploited by METEOR for evaluation. Exploiting this kind of information permits the capturing of domain-relevant synonymy relations, overrides the need for predefined resources and permits the use of METEOR’s synonymy module for evaluation in languages other than English. The only requirement is that a parallel corpus, needed for training the sense induction method, be available in the concerned languages. 3 Data-driven sense induction The semantic analysis method used here is the one proposed in Apidianaki (2008). Her method reveals the senses of ambiguous words of one language by clustering their TEs in another language. The created sense-clusters group semantically similar equivalents, whose relations are discovered from a parallel aligned training corpus. The method is based on the distributional hypotheses of meaning (Harris, 1954) and of semantic similarity (Miller and Charles, 1991), and on the assumption of sense correspondence between words in translation relation in real texts. The analysis is thus performed by combining distributional and translation information from a parallel corpus. Being"
Y09-1007,E09-1010,1,0.816811,"in METEOR for two reasons. Firstly, as the WN module of METEOR is shown to improve the correlation with human judgment, it would be interesting to see if our method has the same effect. Secondly, METEOR is a well established and stable metric and improvement on such a metric would be of more practical use. A clear advantage of this automatically created inventory in comparison to WordNet is that the information is acquired directly from corpora. It is thus relative to the domains of the processed texts and may concern languages for which WordNet-type resources are not available. Additionally, Apidianaki (2009) showed that this data-driven sense induction method provides, as a by-product, information that can be exploited by an unsupervised Word Sense Disambiguation classifier. It consists of the SL distributional information that reveals the similarity of the TEs and can serve to disambiguate new instances of the ambiguous words. This is another advantage of this unsupervised method: it makes it possible to carry out a disambiguation step during evaluation, which could replace METEOR’s “poor-man’s synonymy detection algorithm” (Banerjee and Lavie, 2005). Nevertheless, this inventory is also charact"
Y09-1007,W05-0909,0,0.553531,"French and we analyze the obtained results. Then we show the advantages of integrating automatically acquired semantic information into MT evaluation. Finally we conclude, together with avenues for further work. 2 Lexical variation in existing evaluation metrics BLEU (Papineni et al., 2002) captures lexical variation by the use of multiple reference translations. However, this has been shown to be a rather problematic solution: even if numerous human translations of the same original text are available, which is rarely the case, their use poses additional problems during evaluation.2 METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) matches unigrams between the hypothesis and the reference in a flexible way, by using a stemming and a synonymy module. While the first matches different word forms, the second increases the number of pertinent translations by exploiting WordNet information: a translation is considered to be correct not only if it exactly corresponds to the reference, but also if it is semantically similar to it, i.e. found in the same WordNet synset. Nevertheless, predefined semantic resources like WordNet present some limitations. They cannot be easily updated and adapted to the do"
Y09-1007,P05-1048,0,0.0225795,"on scores often do not reflect the quality of translation, there is a growing tendency towards increasing the correlation of the metrics with human judgments of translation quality. An important factor determining this correlation is the identification of sense correspondences between the hypothesis and the reference, which may exist even if the words used in the translations differ. Capturing this type of correspondence would also allow a more conclusive estimation of the impact of WSD techniques on MT systems than is possible with the current evaluation metrics (Callison-Burch et al., 2006; Carpuat and Wu, 2005; Chan et al., 2007). In this paper, we show how variation at the unigram level can be captured during evaluation using information induced from parallel corpora by an unsupervised sense induction method. This method generates bilingual semantic inventories, where the senses of the words of one language are described by clusters of their semantically similar translation equivalents (TEs) in another language. These sense-clusters, which are similar to WordNet1 synsets, can serve to capture correspondences between synonymous words found in the compared translations. 1 Copyright 2009 by Marianna"
Y09-1007,W08-0309,0,0.0519018,"gments In order to calculate the correlation that METEOR has with human judgments during evaluation in French, we use the WMT08 evaluation shared task dataset.10 All English–French human rankings (307 in total), distributed during this shared evaluation task for estimating the correlation of automatic metrics to human judgments of translation quality, were used for our experiments. The rankings provided here are at the level of the segment. To measure the correlation of the automatic metrics with the human judgments of translation quality, we use Spearman’s rank order correlation coefficient (Callison-Burch et al., 2008). Spearman’s correlation is defined as in (1), where d is the difference between corresponding values in rankings and n is the length of the rankings. ρ=1−( P 6 d2 ) n(n2 − 1) (1) An automatic evaluation metric with a higher correlation value is considered to make predictions that are more similar to the human judgments than a metric with a lower value. For measuring the consistency of the automatic metrics with human judgments, we use the pairwise consistent percentage (Callison-Burch et al., 2008). For every pairwise comparison of two systems on a single sentence by a person, the automatic m"
Y09-1007,P07-1005,0,0.0176917,"reflect the quality of translation, there is a growing tendency towards increasing the correlation of the metrics with human judgments of translation quality. An important factor determining this correlation is the identification of sense correspondences between the hypothesis and the reference, which may exist even if the words used in the translations differ. Capturing this type of correspondence would also allow a more conclusive estimation of the impact of WSD techniques on MT systems than is possible with the current evaluation metrics (Callison-Burch et al., 2006; Carpuat and Wu, 2005; Chan et al., 2007). In this paper, we show how variation at the unigram level can be captured during evaluation using information induced from parallel corpora by an unsupervised sense induction method. This method generates bilingual semantic inventories, where the senses of the words of one language are described by clusters of their semantically similar translation equivalents (TEs) in another language. These sense-clusters, which are similar to WordNet1 synsets, can serve to capture correspondences between synonymous words found in the compared translations. 1 Copyright 2009 by Marianna Apidianaki, Yifan He"
Y09-1007,2009.eamt-1.7,1,0.813123,"the domains of the processed texts and, most importantly, they are not publicly available for languages other than English. This is an important issue concerning METEOR as, when it is used for evaluation in languages other than English, only the exact and stemming matching modules are used, while the synonymy module is not operational and is omitted. This explains why Lavie and Agarwal (2007) propose to develop new synonymy modules for languages other than English, that would be based on alternative methods and could be used in the place of WordNet. Some other metrics (Owczarzak et al., 2007; He and Way, 2009) go beyond pure string matching. They look for “deeper” correspondences and thus correlate better with human judgments of translation quality. The above-mentioned metrics both use syntactic structure and dependency information in order to capture variations between sentences. In Owczarzak et al. (2007), lexical variation is also accommodated by adding WordNet synonyms into the matching process. In previous work by Owczarzak et al. (2006), lexical and syntactic paraphrases were extracted from the bitext used for evaluation using word and phrase alignment. In their work, the target language (TL)"
Y09-1007,2005.mtsummit-papers.11,0,0.011162,"ed on the distributional hypotheses of meaning (Harris, 1954) and of semantic similarity (Miller and Charles, 1991), and on the assumption of sense correspondence between words in translation relation in real texts. The analysis is thus performed by combining distributional and translation information from a parallel corpus. Being totally data-driven this sense induction method is language-independent and permits the creation of sense inventories for different language pairs. 3.1 The training data The training corpus used here is the sentence-aligned English(EN) – French(FR) part of Europarl (Koehn, 2005), which has been lemmatized and tagged by part-of-speech (POS) (Schmid, 1994). As the semantic analysis method is rather sensible to spurious alignments, a number of filters have been applied prior to word alignment in order to ensure the results with the least possible noise. First, function words were deleted in order to keep only the lemmas of content words. Then sentences containing more than five content words (and their translations) were deleted, as well as the sentence pairs presenting a great difference in length (cases where one sentence was three times longer than the other). After"
Y09-1007,P07-2045,0,0.0106248,"Missing"
Y09-1007,W07-0734,0,0.0662391,"obtained results. Then we show the advantages of integrating automatically acquired semantic information into MT evaluation. Finally we conclude, together with avenues for further work. 2 Lexical variation in existing evaluation metrics BLEU (Papineni et al., 2002) captures lexical variation by the use of multiple reference translations. However, this has been shown to be a rather problematic solution: even if numerous human translations of the same original text are available, which is rarely the case, their use poses additional problems during evaluation.2 METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) matches unigrams between the hypothesis and the reference in a flexible way, by using a stemming and a synonymy module. While the first matches different word forms, the second increases the number of pertinent translations by exploiting WordNet information: a translation is considered to be correct not only if it exactly corresponds to the reference, but also if it is semantically similar to it, i.e. found in the same WordNet synset. Nevertheless, predefined semantic resources like WordNet present some limitations. They cannot be easily updated and adapted to the domains of the processed tex"
Y09-1007,J04-2003,0,0.0210714,"been applied at the level of the lexicons as well: first, the TEs of the ambiguous words were filtered on the basis of their score;5 then, an intersection filter was applied, which discards any translation correspondences not found in both lexicons. While eliminating many false TEs, this process eliminated some good ones as well. The reason why we opted for this filtering is that the negative effect of the elimination of good TEs on the semantic analysis is less important than the noise present in the lexicons.6 3 4 5 6 Aligning word types rather than tokens decreases data sparseness effects (Nießen and Ney, 2004). We aligned the corpus using two Giza++ configurations, with and without the mkcls component. As the lexicons generated from the two alignments contained some different entries (SL words), we kept their union in order to increase the coverage. The adopted threshold (0.03) was defined empirically. This could be described as an increase in precision – which is more important in lexicography applications (Och and Ney, 2003) – and a decrease in recall. 55 Table 1: Entries from the EN–FR and the FR–EN sense inventories. Language POS Nouns EN–FR Verbs Adjectives Nouns FR–EN Verbs Adjectives Source"
Y09-1007,J03-1002,0,0.0123803,"c analysis method is rather sensible to spurious alignments, a number of filters have been applied prior to word alignment in order to ensure the results with the least possible noise. First, function words were deleted in order to keep only the lemmas of content words. Then sentences containing more than five content words (and their translations) were deleted, as well as the sentence pairs presenting a great difference in length (cases where one sentence was three times longer than the other). After these filtering steps, word alignment was performed at the level of word types using Giza++ (Och and Ney, 2003).3 Two bilingual lexicons, one for each translation direction (EN–FR/FR–EN), were built from the alignment of word types. In these lexicons, each SL word (w) is associated with the set of TEs to which it was aligned.4 Given the sensibility of the sense induction method to noise, some filtering steps have been applied at the level of the lexicons as well: first, the TEs of the ambiguous words were filtered on the basis of their score;5 then, an intersection filter was applied, which discards any translation correspondences not found in both lexicons. While eliminating many false TEs, this proce"
Y09-1007,W07-0714,1,0.921275,"Missing"
Y09-1007,P09-1034,0,0.204466,"Missing"
Y09-1007,P02-1040,0,0.0878175,"we present how lexical variation is dealt with in existing MT evaluation metrics. In section 3, we describe the sense induction method and the training data used. In section 4, we explain how the automatically built sense inventory is integrated into METEOR. In section 5, we present the experiments carried out in English and in French and we analyze the obtained results. Then we show the advantages of integrating automatically acquired semantic information into MT evaluation. Finally we conclude, together with avenues for further work. 2 Lexical variation in existing evaluation metrics BLEU (Papineni et al., 2002) captures lexical variation by the use of multiple reference translations. However, this has been shown to be a rather problematic solution: even if numerous human translations of the same original text are available, which is rarely the case, their use poses additional problems during evaluation.2 METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) matches unigrams between the hypothesis and the reference in a flexible way, by using a stemming and a synonymy module. While the first matches different word forms, the second increases the number of pertinent translations by exploiting Wor"
Y09-1007,W09-0441,0,0.0457594,"of the text, which makes them more appropriate to the task at hand than synonyms extracted from an external resource like WordNet. However, an important issue concerning the equivalence sets is not addressed: the TL equivalents of a SL word/phrase are not always semantically related. SL words may be ambiguous, in which case their equivalents translate their different senses. There are metrics that use paraphrases or textual entailment features to facilitate automatic evaluation, which are related to our proposed metric. Paraphrase-based metrics, such as ParaEval (Zhou et al., 2006) and TERp (Snover et al., 2009), use paraphrases mined from the corpus as “synonyms”; while in the Textual Entailment-based approach (Pad´o et al., 2009), the metric tries to 2 BLEU puts very few constraints on how n-gram matches can be drawn from the multiple reference translations and so it allows a too high amount of translation variation. Apart from that, the notion of recall - an important parameter in the evaluation of translation quality - is difficult to formulate over multiple reference translations and is not thus taken into account by BLEU (Callison-Burch et al., 2006). 54 determine whether entailment can be infe"
Y09-1007,W06-1610,0,0.0922879,"hrases relevant to the domain of the text, which makes them more appropriate to the task at hand than synonyms extracted from an external resource like WordNet. However, an important issue concerning the equivalence sets is not addressed: the TL equivalents of a SL word/phrase are not always semantically related. SL words may be ambiguous, in which case their equivalents translate their different senses. There are metrics that use paraphrases or textual entailment features to facilitate automatic evaluation, which are related to our proposed metric. Paraphrase-based metrics, such as ParaEval (Zhou et al., 2006) and TERp (Snover et al., 2009), use paraphrases mined from the corpus as “synonyms”; while in the Textual Entailment-based approach (Pad´o et al., 2009), the metric tries to 2 BLEU puts very few constraints on how n-gram matches can be drawn from the multiple reference translations and so it allows a too high amount of translation variation. Apart from that, the notion of recall - an important parameter in the evaluation of translation quality - is difficult to formulate over multiple reference translations and is not thus taken into account by BLEU (Callison-Burch et al., 2006). 54 determine"
Y09-1007,E06-1032,0,\N,Missing
Y09-1007,W06-3112,1,\N,Missing
