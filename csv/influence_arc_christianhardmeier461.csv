2010.iwslt-papers.10,W10-1710,1,0.863634,"Missing"
2010.iwslt-papers.10,S10-1021,0,0.0227125,"n occur anywhere in the text, even in a different sentence. Since a given input word can be translated with different words in the target language and the pronoun must agree with the word that was actually chosen, correct pronoun choice depends on a translation decision taken earlier by the Machine Translation system. Our model attempts to face this challenge by explicitly identifying anaphoric links in the SMT input and measuring in the output how well the translation of an anaphoric pronoun matches the translation of its antecedent. We used the open-source coreference resolution system BART [8] to link pronouns to their antecedents in the text. The preliminary case study described in the preceding section was about German-English translation. In our practical experiments, we worked on the inverse translation direction, English-German, because we had ready access to an English coreference resolver. The coreference resolution system we used was trained on the ACE02-npaper corpus and uses separate models for pronouns and non-pronouns in order to increase pronounresolution performance. For each resolvable pronoun, the system finds a link to exactly one direct antecedent noun phrase. In"
2010.iwslt-papers.10,P07-2045,1,0.0209518,"sentence are satisfied, the sentence is put on the queue. The actual implementation is multi-threaded and feeds a number of parallel decoder processes. The decoder input queue is realised as a priority queue ordered by the number of incoming dependencies of the sentences in order to resolve as many dependencies as possible as early as possible and thus increase the throughput of the system. Since the sentences are not processed in order, a final ordering step restores the original document order. Coreference information was integrated into an SMT system based on the phrase-based Moses decoder [9] in the form of a new model which represents dependencies between pairs of target-language words produced by the MT system. The decoder driver encodes the links found by the coreference resolver in the input passed to the SMT decoder. Pronouns and their antecedents are marked as illustrated in the lower half of figure 2. Each token is annotated with a pair of elements. The first part numbers the antecedents to which there is a reference in the same sentence. The second part contains the number of the sentence-internal antecedent to which this word refers, or the word itself, if it occurred in"
2010.iwslt-papers.10,P02-1038,0,0.0123676,"using the word forms for our word dependency model, we map the antecedent words to a tag representing their gender and number; thus, in the example, the word hospital in the first sentence, which is translated by the system into the neuter singular word Krankenhaus (not shown), gets mapped to the tag neut_sg in the input for sentence 2. Gender and number of German words were annotated using the RFTagger [?]. The representation of the referent words, by contrast, is fully lexicalised. The word dependency module is integrated as an additional feature function in a standard log-linear SMT model [10]. It keeps track of pairs of source words (sant , sref ) participating as antecedent and referent in a coreference link. Usually, the antecedent sant will be processed first; however, it is also possible for the referent sref to be encountered first, either because of a cataphoric link in the source sentence or, more likely, because of word reordering during decoding. When the second element in an antecedent-referent pair is translated, the word dependency module adds a score of the following form: p(Tref |Tant ) = max (tref ,tant )∈Tref ×Tant p(tref |tant ), (1) where Tref is the set of targe"
2010.iwslt-papers.10,W01-1408,0,0.0133752,"he staph. The same hospital had had to contend with a similar infection early this year . It|*-&gt;neut_sg had discharged a patient admitted after a serious traffic accident . Shortly afterward , it|*-&gt;neut_sg had to re-admit the patient because of an MRSA infection , and doctors|1-* have been unable to perform surgery that would be vital to full recovery because they|*-1 have been unable to get rid of the staph . Figure 2: Coreference link annotation and decoder input is another search path that is superior under every possible continuation of the search. This is called hypothesis recombination [11]. Since our model introduces dependencies that can span large parts of the sentence, care must be taken not to recombine hypotheses that could be ranked differently after including the word dependency scores. We therefore extend the decoder search state to include, on the one hand, the set of antecedents already processed and, on the other hand, the set of referents encountered for which no antecedent has been seen yet. In either case, the translation chosen by the decoder is stored along with the item. Hypotheses can only be recombined if both of these sets match. 4.4. Word-Dependency Model:"
2010.iwslt-papers.10,J03-1002,0,0.0236175,"addresses a specific problem of the MT system, evaluation with a general-purpose score such as BLEU may not be fully adequate. Besides measuring overall translation quality, which is what general-purpose measures purport to do, we also want to know details about the impact of the new model on pronoun translation. We therefore propose a method to measure precision and recall of pronoun translations more directly. We use a test corpus with one reference translation, for which we construct word alignments by concatenating it with additional parallel training data, running the GIZA++ word aligner [13] and symmetrising the alignments as is usually done for SMT system training. We also produce word alignments between the source text and the candidate translation by considering the phrase-internal word alignments stored in the phrase table. The basic idea is to count the number of pronouns translated correctly. Doing so would require a 1 : 1 mapping from pronouns to their translations. However, word alignments can link a word to zero, one or more words, so we suggest using a measure based on precision and recall instead. For every pronoun occurring in the source text, we obtain the set of ali"
2010.iwslt-papers.10,P02-1040,0,0.080021,"the source text and the candidate translation by considering the phrase-internal word alignments stored in the phrase table. The basic idea is to count the number of pronouns translated correctly. Doing so would require a 1 : 1 mapping from pronouns to their translations. However, word alignments can link a word to zero, one or more words, so we suggest using a measure based on precision and recall instead. For every pronoun occurring in the source text, we obtain the set of aligned target words in the reference and the candidate translation, R and C, respectively. Inspired by the BLEU score [14], we define the clipped count of a particular candidate word w as the number of times it occurs in the candidate set, limited by the number of times it occurs in the reference set: cclip (w) = min (cC (w), cR (w)) (2) We then consider the match count to be the sum of the clipped counts over all words in the candidate translation aligned to pronouns in the source text, which allows us to define precision and recall in the usual way: Precision = ∑ cclip (w) w∈C |C| ; Recall = ∑ cclip (w) w∈C |R| (3) This measure can be applied both to obtain a comprehensive score for a particular system on a tes"
2010.iwslt-papers.10,W10-1737,0,0.579477,"Missing"
2010.iwslt-papers.10,J90-2002,0,\N,Missing
2010.iwslt-papers.10,W09-0401,0,\N,Missing
2010.jec-1.7,2006.tc-1.9,0,0.0884004,"Missing"
2010.jec-1.7,W09-4610,1,0.847215,"prior translations (cf. the average of 57.3 in table 1) is probably due to the fact that subtitles are shorter and grammatically simpler than Europarl and Acquis sentences. 4.4 Linguistic Information in SMT for Subtitles The results reported in tables 1 and 2 are based on a purely statistical MT system. No linguistic knowledge was included. We wondered whether linguistic features such as Part-of-Speech tags or number information (singular vs. plural) could improve our system. We therefore ran a series of experiments to check this hypothesis using factored SMT for Swedish - Danish translation. Hardmeier and Volk (2009) describe these experiments in detail. Here we summarize the main findings. When we used a large training corpus of around 900,000 subtitles or 10 million tokens per language, the gains from adding linguistic information were generally small. Minor improvements were observed when using additional language models operating on part-of-speech tags and tags from morphological analysis. A technique called analytical translation, which enables the SMT system to back off to separate translation of lemmas and morphological tags (provided by Eckhard Bick’s tools) when the main phrase table does not pro"
2010.jec-1.7,2005.mtsummit-papers.11,0,0.0414572,"Missing"
2010.jec-1.7,2006.tc-1.10,0,0.787235,"Missing"
2010.jec-1.7,J04-4002,0,0.115149,"Missing"
2010.jec-1.7,2005.mtsummit-papers.19,0,0.123956,"Missing"
2010.jec-1.7,2001.mtsummit-papers.68,0,0.0877422,"Missing"
2010.jec-1.7,prokopidis-etal-2008-condensing,0,0.090432,"and (D´ıaz-Cintas and Remael, 2007). Gottlieb (2001) and Pedersen (2007) describe the peculiarities of subtitling in Scandinavia, Nagel et al. (2009) in other European countries. 3 Approaches to the Automatic Translation of Film Subtitles In this section we describe other projects on the automatic translation of subtitles.2 We assume subtitles in one language as input and aim at producing an automatic translation of these subtitles into another language. In this paper we do not deal with the conversion of the film transcript into subtitles which requires shortening the original dialogue (cf. (Prokopidis et al., 2008)). We distinguish between rulebased, example-based, and statistical approaches. 3.1 Rule-based MT of Film Subtitles Popowich et al. (2000) provide a detailed account of a MT system tailored towards the translation of English subtitles into Spanish. Their approach is based on a MT paradigm which relies heavily on lexical resources but is otherwise similar to the transfer-based approach. A unification-based parser analyzes the 2 Throughout this paper we focus on TV subtitles, but in this section we deliberately use the term “film subtitles” in a general sense covering both TV and movie subtitles"
2010.jec-1.7,2009.mtsummit-papers.16,0,0.0275594,"lation suggestions. This takes too much time. Suppressing Bad Translations An issue that has followed us throughout the project is the suppression of (presumably) bad translations. While good machine translations considerably increase the productivity of the post-editors, editing bad translations is tedious and frequently slower than translating from scratch. To take away some of this burden from the post-editors, we experimented with a Machine Learning component to predict confidence scores for the individual subtitles output by our Machine Translation systems. Closely following the work by (Specia et al., 2009), we prepared a data set of 4,000 machine-translated subtitles, manually annotated for translation quality on a 1-4 scale by the post-editors. We extracted around 70 features based on the MT input and output, their similarity and the similarity between the input and the MT training data. Then we trained a Partial Least Squares regressor to predict quality scores for unseen subtitles. Like (Specia et al., 2009), we used Inductive Confidence Machines to calibrate the acceptance threshold of our translation quality filter. We found that a confidence filter with the features proposed by Specia et"
2010.jec-1.7,2007.mtsummit-papers.66,1,0.472329,"rpus they have used for evaluating the MT system). This summary indicates that work on the automatic translation of film subtitles with Statistical MT is limited because of the lack of freely available high-quality training data. Our own efforts are based on large proprietary subtitle data and have resulted in mature MT systems. We will report on them in the following section. 4 Our MT Systems for TV Subtitles We have built Machine Translation systems for translating film subtitles from Swedish to Danish and to Norwegian in a commercial setting. Some of this work has been described earlier by Volk and Harder (2007) and Volk (2008). Most films are originally in English and receive Swedish subtitles based on the English video and audio (sometimes accompanied by an English transcript). The creation of the Swedish subtitle is a manual process done by specially trained subtitlers following company-specific guidelines. In particular, the subtitlers set the time codes (beginning and end time) for each subtitle. They use an in-house tool which allows them to link the subtitle to specific frames in the video. The Danish translator subsequently has access to the original English video and audio but also to the Sw"
2010.jec-1.7,P02-1040,0,\N,Missing
2011.eamt-1.32,C04-1046,0,0.335882,"SVM classifiers. For four different datasets in two language pairs and two text genres, we build and evaluate classifiers based on explicitly extracted feature sets, syntactic tree kernels and their combination. We demonstrate that it is relatively easy to build a reasonable classifier using the tree kernel approach alone and that syntactic tree kernels have something to contribute even in the presence of a traditional feature set. 2 Related work The problem of sentence-level confidence estimation for Machine Translation has been addressed with various Machine Learning techniques in the past. Blatz et al. (2004) present a comparison of different Machine Learning algorithms for MT confidence estimation and a set of features that has become the basis of much later work. They train classifiers trained on data labelled automatically based on the NIST and WER Machine Translation evaluation measures, accepting as good the topscored 5 or 30 percent of the examples. A similar setup and feature set were used by Quirk (2004), who also ran some experiments with a very small manually annotated corpus, using only 350 sentences for training and 150 sentences for testing. A comparable feature set was also used by S"
2011.eamt-1.32,2005.eamt-1.15,0,0.0795013,"Missing"
2011.eamt-1.32,gimenez-marquez-2004-svmtool,0,0.0407629,"Missing"
2011.eamt-1.32,P07-2053,0,0.0768276,"Missing"
2011.eamt-1.32,W10-2910,0,0.0809917,"Missing"
2011.eamt-1.32,P03-1054,0,0.0181708,"xt subtitle – marker for continuation from previous subtitle • a binary feature indicating that the output contains more than three times as many alphabetic tokens as the input 236 Parse trees We annotated all datasets with both parse trees for both the source and the target language. In the source language, English, we were able to produce both constituency and dependency parses. In the target languages, Swedish and Spanish, we limited our experiments to dependency parses because of the better availability of parsing models. English constituency parses were produced with the Stanford parser (Klein and Manning, 2003) using the model bundled with the parser. For dependency parsing, we used the MaltParser (Nivre et al., 2006). POS tagging was done with HunPOS (Hal´acsy et al., 2007) for English and Swedish and SVMTool (Gim´enez and M´arquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009). A recaser based on the Moses SMT system (Koehn et al., 2007) and trained on the WMT 2008 training data was used to transform the lowercase-only Europarl datasets into mixed-case form before tagging and parsing. The MT output was parsed with a standard parser model trained on regular treeb"
2011.eamt-1.32,P07-2045,0,0.00383541,"parses. In the target languages, Swedish and Spanish, we limited our experiments to dependency parses because of the better availability of parsing models. English constituency parses were produced with the Stanford parser (Klein and Manning, 2003) using the model bundled with the parser. For dependency parsing, we used the MaltParser (Nivre et al., 2006). POS tagging was done with HunPOS (Hal´acsy et al., 2007) for English and Swedish and SVMTool (Gim´enez and M´arquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009). A recaser based on the Moses SMT system (Koehn et al., 2007) and trained on the WMT 2008 training data was used to transform the lowercase-only Europarl datasets into mixed-case form before tagging and parsing. The MT output was parsed with a standard parser model trained on regular treebank data. SMT output contains many grammatically malformed sentences. We do not know of a reliable method to assess the impact of this problem on parsing accuracy, nor is it clear what effect reduced parsing accuracy has on classifier performance, since the tree-kernel classifier may very well be able to extract useful information from corrupted parse trees if the corr"
2011.eamt-1.32,W05-0904,0,0.0769039,"Missing"
2011.eamt-1.32,E06-1015,0,0.169602,"n all its siblings must be included as well so that the underlying production rule is completely represented. This kind of kernel is well suited for constituency parse trees and was used in our exFig. 2. A (SSTs). VP N D A tree and some of its Partial Tree Fragments Fig. 3. A tree with some of its partial trees Figure 2: Tree fragments extracted by the Subset (PTs). Implicit feature modelling with tree kernels brought D a Fig. syntactic parse tree its subA 1. treeAand some of its Subset Treewith Fragments trees (STs). VP V Fig. 4. A Tree Kernel and by the Partial Tree Kernel. Illustrations by Moschitti (2006a). constraint over the SSTs, we obtain a more general f periments with constituency trees. For the expericall partial trees (PTs). These can be generated ments with dependency trees, we used the Partial production rules of2006a) the grammar, consequently [V Tree Kernel (Moschitti, instead. It extends valid PTs.Tree Figure shows thatalso thethe number of PTs d the Subset Kernel3 by permitting extractionisofstill tree fragments comprising only part of different su before higher (i.e. 30 PTs). These the children of any given node. Lifting this restrican intuitive quantification of the different"
2011.eamt-1.32,quirk-2004-training,0,0.0386295,"onal feature set. 2 Related work The problem of sentence-level confidence estimation for Machine Translation has been addressed with various Machine Learning techniques in the past. Blatz et al. (2004) present a comparison of different Machine Learning algorithms for MT confidence estimation and a set of features that has become the basis of much later work. They train classifiers trained on data labelled automatically based on the NIST and WER Machine Translation evaluation measures, accepting as good the topscored 5 or 30 percent of the examples. A similar setup and feature set were used by Quirk (2004), who also ran some experiments with a very small manually annotated corpus, using only 350 sentences for training and 150 sentences for testing. A comparable feature set was also used by Soricut and Echihabi (2010). Specia et al. (2009a) use a fairly large feature set including most of the features proposed by Blatz et al. (2004) to train a Partial Least Squares (PLS) regressor on a variety of datasets, both manually and automatically annotated. In another paper from the same year (Specia et al., 2009b), they suggest a way to compute a threshold value to use the PLS regressor as classifier at"
2011.eamt-1.32,P10-1063,0,0.036124,") present a comparison of different Machine Learning algorithms for MT confidence estimation and a set of features that has become the basis of much later work. They train classifiers trained on data labelled automatically based on the NIST and WER Machine Translation evaluation measures, accepting as good the topscored 5 or 30 percent of the examples. A similar setup and feature set were used by Quirk (2004), who also ran some experiments with a very small manually annotated corpus, using only 350 sentences for training and 150 sentences for testing. A comparable feature set was also used by Soricut and Echihabi (2010). Specia et al. (2009a) use a fairly large feature set including most of the features proposed by Blatz et al. (2004) to train a Partial Least Squares (PLS) regressor on a variety of datasets, both manually and automatically annotated. In another paper from the same year (Specia et al., 2009b), they suggest a way to compute a threshold value to use the PLS regressor as classifier at a given target precision using Inductive Confidence Machines. They argue that if the MT output is to be post-edited by professional translators, it may be more important to 234 ensure a reasonable level of precisio"
2011.eamt-1.32,2009.eamt-1.5,0,0.589918,"ferent Machine Learning algorithms for MT confidence estimation and a set of features that has become the basis of much later work. They train classifiers trained on data labelled automatically based on the NIST and WER Machine Translation evaluation measures, accepting as good the topscored 5 or 30 percent of the examples. A similar setup and feature set were used by Quirk (2004), who also ran some experiments with a very small manually annotated corpus, using only 350 sentences for training and 150 sentences for testing. A comparable feature set was also used by Soricut and Echihabi (2010). Specia et al. (2009a) use a fairly large feature set including most of the features proposed by Blatz et al. (2004) to train a Partial Least Squares (PLS) regressor on a variety of datasets, both manually and automatically annotated. In another paper from the same year (Specia et al., 2009b), they suggest a way to compute a threshold value to use the PLS regressor as classifier at a given target precision using Inductive Confidence Machines. They argue that if the MT output is to be post-edited by professional translators, it may be more important to 234 ensure a reasonable level of precision by suppressing bad"
2011.eamt-1.32,2009.mtsummit-papers.16,0,0.511964,"ferent Machine Learning algorithms for MT confidence estimation and a set of features that has become the basis of much later work. They train classifiers trained on data labelled automatically based on the NIST and WER Machine Translation evaluation measures, accepting as good the topscored 5 or 30 percent of the examples. A similar setup and feature set were used by Quirk (2004), who also ran some experiments with a very small manually annotated corpus, using only 350 sentences for training and 150 sentences for testing. A comparable feature set was also used by Soricut and Echihabi (2010). Specia et al. (2009a) use a fairly large feature set including most of the features proposed by Blatz et al. (2004) to train a Partial Least Squares (PLS) regressor on a variety of datasets, both manually and automatically annotated. In another paper from the same year (Specia et al., 2009b), they suggest a way to compute a threshold value to use the PLS regressor as classifier at a given target precision using Inductive Confidence Machines. They argue that if the MT output is to be post-edited by professional translators, it may be more important to 234 ensure a reasonable level of precision by suppressing bad"
2011.eamt-1.32,specia-etal-2010-dataset,0,0.0612414,"features, the main contribution of our work is the use of tree kernels as a way to define a large implicit feature space potentially covering abstract linguistic phenomena with relatively low effort compared to the explicit feature engineering approach of previous work. 3 Datasets The research presented in this paper was mainly developed while working on a confidence estimation component for an MT system for film subtitles, for which we had a specific dataset freshly annotated with quality scores at our disposal. Our annotations were modelled after a collection of annotated data published by Specia et al. (2010a), on which we ran our experiments for comparison since the subtitle dataset cannot be made publicly available. 3.1 Europarl datasets The data collection provided by Specia et al. (2010a) is composed of 4,000 sentences randomly drawn from the development and test sets of the WMT 2008 Machine Translation shared task, translated from English into Spanish with four different Statistical Machine Translation systems. The quality of the MT output for each single sentence was judged by professional translators on a scale ranging from 1 to 4 with the following definitions (Specia et al., 2010a): SMT"
2011.eamt-1.32,2010.jec-1.7,1,0.887644,"Missing"
2020.crac-1.15,amoia-etal-2012-coreference,1,0.59326,"texts – is a well-known effect of the translation process. The levels of explicitness of referring expressions are related to Ariel (1990)’s concept of Accessibility. Morpho-syntactic types of referring expressions are related not only to accessibility, but also to the givenness or salience of a referent in the recipient’s mind (Prince, 1981; Grosz et al., 1995; Gundel et al., 2003). In studies involving register or genre variation, the distribution of morpho-syntactic types of mentions, such as the prevalence of pronouns vs. nouns, also plays an important role (Fox, 1987; Biber et al., 1999; Amoia et al., 2012; Kunz et al., 2016). Morpho-syntactic subtypes of referring expressions, substitution and ellipsis, as well as the scope of antecedents were analysed by Kunz et al. (2017) and Lapshinova-Koltunski and Mart´ınez Mart´ınez (2017) to reveal differences between registers and between the languages English and German. The scope of coreference is reflected in the differentiation between reference to entities vs. events, and in the form of the antecedent (nominal, verbal, clausal). As referring expressions can have more than one antecedent, the distinction between split and simple antecedent is also"
2020.crac-1.15,N18-1118,0,0.0284732,"e area of translation have addressed the importance of coreference (Baker, 2011; Becher, 2011; K¨onigs, 2011). However, these works are example-based and provide neither a comprehensive account, nor empirical evidence for their claims. There are a few corpus-based studies of coreference translation (Zinsmeister et al., 2012; Nov´ak and Nedoluzhko, 2015; Lapshinova-Koltunski et al., 2019b), addressing mostly the challenge of translating pronouns. The awareness of this challenge has also increased in the MT community (Voita et al., 2019; Lapshinova-Koltunski et al., 2019a; Guillou et al., 2018; Bawden et al., 2018; Miculicich Werlen and Popescu-Belis, 2017; Guillou, 2016; Hardmeier and Federico, 2010), and its relevance for multilingual coreference resolution is beyond doubt (Green ˇ et al., 2011; Nov´ak and Zabokrtsk´ y, 2014; Grishina and Stede, 2017). Still, coreference translation is affected by many factors and remains poorly understood. Kunz et al. (2017) analyse coreference and other means of explicit discourse phenomena in English and German comparable texts. They find that English and German differ in the linguistic means available in their language systems to convey coreference. English provi"
2020.crac-1.15,E89-1022,0,0.70907,"Missing"
2020.crac-1.15,W15-3403,0,0.0243057,"following factors: (a) language-specific constraints, (b) functional variation across language registers as well as (spoken or written) mode and (c) effects of the translation process. Translating between languages involves transformation of the source coreference patterns into the target ones. Analysing such patterns can give insights into translation strategies for referring expressions in texts. Variation along the above stated lines (a, b, c) causes a number of problems in multilingual coreference resolution or coreference annotation projection (Postolache et al., 2006; Ogrodniczuk, 2013; Grishina and Stede, 2015; Nov´ak, 2018). Although several studies describe such problems (Grishina and Stede, 2015; Lapshinova-Koltunski and Hardmeier, 2017; Lapshinova-Koltunski et al., 2019b), there is still a lack of understanding as to which linguistic phenomena and concretely, which structures cause these problems. In this paper, we attempt to detect such phenomena for English-German translations in a data set containing two different text registers: TED talks, which represent spoken language, and news, a type of written discourse. Previous studies show that the choice of referring expressions depends on the mod"
2020.crac-1.15,W17-1506,0,0.0183479,"ere are a few corpus-based studies of coreference translation (Zinsmeister et al., 2012; Nov´ak and Nedoluzhko, 2015; Lapshinova-Koltunski et al., 2019b), addressing mostly the challenge of translating pronouns. The awareness of this challenge has also increased in the MT community (Voita et al., 2019; Lapshinova-Koltunski et al., 2019a; Guillou et al., 2018; Bawden et al., 2018; Miculicich Werlen and Popescu-Belis, 2017; Guillou, 2016; Hardmeier and Federico, 2010), and its relevance for multilingual coreference resolution is beyond doubt (Green ˇ et al., 2011; Nov´ak and Zabokrtsk´ y, 2014; Grishina and Stede, 2017). Still, coreference translation is affected by many factors and remains poorly understood. Kunz et al. (2017) analyse coreference and other means of explicit discourse phenomena in English and German comparable texts. They find that English and German differ in the linguistic means available in their language systems to convey coreference. English provides less syntactic flexibility and is restricted in the distribution of referents. German has more options and tends to use more grammatical means of coreference than English (Kunz et al., 2017), which indicates that English and German differ i"
2020.crac-1.15,J95-2003,0,0.861248,"Becher (2011), who grades various types of referring expressions according to their degree of explicitness. This is important for our analyses, as our data contains translations, and explicitation – a higher explicitness of linguistic means in translated texts – is a well-known effect of the translation process. The levels of explicitness of referring expressions are related to Ariel (1990)’s concept of Accessibility. Morpho-syntactic types of referring expressions are related not only to accessibility, but also to the givenness or salience of a referent in the recipient’s mind (Prince, 1981; Grosz et al., 1995; Gundel et al., 2003). In studies involving register or genre variation, the distribution of morpho-syntactic types of mentions, such as the prevalence of pronouns vs. nouns, also plays an important role (Fox, 1987; Biber et al., 1999; Amoia et al., 2012; Kunz et al., 2016). Morpho-syntactic subtypes of referring expressions, substitution and ellipsis, as well as the scope of antecedents were analysed by Kunz et al. (2017) and Lapshinova-Koltunski and Mart´ınez Mart´ınez (2017) to reveal differences between registers and between the languages English and German. The scope of coreference is re"
2020.crac-1.15,W18-6435,1,0.899526,"Missing"
2020.crac-1.15,2010.iwslt-papers.10,1,0.715246,"; Becher, 2011; K¨onigs, 2011). However, these works are example-based and provide neither a comprehensive account, nor empirical evidence for their claims. There are a few corpus-based studies of coreference translation (Zinsmeister et al., 2012; Nov´ak and Nedoluzhko, 2015; Lapshinova-Koltunski et al., 2019b), addressing mostly the challenge of translating pronouns. The awareness of this challenge has also increased in the MT community (Voita et al., 2019; Lapshinova-Koltunski et al., 2019a; Guillou et al., 2018; Bawden et al., 2018; Miculicich Werlen and Popescu-Belis, 2017; Guillou, 2016; Hardmeier and Federico, 2010), and its relevance for multilingual coreference resolution is beyond doubt (Green ˇ et al., 2011; Nov´ak and Zabokrtsk´ y, 2014; Grishina and Stede, 2017). Still, coreference translation is affected by many factors and remains poorly understood. Kunz et al. (2017) analyse coreference and other means of explicit discourse phenomena in English and German comparable texts. They find that English and German differ in the linguistic means available in their language systems to convey coreference. English provides less syntactic flexibility and is restricted in the distribution of referents. German"
2020.crac-1.15,W16-0704,1,0.694459,"Missing"
2020.crac-1.15,L18-1065,1,0.801412,"inal phrase (np.ant), verbal phrase (vp.ant), clauses (clause.ant); 6. chain properties: • number of chains: total number (nr.chain) • chain length: mean chain length (mn.chain.lngth), median chain length (mdn.chain.lngth), standard deviation of chain length (stddv.chain.lngth), longest chain (lngst.chain), number of shortest, i.e. two-member chains (m2.chain), three-member chain (m3.chain), four-member chain (m4.chain) and five and more member chain (m5.chain) • distance between chain members measured in sentences (chain.dist). 4 Data and Methods 4.1 Data For our analyses, we use ParCorFull (Lapshinova-Koltunski et al., 2018), a parallel corpus of EnglishGerman translations that is manually annotated for full coreference chains1 . Coreference chains in this corpus consist of (mostly) chain-initial antecedents and anaphoric expressions that include pronouns, nouns, nominal phrases. Verbal phrases and clauses are also included as antecedents of event anaphors. The authors annotated elliptical constructions and cases of substitution, see details described by Lapshinova-Koltunski and Hardmeier (2017) and Lapshinova-Koltunski et al. (2018). The corpus contains transcribed TED talks and news texts in English (EN) and th"
2020.crac-1.15,D19-6501,1,0.889336,"Missing"
2020.crac-1.15,C14-1003,0,0.0250275,"laims. There are a few corpus-based studies of coreference translation (Zinsmeister et al., 2012; Nov´ak and Nedoluzhko, 2015; Lapshinova-Koltunski et al., 2019b), addressing mostly the challenge of translating pronouns. The awareness of this challenge has also increased in the MT community (Voita et al., 2019; Lapshinova-Koltunski et al., 2019a; Guillou et al., 2018; Bawden et al., 2018; Miculicich Werlen and Popescu-Belis, 2017; Guillou, 2016; Hardmeier and Federico, 2010), and its relevance for multilingual coreference resolution is beyond doubt (Green ˇ et al., 2011; Nov´ak and Zabokrtsk´ y, 2014; Grishina and Stede, 2017). Still, coreference translation is affected by many factors and remains poorly understood. Kunz et al. (2017) analyse coreference and other means of explicit discourse phenomena in English and German comparable texts. They find that English and German differ in the linguistic means available in their language systems to convey coreference. English provides less syntactic flexibility and is restricted in the distribution of referents. German has more options and tends to use more grammatical means of coreference than English (Kunz et al., 2017), which indicates that"
2020.crac-1.15,W18-0709,0,0.0256775,"Missing"
2020.crac-1.15,postolache-etal-2006-transferring,0,0.0808954,"ation in coreference devices depends on the following factors: (a) language-specific constraints, (b) functional variation across language registers as well as (spoken or written) mode and (c) effects of the translation process. Translating between languages involves transformation of the source coreference patterns into the target ones. Analysing such patterns can give insights into translation strategies for referring expressions in texts. Variation along the above stated lines (a, b, c) causes a number of problems in multilingual coreference resolution or coreference annotation projection (Postolache et al., 2006; Ogrodniczuk, 2013; Grishina and Stede, 2015; Nov´ak, 2018). Although several studies describe such problems (Grishina and Stede, 2015; Lapshinova-Koltunski and Hardmeier, 2017; Lapshinova-Koltunski et al., 2019b), there is still a lack of understanding as to which linguistic phenomena and concretely, which structures cause these problems. In this paper, we attempt to detect such phenomena for English-German translations in a data set containing two different text registers: TED talks, which represent spoken language, and news, a type of written discourse. Previous studies show that the choic"
2020.lrec-1.12,2020.lrec-1.467,0,0.024014,"Missing"
2020.lrec-1.12,W18-6003,0,0.0356447,"Missing"
2020.lrec-1.12,W05-0406,0,0.177722,"Missing"
2020.lrec-1.12,2005.mtsummit-papers.11,0,0.286122,"Missing"
2020.lrec-1.12,D13-1030,0,0.0217693,"along the lines of If the Union accepts, could not we implement. . . . In English, however, the it must be taken cataphorically with the this referring to the need to establish standards. . . and the exemplification sentence that follows. In example , on the other hand, the English it refers to all what has previously been mentioned in the long sentence, a typical ‘event’ reading of the pronoun. The French translation, however, prefers a translation with a full lexical noun phrase ces questions (these questions) for the same referential relationship. This is a particular case of a shell-noun (Kolhatkar et al., 2013), and we believe that our method might be useful in identifying this phenomenon using multilingual parallel data. The task could also be approached semantically by identifying all abstract nouns referencing actions, nominalizations, or eventualities in the text. Alternatively, one could focus on particular syntactic configurations as Marasovic et al. (2017). Non-nominal co-reference is much less studied than nominal coreference, partly because of the lack of annotated corpora. In this paper, we have explored the possibility of exploiting parallel multilingual corpora as a means of cheap superv"
2020.lrec-1.12,J18-3007,0,0.0122098,"dans le cadre des interventions d’une minute et je suis heureux d’avoir finalement r´eussi. Il est int´eressant que M. Rogalski ait e´ t´e autoris´e a` prendre la parole trois fois dans l’intervalle. 1. E NTITY READING 2. Madam President, I have been deluged with messages from growers from all over the south-east of England who regard this proposal as near catastrophic. It will result, they tell me, in smaller crops and in higher prices. Related Work Reference to non-nominal antecedents has largely been a niche area in NLP research. It is extensively surveyed in detail in a recent article by Kolhatkar et al. (2018). The most extensive annotation efforts in the field of coreference resolution have focused on nominal coreference. OntoNotes (Pradhan et al., 2013), the largest and most frequently used corpus for training coreference resolution systems, for instance, only includes verbs if “they can be co-referenced ∗ Work completed while the first author was affiliated with the Department of Philosophy, Linguistics and Theory of Science, University of Gothenburg. 99 with an existing noun phrase” according to its guidelines. Corpora with a richer annotation of event pronouns exist, but are much smaller. The"
2020.lrec-1.12,L18-1065,1,0.890527,"Missing"
2020.lrec-1.12,P16-3020,0,0.0539066,"Missing"
2020.lrec-1.12,D17-1018,0,0.0353705,"Missing"
2020.lrec-1.12,D17-1137,1,0.900528,"Missing"
2020.lrec-1.12,D17-1021,0,0.0547188,"Missing"
2020.lrec-1.12,P07-1103,0,0.142518,"Missing"
2020.lrec-1.12,W18-0702,0,0.04441,"Missing"
2020.lrec-1.12,W13-3516,0,0.0786708,"Missing"
2020.lrec-1.12,tiedemann-2012-parallel,0,0.0160633,"Missing"
2020.lrec-1.12,D18-1528,0,0.0309988,"Missing"
2020.wmt-1.58,W19-5301,0,0.033318,"Missing"
2020.wmt-1.58,W16-2348,1,0.830411,"dialogues, which relate to ordering or reserving products and services from a limited set of providers, also follow fairly strong scripts and are anchored in a small discourse universe deﬁned by the products on offer. Their context sensitivity is therefore counterbalanced by domain-speciﬁc conventions and expectations. 1 http://www.statmt.org/wmt20/ chat-task_results_DA.html 2 We tested AllenNLP’s coreference resolution tool (Gardner et al., 2018) on a few examples where pronoun resolution seemed relevant and found that it performed very poorly in these cases, conﬁrming similar conclusions by Bawden (2016). We therefore decided not to model coreference explicitly. 3 http://github.com/chardmeier/ WMT2020-Chat 473 Proceedings of the 5th Conference on Machine Translation (WMT), pages 473–478 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Data The task data consists of parallel task-oriented dialogues between an agent (English) and a customer (German) across six domains: (i) ordering pizza, (ii) making auto repair appointments, (iii) ordering a taxi, (iv) ordering movie tickets, (v) ordering coffee and (vi) making restaurant reservations. The dialogues were initial"
2020.wmt-1.58,W18-6412,0,0.0351884,"Missing"
2020.wmt-1.58,1983.tc-1.13,0,0.102872,"Missing"
2020.wmt-1.58,N18-1118,1,0.843979,"nden. <taxi> <speaker=customer> Perfekt. In Ordnung, verstanden. Table 1: Examples from the dataset annotated with variants of speaker and domain tags. full-dialogue classiﬁer for our submission. Context-level MT Finally, we explore using linguistic context (varying numbers of previous utterances) to improve translation, with the aim that previous context can provide vital information for disambiguation or adaptation. We use the approach of concatenating varying numbers of previous sentences to the current sentence, separated by a sentence boundary token <break> (Tiedemann and Scherrer, 2017; Bawden et al., 2018). This simple strategy was shown to be one of the most effective in a recent comparison of document-level MT approaches (Lopes et al., 2020). To distinguish between different speakers, we also add the speaker tag to the beginning of every utterance. The models are trained to translate both the context and the utterance into the target language (i.e. n-to-n strategy). The candidate utterance is then extracted from the generated output in a preprocessing step. Since the dialogues are bilingual (the agent and customer are speaking in different languages), the original versions of the previous sen"
2020.wmt-1.58,D19-1459,0,0.100276,"/github.com/chardmeier/ WMT2020-Chat 473 Proceedings of the 5th Conference on Machine Translation (WMT), pages 473–478 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Data The task data consists of parallel task-oriented dialogues between an agent (English) and a customer (German) across six domains: (i) ordering pizza, (ii) making auto repair appointments, (iii) ordering a taxi, (iv) ordering movie tickets, (v) ordering coffee and (vi) making restaurant reservations. The dialogues were initially in English, retrieved from a subset of the TaskMaster-1 dataset (Byrne et al., 2019) and then manually translated into German at Unbabel.4 Although the speaker tags are provided for each utterance, the conversations are not explicitly marked with their task domain. The task being to translate the agent’s utterances from English into German and the customer’s utterances from German to English, we evaluate each translation direction separately, using only the agent’s utterances for en–de translation and the customer’s utterances for de–en. For training however, we use the full set of 13,845 utterances for both directions. 3 Approaches We explore four approaches, each of which i"
2020.wmt-1.58,W18-2501,0,0.0148526,"t sensitivities, as the customer and the agent engage in a common activity and continually react to each other’s utterances (Hardmeier, 2014; Bawden, 2018). However, the dialogues, which relate to ordering or reserving products and services from a limited set of providers, also follow fairly strong scripts and are anchored in a small discourse universe deﬁned by the products on offer. Their context sensitivity is therefore counterbalanced by domain-speciﬁc conventions and expectations. 1 http://www.statmt.org/wmt20/ chat-task_results_DA.html 2 We tested AllenNLP’s coreference resolution tool (Gardner et al., 2018) on a few examples where pronoun resolution seemed relevant and found that it performed very poorly in these cases, conﬁrming similar conclusions by Bawden (2016). We therefore decided not to model coreference explicitly. 3 http://github.com/chardmeier/ WMT2020-Chat 473 Proceedings of the 5th Conference on Machine Translation (WMT), pages 473–478 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Data The task data consists of parallel task-oriented dialogues between an agent (English) and a customer (German) across six domains: (i) ordering pizza, (ii) making aut"
2020.wmt-1.58,N19-4009,0,0.0208817,"terances are not from the same speaker, they must ﬁrst be translated by the MT model in the opposite language direction for them to be used as context for the current sentence. 4 2019a and FAIR (Facebook; Ng et al. 2019). All models are transformer-big models (Vaswani et al., 2017): 6 encoder and 6 decoder layers, model dimension of 1024, 16 heads except that UEDIN has a feedforward dimension of 4096 for both the encoder and decoder, and FAIR models increase this dimension to 8192 in the encoder. UEDIN models are implemented in Marian (Junczys-Dowmunt et al., 2018) and FAIR models in Fairseq (Ott et al., 2019). Both model types are trained on parallel and backtranslated monolingual data from the WMT’19 news translation shared task (Barrault et al., 2019). For our ﬁnal submission (using the base FAIR model), we also use noisy channel reranking (Yee et al., 2019), which requires MT models in both directions and a (target) language model. We describe the data processing techniques in Appendix A and list the hyper-parameters in Appendix B. 5 Experimental Results and Analysis We report automatic evaluation results in Section 5.1 and provide a qualitative manual comparison in Section 5.2. 5.1 Automatic e"
2020.wmt-1.58,W18-6478,0,0.0250646,"Pretraining To account for the limited indomain data, we use pre-existing MT models trained for the WMT’19 news task (Barrault et al., 2019) and then continue training on pseudo-indomain web crawled data from the Paracrawl project5 (Ba˜no´ n et al., 2020), before ﬁne-tuning on the in-domain chat training data. We compare two different base systems for each language direction: UEDIN models6 ((Bawden et al., 2019a) and FAIR models (Ng et al., 2019). The pseudo-in-domain data on which training is continued is created by ﬁltering Paracrawl data using dual conditional noisy cross-entropy ﬁltering (Junczys-Dowmunt, 2018). This consists in training a neural language model for each language on the task training data, and jointly scoring each parallel sentence in Paracrawl using the two models. We take the top scoring 2.5 million subset of the original 34 million en–de sentences (those that most resemble the task data). 4 https://github.com/Unbabel/BConTrasT https://www.paracrawl.eu 6 Although the WMT’19 submission included only de–en, we also use the similarly trained model for en–de. 5 Speaker adaption Distinguishing between the two speaker roles is important as they have different contributions to the dialogu"
2020.wmt-1.58,P02-1040,0,0.106385,"l and backtranslated monolingual data from the WMT’19 news translation shared task (Barrault et al., 2019). For our ﬁnal submission (using the base FAIR model), we also use noisy channel reranking (Yee et al., 2019), which requires MT models in both directions and a (target) language model. We describe the data processing techniques in Appendix A and list the hyper-parameters in Appendix B. 5 Experimental Results and Analysis We report automatic evaluation results in Section 5.1 and provide a qualitative manual comparison in Section 5.2. 5.1 Automatic evaluation results We report BLEU scores (Papineni et al., 2002), calculated with SACREBLEU8 (Post, 2018) on the dev set (beam size of 4). Experimental setup We compare two neural MT base system types, both WMT’19 news translation task submissions: UEDIN (University of Edinburgh; Bawden et al. 475 Pretraining The results in Table 2 show that indomain ﬁne-tuning of the pretrained models always gives large gains. The pre-trained FAIR models are better than the pre-trained UEDIN models (Barrault et al., 2019). Fine-tuning on ﬁltered paracrawl and then on the in-domain data gives a slight gain for the UEDIN models (particularly for de–en) but slightly degrades"
2020.wmt-1.58,P07-2045,0,0.0152596,"Missing"
2020.wmt-1.58,D18-2012,0,0.0229367,"Missing"
2020.wmt-1.58,2020.eamt-1.24,1,0.719204,"main tags. full-dialogue classiﬁer for our submission. Context-level MT Finally, we explore using linguistic context (varying numbers of previous utterances) to improve translation, with the aim that previous context can provide vital information for disambiguation or adaptation. We use the approach of concatenating varying numbers of previous sentences to the current sentence, separated by a sentence boundary token <break> (Tiedemann and Scherrer, 2017; Bawden et al., 2018). This simple strategy was shown to be one of the most effective in a recent comparison of document-level MT approaches (Lopes et al., 2020). To distinguish between different speakers, we also add the speaker tag to the beginning of every utterance. The models are trained to translate both the context and the utterance into the target language (i.e. n-to-n strategy). The candidate utterance is then extracted from the generated output in a preprocessing step. Since the dialogues are bilingual (the agent and customer are speaking in different languages), the original versions of the previous sentences can be either in English or in German. While we always translate both the context and the current sentence into the target language o"
2020.wmt-1.58,W19-5333,0,0.214509,"of which is detailed below: (i) pretraining using additional data sources, (ii) speaker adaptation, (iii) domain adaptation and (iv) incorporating previous context. Pretraining To account for the limited indomain data, we use pre-existing MT models trained for the WMT’19 news task (Barrault et al., 2019) and then continue training on pseudo-indomain web crawled data from the Paracrawl project5 (Ba˜no´ n et al., 2020), before ﬁne-tuning on the in-domain chat training data. We compare two different base systems for each language direction: UEDIN models6 ((Bawden et al., 2019a) and FAIR models (Ng et al., 2019). The pseudo-in-domain data on which training is continued is created by ﬁltering Paracrawl data using dual conditional noisy cross-entropy ﬁltering (Junczys-Dowmunt, 2018). This consists in training a neural language model for each language on the task training data, and jointly scoring each parallel sentence in Paracrawl using the two models. We take the top scoring 2.5 million subset of the original 34 million en–de sentences (those that most resemble the task data). 4 https://github.com/Unbabel/BConTrasT https://www.paracrawl.eu 6 Although the WMT’19 submission included only de–en, we also"
2020.wmt-1.58,W18-6319,0,0.0210022,"9 news translation shared task (Barrault et al., 2019). For our ﬁnal submission (using the base FAIR model), we also use noisy channel reranking (Yee et al., 2019), which requires MT models in both directions and a (target) language model. We describe the data processing techniques in Appendix A and list the hyper-parameters in Appendix B. 5 Experimental Results and Analysis We report automatic evaluation results in Section 5.1 and provide a qualitative manual comparison in Section 5.2. 5.1 Automatic evaluation results We report BLEU scores (Papineni et al., 2002), calculated with SACREBLEU8 (Post, 2018) on the dev set (beam size of 4). Experimental setup We compare two neural MT base system types, both WMT’19 news translation task submissions: UEDIN (University of Edinburgh; Bawden et al. 475 Pretraining The results in Table 2 show that indomain ﬁne-tuning of the pretrained models always gives large gains. The pre-trained FAIR models are better than the pre-trained UEDIN models (Barrault et al., 2019). Fine-tuning on ﬁltered paracrawl and then on the in-domain data gives a slight gain for the UEDIN models (particularly for de–en) but slightly degrades the FAIR models. We choose to take as a"
2020.wmt-1.58,N16-1005,0,0.0245163,"ces (those that most resemble the task data). 4 https://github.com/Unbabel/BConTrasT https://www.paracrawl.eu 6 Although the WMT’19 submission included only de–en, we also use the similarly trained model for en–de. 5 Speaker adaption Distinguishing between the two speaker roles is important as they have different contributions to the dialogue; the customer’s utterances are short, interrogative and informal, while the agent’s utterances are often long, informative and more formal. We adapt our models to each speaker by using the speaker identity (provided with the task data) as a pseudo-token (Sennrich et al., 2016a): we prepend a speaker tag to each utterance on both the source and the target side. Domain adaptation Knowing which task the dialogue belongs to (e.g. pizza, ﬁlm) can be important for disambiguation, as described in Section 1. Similarly to speaker adaptation, we adapt to the different tasks (i.e. domains) by prepending a domain tag to each utterance on both the source and target side. We also consider a setup where all the utterances are tagged with speaker and domain-tags (see the example in Table 1). The dataset consists of chats across six different domains (pizza, auto, taxi, movie, cof"
2020.wmt-1.58,P16-1162,0,0.00680666,"ces (those that most resemble the task data). 4 https://github.com/Unbabel/BConTrasT https://www.paracrawl.eu 6 Although the WMT’19 submission included only de–en, we also use the similarly trained model for en–de. 5 Speaker adaption Distinguishing between the two speaker roles is important as they have different contributions to the dialogue; the customer’s utterances are short, interrogative and informal, while the agent’s utterances are often long, informative and more formal. We adapt our models to each speaker by using the speaker identity (provided with the task data) as a pseudo-token (Sennrich et al., 2016a): we prepend a speaker tag to each utterance on both the source and the target side. Domain adaptation Knowing which task the dialogue belongs to (e.g. pizza, ﬁlm) can be important for disambiguation, as described in Section 1. Similarly to speaker adaptation, we adapt to the different tasks (i.e. domains) by prepending a domain tag to each utterance on both the source and target side. We also consider a setup where all the utterances are tagged with speaker and domain-tags (see the example in Table 1). The dataset consists of chats across six different domains (pizza, auto, taxi, movie, cof"
2020.wmt-1.58,W17-4811,0,0.060726,"i> Perfekt. In Ordnung, verstanden. <taxi> <speaker=customer> Perfekt. In Ordnung, verstanden. Table 1: Examples from the dataset annotated with variants of speaker and domain tags. full-dialogue classiﬁer for our submission. Context-level MT Finally, we explore using linguistic context (varying numbers of previous utterances) to improve translation, with the aim that previous context can provide vital information for disambiguation or adaptation. We use the approach of concatenating varying numbers of previous sentences to the current sentence, separated by a sentence boundary token <break> (Tiedemann and Scherrer, 2017; Bawden et al., 2018). This simple strategy was shown to be one of the most effective in a recent comparison of document-level MT approaches (Lopes et al., 2020). To distinguish between different speakers, we also add the speaker tag to the beginning of every utterance. The models are trained to translate both the context and the utterance into the target language (i.e. n-to-n strategy). The candidate utterance is then extracted from the generated output in a preprocessing step. Since the dialogues are bilingual (the agent and customer are speaking in different languages), the original versio"
2020.wmt-1.58,J82-2005,0,0.536531,"Missing"
2021.nodalida-main.34,W17-4717,0,0.0287335,"Missing"
2021.nodalida-main.34,W17-4773,0,0.0378549,"agreement errors, mistranslations of proper names (e.g., Lena as Sarah), or the incorrect use or omission of subjunctive mood in conditional sentences. 7 Related Work Our work draws on two strands of research: automatic post-editing and context-aware MT. Automatic post-editing has a long history in MT (Knight and Chander, 1994), with regular shared tasks (Bojar et al., 2015, 2016, 2017). Neural multi-source APE systems as first proposed by Pal et al. (2016) and Junczys-Dowmunt and Grundkiewicz (2016), some of them including source language information (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Libovický and Helcl, 2017), have come to dominate APE. We take inspiration from the top-performing systems at the WMT19 shared task for architectures and training/decoding tricks (Chatterjee et al., 2019), and make heavy use of synthetic training data (Sennrich et al., 2016a; Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019). Neural context-aware MT can be achieved by integrating context into the main translation model (Jean et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia). Two-stage models with a sentence-level first pass and document-level second pas"
2021.nodalida-main.34,P07-2045,0,0.00804036,"ts the consistent transliteration of proper names into Cyrillic script. These two sets are independent of source context by design, as the model is only evaluated on the generation of consistent repetitions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implem"
2021.nodalida-main.34,P17-2031,0,0.0186668,"nslations of proper names (e.g., Lena as Sarah), or the incorrect use or omission of subjunctive mood in conditional sentences. 7 Related Work Our work draws on two strands of research: automatic post-editing and context-aware MT. Automatic post-editing has a long history in MT (Knight and Chander, 1994), with regular shared tasks (Bojar et al., 2015, 2016, 2017). Neural multi-source APE systems as first proposed by Pal et al. (2016) and Junczys-Dowmunt and Grundkiewicz (2016), some of them including source language information (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Libovický and Helcl, 2017), have come to dominate APE. We take inspiration from the top-performing systems at the WMT19 shared task for architectures and training/decoding tricks (Chatterjee et al., 2019), and make heavy use of synthetic training data (Sennrich et al., 2016a; Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019). Neural context-aware MT can be achieved by integrating context into the main translation model (Jean et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia). Two-stage models with a sentence-level first pass and document-level second pass have been explored for sce"
2021.nodalida-main.34,C04-1072,0,0.21142,"to achieve this. A conservativeness penalty (Junczys-Dowmunt and Grundkiewicz, 2016), denoted c, penalises the score of each prediction that is not in src or mt. Formally, let Vc = Vsrc ∪ Vmt be the subset of the full vocabulary V that occurs in an input segment. Given a |V |-sized vector of candidates ht at time step t, the score of each candidate v is defined as:  ht (v) − c if v ∈ V Vc ht (v) = (1) ht (v) otherwise. Second, similar to Lopes et al. (2019), we apply a data weighting strategy during training. We assign each training sample a weight that is defined as BLEUsmooth (mt, ref ) (Lin and Och, 2004) to upweight samples that require little post-editing. 3 Data and Preprocessing We use all of the English-to-Russian data released by Voita et al. (2019a)1 , including: (1) 6M context1 https://github.com/lena-voita/goodtranslation-wrong-in-context Model Deixis Lex.c. Ell.infl. Ell.VP BLEU Results reported by Voita et al. (2019a): Baseline 50.0 45.9 53.0 DocRepair 91.8 80.6 86.4 28.4 75.2 32.41 34.60 Our experiments: DocRepair DocRepair (+P) Transference Transference (+P) 69.0 71.8 73.0 82.8 32.69 32.38 30.56 32.53 88.6 87.6 86.8 87.8 70.5 67.6 62.9 65.4 83.8 82.2 81.6 84.8 Experiments marked +"
2021.nodalida-main.34,L18-1275,0,0.0122857,"U Results reported by Voita et al. (2019a): Baseline 50.0 45.9 53.0 DocRepair 91.8 80.6 86.4 28.4 75.2 32.41 34.60 Our experiments: DocRepair DocRepair (+P) Transference Transference (+P) 69.0 71.8 73.0 82.8 32.69 32.38 30.56 32.53 88.6 87.6 86.8 87.8 70.5 67.6 62.9 65.4 83.8 82.2 81.6 84.8 Experiments marked +P use the ParData corpus. Table 1: BLEU score on general test set and accuracy on contrastive test sets (deixis, lexical consistency, ellipsis (inflection), and VP ellipsis). agnostic and 1.5M context-aware (4 consecutive sentences in each sample) data from the OpenSubtitles2018 corpus (Lison et al., 2018); (2) Russian monolingual data in 30M groups of 4 consecutive sentences gathered by Voita et al. (2019a). We reuse the synthetic training data for APE generated by Voita et al. (2019a), treating Russian monolingual data as ref, a sentence-level English backtranslation as src, and the Russian roundtrip translation as mt. The evaluation data consists of general test sets extracted from the training data and four contrastive test sets to evaluate specific contextual phenomena. The four contrastive test sets have a narrow focus on specific discourse-level phenomena. The “Deixis” set targets consis"
2021.nodalida-main.34,W19-5413,0,0.0188685,"entence-level main model, we need to control how aggressively APE can modify mt to prevent over-correction. We adopt two strategies from the APE literature to achieve this. A conservativeness penalty (Junczys-Dowmunt and Grundkiewicz, 2016), denoted c, penalises the score of each prediction that is not in src or mt. Formally, let Vc = Vsrc ∪ Vmt be the subset of the full vocabulary V that occurs in an input segment. Given a |V |-sized vector of candidates ht at time step t, the score of each candidate v is defined as:  ht (v) − c if v ∈ V Vc ht (v) = (1) ht (v) otherwise. Second, similar to Lopes et al. (2019), we apply a data weighting strategy during training. We assign each training sample a weight that is defined as BLEUsmooth (mt, ref ) (Lin and Och, 2004) to upweight samples that require little post-editing. 3 Data and Preprocessing We use all of the English-to-Russian data released by Voita et al. (2019a)1 , including: (1) 6M context1 https://github.com/lena-voita/goodtranslation-wrong-in-context Model Deixis Lex.c. Ell.infl. Ell.VP BLEU Results reported by Voita et al. (2019a): Baseline 50.0 45.9 53.0 DocRepair 91.8 80.6 86.4 28.4 75.2 32.41 34.60 Our experiments: DocRepair DocRepair (+P) T"
2021.nodalida-main.34,D18-1325,0,0.0207678,"g on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably. 1 Introduction Neural machine translation (NMT) has significantly improved the state of the art in MT (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) on the sentence level. However, accurate translation requires looking at larger units than individual sentences (Hardmeier, 2014), and context-aware NMT has recently become a popular research direction (Miculicich et al., 2018; Scherrer et al., 2019; Junczys-Dowmunt, 2019). One approach to discourse-level processing in NMT is automatic post-editing of the output of a sentence-level system. DocRepair (Voita et al., 2019a) is a monolingual sequence-to-sequence model to correct inconsistencies in groups of adjacent sentence-level translations, showing improvements for specific discourse-level phenomena such as the generation of inflections in elliptic sentences. The hypotheses explored in this work are (1) that the coherence of the translation can be further improved by exploiting context in the source language, and ("
2021.nodalida-main.34,P16-2046,0,0.0455034,"Missing"
2021.nodalida-main.34,W19-5414,0,0.0303983,"Missing"
2021.nodalida-main.34,P02-1040,0,0.113028,"tions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implementation of the Transference architecture, using the same configuration as Pal et al. (2019).2 Detailed hyperparameters are listed in Appendix A. We train our document-level models on the 30M pairs of syn"
2021.nodalida-main.34,D19-6506,0,0.0178,"matic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably. 1 Introduction Neural machine translation (NMT) has significantly improved the state of the art in MT (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) on the sentence level. However, accurate translation requires looking at larger units than individual sentences (Hardmeier, 2014), and context-aware NMT has recently become a popular research direction (Miculicich et al., 2018; Scherrer et al., 2019; Junczys-Dowmunt, 2019). One approach to discourse-level processing in NMT is automatic post-editing of the output of a sentence-level system. DocRepair (Voita et al., 2019a) is a monolingual sequence-to-sequence model to correct inconsistencies in groups of adjacent sentence-level translations, showing improvements for specific discourse-level phenomena such as the generation of inflections in elliptic sentences. The hypotheses explored in this work are (1) that the coherence of the translation can be further improved by exploiting context in the source language, and (2) that the omission of"
2021.nodalida-main.34,E17-3017,1,0.843703,"Missing"
2021.nodalida-main.34,P16-1009,1,0.922375,"llic script. These two sets are independent of source context by design, as the model is only evaluated on the generation of consistent repetitions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implementation of the Transference architecture, using the same con"
2021.nodalida-main.34,P16-1162,1,0.369121,"llic script. These two sets are independent of source context by design, as the model is only evaluated on the generation of consistent repetitions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implementation of the Transference architecture, using the same con"
2021.nodalida-main.34,W17-4811,0,0.0197597,"nt and Grundkiewicz (2016), some of them including source language information (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Libovický and Helcl, 2017), have come to dominate APE. We take inspiration from the top-performing systems at the WMT19 shared task for architectures and training/decoding tricks (Chatterjee et al., 2019), and make heavy use of synthetic training data (Sennrich et al., 2016a; Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019). Neural context-aware MT can be achieved by integrating context into the main translation model (Jean et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia). Two-stage models with a sentence-level first pass and document-level second pass have been explored for scenarios with asymmetric training data. Voita et al. (2019b) introduces a two-pass model where, unlike in APE, the second-pass is tightly integrated with the first-pass model, reusing its hidden representations. Apart from Voita et al. (2019a), the model closest to ours is by Junczys-Dowmunt (2019), who explored document-level APE, but only manually evaluated its efficacy as part of a large model ensemble. 8 Conclusion Our human evaluation shows that mono"
2021.nodalida-main.34,D19-1081,1,0.849733,"Missing"
2021.nodalida-main.34,P19-1116,1,0.890439,"Missing"
2021.unimplicit-1.7,W18-0528,0,0.0146796,"the training data. The development and test data was verified by human annotators (see Roth and Anthonio, 2021, for details). There are two subsets: • Sentences extracted from the revision history, which later received edits which made the sentence more precise. These are labelled REQ REVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of sentences 39187 3264 3458 Table 2: Statistics of the dataset. Table 1: Examples with generic pronouns that require revision. 2 Req Revision 19599 1632 • Sentences that remained unchanged over multiple revisions of the article. The"
2021.unimplicit-1.7,2020.lrec-1.702,0,0.0355272,"EVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of sentences 39187 3264 3458 Table 2: Statistics of the dataset. Table 1: Examples with generic pronouns that require revision. 2 Req Revision 19599 1632 • Sentences that remained unchanged over multiple revisions of the article. These are labelled KEEP UNREVIS. The dataset includes training, development and test sets. However, the type of edit in case of a revision and the revised version of the target sentence, are available only for the training set. We therefore used k-fold cross-validation to randomly partiti"
2021.unimplicit-1.7,2020.emnlp-main.675,0,0.0610568,"Missing"
2021.unimplicit-1.7,N19-1423,0,0.0177259,"beddings. Therefore, we concatenated the BERT embeddings and neuralcoref vectors. the dimensions of the concatenated output vector are 1418. Mention Extraction 5 Mention BERT Embeddings 5 https://github.com/huggingface/neuralcoref. 60 https://github.com/hanxiao/bert-as-service Model M MB M+MB training data, where we use cross-validation, we report the average scores across the five folds. 6.2 Recall 0.2000 0.6783 0.7273 F1 -score 0.0510 0.3799 0.3797 Acc 0.7123 0.6772 0.6523 Sentence-Level System Table 4: Results of our models on mention-level. For the sentence-level system, we use BERT-Base (Devlin et al., 2019), uncased model (12 transformer blocks, 768 hidden size, 12 attention heads and 110M parameters) fine-tuned with an additional output layer on top of BERT’s final representation. We use the Huggingface Transformers library with TensorFlow and load a pre-trained BERT from the Transformers library. We train this model for 2 epochs with a learning rate of 3 · 10−5 and batch size 32. The mention-level system does not have extracted mentions for all sentences, and therefore does not provide predictions for all sentences. In our combined system we use the predictions from the mention-level system as"
2021.unimplicit-1.7,2021.unimplicit-1.4,0,0.0796605,"un phrase. As a result, our proposed classification model for the task of Revision Requirements Detection is based on extracting mention embeddings for each sentence The Revision Requirements task aims to recognize whether or not a sentence requires revision. Revision Requirements prediction not only acts as a standalone tool for grammar correction but also has potential applications in natural language processing (NLP) such as ambiguity detection, machine translation refinement, sentence understanding, knowledge base construction, etc. The shared task on implicit and underspecified language (Roth and Anthonio, 2021)1 aims to provide a binary classification for revision requirements to make a prediction of whether sentences in instructional texts require revision to improve understandability. Since instructional texts must be clear enough so that readers and machines can actually achieve the goal described by the instructions, this task focuses on modeling implicit elements that make the sentence more precise and clear. The dataset used in this shared task consists of instances from wikiHowToImprove, a collection of instructional texts, which has recently been introduced 1 https://unimplicit.github.io 58"
2021.unimplicit-1.7,P14-2066,0,0.0142564,"ow.com articles. These how-to articles cover many fields such as Arts and Entertainment, Computers and Electronics, Health, along with their revision history. The revisions and classes were extracted automatically from the training data. The development and test data was verified by human annotators (see Roth and Anthonio, 2021, for details). There are two subsets: • Sentences extracted from the revision history, which later received edits which made the sentence more precise. These are labelled REQ REVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of senten"
2021.unimplicit-1.7,P17-1144,0,0.0224886,"ubsets: • Sentences extracted from the revision history, which later received edits which made the sentence more precise. These are labelled REQ REVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of sentences 39187 3264 3458 Table 2: Statistics of the dataset. Table 1: Examples with generic pronouns that require revision. 2 Req Revision 19599 1632 • Sentences that remained unchanged over multiple revisions of the article. These are labelled KEEP UNREVIS. The dataset includes training, development and test sets. However, the type of edit in case of a revision a"
C16-1088,D14-1179,0,0.0143101,"Missing"
C16-1088,P07-2053,0,0.0399539,"Missing"
C16-1088,P07-2045,0,0.00683414,"Missing"
C16-1088,D15-1176,0,0.0309526,"Missing"
C16-1088,P03-1021,0,0.0278132,"Missing"
C16-1088,W11-1512,0,0.043462,"Missing"
C16-1088,W11-0415,0,0.0634695,"Missing"
C16-1088,A97-1014,0,0.463453,"Missing"
D12-1108,W09-1114,0,0.0292868,"t promising step. Our main contribution with respect to the work by Langlais et al. (2007) is the introduction of the possibility of handling document-level models by lifting the assumption of sentence independence. As a consequence, enumerating the entire neighbourhood becomes too expensive, which is why we resort to a “first-choice” strategy that non-deterministically generates states and accepts the first one encountered that meets the acceptance criterion. More recently, Gibbs sampling was proposed as a way to generate samples from the posterior distribution of a phrase-based SMT decoder (Arun et al., 2009; Arun et al., 2010), a process that resembles local search in its use of a set of state-modifying operators to generate a sequence of decoder states. Where local search seeks for the best state attainable from a given initial state, Gibbs sampling produces a representative sample from the posterior. Like all work on SMT decoding that we know of, the Gibbs sampler presented by Arun et al. (2010) assumes independence of sentences and considers the complete neighbourhood of each state before taking a sample. 6 Conclusion In the last twenty years of SMT research, there has been a strong assumptio"
D12-1108,W11-1014,0,0.00662194,"Missing"
D12-1108,P01-1030,0,0.0117476,"d to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advo1187 cated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mention"
D12-1108,N03-1010,0,0.0154231,"ins from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advo1187 cated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustra"
D12-1108,D11-1084,0,0.369773,"guage Learning, pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by the application of a s"
D12-1108,2010.iwslt-papers.10,1,0.837142,"ocument is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function. This setup gives us complete freedom to define scoring functions over the entire document. Moreover, by optionally initialising the st"
D12-1108,W11-2123,0,0.0368737,"is important to keep 1180 The problem addressed by the decoder is the search for the state Sˆ with maximal score, such that Sˆ = arg max f (S). S (5) The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative infinity in order to limit the gaps between the coverage sets of adjacent anchored phrase pairs to a maximum value. In DP search, the distortion limit is usually enforced directly by the search algorithm and is not added as a feature. In our decoder, however, this restriction is not required to limit complexity, so we deci"
D12-1108,D07-1103,0,0.0127504,"left to right until the whole range [p; q] is covered. 4 goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. Experimental Results In this section,"
D12-1108,P10-4006,0,0.0117936,"Missing"
D12-1108,N03-1017,0,0.283168,"tic phenomena such as pronominal anaphora cannot be translated correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elim"
D12-1108,P07-2045,0,0.0678837,"to a linear combination of K feature functions hk (S), each with a constant weight λk , so K f (S) = SMT Model (1) ∑ λk hk (S). (4) k=1 Our decoder is based on local search, so its state at any time is a representation of a complete translation of the entire document. Even though the decoder operates at the document level, it is important to keep 1180 The problem addressed by the decoder is the search for the state Sˆ with maximal score, such that Sˆ = arg max f (S). S (5) The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative"
D12-1108,2007.tmi-papers.13,0,0.889821,"oves are rejected in a row (rejection limit). Algorithm 1 Decoding algorithm Input: an initial document state S; search parameters maxsteps and maxrejected Output: a modified document state 1: nsteps ← 0 2: nrejected ← 0 3: while nsteps < maxsteps and nrejected < maxrejected do 4: S0 ← Neighbour(S) 5: if Accept( f (S0 ), f (S)) then 6: S ← S0 7: nrejected ← 0 8: else 9: nrejected ← nrejected + 1 10: end if 11: nsteps ← nsteps + 1 12: end while 13: return S A notable difference between this algorithm and other hill climbing algorithms that have been used for SMT decoding (Germann et al., 2004; Langlais et al., 2007) is its non-determinism. Previous work for sentence-level decoding employed a steepest ascent strategy which amounts to enumerating the complete neighbourhood of the current state as defined by the state operations and selecting the next state to be the best state found in the neighbourhood of the current one. Enumerating all neighbours of a given state, costly as it is, has the advantage that it makes it easy to prove local optimality of a state by recognising that all possible successor states have lower scores. It can be rather inefficient, since at every step only one modification will be"
D12-1108,2008.jeptalnrecital-long.12,0,0.0149019,"al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advo1187 cated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustrated with the example of a reversed n-gram language model, which the authors claim would be difficult to implement in a beam search decoder. Similarly to the work by Germann et al. (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. Our main contribu"
D12-1108,W10-1737,0,0.219847,"Missing"
D12-1108,W01-1408,0,0.0825319,"ring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of"
D12-1108,P03-1021,0,0.0172753,"e training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. Experimental Results In this section, we present the results of a series of experiments with our document decoder. The 1184 Stability An important difference between our decoder and the classical DP decoder as well as previous work in SMT decoding with local search is that our decoder is inherently"
D12-1108,P02-1040,0,0.105323,"se in scores for all three test sets demonstrates that the hill climbing decoder manages to fix some of the search errors made by the DP search. The last row contains the scores obtained by adding in the semantic language model. Scores are presented for three publicly available test sets from recent WMT Machine Translation shared tasks, of which one (newstest2009) was used to monitor progress during development and select the final model. Adding the semantic language model results in a small increase in NIST scores (Doddington, 2002) for all three test sets as well as a small BLEU score gain (Papineni et al., 2002) for two out of three corpora. We note that the NIST score turned out to react more sensitively to improvements due to the semantic LM in all our experiments, which is reasonable because the model specifically targets content words, which benefit from the information weighting done by the NIST score. While the results we present do not constitute compelling evidence in favour of our semantic LM in its current form, they do suggest that this model could be improved to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examin"
D12-1108,W10-1728,1,0.859272,"ational Natural c Language Learning, pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by"
D12-1108,J03-1005,0,0.0104688,"ential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of final words with a bette"
D12-1108,P97-1037,0,0.0171941,"correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a"
D12-1108,D07-1053,0,0.0691649,"Missing"
D13-1037,S10-1021,0,0.134997,"to interesting insights about anaphora resolution in a multi-lingual context. In particular, we show in this paper that the pronoun prediction task makes it possible to model the resolution of pronominal anaphora as a latent variable and opens up a way to solve a task relying on anaphora resolution without using any data annotated for anaphora. This is what we consider the main contribution of our present work. We start by modelling cross-lingual pronoun prediction as an independent machine learning task after doing anaphora resolution in the source language (English) using the BART software (Broscheit et al., 2010). We show that it is difficult to achieve satisfactory performance with standard maximum380 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 380–391, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics The latest version released in March is equipped with ... It is sold at ... La dernière version lancée en mars est dotée de ... • est vendue ... Figure 1: Task setup entropy classifiers especially for low-frequency pronouns such as the French feminine plural pronoun elles. We propose a neural network classifi"
D13-1037,2012.eamt-1.60,0,0.0357142,"vely. The target words are represented as one-hot vectors with the dimensionality of the target language vocabulary. These vectors are then averaged to yield a single vector per antecedent candidate. Finally, the vectors of all candidates for a given training example are weighted by the probabilities assigned to them by the anaphora resolver (p1 and p2 ) and summed to yield a single vector per training example. 3 Data Sets and External Tools We run experiments with two different test sets. The TED data set consists of around 2.6 million tokens of lecture subtitles released in the WIT3 corpus (Cettolo et al., 2012). The WIT3 training data yields 71,052 examples, which were randomly partitioned into a training set of 63,228 examples and a test set of 7,824 examples. The official WIT3 development and test sets were not used in our experiments. The news-commentary data set is version 6 of the parallel news-commentary corpus released as a part of the WMT 2011 training data1 . It contains around 2.8 million tokens of news text and yields 31,017 data points, 1 http://www.statmt.org/wmt11/translation-task. html (3 July 2013). 382 The feature setup of all our classifiers requires the detection of potential ante"
D13-1037,E09-1018,0,0.0150325,"ignments to project coreference annotations from one language to another with a view to training anaphora resolvers in the target language (Postolache et al., 2006; de Souza and Or˘asan, 2011). Rahman and Ng (2012) instead use machine translation to translate their test 389 data into a language for which they have an anaphora resolver and then project the annotations back to the original language. Completely unsupervised monolingual anaphora resolution has been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named entity recognition and semantic role labelling (Collobert et al., 2011). In tasks related to anapho"
D13-1037,W05-0612,0,0.0253418,"ther work has used word alignments to project coreference annotations from one language to another with a view to training anaphora resolvers in the target language (Postolache et al., 2006; de Souza and Or˘asan, 2011). Rahman and Ng (2012) instead use machine translation to translate their test 389 data into a language for which they have an anaphora resolver and then project the annotations back to the original language. Completely unsupervised monolingual anaphora resolution has been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named entity recognition and semantic role labelling (Collobert et al., 2011)"
D13-1037,E12-3001,0,0.373892,"with good results to project coreference annotations from one language into another by using word alignments (Postolache et al., 2006; Rahman and Ng, 2012). On the other hand, what is true in general need not be true for all types of linguistic elements. For instance, a substantial percentage of the English thirdperson subject pronouns he, she, it and they does not get realised as pronouns in French translations (Hardmeier, 2012). Moreover, it has been recognised by various authors in the statistical machine translation (SMT) community (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012) that pronoun translation is a difficult problem because, even when a pronoun does get translated as a pronoun, it may require choosing the correct word form based on agreement features that are not easily predictable from the source text. The work presented in this paper investigates the problem of cross-lingual pronoun prediction for English-French. Given an English pronoun and its discourse context as well as a French translation of the same discourse and word alignments between the two languages, we attempt to predict the French word aligned to the English pronoun. As far as we know, this"
D13-1037,2010.iwslt-papers.10,1,0.536436,"this fact has been exploited with good results to project coreference annotations from one language into another by using word alignments (Postolache et al., 2006; Rahman and Ng, 2012). On the other hand, what is true in general need not be true for all types of linguistic elements. For instance, a substantial percentage of the English thirdperson subject pronouns he, she, it and they does not get realised as pronouns in French translations (Hardmeier, 2012). Moreover, it has been recognised by various authors in the statistical machine translation (SMT) community (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012) that pronoun translation is a difficult problem because, even when a pronoun does get translated as a pronoun, it may require choosing the correct word form based on agreement features that are not easily predictable from the source text. The work presented in this paper investigates the problem of cross-lingual pronoun prediction for English-French. Given an English pronoun and its discourse context as well as a French translation of the same discourse and word alignments between the two languages, we attempt to predict the French word aligned to the English pronoun. As far a"
D13-1037,P03-1054,0,0.00907702,"e anaphora resolver BART to generate this information. BART (Broscheit et al., 2010) is an anaphora resolution toolkit consisting of a markable detection and feature extraction pipeline based on a variety of standard natural language processing (NLP) tools and a machine learning component to predict coreference links including both pronominal anaphora and noun-noun coreference. In our experiments, we always use BART’s markable detection and feature extraction machinery. Markable detection is based on the identification of noun phrases in constituency parses generated with the Stanford parser (Klein and Manning, 2003). The set of features extracted by BART is an extension of the widely used mention-pair anaphora resolution feature set by Soon et al. (2001) (see below, Section 6). In the experiments of the next two sections, we also use BART to predict anaphoric links for pronouns. The model used with BART is a maximum entropy ranker trained on the ACE02-npaper corpus (LDC2003T11). In order to obtain a probability distribution over antecedent candidates rather than onebest predictions or coreference sets, we modified the ranking component with which BART resolves pronouns to normalise and output the scores"
D13-1037,N12-1005,0,0.00779725,"been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named entity recognition and semantic role labelling (Collobert et al., 2011). In tasks related to anaphora resolution, standard feedforward neural networks have been tested as a classifier in an anaphora resolution system (Stuckardt, 2007), but the network design presented in our work is novel. 9 Conclusion In this paper, we have introduced cross-lingual pronoun prediction as an independent natural language processing task. Even though it is not an end-to-end task, pronoun prediction is interesting for several reasons. It is related to the problem o"
D13-1037,W10-1737,0,0.450652,"Missing"
D13-1037,J03-1002,0,0.00316037,"ference resolution system (BART) to predict anaphoric links. Anaphora resolution is done by our neural network classifier and requires only some quantity of word-aligned parallel data for training, completely obviating the need for a coreference-annotated training set. 2 Task Setup The overall setup of the classification task we address in this paper is shown in Figure 1. We are given an English discourse containing a pronoun along with its French translation and word alignments between the two languages, which in our case were computed automatically using a standard SMT pipeline with GIZA++ (Och and Ney, 2003). We focus on the four English third-person subject pronouns he, she, it and they. The output of the classifier is a multinomial distribution over six classes: the four French subject pronouns il, elle, ils and elles, corresponding to masculine and feminine singular and plural, respectively; the impersonal pronoun ce/c’, which occurs in some very frequent constructions such as c’est (it is); and a sixth class OTHER, which indicates that none of these pronouns was used. In general, a pronoun may be aligned to multiple words; in this case, a training example is counted as a positive example for"
D13-1037,D08-1068,0,0.0258809,"to English-Czech data to resolve different uses of the pronoun it. Other work has used word alignments to project coreference annotations from one language to another with a view to training anaphora resolvers in the target language (Postolache et al., 2006; de Souza and Or˘asan, 2011). Rahman and Ng (2012) instead use machine translation to translate their test 389 data into a language for which they have an anaphora resolver and then project the annotations back to the original language. Completely unsupervised monolingual anaphora resolution has been approached using, e. g., Markov logic (Poon and Domingos, 2008) and the Expectation-Maximisation algorithm (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). To the best of our knowledge, the direct application of machine learning techniques to parallel data in a task related to anaphora resolution is novel in our work. Neural networks and deep learning techniques have recently gained some popularity in natural language processing. They have been applied to tasks such as language modelling (Bengio et al., 2003; Schwenk, 2007), translation modelling in statistical machine translation (Le et al., 2012), but also part-ofspeech tagging, chunking, named en"
D13-1037,postolache-etal-2006-transferring,0,0.0935963,"Missing"
D13-1037,N12-1090,0,0.203502,"tion When texts are translated from one language into another, the translation reconstructs the meaning or function of the source text with the means of the target language. Generally, this has the effect that the entities occurring in the translation and their mutual relations will display similar patterns as the entities in the source text. In particular, coreference patterns tend to be very similar in translations of a text, and this fact has been exploited with good results to project coreference annotations from one language into another by using word alignments (Postolache et al., 2006; Rahman and Ng, 2012). On the other hand, what is true in general need not be true for all types of linguistic elements. For instance, a substantial percentage of the English thirdperson subject pronouns he, she, it and they does not get realised as pronouns in French translations (Hardmeier, 2012). Moreover, it has been recognised by various authors in the statistical machine translation (SMT) community (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012) that pronoun translation is a difficult problem because, even when a pronoun does get translated as a pronoun, it may require choosing the c"
D13-1037,J01-4004,0,0.560755,"ection and feature extraction pipeline based on a variety of standard natural language processing (NLP) tools and a machine learning component to predict coreference links including both pronominal anaphora and noun-noun coreference. In our experiments, we always use BART’s markable detection and feature extraction machinery. Markable detection is based on the identification of noun phrases in constituency parses generated with the Stanford parser (Klein and Manning, 2003). The set of features extracted by BART is an extension of the widely used mention-pair anaphora resolution feature set by Soon et al. (2001) (see below, Section 6). In the experiments of the next two sections, we also use BART to predict anaphoric links for pronouns. The model used with BART is a maximum entropy ranker trained on the ACE02-npaper corpus (LDC2003T11). In order to obtain a probability distribution over antecedent candidates rather than onebest predictions or coreference sets, we modified the ranking component with which BART resolves pronouns to normalise and output the scores assigned by the ranker to all candidates instead of picking the highest-scoring candidate. 4 Baseline Classifiers In order to create a simple"
D13-1037,J03-4003,0,\N,Missing
D13-1037,sagot-etal-2006-lefff,0,\N,Missing
D17-1137,Q13-1034,0,0.0711784,"Missing"
D17-1137,W05-0406,0,0.465277,"Missing"
D17-1137,P02-1011,0,0.286966,"nts using a M AX E NT classifier trained on gold-standard data and self-training experiments of an R NN trained on silver-standard data, annotated using the M AX E NT classifier. Lastly, we report on an analysis of the strengths of these two models. 1 Christian Hardmeier He lost his job. It came as a total surprise. We propose the identification of the three usage types of it, namely anaphoric, event reference, and Related Work Due to its difficulty, proposals for the identification and the subsequent resolution of abstract anaphora (i.e., event reference) are scarce (Eckert and Strube, 2000; Byron, 2002; Navarretta, 2004; M¨uller, 2007). The automatic detection of instances of pleonastic ‘it’, on the other hand, has been addressed by the non-referential ‘it’ detector NADA (Bergsma and Yarowsky, 2011), and also in the context of several coreference resolution systems, including the Stanford sieve-based coreference resolution system (Lee et al., 2011). The coreference resolution task focuses on the resolution of nominal anaphoric pronouns, de facto grouping our event and pleonastic categories together and discarding both of them. The coreference resolution task can be seen as a two-step proble"
D17-1137,W15-2509,1,0.841334,"lem: mention identification followed by antecedent identification. Identifying instances of pleonastic ‘it’ typically takes place in the mention identification step. The recognition of event reference ‘it’ is, however, to our knowledge not currently included in any such systems, although from a linguistic point of view, event instances are also referential (Boyd et al., 2005). As suggested by Lee et al., (2016), it would be advantageous to incorporate event reference resolution in the second step. In the context of machine translation, work by Le Nagard and Koehn (2010); Nov´ak et al. (2013); Guillou (2015) and Lo´aiciga et al. (2016) have also considered disambiguating the function of the pronoun ‘it’ in the interest of improving pronoun translation into different languages. 1325 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1325–1331 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 3 Disambiguating ‘it’ 3.1 Labeled Data The ParCor corpus (Guillou et al., 2014) and DiscoMT2015.test dataset (Hardmeier et al., 2016) were used as gold-standard data. Under the ParCor annotation scheme, which was used to annot"
D17-1137,W16-2345,1,0.847517,"n performance. For feature 8, adding one of the 26 WordNet (Princeton University, 2010) types of nouns had no effect. The feature combination of noun and adjectives to the left or right also had no effect. Feature ablation tests revealed that while combining all features is beneficial for the prediction of the anaphoric and pleonastic classes, the same is Unlabeled Data Given the small size of the gold-standard data, and with the aim of gaining insight from unstructured and unseen data, we used the M AX E NT classifier to label additional data from the pronoun prediction shared task at WMT16 (Guillou et al., 2016). This new silver-standard training corpus comprises 1,101,922 sentences taken from the Europarl (3,752,440 sentences), News (344,805 sentences) and TED talks (380,072 sentences) sections of the shared task training data. 3.6 RNN Our second system is a bidirectional recurrent neural network (RNN) which reads the context words and then makes a decision based on the representations that it builds. Concretely, it consists on wordlevel embeddings of size 90, two layers of Gated 1327 R EFERENCE RELATIONSHIP (1) NP antecedent in previous 2 sentences e.g. The infectious disease that’s killed more hum"
D17-1137,guillou-etal-2014-parcor,1,0.795967,"corporate event reference resolution in the second step. In the context of machine translation, work by Le Nagard and Koehn (2010); Nov´ak et al. (2013); Guillou (2015) and Lo´aiciga et al. (2016) have also considered disambiguating the function of the pronoun ‘it’ in the interest of improving pronoun translation into different languages. 1325 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1325–1331 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 3 Disambiguating ‘it’ 3.1 Labeled Data The ParCor corpus (Guillou et al., 2014) and DiscoMT2015.test dataset (Hardmeier et al., 2016) were used as gold-standard data. Under the ParCor annotation scheme, which was used to annotate both corpora, pronouns are manually labeled according to their function: anaphoric, event reference, pleonastic, etc. For all instances of ‘it’ in the corpora, we extracted the sentence-internal position of the pronoun, the sentence itself, and the two previous sentences. All examples were shuffled before the corpus was divided, ensuring a balanced distribution of the classes (Table 1). The pronouns ‘this’ and ‘that’, when used as event referenc"
D17-1137,W11-2123,0,0.014313,"un ‘it’ (Guillou, 2016). We therefore automatically substituted all instances of event reference ‘this’ and ‘that’ with ‘it’ to increase the number of training examples. Data set Training Dev Test Total Event 504 157 169 830 Anaphoric 779 252 270 1,301 Pleonastic 221 92 62 375 Total 1,504 501 501 2,506 Table 1: Distribution of classes in the data. 3.2 Baselines We provide two different baselines (MC and LM BASELINE in Table 2). The first is a setting in which all instances are assigned to the majority class it-anaphoric. The second baseline system is a 3-gram language model built using KenLM (Heafield, 2011) and trained on a modified version of the annotated corpus in which every instance of ‘it’ is concatenated with its function (e.g. ‘itevent’). At test time, the ‘it’ position is filled with each of the three it-function labels in turn, the language model is queried, and the highest scoring option is chosen. 3.3 Features We designed features to capture not only the token context, but also the syntactic and semantic context preceding the pronouns and, where appropriate, their antecedents/referents, as well as the pronoun head. We used the output of the POS tagger and dependency parser of Bohnet"
D17-1137,C16-1245,0,0.0224391,"g, but there is room for improvement. The selftraining experiment demonstrated the benefit of combining gold-standard and silver-standard data. We also found that the R NN -C OMBINED system is better at handling difficult and ambiguous referring relationships, while the M AX E NT performed better for the nominal anaphoric case, when the antecedent is close. Since the two models have different strengths, in future work we plan to enrich the training data with re-training instances from the silver data where the two systems agree, in order to reduce the amount of noise, following the example of Jiang et al. (2016). Ultimately, we aim towards integrating the itprediction system within a full machine translation pipeline and a coreference resolution system. In the first case, the different translations of pronoun ‘it’ can be constrained according to their function. In the second case, the performance of a coreference resolution system vs a modified version using the three-way distinction can be measured. Acknowledgments This work was supported by the Swedish Research Council under project 2012-916 DiscourseOriented Statistical Machine Translation. We used computing resources on the Abel cluster, owned by"
D17-1137,P16-3020,0,0.132882,"on the resolution of nominal anaphoric pronouns, de facto grouping our event and pleonastic categories together and discarding both of them. The coreference resolution task can be seen as a two-step problem: mention identification followed by antecedent identification. Identifying instances of pleonastic ‘it’ typically takes place in the mention identification step. The recognition of event reference ‘it’ is, however, to our knowledge not currently included in any such systems, although from a linguistic point of view, event instances are also referential (Boyd et al., 2005). As suggested by Lee et al., (2016), it would be advantageous to incorporate event reference resolution in the second step. In the context of machine translation, work by Le Nagard and Koehn (2010); Nov´ak et al. (2013); Guillou (2015) and Lo´aiciga et al. (2016) have also considered disambiguating the function of the pronoun ‘it’ in the interest of improving pronoun translation into different languages. 1325 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1325–1331 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 3 Disambiguating ‘it’ 3.1"
D17-1137,W16-2351,1,0.687588,"Missing"
D17-1137,loaiciga-etal-2014-english,1,0.902156,"Missing"
D17-1137,W16-2353,0,0.0202704,"ent portions of the test-set. For each category, we test whether M AX E NT is better or worse than RNN-C OMBINED. A * indicates significance at p &lt; 0.001 using McNemar’s χ2 test. Recurrent Units (GRUs) of size 90 as well, and a final softmax layer to make the predictions. The network uses a context window of 50 tokens both to the left and right of the ‘it’ to be predicted. The features described above are also fed to the network in the form of one-hot vectors. The system uses the adam optimizer and the categorical crossentropy loss function. We chose this architecture following the example of Luotolahti et al. (2016), who built a system for the related task of crosslingual pronoun prediction. 4 Discussion We report all of the results in Table 2. M AX E NT and R NN -G OLD are trained on the gold-standard data only. R NN -S ILVER is trained on the silverstandard data (annotated using the M AX E NT classifier). R NN -C OMBINED is trained on both the silver-standard and gold-standard data. The M AX E NT and R NN models show improvements, albeit small for the it-event class, over the baseline systems. Since they are trained on the same gold-standard data, one would expect R NN G OLD to perform similarly to M A"
D17-1137,N03-5008,0,0.022921,"ubject (verbs only). An estimate of the likelihood of a verb taking a event subject was computed over the Annotated English Gigaword v.5 corpus (Napoles et al., 2012). We considered two cases favouring event subjects that may be identified by exploiting the parse annotation of the Gigaword corpus. The first case is when the subject is a gerund and the second case is composed of ‘this’ pronoun subjects. 13. Non-referential probability assigned to the instance of ‘it’ by NADA (Bergsma and Yarowsky, 2011). 3.4 MaxEnt The M AX E NT classifier is trained using the Stanford Maximum Entropy package (Manning and Klein, 2003) with all of the features described above. We also experimented with other features and options. For features 1 and 2, a window mate-tools/downloads/list 1326 MC BASELINE it-anaphoric Precision 0.539 Dev-set Recall F1 1 0.700 LM BASELINE it-anaphoric it-pleonastic it-event M AX E NT it-anaphoric it-pleonastic it-event R NN -G OLD it-anaphoric it-pleonastic it-event R NN -S ILVER it-anaphoric it-pleonastic it-event R NN -C OMBINED it-anaphoric it-pleonastic it-event Precision 0.613 0.169 0.459 Precision 0.685 0.884 0.545 Precision 0.544 0.274 0.355 Precision 0.661 0.725 0.438 Precision 0.697 0."
D17-1137,P07-1103,0,0.785802,"Missing"
D17-1137,W12-3018,0,0.0406876,"Missing"
D17-1137,W10-1737,0,0.155649,"Missing"
D17-1137,C04-1034,0,0.0406805,"AX E NT classifier trained on gold-standard data and self-training experiments of an R NN trained on silver-standard data, annotated using the M AX E NT classifier. Lastly, we report on an analysis of the strengths of these two models. 1 Christian Hardmeier He lost his job. It came as a total surprise. We propose the identification of the three usage types of it, namely anaphoric, event reference, and Related Work Due to its difficulty, proposals for the identification and the subsequent resolution of abstract anaphora (i.e., event reference) are scarce (Eckert and Strube, 2000; Byron, 2002; Navarretta, 2004; M¨uller, 2007). The automatic detection of instances of pleonastic ‘it’, on the other hand, has been addressed by the non-referential ‘it’ detector NADA (Bergsma and Yarowsky, 2011), and also in the context of several coreference resolution systems, including the Stanford sieve-based coreference resolution system (Lee et al., 2011). The coreference resolution task focuses on the resolution of nominal anaphoric pronouns, de facto grouping our event and pleonastic categories together and discarding both of them. The coreference resolution task can be seen as a two-step problem: mention identif"
D17-1137,W11-1902,0,0.0419824,"of the three usage types of it, namely anaphoric, event reference, and Related Work Due to its difficulty, proposals for the identification and the subsequent resolution of abstract anaphora (i.e., event reference) are scarce (Eckert and Strube, 2000; Byron, 2002; Navarretta, 2004; M¨uller, 2007). The automatic detection of instances of pleonastic ‘it’, on the other hand, has been addressed by the non-referential ‘it’ detector NADA (Bergsma and Yarowsky, 2011), and also in the context of several coreference resolution systems, including the Stanford sieve-based coreference resolution system (Lee et al., 2011). The coreference resolution task focuses on the resolution of nominal anaphoric pronouns, de facto grouping our event and pleonastic categories together and discarding both of them. The coreference resolution task can be seen as a two-step problem: mention identification followed by antecedent identification. Identifying instances of pleonastic ‘it’ typically takes place in the mention identification step. The recognition of event reference ‘it’ is, however, to our knowledge not currently included in any such systems, although from a linguistic point of view, event instances are also referent"
D17-1137,W13-3307,0,0.0521234,"Missing"
D17-1137,W04-0713,0,\N,Missing
D18-1334,P11-2031,0,0.0692818,"ed systems performed on the general test set compared to the baseline. In Table 2, the BLEU scores for 10 baseline and 10 gender-enhanced NMT systems are presented. Systems FR ES EL IT PT DA DE FI SV NL EN 37.82 42.47 31.38 31.46 36.11 36.69 28.28 21.82 35.42 28.35 EN-TAG 39.26* 42.28 31.54 31.75* 36.33 37.00* 28.05 21.35* 35.19 28.22 Table 2: BLEU scores for the 10 baseline (denoted with EN) and the 10 gender-enhanced NMT (denoted with ENTAG) systems. Entries labeled with * present statistically significant differences (p < 0.05). Statistical significance was computed with the MultEval tool (Clark et al., 2011). While most of the BLEU-scores (Papineni et al., 2002) in Table 2 are consistent with our hypothesis, showing (significant) improvements for the NMT systems enriched with a gender tag (EN-TAG) over the baseline systems (EN) for French, Italian, Portuguese and Greek, the Spanish enriched system surprisingly does not (–0.19 BLEU). As hypothesized, the Dutch, German, Finnish and Swedish systems do not improve. However, the Danish (EN–DA) enriched NMT system does achieve a significant +0.31 BLEU improvement. We expected to see the strongest improvements in sentences uttered by female speakers as,"
D18-1334,P17-4012,0,0.0290805,"(Sennrich et al., 2016), etc. (1) “FEMALE Madam President, as a...” For each of these language pairs we trained two NMT systems: a baseline and a tagged one. We evaluated the performance of all our systems on a randomly selected 2K general test set. Moreover, we further evaluated the EN–FR systems on 2K male-only and female-only test sets to have a look at the system performance with respect to genderrelated issues. We also looked at two additional male and female test sets in which the first person singular pronoun appeared. 4.2 Description of the NMT Systems We used the OpenNMT-py toolkit (Klein et al., 2017) to train the NMT models. The models are sequence-to-sequence encoder-decoders with LSTMs as the recurrent unit (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014) trained with the default parameters. In order to bypass the OOV problem and reduce the number of dictionary entries, we use word-segmentation with BPE (Sennrich, 2015). We ran the BPE algorithm with 89,500 operations (Sennrich, 2015). All systems are trained for 13 epochs and the best model is selected for evaluation. 5 Age groups Experimental Setup Results In this section we discuss some of the results obtained. We hy"
D18-1334,2005.mtsummit-papers.11,0,0.207863,"ystems is finding large enough annotated parallel datasets with speaker information. Rabinovich et al. (2017) published an annotated parallel dataset for EN–FR and EN–DE. However, for many other language pairs no sufficiently large annotated datasets are available. To address the aforementioned problem, we published online a collection of parallel corpora licensed under the Creative Commons Attribution 4.0 International License for 20 language pairs (Vanmassenhove and Hardmeier, 2018).2 We followed the approach described by Rabinovich et al. (2017) and tagged parallel sentences from Europarl (Koehn, 2005) with speaker information (name, gender, age, date of birth, euroID and date of the session) by retrieving speaker information provided by tags in the Europarl source files. The Europarl source files contain information about the speaker on the paragraph level and the filenames contain the data of the session. By retrieving the names of the speakers together with meta-information on the members of the European Parliament (MEPs) released by Rabinovich et al. (2017) (which includes among others name, country, date of birth and gender predictions per MEP), we were able to retrieve demographic ann"
D18-1334,D15-1130,0,0.439655,"syntax (Coates, 2015). The increasing amount of work on automatic author classification (or ‘author profiling’) reaching relatively high accuracies on domain-specific data corroborates these findings (Rangel et al., 2013; Santosh et al., 2013). However, determining the gender of an author based solely on text is not a solved issue. Likewise, the selection of the most informative features for gender classification remains a difficult task (Litvinova et al., 2016). When translating from one language into another, original author traits are partially lost, both in human and machine translations (Mirkin et al., 2015; Rabinovich et al., 2017). However, in the field of Machine Translation (MT) one of the most observable consequences of this missing information are morphologically incorrect variants due to a lack of agreement in number and gender with the subject. Such errors harm the overall fluency and adequacy of the translated sentence. Furthermore, gender-related errors are not just harming the quality of the translation as getting the gender right is also a matter of basic politeness. Current systems have a tendency to perpetuate a male bias which amounts to negative discrimination against half the po"
D18-1334,E17-1101,0,0.472965,". The increasing amount of work on automatic author classification (or ‘author profiling’) reaching relatively high accuracies on domain-specific data corroborates these findings (Rangel et al., 2013; Santosh et al., 2013). However, determining the gender of an author based solely on text is not a solved issue. Likewise, the selection of the most informative features for gender classification remains a difficult task (Litvinova et al., 2016). When translating from one language into another, original author traits are partially lost, both in human and machine translations (Mirkin et al., 2015; Rabinovich et al., 2017). However, in the field of Machine Translation (MT) one of the most observable consequences of this missing information are morphologically incorrect variants due to a lack of agreement in number and gender with the subject. Such errors harm the overall fluency and adequacy of the translated sentence. Furthermore, gender-related errors are not just harming the quality of the translation as getting the gender right is also a matter of basic politeness. Current systems have a tendency to perpetuate a male bias which amounts to negative discrimination against half the population and this has been"
D18-1334,Q15-1013,0,0.0206909,"a look at the system performance with respect to genderrelated issues. We also looked at two additional male and female test sets in which the first person singular pronoun appeared. 4.2 Description of the NMT Systems We used the OpenNMT-py toolkit (Klein et al., 2017) to train the NMT models. The models are sequence-to-sequence encoder-decoders with LSTMs as the recurrent unit (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014) trained with the default parameters. In order to bypass the OOV problem and reduce the number of dictionary entries, we use word-segmentation with BPE (Sennrich, 2015). We ran the BPE algorithm with 89,500 operations (Sennrich, 2015). All systems are trained for 13 epochs and the best model is selected for evaluation. 5 Age groups Experimental Setup Results In this section we discuss some of the results obtained. We hypothesized that the male/female tags would be particularly helpful for French, Portuguese, Italian, Spanish and Greek, where adjectives and even verb forms can be marked by the gender of the speaker. Since, according to the literature, women and men also make use of different syntactic constructions and make different 3005 word choices, we als"
D18-1334,N16-1005,0,0.296255,"MT (SMT) systems as a domain-adaptation task treating the female and male gender as two different domains. They applied two common simple domain-adaptation techniques in order to create personalized SMT: (1) using gender-specific phrase-tables and language models, and (2) using a gender-specific tuning set. Although their models did not improve over the baseline, their work provides a detailed analysis of gender traits in human and machine translation. Our work is, to the best of our knowledge, the first to attempt building a speaker-informed NMT system. Our approach is similar to the work of Sennrich et al. (2016) on controlling politeness, where some sentence of the training data are followed with an ‘informal’ or ‘polite’ tag indicating the level of politeness expressed. 3 Compilation of Datasets One of the main obstacles for more personalized MT systems is finding large enough annotated parallel datasets with speaker information. Rabinovich et al. (2017) published an annotated parallel dataset for EN–FR and EN–DE. However, for many other language pairs no sufficiently large annotated datasets are available. To address the aforementioned problem, we published online a collection of parallel corpora l"
D18-1334,P02-1040,0,0.101067,"d to the baseline. In Table 2, the BLEU scores for 10 baseline and 10 gender-enhanced NMT systems are presented. Systems FR ES EL IT PT DA DE FI SV NL EN 37.82 42.47 31.38 31.46 36.11 36.69 28.28 21.82 35.42 28.35 EN-TAG 39.26* 42.28 31.54 31.75* 36.33 37.00* 28.05 21.35* 35.19 28.22 Table 2: BLEU scores for the 10 baseline (denoted with EN) and the 10 gender-enhanced NMT (denoted with ENTAG) systems. Entries labeled with * present statistically significant differences (p < 0.05). Statistical significance was computed with the MultEval tool (Clark et al., 2011). While most of the BLEU-scores (Papineni et al., 2002) in Table 2 are consistent with our hypothesis, showing (significant) improvements for the NMT systems enriched with a gender tag (EN-TAG) over the baseline systems (EN) for French, Italian, Portuguese and Greek, the Spanish enriched system surprisingly does not (–0.19 BLEU). As hypothesized, the Dutch, German, Finnish and Swedish systems do not improve. However, the Danish (EN–DA) enriched NMT system does achieve a significant +0.31 BLEU improvement. We expected to see the strongest improvements in sentences uttered by female speakers as, according to our initial analysis, the male data was o"
D18-1334,Q17-1024,0,\N,Missing
D18-1513,D14-1020,0,0.0182482,"gnment improvements are made. For the following systems we observe a very small increase in APT score for each of the two weight settings we consider, when alignment heuristics are applied: UU - HARDMEIER (+0.8), ITS 2 (+0.8), BASELINE (+0.8), YANDEX (+0.8), and NYU (+0.4). However, these small improvements are not sufficient to affect the system rankings. It seems, therefore, that the alignment heuristic has only a small impact on the validity of the score. To assess differences in correlation with human judgment for pairs of APT settings, we run Williams’s significance test (Williams, 1959; Graham and Baldwin, 2014). The test reveals that differences in correlation between the various configurations of APT and human judgements are not statistically significant (p > 0.2 in all cases). 4799 3 Personal recommendation by Lesly Miculicich Werlen. c1 c2 Pearson Spearman With alignment heuristics 1 0 1 0.5 0.848 0.853 0.820 0.815 Without alignment 1 0 heuristics 1 0.5 0.850 0.855 0.820 0.811 Table 2: Correlation of APT and human judgements Category Anaphoric intra sbj it intra nsbj it inter sbj it inter nsbj it intra they inter they sg they group it/they Event it Pleonastic it Generic you Deictic sg you Deictic"
D18-1513,W15-2509,1,0.796787,"are then subcategorised according to morphosyntactic criteria, whether the antecedent is a group noun, whether the ancedent is in the same or a different sentence, and whether an addressee reference pronoun refers to one or more specific people (deictic) or to people in general (generic). Our dataset contains human judgements on the performance of nine MT systems on the translation of the 250 pronouns in the PROTEST test suite. The systems include five submissions to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) – four phrase-based SMT systems AUTO - POST EDI T (Guillou, 2015), UU - HARDMEIER (Hardmeier et al., 2015), IDIAP (Luong et al., 2015), UU - TIEDEMANN (Tiedemann, 2015), a rule-based system ITS 2 (Lo´aiciga and Wehrli, 2015), and the shared task baseline (also phrase-based SMT). Three NMT systems are included for comparison: LIMSI (Bawden et al., 2017), NYU (Jean et al., 2014), and YANDEX (Voita et al., 2018). Manual evaluation was conducted using the PROTEST graphical user interface and accompanying guidelines (Hardmeier and Guillou, 2016). The annotators were asked to make judgements (correct/incorrect) on the translations of the pronouns and antecedent h"
D18-1513,W15-2512,0,0.0395025,"Missing"
D18-1513,L16-1100,1,0.768473,"Missing"
D18-1513,W15-2513,0,0.0190383,"hether the antecedent is a group noun, whether the ancedent is in the same or a different sentence, and whether an addressee reference pronoun refers to one or more specific people (deictic) or to people in general (generic). Our dataset contains human judgements on the performance of nine MT systems on the translation of the 250 pronouns in the PROTEST test suite. The systems include five submissions to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) – four phrase-based SMT systems AUTO - POST EDI T (Guillou, 2015), UU - HARDMEIER (Hardmeier et al., 2015), IDIAP (Luong et al., 2015), UU - TIEDEMANN (Tiedemann, 2015), a rule-based system ITS 2 (Lo´aiciga and Wehrli, 2015), and the shared task baseline (also phrase-based SMT). Three NMT systems are included for comparison: LIMSI (Bawden et al., 2017), NYU (Jean et al., 2014), and YANDEX (Voita et al., 2018). Manual evaluation was conducted using the PROTEST graphical user interface and accompanying guidelines (Hardmeier and Guillou, 2016). The annotators were asked to make judgements (correct/incorrect) on the translations of the pronouns and antecedent heads whilst ignoring the correctness of other words (except in cases"
D18-1513,guillou-etal-2014-parcor,1,0.887739,"different/incompatible, or there may be no translation in: (4) the MT output, (5) the reference, (6) either the MT output or the reference. Each of these cases may be assigned a weight between 0 and 1 to determine the level of correctness. 3 The PROTEST Dataset We study the behaviour of the two automatic metrics using the PROTEST test suite (Guillou and Hardmeier, 2016). The test suite comprises 250 hand-selected personal pronoun tokens taken from the DiscoMT2015.test dataset of TED talk transcriptions and translations (Hardmeier et al., 2016) and annotated according to the ParCor guidelines (Guillou et al., 2014). It is structured according to a linguistic typology motivated by work on functional grammar by Dik (1978) and Halliday (2004). Pronouns are first categorised according to their function: anaphoric: I have a bicycle. It is red. event: He lost his job. It was a shock. pleonastic: It is raining. addressee reference: You’re welcome. They are then subcategorised according to morphosyntactic criteria, whether the antecedent is a group noun, whether the ancedent is in the same or a different sentence, and whether an addressee reference pronoun refers to one or more specific people (deictic) or to p"
D18-1513,2010.iwslt-papers.10,1,0.901872,"tions to be rejected because they were missed by the annotator. 4 Score Alig. corr. Reference BASELINE IDIAP UU - TIED . UU - HARD . POST EDI T ITS 2 LIMSI NYU YANDEX Accuracy versus Precision/Recall There are three ways in which APT differs from AutoPRF: the scoring statistic, the alignment heuristic in APT, and the definition of pronoun equivalence. APT is a measure of accuracy: It reflects the proportion of source pronouns for which an acceptable translation was produced in the target. AutoPRF, by contrast, is a precision/recall metric on the basis of clipped counts. Hardmeier and Federico (2010) motivate the use of precision and recall by pointing out that word alignments are not 1 : 1, so each pronoun can be linked to multiple elements in the target language, both in the reference translation and in the MT output. Their metric is designed to account for all linked words in such cases. To test the validity of this argument, we examined the subset of examples of 8 systems in our English-French dataset1 giving rise to a clipped count greater than one2 and found that these examples follow very specific patterns. All 143 cases included exactly one personal pronoun. In 99 cases, the addit"
D18-1513,W16-3418,1,0.919993,"s to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) – four phrase-based SMT systems AUTO - POST EDI T (Guillou, 2015), UU - HARDMEIER (Hardmeier et al., 2015), IDIAP (Luong et al., 2015), UU - TIEDEMANN (Tiedemann, 2015), a rule-based system ITS 2 (Lo´aiciga and Wehrli, 2015), and the shared task baseline (also phrase-based SMT). Three NMT systems are included for comparison: LIMSI (Bawden et al., 2017), NYU (Jean et al., 2014), and YANDEX (Voita et al., 2018). Manual evaluation was conducted using the PROTEST graphical user interface and accompanying guidelines (Hardmeier and Guillou, 2016). The annotators were asked to make judgements (correct/incorrect) on the translations of the pronouns and antecedent heads whilst ignoring the correctness of other words (except in cases where it impacted the annotator’s ability to make a judgement). The annotations were carried out by two bilingual English-French speakers, both of whom are native speakers of French. Our human judgements differ in important ways from the human evaluation conducted for the same set of systems at DiscoMT 2015 (Hardmeier et al., 2015), which was carried out by non-native speakers over an unbalanced data sample u"
D18-1513,W15-2501,1,0.946423,"or MT Two reference-based automatic metrics of pronoun translation have been proposed in the literature. 4797 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4797–4802 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics The first (Hardmeier and Federico, 2010) is a variant of precision, recall and F-score that measures the overlap of pronouns in the MT output with a reference translation. It lacks an official name, so we refer to it as AutoPRF following the terminology of the DiscoMT 2015 shared task (Hardmeier et al., 2015). The scoring process relies on a word alignment between the source and the MT output, and between the source and the reference translation. For each input pronoun, it computes a clipped count (Papineni et al., 2002) of the overlap between the aligned tokens in the reference and the MT output. The clipped count of a given word is defined as the number of times it occurs in the MT output, limited by the number of times it occurs in the reference translation. The final metric is then calculated as the precision, recall and F-score based on these clipped counts. Miculicich Werlen and Popescu-Beli"
D18-1513,P02-1040,0,0.115966,"f] c. J’ai une bicyclette. Elle est rouge. [MT] However, the problem is more complex in practice because there is often no 1 : 1 correspondence between pronouns in two languages. This is easily demonstrated at the corpus level by observing that the number of pronouns varies significantly across languages in parallel texts (Mitkov * Both authors contributed equally. and Barbu, 2003), but it tends to be difficult to predict in individual cases. In general MT research, significant progress was enabled by the invention of automatic evaluation metrics based on reference translations, such as BLEU (Papineni et al., 2002). Attempting to create a similar framework for efficient research, researchers have proposed automatic reference-based evaluation metrics specifically targeting pronoun translation: AutoPRF (Hardmeier and Federico, 2010) and APT (Miculicich Werlen and PopescuBelis, 2017). We study the performance of these metrics on a dataset of English-French translations and investigate to what extent automatic evaluation based on reference translations provides insights into how well an MT system handles pronouns. Our analysis clarifies the conceptual differences between AutoPRF and APT, uncovering weakness"
D18-1513,W15-2515,0,0.0203897,", whether the ancedent is in the same or a different sentence, and whether an addressee reference pronoun refers to one or more specific people (deictic) or to people in general (generic). Our dataset contains human judgements on the performance of nine MT systems on the translation of the 250 pronouns in the PROTEST test suite. The systems include five submissions to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) – four phrase-based SMT systems AUTO - POST EDI T (Guillou, 2015), UU - HARDMEIER (Hardmeier et al., 2015), IDIAP (Luong et al., 2015), UU - TIEDEMANN (Tiedemann, 2015), a rule-based system ITS 2 (Lo´aiciga and Wehrli, 2015), and the shared task baseline (also phrase-based SMT). Three NMT systems are included for comparison: LIMSI (Bawden et al., 2017), NYU (Jean et al., 2014), and YANDEX (Voita et al., 2018). Manual evaluation was conducted using the PROTEST graphical user interface and accompanying guidelines (Hardmeier and Guillou, 2016). The annotators were asked to make judgements (correct/incorrect) on the translations of the pronouns and antecedent heads whilst ignoring the correctness of other words (except in cases where it impacted the annotator’s"
D18-1513,P18-1117,0,0.255039,"nslations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semiautomatic metrics and test suites in place of fully automatic metrics. 1 Introduction As the general quality of machine translation (MT) increases, there is a growing interest in improving the translation of specific linguistic phenomena. A case in point that has been studied in the context of both statistical (Hardmeier, 2014; Guillou, 2016; Lo´aiciga, 2017) and neural MT (Bawden et al., 2017; Voita et al., 2018) is that of pronominal anaphora. In the simplest case, translating anaphoric pronouns requires the generation of corresponding word forms respecting the grammatical constraints on agreement in the target language, as in the following English-French example, where the correct form of the pronoun in the second sentence varies depending on which of the (equally correct) translations of the word bicycle was used in the first: (1) a. I have a bicycle. It is red. b. J’ai un v´elo. Il est rouge. [ref] c. J’ai une bicyclette. Elle est rouge. [MT] However, the problem is more complex in practice becaus"
guillou-etal-2014-parcor,broscheit-etal-2010-extending,0,\N,Missing
guillou-etal-2014-parcor,de-marneffe-etal-2006-generating,0,\N,Missing
guillou-etal-2014-parcor,W11-1211,0,\N,Missing
guillou-etal-2014-parcor,J93-2004,0,\N,Missing
guillou-etal-2014-parcor,popescu-belis-etal-2012-discourse,0,\N,Missing
guillou-etal-2014-parcor,W11-1901,0,\N,Missing
guillou-etal-2014-parcor,D13-1037,1,\N,Missing
guillou-etal-2014-parcor,W11-1902,0,\N,Missing
guillou-etal-2014-parcor,W08-0325,0,\N,Missing
guillou-etal-2014-parcor,P06-1055,0,\N,Missing
guillou-etal-2014-parcor,E12-3001,1,\N,Missing
guillou-etal-2014-parcor,W10-1737,0,\N,Missing
guillou-etal-2014-parcor,2010.iwslt-papers.10,1,\N,Missing
guillou-etal-2014-parcor,prasad-etal-2008-penn,1,\N,Missing
guillou-etal-2014-parcor,hajic-etal-2012-announcing,0,\N,Missing
guillou-etal-2014-parcor,postolache-etal-2006-transferring,0,\N,Missing
guillou-etal-2014-parcor,M98-1029,0,\N,Missing
guillou-etal-2014-parcor,W13-3306,0,\N,Missing
guillou-etal-2014-parcor,2012.eamt-1.60,0,\N,Missing
I17-1018,W14-4012,0,0.0792603,"Missing"
I17-1018,P08-1102,0,0.049283,"Missing"
I17-1018,P09-1058,0,0.586714,"tal number of combinatory labels. The efficiency can be improved if we reduce k. For some POS tags, combining them with the full boundary tags is redundant. For instance, only the functional word 的 can be tagged as DEG in Chinese Treebank (Xue et al., 2005). Since it is a single-character word, combinatory tags of B-DEG, I-DEG, and E-DEG never occur in the experimental data and should therefore be pruned to reduce the search space. Similarly, if the maximum length of words under a given POS tag is two in the training data, we prune the corresponding label. Tagging Scheme Following the work of Kruengkrai et al. (2009a), the employed tags indicating the word boundaries are B, I, E, S representing a character at the beginning, inside, end of a word or as a singlecharacter word. The CRF layer models conditional scores over all possible combinatory labels given the input characters. Incorporating the transition scores between the successive labels, the op1 夏 Output Our baseline model is an adaptation of BiRNNCRF. As illustrated in Figure 1, the Chinese characters are represented as vectors and fed into the bidirectional recurrent layers. The character representations will be described in detail in the followi"
I17-1018,D15-1176,0,0.0393132,"n Chinese dictionaries. In our approach, they are retrieved via the unicode representation of Chinese characters as the characters that share the same radical are grouped together. They are organised in consistent with the categorisation in Kangxi Dictionary (康熙字典), in which all the Chinese characters are grouped under 214 different radicals. We only employ the radicals of the common characters in the unicode range of (U+4E00, U+9FFF). For the characters out of the range and the non-Chinese characters, we use a single special vector as their radical representations. where f is usually an RNN (Ling et al., 2015) or a CNN (dos Santos and Zadrozny, 2014). In this paper, instead of completely relying on the BiRNN to extract contextual features from context-free character representations, we encode rich local information in the character vectors via employing the incrementally concatenated n-gram representation as demonstrated in Figure 2. In the example, the vector representation of the pivot character 太 in the given context is the concatenation of the context-free vector representation Vi,i of 太 itself along with Vi−1,i of the bigram 天太 as well as Vi−1,i+1 of the trigram 天太热. Instead of constructing th"
I17-1018,P16-1101,0,0.0838702,"nn@helsinki.fi Abstract joint model which predicts the combinatory labels of segmentation boundaries and POS tags at the character level. Joint segmentation and POS tagging becomes a standard character-based sequence tagging problem and therefore the general machine learning algorithms for structured prediction can be applied. The bidirectional recurrent neural network (RNN) using conditional random fields (CRF) (Lafferty et al., 2001) as the output interface for sentence-level optimisation (BiRNN-CRF) achieves state-of-the-art accuracies on various sequence tagging tasks (Huang et al., 2015; Ma and Hovy, 2016) and outperforms the traditional linear statistical models. RNNs with gated recurrent cells, such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU) (Cho et al., 2014) are capable of capturing long dependencies and retrieving rich global information. The sequential CRF on top of the recurrent layers ensures that the optimal sequence of tags over the entire sentence is obtained. In this paper, we model joint segmentation and POS tagging as a fully character-based sequence tagging problem via predicting the combinatory labels. The BiRNN-CRF archit"
I17-1018,W04-3236,0,0.174321,"egmentation and POS tagging. 1 Introduction Word segmentation and part-of-speech (POS) tagging are core steps for higher-level natural language processing (NLP) tasks. Given the raw text, segmentation is applied at the very first step and POS tagging is performed on top afterwards. As by convention the words in Chinese are not delimited by spaces, segmentation is non-trivial, but its accuracy has a significant impact on POS tagging. Moreover, POS tags provide useful information for word segmentation. Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a 173 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 173–183, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (summer) (Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding. According to the experimental results, our pro"
I17-1018,D14-1162,0,0.0852471,"hinese characters are logograms. As opposed to alphabetical languages, there is rich information 175 Char. embedding size n-gram embedding size Radical embedding size Character font Character size GRU state size Conv. filter size Conv. filter number Max pooling size Fully-connected size Optimiser Initial learning rate Decay rate Gradient Clipping Dropout rate Batch size 2.3.3 Pre-trained Character Embeddings The context-free vector representations of single characters introduced in section 2.3.1 can be replaced by pre-trained character embeddings retrieved from large corpora. We employ GloVe (Pennington et al., 2014) to train our character embeddings on Wikipedia2 and the freely available Sogou News Corpora (SogouCS).3 We use randomly initialised vectors as the representations of the characters that are not in the embedding vocabulary. Pre-trained embeddings for higher-order n-grams are not employed in this paper. 2.4 Ensemble Decoding At the final decoding phase, we use ensemble decoding, a simple averaging technique, to mitigate the deviations led by random weight initialisation of the neural network. For the chain CRF decoder, the final sequence of the combinatory tags y is obtained via the conditional"
I17-1018,P14-2042,0,0.164784,"Missing"
I17-1018,P11-1139,0,0.136417,"Missing"
I17-1018,D16-1157,0,0.0159777,"training algorithm is employed for sentence level optimisation, which is the same as the training algorithm of the BiRNNCRF model. Their proposed model is not evaluated on CTB5 and therefore difficult to be compared with our system. Kong et al. (2015) apply segmental recurrent neural networks to joint segmentation and POS tagging but the evaluation results are substantially below the state-of-the-art on CTB5. Bojanowski et al. (2016) retrieve word embeddings via representing words as a bag of character n-grams for morphologically rich languages. A similar character n-gram model is proposed by Wieting et al. (2016). Sun et al. (2014) attempt to encode radical information into the conventional character embeddings. The radicalenhanced embeddings are employed and evaluated for Chinese segmentation. The results show that radical-enhanced embeddings outperform both skip-ngram and continues bag-of-word (Mikolov et al., 2013) in word2vec. Table 8: Result comparisions on CTB5 in F1scores. used for processing very large files. The memory demand of decoding is drastically milder compared to training, a large batch size therefore can be employed. The tagger takes constant time to build the sub-computational graph"
I17-1018,P16-1040,0,0.0693485,"e are in comparison to Single (** p &lt; 0.01, * p &lt; 0.05) performs very well despite being fully character based. Moreover, it has clear advantages when applied to smaller datasets like UD Chinese, while the prevalence is much smaller on CTB5. Both our model and ZPar segment OOV words in UD Chinese with higher accuracies than the ones in CTBs despite that UD Chinese is notably smaller and the overall OOV rate is higher. Compared to CTB, the words in UD Chinese are more fine-grained and the average word length is shorter, which makes it easier for the tagger to correctly segment the OOV words as Zhang et al. (2016) show that the longer words are more difficult to be segmented correctly. For joint POS tagging for OOV words, the two systems both perform significantly better on CTB5 as it is only composed of news text. BN CS FM MG NS SM SP WB Ensemble Seg Seg&Tag 97.89* 94.48** 96.67** 91.78** 96.54** 91.92** 94.54** 89.23** 97.56 93.92** 96.43** 91.78** 97.29** 93.93** 94.27** 88.44** Seg 97.68 95.61 96.30 94.22 97.49 96.13 96.69 93.38 ZPar Seg&Tag 94.22 90.15 91.51 88.60 93.70 90.32 93.35 86.88 Table 7: Evaluation on Broadcast News (BN), Conversations (CS), Forum (FM), Magazine (MG), News (NS), Short Mes"
I17-1018,P08-1101,0,0.127339,"S tagging. 1 Introduction Word segmentation and part-of-speech (POS) tagging are core steps for higher-level natural language processing (NLP) tasks. Given the raw text, segmentation is applied at the very first step and POS tagging is performed on top afterwards. As by convention the words in Chinese are not delimited by spaces, segmentation is non-trivial, but its accuracy has a significant impact on POS tagging. Moreover, POS tags provide useful information for word segmentation. Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a 173 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 173–183, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (summer) (Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding. According to the experimental results, our proposed model outperforms"
I17-1018,D10-1082,0,0.638575,"vide useful information for word segmentation. Thus, modelling word segmentation and POS tagging jointly can outperform the pipeline models (Ng and Low, 2004; Zhang and Clark, 2008). POS tagging is a typical sequence tagging problem over segmented words, while segmentation also can be modelled as a character-level tagging problem via predicting the labels that identify the word boundaries. Ng and Low (2004) propose a 173 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 173–183, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (summer) (Zhang and Clark, 2010), which is a state-of-theart joint tagger using structured perceptron and beam decoding. According to the experimental results, our proposed model outperforms ZPar on all the datasets in terms of accuracy. The main contributions of this work include: 1. We apply the BiRNN-CRF model for general sequence tagging to joint segmentation and POS tagging for Chinese and achieve state-of-the-art accuracy. The experimental results show that our tagger is robust and accurate across datasets of different sizes, genres and annotation schemes. 2. We propose a novel approach for vector representations of ch"
I17-1018,D13-1061,0,0.225031,"Missing"
I17-1018,I11-1035,0,\N,Missing
I17-1018,L16-1262,1,\N,Missing
I17-2015,W14-4012,0,0.0329254,"Missing"
I17-2015,Y11-1011,0,0.0254307,"substantially higher scores as it returns much fewer PP. Referring to the trivial examples as well as the fact that only TP are meaningful to higher-level applications, S1 and S2 perform equally poorly, which is consistent with recall but not precision. Furthermore, a segmenter with less TP may achieve higher precision if it is drastically under-segmenting, as demonstrated by the comparison between S1 and S3. 2.2 True Negative Rate Neither recall nor precision measure how well the system rejects the negatives. True negative rate (TNR) is therefore proposed by Powers (2011) as the complement. Jiang et al. (2011) show that segmenters measured by TNR are better correlated than precision and recall with their actual performances within IR systems. For WS, it is not straightforward to compute TNR by directly normalising the true negatives (TN) by the real negatives (RN). However, it can be indirectly computed via TP, PP, RP and the total number of possible output TW given a sentence. Regarding the input characters as a string, TW is equal to the number )N , where N is the number of of substrings as (1+N 2 the characters. RN can then be computed by subtracting RP, the number of words in reference. The fal"
I17-2015,I17-1018,1,0.841883,"ation (Palmer and Burger, 1997). The drawback is that incorrectly segmented words that are not interesting to higherlevel applications still contribute to the scores as long as one of the two associated boundaries is correctly detected. 1 Score 0.8 0.6 0.4 3 Experiments 0.2 To further investigate the correlations and drawbacks of the metrics discussed in the previous section experimentally, we employ a neural-based word segmenter to see how they measure the segmentation performance in a real scenario. The segmenter is a simplified version of the joint segmentation and POS tagger introduced in Shao et al. (2017). It is fully character-based. The vector representations of input characters are passed to the prevalent bidirectional recurrent neural network equipped with gated recurrent unit (GRU) (Cho et al., 2014) as the basic cell. A time-wise softmax layer is added as the inference for the recurrent layers to obtain probability distribution of binary tags that indicate the boundaries of the words. Cross-entropy with respect to time step is applied as the loss function. We train the segmenter for 30 epochs and pick the weights of the best epoch that minimises the loss on the development set. The Chine"
I17-2015,W03-1719,0,0.0553146,"n characters (Goldwater et al., 2007). It is the initial step for most higher level natural language processing tasks, such as part-of-speech tagging, syntactic analysis, information retrieval and machine translation. Thus, correct segmentation is crucial as segmentation errors propagate to higher level tasks. Because only correctly segmented words are meaningful to higher level tasks, word level precision, recall and their evenly-weighted average F1-score that are customised from information retrieval (IR) (Kent et al., 1955) are conventionally used as the standard evaluation metrics for WS (Sproat and Emerson, 2003; Qiu et al., 2015). In this paper, we thoroughly investigate precision and recall in addition to true negative rate in the scope of WS, with a special focus on the drawbacks of precision. Precision and F1-score can be misleading as an under-splitting system may obtain higher precision despite having fewer cor∀ip ∈ RP , ∀in ∈ RN , ∃I, {ip , in } ⊂ I Precision and recall are thus not directly correlated. For IR, system performance is well measured only if both precision and recall are used as it is trivial to optimise with respect to either precision or recall, but difficult to improve both. Th"
L16-1100,guillou-etal-2014-parcor,1,0.655574,"anslation (Hardmeier et al., 2015). This test set contains English transcriptions of 12 TED conference talks (and their French translations), selected in such a way that the texts include a reasonable number of instances of some less frequent pronoun types. Since we provide complete texts, rather than a collection of isolated sentences or passages, any MT system being tested has access to full document context for each pronoun token, which is essential for discourse-enabled translation. The English source texts were annotated manually for reduced coreference in the style of the ParCor corpus (Guillou et al., 2014). These annotations form the basis for our categorisation and selection of pronoun tokens, and the evaluation procedure. Pronouns are annotated according to the principal functional categories (types) of ParCor. There are three types of pronominal reference: Anaphoric, event and extra-textual reference. Anaphoric pronouns are the most typical case. They refer to an entity mentioned earlier, typically in the form of a noun phrase (NP), in the discourse. The mention referred to is called the pronoun’s antecedent. Consider Ex. 1, in which the anaphoric pronoun “it” refers to “bicycle” (its antece"
L16-1100,E12-3001,1,0.952047,"ranslation. Keywords: Evaluation, pronouns, machine translation 1. Motivation In most current statistical machine translation (SMT) methods, output words are generated in correspondence with the input words, according to word-alignments found at training time. In addition to word-alignments, only very limited context information is taken into account in the generation process. While the approach works well for content words, it does not for function words, such as pronouns and negation markers, which are critical to meaning (Hardmeier et al., 2015; Hardmeier et al., 2013; Nov´ak et al., 2013; Guillou, 2012). Pronouns have different functions, and their use varies between languages. Some pronouns function as referring elements, creating a link to an element occurring elsewhere in the discourse. Others are simply to ensure a grammatical sentence. For example pleonastic pronouns, such as the “it” in “It is raining” or “il” in “Il pleut”, are used to fill the subject position. In many languages, pronouns are morphologically marked for categories such as gender and number, subject to certain agreement constraints that must be satisfied according to the rules of the target language. This is mostly a p"
L16-1100,W15-2509,1,0.81587,"ue les entreprises sont des agents du changement si elles sont bien dirig´ees [Reference] (8) vous eˆ tes de ceux qui croient que les entreprises sont un agent de changement , si elles sont bien g´er´ees . [IDIAP] (9) Vous eˆ tes de ceux qui croient que les entreprises sont un agent de changement s’ ils sont bien g´er´es . [Baseline] Knowing the design of the DiscoMT 2015 systems is also useful when interpreting results. This information can be found in the system description papers, which are available for all systems except A3-108. One pattern that can be observed is that the auto-postEDIt (Guillou, 2015) and ITS2 (Lo´aiciga and Wehrli, 2015) systems both perform particularly poorly for the event and pleonastic categories and this 640 may be due to design similarities for these systems. Both systems make use of rules; ITS2 is a rule-based MT system and auto-postEDIt uses rules to automatically post-edit the output of a baseline phrase-based SMT system. In addition, the focus of both systems is on producing gendered pronoun translations. The auto-postEDIt system uses a simple rule to replace the translations of non-anaphoric pronouns that do not match a predefined set with the token “ce”. The I"
L16-1100,2010.iwslt-papers.10,1,0.560453,"tion metrics such as BLEU (Papineni et al., 2002) to guide their development efforts. Most automatic metrics assume that overlap of the MT output with a human-generated reference translation may be used as a proxy for correctness. In the case of anaphoric pronouns, this assumption breaks down. If the pronoun’s antecedent is translated in a way that differs from the reference translation, a different pronoun may be required: One that matches the reference translation may in fact be wrong. This shortcoming of existing automatic evaluation metrics is widely recognised (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2011), but so far, no viable alternatives have been proposed. Hardmeier and Federico (2010) suggest using a precision/recall-based measure that is more sensitive to pronouns than general-purpose metrics. However, this metric shares the fundamental shortcomings of all reference-based metrics, and its correlation with human judgements on pronoun correctness is weak (Hardmeier et al., 2015). In view of these difficulties, Hardmeier (2015) suggests using a test suite composed of carefully selected pronoun tokens which can be checked individually using an automatic evaluation script, ins"
L16-1100,D13-1037,1,0.866868,"to the DiscoMT 2015 shared task on pronoun translation. Keywords: Evaluation, pronouns, machine translation 1. Motivation In most current statistical machine translation (SMT) methods, output words are generated in correspondence with the input words, according to word-alignments found at training time. In addition to word-alignments, only very limited context information is taken into account in the generation process. While the approach works well for content words, it does not for function words, such as pronouns and negation markers, which are critical to meaning (Hardmeier et al., 2015; Hardmeier et al., 2013; Nov´ak et al., 2013; Guillou, 2012). Pronouns have different functions, and their use varies between languages. Some pronouns function as referring elements, creating a link to an element occurring elsewhere in the discourse. Others are simply to ensure a grammatical sentence. For example pleonastic pronouns, such as the “it” in “It is raining” or “il” in “Il pleut”, are used to fill the subject position. In many languages, pronouns are morphologically marked for categories such as gender and number, subject to certain agreement constraints that must be satisfied according to the rules of th"
L16-1100,W15-2501,1,0.903115,"of the systems submitted to the DiscoMT 2015 shared task on pronoun translation. Keywords: Evaluation, pronouns, machine translation 1. Motivation In most current statistical machine translation (SMT) methods, output words are generated in correspondence with the input words, according to word-alignments found at training time. In addition to word-alignments, only very limited context information is taken into account in the generation process. While the approach works well for content words, it does not for function words, such as pronouns and negation markers, which are critical to meaning (Hardmeier et al., 2015; Hardmeier et al., 2013; Nov´ak et al., 2013; Guillou, 2012). Pronouns have different functions, and their use varies between languages. Some pronouns function as referring elements, creating a link to an element occurring elsewhere in the discourse. Others are simply to ensure a grammatical sentence. For example pleonastic pronouns, such as the “it” in “It is raining” or “il” in “Il pleut”, are used to fill the subject position. In many languages, pronouns are morphologically marked for categories such as gender and number, subject to certain agreement constraints that must be satisfied acco"
L16-1100,W15-2522,1,0.724803,"nslation may in fact be wrong. This shortcoming of existing automatic evaluation metrics is widely recognised (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2011), but so far, no viable alternatives have been proposed. Hardmeier and Federico (2010) suggest using a precision/recall-based measure that is more sensitive to pronouns than general-purpose metrics. However, this metric shares the fundamental shortcomings of all reference-based metrics, and its correlation with human judgements on pronoun correctness is weak (Hardmeier et al., 2015). In view of these difficulties, Hardmeier (2015) suggests using a test suite composed of carefully selected pronoun tokens which can be checked individually using an automatic evaluation script, instead of an aggregate measure over a complete test set, to evaluate pronoun correctness. 2. Overview The PROTEST test suite comprises 250 hand-selected pronoun tokens and an automatic evaluation script which compares the output of an MT system against a reference translation. Pronoun tokens are categorised according to problems that MT systems face when translating pronouns, and the set is small enough to allow for manual evaluation and inspection"
L16-1100,W10-1737,0,0.0352018,"Missing"
L16-1100,W15-2512,0,0.256661,"Missing"
L16-1100,W15-2513,0,0.0305523,"utperform the baseline on certain categories such as intra-sentential subject anaphoric “it”, whilst most systems perform very poorly on event reference and pleonastic pronouns. This breakdown is a good starting point for a more detailed investigation of the problem, including manual verification of the mismatches found by the automatic evaluation script. The counts in Table 3 sum the number of pronoun tokens for which the translations by MT systems match those in the reference. To get a better idea of how systems compare, we can look at individual translations. For example, the IDIAP system (Luong et al., 2015) has fewer reference translation matches for intra-sentential anaphoric “they” than the baseline. However, it produces some pronoun translations that are better than those produced by the baseline. For example, the IDIAP system translates “corporations” and “they” as “les enterprises” and “elles” (Ex. 8) as per the reference (Ex. 7), but the baseline system provides a non-matching (and incorrect) translation of the pronoun (“ils” [masc. pl.] does not agree with “enterprises” [fem. pl.]). (6) You are one of those people who believe that corporations are an agent of change if they are run well ."
L16-1100,I13-1142,0,0.140541,"Missing"
L16-1100,P02-1040,0,0.128612,"gender and number, subject to certain agreement constraints that must be satisfied according to the rules of the target language. This is mostly a problem for referring pronouns, where generating the correct form requires identifying what the pronoun refers to (anaphora resolution). Evaluation poses a particular problem for researchers interested in pronoun generation in machine translation (MT). Owing to the cost and difficulty of manual evaluation (including manual post-editing based methods as a means to assess MT quality), MT researchers rely on automatic evaluation metrics such as BLEU (Papineni et al., 2002) to guide their development efforts. Most automatic metrics assume that overlap of the MT output with a human-generated reference translation may be used as a proxy for correctness. In the case of anaphoric pronouns, this assumption breaks down. If the pronoun’s antecedent is translated in a way that differs from the reference translation, a different pronoun may be required: One that matches the reference translation may in fact be wrong. This shortcoming of existing automatic evaluation metrics is widely recognised (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2011), but"
L16-1100,W15-2515,0,0.0337574,"SMT system. In addition, the focus of both systems is on producing gendered pronoun translations. The auto-postEDIt system uses a simple rule to replace the translations of non-anaphoric pronouns that do not match a predefined set with the token “ce”. The ITS2 system ignores the problem of translating event reference and pleonastic pronouns altogether. Evidently these strategies will be beaten by more sophisticated approaches such as those provided by some of the other systems. This is reflected in the results in Table 3. Another clear pattern is the similarity in performance of UU-Tiedemann (Tiedemann, 2015) and the baseline system. Both are phrase-based SMT systems trained using the same data. In contrast to the other systems, the UUTiedemann system does not attempt to resolve pronominal anaphora explicitly. Instead, it uses a cross-sentence n-gram model over determiners and pronouns which aims to bias the SMT model towards selecting correct pronouns. In many ways it could be considered the system closest in design to that of the baseline. The systems generally performed well on the translation of addressee reference “you”, as compared with the baseline. However, none of the systems was designed"
L18-1065,W17-4809,0,0.145606,"Missing"
L18-1065,L16-1100,1,0.872921,"ere are a few corpus-based studies of coreference translation (Nov´ak and Nedoluzhko, 2015; Nov´ak et al., 2013; Guillou and Webber, 2015). For the languages under analysis, it has been empirically shown to be a relevant problem (Hardmeier and Federico, 2010; Guillou, 2016). In the MT community, the awareness of the problem has been increased with three recent shared tasks on pronoun translation (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). In recognition of the difficulty of the problem, test suite-based evaluation methods for pronoun translation have been proposed (Guillou and Hardmeier, 2016; Bawden et al., 2017). At the same time, coreference translation and multilingual coreference resolution is still a complex problem, as we observe a widespread lack of understanding of this phenomenon. Existing coreference resolution tools are known to be unreliable as they introduce an unacceptable number of errors, and therefore manually annotated parallel resources are absolutely indispensable for the development of coreference-aware MT systems and other multilingual language technologies, including cross-lingual coreference ˇ resolution (Grishina, 2017; Nov´ak and Zabokrtsk´ y, 2014; Gree"
L18-1065,W15-2503,0,0.0191198,"coreference between exactly the same entities in both cases, and it is semantically enriched by the instrumental relation in the second. An MT system is very likely to output the personal pronoun sie instead of das (Wir arbeiten f¨ur Wohlstand und Chancen, weil [sie] richtig sind) making this pronoun refer to the entities and not the event, which would sound less natural in German. 2. Related Work The challenge of translating pronouns has been a recurring topic in recent studies. There are a few corpus-based studies of coreference translation (Nov´ak and Nedoluzhko, 2015; Nov´ak et al., 2013; Guillou and Webber, 2015). For the languages under analysis, it has been empirically shown to be a relevant problem (Hardmeier and Federico, 2010; Guillou, 2016). In the MT community, the awareness of the problem has been increased with three recent shared tasks on pronoun translation (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). In recognition of the difficulty of the problem, test suite-based evaluation methods for pronoun translation have been proposed (Guillou and Hardmeier, 2016; Bawden et al., 2017). At the same time, coreference translation and multilingual coreference resolution is st"
L18-1065,W16-2345,1,0.878368,"the entities and not the event, which would sound less natural in German. 2. Related Work The challenge of translating pronouns has been a recurring topic in recent studies. There are a few corpus-based studies of coreference translation (Nov´ak and Nedoluzhko, 2015; Nov´ak et al., 2013; Guillou and Webber, 2015). For the languages under analysis, it has been empirically shown to be a relevant problem (Hardmeier and Federico, 2010; Guillou, 2016). In the MT community, the awareness of the problem has been increased with three recent shared tasks on pronoun translation (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). In recognition of the difficulty of the problem, test suite-based evaluation methods for pronoun translation have been proposed (Guillou and Hardmeier, 2016; Bawden et al., 2017). At the same time, coreference translation and multilingual coreference resolution is still a complex problem, as we observe a widespread lack of understanding of this phenomenon. Existing coreference resolution tools are known to be unreliable as they introduce an unacceptable number of errors, and therefore manually annotated parallel resources are absolutely indispensable for the developm"
L18-1065,2010.iwslt-papers.10,1,0.784992,"tion in the second. An MT system is very likely to output the personal pronoun sie instead of das (Wir arbeiten f¨ur Wohlstand und Chancen, weil [sie] richtig sind) making this pronoun refer to the entities and not the event, which would sound less natural in German. 2. Related Work The challenge of translating pronouns has been a recurring topic in recent studies. There are a few corpus-based studies of coreference translation (Nov´ak and Nedoluzhko, 2015; Nov´ak et al., 2013; Guillou and Webber, 2015). For the languages under analysis, it has been empirically shown to be a relevant problem (Hardmeier and Federico, 2010; Guillou, 2016). In the MT community, the awareness of the problem has been increased with three recent shared tasks on pronoun translation (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). In recognition of the difficulty of the problem, test suite-based evaluation methods for pronoun translation have been proposed (Guillou and Hardmeier, 2016; Bawden et al., 2017). At the same time, coreference translation and multilingual coreference resolution is still a complex problem, as we observe a widespread lack of understanding of this phenomenon. Existing coreference resolut"
L18-1065,W15-2501,1,0.944934,"g this pronoun refer to the entities and not the event, which would sound less natural in German. 2. Related Work The challenge of translating pronouns has been a recurring topic in recent studies. There are a few corpus-based studies of coreference translation (Nov´ak and Nedoluzhko, 2015; Nov´ak et al., 2013; Guillou and Webber, 2015). For the languages under analysis, it has been empirically shown to be a relevant problem (Hardmeier and Federico, 2010; Guillou, 2016). In the MT community, the awareness of the problem has been increased with three recent shared tasks on pronoun translation (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). In recognition of the difficulty of the problem, test suite-based evaluation methods for pronoun translation have been proposed (Guillou and Hardmeier, 2016; Bawden et al., 2017). At the same time, coreference translation and multilingual coreference resolution is still a complex problem, as we observe a widespread lack of understanding of this phenomenon. Existing coreference resolution tools are known to be unreliable as they introduce an unacceptable number of errors, and therefore manually annotated parallel resources are absolutely indispen"
L18-1065,D12-1045,0,0.0426751,"coreference translation and multilingual coreference resolution is still a complex problem, as we observe a widespread lack of understanding of this phenomenon. Existing coreference resolution tools are known to be unreliable as they introduce an unacceptable number of errors, and therefore manually annotated parallel resources are absolutely indispensable for the development of coreference-aware MT systems and other multilingual language technologies, including cross-lingual coreference ˇ resolution (Grishina, 2017; Nov´ak and Zabokrtsk´ y, 2014; Green et al., 2011), information extraction (Lee et al., 2012; Zelenko et al., 2004) and question answering (Morton, 1999; Hartrumpf et al., 2008). Most existing coreference corpora are not parallel. The only resources for the language pair English-German that are known to us include the GECCo corpus (Lapshinova-Koltunski and Kunz, 2014), the ParCor corpus (Guillou et al., 2014) and the multilingual coreference corpus described by (Grishina and Stede, 2015). The first corpus contains annotations of the source texts only and is available with restrictions on some texts. The second resource considers only pairwise annotation of anaphoric pronouns and thei"
L18-1065,W17-4801,1,0.911047,"Missing"
L18-1065,H05-1004,0,0.153687,"Missing"
L18-1065,W99-0212,0,0.233411,"is still a complex problem, as we observe a widespread lack of understanding of this phenomenon. Existing coreference resolution tools are known to be unreliable as they introduce an unacceptable number of errors, and therefore manually annotated parallel resources are absolutely indispensable for the development of coreference-aware MT systems and other multilingual language technologies, including cross-lingual coreference ˇ resolution (Grishina, 2017; Nov´ak and Zabokrtsk´ y, 2014; Green et al., 2011), information extraction (Lee et al., 2012; Zelenko et al., 2004) and question answering (Morton, 1999; Hartrumpf et al., 2008). Most existing coreference corpora are not parallel. The only resources for the language pair English-German that are known to us include the GECCo corpus (Lapshinova-Koltunski and Kunz, 2014), the ParCor corpus (Guillou et al., 2014) and the multilingual coreference corpus described by (Grishina and Stede, 2015). The first corpus contains annotations of the source texts only and is available with restrictions on some texts. The second resource considers only pairwise annotation of anaphoric pronouns and their antecedents. The third corpus, although containing annotat"
L18-1065,C14-1003,0,0.291846,"Missing"
L18-1065,I13-1142,0,0.0523807,"Missing"
L18-1065,P14-2006,0,0.0252758,"Missing"
L18-1065,W04-0704,0,0.0577908,"lation and multilingual coreference resolution is still a complex problem, as we observe a widespread lack of understanding of this phenomenon. Existing coreference resolution tools are known to be unreliable as they introduce an unacceptable number of errors, and therefore manually annotated parallel resources are absolutely indispensable for the development of coreference-aware MT systems and other multilingual language technologies, including cross-lingual coreference ˇ resolution (Grishina, 2017; Nov´ak and Zabokrtsk´ y, 2014; Green et al., 2011), information extraction (Lee et al., 2012; Zelenko et al., 2004) and question answering (Morton, 1999; Hartrumpf et al., 2008). Most existing coreference corpora are not parallel. The only resources for the language pair English-German that are known to us include the GECCo corpus (Lapshinova-Koltunski and Kunz, 2014), the ParCor corpus (Guillou et al., 2014) and the multilingual coreference corpus described by (Grishina and Stede, 2015). The first corpus contains annotations of the source texts only and is available with restrictions on some texts. The second resource considers only pairwise annotation of anaphoric pronouns and their antecedents. The thir"
L18-1065,W15-3403,0,0.273741,"Missing"
L18-1065,guillou-etal-2014-parcor,1,0.852711,"allel resources are absolutely indispensable for the development of coreference-aware MT systems and other multilingual language technologies, including cross-lingual coreference ˇ resolution (Grishina, 2017; Nov´ak and Zabokrtsk´ y, 2014; Green et al., 2011), information extraction (Lee et al., 2012; Zelenko et al., 2004) and question answering (Morton, 1999; Hartrumpf et al., 2008). Most existing coreference corpora are not parallel. The only resources for the language pair English-German that are known to us include the GECCo corpus (Lapshinova-Koltunski and Kunz, 2014), the ParCor corpus (Guillou et al., 2014) and the multilingual coreference corpus described by (Grishina and Stede, 2015). The first corpus contains annotations of the source texts only and is available with restrictions on some texts. The second resource considers only pairwise annotation of anaphoric pronouns and their antecedents. The third corpus, although containing annotations of all referring expressions appearing in a coreference chain, is very small (ca. 11,000 words per language). For this reason, we have created an English-German parallel corpus which contains annotation of full coreference chains on the basis of the ParCo"
L18-1065,poesio-artstein-2008-anaphoric,0,0.074243,"rman (translations of the English TED talks contained in the DiscoMT data and the news texts from the WMT data), as well as 10,644 tokens of the English data did not contain any annotations and were thus annotated from scratch. The total number of tokens in the annotated corpus amounts to ca. 160,000. The annotated resource that we have created represents a reasonably-sized data set for training coreference resolution components that can be used for MT or other crosslingual applications. It is comparable in size (with a larger amount of text, but fewer annotated mentions) to the ARRAU corpus (Poesio and Artstein, 2008), which features a similarly rich coreference annotation and covers a greater variety of genres, but does not include multilingual parallel text. Although the amount of data is not enough to train an MT system, this dataset will be large enough for MT tuning, testing and evaluation, which is an important improvement over the existing data situation. 6. Annotation Results We present an overview of the annotated structures (absolute numbers) in Table 2 below. In total, the corpus contains about 15,000 annotated mentions at the moment. The annotated mentions are classified according to their morp"
P13-4033,W09-2404,0,0.256094,"på (particular attentive on) + Simplified expression Table 2: Example translation snippets with comments Feature Baseline TTR OVIX QW QP All BLEU 0.243 0.243 0.243 0.242 0.243 0.235 OVIX 56.88 55.25 54.65 57.16 57.07 47.80 LIX 51.17 51.04 51.00 51.16 51.06 49.29 Table 1: Results for adding single lexical consistency features to Docent To evaluate our system we used the BLEU score (Papineni et al., 2002) together with a set of readability metrics, since readability is what we hoped to improve by adding consistency features. Here we used OVIX to confirm a direct impact on consistency, and LIX (Björnsson, 1968), which is a common readability measure for Swedish. Unfortunately we do not have access to simplified translated text, so we calculate the MT metrics against a standard reference, which means that simple texts will likely have worse scores than complicated texts closer to the reference translation. We tuned the standard features using Moses and MERT, and then added each lexical consistency feature with a small weight, using a grid search approach to find values with a small impact. The results are shown in Table 1. As can be seen, for individual features the translation quality was maintained"
P13-4033,D12-1026,0,0.0267005,"se and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2"
P13-4033,E12-3001,0,0.0603375,"the GNU General Public License and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based S"
P13-4033,2010.iwslt-papers.10,1,0.855274,"s. The code is released under the GNU General Public License and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-leve"
P13-4033,D12-1108,1,0.812834,"(Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is performed by splitting the input sentence into a number of contiguous word sequences, called phrases, which are translated into the target language through a phrase dictionary lookup and optionally reordered. The choice between different translations of an ambiguous"
P13-4033,W11-2123,0,0.0578591,"e models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level models provided by Docent include the phrase table, n-gram language models implemented with the KenLM toolkit (Heafield, 2011), an unlexicalised distortion cost model with geometric decay (Koehn et al., 2003) and a word penalty cost. All of these features are designed to be compatible with the corresponding features in Moses. From among the typical set of baseline features in Moses, we have not implemented the lexicalised distortion model, but this model could easily be added if required. Docent uses the same binary file format for phrase tables as Moses, so the same training apparatus can be used. DP-based SMT decoders have a parameter called distortion limit that limits the difference in word order between the inpu"
P13-4033,D11-1125,0,0.0200806,"iscourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using different optimisation algorithms such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), and selective tuning of a subset of features. Since document features only give meaningful scores on the document level and not on the sentence level, we naturally perform optimisation on document level, which typically means that we need more data than for the optimisation of sentence-based decoding. The results we obtain are relatively stable and competitive with sentence-level optimisation of the same models (Stymne et al., 2013a). 4 Implementing Feature Models Efficiently While translating a document, the local search decoder attempts to make a great number of moves. For each move, a sco"
P13-4033,N03-1017,0,0.011021,"ystematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is"
P13-4033,P07-2045,0,0.0251314,"ed SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is performed by splitting the input sentence into a number of contiguous word sequences, called phrases, which are translated into the target language through a phrase dictionary lookup and optionally reordered. The choice between different translations of an ambiguous source phrase and the ordering of the target phrases are guided by a scoring function that combines a set of scores taken from the phrase table with scores from other models such as an n-gram language model. The actual translation process is realised as a search for the highest-scoring translation in the space of a"
P13-4033,2007.tmi-papers.13,0,0.0462402,"ocument at every 1 https://github.com/chardmeier/docent/wiki 193 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 193–198, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics stage of the search progress. Search proceeds by making small changes to the current search state in order to transform it gradually into a better translation. This differs from the DP algorithm used in other decoders, which starts with an empty translation and expands it bit by bit. It is similar to previous work on phrase-based SMT decoding by Langlais et al. (2007), but enables the creation of document-level models, which was not addressed by earlier approaches. Docent currently implements two search algorithms that are different generalisations of the hill climbing local search algorithm by Hardmeier et al. (2012). The original hill climbing algorithm starts with an initial state and generates possible successor states by randomly applying simple elementary operations to the state. After each operation, the new state is scored and accepted if its score is better than that of the previous state, else rejected. Search terminates when the decoder cannot f"
P13-4033,W10-1737,0,0.254585,"Missing"
P13-4033,2012.amta-papers.20,0,0.0288297,"r other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local"
P13-4033,W03-2117,0,0.0178285,"as originally proposed by Hardmeier et al. (2012) to improve lexical cohesion in a document. It is a cross-sentence model over sequences of content words that are scored based on their similarity in a word vector space. The readability models serve to improve the readability of the translation by encouraging the selection of easier and more consistent target words. They are described and demonstrated in more detail in section 5. Docent can read input files both in the NISTXML format commonly used to encode documents in MT shared tasks such as NIST or WMT and in the more elaborate MMAX format (Müller and Strube, 2003). The MMAX format makes it possible to include a wide range of discourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using dif"
P13-4033,P03-1021,0,0.00654474,"e a wide range of discourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using different optimisation algorithms such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), and selective tuning of a subset of features. Since document features only give meaningful scores on the document level and not on the sentence level, we naturally perform optimisation on document level, which typically means that we need more data than for the optimisation of sentence-based decoding. The results we obtain are relatively stable and competitive with sentence-level optimisation of the same models (Stymne et al., 2013a). 4 Implementing Feature Models Efficiently While translating a document, the local search decoder attempts to make a great number"
P13-4033,P02-1040,0,0.105205,"ound to genitive construction − Changing long compound to English-based abbreviation − Removal of important word − Bad grammar because of changed part of speech and missing verb planen (the plan) särskilt uppmärksam på (particular attentive on) + Simplified expression Table 2: Example translation snippets with comments Feature Baseline TTR OVIX QW QP All BLEU 0.243 0.243 0.243 0.242 0.243 0.235 OVIX 56.88 55.25 54.65 57.16 57.07 47.80 LIX 51.17 51.04 51.00 51.16 51.06 49.29 Table 1: Results for adding single lexical consistency features to Docent To evaluate our system we used the BLEU score (Papineni et al., 2002) together with a set of readability metrics, since readability is what we hoped to improve by adding consistency features. Here we used OVIX to confirm a direct impact on consistency, and LIX (Björnsson, 1968), which is a common readability measure for Swedish. Unfortunately we do not have access to simplified translated text, so we calculate the MT metrics against a standard reference, which means that simple texts will likely have worse scores than complicated texts closer to the reference translation. We tuned the standard features using Moses and MERT, and then added each lexical consisten"
P13-4033,W13-3308,1,0.807594,"oes not require that another phrase be moved in the opposite direction at the same time. A pair of operations called permute-phrases and linearisephrases can reorder a sequence of phrases into random order and back into the order corresponding to the source language. Since the search algorithm in Docent is stochastic, repeated runs of the decoder will generally produce different output. However, the variance of the output is usually small, especially when initialising with a DP search pass, and it tends to be lower than the variance introduced by feature weight tuning (Hardmeier et al., 2012; Stymne et al., 2013a). 3 Available Feature Models In its current version, Docent implements a selection of sentence-local feature models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level mo"
P13-4033,W13-5634,1,0.909717,"oes not require that another phrase be moved in the opposite direction at the same time. A pair of operations called permute-phrases and linearisephrases can reorder a sequence of phrases into random order and back into the order corresponding to the source language. Since the search algorithm in Docent is stochastic, repeated runs of the decoder will generally produce different output. However, the variance of the output is usually small, especially when initialising with a DP search pass, and it tends to be lower than the variance introduced by feature weight tuning (Hardmeier et al., 2012; Stymne et al., 2013a). 3 Available Feature Models In its current version, Docent implements a selection of sentence-local feature models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level mo"
P13-4033,W10-2602,1,0.794735,"the readability of texts by encouraging simple and consistent terminology (Stymne et al., 2013b). This work is a first step towards achieving joint SMT and text simplification, with the final goal of adapting MT to user groups such as people with reading disabilities. Lexical consistency modelling for SMT has been attempted before. The suggested approaches have been limited by the use of sentence-level decoders, however, and had to resort to procedures like post processing (Carpuat, 2009), multiple decoding runs with frozen counts from previous runs (Ture et al., 2012), or cache-based models (Tiedemann, 2010). In Docent, however, we always have access to a full document translation, which makes it straightforward to include features directly into the decoder. We implemented four features on the document level. The first two features are type token ratio (TTR) and a reformulation of it, OVIX, which is less sensitive to text length. These ratios have been related to the “idea density” of a text (Mühlenbock and Kokkinakis, 2009). We also wanted to encourage consistent translations of words, for which we used the Q-value (Deléger et al., 2006), which has been proposed to measure term quality. We appli"
P13-4033,N12-1046,0,0.0590301,"can be used in Docent in order to improve the readability of texts by encouraging simple and consistent terminology (Stymne et al., 2013b). This work is a first step towards achieving joint SMT and text simplification, with the final goal of adapting MT to user groups such as people with reading disabilities. Lexical consistency modelling for SMT has been attempted before. The suggested approaches have been limited by the use of sentence-level decoders, however, and had to resort to procedures like post processing (Carpuat, 2009), multiple decoding runs with frozen counts from previous runs (Ture et al., 2012), or cache-based models (Tiedemann, 2010). In Docent, however, we always have access to a full document translation, which makes it straightforward to include features directly into the decoder. We implemented four features on the document level. The first two features are type token ratio (TTR) and a reformulation of it, OVIX, which is less sensitive to text length. These ratios have been related to the “idea density” of a text (Mühlenbock and Kokkinakis, 2009). We also wanted to encourage consistent translations of words, for which we used the Q-value (Deléger et al., 2006), which has been p"
Q15-1033,P11-1022,0,0.0277316,"0.4777 VBP WHNP NN JJ SQ 0.4653 0.4508 0.4274 0.4021 0.4000 Table 3: Top 15 symbols sorted according to their obtained λ values in the SASSTKfull model with fixed α. The numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the correspo"
Q15-1033,D14-1190,1,0.899543,"Missing"
Q15-1033,W14-3338,1,0.917946,"riances into account, grid search would still need around 10 times more computation 4 For specific details on the SVM models used in all experiments performed in this paper we refer the reader to Appendix A. 467 Emotion Analysis The goal of Emotion Analysis is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels for each text (one for each emotion) instead of a single label. Beck et al. (2014a) used a multi-task GP for this task with a bag-of-words feature representation. In theory, it is possible to combine their multi-task kernel with our tree kernels, but to keep the focus of the experiments on testing tree kernel approaches, here we use independently trained models, one per emotion. Dataset We use the dataset provided by the “Affective Text” shared task in SemEval2007 (Strapparava and Mihalcea, 2007), which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Surprise. For each emotion, a score between 0 and 100 is given"
Q15-1033,C04-1046,0,0.231901,"Missing"
Q15-1033,P13-1004,1,0.946875,"of these issues, but have several limitations (see §6 for details). Our proposed approach for model selection relies on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a widely used Bayesian kernel machine. GPs allow efficient and fine-grained model selection by maximizing the evidence on the training data using gradient-based methods, dropping the requirement for development data. As a Bayesian procedure, GPs also naturally balance between model capacity and generalization. GPs have been shown to achieve state of the art performance in various regression tasks (Hensman et al., 2013; Cohn and Specia, 2013). Therefore, we base our approach on this framework. While prediction performance is important to consider (as we show in our experiments), we are 461 Transactions of the Association for Computational Linguistics, vol. 3, pp. 461–473, 2015. Action Editor: Stefan Riezler. Submission batch: 2/2015; Revision batch 7/2015; Published 8/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. mainly interested in two other significant aspects that are enabled by our approach: • Gradient-based methods are more efficient than grid search for high dimensional space"
Q15-1033,W12-3112,1,0.856136,"rresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator. • Eng"
Q15-1033,2011.eamt-1.32,1,0.860626,"he numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited b"
Q15-1033,P10-1064,0,0.0293227,"Missing"
Q15-1033,D14-1219,0,0.0166756,"re space. Also, their method does not take into account the underlying learning algorithm. Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geo"
Q15-1033,P14-5010,0,0.00433599,"on. Grid search is embarassingly parallelizable since each grid point can run in a different core. However, the GP optimization can also benefit from multiple cores by running each kernel computation inside the Gram matrix in parallel. To keep the comparisons simpler, the results shown in this section use a single core but all experiments in §5 employ parallelization in the Gram matrix computation level (for both SVM and GP models). 5 NLP Experiments Our experiments with NLP data address two regression tasks: Emotion Analysis and Quality Estimation. For both tasks, we use the Stanford parser (Manning et al., 2014) to obtain constituency trees for all sentences. Also, rather than using data official splits, we perform 5-fold cross-validation in order to obtain more reliable results. 5.1 Figure 4: Results from performance experiments. The x axis corresponds to wall clock time in seconds and it is in log scale. The y axis shows RMSE on the test set. The blue dashed line corresponds to the RMSE value obtained after L-BFGS converged. Error bars are obtained by measuring one standard deviation over the 20 runs made in each experiment. We can see that optimizing the GP model is consistently much faster than d"
Q15-1033,J08-2003,0,0.0337766,"Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very r"
Q15-1033,E06-1015,0,0.586128,"0 pr(n1 ) 6= pr(n2 )    λ pr(n1 ) = pr(n2 ) ∧ ∆(n1 , n2 ) =  preterm(n1 )    λg(n , n ) otherwise, 1 2 where pr(n) is the grammar production at node n and preterm(n) returns true if n is a pre-terminal node. The function g is defined as follows: g(n1 , n2 ) = |n1 | Y (α + ∆(cin1 , cin2 )) , (3) i=1 where |n |is the number of children of node n and cin is the ith child of node n. This recursive definition is calculated efficiently by employing dynamic programming to cache intermediate ∆ results. Equation 3 also adds another hyperparameter, α. This hyperparameter was introduced by Moschitti (2006b)3 as a way to select between two different tree kernels. If α = 1, we get the original SSTK, if α = 0, then we obtain the Subtree Kernel, which only allows fragments with terminal symbols 3 In his original formulation, this hyperparameter was named σ but here we use α to not confuse it with the GP noise hyperparameter. as leaves. We can also interpret the Subtree Kernel as a “sparse” version of the SSTK, where the “nonsubtree” fragments have their weights equal to zero. Even though fragment weights are affected by both kernel hyperparameters, previous work did not discuss their effects. The"
Q15-1033,W10-2926,0,0.0265089,"t contains complete grammar rules (see Figure 1 for an example). Consider the set of nodes in the two trees as N1 and N2 respectively. We define Ii (n) as an indicator function that returns 1 if fragment fi ∈ F has root n and 0 otherwise. A SSTK can then be defined as: X X k(t1 , t2 ) = ∆(n1 , n2 ) , (2) n1 ∈N1 n2 ∈N2 where ∆(n1 , n2 ) = |F | X λ s(i) 2 Ii (n1 )Ii (n2 ) i=1 and s(i) is the number of fragments in i with at least one child2 . The formulation in Equation 2 is the same as the one shown in Equation 1, except that we are now restricting the weights w(f ) to be a function of a 2 See Pighin and Moschitti (2010) for details and a proof on this derivation. 463 Tree S A B A B a b A B A B A B A B a S b Fragments S S a S b a b Figure 1: An example tree and the respective set of tree fragments defined by a SSTK. hyperparameter λ. The original goal of λ is to act as a decay factor that penalizes contributions from larger fragments cf smaller ones (and therefore, it should be in the [0, 1] interval). Without this factor, the resulting distribution over tree pairs is skewed, giving extremely large values when trees are equal and rapidly decreasing for small differences over fragment counts. The decay factor"
Q15-1033,P13-1147,0,0.0315825,"ion is random search (Bergstra and Bengio, 2012). Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones). Structural kernels have been successfully employed in a number of NLP tasks. The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers. Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014). Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013). String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very recently they have been successfully employed in a fe"
Q15-1033,D13-1100,1,0.910314,"Missing"
Q15-1033,S14-2138,0,0.0219346,"Arciprete, 2009). However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach. Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004). Only very recently they have been successfully employed in a few NLP tasks such as translation quality estimation (Cohn and Specia, 2013; Beck et al., 2014b), detection of temporal patterns in text (Preot¸iuc-Pietro and Cohn, 2013), semantic similarity (Rios and Specia, 2014) and emotion analysis (Beck et al., 2014a). In terms of feature 471 representations, previous work focused on the vectorial inputs and applied well-known kernels for these inputs, e.g. the RBF kernel. As shown on §5.2, our approach is orthogonal to these previous ones, since kernels can be easily combined in different ways. It is important to note that we are not the first ones to combine GPs with kernels on structured inputs. Driessens et al. (2006) employed a combination of GPs and graph kernels for reinforcement learning. However, unlike our approach, they did not attempt model selection, e"
Q15-1033,2009.eamt-1.5,1,0.873699,"de a quality prediction for new, unseen machine translated texts (Blatz et al., 2004; Bojar et al., 2014). ExamJJR PRP$ WDT RBR VBG 0.8333 0.6933 0.6578 0.5445 0.5163 WHADVP QP JJS NNS . 0.5004 0.5001 0.4996 0.4961 0.4777 VBP WHNP NN JJ SQ 0.4653 0.4508 0.4274 0.4021 0.4000 Table 3: Top 15 symbols sorted according to their obtained λ values in the SASSTKfull model with fixed α. The numbers are the corresponding λ values, averaged over all six emotions. ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011). While various quality metrics exist, here we focus on post-editing time prediction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to te"
Q15-1033,P13-4014,1,0.861222,"Missing"
Q15-1033,2011.eamt-1.12,1,0.842829,"ction. Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012). While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels. This makes Quality Estimation a good benchmark task to test our models. Datasets We use two publicly available datasets containing post-edited machine translated sentences. Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time. • French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator. • English-Spanish (en-es): This dataset was used in the WMT14 Quality Estimation shared task (Bojar et al., 2014), containing 858 sentences translated from English into Spanish and post-edited by an expert translator. For each dataset, post-editing times are first divided by the translation output length (obtaining the post-editing time per word) and then mean normalized. 469 Models Since our data consists of pairs of trees, our models in this task use a pair of tree kernels. We combine these two ke"
Q15-1033,D09-1010,0,0.0764496,"Missing"
Q15-1033,S07-1013,0,\N,Missing
Q18-1030,K17-3005,0,0.024135,"gmentation and parsing F1-score (76.95) is reported and compared against the parsing score (85.70) with gold word segmentation. The evaluation scores re5 LDC2010T13, LDC2011T09, LDC2010T08 LDC2012T07 7 LDC2012E93,98,89,99,107,125, LDC2013E12,21 6 ported in both Monroe et al. (2014) and Goldberg and Elhadad (2013) are not directly comparable to the evaluation scores on Arabic and Hebrew in this paper, as they are obtained on different datasets. For universal word segmentation, apart from UDPipe described in Section 6.3, there are several systems that are developed for specific language groups. Che et al. (2017) build a similar Bi-LSTM word segmentation model targeting languages without space delimiters like Chinese and Japanese. The proposed model incorporates rich statistics-based features gathered from large-scale unlabelled data, such as character unigram embeddings, character bigram embeddings and the point-wise mutual information of adjacent characters. Bj¨orkelund et al. (2017) use a CRF-based tagger for multiword token rich languages like Arabic and Hebrew. A predicted Levenshtein edit script is employed to transform the multiword tokens into their components. The evaluation scores on a selec"
Q18-1030,D15-1141,0,0.0157937,"r lexicon size, are relatively stable across different UD treebanks for the same language, which indicates that they do capture properties of these languages, although some variation inevitably occurs due to corpus properties like genre. In this paper, we thoroughly investigate the correlations between the proposed statistical factors and segmentation accuracy. Moreover, we aim to find specific settings that can be applied to improve segmentation accuracy for each language group. 4 Sequence Tagging Model Word segmentation can be modelled as a characterlevel sequence labelling task (Xue, 2003; Chen et al., 2015). Characters as basic input units are passed into a sequence labelling model and a sequence of tags that are associated with word boundaries are predicted. In this section, we introduce the boundary tags adopted in this paper. Theoretically, binary classification is sufficient to indicate whether a character is the end of a word for segmentation. In practice, more fine-grained tagsets result in higher segmentation accuracy (Zhao et al., 2006). Following the work of Shao et al. (2017), we employ a baseline tagset consisting of four tags: B, I, E, and S, to indicate a character positioned at the"
Q18-1030,W14-4012,0,0.0145111,"Missing"
Q18-1030,K17-3022,1,0.898384,"Missing"
Q18-1030,K17-3002,0,0.0894629,"ucer is much more powerful in processing the non-segmental multiword tokens that are not covered by the dictionary than the suffix rules for analysing multiword tokens in UDPipe. UDPipe obtains higher scores on a few datasets. Our model overfits the small training data of Uyghur Arabic Chinese Hebrew Japanese Vietnamese Segmentation Accuracy UDPipe This Paper 93.77 97.16 90.47 93.82 85.16 91.01 92.03 93.77 85.53 87.79 UDPipe parser UAS LAS UDPipe This Paper UDPipe This Paper 72.34 78.22 66.41 71.79 63.20 67.91 59.07 63.31 62.14 71.18 57.82 66.59 78.08 81.77 76.73 80.83 47.72 50.87 43.10 46.03 Dozat et al. (2017) UAS LAS UDPipe This Paper UDPipe This Paper 77.52 83.55 72.89 78.42 71.24 76.33 68.20 73.04 67.61 76.39 64.02 72.37 80.21 83.79 79.44 82.99 50.28 53.78 45.54 48.86 Table 12: Extrinsic evaluations with dependency parsing on the test sets. The parsing accuracies are reported in unlabelled attachment score (UAS) and labelled attachment score (LAS). Buryat Kurmanji North Sami Upper Sorbian Space 71.99 78.97 79.07 72.35 NLTK 97.99 97.37 99.20 94.60 Sample 88.07 93.37 92.82 93.34 Transfer 97.99 (Russian) 96.71 (Spanish) 99.81 (German) 93.66 (Spanish) Table 13: Evaluation on the surprise languages."
Q18-1030,J13-1007,0,0.0302636,"ly enlarges the search space and therefore the model becomes extremely inefficient both for training and tagging. The joint POS tagging model is nonetheless applicable to Japanese and Vietnamese. Monroe et al. (2014) present a data-driven word segmentation system for Arabic based on a sequence labelling framework. An extended tagset is designed for Arabic-specific orthographic rules and applied together with hand-crafted features in a CRF framework. It obtains 98.23 F1-score on newswire Arabic Treebank,5 97.61 on Broadcast News Treebank,6 and 92.10 on the Egyptian Arabic dataset.7 For Hebrew, Goldberg and Elhadad (2013) perform word segmentation jointly with syntactic disambiguation using lattice parsing. Each lattice arc corresponds to a word and its corresponding POS tag, and a path through the lattice corresponds to a specific word segmentation and POS tagging of the sentence. The proposed model is evaluated on the Hebrew Treebank (Guthmann et al., 2009). The joint word segmentation and parsing F1-score (76.95) is reported and compared against the parsing score (85.70) with gold word segmentation. The evaluation scores re5 LDC2010T13, LDC2011T09, LDC2010T08 LDC2012T07 7 LDC2012E93,98,89,99,107,125, LDC201"
Q18-1030,W02-0109,0,0.0156233,"o Average F1-scores. The scores of the surprise languages are excluded and presented separately as no corresponding UDPipe models are available. Our system obtains higher segmentation accuracy overall. It achieves substantially better accuracies on languages that are challenging to segment, namely Chinese, Japanese, Vietnamese, Arabic and Hebrew. The two systems yield very similar scores, when these languages are excluded as shown in Table 8, in which the two systems are also compared with two rule-based baselines, a simple space-based tokeniser and the tokenisation model for English in NLTK (Loper and Bird, 2002). The NLTK model obtains relatively high accuracy while the spacebased baseline substantially underperforms, which indicates that relying on white space alone is insufficient for word segmentation in general. On the majority of the space-delimited languages without productive non-segmental multiword tokens, both UDPipe and our segmentation system yield near-perfect scores in Table 9. In general, referring back to Figure 1, languages that are clustered at the bottom-left corner are relatively trivial to segment. The evaluation scores are notably lower on Semitic languages as well as languages w"
Q18-1030,P14-2034,0,0.0733593,"the universal model presented in this paper can be obtained. However, the joint POS tagging system is difficult to generalise as single characters in space-delimited languages are usually not informative for POS tagging. Additionally, compared to Chinese, sentences in space-delimited languages have a much greater number of characters on average. Combining the POS tags with segmentation tags drastically enlarges the search space and therefore the model becomes extremely inefficient both for training and tagging. The joint POS tagging model is nonetheless applicable to Japanese and Vietnamese. Monroe et al. (2014) present a data-driven word segmentation system for Arabic based on a sequence labelling framework. An extended tagset is designed for Arabic-specific orthographic rules and applied together with hand-crafted features in a CRF framework. It obtains 98.23 F1-score on newswire Arabic Treebank,5 97.61 on Broadcast News Treebank,6 and 92.10 on the Egyptian Arabic dataset.7 For Hebrew, Goldberg and Elhadad (2013) perform word segmentation jointly with syntactic disambiguation using lattice parsing. Each lattice arc corresponds to a word and its corresponding POS tag, and a path through the lattice"
Q18-1030,L16-1262,1,0.890118,"Missing"
Q18-1030,W13-3504,0,0.0230707,"es without word delimiters. Nonetheless, our system obtains substantially higher scores on the languages that are more challenging to process. For Chinese, Japanese and Vietnamese, our system benefits substantially from the concatenated 3-gram character representation, which has been demonstrated in Section 6.2.2. Besides, we employ a more fine-grained tagset with CRF loss instead of the binary tags adopted in UDPipe. As presented in Zhao et al. (2006), more fine-grained tagging schemes outperform binary tags, which is supported by the experimental results on morpheme segmentation reported in Ruokolainen et al. (2013). We further investigate the merits of the finegrained tags over the binary tags as well as the effectiveness of the CRF interface by the experiments presented in Table 10 with the variances of our segmentation system. The fine-grained tags denote the boundary tags introduced in Table 3. The binary Dataset Ancient Greek Arabic-PUD Catalan Czech Czech-PUD Dutch-LassySmall English-PUD Finnish French French-Sequoia German Greek Hindi-PUD Irish Japanese Korean Latin-PROIEL Norwegian-Nynorsk Polish Portuguese-PUD Russian-PUD Slovenian Spanish-AnCora Swedish-LinES Turkish-PUD Uyghur UDPipe 99.98 90."
Q18-1030,I17-1018,1,0.946453,"equence Tagging Model Word segmentation can be modelled as a characterlevel sequence labelling task (Xue, 2003; Chen et al., 2015). Characters as basic input units are passed into a sequence labelling model and a sequence of tags that are associated with word boundaries are predicted. In this section, we introduce the boundary tags adopted in this paper. Theoretically, binary classification is sufficient to indicate whether a character is the end of a word for segmentation. In practice, more fine-grained tagsets result in higher segmentation accuracy (Zhao et al., 2006). Following the work of Shao et al. (2017), we employ a baseline tagset consisting of four tags: B, I, E, and S, to indicate a character positioned at the beginning (B), inside (I), or at the end (E) of a word, or occurring as a single-character word (S). The baseline tagset can be applied to word segmentation of Chinese and Japanese without further modification. For languages with space-delimiters, we add an extra tag X to mark the characters, mostly spaces, that do not belong to any words/tokens. As illustrated in Figure 2, the regular spaces are marked with X while the space in a multitoken word like 50 000 is disambiguated with I."
Q18-1030,K17-3009,0,0.073967,"Missing"
Q18-1030,O03-4002,0,0.67766,"rent definitions of the concept of a word. In this paper, we will follow the teminologies of Universal Dependencies (UD), where words are defined as basic syntactic units that do not always coincide with phonological or orthographic words. Some orthographic tokens, known in UD as multiword tokens, therefore need to be broken into smaller units that cannot always be obtained by splitting the input character sequence.1 To perform word segmentation in the UD framework, neither rule-based tokenisers that rely on white space nor the naive character-level sequence tagging model proposed previously (Xue, 2003) are ideal. In this paper, we present an enriched sequence labelling model for universal word segmentation. It is capable of segmenting languages in very diverse written forms. Furthermore, it simultaneously identifies the multiword tokens defined by the UD framework that cannot be resolved simply by splitting 1 Note that this notion of multiword token has nothing to do with the notion of multiword expression (MWE) as discussed, for example, in Sag et al. (2002). 421 Transactions of the Association for Computational Linguistics, vol. 6, pp. 421–435, 2018. Action Editor: Sebastian Pad´o . Submi"
Q18-1030,Y06-1012,0,0.037929,"tion accuracy for each language group. 4 Sequence Tagging Model Word segmentation can be modelled as a characterlevel sequence labelling task (Xue, 2003; Chen et al., 2015). Characters as basic input units are passed into a sequence labelling model and a sequence of tags that are associated with word boundaries are predicted. In this section, we introduce the boundary tags adopted in this paper. Theoretically, binary classification is sufficient to indicate whether a character is the end of a word for segmentation. In practice, more fine-grained tagsets result in higher segmentation accuracy (Zhao et al., 2006). Following the work of Shao et al. (2017), we employ a baseline tagset consisting of four tags: B, I, E, and S, to indicate a character positioned at the beginning (B), inside (I), or at the end (E) of a word, or occurring as a single-character word (S). The baseline tagset can be applied to word segmentation of Chinese and Japanese without further modification. For languages with space-delimiters, we add an extra tag X to mark the characters, mostly spaces, that do not belong to any words/tokens. As illustrated in Figure 2, the regular spaces are marked with X while the space in a multitoken"
W09-4610,W07-0735,0,0.0414301,"Missing"
W09-4610,D07-1091,0,0.416643,"Statistical Machine Translation (SMT) system of film subtitles can be improved by using linguistic annotations. To this end, a subset of Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 57–64 Martin Volk Universit¨at Z¨urich Inst. f¨ur Computerlinguistik Binzm¨uhlestrasse 14 CH-8050 Z¨urich volk@cl.uzh.ch 1 million subtitles of the training corpus used by Volk and Harder was morphologically annotated with the DanGram parser (Bick, 2001). We integrated the annotations into the translation process using the methods of factored Statistical Machine Translation (Koehn and Hoang, 2007) implemented in the widely used Moses software. After describing the corpus data and giving a short overview over the methods used, we present a number of experiments comparing different factored SMT setups. The experiments are then replicated with reduced training corpora which contain only part of the available training data. These series of experiments provide insights about the impact of corpus size on the effectivity of using linguistic abstractions for SMT. 2 Machine translation of subtitles As a text genre, subtitles play a curious role in a complex environment of different media and mo"
W09-4610,N03-1017,0,0.0046775,"obability is decomposed into a loglinear combination of a number of feature functions hi (S, T ), which map a pair of a source and a target language element to a score based on different submodels such as translation models or language models. Each feature function is associated with a weight λi that specifies its contribution to the overall score: Tˆ = arg max log p(T |S) T = arg max T ∑ λi hi (S, T ) i The translation models employed in factored SMT are phrase-based. The phrases included in a translation model are extracted from a wordaligned parallel corpus with the techniques described by Koehn et al. (2003). The associated probabilities are estimated by the relative frequencies of the extracted phrase pairs in the same corpus. For language modelling, we used the SRILM toolkit (Stolcke, 2002); unless otherwise specified, 6-gram language models with modified KneserNey smoothing were used. The SMT decoder tries to translate the words and phrases of the source language sentence in the order in which they occur in the input. If the target language requires a different word order, reordering is possible at the cost of a score penalty. The translation model has no notion of sequence, so it cannot contr"
W09-4610,2005.iwslt-1.8,0,0.0444571,"Missing"
W09-4610,P07-2045,0,0.126766,"Statistical Machine Translation (SMT) system of film subtitles can be improved by using linguistic annotations. To this end, a subset of Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 57–64 Martin Volk Universit¨at Z¨urich Inst. f¨ur Computerlinguistik Binzm¨uhlestrasse 14 CH-8050 Z¨urich volk@cl.uzh.ch 1 million subtitles of the training corpus used by Volk and Harder was morphologically annotated with the DanGram parser (Bick, 2001). We integrated the annotations into the translation process using the methods of factored Statistical Machine Translation (Koehn and Hoang, 2007) implemented in the widely used Moses software. After describing the corpus data and giving a short overview over the methods used, we present a number of experiments comparing different factored SMT setups. The experiments are then replicated with reduced training corpora which contain only part of the available training data. These series of experiments provide insights about the impact of corpus size on the effectivity of using linguistic abstractions for SMT. 2 Machine translation of subtitles As a text genre, subtitles play a curious role in a complex environment of different media and mo"
W09-4610,P03-1021,0,0.00961043,"Missing"
W09-4610,P02-1040,0,0.0753254,"Missing"
W09-4610,2007.mtsummit-papers.66,1,0.703767,"yntactic simplicity, and the immense text volumes processed daily by specialised subtitling companies make it possible to produce raw translations of film subtitles with statistical methods quite effectively. If these raw translations are subsequently post-edited by skilled staff, production quality translations can be obtained with considerably less effort than if the subtitles were translated by human translators with no computer assistance. A successful subtitle Machine Translation system for the language pair Swedish–Danish, which has now entered into productive use, has been presented by Volk and Harder (2007). The goal of the present study is to explore whether and how the quality of a Statistical Machine Translation (SMT) system of film subtitles can be improved by using linguistic annotations. To this end, a subset of Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 57–64 Martin Volk Universit¨at Z¨urich Inst. f¨ur Computerlinguistik Binzm¨uhlestrasse 14 CH-8050 Z¨urich volk@cl.uzh.ch 1 million subtitles of the training corpus used by Volk and Harder was morphologically annotated with the DanGram parser (Bick, 2001). We integrated the annotations into the trans"
W09-4610,C90-3030,0,0.0224009,"title text genre in this paper. D´ıazCintas and Remael (2007) provide a detailed introduction, including the linguistics of subtitling and translation issues, and Pedersen (2007) discusses the peculiarities of subtitling in Scandinavia. 3 Constraint Grammar annotations To explore the potential of linguistically annotated data, our complete subtitle corpus, both in Danish and in Swedish, was linguistically analysed with the DanGram Constraint Grammar (CG) parser (Bick, 2001), a system originally developed for the analysis of Danish for which there is also a Swedish grammar. Constraint Grammar (Karlsson, 1990) is a formalism for natural language parsing. Conceptually, a CG parser first produces possible analyses for each word by considering its morphological features and then applies constraining rules to filter out analyses that do not fit into the context. Thus, the word forms are gradually disambiguated, until only one analysis remains; multiple analyses may be retained if the sentence is ambiguous. The annotations produced by the DanGram parser were output as tags attached to individual words as in the following example: $Vad vet du om det $? [vad] &lt;interr&gt; INDP NEU S NOM @ACC&gt; [veta] &lt;mv&gt; V PR"
W10-1710,N04-1022,0,0.0353991,"phological Reduction and Chunk-based Reordering Christian Hardmeier, Arianna Bisazza and Marcello Federico Fondazione Bruno Kessler Human Language Technologies Trento, Italy {hardmeier,bisazza,federico}@fbk.eu Abstract a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). FBK participated in the WMT 2010 Machine Translation shared task with phrase-based Statistical Machine Translation systems based on the Moses decoder for English-German and German-English translation. Our work concentrates on exploiting the available language modelling resources by using linear mixtures of large 6-gram language models and on addressing linguistic differences between English and German with methods based on word lattices. In particular, we use lattices to integrate a morphological analyser for German into our system, and we pres"
W10-1710,W09-0435,0,0.190543,"Missing"
W10-1710,P03-1021,0,0.0227324,"Missing"
W10-1710,P02-1040,0,0.0782298,"Christian Hardmeier, Arianna Bisazza and Marcello Federico Fondazione Bruno Kessler Human Language Technologies Trento, Italy {hardmeier,bisazza,federico}@fbk.eu Abstract a recasing step, we retained the data in document case throughout our system, except for the morphologically normalised word forms described in section 3. Our phrase tables were trained with the standard Moses training script, then filtered based on statistical significance according to the method described by Johnson et al. (2007). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al., 2002). FBK participated in the WMT 2010 Machine Translation shared task with phrase-based Statistical Machine Translation systems based on the Moses decoder for English-German and German-English translation. Our work concentrates on exploiting the available language modelling resources by using linear mixtures of large 6-gram language models and on addressing linguistic differences between English and German with methods based on word lattices. In particular, we use lattices to integrate a morphological analyser for German into our system, and we present some initial work on rule-based word reorder"
W10-1710,W10-1735,1,0.823142,"s a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish (Bisazza and Federico, 2010), we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after 20.6 21.1 note: only News LM, case-sensitive evaluation Table 5: Results with morphological reduction and chunk reordering on newstest 2009/2010 ual inspection of a data sample, we then identified a few recurrent patterns of long reorderings involving the verbs. In particular, we focused on clause-final verbs in German SOV clauses, which we move to the left in order to approximate the Eng"
W10-1710,P08-1115,0,0.293714,"Missing"
W10-1710,D07-1103,0,0.0373687,"Missing"
W10-1710,E03-1076,0,0.286632,"n our parallel computing environment. We are working on methods to reduce and distribute disk accesses to large language models, which will be implemented in the IRSTLM language modelling toolkit (Federico et al., 2008). By doing so, we hope to overcome the current limitations and exploit the power of language model mixtures more fully. The Gertwol-based morphological reduction and decompounding component we used is a working solution that results in a significant improvement in translation quality. It is an alternative to the popular statistical compound splitting methods, such as the one by Koehn and Knight (2003), incorporating a greater amount of linguistic knowledge and offering morphological reduction even of simplex words to their base form in addition. It would be interesting to compare the relative performance of the two approaches systematically. Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish (Bisazza and Federico, 2010), we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, so"
W10-1710,P07-2045,1,0.0183052,"some more effort into the inverse translation direction to make better use of the abundance of language modelling data available for English and to address the richness of German morphology, which makes it hard for a Statistical Machine Translation (SMT) system to achieve good vocabulary coverage. In the remainder of this section, an overview of the common features of our systems will be given. The next two sections provide a more detailed description of our approaches to language modelling, morphological preprocessing and word reordering. Both of our systems were based on the Moses decoder (Koehn et al., 2007). They were similar to the WMT 2010 Moses baseline system. Instead of lowercasing the training data and adding 1. For a linear mixture of the complete set of 24 language models, we estimated a set of 88 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88–92, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Corpus Europarl v5 News News commentary 10 Gigaword v3: 6 models Gigaword 2007/08: 6 models 109 fr-en UNDOC fr-en CzEng: 7 models Total: 24 models n-grams 115,702,157 1,437,562,740 10,381,511 7,990,828,834 1,418"
W11-2144,S10-1021,0,0.0238005,"ds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference link is sentence-internal, the translation lookup is done dynamically by the decoder during search. In either case, the word-dependency model adds a feature function to the decoder score representing the probability of a particular"
W11-2144,de-marneffe-etal-2006-generating,0,0.0128364,"Missing"
W11-2144,D08-1089,0,0.301274,"ench Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentenc"
W11-2144,P07-2053,0,0.0307365,"Missing"
W11-2144,2010.iwslt-papers.10,1,0.918745,"ing sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference li"
W11-2144,P06-1063,0,0.0313791,"Missing"
W11-2144,2005.iwslt-1.8,0,0.214486,"Missing"
W11-2144,P07-2045,1,0.0116502,"at WMT 2011. We created two largely independent systems for English-to-French and Haitian Creole-toEnglish translation to evaluate different features and components from our ongoing research on these language pairs. Key features of our systems include anaphora resolution, hierarchical lexical reordering, data selection for language modelling, linear transduction grammars for word alignment and syntaxbased decoding with monolingual dependency information. 1 English to French Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora"
W11-2144,J93-2004,0,0.0368938,"he overall best scores in both translation directions. The fact that both alignments lead to complementary information can be seen in the size of the phrase tables extracted (see table 3). 2.2 Syntax-based SMT We used Moses and its syntax-mode for our experiments with hierarchical phrase-based and syntaxaugmented models. Our main interest was to investigate the influence of monolingual parsing on the translation performance. In particular, we tried to integrate English dependency parses created by MaltParser (Nivre et al., 2007) trained on the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) extended with about 4000 questions 3 We actually swapped the development set and the test set by mistake. But, of course, we never mixed development and test data in any result reported. null from the Question Bank (Judge et al., 2006). The conversion to dependency trees was done using the Stanford Parser (de Marneffe et al., 2006). Again, we ran both translation directions to test our settings in more than just one task. Interesting here is also the question whether there are significant differences when integrating monolingual parses on the source or on the target side. The motivation for a"
W11-2144,J03-1002,0,0.00544101,"MT 2011 to build a large scale-background language model. The English data from the Haitian Creole task were used as a separate domain-specific language model. For the other translation direction we only used the in-domain data provided. We used standard 5-gram models with Witten-Bell discounting and backoff interpolation for all language models. For the translation model we applied standard techniques and settings for phrase extraction and score estimations. However, we applied two different systems for word alignment: One is the standard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be ext"
W11-2144,2011.eamt-1.42,1,0.822224,"e a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of pairs of strings, and thus represents a relation between two languages. 375 each other. By duplicating the training corpus and aligning each copy of the corpus with a different alignment tool, the phrase extractor seems to be able to pick the best of both worlds, producing a phrase table that is superior to one produced with either"
W11-2144,N10-1050,1,0.845828,"dard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of"
W11-2144,C08-1144,0,0.0330283,"lish test set with (=malt) or without (=hiero) English parse trees and various parse relaxation strategies. The final system submitted to WMT11 is malt(target)-samt2. rule extraction is based on tree manipulation and relaxed extraction algorithms. Moses implements several algorithms that have been proposed in the literature. Tree binarisation is one of them. This can be done in a left-branching and in a right-branching mode. We used a combination of both in the settings denoted as binarised. The other relaxation algorithms are based on methods proposed for syntaxaugmented machine translation (Zollmann et al., 2008). We used two of them: samt1 combines pairs of neighbouring children nodes into combined complex nodes and creates additional complex nodes of all children nodes except the first child and similar complex nodes for all but the last child. samt2 combines any pair of neighbouring nodes even if they are not children of the same parent. All of these relaxation algorithms lead to increased rule sets (table 4). In terms of translation performance there seems to 377 be a strong correlation between rule table size and translation quality as measured by BLEU. None of the dependency-based models beats t"
W12-3112,W12-3102,0,0.16462,"Missing"
W12-3112,gimenez-marquez-2004-svmtool,0,0.0199658,"Missing"
W12-3112,P07-2053,0,0.040547,"Missing"
W12-3112,2011.eamt-1.32,1,0.932815,"ystem, along with the models used and diagnostic output produced by the SMT system as well as manual translation quality annotations on a 1–5 scale for each sentence. Additionally, a set of 17 baseline features was made available to the participants. Systems were evaluated on a test set of 422 sentences annotated in the same way. Uppsala University submitted two systems to this shared task. Our systems were fairly successful and achieved results that were outperformed by only one competing group. They improve over the baseline performance in two ways, building on and extending earlier work by Hardmeier (2011), on which the system description in the following sections is partly based: On the one hand, we enhance the set of 17 baseline features provided by the organisers with another 82 explicitly defined features. On the other hand, we use syntactic tree kernels to extract implicit features from constituency and dependency parse trees over the input sentences and the Machine Translation (MT) output. The experimental results confirm the findings of our earlier work, showing tree kernels to be a valuable tool for rapid prototyping of QE systems. 2 Features Our QE systems used two types of features: O"
W12-3112,W10-2910,0,0.0230381,"al., 2006). POS tagging was done with HunPOS (Hal´acsy et al., 2007) for English and SVMTool (Gim´enez and M´arquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009). As in previous work (Hardmeier, 2011), we treated the parser as a black box and made no attempt to handle the fact that parsing accuracy may be decreased over malformed SMT output. To be used with tree kernels, the output of the dependency parser had to be transformed into a single tree structure with a unique label per node and unlabelled edges, similar to a constituency parse tree. We followed Johansson and Moschitti (2010) in using a tree representation which encodes partof-speech tags, dependency relations and words as sequences of child nodes (see fig. 1). Figure 1: Representation of the dependency tree fragment VP S for the words Nicole ’s dad NP Mary brought NP V brought NP V VP N D N D N D N a cat a cat 2006b). Predicted scores less than 1 were set to 1 and predicted scores greater than 5 were set to 5 as this was known to be the range of valid scores. Our learning algorithm had some free hyperparameters. Three of them were optimised by joint grid search with 5-fold cross-validation over the training set:"
W12-3112,P03-1054,0,0.00290698,"with 1 : 1, 1 : n, n : 1 and m : n alignments (10 features) • average number of translations per word, unweighted and weighted by word frequency and reciprocal word frequency (3 features) 110 Parse trees Both the English input text and the Spanish Machine Translations were annotated with syntactic parse trees from which to derive implicit features. In English, we were able to produce both constituency and dependency parses. In Spanish, we were limited to dependency parses because of the better availability of parsing models. English constituency parses were produced with the Stanford parser (Klein and Manning, 2003) using the model bundled with the parser. For dependency parsing, we used MaltParser (Nivre et al., 2006). POS tagging was done with HunPOS (Hal´acsy et al., 2007) for English and SVMTool (Gim´enez and M´arquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009). As in previous work (Hardmeier, 2011), we treated the parser as a black box and made no attempt to handle the fact that parsing accuracy may be decreased over malformed SMT output. To be used with tree kernels, the output of the dependency parser had to be transformed into a single tree structure with a"
W12-3112,E06-1015,0,0.689341,"D N N D D N N brought D nomial instead. The improvement an plan over the Gaussian NP NP NP a a a cat cat a a cat kernel was, however, marginal. N D N D direct stock purchase a cat N D V N A tree and some of its Partial Tree Fragments 3.3 Tree kernels To exploit parse tree information in our Machine Fig. 3. A tree with some of its partial trees Fig. 4. A dependency tree of a question. Figure 2: Tree fragments extracted by the Subset Tree Learning (ML) component, we used tree kernel (PTs). Kernel and by the Partial Tree Kernel. Illustrations by functions. Tree kernels (Collins and Duffy, 2001) Moschitti (2006a). are kernel functions defined over pairs of tree structures. They measure the similarity between two trees counting form the number of common substructures. 3 Machine Learning component constraint over the SSTs, we obtain a morebygeneral of substructures that we Implicitly, they define an infinite-dimensional feacall by the application of partial 3.1 partial Overviewtrees (PTs). These can be generated ture space whose dimensions correspond to all posproduction of the consequently [VP [V]] and are [VP The QE sharedrules task asked bothgrammar, for an estimate of sible tree fragments. Feature"
W12-3112,2009.mtsummit-papers.16,0,0.012036,"he complete set of 17 baseline features provided by the workshop organisers. Additionally, the UU best system also contained all the features presented by Hardmeier (2011) with the exception 109 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 109–113, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of a few features specific to the film subtitle genre and inapplicable to the text type of the shared task, as well as a small number of features not included in that work. Many of these features were modelled on QE features described by Specia et al. (2009). In particular, the following features were included in addition to the baseline feature set: • number of words, length ratio (4 features) • source and target type-token ratios (2 features) • number of tokens matching particular patterns (3 features each): – – – – – – – – – – numbers opening and closing parentheses strong punctuation signs weak punctuation signs ellipsis signs hyphens single and double quotes apostrophe-s tokens short alphabetic tokens (≤ 3 letters) long alphabetic tokens (≥ 4 letters) • translation model entropy for the input words, cumulatively per sentence and averaged per"
W12-3112,nivre-etal-2006-maltparser,1,\N,Missing
W13-2229,J03-1002,0,0.0232763,"Missing"
W13-2229,D12-1108,1,0.797773,"n of the search space. While this decoding approach delivers excellent search performance at a very reasonable speed, it limits the information available to the feature models to an n-gram window similar to a language model history. In stack decoding, it is difficult to implement models with sentence-internal long-range dependencies and cross-sentence dependencies, where the model score of a given sentence depends on the translations generated for another sentence. In contrast to this very popular stack decoding approach, our decoder Docent implements a search procedure based on local search (Hardmeier et al., 2012). At any stage of the search process, its search state consists of a complete document translation, making it easy for feature models to access the complete document with its current translation at any point in time. The search algorithm is a stochastic variant of standard hill climbing. At each step, it generates a successor of the current search state by randomly applying We describe the Uppsala University system for WMT13, for English-to-German translation. We use the Docent decoder, a local search decoder that translates at the document level. We add tunable distortion limits, that is, sof"
W13-2229,P03-1021,0,0.00789271,"he translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimization was performed using MERT (Och, 2003) at the document-level (Stymne et al., 2013a). In this setup we calculate both model and metric scores on the document-level instead of on the sentence-level. We produce kbest lists by sampling from the decoder. In each optimization run we run 40,000 hill-climbing iterations of the decoder, and sample translations with interval 100, from iteration 10,000. This procedure has been shown to give competitive results to standard tuning with Moses (Koehn et al., 2007) with relatively stable results (Stymne et al., 2013a). For tuning data we concatenated the tuning sets news-test 2008–2010 and newssy"
W13-2229,P02-1040,0,0.0863828,"y the WMT13 workshop. We always concatenated the two bilingual corpora Europarl and News Commentary, which we will call EP-NC. We pre-processed all corpora by using the tools provided for tokenization and we also lower-cased all corpora. For the bilingual corpora we also filtered sentence pairs with a length ratio larger than three, or where either sentence was longer than 60 tokens. Recasing was performed as a post-processing step, trained using the resources To evaluate our system we use newstest2012, which has 99 documents and 3003 sentences. In this article we give lower-case Bleu scores (Papineni et al., 2002), except in Section 6 where we investigate the effect of different recasing models. 226 Cleaning None Basic Langid Alignment-based Sentences 2,399,123 2,271,912 2,072,294 1,512,401 Reduction sentences into four categories, cases where both languages were correctly identified, but under the confidence threshold of 0.999, cases where both languages were incorrectly identified, and cases where one language was incorrectly identified. Overall the language identification was accurate on 54 of the 93 removed sentences. In 18 of the cases where it was wrong, the sentences were not translation corresp"
W13-2229,P13-4033,1,0.906674,"at the document level. We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Docent. We also investigate cleaning of the noisy Common Crawl corpus. We show that we can use alignment-based filtering for cleaning with good results. Finally we investigate effects of corpus selection for recasing. 1 The Docent Decoder Introduction In this paper we present the Uppsala University submission to WMT 2013. We have submitted one system, for translation from English to German. In our submission we use the document-level decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013). In the current setup, we take advantage of Docent in that we introduce tunable distortion limits, that is, modeling distortion limits as soft constraints instead of as hard constraints. In addition we perform experiments on corpus cleaning. We investigate how the noisy Common Crawl corpus can be cleaned, and suggest an alignmentbased cleaning method, which works well. We also investigate corpus selection for recasing. In Section 2 we introduce our decoder, Docent, followed by a general system description in Section 3. In Section 4 we describe our experiments with corpus cleaning, and in Sect"
W13-2229,E12-1055,0,0.0123678,"ined two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were trained using the SRILM toolkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimizati"
W13-2229,W11-2123,0,0.0206337,"-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were trained using the SRILM toolkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Doce"
W13-2229,D07-1103,0,0.0298921,"lkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimization was performed using MERT (Och, 2003) at the document-level (Stymne et al., 2013a). In this setup we calculate both model and metric scores on the document-level instead of on the sentence-level. We produce kbest lists by sampling from the decoder. In each optimization run we run 40,000 hill-c"
W13-2229,N03-1017,0,0.0408954,"Missing"
W13-2229,W13-3308,1,0.864943,"of the complete document. On the downside, there is an increased risk of search errors because the document-level hill-climbing decoder cannot make as strong assumptions about the problem structure as the stack decoder does. In practice, this drawback can be mitigated by initializing the hill-climber with the output of a stack decoding pass using the baseline set of models without document-level features (Hardmeier et al., 2012). Since its inception, Docent has been used to experiment with document-level semantic language models (Hardmeier et al., 2012) and models to enhance text readability (Stymne et al., 2013b). Work on other discourse phenomena is ongoing. In the present paper, we focus on sentence-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were tra"
W13-2229,2005.iwslt-1.8,0,0.114614,"Missing"
W13-2229,W13-5634,1,0.871264,"of the complete document. On the downside, there is an increased risk of search errors because the document-level hill-climbing decoder cannot make as strong assumptions about the problem structure as the stack decoder does. In practice, this drawback can be mitigated by initializing the hill-climber with the output of a stack decoding pass using the baseline set of models without document-level features (Hardmeier et al., 2012). Since its inception, Docent has been used to experiment with document-level semantic language models (Hardmeier et al., 2012) and models to enhance text readability (Stymne et al., 2013b). Work on other discourse phenomena is ongoing. In the present paper, we focus on sentence-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were tra"
W13-2229,W08-0318,0,0.0188269,"s EP-NC-News EP-NC-CC-News EP-NC 13.8 13.9 13.9 13.9 13.9 EP-NC-CC 14.4 14.5 14.5 14.5 14.5 Language model News EP-NC-News 14.8 14.8 14.9 14.8 14.9 14.9 14.9 14.9 14.9 14.9 EP-NC-CC-News 14.8 14.8 14.9 14.9 15.0 Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and translation model (TM) for recasing Test system Docent (random) Docent (stack) Moses Docent (random) Docent (stack) Moses ing. It is common to train the system on truecased data instead of lower-cased data, which has been shown to lead to small gains for the English– German language pair (Koehn et al., 2008). In this framework there is still a need to find the correct case for the first word of each sentence, for which a similar corpus study might be useful. 7 Tuning system Docent Docent Docent Moses Moses Moses Bleu 15.7 15.9 15.9 15.9 16.8 16.8 Table 8: Bleu scores for Docent initialized randomly or with stack decoding compared to Moses. Tuning is performed with either Moses or Docent. For the top line we used tunable distortion limits 6,10 with Docent, in the other cases a standard hard distortion limit of 6, since Moses does not allow soft distortion limits. Comparison to Moses So far we have"
W13-2229,P12-3005,0,0.118617,"Missing"
W13-2229,P07-2045,0,\N,Missing
W13-3308,D11-1084,0,0.0941275,"in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing"
W13-3308,W12-3102,0,0.0373536,"Missing"
W13-3308,D07-1007,0,0.0195191,"onnectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-be"
W13-3308,D12-1026,0,0.234011,"ion (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2"
W13-3308,W09-2404,0,0.411772,"ng has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Tr"
W13-3308,E12-3001,0,0.09251,"ic programming for exploring a large search space (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translated sentences (Hardmeier and Federico, 2010), unfortunately without any convincing results. To address the translation of discourse connectives, source-side pre-processing has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good tra"
W13-3308,P07-1005,0,0.016844,"ed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-best list that have a high score on some MT metric. The translation step"
W13-3308,2010.iwslt-papers.10,1,0.900759,"he dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translated sentences (Hardmeier and Federico, 2010), unfortunately without any convincing results. To address the translation of discourse connectives, source-side pre-processing has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues f"
W13-3308,N12-1047,0,0.127102,"best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works well for small sets of features. In this paper we ar"
W13-3308,D12-1108,1,0.932337,"slation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimiz"
W13-3308,D08-1024,0,0.0690584,"Missing"
W13-3308,P13-4033,1,0.695247,"each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to i"
W13-3308,P11-2031,0,0.0276307,"ment-level feature weight optimization for SMT. We first describe the experimental setup, followed by baseline results using sentencelevel optimization. We then present validation experiments with standard sentence-level features, 1 http://www.statmt.org/wmt13/ translation-task.html 63 System Moses Docent-M Docent-R Moses Docent-M Docent-R million iterations. We show results on the Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) metrics. For German–English we show the average result and standard deviation of three optimization runs, to control for optimizer instability as proposed by Clark et al. (2011). For English–Swedish we report results on single optimization runs, due to time constraints. 5.2 Bleu 17.7 17.7 15.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to show the effectiveness of the document-level tuning procedure described above. In order to do this, we created a baseline using sentence-level optimization with a tuning set of 2525 sentences and the News2009"
W13-3308,D11-1125,0,0.344821,"til some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works well for small sets of features. In this paper we are not concerned with the actu"
W13-3308,W12-3139,0,0.205173,"5.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to show the effectiveness of the document-level tuning procedure described above. In order to do this, we created a baseline using sentence-level optimization with a tuning set of 2525 sentences and the News2009 corpus for evaluation. Increasing the tuning set is known to give only modest improvements (Turchi et al., 2012; Koehn and Haddow, 2012). The feature weights optimized with the standard Moses decoder can then directly be used in our document-level decoder as we only include sentence-level features in our baseline model. As expected, these optimized weights also lead to a better performance in document-level decoding compared to an untuned model as shown in Table 2. Note, that Docent can be initialized in two ways, by Moses and randomly. Not surprisingly, the result for the runs initialized with Moses are identical with the pure sentence-level decoder. Initializing randomly gives a slightly lower Bleu score but with a larger va"
W13-3308,N12-1023,0,0.0116367,"repeated using the new weights for decoding, and optimization is continued on a new k-best list, or on a combination of all k-best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are ba"
W13-3308,N03-1017,0,0.0371705,"rity of scoring and the process of extracting k-best lists. For document-level features we do not have meaningful scores on the sentence level which are required in standard optimization frameworks. Furthermore, the extraction of k-best lists is not as Here we instead choose to work with the recent document-level SMT decoder Docent (Hardmeier et al., 2012). Unlike in traditional decoding were documents are generated sentence by sentence, feature models in Docent always have access to the complete discourse context, even before decoding is finished. It implements the phrase-based SMT approach (Koehn et al., 2003) and is based on local search, where a state consists of a full translation of a document, which is improved by applying a series of operations to improve the translation. A hill-climbing strategy is used to find a (local) maximum. The operations allow changing the translation of a phrase, changing the word order by swapping the positions of two phrases, and resegmenting phrases. The initial state can either be initialized randomly in monotonic order, or be based on an initial run from a standard sentence-based decoder. The number of iterations in the decoder is controlled by two parameters, t"
W13-3308,P12-1048,0,0.0480289,"t al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-best list that have a high score on some MT metric. The translation step is then repeated using the new weights for decoding, and optimization is continued on a new k-best list, or on a combination of all k-best lists. This is repeated until"
W13-3308,W10-2602,1,0.888675,"shop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase t"
W13-3308,2005.mtsummit-papers.11,0,0.014426,"is already somewhat unstable this is a potential issue that needs to be explored further, which we do in Section 5. Implementation-wise we adapted Docent to output k-lists and adapted the infrastructure available for tuning in the Moses decoder (Koehn et al., 2007) to work with document-level scores. This setup allows us to use the variety of optimization procedures implemented there. 5 5.1 Experimental Setup Most of our experiments are for German-toEnglish news translation using data from the WMT13 workshop.1 We also show results with document-level features for English-to-Swedish Europarl (Koehn, 2005). The size of the training, tuning, and test sets are shown in Table 1. First of all, we need to extract documents for tuning and testing with Docent. Fortunately, the news data already contain document markup, corresponding to individual news articles. For Europarl we define a document as a consecutive sequence of utterances from a single speaker. To investigate the effect of the size of the tuning set, we used different subsets of the available tuning data. All our document-level experiments are carried out with Docent but we also contrast with the Moses decoder (Koehn et al., 2007). For the"
W13-3308,W10-1737,0,0.202315,"Missing"
W13-3308,N12-1046,0,0.199813,"el features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to int"
W13-3308,W12-0117,0,0.0131317,"ed a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has"
W13-3308,2011.mtsummit-papers.13,0,0.118068,"ts for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type o"
W13-3308,2012.amta-papers.20,0,0.335871,"eight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities f"
W13-3308,W01-1408,0,0.0344757,"res to model discourse phenomena such as anaphora, discourse connectives, and lexical consistency. In this paper, we therefore propose an approach that supports discourse-wide features in documentlevel decoding by adapting existing frameworks for sentence-level optimization. Furthermore, we include a thorough empirical investigation of this approach. Introduction 2 Discourse-Level SMT Traditional SMT systems translate texts sentence by sentence, assuming independence between sentences. This assumption allows efficient algorithms based on dynamic programming for exploring a large search space (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translat"
W13-3308,P03-1021,0,0.0673075,"est list, or on a combination of all k-best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works"
W13-3308,P02-1040,0,0.0886974,"riments, the decoder always stopped when reaching the rejection limit, usually between 1–5 Experiments In this section we report experimental results where we investigate several issues in connection with document-level feature weight optimization for SMT. We first describe the experimental setup, followed by baseline results using sentencelevel optimization. We then present validation experiments with standard sentence-level features, 1 http://www.statmt.org/wmt13/ translation-task.html 63 System Moses Docent-M Docent-R Moses Docent-M Docent-R million iterations. We show results on the Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) metrics. For German–English we show the average result and standard deviation of three optimization runs, to control for optimizer instability as proposed by Clark et al. (2011). For English–Swedish we report results on single optimization runs, due to time constraints. 5.2 Bleu 17.7 17.7 15.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to sh"
W13-3308,W13-5634,1,0.917086,"an restarting the decoder from the previous best state. 62 Training Tuning Test German–English Type Sentences Europarl 1.9M News Commentary 178K News2009 2525 News2008-2010 7567 News2012 3003 Documents – – 111 345 99 English–Swedish Type Sentences Europarl 1.5M – – Europarl (Moses) 2000 Europarl (Docent) 1338 Europarl 690 Documents – – – 100 20 Table 1: Domain and number of sentences and documents for the corpora which can be compared to standard optimization. Finally, we report results with a set of documentlevel features that have been proposed for joint translation and text simplification (Stymne et al., 2013). As seen in Figure 1, there are some additional parameters in our procedure: the sample start iteration and the sample interval. We also need to set the number of decoder iterations to run. In Section 5 we empirically investigate the effect of these parameters. Compared to sentence-level optimization, we also have a smaller number of units to get scores from, since we use documents as units, and not sentences. The importance of this depends on the optimization algorithm. MERT calculates metric scores over the full tuning set, not for individual sentences, and should not be affected too much b"
W13-3308,P07-2045,0,\N,Missing
W13-5634,2012.eamt-1.33,0,0.0610232,"Missing"
W13-5634,W09-2404,0,0.0675313,"summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NO"
W13-5634,W12-3156,0,0.0177139,"easured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electr"
W13-5634,D07-1007,0,0.0351635,"relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document leve"
W13-5634,P07-1005,0,0.0350039,"bility and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document level, which would be an"
W13-5634,daelemans-etal-2004-automatic,0,0.0211848,"a har sagt det - EU:s möte i Lissabon lagt särskild vikt vid vår för att genomföra risk i så att den plan att bli klar under 2003. SL (high) Som ledamöterna vet vissa talare har nämnt - Europeiska rådet i Lissabon särskilt uppmärksammat främja våra ansträngningar att genomföra riskkapital så att handlingsplanen avslutas 2003. Table 5: Examples of translation output from a sample of systems sentence compression (e.g., Knight and Marcu, 2000; Specia, 2010). Furthermore, there is a wide range of publications using other methods for monolingual sentence compression and text simplification, (e.g., Daelemans et al., 2004; Cohn and Lapata, 2009). Readability has also been investigated as an effect of text summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexi"
W13-5634,D11-1108,0,0.0132082,"et al. (2012) investigate the task of translating subtitles where time and space constraints are important, which leads to the task of sentence compression, which is related to our work on simplifying translated texts. They introduce dynamic length penalties which they integrate in a standard SMT decoder. Their model successfully compresses subtitles on three data sets. However, they also show that a similar compression can be achieved with appropriate tuning data that meets the length constraints. There are also a number of studies that use SMT techniques for monolingual paraphrasing (e.g., Ganitkevitch et al., 2011) and Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 381 of 474] Source As the honourable Members know - some speakers have mentioned it - the European Council at Lisbon paid particular attention to promoting our efforts to implement risk capital in such a way that the action plan will be finished in 2003. Baseline Som de ärade ledamöterna vet - vissa talare har nämnt det - som Europeiska rådet i Lissabon ägnat särskild uppmärksamhet åt att främja våra ansträngningar att genomföra riskkapital på ett s"
W13-5634,D10-1016,0,0.0191157,"on metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document level, which would be an interesting direction for future work. 6 Conclusion a"
W13-5634,D12-1108,1,0.934115,"rt words as indicators. Our goal is to incorporate these features in machine translation in order to combine text simplification and adequate translation in one system. To the best of our knowledge, this has not been attempted before and represents a novel and challenging idea in the field of MT research. Global features such as the ones mentioned above require new approaches to the general problem of decoding in SMT. Fortunately, we have recently presented a new document-level decoder, which, contrary to standard SMT decoders, translates documents as a unit instead of sentences in isolation (Hardmeier et al., 2012). This allows us to define document-wide features in the target language to test our ideas. Our application is also a good test case for the capabilities of the decoder and we would like to use our findings in future developments of general user-targeted machine translation. The contributions of this paper are thus two-fold: (1) We show that document-wide decoding can effectively use global features and (2) we demonstrate that readability features can be used in SMT to produce simplified text translations. The remainder of the paper is organized as follows: First, we introduce important backgr"
W13-5634,2005.mtsummit-papers.11,0,0.0391844,"alue feature both on word level and on phrase level. On the phrase level we consider the phrases used by the SMT decoder, and on the word level we consider individual source words, and their alignment to 0 − N target words. TTR = Q-value = 4 C(tokens) C(types) f (st) n(s) + n(t) (7) (8) Experiments In the following, we show results for our experiments with the Docent decoder that include readability features and compare them to runs without them. The systems are evaluated using both MT and readability metrics. 4.1 Experimental Setup We evaluate our models on parliamentary texts from Europarl (Koehn, 2005), which contain both complex sentences and a lot of domain-specific terminology. All tests are performed for English–Swedish translation. Our system is trained on 1,488,322 sentences. For evaluation, we extracted 20 documents with a total of 690 sentences from a separate part of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We init"
W13-5634,P07-2045,0,0.0103962,"both MT and readability metrics. 4.1 Experimental Setup We evaluate our models on parliamentary texts from Europarl (Koehn, 2005), which contain both complex sentences and a lot of domain-specific terminology. All tests are performed for English–Swedish translation. Our system is trained on 1,488,322 sentences. For evaluation, we extracted 20 documents with a total of 690 sentences from a separate part of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We initialized our experiments with a Moses model that uses standard features of a phrase-based system: a 5-gram language model, five translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weig"
W13-5634,N03-1017,0,0.0152721,"endence between the sentences in a text. This independence assumption is exploited in the most popular SMT decoding algorithms, which efficiently explore a very large search space by using dynamic programming (Och et al., 2001). Integrating discourse-wide information into traditional SMT decoders is difficult because of these dynamic programming assumptions. We therefore implement our document-level readability models in the recently published document-level SMT decoder Docent (Hardmeier et al., 2012), which does not have these limitations. The model implemented by Docent is phrase-based SMT (Koehn et al., 2003). The decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by applying a series of operations using a hill climbing strategy to find a (local) maximum of the score function. The three operations used are to change the translation of phrases, to swap the position of two phrases , and to resegment phrases. This setup is not limited by dynamic programming constraints, so we can define Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference"
W13-5634,P03-1021,0,0.0433779,"of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We initialized our experiments with a Moses model that uses standard features of a phrase-based system: a 5-gram language model, five translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weights for the readability-based features with low, medium, and high impact relative to the standard features. We performed automatic evaluations using a set of common metrics for MT quality and readability. For MT quality we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Con"
W13-5634,W01-1408,0,0.0740805,"paper is organized as follows: First, we introduce important background on document-level decoding and readability. Thereafter, we present our experiments using a set of global features. Finally, we add some information about related work, summarize our findings and give ideas about future work. 2 Document-wide SMT Most current SMT systems translate sentences individually, assuming independence between the sentences in a text. This independence assumption is exploited in the most popular SMT decoding algorithms, which efficiently explore a very large search space by using dynamic programming (Och et al., 2001). Integrating discourse-wide information into traditional SMT decoders is difficult because of these dynamic programming assumptions. We therefore implement our document-level readability models in the recently published document-level SMT decoder Docent (Hardmeier et al., 2012), which does not have these limitations. The model implemented by Docent is phrase-based SMT (Koehn et al., 2003). The decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by applying a series of operations using a hill climb"
W13-5634,P02-1040,0,0.0861851,"ive translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weights for the readability-based features with low, medium, and high impact relative to the standard features. We performed automatic evaluations using a set of common metrics for MT quality and readability. For MT quality we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 378 of 474] Feature Reference Baseline OVIX TTR Qw Qp nLW nXLW SL Weight – – low medium high low medium high low medium high low medium high low medium high low medium high low medium high BLEU↑ – 0.243 0.243 0.228 0.144 0.243 0.225 0.150 0.242 0.231 0.165 0.243 0.229 0.097 0.244 0.225 0.106 0.241 0.225 0.224 0.242 0.211 0.150 NIST↑ – 6.12 6.11 5.83 4.41 6.12 5.75 4.48 6.10 5.90 4.93 6.12 5.99 3.90 6.14 5.96 4.11 6.10 5.85 5"
W13-5634,W11-4627,0,0.0286822,"det i Lissabon särskilt uppmärksammat främja våra ansträngningar att genomföra riskkapital så att handlingsplanen avslutas 2003. Table 5: Examples of translation output from a sample of systems sentence compression (e.g., Knight and Marcu, 2000; Specia, 2010). Furthermore, there is a wide range of publications using other methods for monolingual sentence compression and text simplification, (e.g., Daelemans et al., 2004; Cohn and Lapata, 2009). Readability has also been investigated as an effect of text summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translatio"
W13-5634,P11-4010,1,0.864145,"Missing"
W13-5634,W10-2602,1,0.879769,"rgarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Pr"
W14-3312,J92-4003,0,0.0276371,"nd 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special placeholders. At decoding time, if a placeholder is encountered in a target language phrase, the applicable pronouns are generated with equal translation model proba"
W14-3312,P10-2033,0,0.0198937,"e case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The final system is a two-step model in which we apply translation and language models trained on preordered target language data to perform the first step, which also includes a reordered POS language model. The second step is also treated as a translation problem as in Sudoh et al. (2011), and in our newstest2013 19.3 19.4 18.6 19.5 19.5 19.7 newstest2014"
W14-3312,W13-2210,0,0.0299303,"Missing"
W14-3312,P05-1066,0,0.162059,"Missing"
W14-3312,N13-1073,0,0.0402683,"US The fall of Saddam ushers in the right circumstances. Der Sturz von Saddam leitet solche richtigen Umst¨ande ein. Der Sturz von Saddam ein leitet solche richtigen Umst¨ande. Table 1: Two examples of pre-ordering outputs. The first two lines are the original English and German sentences and the third line shows the reordered sentence. We use three systems based on Moses to compare the effect of reordering on alignment and translation. All systems are case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation"
W14-3312,C10-1043,0,0.0141995,"l training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering be"
W14-3312,E12-1074,0,0.0128122,"tables. Another reason is the possible distance of finite and infinitival verbs in German verb phrases that can lead to the same problems described above with verb-particle constructions. The auxiliary or modal verb is placed at the second position but the main verb appears at the end of the associated verb phrase. The distances can be arbitrarily long and long-range dependencies are quite frequent. Similarly, negation particles and adverbials move away from the inflected verb forms in certain constructions. For more details on specific phenomena in German, we refer to (Collins et al., 2005; Gojun and Fraser, 2012). Pre-ordering, i.e. moving English words into German word order does not seem to be a good option as we loose the connection between related items when moving particles and main verbs away from their associated elements. Hence, we are interested in reordering the target language German into English word order which can be beneficial in two ways: (i) Reordering the German part of the parallel training data makes it possible to improve word alignment (which tends to prefer monotonic mappings) and subsequent phrase extraction which leads to better translation models. (ii) We can explore a two-st"
W14-3312,D13-1037,1,0.676415,"n Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dyn"
W14-3312,W11-2123,0,0.0336108,"em submitted by Cho et al. (2013) to the WMT 2013 shared task. Our phrase table is trained on data taken from the News commentary, Europarl, UN, Common crawl and 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special pl"
W14-3312,D07-1103,0,0.0113749,"Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and o"
W14-3312,2005.iwslt-1.8,0,0.039688,"tymne et al., 2013) that it was useful to relax the hard distortion limit by either using a soft constraint, which could be tuned, or removing the limit completely. In that work we still used the standard parametrization of distortion, based on the positions of the first and last words in phrases. Our Docent decoder, however, always provides us with a full target translation that is step-wise improved, which means that we can apply distortion measures on the phrase-level without resorting to heuristics, which, for instance, are needed in the case of the lexicalized reordering models in Moses (Koehn et al., 2005). Because of this it is possible to use phrase-based distortion, where we calculate distortion based on the order of phrases, not on the order of some words. It is possible to parametrize phrase-distortion in different ways. In this work we use the phrase-distortion distance and a soft limit on the distortion distance, to mimic the word-based distortion. In our experiments we always set the soft limit to a distance of four phrases. In addition we use a measure based on how many crossings a phrase order gives rise to. We thus have three phrase-distortion features. As captured by lexicalized reo"
W14-3312,2009.mtsummit-posters.13,0,0.0172318,"e shortest distance between any pair of words in the aligned sets. The network is a binary classifier trained to discriminate positive examples extracted from human-made reference 123 amod nn auxpass by SMT systems with limited reordering capabilities such as phrase-based models. Preordering is often done on the entire training data as well to optimize translation models for the pre-ordered input. Less common is the idea of post-ordering, which refers to a separate step after translating source language input to an intermediate target language with corrupted (source-language like) word order (Na et al., 2009; Sudoh et al., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using th"
W14-3312,D12-1108,1,0.882504,"mne J¨org Tiedemann Aaron Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2"
W14-3312,W09-0435,0,0.11068,"trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering becomes attractive for several reasons: One reason is"
W14-3312,P13-4033,1,0.682907,"n Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dyn"
W14-3312,W11-2124,0,0.0221714,"ared task. Our phrase table is trained on data taken from the News commentary, Europarl, UN, Common crawl and 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special placeholders. At decoding time, if a placeholder is encounte"
W14-3312,W01-1408,0,0.0359546,"et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level and long-distance features into a traditional SMT decoder. In contrast to this very popular stack decoding approach, our decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a) implements a search procedure based on local search. At any stage of the search process, its search state consists of a complete document translation, making it easy for feature models to access the complete document Joakim Nivre with its current translation at any point in time. The search algorithm is a stochastic"
W14-3312,P03-1021,0,0.00912326,"our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain lang"
W14-3312,popovic-ney-2006-pos,0,0.0838413,"Missing"
W14-3312,2007.tmi-papers.21,0,0.0459927,"a and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering becomes attractive for sever"
W14-3312,W10-1727,1,0.857022,"e-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The final system is a two-step model in which we apply translation and language models trained on preordered target language data to perform the first step, which also includes a reordered POS language model. The second step is also treated as a translation problem as in Sudoh et al. (2011), and in our newstest2013 19.3 19.4 18.6 19.5 19.5 19.7 newstest2014 19.1 19.3 18.7 19.3 1"
W14-3312,W13-2229,1,0.877852,"l., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are give"
W14-3312,2011.mtsummit-papers.36,0,0.175414,"ce between any pair of words in the aligned sets. The network is a binary classifier trained to discriminate positive examples extracted from human-made reference 123 amod nn auxpass by SMT systems with limited reordering capabilities such as phrase-based models. Preordering is often done on the entire training data as well to optimize translation models for the pre-ordered input. Less common is the idea of post-ordering, which refers to a separate step after translating source language input to an intermediate target language with corrupted (source-language like) word order (Na et al., 2009; Sudoh et al., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et"
W14-3312,tiedemann-2012-parallel,1,0.687825,"st¨ande ein. Der Sturz von Saddam ein leitet solche richtigen Umst¨ande. Table 1: Two examples of pre-ordering outputs. The first two lines are the original English and German sentences and the third line shows the reordered sentence. We use three systems based on Moses to compare the effect of reordering on alignment and translation. All systems are case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The"
W14-3312,C04-1073,0,0.0399491,"et side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Po"
W14-3312,P07-2045,0,\N,Missing
W14-3312,W12-3144,0,\N,Missing
W14-3312,2011.iwslt-evaluation.9,0,\N,Missing
W15-2501,W15-2508,1,0.700494,"all submissions, both primary and secondary, in terms of macro-averaged F-score, several systems performed better in terms of accuracy. The other systems did not use explicit anaphora resolution, but attempted to gather relevant information about possible antecedents by considering a certain number of preceding, or preceding and following, noun phrases. They differed in the type of classifier and in the information sources used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED U"
W15-2501,E06-1032,0,0.027114,"itself achieves the best scores, but considering the inadequacy of BLEU for pronoun evaluation, we do not see this as a major concern in itself. The other submissions fall behind in terms of automatic MT metrics. The UU - HARDMEIER system is similar to the other SMT systems, but uses different language and translation models, which evidently do not yield the same level of raw MT performance as the baseline system. ITS 2 is a rule-based system. Since it is well known that n-gram-based evaluation metrics do not always do full justice to rule-based MT approaches not using n-gram language models (Callison-Burch et al., 2006), it is difficult to draw definite conclusions from this system’s lower scores. Finally, the extremely low scores for the A 3-108 system indicate serious problems with translation quality, an impression that we easily confirmed by examining the system output. 8 5 The low scores for the ITS 2 system were partly due to a design decision. The anaphora prediction component of ITS 2 only generated the personal pronouns il, elle, ils and elles; this led to zero recall for ce and ça/cela and, as a consequence, to a large number of misses that would have been comparatively easy to predict with an n-gr"
W15-2501,P14-1065,1,0.749966,"Missing"
W15-2501,2012.eamt-1.60,1,0.298652,"est data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare"
W15-2501,2010.iwslt-papers.10,1,0.800853,"g such a setup makes it possible to explore a variety of approaches for solving the problem at hand since the participating groups independently come up with various ways to address it. All of this is highly beneficial for continued research as it creates a well-defined benchmark with a low entry barrier, a set of results to compare to, and a collection of properly evaluated ideas to start from. We decided to base this shared task on the problem of pronoun translation. Historically, this was one of the first discourse problems to be considered in the context of SMT (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010); yet, it is still far from being solved. For an overview of the existing work on pronoun translation, we refer the reader to Hardmeier (2014, Section 2.3.1). The typical case is an anaphoric pronoun – one that refers to an entity mentioned earlier in the discourse, its antecedent. Many languages have agreement constraints between pronouns and their antecedents. In translation, these constraints must be satisfied in the target language. Note that source language information is not enough for this task. To see why, consider the following example for English– French:1 We describe the design, the"
W15-2501,P13-4033,1,0.934301,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,chrupala-etal-2008-learning,0,0.0467164,"Missing"
W15-2501,W11-2107,0,0.0222412,"Missing"
W15-2501,D13-1037,1,0.793439,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,N13-1073,0,0.0272151,"inting characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoot"
W15-2501,W15-2510,1,0.750725,"hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), they do not specifically focus on pronoun translation. 5 Machine Translation Evaluation http://stp.lingfil.uu.se/~ch/DiscoMT2015.maneval/index.php Machine Translation Eva"
W15-2501,J07-3002,0,0.00746111,"n order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all three models, the results on pronoun links are better than those on all links. More"
W15-2501,P13-2121,0,0.00705082,"Missing"
W15-2501,D07-1103,0,0.0077006,"00 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200"
W15-2501,guillou-etal-2014-parcor,1,0.687529,"g requirements: 1. The talks have been transcribed (in English) and translated into French. 2. They were not included in the training, development, and test datasets of any IWSLT evaluation campaign, so DiscoMT.tst2015 can be used as held-out data with respect to those. 3. They contain a sufficient number of tokens of the English pronouns it and they translated into the French pronouns listed in Table 1. 4. They amount to a total number of words suitable for evaluation purposes (e.g., tens of thousands). 2 http://www.ted.com 3 The following overview of text characteristics is based on work by Guillou et al. (2014). 3 To meet requirement 3, we selected talks for which the combined count of the rarer classes ça, cela, elle, elles and on was high. The resulting distribution of pronoun classes, according to the extraction procedure described in Section 5.1, can be found in Table 8 further below. We aimed to have at least one pair of talks given by the same speaker and at least one pair translated by the same translator. These two features are not required by the DiscoMT shared task, but could be useful for further linguistic analysis, such as the influence of speakers and translators on the use of pronouns"
W15-2501,W15-2509,0,0.0482925,"gy of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T sy"
W15-2501,W14-3352,1,0.899484,"Missing"
W15-2501,D14-1027,1,0.902972,"Missing"
W15-2501,P07-2045,0,0.00942963,"2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million w"
W15-2501,2005.mtsummit-papers.11,0,0.0282203,"e resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For compl"
W15-2501,W15-2514,0,0.067491,"Missing"
W15-2501,W10-1737,0,0.740286,"Missing"
W15-2501,W11-1902,0,0.185739,"sions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with"
W15-2501,W15-2512,0,0.121441,"glish-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For completeness, we also provided a recasing model that was trained on the same dataset to render it straightforward to produce case-sensitive output, which we required as the final submission. 4.2 ITS 2 (Loáiciga and Wehrli, 2015) was a rulebased machine translation system using syntaxbased transfer. For the shared task, it was extended with an anaphora resolution component influenced by Binding Theory (Chomsky, 1981). For the sixth submission, A 3-108, no system description paper was submitted. Its output seemed to have been affected by problems at the basic MT level, yielding very bad translation quality. 4.3 Evaluation Methods Evaluating machine translations for pronoun correctness automatically is difficult because standard assumptions fail. In particular, it is incorrect to assume that a pronoun is translated corr"
W15-2501,2006.amta-papers.25,0,0.144804,"Missing"
W15-2501,W15-2511,0,0.0430542,"s used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED UEDIN MALTA 2 MALTA WHATELLES UEDIN 2 UU - TIED 2 GENEVA GENEVA 2 IDIAP IDIAP 2 A 3-108 ( WITHDRAWN ) Macro-F Accuracy ce cela elle elles F-score il 0.584 0.579 0.571 0.565 0.561 0.553 0.550 0.539 0.437 0.421 0.206 0.164 0.129 0.122 0.663 0.742 0.723 0.740 0.732 0.721 0.714 0.734 0.592 0.579 0.307 0.407 0.240 0.325 0.817 0.862 0.823 0.875 0.853 0.862 0.823 0.849 0.647 0.611 0.282 0.152 0.225 0.220 0.346 0.235 0.213 0.1"
W15-2501,W14-3334,1,0.85254,"ed the same bitext as for the MT baseline in the first task (Section 4.1); we pre-processed it like before, except for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignme"
W15-2501,W15-2513,0,0.169961,"ompared to the perhaps more obvious methodology of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline"
W15-2501,tiedemann-2012-parallel,1,0.765503,"ic or human processing. Table 3 shows some statistics and metadata about the TED talks that are part of the DiscoMT.tst2015 set. talk id segs 205 1756 1819 1825 1894 1935 1938 1950 1953 1979 2043 2053 189 186 147 120 237 139 107 243 246 160 175 144 4,188 4,320 2,976 2,754 5,827 3,135 2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment"
W15-2501,W15-2515,1,0.722711,"and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphor"
W15-2501,C96-2141,0,0.0650665,"for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . Fo"
W15-2501,P00-1056,0,0.0244007,"reated automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all t"
W15-2501,W15-2516,0,0.0553089,"air, and (ii) the sums for each row/column; • accuracy; All six groups with system description papers used some form of machine learning. The main difference was whether or not they explicitly attempted to resolve pronominal coreference. Two systems relied on explicit anaphora resolution: UEDIN and MALTA. They both applied the Stanford coreference resolver (Lee et al., 2011) on the source language text, then projected the antecedents to the target language through the word alignments, and finally obtained morphological tags with the Morfette software (Chrupała et al., 2008). The UEDIN system (Wetzel et al., 2015) was built around a maximum entropy classifier. In addition to local context and antecedent information, it used the NADA tool (Bergsma and Yarowsky, 2011) to identify nonreferring pronouns and included predictions by a standard n-gram language model as a feature. The MALTA system (Pham and van der Plas, 2015) was based on a feed-forward neural network combined with word2vec continuous-space word embeddings (Mikolov et al., 2013). It used local context and antecedent information. • precision (P), recall (R), and F-score for each label; • micro-averaged P, R, F-score (note that in our setup, mi"
W15-2501,J03-1002,0,0.00752171,"ing two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and s"
W15-2501,P03-1021,0,0.00636797,"filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version"
W15-2501,P02-1040,0,\N,Missing
W15-2501,W14-3302,0,\N,Missing
W15-2508,S10-1021,0,0.0757872,"Missing"
W15-2508,E12-3001,0,0.260488,"Missing"
W15-2508,2010.iwslt-papers.10,1,0.942001,"Missing"
W15-2508,P13-4033,1,0.916362,"Missing"
W15-2508,D13-1037,1,0.923019,"Missing"
W15-2508,W15-2501,1,0.764343,"Missing"
W15-2508,P14-6007,0,0.0889922,"Missing"
W15-2508,P07-2045,0,0.0057455,"Missing"
W15-2508,2005.mtsummit-papers.11,0,0.333417,"Missing"
W15-2508,W10-1737,0,0.664347,"Missing"
W15-2508,1995.tmi-1.6,0,0.128158,"Missing"
W15-2510,D13-1037,1,0.896874,"are optimised towards the BLEU score (Papineni et al., 2002) with the MERT algorithm (Och, 2003) as implemented in Moses. Introduction One of Uppsala University’s submissions to the pronoun-focused translation task at DiscoMT 2015 is a document-level phrase-based statistical machine translation (SMT) system integrating a neural network classifier for pronoun prediction. The system unites various contributions to discourselevel machine translation that we made during the last few years: The translation system uses our document-level decoder for phrase-based SMT, Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). The pronoun prediction network was first described by Hardmeier et al. (2013b), and its integration into the decoder by Hardmeier (2014, Chapter 9). In comparison to previous work, the size of the parallel training corpus has been reduced to be more consistent with the official data sets of the shared task. However, for practical reasons, we still use previously trained models that do not match the constraints of the official data sets exactly. Also, while the latent anaphora resolution approach of Hardmeier et al. (2013b) is used for training, allowing us to train our system without runni"
W15-2510,W15-2501,1,0.814857,"earlier experiments suggested that the latent anaphora resolution method integrated in the pronoun prediction network, though useful for training, may not be sufficient for good performance at test time, we decided to use annotations created with an external coreference resolution system when translating the test set. Coreference links were 1 We are grateful to Liane Guillou for providing us with ready-made CoreNLP annotations of the DiscoMT test set. 2 For a presentation and discussion of the complete shared task methodology and results, we refer the reader to the shared task overview paper (Hardmeier et al., 2015). 74 Precision This system Rmax Fmax Baseline Fmax ce ça/cela elle elles il ils on 29/ 9/ 3/ 3/ 7/ 45/ 0/ 35 10 9 3 43 54 0 (0.829) (0.900) (0.333) (1.000) (0.163) (0.833) (n/a) 32/ 22/ 3/ 4/ 11/ 45/ 0/ 45 60 20 15 19 48 0 (0.711) (0.367) (0.150) (0.267) (0.579) (0.938) (n/a) 0.765 0.521 0.207 0.421 0.254 0.882 n/a 0.832 0.631 0.452 0.436 0.522 0.900 n/a Micro-average 96/154 (0.623) 96/177 (0.542) 0.580 0.699 Accuracy with OTHER: 122/210 = 0.581 (Baseline: 0.676) Accuracy without OTHER: 96/183 = 0.525 (Baseline: 0.630) 6 bad translations (Baseline: 9) Table 1: Manual evaluation results for the"
W15-2510,P07-2045,0,0.0115738,"ly, we include a 4-gram bilingual LM (Niehues et al., 2011) and a 9-gram LM over Brown clusters (Brown et al., 1992). Both of these are trained with SRILM (Stolcke et al., 2011) using Witten-Bell smoothing (Witten and Bell, 1991) over a corpus consisting of TED, Europarl, News commentary and United Nations data. Unlike the official baseline, we do not use any lowercasing, recasing or truecasing steps in our training procedure. Instead, all our models are trained directly on the original text in the form in which it occurs in the corpus data. The phrase table is trained with the Moses toolkit (Koehn et al., 2007), and the feature weights of all the models except for the pronoun prediction classifier are optimised towards the BLEU score (Papineni et al., 2002) with the MERT algorithm (Och, 2003) as implemented in Moses. Introduction One of Uppsala University’s submissions to the pronoun-focused translation task at DiscoMT 2015 is a document-level phrase-based statistical machine translation (SMT) system integrating a neural network classifier for pronoun prediction. The system unites various contributions to discourselevel machine translation that we made during the last few years: The translation syst"
W15-2510,J13-4004,0,0.213815,"014, Chapter 9). In comparison to previous work, the size of the parallel training corpus has been reduced to be more consistent with the official data sets of the shared task. However, for practical reasons, we still use previously trained models that do not match the constraints of the official data sets exactly. Also, while the latent anaphora resolution approach of Hardmeier et al. (2013b) is used for training, allowing us to train our system without running anaphora resolution over the entire training corpus, we rely on coreference annotations generated with the Stanford CoreNLP toolkit (Lee et al., 2013) at test time, as we believe them to be more reliable. To increase the effect of the pronoun prediction model, our system uses pronoun placeholders for the pronouns il, elle, ils and elles (Hardmeier, 2014, Chapter 9). In the phrase table and the main LM, these pronouns are substituted by four placeholders, LCPRONOUN - SG and UCPRONOUN - SG for upperand lowercase il or elle and LCPRONOUN - PL and UCPRONOUN - PL for upper- and lowercase ils and 72 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 72–77, c Lisbon, Portugal, 17 September 2015. 2015 Associatio"
W15-2510,J92-4003,0,0.373178,"the shared task organisers. The system we use is a standard phrase-based SMT system with a phrase table trained on the TED, Europarl (v7) and News commentary (v9) corpora. The system has 3 language models (LMs). The main LM is a 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained with KenLM (Heafield, 2011) on the TED, News commentary and News crawl corpora provided for the WMT 2014 shared task (Bojar et al., 2014) and the French Gigaword corpus, LDC2011T10. Additionally, we include a 4-gram bilingual LM (Niehues et al., 2011) and a 9-gram LM over Brown clusters (Brown et al., 1992). Both of these are trained with SRILM (Stolcke et al., 2011) using Witten-Bell smoothing (Witten and Bell, 1991) over a corpus consisting of TED, Europarl, News commentary and United Nations data. Unlike the official baseline, we do not use any lowercasing, recasing or truecasing steps in our training procedure. Instead, all our models are trained directly on the original text in the form in which it occurs in the corpus data. The phrase table is trained with the Moses toolkit (Koehn et al., 2007), and the feature weights of all the models except for the pronoun prediction classifier are opti"
W15-2510,W11-2124,0,0.0270513,"system is different from the official baseline provided by the shared task organisers. The system we use is a standard phrase-based SMT system with a phrase table trained on the TED, Europarl (v7) and News commentary (v9) corpora. The system has 3 language models (LMs). The main LM is a 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained with KenLM (Heafield, 2011) on the TED, News commentary and News crawl corpora provided for the WMT 2014 shared task (Bojar et al., 2014) and the French Gigaword corpus, LDC2011T10. Additionally, we include a 4-gram bilingual LM (Niehues et al., 2011) and a 9-gram LM over Brown clusters (Brown et al., 1992). Both of these are trained with SRILM (Stolcke et al., 2011) using Witten-Bell smoothing (Witten and Bell, 1991) over a corpus consisting of TED, Europarl, News commentary and United Nations data. Unlike the official baseline, we do not use any lowercasing, recasing or truecasing steps in our training procedure. Instead, all our models are trained directly on the original text in the form in which it occurs in the corpus data. The phrase table is trained with the Moses toolkit (Koehn et al., 2007), and the feature weights of all the mod"
W15-2510,D12-1108,1,0.926289,"n prediction classifier are optimised towards the BLEU score (Papineni et al., 2002) with the MERT algorithm (Och, 2003) as implemented in Moses. Introduction One of Uppsala University’s submissions to the pronoun-focused translation task at DiscoMT 2015 is a document-level phrase-based statistical machine translation (SMT) system integrating a neural network classifier for pronoun prediction. The system unites various contributions to discourselevel machine translation that we made during the last few years: The translation system uses our document-level decoder for phrase-based SMT, Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). The pronoun prediction network was first described by Hardmeier et al. (2013b), and its integration into the decoder by Hardmeier (2014, Chapter 9). In comparison to previous work, the size of the parallel training corpus has been reduced to be more consistent with the official data sets of the shared task. However, for practical reasons, we still use previously trained models that do not match the constraints of the official data sets exactly. Also, while the latent anaphora resolution approach of Hardmeier et al. (2013b) is used for training, allowing us to train"
W15-2510,P02-1040,0,0.0917025,"th SRILM (Stolcke et al., 2011) using Witten-Bell smoothing (Witten and Bell, 1991) over a corpus consisting of TED, Europarl, News commentary and United Nations data. Unlike the official baseline, we do not use any lowercasing, recasing or truecasing steps in our training procedure. Instead, all our models are trained directly on the original text in the form in which it occurs in the corpus data. The phrase table is trained with the Moses toolkit (Koehn et al., 2007), and the feature weights of all the models except for the pronoun prediction classifier are optimised towards the BLEU score (Papineni et al., 2002) with the MERT algorithm (Och, 2003) as implemented in Moses. Introduction One of Uppsala University’s submissions to the pronoun-focused translation task at DiscoMT 2015 is a document-level phrase-based statistical machine translation (SMT) system integrating a neural network classifier for pronoun prediction. The system unites various contributions to discourselevel machine translation that we made during the last few years: The translation system uses our document-level decoder for phrase-based SMT, Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). The pronoun prediction network was"
W15-2510,J01-4004,0,0.207154,"head finder (Collins, 1999), again represented as onehot vectors. These vectors cannot be fed into the network directly because their number depends on the number of antecedent candidates and on the number of TL words aligned to the head word of each antecedent. Instead, they are averaged to yield a single vector per antecedent candidate. Finally, anaphoric link vectors (T) describe the relationship between an anaphor and a particular antecedent candidate. These vectors are generated by the feature extraction machinery in BART and include a standard set of features for coreference resolution (Soon et al., 2001; Uryupina, 2006) borrowed wholesale from a working coreference system. In the forward propagation pass, the input word representations are mapped to a low-dimensional representation in an embedding layer (E). In this layer, the embedding weights for all the SL vectors (the pronoun and its 6 context words) are tied, so if two words are the same, they are mapped to the same lower-dimensional embedding regardless of their position relative to the pronoun. To process the information contained in the antecedents, the network first computes the link probability for each antecedent candidate. The an"
W15-2510,P13-4033,1,0.947351,"are optimised towards the BLEU score (Papineni et al., 2002) with the MERT algorithm (Och, 2003) as implemented in Moses. Introduction One of Uppsala University’s submissions to the pronoun-focused translation task at DiscoMT 2015 is a document-level phrase-based statistical machine translation (SMT) system integrating a neural network classifier for pronoun prediction. The system unites various contributions to discourselevel machine translation that we made during the last few years: The translation system uses our document-level decoder for phrase-based SMT, Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). The pronoun prediction network was first described by Hardmeier et al. (2013b), and its integration into the decoder by Hardmeier (2014, Chapter 9). In comparison to previous work, the size of the parallel training corpus has been reduced to be more consistent with the official data sets of the shared task. However, for practical reasons, we still use previously trained models that do not match the constraints of the official data sets exactly. Also, while the latent anaphora resolution approach of Hardmeier et al. (2013b) is used for training, allowing us to train our system without runni"
W15-2510,uryupina-2006-coreference,0,0.0292907,"s, 1999), again represented as onehot vectors. These vectors cannot be fed into the network directly because their number depends on the number of antecedent candidates and on the number of TL words aligned to the head word of each antecedent. Instead, they are averaged to yield a single vector per antecedent candidate. Finally, anaphoric link vectors (T) describe the relationship between an anaphor and a particular antecedent candidate. These vectors are generated by the feature extraction machinery in BART and include a standard set of features for coreference resolution (Soon et al., 2001; Uryupina, 2006) borrowed wholesale from a working coreference system. In the forward propagation pass, the input word representations are mapped to a low-dimensional representation in an embedding layer (E). In this layer, the embedding weights for all the SL vectors (the pronoun and its 6 context words) are tied, so if two words are the same, they are mapped to the same lower-dimensional embedding regardless of their position relative to the pronoun. To process the information contained in the antecedents, the network first computes the link probability for each antecedent candidate. The anaphoric link feat"
W15-2510,P08-4003,0,0.0255023,"he weighted sum is then concatenated with the source language embeddings in the E layer. The embedding of the antecedent word vectors is independent from that of the SL features since they refer to a different vocabulary. The Pronoun Prediction Network We model pronoun prediction with the feedforward neural network classifier introduced by Hardmeier et al. (2013b). Its overall structure is shown in figure 1. To create input data for the network, we first generate a set of antecedent candidates for a given pronoun by running the preprocessing pipeline of the coreference resolution system BART (Versley et al., 2008). Each training example for our network can have an arbitrary number of antecedent candidates. Next, we prepare three types of features. Anaphor context features describe the source language (SL) pronoun (P) and its immediate context consisting of three words to its left (L1 to L3) and three words to its right (R1 to R3), encoded as one-hot vectors. Antecedent features (A) describe an antecedent candidate. CanIn the next step, the entire E layer is mapped to another hidden layer (H), which is in turn connected to a binary output layer predicting the classes il and elle for the singular classif"
W15-2510,J03-4003,0,\N,Missing
W15-2510,W14-3302,0,\N,Missing
W15-2510,W11-2123,0,\N,Missing
W15-2510,P03-1021,0,\N,Missing
W15-2522,J90-2002,0,0.872866,"Missing"
W15-2522,J07-2003,0,0.0490538,"ntuitive than in the examples cited. In practical applications, word alignments are essentially defined by what is found by the statistical alignment models used, and the issue of interpreting them is usually evaded. The cross-linguistic relation defined by word alignments is a sort of translational equivalence relation. It maps linguistic elements of the SL to elements of the TL that are presumed to have the same meaning, or convey the same message. The same is true of the phrase pairs of phrase-based SMT (Koehn et al., 2003) and the synchronous contextfree grammar rules of hierarchical SMT (Chiang, 2007), which are usually created from simple word alignments with mostly heuristic methods. None of these approaches exploits any procedural knowledge about linguistic techniques and their effects in the source and target community. Instead, it is assumed that each source text has an equivalent target text, possibly dependent on a set of context variables generally subsumed under the concept of domain, and that this target text can be constructed compositionally in a bottom-up fashion. The generation of word alignments is generally governed by two effects: A statistical dictionary or translation ta"
W15-2522,2012.eamt-1.2,0,0.0179585,"translation differs from that of the original text. A few such examples are mentioned in the literature. Stymne et al. (2013) describe an SMT system that combines translation with text simplification to cater to target groups with reading difficulties of various types. One of their main problems is the lack of training data having the desired properties on the TL side. However, even if such training data is available, SMT training is not necessarily successful. A case in point is the translation of film subtitles, where the target side is often shortened as well as translated (Pedersen, 2007; Fishel et al., 2012). Anecdotal evidence 170 suggests that MT systems easily learn the length ratio, but truncate the texts in an erratic way that has a negative effect on translation quality. 6 Some Suggestions Most current approaches to SMT are founded on word alignments in the spirit of Brown et al. (1990). These word alignments have no clear theoretical status, but they can be seen as an embodiment of a fairly traditional concept of translational equivalence. Equivalence in SMT is strongly surfaceoriented, and SMT technology has traditionally eschewed all abstract representations of meaning, mapping tokens of"
W15-2522,N03-1017,0,0.0091885,"e complex sentences, where the correspondence between source and target words is less intuitive than in the examples cited. In practical applications, word alignments are essentially defined by what is found by the statistical alignment models used, and the issue of interpreting them is usually evaded. The cross-linguistic relation defined by word alignments is a sort of translational equivalence relation. It maps linguistic elements of the SL to elements of the TL that are presumed to have the same meaning, or convey the same message. The same is true of the phrase pairs of phrase-based SMT (Koehn et al., 2003) and the synchronous contextfree grammar rules of hierarchical SMT (Chiang, 2007), which are usually created from simple word alignments with mostly heuristic methods. None of these approaches exploits any procedural knowledge about linguistic techniques and their effects in the source and target community. Instead, it is assumed that each source text has an equivalent target text, possibly dependent on a set of context variables generally subsumed under the concept of domain, and that this target text can be constructed compositionally in a bottom-up fashion. The generation of word alignments"
W15-2522,2005.mtsummit-papers.11,0,0.0366977,"with domain adaptation techniques. Although there is a great deal of literature on domain adaptation, few authors care to define exactly what a domain is. Frequently, a corpus of data from a single source, or a collection of corpora from similar sources, is referred to as a domain, so that researchers will refer to the “News” domain (referring to diverse collections of news documents from one or more sources such as news agencies or newspapers) or the “Europarl” domain (referring to the collection of documents from the proceedings of the European parliament published in the Europarl corpus) (Koehn, 2005) without investigating the homogeneity of these data sources in detail. Koehn (2010, 53) briefly discusses the domain concept. He seems to use the word as a synonym of “text type”, characterised by (at least) the dimensions of “modality” (spoken or written language) and “topic”. Bungum and Gamb¨ack (2011) present an interesting study of how the term is used in SMT research and how it relates to similar concepts in cognitive linguistics. In general, however, the term is used in a rather vague way and can encompass a variety of corpus-level features connected with genre conventions or the circum"
W15-2522,J10-4005,0,0.0236062,"domain adaptation, few authors care to define exactly what a domain is. Frequently, a corpus of data from a single source, or a collection of corpora from similar sources, is referred to as a domain, so that researchers will refer to the “News” domain (referring to diverse collections of news documents from one or more sources such as news agencies or newspapers) or the “Europarl” domain (referring to the collection of documents from the proceedings of the European parliament published in the Europarl corpus) (Koehn, 2005) without investigating the homogeneity of these data sources in detail. Koehn (2010, 53) briefly discusses the domain concept. He seems to use the word as a synonym of “text type”, characterised by (at least) the dimensions of “modality” (spoken or written language) and “topic”. Bungum and Gamb¨ack (2011) present an interesting study of how the term is used in SMT research and how it relates to similar concepts in cognitive linguistics. In general, however, the term is used in a rather vague way and can encompass a variety of corpus-level features connected with genre conventions or the circumstances of text use. There is a clear tendency in current SMT to treat all aspects"
W15-2522,P02-1040,0,0.0951105,"n. At translation time, domain adaptation techniques increase the likelihood of correct translations on average, but they do not provide the MT system with any information to support decision-making in particular cases. Therefore, domain adaptation does not appear to be promising as a method to impress a deeper linguistic understanding on SMT; instead, we should strive to overcome the strict dichotomy between word-level and corpus-level modelling and create an additional layer of modelling between the two extremes. Our stance on evaluation is similar. Aggregating evaluation methods like BLEU (Papineni et al., 2002) give a useful overview of the quality of a translation, but they do not afford specific information and leave too many details to chance. One possible alternative is the creation of test suites with carefully selected examples permitting quick, targeted manual evaluation of specific phenomena in the development phase. 7 Conclusions Current SMT rests on assumptions of straightforward translational equivalence that oversimplify the complexity of the translation process. Most fundamentally, the central concept of word alignment works well for content words, but is problematic for function words."
W15-2522,W13-5634,1,0.878065,"rn tendencies, but not actual dependencies. As an example, if a target language distinguishes between different levels of formality in its forms of address, domain easily captures which forms are generally preferred in a particular corpus, but it offers no help to decide which form should be selected in each individual case. In addition, there are circumstances in which the intentionality of the translation process cannot be ignored completely. This happens mostly when the intention of the translation differs from that of the original text. A few such examples are mentioned in the literature. Stymne et al. (2013) describe an SMT system that combines translation with text simplification to cater to target groups with reading difficulties of various types. One of their main problems is the lack of training data having the desired properties on the TL side. However, even if such training data is available, SMT training is not necessarily successful. A case in point is the translation of film subtitles, where the target side is often shortened as well as translated (Pedersen, 2007; Fishel et al., 2012). Anecdotal evidence 170 suggests that MT systems easily learn the length ratio, but truncate the texts i"
W16-2345,D12-1133,0,0.253709,"set of the provided training data that has well-defined document boundaries in order to allow for meaningful extraction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddi"
W16-2345,W16-2348,0,0.0249438,"urce word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER class. The difference between the primary and contrastive systems is small. In the primary system, the feature val"
W16-2345,P06-1005,0,0.150956,"res based on the target-language model estimates provided by the baseline system, linguistic features concerning the source word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER clas"
W16-2345,2012.eamt-1.60,1,0.85967,"predicting all of the other pronouns, the system relied solely on the scores coming from the proposed PLM model. This target-side PLM model uses a large target-language training dataset to learn a probabilistic relation between each target pronoun and the distribution of the gender-number of its preceding nouns and pronouns. For prediction, given each source pronoun “it” or “they”, the system uses the PLM to score all possible candidates and to select the one with the highest score. In addition to the PoS-tagged lemmatised data that was provided for the shared task, the WIT3 parallel corpus (Cettolo et al., 2012), provided as part of the training data at the DiscoMT 2015 workshop, was used to train the PLM model. Furthermore, a French PoS-tagger, Morfette (Chrupala et al., 2008), was employed for gendernumber extraction. Before extracting the examples as feature vectors, the data is linguistically preprocessed usˇ ing the Treex framework (Popel and Zabokrtsk´ y, 2010). The source-language texts undergo a thorough analysis and are enriched with PoS tags, dependency syntax, as well as semantic roles and coreference for English. On the other hand, only grammatical genders are assigned to nouns in the tar"
W16-2345,chrupala-etal-2008-learning,0,0.0898214,"Missing"
W16-2345,W16-2350,1,0.838182,"on. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trai"
W16-2345,W11-2123,0,0.0192239,"e classifier is trained on a combination of semantic, based on lexical resources such as VerbNet (Schuler, 2005) and WordNet (Miller, 1995), and frequencies computed over the annotated Gigaword corpus (Napoles et al., 2012), syntactic, from the dependency parser in the Mate tools (Bohnet et al., 2013), and contextual features. The event classification results are modest, reaching only 54.2 F-score for the event class. The translation model, into which the classifier is integrated, is a 6-gram language model computed over target lemmata using modified KneserNey smoothing and the KenLM toolkit (Heafield, 2011). In addition to the pure target lemma context, it also has access to the identity of the sourcelanguage pronoun, used as a concatenated label to each REPLACE item. This provides information about the number marking of the pronouns in the source, and also allows for the incorporation of the output of the ‘it’-label classifier. To predict classes for an unseen test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels p"
W16-2345,W16-2349,0,0.0373816,"sing the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun translation decisions. The model performs reasonably well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features"
W16-2345,W10-1737,0,0.434398,"Missing"
W16-2345,guillou-etal-2014-parcor,1,0.910739,"the fact that all talks are originally given in English, which means that French–English translation is in reality a back-translation. • she: feminine singular subject pronoun; 3 1 We explain below in Section 3.3.3 how non-subject pronouns are filtered out from the data. 528 TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. As shown in analysis presented by Guillou et al. (2014), TED talks differ from other text types with respect to pronoun usage. TED speakers frequently use first- and second-person pronouns (singular and plural): first-person to refer to themselves and their colleagues or to themselves and the audience, second-person to refer to the audience, the larger set of viewers, or people in general. TED speakers often use the pronoun “they” without a specific textual antecedent, in sentences such as “This is what they think.” They also use deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audie"
W16-2345,W16-2351,1,0.900928,"Missing"
W16-2345,E12-3001,1,0.880326,"it is required by syntax to fill the subject position. An event reference pronoun may refer to a verb phrase (VP), a clause, an entire sentence, or a longer passage of text. Examples of each of these pronoun functions are provided in Figure 1. It is clear that instances of the English pronoun “it” belonging to each of these functions would have different translation requirements in French and German. Introduction Pronoun translation poses a problem for current state-of-the-art Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). 525 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 525–542, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 The problem of pronouns in machine translation has long been studied. In particular, for SMT systems, the recent previous studies cited above have focused on the translation of anaphoric pronouns. In this case, a well-known constraint of languages with grammatical gender is that agreement must hold between an anaphoric pronoun and the NP with which it corefers, called its antecede"
W16-2345,W16-2352,1,0.881771,"Missing"
W16-2345,W16-2353,0,0.0435664,"s useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed sequences of these embeddings within a certain window to the left and to the right of the target pronoun. The window size used by the system is 50 tokens or until the end of the sentence boundary. All of these inputs are read"
W16-2345,2010.iwslt-papers.10,1,0.888921,"Missing"
W16-2345,D13-1037,1,0.883273,"3 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trained 6gram language model identical to the contrastive system of the UUPPSALA submission described above. The"
W16-2345,S16-1001,1,0.795211,"d 69.76 in macro-averaged recall. This is very much above the performance of baseline0 and baseline-1.5, which are in the low-mid 40s. It is also well above the majority/random baseline (not shown) at 11.11, which is outperformed by far by all systems. Note that the top-3 systems in terms of macro-averaged recall are also the top-3 in terms of accuracy, but in different order. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we adopted macro-averaged recall, which was also recently adopted by some other competitions, e.g., by SemEval-2016 Task 4 (Nakov et al., 2016). Moreover, as in 2015, we also report accuracy as a secondary evaluation measure. Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. 8 If the test data did not have any instances of some of the classes, we excluded these classes from the macro-a"
W16-2345,W15-2501,1,0.657407,"ould replace a placeholder value (represented by the token REPLACE) in the target-language text. It requires no specific Machine Translation (MT) expertise and is interesting as a machine learning task in its own right. Within the context of SMT, one could think of the task of cross-lingual pronoun prediction as a component of an SMT system. This component may take the form of a decoder feature or it may be used to provide “corrected” pronoun translations in a post-editing scenario. The design of the WMT 2016 shared task has been influenced by the design and the results of a 2015 shared task (Hardmeier et al., 2015) organised at the EMNLP workshop on Discourse in MT (DiscoMT). The first intuition about evaluating pronoun translation is to require participants to submit MT systems — possibly with specific strategies for pronoun translation — and to estimate the correctness of the pronouns they output. This estimation, however, cannot be performed with full reliability only by comparing pronouns across candidate and reference translations because this would miss the legitimate variation of certain pronouns, as well as variations in gender or number of the antecedent itself. Human judges are thus required f"
W16-2345,W16-2354,0,0.0469939,"Missing"
W16-2345,H05-1108,0,0.0601982,"Missing"
W16-2345,W14-3334,1,0.800608,"he OTHER class. For the DiscoMT 2015 shared task, we explored this issue for English–French and found that GIZA++ model 4 and HMM with grow-diag-final-and symmetrisation gave the best results. For pronoun– pronoun links, we had an F-score of 0.96, with perfect recall and precision of 0.93 (Hardmeier et al., 2015). This was slightly higher than for other links, which had an F-score of 0.92. For German–English, we explored this issue this year since it is a new language pair. We used an aligned gold standard of 987 sentences from (Pad´o and Lapata, 2005), which has been extensively evaluated by Stymne et al. (2014). We used the same methodology as in 2015, and performed an evaluation on the subset of links between the pronouns we are interested in. We report precision and recall of links both for the pronoun subset and for all links, shown in Table 4. The alignment quality is considerably worse than for French–English both for all links and for pronouns, but again the results for pronouns is better than for all links in both precision and recall. 6 https://github.com/slavpetrov/ universal-pos-tags 530 Alignment Symmetrisation Model 4 fast-align gdfa HMM gd gdf ∪ ∩ All links P R Pronouns P R .75 .69 .80"
W16-2345,W16-2355,1,0.832701,"the test dataset is imbalanced. Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset. In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.). Standard F1 and macro-averaged F1 are also sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail. The UU-S TYMNE systems (Stymne, 2016) use linear SVM classifiers for all language pairs. A number of different features were explored, but anaphora is not explicitly modelled. The features used can be grouped in the following way: source pronouns, local context words/lemmata, preceding nouns, target PoS n-grams with two different PoS tag-sets, dependency heads of pronouns, target LM scores, alignments, and pronoun position. A joint tagger and dependency parser on the source text is used for some of the features. The primary system is a 2-step classifier where a binary classifier is first used to distinguish between the OTHER clas"
W16-2345,petrov-etal-2012-universal,0,0.0937891,"Missing"
W16-2345,W16-2356,1,0.48149,"networks, except for the embedding for the aligned pronoun. All outputs of the recurrent layers are concatenated to a single vector along with the embedding of the aligned pronoun. This vector is then used to make the pronoun prediction by a dense neural network layer. The primary systems are trained to optimise macro-averaged recall and the contrastive systems are optimised without preference towards rare classes. The system is trained only on the shared task data and all parts of the data, in-domain and out-of-domain, are used for training the system. 5.5 5.6 UHELSINKI The UHELSINKI system (Tiedemann, 2016) implements a simple linear classifier based on LibSVM with its L2-loss SVC dual solver. The system applies local source-language and target-language context using the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun tra"
W16-2345,W16-2357,0,0.0259257,"well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features: tokens and their PoS tags are extracted from a context window around source- and targetside pronouns. N -gram combinations of these features are included by concatenating adjacent tokens or PoS tags. Furthermore, the pleonastic use of a pronoun is detected with NADA (Bergsma and Yarowsky, 2011) on the source side. 534 This CRF approach has been applied only to German, but there are plans to extend it to other languages. This indicates that the NN mechanism is quite effective. Th"
W16-2345,sagot-2010-lefff,0,0.0184156,"traction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed seq"
W16-2345,schmid-etal-2004-smor,0,0.0349386,"test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the pr"
W16-2345,W12-3018,0,\N,Missing
W16-2345,2015.iwslt-evaluation.1,1,\N,Missing
W16-2345,W14-6111,0,\N,Missing
W16-2350,W15-2501,1,0.769579,"on the bottom . REPLACE 0 avoir ce leurre de pˆeche au-dessous . ils LM training data: LM test data: It REPLACE ils avoir ce leurre de pˆeche au-dessous . It REPLACE avoir ce leurre de pˆeche au-dessous . Figure 2: Data for the source-aware language model candidates varies because all weights related to antecedent word features and anaphoric link features are shared between all antecedent candidates. The model is trained for 60 epochs on the training data in the IWSLT set; the other training data sets are not used. 3 Source-Aware Language Model In the pronoun prediction task at DiscoMT 2015 (Hardmeier et al., 2015), it turned out that a simple n-gram model considering only the target-side local context of the word to be predicted outperformed all submissions to the shared task. These results suggest that it is important to include strong n-gram modelling capacities into any system. The neural network system described in the previous section does not necessarily have this, so we decided to address this problem with a system combination approach. The official baseline of the current shared task is identical to that of the previous year, but the task is different in that the target language words are provi"
W16-2350,D13-1037,1,0.84788,"an to the anaphora resolution subnetwork. 1 • Substitution of the coreference preprocessing component by CORT (Martschat and Strube, 2015). • Inclusion of target-language context lemma and part-of-speech features. • (Accidental) omission of a hidden layer in the submitted systems. • Substitution of the internal softmax layer (V) by a sigmoid layer. Introduction The primary submission of the UU-Hardmeier system to the pronoun prediction shared task at WMT 2016 (Guillou et al., 2016) consists of two components. The first is a reimplementation of the pronoun prediction neural network proposed by Hardmeier et al. (2013). The other system component is based on a standard n-gram language model over the lemmas of the target side. Apart from implementation details, the main difference between this model and the official baseline provided by the shared task organisers is the integration of information about the pronoun found on the source side, which allows the model to recognise whether a given pronoun was singular or plural in the source. 2 Neural Network Component The first component of our model is a modified reimplementation of the pronoun prediction network introduced by Hardmeier et al. (2013). The main di"
W16-2350,W11-2123,0,0.0614029,"guage model is an n-gram model trained on an artificial corpus generated from the target lemmas of the parallel training (Figure 2). Before every REPLACE tag occurring in the data, we insert the source pronoun aligned to the tag (without lowercasing or any other processing). The alignment information attached to the REPLACE tag in the shared task data files is stripped off. In the training data, we instead add the pronoun class to be predicted. The n-gram model used for this component is a 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with the KenLM toolkit (Heafield, 2011) on the complete set of training data provided for the shared task. To predict classes for an unseen test set, we first convert it to a format matching that of the training data, but with a uniform, unannotated RE PLACE tag used for all classes. We then recover the tag annotated with the correct solution using the disambig tool of the SRILM language modelling toolkit (Stolcke et al., 2011). This tool runs the Viterbi algorithm to select the most probable mapping of each token from among a set of possible alternatives. The map used for this task trivially maps all tokens to themselves with the"
W16-2350,W16-2351,1,0.568517,"Missing"
W16-2350,Q15-1029,0,0.152661,"ons to the WMT 2016 shared task on cross-lingual pronoun prediction. Our model is a system combination of two different approaches, one based on a neural network with latent anaphora resolution and the other one on an n-gram model with an additional dependency on the source pronoun. The combination of the two models results in an improvement over each individual system, but it appears that the contribution of the neural network is more likely due to its context modelling capacities than to the anaphora resolution subnetwork. 1 • Substitution of the coreference preprocessing component by CORT (Martschat and Strube, 2015). • Inclusion of target-language context lemma and part-of-speech features. • (Accidental) omission of a hidden layer in the submitted systems. • Substitution of the internal softmax layer (V) by a sigmoid layer. Introduction The primary submission of the UU-Hardmeier system to the pronoun prediction shared task at WMT 2016 (Guillou et al., 2016) consists of two components. The first is a reimplementation of the pronoun prediction neural network proposed by Hardmeier et al. (2013). The other system component is based on a standard n-gram language model over the lemmas of the target side. Apart"
W16-2351,W15-2501,1,0.875632,"of a n-gram language model that operates over target-language lemmas, but also has access to the identity of the source-language pronouns. Source-aware language models are also provided for the other tasks: English-to-German, Germanto-English and French-to-English. 2 Previous Work (1) John arrived late. [This/it] annoyed Mary. Work on pronoun translation, in which a complete machine translation pipeline is provided, has also considered different functions of the pronoun “it”. Le Nagard and Koehn (2010) identify and exclude instances of pleonastic “it” in their Englishto-French system. Guillou (2015) distinguishes between anaphoric vs. non-anaphoric pronouns in an English-to-French automatic post-editing system. Nov´ak et al. (2013) consider the translation of three different uses of “it” in Englishto-Czech translation: referential it, referring to a noun phrase, anaphoric it, referring to a verb phrase, and pleonastic it. These three categories correspond to those that we refer to as anaphoric, event reference and pleonastic, respectively. Work by Navarretta (2004) and Dipper et al. (2011) has focused on resolving abstract anaphora in Danish and on the manual annotation of abstract anaph"
W16-2351,Q13-1034,0,0.0764328,"Missing"
W16-2351,W16-2350,1,0.527781,"instance of “it” depending on its function. For example, anaphoric “it” may be translated with the third-person singular pronouns “il” [masc.] and “elle” [fem.], or with an nongendered demonstrative such as “cela”. The French pronoun “ce” may function as both an event reference and a pleonastic pronoun, but “il” is used only as a pleonastic pronoun. As revealed in an analysis of the systems submitted to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015a), the translation of pleonastic and event reference pronouns poses a particular problem for MT systems (Guillou and Hardmeier, 2016). Poor performance may be attributed to the inability of the systems to disambiguate the various possible functions of the pronoun “it”. In the case of systems that incorporate coreference resolution and methods for identifying instances of pleonastic “it”, inaccurate output may harm translation performance. No suitable tools exist for the detection of event reference pronouns in English. To address the problem of disambiguating the function of “it”, we propose a classifier that uses information from the current and previous sentences, as well as external tools, and indicates for each instance"
W16-2351,L16-1100,1,0.802301,"nslating an instance of “it” depending on its function. For example, anaphoric “it” may be translated with the third-person singular pronouns “il” [masc.] and “elle” [fem.], or with an nongendered demonstrative such as “cela”. The French pronoun “ce” may function as both an event reference and a pleonastic pronoun, but “il” is used only as a pleonastic pronoun. As revealed in an analysis of the systems submitted to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015a), the translation of pleonastic and event reference pronouns poses a particular problem for MT systems (Guillou and Hardmeier, 2016). Poor performance may be attributed to the inability of the systems to disambiguate the various possible functions of the pronoun “it”. In the case of systems that incorporate coreference resolution and methods for identifying instances of pleonastic “it”, inaccurate output may harm translation performance. No suitable tools exist for the detection of event reference pronouns in English. To address the problem of disambiguating the function of “it”, we propose a classifier that uses information from the current and previous sentences, as well as external tools, and indicates for each instance"
W16-2351,W11-2123,0,0.384016,"t (Princeton University, 2010) contains 26 types of nouns), but this had no effect. The feature combination of noun and adjectives to the left or right also had no effect. 7. Presence of ‘that’ complement in previous sentence. A binary feature which follows Navarretta (2004)’s conclusion (for Danish) that a particular demonstrative pronoun (dette) is often used to refer to the last mentioned situation in the previous sentence, often expressed in a subordinated clause. 3.3 Results For development and comparison we built two different baselines. One is a 3-gram language model built using KenLM (Heafield, 2011) and trained over a modified version of the annotated corpus in which every it is concatenated with its type (e.g. it event). For testing, the it position is filled with each of the three it label and the language model is queried. This baseline functions in a very similar way to the share-task own baseline. Table 2 presents the results of this baseline using 14-fold cross-validation and a single held-out test set (all test-set mentions refer to the same test set). The motivation for the choice of the number of folds is threefold. First, we wanted to respect document boundaries; second, we aim"
W16-2351,guillou-etal-2014-parcor,1,0.911178,"ase of systems that incorporate coreference resolution and methods for identifying instances of pleonastic “it”, inaccurate output may harm translation performance. No suitable tools exist for the detection of event reference pronouns in English. To address the problem of disambiguating the function of “it”, we propose a classifier that uses information from the current and previous sentences, as well as external tools, and indicates for each instance of “it” whether the pronoun function is anaphoric, pleonastic or event reference. The classifier was trained using data from the ParCor corpus (Guillou et al., 2014) and the DiscoMT2015.test dataset (Hardmeier et al., 2016). In both corpora, pronouns are labelled according to their function, following the ParCor annotation scheme. The classifier is incorporated We present our systems for the WMT 2016 shared task on cross-lingual pronoun prediction. The main contribution is a classifier used to determine whether an instance of the ambiguous English pronoun “it” functions as an anaphoric, pleonastic or event reference pronoun. For the English-to-French task the classifier is incorporated in an extended baseline, which takes the form of a source-aware langua"
W16-2351,W10-1737,0,0.438141,"Missing"
W16-2351,W11-1902,0,0.0666122,"gories correspond to those that we refer to as anaphoric, event reference and pleonastic, respectively. Work by Navarretta (2004) and Dipper et al. (2011) has focused on resolving abstract anaphora in Danish and on the manual annotation of abstract anaphora in English and German. Abstract anaphora, in which pronouns refer to abstract entities such as facts or events, is referred to as event reference in this paper. The automatic detection of instances of pleonastic “it” has been addressed by NADA (Bergsma and Yarowsky, 2011), and also by the Stanford sieve-based coreference resolution system (Lee et al., 2011). The cross-lingual pronoun prediction task formalised by Hardmeier (2014) was first introduced as a shared task at DiscoMT 2015 (Hardmeier et al., 2015a). The participants used a range of features in their classifiers, but this paper marks the first attempt to incorporate a component to disambiguate the various uses of “it”. 3 3.1 To increase the number of training examples, instances of event reference “this” and “that” are replaced with “it” and added to the training data. The data was divided into 1504 instances for training, and 501 each for the development and test sets. All sentences we"
W16-2351,W15-2509,1,0.8636,"he form of a n-gram language model that operates over target-language lemmas, but also has access to the identity of the source-language pronouns. Source-aware language models are also provided for the other tasks: English-to-German, Germanto-English and French-to-English. 2 Previous Work (1) John arrived late. [This/it] annoyed Mary. Work on pronoun translation, in which a complete machine translation pipeline is provided, has also considered different functions of the pronoun “it”. Le Nagard and Koehn (2010) identify and exclude instances of pleonastic “it” in their Englishto-French system. Guillou (2015) distinguishes between anaphoric vs. non-anaphoric pronouns in an English-to-French automatic post-editing system. Nov´ak et al. (2013) consider the translation of three different uses of “it” in Englishto-Czech translation: referential it, referring to a noun phrase, anaphoric it, referring to a verb phrase, and pleonastic it. These three categories correspond to those that we refer to as anaphoric, event reference and pleonastic, respectively. Work by Navarretta (2004) and Dipper et al. (2011) has focused on resolving abstract anaphora in Danish and on the manual annotation of abstract anaph"
W16-2351,W15-2512,1,0.893597,"Missing"
W16-2351,loaiciga-etal-2014-english,1,0.868572,"Missing"
W16-2351,N03-5008,0,0.0586973,"ining examples, instances of event reference “this” and “that” are replaced with “it” and added to the training data. The data was divided into 1504 instances for training, and 501 each for the development and test sets. All sentences were shuffled before the corpus was divided, promoting a balanced distribution of the classes (Table 1). Data Set Training Dev Test Total itEvent 504 157 169 830 Anaphoric 779 252 270 1301 Pleonastic 221 92 62 375 Total 1504 501 501 2506 Table 1: Distribution of classes in the training data All classifiers were trained using the Stanford Maximum Entropy package (Manning and Klein, 2003). 3.2 Features To parse the corpus, we used the joint part-ofspeech tagger and dependency parser of Bohnet et al. (2013) from the Mate toolkit. We used the pre-trained models for English that are available online2 . In addition, the corpus was lemmatised using the TreeTagger lemmatiser (Schmid, 1994). Although other tools were used, we relied on the output of these two parsers to extract most of our features. For each training example, we extract the following information: Disambiguating “it” 1. Previous three tokens. This includes words and punctuation. It also includes the tokens in the prev"
W16-2351,W12-3018,0,0.157594,"Missing"
W16-2351,C04-1034,0,0.817682,"he pronoun “it”. Le Nagard and Koehn (2010) identify and exclude instances of pleonastic “it” in their Englishto-French system. Guillou (2015) distinguishes between anaphoric vs. non-anaphoric pronouns in an English-to-French automatic post-editing system. Nov´ak et al. (2013) consider the translation of three different uses of “it” in Englishto-Czech translation: referential it, referring to a noun phrase, anaphoric it, referring to a verb phrase, and pleonastic it. These three categories correspond to those that we refer to as anaphoric, event reference and pleonastic, respectively. Work by Navarretta (2004) and Dipper et al. (2011) has focused on resolving abstract anaphora in Danish and on the manual annotation of abstract anaphora in English and German. Abstract anaphora, in which pronouns refer to abstract entities such as facts or events, is referred to as event reference in this paper. The automatic detection of instances of pleonastic “it” has been addressed by NADA (Bergsma and Yarowsky, 2011), and also by the Stanford sieve-based coreference resolution system (Lee et al., 2011). The cross-lingual pronoun prediction task formalised by Hardmeier (2014) was first introduced as a shared task"
W16-2351,W13-3307,0,0.185386,"Missing"
W16-2351,W04-0713,0,\N,Missing
W16-2351,W16-2345,1,\N,Missing
W16-3414,W08-0312,0,0.0244613,", one can generate longer sentences within a specific genre where the translation quality is known to be higher, and shorter sentences in other more difficult genres. This will generate higher overall BLEU scores due to the fact that the brevity penalty works on whole documents rather than sentence-by-sentence, but the final translation quality would clearly have been higher if combined systems had been used, each optimised for a particular genre. Despite these and other issues, however, BLEU has been shown to correlate extremely well with human judgement of translation quality in many cases (Agarwal and Lavie, 2008; Farr´us et al., 2012). There have been a lot of recent efforts to develop more sophisticated metrics that counteract some of BLEU’s weaknesses (Mach´acˇ ek and Bojar, 2013), but for the time being it remains ubiquitous in SMT. For this reason, the computation of oracle BLEU hypotheses is an active field (Wisniewski et al., 2010; Sokolov et al., 2012). Oracle BLEU hypotheses are those in the search space of a PBSMT decoder with the highest BLEU scores. Ultimately we want our translation systems to find these hypotheses on unseen data; calculating them when a reference is available can help id"
W16-3414,E06-1032,0,0.471519,"fficult to take recall into account, which could lead to short sentences scoring unfairly highly. To prevent this from occurring, a brevity penalty is introduced, lowering the BLEU score for cases where the length of the candidate translation c is less than the length of the reference translation r. The equation for BLEU is as follows: ! N X log pn (1) BLEU = min (exp (1 − r/c) , 1) · exp N n=1 Obvious problems with BLEU are that it gives all words equal weighting and harshly punishes synonyms and elaborations, as well as words such as ‘thus’ or ‘however’ spliced occasionally into a text (see Callison-Burch et al. (2006) for a full discussion of these shortcomings). Chiang et al. (2008) meanwhile describe several situations where they are able to obtain highly dubious improvements in BLEU. They point out, for example, that if translating multiple genres at the same time, one can generate longer sentences within a specific genre where the translation quality is known to be higher, and shorter sentences in other more difficult genres. This will generate higher overall BLEU scores due to the fact that the brevity penalty works on whole documents rather than sentence-by-sentence, but the final translation quality"
W16-3414,D08-1064,0,0.300926,"oring unfairly highly. To prevent this from occurring, a brevity penalty is introduced, lowering the BLEU score for cases where the length of the candidate translation c is less than the length of the reference translation r. The equation for BLEU is as follows: ! N X log pn (1) BLEU = min (exp (1 − r/c) , 1) · exp N n=1 Obvious problems with BLEU are that it gives all words equal weighting and harshly punishes synonyms and elaborations, as well as words such as ‘thus’ or ‘however’ spliced occasionally into a text (see Callison-Burch et al. (2006) for a full discussion of these shortcomings). Chiang et al. (2008) meanwhile describe several situations where they are able to obtain highly dubious improvements in BLEU. They point out, for example, that if translating multiple genres at the same time, one can generate longer sentences within a specific genre where the translation quality is known to be higher, and shorter sentences in other more difficult genres. This will generate higher overall BLEU scores due to the fact that the brevity penalty works on whole documents rather than sentence-by-sentence, but the final translation quality would clearly have been higher if combined systems had been used,"
W16-3414,D12-1108,1,0.853092,"-gram language models and distortion cost are implemented in Docent, along with documentlevel models including a length parity model, a semantic language model and several readability models. The initial translation can be created either by generating a random segmentation and taking random translations from the phrase table in monotonic order, or by a run from Moses. Docent is not designed to perform better than Moses when only sentence-level features are used; its advantage lies in the ability to use features that disable recombination. Information about Docent’s performance can be found in Hardmeier et al. (2012). 3.1 BLEU decoding BLEU decoding is the name we have given to a particular mode of decoding in Docent whereby proposed changes to the translation are only accepted by the decoder if they result in an increase in the BLEU score. A new feature model, BleuModel, was implemented in Docent. Before decoding begins, BleuModel processes and stores the lengths of the reference translations, as well as the lengths of individual sentences within those translations and n-gram counts for 1 ≤ n ≤ 4. Once an initial candidate translation for each document has been created, BleuModel calculates the BLEU scor"
W16-3414,P13-4033,1,0.893529,"). Oracle BLEU hypotheses are those in the search space of a PBSMT decoder with the highest BLEU scores. Ultimately we want our translation systems to find these hypotheses on unseen data; calculating them when a reference is available can help identify deficiencies in current systems and facilitate the development of new techniques. BLEU oracles are also useful during feature-weight tuning, Climbing Mount BLEU 271 though it has been pointed out that relying too heavily on BLEU here can lead to poor results (Liang et al., 2006; Chiang, 2012). 3 Docent Docent is a decoder for phrase-based SMT (Hardmeier et al., 2013). In Docent’s search algorithm, feature models have access to a complete translation of a whole document at all stages of the search process. The algorithm is a stochastic variant of standard hill climbing: at each step, the decoder generates a successor of the current translation by randomly applying one of a set of state-changing operations at a random location in the document, and accepts the new translation only if it has a better score than the previous translation. Implemented operations include changing the translation of a phrase, changing the word order by swapping the positions of tw"
W16-3414,D11-1125,0,0.0160447,"produced independently of BLEU, then BLEU is often a good metric to distinguish their quality; however this 280 Smith et al. does not imply that actively looking for translations with high BLEU score will result in high quality. There is clearly a high-BLEU area of the search space with low quality translations. This problem has previously been encountered by researchers working on feature-weight tuning (Liang et al., 2006; Chiang, 2012). Searching for weights that produce high BLEU scores on development data is a central part of many standard tuning algorithms such as MERT (Och, 2003), PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007). In reality feature models are selected in such a way that ending up in this strange region of the search space is unlikely, but if we blindly optimise feature weights using BLEU, we could run the risk of moving dangerously close. Despite these problems, BLEU decoding may still have the potential to be applied during tuning to improve translation quality. We have seen that in its pure form, BLEU decoding leads us far from the area of the search space containing good translations, and optimising our models towards finding these regions is unlikely to be a good"
W16-3414,P06-1096,0,0.0414745,"Missing"
W16-3414,W13-2202,0,0.144538,"enerate higher overall BLEU scores due to the fact that the brevity penalty works on whole documents rather than sentence-by-sentence, but the final translation quality would clearly have been higher if combined systems had been used, each optimised for a particular genre. Despite these and other issues, however, BLEU has been shown to correlate extremely well with human judgement of translation quality in many cases (Agarwal and Lavie, 2008; Farr´us et al., 2012). There have been a lot of recent efforts to develop more sophisticated metrics that counteract some of BLEU’s weaknesses (Mach´acˇ ek and Bojar, 2013), but for the time being it remains ubiquitous in SMT. For this reason, the computation of oracle BLEU hypotheses is an active field (Wisniewski et al., 2010; Sokolov et al., 2012). Oracle BLEU hypotheses are those in the search space of a PBSMT decoder with the highest BLEU scores. Ultimately we want our translation systems to find these hypotheses on unseen data; calculating them when a reference is available can help identify deficiencies in current systems and facilitate the development of new techniques. BLEU oracles are also useful during feature-weight tuning, Climbing Mount BLEU 271 th"
W16-3414,P03-1021,0,0.0167339,"translations are produced independently of BLEU, then BLEU is often a good metric to distinguish their quality; however this 280 Smith et al. does not imply that actively looking for translations with high BLEU score will result in high quality. There is clearly a high-BLEU area of the search space with low quality translations. This problem has previously been encountered by researchers working on feature-weight tuning (Liang et al., 2006; Chiang, 2012). Searching for weights that produce high BLEU scores on development data is a central part of many standard tuning algorithms such as MERT (Och, 2003), PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007). In reality feature models are selected in such a way that ending up in this strange region of the search space is unlikely, but if we blindly optimise feature weights using BLEU, we could run the risk of moving dangerously close. Despite these problems, BLEU decoding may still have the potential to be applied during tuning to improve translation quality. We have seen that in its pure form, BLEU decoding leads us far from the area of the search space containing good translations, and optimising our models towards finding these regi"
W16-3414,P02-1040,0,0.0981912,"nce translation exactly, are rarely possible; meanwhile we add to the extensive literature on problems and biases with the BLEU metric itself, showing for the first time clear examples of sentences from documents with high BLEU scores with obvious poor translation quality. The paper is structured in the following manner: Section 2 describes the BLEU metric, Section 3 presents the Docent decoder and BLEU decoding, Section 4 details 270 Smith et al. experiments carried out with BLEU decoding and presents their results, while Section 5 comprises a discussion. 2 BLEU The BLEU score, introduced by Papineni et al. (2002), is a metric for evaluating the quality of a candidate translation by comparing it to one or more reference translations. For 1 ≤ n ≤ N , where normally N = 4, each n-gram in each candidate sentence is checked against all of the references in order to calculate precision. To count towards precision, the candidate n-gram need only appear in one of the references; this helps to account for possible variations in style and word choice. However, the same n-gram appearing more than once in the candidate is only counted multiple times if it also appears multiple times in a single reference. BLEU is"
W16-3414,E12-1013,0,0.01791,"arly have been higher if combined systems had been used, each optimised for a particular genre. Despite these and other issues, however, BLEU has been shown to correlate extremely well with human judgement of translation quality in many cases (Agarwal and Lavie, 2008; Farr´us et al., 2012). There have been a lot of recent efforts to develop more sophisticated metrics that counteract some of BLEU’s weaknesses (Mach´acˇ ek and Bojar, 2013), but for the time being it remains ubiquitous in SMT. For this reason, the computation of oracle BLEU hypotheses is an active field (Wisniewski et al., 2010; Sokolov et al., 2012). Oracle BLEU hypotheses are those in the search space of a PBSMT decoder with the highest BLEU scores. Ultimately we want our translation systems to find these hypotheses on unseen data; calculating them when a reference is available can help identify deficiencies in current systems and facilitate the development of new techniques. BLEU oracles are also useful during feature-weight tuning, Climbing Mount BLEU 271 though it has been pointed out that relying too heavily on BLEU here can lead to poor results (Liang et al., 2006; Chiang, 2012). 3 Docent Docent is a decoder for phrase-based SMT (H"
W16-3414,D07-1080,0,0.0294765,"then BLEU is often a good metric to distinguish their quality; however this 280 Smith et al. does not imply that actively looking for translations with high BLEU score will result in high quality. There is clearly a high-BLEU area of the search space with low quality translations. This problem has previously been encountered by researchers working on feature-weight tuning (Liang et al., 2006; Chiang, 2012). Searching for weights that produce high BLEU scores on development data is a central part of many standard tuning algorithms such as MERT (Och, 2003), PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007). In reality feature models are selected in such a way that ending up in this strange region of the search space is unlikely, but if we blindly optimise feature weights using BLEU, we could run the risk of moving dangerously close. Despite these problems, BLEU decoding may still have the potential to be applied during tuning to improve translation quality. We have seen that in its pure form, BLEU decoding leads us far from the area of the search space containing good translations, and optimising our models towards finding these regions is unlikely to be a good idea. However, by combining BLEU"
W16-3414,D10-1091,0,0.0192151,"slation quality would clearly have been higher if combined systems had been used, each optimised for a particular genre. Despite these and other issues, however, BLEU has been shown to correlate extremely well with human judgement of translation quality in many cases (Agarwal and Lavie, 2008; Farr´us et al., 2012). There have been a lot of recent efforts to develop more sophisticated metrics that counteract some of BLEU’s weaknesses (Mach´acˇ ek and Bojar, 2013), but for the time being it remains ubiquitous in SMT. For this reason, the computation of oracle BLEU hypotheses is an active field (Wisniewski et al., 2010; Sokolov et al., 2012). Oracle BLEU hypotheses are those in the search space of a PBSMT decoder with the highest BLEU scores. Ultimately we want our translation systems to find these hypotheses on unseen data; calculating them when a reference is available can help identify deficiencies in current systems and facilitate the development of new techniques. BLEU oracles are also useful during feature-weight tuning, Climbing Mount BLEU 271 though it has been pointed out that relying too heavily on BLEU here can lead to poor results (Liang et al., 2006; Chiang, 2012). 3 Docent Docent is a decoder"
W16-3414,P10-1049,0,0.0304864,"Missing"
W16-3418,L16-1100,1,0.849493,"syntactic variability in pronoun translation is generally high even in closely parallel texts, which creates difficulties both for translation modelling and for MT evaluation. We hope that our contribution will make it easier for MT researchers to anchor their decisions in descriptive corpus data and face the full complexity of pronoun translation. 2 The PROTEST Pronoun Evaluation Test Suite To address the problem of evaluation, Hardmeier (2015) suggests using a test suite composed of carefully selected pronoun tokens which can then be checked individually to evaluate pronoun correctness. In Guillou and Hardmeier (2016) we introduce PROTEST, a test suite comprising 250 hand-selected pronoun tokens exposing particular problems in English-French pronoun translation, along with an automatic evaluation script. The pronoun analysis tool and methodology presented here are specifically designed to be used with the PROTEST test suite. They can be applied to any parallel corpus with (manual or automatic) coreference resolution and word alignments, although pro-drop languages might require changes to the guidelines. The pronoun tokens in PROTEST are extracted from the DiscoMT2015.test dataset (Hardmeier et al., 2016),"
W16-3418,W15-2503,1,0.849195,"instructed to evaluate the translation of the clause instead of the pronoun in isolation. Both annotators marked the translation as correct, which one might expect given that the French translation is taken from the reference. Examples such as this present a problem for both manual and automated evaluation of pronoun translation in MT, which until now has considered pronoun translation at the token level. 328 6 Hardmeier and Guillou Related Work The PROTEST pronoun analysis tool shares some similarities with the interface for the pronoun selection task (Hardmeier, 2014) which has been used by Guillou and Webber (2015) and in the manual evaluation of the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015). In the pronoun selection task, pronouns in the sourcelanguage text are highlighted and their corresponding translations in the MT output are replaced with a placeholder. The role of the human annotator is to select, from a given list of options, which pronoun should replace the placeholder. In this way, the annotator is not biased by the pronoun translation in the MT output. In contrast, our tool presents the annotator with the translation of the pronoun in context and poses questions"
W16-3418,guillou-etal-2014-parcor,1,0.833499,"posing particular problems in English-French pronoun translation, along with an automatic evaluation script. The pronoun analysis tool and methodology presented here are specifically designed to be used with the PROTEST test suite. They can be applied to any parallel corpus with (manual or automatic) coreference resolution and word alignments, although pro-drop languages might require changes to the guidelines. The pronoun tokens in PROTEST are extracted from the DiscoMT2015.test dataset (Hardmeier et al., 2016), which has been manually annotated according to the ParCor annotation guidelines (Guillou et al., 2014). The pronoun tokens are categorised according to a range of different problems that MT systems face when translating pronouns. At the top level the categories capture pronoun function, with four different functions represented in the test suite3 (Fig. 1). Anaphoric pronouns refer to an antecedent. Pleonastic pronouns, in contrast, do not refer to anything. Event reference pronouns refer to a verb, verb phrase, clause or even an entire sentence. Finally, addressee reference pronouns are used to refer to the reader/audience. At a second level of classification, we distinguish other features lik"
W16-3418,W15-2522,1,0.760257,"from the reference translation, a different pronoun may be required. One that matches the reference translation may in fact be wrong. In less complex cases, too, the syntactic variability in pronoun translation is generally high even in closely parallel texts, which creates difficulties both for translation modelling and for MT evaluation. We hope that our contribution will make it easier for MT researchers to anchor their decisions in descriptive corpus data and face the full complexity of pronoun translation. 2 The PROTEST Pronoun Evaluation Test Suite To address the problem of evaluation, Hardmeier (2015) suggests using a test suite composed of carefully selected pronoun tokens which can then be checked individually to evaluate pronoun correctness. In Guillou and Hardmeier (2016) we introduce PROTEST, a test suite comprising 250 hand-selected pronoun tokens exposing particular problems in English-French pronoun translation, along with an automatic evaluation script. The pronoun analysis tool and methodology presented here are specifically designed to be used with the PROTEST test suite. They can be applied to any parallel corpus with (manual or automatic) coreference resolution and word alignm"
W16-3418,2010.iwslt-papers.10,1,0.900957,"graphical pronoun analysis tool and its accompanying annotation guidelines. By encouraging the manual examination and evaluation of individual pronoun tokens, we hope to understand better how well MT systems perform when translating different categories of pronouns, and gain insights as to where MT systems perform poorly and why. Keywords: Evaluation, machine translation, pronouns, graphical interface, manual annotation 1 Introduction Pronoun translation poses a problem for statistical machine translation (SMT). Despite recent efforts, little progress has been made (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Novák, 2011; Guillou, 2012; Hardmeier, 2014). Most recently, the results of the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) revealed that even discourse-aware Machine Translation (MT) systems were unable to beat a simple phrase-based SMT baseline. We believe that there are two important obstacles that currently limit progress in pronoun translation. Firstly, we need to obtain a deeper understanding of the problems that MT systems face when translating pronouns, and of the performance of our systems when faced with these problems. Secondly, we lack evaluation meth"
W16-3418,W15-2501,1,0.925222,"we hope to understand better how well MT systems perform when translating different categories of pronouns, and gain insights as to where MT systems perform poorly and why. Keywords: Evaluation, machine translation, pronouns, graphical interface, manual annotation 1 Introduction Pronoun translation poses a problem for statistical machine translation (SMT). Despite recent efforts, little progress has been made (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Novák, 2011; Guillou, 2012; Hardmeier, 2014). Most recently, the results of the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) revealed that even discourse-aware Machine Translation (MT) systems were unable to beat a simple phrase-based SMT baseline. We believe that there are two important obstacles that currently limit progress in pronoun translation. Firstly, we need to obtain a deeper understanding of the problems that MT systems face when translating pronouns, and of the performance of our systems when faced with these problems. Secondly, we lack evaluation methodologies that specifically target pronoun translation and that are capable of providing a detailed overview of system performance. In this paper, we pres"
W16-3418,P07-2045,0,0.00621957,"Missing"
W16-3418,W10-1737,0,0.060278,"Missing"
W16-3418,P02-1040,0,0.0945131,"these problems. Secondly, we lack evaluation methodologies that specifically target pronoun translation and that are capable of providing a detailed overview of system performance. In this paper, we present a graphical tool and an evaluation methodology for manual assessment and investigation of pronoun translation that address both of these factors. A Graphical Pronoun Analysis Tool for the PROTEST Test Suite 319 When dealing with pronouns, many of the fundamental assumptions cherished by the MT community break down. MT researchers routinely rely on automatic evaluation metrics such as BLEU (Papineni et al., 2002) to guide their development efforts. These automated metrics typically assume that overlap of the MT output with a humangenerated reference translation may be used as a proxy for correctness. This assumption fails for certain types of pronouns. In particular, it does not hold in the important case of anaphoric pronouns, which refer back to a mention introduced earlier in the discourse (an antecedent): If the pronoun’s antecedent is translated in a way that differs from the reference translation, a different pronoun may be required. One that matches the reference translation may in fact be wron"
W16-3418,P11-4010,0,0.025198,"l presents the annotator with the translation of the pronoun in context and poses questions about its translation. Furthermore, the pronoun analysis tool is not just an annotation interface. It enables researchers to examine translations in detail and to browse and compare translations by different systems, and annotations by one or more annotators. In spirit, the tool is similar to other user interfaces for manual data inspection such as the analysis.perl utility for BLEU score analysis distributed with Moses (Koehn et al., 2007) or the Blast interface for manual error analysis in MT output (Stymne, 2011). Our tool is novel in that it focuses on a specific linguistic problem in translation and links manual inspection and evaluation with a manually selected test suite and the possibility of feeding back the annotations into a semi-automatic evaluation process. The underlying approach of the automatic evaluation script included as part of PROTEST is similar in its methodology to the ACT metric for assessing the translation of discourse connectives (Hajlaoui and Popescu-Belis, 2013). Like PROTEST, ACT attempts to match translations in the MT output with those in the reference translation and refe"
W16-3418,E12-3001,1,\N,Missing
W17-4801,D12-1133,0,0.165034,"16 (Guillou et al., 2016), but the differences in the resulting evaluation scores are actually minor. As we have explained above, the shared task focused primarily on subject pronouns. However, in English and German, some pronouns are ambiguous between subject and object position, e.g., the English it and the German es and sie. In order to address this issue, in 2016 we introduced filtering of object pronouns based on dependency parsing. This filtering removed all pronoun instances that did not have a subject dependency label.6 For joint dependency parsing and POS-tagging, we used Mate Tools (Bohnet and Nivre, 2012), with default models. Since in 2016 we found that this filtering was very accurate, this year we performed only automatic filtering for the training and the development, and also for the test datasets. Note that since only subject pronouns can be realized as prodropped pronouns in Spanish, subject filtering was not necessary. 4 Baseline Systems The baseline system is based on an n-gram language model (LM). The architecture is the same as that used for the WMT 2016 cross-lingual pronoun prediction task.7 In 2016, most systems outperformed this baseline, and for the sake of comparison, we thoug"
W17-4801,W16-2350,1,0.857196,"Missing"
W17-4801,W17-4807,1,0.863641,"Missing"
W17-4801,2010.iwslt-papers.10,1,0.907647,"is is hard as selecting the correct pronoun may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-au"
W17-4801,W15-2501,1,0.855394,"gradually raised interest in the research community for a shared task that would allow to compare various competing proposals and to quantify the extent to which they improve the translation of different pronouns for different language pairs and different translation directions. However, evaluating pronoun translation comes with its own challenges, as reference-based evaluation, which is standard for machine translation in general, cannot easily take into account legitimate variations of translated pronouns or their placement in the sentence. Thus, building upon experience from DiscoMT 2015 (Hardmeier et al., 2015) and WMT 2016 (Guillou et al., 2016), this year’s cross-lingual pronoun prediction shared task has been designed to test the capacity of the participating systems for translating pronouns correctly, in a framework that allows for objective evaluation, as we will explain below. 2 ce OTHER ce|PRON qui|PRON It ’s an idiotic debate . It has to stop . REPLACE 0 eˆ tre|VER un|DET d´ebat|NOM idiot|ADJ REPLACE 6 devoir|VER stopper|VER .|. 0-0 1-1 2-2 3-4 4-3 6-5 7-6 8-6 9-7 10-8 Figure 2: English→French example from the development dataset. First come the gold class labels, followed by the pronouns (t"
W17-4801,N13-1073,0,0.0460377,"raka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED talks, one for each subtask. However, we chose the current setup as using a smaller set of documents r"
W17-4801,W17-4806,0,0.0618776,"ial anaphora) or in different sentences (inter-sentential anaphora). Most MT systems translate sentences in isolation, and thus inter-sentential anaphoric pronouns will be translated without knowledge of their antecedent, and thus pronoun-antecedent agreement cannot be guaranteed. NMT yields generally higher-quality translation, but is harder to analyze, and thus little is known about how well it handles pronoun translation. Yet, it is clear that it has access to larger context compared to phrase-based SMT models, potentially spanning multiple sentences, which can improve pronoun translation (Jean et al., 2017a). Motivated by these challenges, the DiscoMT 2017 workshop on Discourse in Machine Translation offered a shared task on cross-lingual pronoun prediction. This was a classification task, asking the participants to make predictions about which pronoun should replace a placeholder in the target-language text. The task required no MT expertise and was designed to be interesting as a machine learning task on its own right, e.g., for researchers working on co-reference resolution. Source Target POS tags Reference The above constraints start playing a role in pronoun translation in situations where"
W17-4801,E12-3001,0,0.0230275,"may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the sou"
W17-4801,2005.mtsummit-papers.11,0,0.0869283,"teresting research challenges from the perspective of both speech recognition and machine translation. Therefore, both research communities are making increased use of them in building benchmarks. TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. 3.1.2 Europarl and News For training purposes, in addition to TED talks, we further made available the Europarl3 (Koehn, 2005) and News Commentary4 corpora for all language pairs but Spanish-English, for which only TED talks and Europarl were available. We used the alignments provided by OPUS, including the document boundaries from the original sources. For Europarl, we used ver. 7 of the data release, and for News Commentary we used ver. 9. 3.2 Test Set Selection We selected the test data from talks added recently to the TED repository such that: 1. The talks have been transcribed (in English) and translated into both German and French. 2. They were not used in the IWSLT evaluation campaigns, nor in the DiscoMT 2015"
W17-4801,2005.iwslt-1.8,0,0.121419,"OS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English dir"
W17-4801,W10-1737,0,0.13529,"Missing"
W17-4801,J03-1002,0,0.0103978,"rted the TreeTagger’s POS tags to the target coarse POS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction,"
W17-4801,P14-2050,0,0.026726,"that is aligned to the pronoun to be predicted. All input sequences are fed in an embedding layer followed by two layers of GRUs. The values in the last layer form a vector, which is further concatenated to the pronoun alignment embeddings, to form a larger vector, which is then used to make the final prediction using a dense neural network. The pretraining is a modification of the skip-gram model of WORD 2 VEC (Mikolov et al., 2013), in which along with the skip-gram token context, all target sentence pronouns are predicted as well. The process of pretraining is performed using WORD 2 VECF (Levy and Goldberg, 2014). 5.2 NYU The NYU system (Jean et al., 2017b) uses an attention-based neural machine translation model and three variants that incorporate information from the preceding source sentence. The sentence is added as an auxiliary input using additional encoder and attention models. The systems are not specifically designed for pronoun prediction and may be used to generate complete sentence translations. They are trained exclusively on the data provided for the task, using the text only and ignoring the provided POS tags and alignments. 5.4 UU-Hardmeier The UU- HARDMEIER system (Hardmeier, 2017) is"
W17-4801,petrov-etal-2012-universal,0,0.0455296,"Missing"
W17-4801,W16-2351,1,0.891541,"Missing"
W17-4801,S17-2088,1,0.872336,"Missing"
W17-4801,W16-2202,0,0.0276957,"Missing"
W17-4801,D15-1166,0,0.0502002,"nt. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the targetlanguage lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of cl"
W17-4801,W16-2353,0,0.0406223,"/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a different tag: EP. Still, we decided to use the same filtering this year, to keep the task sta"
W17-4801,L16-1680,0,0.054481,"Missing"
W17-4801,W17-4808,0,0.0205662,"mata constructed from news texts, parliament debates, and the TED talks of the training/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a differe"
W17-4801,W16-2355,1,0.796565,"the data is used in each epoch. For the primary system, all classes are sampled equally, as long as there are enough instances for each class. Although this sampling method biases the system towards macro-averaged recall, on the test data the system performed very well in terms of both macro-averaged recall and accuracy. The secondary system uses a sampling method in which the samples are proportional to the class distribution in the development dataset. 5.5 UU-Stymne16 The UU-S TYMNE 16 system uses linear SVM classifiers, and it is the same system that was submitted for the 2016 shared task (Stymne, 2016). It is based mainly on local features, and anaphora is not explicitly modeled. The features used include source pronouns, local context words/lemmata, target POS n-grams with two different POS tagsets, dependency heads of pronouns, alignments, and position of the pronoun. A joint tagger and dependency parser (Bohnet and Nivre, 2012) is used on the source text in order to produce some of the features. Overall, the source pronouns, the local context and the dependency features performed best across all language pairs. 8 7 Stymne (2016) describes several variations of the method, including both"
W17-4801,W17-4805,1,0.882705,"Missing"
W17-4801,S16-1001,1,0.80372,"erforming system here is T URKU NLP with a macro-averaged recall of 58.82. However, it is nearly tied with U PPSALA, and both are somewhat close to NYU. Noteworthy, though, is that the highest-scoring system on macro-average recall is the contrastive system of NYU; NYU also has the second-best accuracy, outperformed only by U PPSALA. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we followed the setup of 2016, where we switched to macroaveraged recall, which was also recently adopted by some other competitions, e.g., by SemEval2016/2017 Task 4 (Nakov et al., 2016; Rosenthal et al., 2017). Moreover, as in 2015 and 2016, we also report accuracy as a secondary evaluation measure (but we abandon F1 altogether). Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. The advantage of macro-averaged recall over acc"
W17-4801,C96-2141,0,0.453266,"c information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED t"
W17-4807,W16-2350,1,0.812451,"shared task on cross-lingual pronoun prediction. The system is an ensemble of convolutional neural networks combined with a source-aware n-gram language model. 1 Overview For the 2017 cross-lingual pronoun prediction shared task, we chose to create a system that could be implemented very quickly while still providing an interesting comparison to the other systems we expect to participate in the shared task. The core components of our system are a convolutional neural network that evaluates the context of the source and target context of the examples. As in our systems from the previous year (Hardmeier, 2016; Lo´aiciga et al., 2016), we also use a sourceaware n-gram language model as a complementary component. In contrast to 2016, our neural network classifier does not attempt to model pronominal anaphora explicitly. This change was made to simplify the model and avoid the heavyweight preprocessing that our earlier systems required. Instead, we focused on implementing a more sophisticated system combination method that permits the construction of a larger ensemble of models. 2 Convolutional neural network The neural network architecture of our pronoun prediction model is loosely inspired by the w"
W17-4807,2010.iwslt-papers.10,1,0.922282,"Missing"
W17-4807,D13-1037,1,0.779497,"Missing"
W17-4807,W11-2123,0,0.0120601,"guage model is an n-gram model trained on an artificial corpus generated from the target lemmas of the parallel training (Figure 1). Before every REPLACE tag occurring in the data, we insert the source pronoun aligned to the tag (without lowercasing or any other processing). The alignment information attached to the REPLACE tag in the shared task data files is stripped off. In the training data, we instead add the pronoun class to be predicted. The n-gram model used for this component is a 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with the KenLM toolkit (Heafield, 2011) on the complete set of training data provided for the shared task. To predict classes for an unseen test set, we first convert it to a format matching that of the training data, but with a uniform, unannotated REPLACE tag used for all classes. We then recover the tag annotated with the correct solution using the disambig tool of the SRILM language modelling toolkit (Stolcke et al., 2011). This tool runs the Viterbi algo59 Source: Target lemmas: Solution: It ’s got these fishing lures on the bottom . REPLACE 0 avoir ce leurre de pˆeche au-dessous . ils LM training data: LM test data: It REPLAC"
W17-4807,W16-2351,1,0.553373,"Missing"
W17-4807,W17-4801,1,0.863931,"Missing"
W17-4810,C10-1011,0,0.0605544,"Missing"
W17-4810,W13-3303,0,0.126721,"rwarten von Ihren Angestellten, dass sie tun worum Sie sie gebeten haben, wenn sie die Aufgabe ausgef¨uhrt haben, k¨onnen sie Zus¨atzliches tun. is shared across all languages, they may differ considerably in the range of referring expressions. The phenomenon of explicitation in translation is often understood to occur when a translation explicitly realises meanings that were implicit in its source text. In terms of discourse phenomena, this would mean that a source text does not contain linguistic markers that trigger some discourse relations, whereas its translation does, as was analysed by Meyer & Webber (2013) or by Becher (2011b), including also the opposite process of implicitation. In other studies, explicitation is seen if a translated text realises meanings with more explicit means than the source text does. In relation to coreference, some referring expressions can be more explicit than the others, as in example (4) in Section 2 above. For instance, Becher (2011a, p. 98) presents a scale for the explicitness of various referring expressions for the language pair English-German. Most of these studies start from the description of the expressions existing in the language systems they compare, a"
W17-4810,D13-1032,0,0.0221516,"Missing"
W17-4810,W15-3403,0,0.223959,"el corpus, the same type of resource that is typically used for training MT systems. We begin by defining a set of surface patterns that identify the discourse structures of interest and permit their automatic extraction. We then use the word alignments to establish correspondences between the languages. We particularly focus on those cases where there is a relevant pattern in one language, but the word aligner is unable to find a corresponding structure in the other. Such alignment discrepancies can simply be due to alignment errors, but they can also stem from systematic language contrasts (Grishina and Stede, 2015, p. 19–20) or from the phenomena of explicitation and implicitation in the translation process. Our general goal is to explore these alignment discrepancies and analyse their causes. We use a corpus of English-German translations that we automatically annotate for part-of-speech and dependency information. Alignment discrepancies are detected with the help of sentence and word alignment of the annotated structures. Thus we do not use any manually annotated resources, and linguistic knowledge involved is rather of shallow character. Specific cases of extracted discrepancies represented through"
W17-4810,N03-1017,0,0.0212904,"Missing"
W17-4810,postolache-etal-2006-transferring,0,0.0468099,"o detect patterns revealing language contrasts or the phenomena of explicitation/implicitation that we define in form of alignment discrepancies. In a parallel corpus of English-German translations, we use automatic word alignment to extract transformation patterns. Those sentence pairs which contain a discourse structure in either the source or the target sentence and for which no transformation patterns could be extracted are defined as alignment discrepancies. 3 Related Work The method that we use to extract transformation patterns is similar to coreference annotation projection applied by Postolache et al. (2006) and by Grishina & Stede (2015). Both studies use data manually annotated for coreference relations. In our approach, we use automatic annotations only that allow us to define candidate referring expressions – linguistic expressions that are potential members of a coreference chain (not resolved by a human annotator). Postolache et al. (2006) mark patterns containing heads of the resulting referring expression in the target language aligned with heads of the source referring expressions. Although they mention the situations when the source head is not aligned with any target word or no words o"
W17-4810,1995.mtsummit-1.1,0,0.0449413,"Missing"
W18-0711,J95-2003,0,0.514969,"Missing"
W18-0711,guillou-etal-2014-parcor,1,0.834123,"und you. c. You carry a phone. It wouldn’t hurt you to call once in a while. 4 Inspection of Figure 1 suggests a possible interaction whereby the effect of verb type looks stronger in the It condition than in the This condition. The lack of a significant interaction in the model may reflect the fact that the co-reference rate for non-alternating verbs in the This condition is already near ceiling and there may be little room for (measuring) a further increase. 5 Specifically, the ParCorFull corpus includes the datasets The binary outcome of entity/event co-reference used in the ParCor corpus (Guillou et al., 2014), the DiscoMT workshop (Hardmeier et al., 2016) and the test sets from the WMT 2017 shared task (Bojar et al., 2017). 100 passages showed that verbs which permit an agent alternation as either an implicit or explicit argument are more prone to trigger an event co-referent than an entity one. This finding is potentially useful as an additional feature for anaphoricity detection or event mention identification in co-reference resolution systems. Furthermore, we saw a bias towards event coreference for the corpus passages in Study 2 that were known to have yielded event co-reference in their orig"
W18-0711,W05-0406,0,0.0929055,"Missing"
W18-0711,D17-1138,0,0.015485,"hat degree event instances serve as antecedents when a competing entity referent is also available. The goal is to model human choices as a baseline to inform coreference systems. We report two psycholinguistic studies that use a story-continuation task to measure participants’ resolution of pronouns It and This. Improving our understanding of the interpretation of the “difficult” anaphoric cases is a step towards better anaphora and co-reference systems. It has been noted that current systems struggle to identify this type of reference and that anaphoricity determiners have poor performance (Heinzerling et al., 2017). It, This and That are also frequent in dialogue data for which co-reference sysAnaphora resolution systems require both an enumeration of possible candidate antecedents and an identification process of the antecedent. This paper focuses on (i) the impact of the form of referring expression on entity-vsevent preferences and (ii) how properties of the passage interact with referential form. Two crowd-sourced story-continuation experiments were conducted, using constructed and naturally-occurring passages, to see how participants interpret It and This pronouns following a context sentence that"
W18-0711,L18-1065,1,0.798524,"icipants 4.3 Procedure, annotation, and analysis The procedure was identical to that in Study 1. The annotation followed that described for Study 1. As an illustration, example (6) shows a passage whose original co-reference relation was one between an it pronoun and an entity antecedent. The continuations in (7) were annotated as event co-reference (7-a), entity co-reference (7-b), and no co-reference when the It prompt was classed as being used pleonastically (7-c). Study 2: Corpus passages Materials The 48 target passages are minimally edited sentences extracted from the ParCorFull corpus (Lapshinova-Koltunski et al., 2018). This is a German-English parallel corpus annotated with full co-reference. Although the corpus is designed for nominal co-reference, it includes annotations of two types of antecedents: entities and events. Entities can be either pronouns or NPs, whereas events can be VPs, clauses or a set of clauses. ParCorFull includes texts from TED talks transcripts and also newswire data.5 Since pronouns (6) You carry a phone. It knows where you are. [original co-reference: entity∼it] (7) a. You carry a phone. This is something that just about everyone does these days. b. You carry a phone. It is capabl"
W18-0711,P16-3020,0,0.016602,"vent coreference for the corpus passages in Study 2 that were known to have yielded event co-reference in their original passages. This suggests that there are properties of the context sentence that may make salient an event over an entity. If there are event-favoring properties of the context sentence that human participants are sensitive to, it is a tractable task to build automatic classifiers that learn to recognize such properties. This supports the idea that the task of differentiating anaphoric and pleonastic instances of It (Evans, 2001; Boyd et al., 2005; Bergsma and Yarowsky, 2011; Lee et al., 2016; Lo´aiciga et al., 2017) could potentially improve performance. Although presumably (machine) learnable, the question of what exactly constitutes an event remains unanswered. A number of ambiguous examples which were excluded from our analysis included entities that are close to their entailed event (e.g., The bomb that the arsonists had planted exploded violently) or that were very abstract (e.g., The greatest opportunity materialized unexpectedly. It/This was almost like magic.). was again modeled with a logistic regression. We included fixed effects for prompt type, original passage co-ref"
W18-0711,D17-1137,1,0.887305,"Missing"
W18-0711,P07-1103,0,0.0408854,"Missing"
W18-0711,W10-0719,0,0.0100095,"nces have been observed between the use of proximal and distal demonstratives this and that (C ¸ okal et al., 2014), we targeted only one demonstrative pronoun in order to simplify the design. This is in keeping with observations about the functional grouping of a number of pronouns (zeros, demonstratives, and personal pronouns) when used deictically (Webber, 1990). 3.1 cooking for the BBQ even though he hates BBQ. He prefers mac ’n ). 3.2 Participants Twenty-seven monolingual English-speaking participants aged 19-63 (mean age 36, σ=11.2; 15 male) were recruited from Amazon’s Mechanical Turk (Munro et al., 2010; Gibson et al., 2011) and received $4 for an estimated 30-minute task. 3.3 Procedure Continuations were collected via a web-based interface that participants could access from their own computer. Each item was presented on a page by itself with a text box for participants to use for writing their continuation. 3.4 Annotation and analysis Continuations for experimental items were annotated for type of co-reference (entity vs event). The four authors of this paper shared the annotation such that all target continuations were coded by two annotators. To be conservative, annotators were blind to"
W18-0711,W16-0707,0,0.0478365,"Missing"
W18-0711,P89-1007,0,0.780314,"Missing"
W18-0711,W13-3516,0,0.0503471,"Missing"
W18-2406,L18-1065,1,0.882658,"Missing"
W18-2406,W10-0719,0,0.0619773,"Missing"
W18-2406,W13-3516,0,0.122657,"Missing"
W18-6305,L16-1100,1,0.826254,"rough human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work has specifically targeted the differences in performance between NMT and SMT (Burlot and Yvon, 2017; Sennrich, 2017). There are also other types of error analysis targeting this difference, e.g. bas"
W18-6305,N18-1118,0,0.0110707,"tems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific err"
W18-6305,guillou-etal-2014-parcor,1,0.901195,"Missing"
W18-6305,D16-1025,0,0.0160536,"comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work has specifically targeted the differences in performance between NMT and SMT (Burlot and Yvon, 2017; Sennrich, 2017). There are also other types of error analysis targeting this difference, e.g. based on post-edits (Bentivogli et al., 2016). For Croatian in particular, Klubiˇcka et al. (2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along with the observations based on the manual data analysis. 4.1 Parallel Corpora As the use of coreference phenomena varies across different registers and text types, we decided to perform the analysis on corpora from three different domains: • DGT-TM (Steinberger et al., 2012): EU legal texts, 950K sentences • SETIMES2 (Tiedemann, 2009):"
W18-6305,2010.iwslt-papers.10,1,0.808379,"ulated through word order, which makes pleonastic pronouns largely redundant in Croatian. Finally, it does not easily create participial constructions, preferring to elaborate the concise English participial expressions into full, finite relative clauses using the relative pronoun koji. 37 ysis of SMT and NMT systems, finding that the translation of function words in general is considerably improved in NMT. However, they do not present separate results for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomen"
W18-6305,W15-2501,1,0.851562,"sis of SMT and NMT systems, finding that the translation of function words in general is considerably improved in NMT. However, they do not present separate results for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall sc"
W18-6305,P17-4012,0,0.0119387,"one of the authors, who is a native speaker of Croatian. To reflect the scalar nature of error severity, we assign a penalty to each error category. This also enables us to produce a provisional score for relative comparison and evaluation of the systems. Some clarification might be needed for categories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is largely focused on intra-sentential phenomen"
W18-6305,W17-4705,0,0.0120542,"mains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work ha"
W18-6305,P07-2045,0,0.00690842,"in Table 3. The evaluation was performed by one of the authors, who is a native speaker of Croatian. To reflect the scalar nature of error severity, we assign a penalty to each error category. This also enables us to produce a provisional score for relative comparison and evaluation of the systems. Some clarification might be needed for categories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is"
W18-6305,N03-1017,0,0.054564,"d The three datasets cover an interesting range from very formal, strictly standardized and highly repetitive texts (DGT) to fairly loose and informal translation of speeches (TedTalks). For the purposes of the analyses, English is taken as the source and Croatian as the target language. The corpora were tokenized, tagged for parts of speech and parsed using the pre-trained models for the respective languages developed for the annotation pipeline UDPipe (2017). The parallel data were then aligned at word-level with efmaral (Östling and Tiedemann, 2016), using the grow-diag-finaland heuristic (Koehn et al., 2003). Relying on the approach of LapshinovaKoltunski and Hardmeier (2017), we used POStags and dependency information to extract a highrecall list of pronouns and determiners in both languages, in order to identify potentially interesting coreference patterns. The main criterion for their extraction was the pron or det tag, as the original research has found this approach to permit reliable identification of phenomena, even with multi-word units. Similarly to LapshinovaKoltunski and Hardmeier (2017), we couple the 38 POS-tags with syntactic information to create linguistic patterns in the format l"
W18-6305,E12-3001,0,0.021885,"akes pleonastic pronouns largely redundant in Croatian. Finally, it does not easily create participial constructions, preferring to elaborate the concise English participial expressions into full, finite relative clauses using the relative pronoun koji. 37 ysis of SMT and NMT systems, finding that the translation of function words in general is considerably improved in NMT. However, they do not present separate results for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-awa"
W18-6305,E17-2060,0,0.0163927,"y do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work has specifically targeted the differences in performance between NMT and SMT (Burlot and Yvon, 2017; Sennrich, 2017). There are also other types of error analysis targeting this difference, e.g. based on post-edits (Bentivogli et al., 2016). For Croatian in particular, Klubiˇcka et al. (2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along"
W18-6305,W17-4810,1,0.859017,"ivalent counterpart in the other language. We use the same procedure to automatically extract phenomena, but extend the methodology to include cases where the phenomenon does have an equivalent construction in the other language, despite the alignment data suggesting that it is more frequently left unaligned. In this research, we perform an in-depth study of the way in which diverse discourse phenomena are handled in translation from English to Croatian. We investigate both human translation and the output of different types of MT systems. In the first step, we use the extended methodology of Lapshinova-Koltunski and Hardmeier (2017) to extract interesting diverging discourse patterns that commonly occur in the parallel data. While reflections on the relevant linguistic intuitions are given as a reference, the selection of the phenomena chosen for further examination is primarily based on the data obtained from corpora. This makes our approach strongly usage-based and provides ample space for making observations unconstrained by a particular theoretical framework. In the second step, we construct a dataset with sentences containing challenging discourse phenomena identified in the analysis of human translations. The const"
W18-6305,P16-1162,0,0.0117969,"ategories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is largely focused on intra-sentential phenomena. Although the segmented nature of the artificially constructed test set might be considered a constraint, it is difficult to find an alternative way of testing such a variety of phenomena, while retaining as much data as possible for training. 4 http://hdl.handle.net/11234/1-2855 5 Error Analysis"
W18-6305,W10-1737,0,0.0804279,"Missing"
W18-6305,steinberger-etal-2012-dgt,0,0.0246511,"r analysis targeting this difference, e.g. based on post-edits (Bentivogli et al., 2016). For Croatian in particular, Klubiˇcka et al. (2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along with the observations based on the manual data analysis. 4.1 Parallel Corpora As the use of coreference phenomena varies across different registers and text types, we decided to perform the analysis on corpora from three different domains: • DGT-TM (Steinberger et al., 2012): EU legal texts, 950K sentences • SETIMES2 (Tiedemann, 2009): newspaper articles, 200K sentences • TedTalks (Tiedemann, 2012): speeches, 86K sentences prepared The three datasets cover an interesting range from very formal, strictly standardized and highly repetitive texts (DGT) to fairly loose and informal translation of speeches (TedTalks). For the purposes of the analyses, English is taken as the source and Croatian as the target language. The corpora were tokenized, tagged for parts of speech and parsed using the pre-trained models for the respective languages developed for the annotation"
W18-6305,D15-1166,0,0.0122494,"o produce a provisional score for relative comparison and evaluation of the systems. Some clarification might be needed for categories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is largely focused on intra-sentential phenomena. Although the segmented nature of the artificially constructed test set might be considered a constraint, it is difficult to find an alternative way of testing such a va"
W18-6305,K17-3009,0,0.0331541,"Missing"
W18-6305,W13-3303,0,0.0214506,"urse phenomena in translation. Given the immense variety of linguistic phenomena that fall within the scope of the term, research on discourse phenomena in translation has often focused on a limited group of phenomena (e.g. Furkó, 2014; Zinsmeister et al., 2012; Bührig and House, 2004), which frequently have to be studied in reference to particular registers (Kunz and LapshinovaKoltunski, 2015). Moreover, the pronouncedly language-specific character of their form has led to examinations of explicitation and implicitation of these phenomena in translation (Blum-Kulka, 1986). On a similar note, Meyer and Webber (2013) compare implicitation tendencies in human and machine translation and find that the latter displays more cases where the phenomena are kept in translation. Scarton and Specia (2015) assess the impact of discourse structures on MT quality through quantitative analysis, while Lapshinova-Koltunski (2017) compares human and machine translations to identify and describe variation in the distribution of different cohesive devices. On the other hand, a variety of approaches have also been proposed to incorporate discursive inforMotivation As a South Slavic language, Croatian is a morphologically ric"
W18-6305,stymne-ahrenberg-2012-practice,1,0.840746,"ntext-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct a"
W18-6305,W17-4811,0,0.0109415,". mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error an"
W18-6305,tiedemann-2012-parallel,0,0.0657745,"(2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along with the observations based on the manual data analysis. 4.1 Parallel Corpora As the use of coreference phenomena varies across different registers and text types, we decided to perform the analysis on corpora from three different domains: • DGT-TM (Steinberger et al., 2012): EU legal texts, 950K sentences • SETIMES2 (Tiedemann, 2009): newspaper articles, 200K sentences • TedTalks (Tiedemann, 2012): speeches, 86K sentences prepared The three datasets cover an interesting range from very formal, strictly standardized and highly repetitive texts (DGT) to fairly loose and informal translation of speeches (TedTalks). For the purposes of the analyses, English is taken as the source and Croatian as the target language. The corpora were tokenized, tagged for parts of speech and parsed using the pre-trained models for the respective languages developed for the annotation pipeline UDPipe (2017). The parallel data were then aligned at word-level with efmaral (Östling and Tiedemann, 2016), using t"
W18-6305,P02-1040,0,0.101538,"n techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, develo"
W18-6305,vilar-etal-2006-error,0,0.0949915,"Missing"
W18-6305,P18-1117,0,0.0104849,"lts for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT"
W18-6305,D17-1301,0,0.0113975,"oreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There"
W18-6435,N18-1118,0,0.296545,"or the evaluation of morphology in the English-to-Latvian and to-Czech language pairs; Sennrich (2017), who evaluates noun phrase and subject-verb agreement, particle verbs, polarity, and transliteration; and Rios Gonzales et al. (2017) whose work concentrates on word sense disambiguation for the German-to-English and Germanto-French pairs. The test suite used in our work is based on the PROTEST test suite, which was originally created for English–French by Guillou and Hardmeier (2016). Closest to our work is the test suite of English-to-French anaphoric pronouns and coherence and cohesion by Bawden et al. (2018). Their test suite includes 50 examples of contrastive pairs of sentences, which are manually created and targeted towards object pronouns. 3 Test Suite Construction The data for our test suite was taken from the ParCorFull corpus (Lapshinova-Koltunski et al., 2018), a German-English parallel corpus manually annotated for co-reference. Although the corpus is designed for nominal co-reference, it includes annotations of two types of antecedents: entities and events. Entities can be either pronouns or noun phrases, whereas events can be verb phrases, clauses, or a set of clauses. ParCorFull incl"
W18-6435,D12-1091,0,0.0395908,"Missing"
W18-6435,W17-4705,0,0.0208249,"metrics computed by matching the candidate and reference translations offer little explanation of the causes for error. Additionally, the neural architectures of current end-to-end systems make it difficult to find out where exactly a translation went wrong by inspection. Test suites ease the evaluation process in general, since they allow us to simultaneously measure quantitative performance and diagnose qualitative shortcomings with regard to the targeted set of problems. Test suites assessing NMT have focused on contrastive pairs or sets of sentences automatically generated. These include Burlot and Yvon (2017), for the evaluation of morphology in the English-to-Latvian and to-Czech language pairs; Sennrich (2017), who evaluates noun phrase and subject-verb agreement, particle verbs, polarity, and transliteration; and Rios Gonzales et al. (2017) whose work concentrates on word sense disambiguation for the German-to-English and Germanto-French pairs. The test suite used in our work is based on the PROTEST test suite, which was originally created for English–French by Guillou and Hardmeier (2016). Closest to our work is the test suite of English-to-French anaphoric pronouns and coherence and cohesion"
W18-6435,L16-1100,1,0.826862,"Missing"
W18-6435,D18-1513,1,0.922493,"stic problems. Whilst BLEU scores as a measure of general translation quality are strongly correlated with pronoun correctness, there are significant outliers that would be missed by an evaluation focusing on BLEU only. Moreover, evaluating pro570 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 570–577 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64062 noun translations by comparison with a reference translation is not reliable for all types of pronouns (Guillou and Hardmeier, 2018). This fact limits the usefulness of automatic pronoun evaluation metrics such as APT (Miculicich Werlen and PopescuBelis, 2017) and affects the semi-automatic evaluation of our test suite as well. 2 Related Work Research on pronoun translation was boosted by three past shared tasks (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). They focused on English, French, German and Spanish in different directions. To avoid the effort and cost of manual evaluation, the tasks were designed and evaluated as classification rather than MT tasks, except for the first year, which featu"
W18-6435,W16-2345,1,0.934672,"being generated, but aspects of translation that require keeping track of long-distance dependencies continue to pose problems. Linguistically, long-distance dependencies often arise from discourse-level phenomena such as pronominal reference, lexical cohesion, text structure, etc. Initially largely ignored, such problems have attracted increasing attention in the statistical MT (SMT) community in recent years (Hardmeier, 2012; Sim Smith, 2017). One important problem that has proved to be surprisingly difficult despite extensive research is the translation of pronouns (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). * All authors contributed equally. Since the invention of the BLEU score (Papineni et al., 2002), the MT community has measured progress to a large extent with the help of summary scores that are easy to compute, but strongly affected by the corpus-level frequency of certain phenomena, and that tend to neglect specific linguistic relations and problems that occur infrequently. The advent of neural MT (NMT) with its improved capacity for modeling more complex relationships between linguistic elements has brought an increased interest in linguistic problems perceived a"
W18-6435,guillou-etal-2014-parcor,1,0.785821,"object pronouns. 3 Test Suite Construction The data for our test suite was taken from the ParCorFull corpus (Lapshinova-Koltunski et al., 2018), a German-English parallel corpus manually annotated for co-reference. Although the corpus is designed for nominal co-reference, it includes annotations of two types of antecedents: entities and events. Entities can be either pronouns or noun phrases, whereas events can be verb phrases, clauses, or a set of clauses. ParCorFull includes texts from TED talks transcripts and newswire data. Specifically, it includes the datasets used in the ParCor corpus (Guillou et al., 2014), the DiscoMT workshop (Hardmeier et al., 2016), and the test sets from the WMT 2017 shared task (Bojar et al., 2017). We constructed a test suite of 200 pronoun translation examples for English–German with a focus on the ambiguous English pronouns it and they and the aim of providing a set of examples that represents the different problems machine translation researchers should consider. We extracted the examples from the TED talks section of ParCorFull. The selection is based on a two-level hierarchy which considers pronoun function at the top level, followed by other pronoun attributes at t"
W18-6435,W15-2522,1,0.873867,"t strongly affected by the corpus-level frequency of certain phenomena, and that tend to neglect specific linguistic relations and problems that occur infrequently. The advent of neural MT (NMT) with its improved capacity for modeling more complex relationships between linguistic elements has brought an increased interest in linguistic problems perceived as difficult, which are often not captured well by metrics like BLEU. It has been suggested that test suites composed of difficult cases could provide more relevant insights into the performance of MT systems than corpus-level summary scores (Hardmeier, 2015). In this paper, we present a semi-automatic evaluation of the systems participating in the English– German news translation track of the MT shared task at the WMT 2018 conference. The analysis was carried out with the help of an English–German adaptation of the PROTEST test suite for pronoun translation (Guillou and Hardmeier, 2016). The test suite allows us to perform a fine-grained evaluation for different types of pronouns. Whilst the translation of event pronouns, which caused serious problems in earlier evaluations of SMT systems (Hardmeier et al., 2015; Hardmeier and Guillou, 2018), see"
W18-6435,W16-3418,1,0.912076,"Missing"
W18-6435,W15-2501,1,0.954751,"d of the word currently being generated, but aspects of translation that require keeping track of long-distance dependencies continue to pose problems. Linguistically, long-distance dependencies often arise from discourse-level phenomena such as pronominal reference, lexical cohesion, text structure, etc. Initially largely ignored, such problems have attracted increasing attention in the statistical MT (SMT) community in recent years (Hardmeier, 2012; Sim Smith, 2017). One important problem that has proved to be surprisingly difficult despite extensive research is the translation of pronouns (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). * All authors contributed equally. Since the invention of the BLEU score (Papineni et al., 2002), the MT community has measured progress to a large extent with the help of summary scores that are easy to compute, but strongly affected by the corpus-level frequency of certain phenomena, and that tend to neglect specific linguistic relations and problems that occur infrequently. The advent of neural MT (NMT) with its improved capacity for modeling more complex relationships between linguistic elements has brought an increased interest in linguisti"
W18-6435,W17-4806,0,0.0188336,", French, German and Spanish in different directions. To avoid the effort and cost of manual evaluation, the tasks were designed and evaluated as classification rather than MT tasks, except for the first year, which featured both MT and classification tasks. At the time of the first of these shared tasks, phrase-based SMT systems were still competitive and the winning system was a strong n-gram language model (not involving any translation) trained as a baseline. By the time of the last pronoun focused shared task, however, an NMT system with no explicit knowledge about pronouns ranked first (Jean et al., 2017). Automatic metrics computed by matching the candidate and reference translations offer little explanation of the causes for error. Additionally, the neural architectures of current end-to-end systems make it difficult to find out where exactly a translation went wrong by inspection. Test suites ease the evaluation process in general, since they allow us to simultaneously measure quantitative performance and diagnose qualitative shortcomings with regard to the targeted set of problems. Test suites assessing NMT have focused on contrastive pairs or sets of sentences automatically generated. The"
W18-6435,2005.iwslt-1.8,0,0.0666503,"for the APT score (r = 0.887, N = 15, p < 0.001). These results, however, should be taken with a grain of salt, as we argue further in Section 5. 4.2 Semi-automatic Evaluation The semi-automatic evaluation method is a twopass procedure. It is motivated by the observation that automatic reference-based methods can identify correct examples with relatively high precision, but low recall (Guillou and Hardmeier, 2018). The evaluation procedure relies on word alignments, which were generated automatically by running Giza++ (Och and Ney, 2003) in both directions with grow-diag-final symmetrization (Koehn et al., 2005). The word alignments for the examples in the reference translation were corrected manually. In the first step, the candidate translations are matched against the reference translation to ap572 2 The APT score could not be computed for the LMU-uns system because the scorer cannot handle completely untranslated sentences, which occur occasionally in the output of that system. BLEU APT 30 Systems S G N e− −U lin TH RW t A e− lin on on F m e− −n lin U on LM Y e− on lin JH U Z B e− e− lin on on lin d T KI n di pr o T− M M C ue M M S− U n ia ar −u U LM TT S ns G N e− −U lin TH RW t A e− lin on on F"
W18-6435,L18-1065,1,0.848827,"entrates on word sense disambiguation for the German-to-English and Germanto-French pairs. The test suite used in our work is based on the PROTEST test suite, which was originally created for English–French by Guillou and Hardmeier (2016). Closest to our work is the test suite of English-to-French anaphoric pronouns and coherence and cohesion by Bawden et al. (2018). Their test suite includes 50 examples of contrastive pairs of sentences, which are manually created and targeted towards object pronouns. 3 Test Suite Construction The data for our test suite was taken from the ParCorFull corpus (Lapshinova-Koltunski et al., 2018), a German-English parallel corpus manually annotated for co-reference. Although the corpus is designed for nominal co-reference, it includes annotations of two types of antecedents: entities and events. Entities can be either pronouns or noun phrases, whereas events can be verb phrases, clauses, or a set of clauses. ParCorFull includes texts from TED talks transcripts and newswire data. Specifically, it includes the datasets used in the ParCor corpus (Guillou et al., 2014), the DiscoMT workshop (Hardmeier et al., 2016), and the test sets from the WMT 2017 shared task (Bojar et al., 2017). We"
W18-6435,W17-4801,1,0.891745,"Missing"
W18-6435,W17-4802,0,0.0930011,"T evaluation because they are computed on a different test set, containing texts from System BLEU Microsoft-Marian NTT UCAM uedin MMT-prod KIT online-Z online-B online-Y JHU online-F LMU-nmt online-A online-G RWTH-UNS LMU-uns 32.6 31.8 32.3 30.7 33.2 31.6 32.5 32.7 31.9 28.8 18.8 28.5 27.4 22.3 13.7 10.5 3 7 5 9 1 8 4 2 6 10 14 11 12 13 15 16 APT 66.0 7 70.0 1 69.0 2 68.0 4 65.0 8 68.5 3 66.5 6 62.5 10 68.0 5 62.0 12 60.5 13 63.0 9 62.0 11 59.5 14 54.5 15 – Table 1: Automatic evaluation results. a different domain. For a more pronoun-specific evaluation, we also compute APT scores (Miculicich Werlen and Popescu-Belis, 2017).2 For better comparability, the set of pronouns evaluated by APT was restricted to the 200 items included in the test suite. Following the recommendations of Guillou and Hardmeier (2018), we did not define any “equivalent” pronouns in the APT metric, but counted exact matches only. A regression fit between the BLEU scores obtained and the number of examples annotated as correct by each system indicates a strong correlation between the two (Figure 2; r = 0.912, N = 16, p < 0.001), as does a similar analysis for the APT score (r = 0.887, N = 15, p < 0.001). These results, however, should be tak"
W18-6435,J03-1002,0,0.0243755,"e two (Figure 2; r = 0.912, N = 16, p < 0.001), as does a similar analysis for the APT score (r = 0.887, N = 15, p < 0.001). These results, however, should be taken with a grain of salt, as we argue further in Section 5. 4.2 Semi-automatic Evaluation The semi-automatic evaluation method is a twopass procedure. It is motivated by the observation that automatic reference-based methods can identify correct examples with relatively high precision, but low recall (Guillou and Hardmeier, 2018). The evaluation procedure relies on word alignments, which were generated automatically by running Giza++ (Och and Ney, 2003) in both directions with grow-diag-final symmetrization (Koehn et al., 2005). The word alignments for the examples in the reference translation were corrected manually. In the first step, the candidate translations are matched against the reference translation to ap572 2 The APT score could not be computed for the LMU-uns system because the scorer cannot handle completely untranslated sentences, which occur occasionally in the output of that system. BLEU APT 30 Systems S G N e− −U lin TH RW t A e− lin on on F m e− −n lin U on LM Y e− on lin JH U Z B e− e− lin on on lin d T KI n di pr o T− M M"
W18-6435,P02-1040,0,0.103259,"blems. Linguistically, long-distance dependencies often arise from discourse-level phenomena such as pronominal reference, lexical cohesion, text structure, etc. Initially largely ignored, such problems have attracted increasing attention in the statistical MT (SMT) community in recent years (Hardmeier, 2012; Sim Smith, 2017). One important problem that has proved to be surprisingly difficult despite extensive research is the translation of pronouns (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). * All authors contributed equally. Since the invention of the BLEU score (Papineni et al., 2002), the MT community has measured progress to a large extent with the help of summary scores that are easy to compute, but strongly affected by the corpus-level frequency of certain phenomena, and that tend to neglect specific linguistic relations and problems that occur infrequently. The advent of neural MT (NMT) with its improved capacity for modeling more complex relationships between linguistic elements has brought an increased interest in linguistic problems perceived as difficult, which are often not captured well by metrics like BLEU. It has been suggested that test suites composed of dif"
W18-6435,W17-4702,0,0.0668102,"nslation went wrong by inspection. Test suites ease the evaluation process in general, since they allow us to simultaneously measure quantitative performance and diagnose qualitative shortcomings with regard to the targeted set of problems. Test suites assessing NMT have focused on contrastive pairs or sets of sentences automatically generated. These include Burlot and Yvon (2017), for the evaluation of morphology in the English-to-Latvian and to-Czech language pairs; Sennrich (2017), who evaluates noun phrase and subject-verb agreement, particle verbs, polarity, and transliteration; and Rios Gonzales et al. (2017) whose work concentrates on word sense disambiguation for the German-to-English and Germanto-French pairs. The test suite used in our work is based on the PROTEST test suite, which was originally created for English–French by Guillou and Hardmeier (2016). Closest to our work is the test suite of English-to-French anaphoric pronouns and coherence and cohesion by Bawden et al. (2018). Their test suite includes 50 examples of contrastive pairs of sentences, which are manually created and targeted towards object pronouns. 3 Test Suite Construction The data for our test suite was taken from the Par"
W18-6435,E17-2060,0,0.0268345,"r error. Additionally, the neural architectures of current end-to-end systems make it difficult to find out where exactly a translation went wrong by inspection. Test suites ease the evaluation process in general, since they allow us to simultaneously measure quantitative performance and diagnose qualitative shortcomings with regard to the targeted set of problems. Test suites assessing NMT have focused on contrastive pairs or sets of sentences automatically generated. These include Burlot and Yvon (2017), for the evaluation of morphology in the English-to-Latvian and to-Czech language pairs; Sennrich (2017), who evaluates noun phrase and subject-verb agreement, particle verbs, polarity, and transliteration; and Rios Gonzales et al. (2017) whose work concentrates on word sense disambiguation for the German-to-English and Germanto-French pairs. The test suite used in our work is based on the PROTEST test suite, which was originally created for English–French by Guillou and Hardmeier (2016). Closest to our work is the test suite of English-to-French anaphoric pronouns and coherence and cohesion by Bawden et al. (2018). Their test suite includes 50 examples of contrastive pairs of sentences, which a"
W18-6435,W17-4814,0,0.0485799,"on Data-driven machine translation (MT) systems are very good at making translation choices based on the words in the immediate neighbourhood of the word currently being generated, but aspects of translation that require keeping track of long-distance dependencies continue to pose problems. Linguistically, long-distance dependencies often arise from discourse-level phenomena such as pronominal reference, lexical cohesion, text structure, etc. Initially largely ignored, such problems have attracted increasing attention in the statistical MT (SMT) community in recent years (Hardmeier, 2012; Sim Smith, 2017). One important problem that has proved to be surprisingly difficult despite extensive research is the translation of pronouns (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). * All authors contributed equally. Since the invention of the BLEU score (Papineni et al., 2002), the MT community has measured progress to a large extent with the help of summary scores that are easy to compute, but strongly affected by the corpus-level frequency of certain phenomena, and that tend to neglect specific linguistic relations and problems that occur infrequently. The advent of neural"
W18-6435,W17-4811,0,0.0774457,"that NMT systems are quite good at identifying pronouns with event reference and producing appropriate translations for them. 6 of context for the translation of pleonastic, event and intra-sentential anaphoric pronouns. Pleonastic pronouns are handled almost perfectly by most systems, so we suggest that future evaluations emphasize the more challenging cases. Anaphoric pronouns depending on the inter-sentential context remain a significant challenge. They present an ideal test case for the development of contextaware NMT systems. Research in that direction has recently gained some traction (Tiedemann and Scherrer, 2017; Wang et al., 2017; Tu et al., 2018) and has claimed promising results specifically for pronoun translation (Voita et al., 2018). It remains to be seen whether the development of such methods will lead to a breakthrough in the translation of inter-sentential anaphoric pronouns in the near future. Conclusions We have presented a detailed analysis of 16 NMT systems, assessing their performance in the translation of pronouns using a semi-automatic evaluation based on a balanced test suite. The results reinforce the idea that automatic evaluation scores are correlated with manual evaluation resul"
W18-6435,Q18-1029,0,0.0537,"ronouns with event reference and producing appropriate translations for them. 6 of context for the translation of pleonastic, event and intra-sentential anaphoric pronouns. Pleonastic pronouns are handled almost perfectly by most systems, so we suggest that future evaluations emphasize the more challenging cases. Anaphoric pronouns depending on the inter-sentential context remain a significant challenge. They present an ideal test case for the development of contextaware NMT systems. Research in that direction has recently gained some traction (Tiedemann and Scherrer, 2017; Wang et al., 2017; Tu et al., 2018) and has claimed promising results specifically for pronoun translation (Voita et al., 2018). It remains to be seen whether the development of such methods will lead to a breakthrough in the translation of inter-sentential anaphoric pronouns in the near future. Conclusions We have presented a detailed analysis of 16 NMT systems, assessing their performance in the translation of pronouns using a semi-automatic evaluation based on a balanced test suite. The results reinforce the idea that automatic evaluation scores are correlated with manual evaluation results, but they also confirm that automa"
W18-6435,P18-1117,0,0.0664583,"t for the translation of pleonastic, event and intra-sentential anaphoric pronouns. Pleonastic pronouns are handled almost perfectly by most systems, so we suggest that future evaluations emphasize the more challenging cases. Anaphoric pronouns depending on the inter-sentential context remain a significant challenge. They present an ideal test case for the development of contextaware NMT systems. Research in that direction has recently gained some traction (Tiedemann and Scherrer, 2017; Wang et al., 2017; Tu et al., 2018) and has claimed promising results specifically for pronoun translation (Voita et al., 2018). It remains to be seen whether the development of such methods will lead to a breakthrough in the translation of inter-sentential anaphoric pronouns in the near future. Conclusions We have presented a detailed analysis of 16 NMT systems, assessing their performance in the translation of pronouns using a semi-automatic evaluation based on a balanced test suite. The results reinforce the idea that automatic evaluation scores are correlated with manual evaluation results, but they also confirm that automatic evaluation can provide a misleading picture of the behavior of some systems. The evaluat"
W18-6435,D17-1301,0,0.0343026,"od at identifying pronouns with event reference and producing appropriate translations for them. 6 of context for the translation of pleonastic, event and intra-sentential anaphoric pronouns. Pleonastic pronouns are handled almost perfectly by most systems, so we suggest that future evaluations emphasize the more challenging cases. Anaphoric pronouns depending on the inter-sentential context remain a significant challenge. They present an ideal test case for the development of contextaware NMT systems. Research in that direction has recently gained some traction (Tiedemann and Scherrer, 2017; Wang et al., 2017; Tu et al., 2018) and has claimed promising results specifically for pronoun translation (Voita et al., 2018). It remains to be seen whether the development of such methods will lead to a breakthrough in the translation of inter-sentential anaphoric pronouns in the near future. Conclusions We have presented a detailed analysis of 16 NMT systems, assessing their performance in the translation of pronouns using a semi-automatic evaluation based on a balanced test suite. The results reinforce the idea that automatic evaluation scores are correlated with manual evaluation results, but they also c"
W19-2803,D14-1162,0,0.0897242,"not reduce the length of the mentions in the data for YangLM to one as in the original model but use the setting as described above. 4 Implementation We implemented all models in Python using the PyTorch deep learning library (Paszke et al., 2017). As candidate hyperparameters for the hidden size of the LSTM and word embedding layer, we tried the values 32, 48, 64, 128, 256, 512. We employ dropout (Srivastava et al., 2014) with candidate rates of 0.0, 0.1 or 0.2 and for the Adam optimizer (Kingma and Ba, 2014), we tried the learning rates 0.01, 0.005 and 0.001. We tried the models with GloVe (Pennington et al., 2014) and with randomly initialized, learnable word embeddings. We also experimented with a weighted loss with the intention to force the models to produce more entity mentions. Based on the experimental results on the development set, we chose a hyperparameter setting 16 for the model based on Yang et al. (2016) with 64 hidden units for both LSTM hidden size and word embedding size, Adam optimizer with λ = 0.005, a dropout rate of 0.2 and randomly initialized word embeddings. The model was trained for 20 epochs. The best hyperparameter setting for the model based on EntityNLM was very similar and"
W19-2803,D17-1131,0,0.0246784,"Missing"
W19-2803,N18-1204,0,0.0476285,"Missing"
W19-2803,P82-1020,0,0.749221,"Missing"
W19-2803,D17-1195,0,0.515413,"anguage Modelling: Approaches and Problems Jenny Kunz and Christian Hardmeier Department of Linguistics and Philology Uppsala University 752 36 Uppsala, Sweden jenny.kunz.7402@student.uu.se christian.hardmeier@lingfil.uu.se Abstract in which decisions are made. SetLM is a simpler architecture with fewer loss functions. It replaces the latent variable modelling the decision whether to produce an entity with two extra embeddings, one for a new entity (similar to the other models) and one for the case that the token does not belong to an entity. We replicate the results of Yang et al. (2016) and Ji et al. (2017) with an independent reimplementation of their models in a comparable experimental setup and evaluate the models in terms of overall language modelling performance performance in comparison with a simple RNN-LM. We also study the accuracy and precision/recall in each individual decision step and look at the convergence of variables. We find that YangLM outperforms the other models in terms of perplexity, whereas SetLM achieves the best results for the entity yes/no prediction. None of the entity-enabled LMs is competitive with a simple RNN-LM having a higher number of hidden units, and we do n"
W19-2805,L18-1065,1,0.864696,".loaiciga@gu.se christian.hardmeier@lingfil.uu.se pauline.krielke@uni-saarland.de Abstract quity of this problem in all kind of parallel linguistic annotation, it has received little attention in the existing works. In this paper, we address the problem of crosslingual incongruences in the manual annotation of coreference. To our knowledge, none of the existing studies on parallel coreference annotation (Dipper and Zinsmeister, 2012; Zik´anov´a et al., 2015; Grishina and Stede, 2017) addresses this issue. We analyse the incongruences in coreference chains in a subset of the corpus ParCorFull (Lapshinova-Koltunski et al., 2018), an English-German parallel corpus containing manual annotations of coreference. We automatically extract annotated chains from the corpus, create an alignment between chains in the source and target language and identify those that do not have parallel equivalents in the source or the target language. Among the parallel chains, we detect those with differences in English and German. Our use of word alignments to map coreference structures between language is similar to the existing studies on annotation projection (e.g. Yarowsky et al., 2001) or specifically on multilingual coreference proje"
W19-2805,W15-3403,0,0.208847,"al annotations of coreference. We automatically extract annotated chains from the corpus, create an alignment between chains in the source and target language and identify those that do not have parallel equivalents in the source or the target language. Among the parallel chains, we detect those with differences in English and German. Our use of word alignments to map coreference structures between language is similar to the existing studies on annotation projection (e.g. Yarowsky et al., 2001) or specifically on multilingual coreference projection (Postolache et al., 2006; Ogrodniczuk, 2013; Grishina and Stede, 2015; Nov´ak, 2018). However, in contrast to annotation projection, we do not aim to produce any automatic annotations. Instead, we use automated methods to discover incongruences in the existing annotations produced manually on parallel texts. The cross-lingual variation in the chains is then analysed both quantitatively and qualitatively. We develop a typology of the incongruences encountered in ParCorFull, illustrated with corpus examples, and present empirical results on the prevalence of different types of variations using a corpus sample. The results of this study facilitate the analysis of"
W19-2805,W17-1506,0,0.201791,"ersity of Gothenburg, Sweden 3 Department of Linguistics and Philology, Uppsala University, Sweden e.lapshinova@mx.uni-saarland.de sharid.loaiciga@gu.se christian.hardmeier@lingfil.uu.se pauline.krielke@uni-saarland.de Abstract quity of this problem in all kind of parallel linguistic annotation, it has received little attention in the existing works. In this paper, we address the problem of crosslingual incongruences in the manual annotation of coreference. To our knowledge, none of the existing studies on parallel coreference annotation (Dipper and Zinsmeister, 2012; Zik´anov´a et al., 2015; Grishina and Stede, 2017) addresses this issue. We analyse the incongruences in coreference chains in a subset of the corpus ParCorFull (Lapshinova-Koltunski et al., 2018), an English-German parallel corpus containing manual annotations of coreference. We automatically extract annotated chains from the corpus, create an alignment between chains in the source and target language and identify those that do not have parallel equivalents in the source or the target language. Among the parallel chains, we detect those with differences in English and German. Our use of word alignments to map coreference structures between l"
W19-2805,W18-0709,0,0.0384915,"Missing"
W19-2805,J03-1002,0,0.0354264,"Missing"
W19-2805,postolache-etal-2006-transferring,0,0.343568,"glish-German parallel corpus containing manual annotations of coreference. We automatically extract annotated chains from the corpus, create an alignment between chains in the source and target language and identify those that do not have parallel equivalents in the source or the target language. Among the parallel chains, we detect those with differences in English and German. Our use of word alignments to map coreference structures between language is similar to the existing studies on annotation projection (e.g. Yarowsky et al., 2001) or specifically on multilingual coreference projection (Postolache et al., 2006; Ogrodniczuk, 2013; Grishina and Stede, 2015; Nov´ak, 2018). However, in contrast to annotation projection, we do not aim to produce any automatic annotations. Instead, we use automated methods to discover incongruences in the existing annotations produced manually on parallel texts. The cross-lingual variation in the chains is then analysed both quantitatively and qualitatively. We develop a typology of the incongruences encountered in ParCorFull, illustrated with corpus examples, and present empirical results on the prevalence of different types of variations using a corpus sample. The resu"
W19-2805,H01-1035,0,0.379211,"in a subset of the corpus ParCorFull (Lapshinova-Koltunski et al., 2018), an English-German parallel corpus containing manual annotations of coreference. We automatically extract annotated chains from the corpus, create an alignment between chains in the source and target language and identify those that do not have parallel equivalents in the source or the target language. Among the parallel chains, we detect those with differences in English and German. Our use of word alignments to map coreference structures between language is similar to the existing studies on annotation projection (e.g. Yarowsky et al., 2001) or specifically on multilingual coreference projection (Postolache et al., 2006; Ogrodniczuk, 2013; Grishina and Stede, 2015; Nov´ak, 2018). However, in contrast to annotation projection, we do not aim to produce any automatic annotations. Instead, we use automated methods to discover incongruences in the existing annotations produced manually on parallel texts. The cross-lingual variation in the chains is then analysed both quantitatively and qualitatively. We develop a typology of the incongruences encountered in ParCorFull, illustrated with corpus examples, and present empirical results on"
W19-2805,2005.iwslt-1.8,0,0.200591,"Missing"
W19-3801,W19-3816,0,0.253863,"nder Bias in Natural Language Processing, pages 1–7 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics with separate scores for masculine and feminine examples. To simplify evaluation, we did not disaggregate evaluation for this shared task, but instead encouraged fairness by not releasing the balance of masculine to feminine examples in the final evaluation data.1 The competition was run on Kaggle2 , a wellknown platform for competitive data science and machine learning projects with an active community of participants and support. 2.1 Attree (2019) Wang (2019) Abzaliev (2019) F1 96.2 95.7 95.4 Bias 0.99 0.99 0.99 Table 1: Performance of prize-winning submissions on the blind Kaggle evaluation set. logloss was the official task metric, and correlates well with F1 score, which was used in the original GAP work. 760 clean examples was dispersed in a larger set of 11,599 unlabeled examples to produce a set of 12,359 examples that competing systems had to rate. This augmentation was to discourage submissions based on manual labeling. We note many competing systems used the original GAP evaluation data4 as training data for this task, given that the two have the same fo"
W19-3801,W19-3812,0,0.113421,"all from individual contributors, while academic researchers worked in groups. This correlation is somewhat indicative of performance: individual contributors from industry won all three monetary prizes, and only one academic group featured in the top ten submissions. A possible factor in this was the concurrent timing of the competition with other conference deadlines. 5 https://github.com/allenai/allennlp/blob/ master/allennlp/modules/span_extractors/self_ attentive_span_extractor.py 3 Attree (2019) Wang (2019) Abzaliev (2019) Yang et al. (2019) Ionita et al. (2019)* Liu (2019) Chada (2019) Bao and Qiao (2019) Ionita et al. (2019)* Lois et al. (2019) Xu and Yang (2019) Place 1 2 3 4 5 7 9 14 22 46 67 logloss 0.13667 0.17289 0.18397 0.18498 0.19189 0.19473 0.20238 0.20758 0.22562 0.30151 0.39479 Members 1 1 1 4 1 1 1 2 4 3 2 Affiliation Industry Industry Industry Academic Other Industry Industry Academic Mixed Academic Academic Region USA Asia Europe Asia Africa USA USA Europe Mixed Europe USA Table 2: Teams with accepted system description papers. *Note the two teams placing 5 and 22 submitted a combined system description paper. Rank logloss Fine-tuning Attree (2019) Wang (2019) Abzaliev (2019) Ya"
W19-3801,W19-3809,0,0.0162363,"is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task was to encourage research in gender-fair models for NLP by providing a wel"
W19-3801,N19-1063,0,0.0205593,"titividad and the European Regional Development Fund and the Agencia Estatal de Investigaci´on, through the post-doctoral senior grant Ram´on y Cajal and by the Swedish Research Council through grant 2017-930. Gender It is encouraging to see submitted systems improve the gender gap so close to parity at 0.99, particularly as no special modeling strategies were required. Indeed, Abzaliev (2019) reported that a handcrafted pronoun gender feature had no impact. Moreover, Bao and Qiao (2019) report that BERT encodings show no significant gender bias on either WEAT (Caliskan et al., 2017) or SEAT (May et al., 2019). We look forward to studies considering potential biases in BERT across more tasks and dimensions of diversity. References The teams competing in the shared task made effective use of BERT in at least three distinct methods: fine-tuning, feature extraction, and masked language modeling. Many system papers noted the incredible power of the model (see, e.g. Attree (2019) for a good analysis), particularly when compared to hand-crafted features (Abzaliev, 2019). We also believe the widespread use of BERT is related to the low rate of external data usage, as it is easier for most teams to reuse a"
W19-3801,W19-3819,0,0.0589937,"issions were all from individual contributors, while academic researchers worked in groups. This correlation is somewhat indicative of performance: individual contributors from industry won all three monetary prizes, and only one academic group featured in the top ten submissions. A possible factor in this was the concurrent timing of the competition with other conference deadlines. 5 https://github.com/allenai/allennlp/blob/ master/allennlp/modules/span_extractors/self_ attentive_span_extractor.py 3 Attree (2019) Wang (2019) Abzaliev (2019) Yang et al. (2019) Ionita et al. (2019)* Liu (2019) Chada (2019) Bao and Qiao (2019) Ionita et al. (2019)* Lois et al. (2019) Xu and Yang (2019) Place 1 2 3 4 5 7 9 14 22 46 67 logloss 0.13667 0.17289 0.18397 0.18498 0.19189 0.19473 0.20238 0.20758 0.22562 0.30151 0.39479 Members 1 1 1 4 1 1 1 2 4 3 2 Affiliation Industry Industry Industry Academic Other Industry Industry Academic Mixed Academic Academic Region USA Asia Europe Asia Africa USA USA Europe Mixed Europe USA Table 2: Teams with accepted system description papers. *Note the two teams placing 5 and 22 submitted a combined system description paper. Rank logloss Fine-tuning Attree (2019) Wang (2019"
W19-3801,N19-1423,0,0.180018,"rom existing unbalanced datasets. The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), corefe"
W19-3801,D18-1302,0,0.0229876,"duction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task was to encourage research in gender-fair"
W19-3801,W19-3821,1,0.884179,"Missing"
W19-3801,N06-2015,0,0.228659,"Missing"
W19-3801,W19-3621,0,0.0381936,"use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natur"
W19-3801,D12-1071,0,0.0329911,". The different models built from BERT are summarized in Table 3. Eight of the eleven system descriptions used BERT via fine-tuning, the technique recommended in Devlin et al. (2019). To do this, the original GAP data release was used as a tuning set to learn a classifier on top of BERT to predict whether the target pronoun referred to Name A, Name B, or Neither. Abzaliev (2019) also made use of the available datasets for coreference resolution: OntoNotes 5.0 (Pradhan and Xue, 2009), WinoBias (Zhao et al., 2018), WinoGender (Rudinger et al., 2018), and the Definite Pronoun Resolution Dataset (Rahman and Ng, 2012). Given the multiple BERT models available, it was possible to learn multiple such classifiers; teams marked ensemble fine-tuned multiple base BERT models and ensembled their predictions, while teams marked single produced just one, from a BERT-Large variant. An alternative way to use BERT in NLP modeling is as a feature extractor. Teams using BERT in this capacity represented mention spans as input vectors to a neural structure (typically a linear structure, e.g. feed-forward network) that learned some sort of mention compatibility, via interaction or feature crossing. To derive mention-span"
W19-3801,N18-2002,0,0.0607398,"Missing"
W19-3801,Q19-1026,0,0.02881,"ering a wide diversity of geographic locations and affiliations, see Section 3.1. Table 1 lists results for the three prize-winning systems: Attree (2019), Wang (2019), and Abzaliev (2019). 3 All system descriptions were from teams who used BERT (Devlin et al., 2019), a method to create context-sensitive word embeddings by pretraining a deep self-attention neural network on a training objective optimizing for cloze word prediction and recognition of adjacent sentences. This is perhaps not surprising, given the recent success of BERT for modeling a wide range of NLP tasks (Tenney et al., 2019; Kwiatkowski et al., 2019) and the small amount of training data available for GAP resolution (which makes LM pretraining particularly attractive). The different models built from BERT are summarized in Table 3. Eight of the eleven system descriptions used BERT via fine-tuning, the technique recommended in Devlin et al. (2019). To do this, the original GAP data release was used as a tuning set to learn a classifier on top of BERT to predict whether the target pronoun referred to Name A, Name B, or Neither. Abzaliev (2019) also made use of the available datasets for coreference resolution: OntoNotes 5.0 (Pradhan and Xue"
W19-3801,D18-1334,1,0.824962,"tics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task was to encourage research in gender-fair models for NLP by providing a well-defined task that is known to be sensitive to gend"
W19-3801,W19-3818,0,0.0152269,"licity. Two other systems stood out as novel in their approach to the task: Chada (2019) reformulated GAP reference resolution as a question answering task, and Lois et al. (2019) used BERT in a third way, directly applying the masked language modeling task to predicting resolutions. Despite the scarcity of data for this challenge, there was little use of extra resources. Only two teams made use of the URL given in the example, with Attree (2019) using it only indirectly as part of a coreference heuristic fed into evidence pooling. Two teams augmented the GAP data by using name substitutions (Liu, 2019; Lois et al., 2019) and two automatically created extra examples of the minority label Neither (Attree, 2019; Bao and Qiao, 2019). 4 Discussion Running the GAP shared task has taught us many valuable things about reference, gender, and BERT models. Based on these, we make recommendations for future work expanding from this shared task into different languages and domains. GAP Given the incredibly strong performance of the submitted systems, it is tempting to ask whether GAP resolution is solved. We suggest the answer is no. Firstly, the shared task only tested one of the four original GAP set"
W19-3801,W19-3813,0,0.168496,"rkshop on Gender Bias in Natural Language Processing, pages 1–7 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics with separate scores for masculine and feminine examples. To simplify evaluation, we did not disaggregate evaluation for this shared task, but instead encouraged fairness by not releasing the balance of masculine to feminine examples in the final evaluation data.1 The competition was run on Kaggle2 , a wellknown platform for competitive data science and machine learning projects with an active community of participants and support. 2.1 Attree (2019) Wang (2019) Abzaliev (2019) F1 96.2 95.7 95.4 Bias 0.99 0.99 0.99 Table 1: Performance of prize-winning submissions on the blind Kaggle evaluation set. logloss was the official task metric, and correlates well with F1 score, which was used in the original GAP work. 760 clean examples was dispersed in a larger set of 11,599 unlabeled examples to produce a set of 12,359 examples that competing systems had to rate. This augmentation was to discourage submissions based on manual labeling. We note many competing systems used the original GAP evaluation data4 as training data for this task, given that the two"
W19-3801,W19-3811,1,0.859601,"Missing"
W19-3801,Q18-1042,1,0.937536,"near-human accuracy while achieving near gender-parity at 0.99, measured by the ratio between F1 scores on feminine and masculine examples. We are excited for future work extending this success to more languages, domains, and tasks. However, we especially note future work in algorithms which achieve fair outcomes given biased data, given the wealth of information from existing unbalanced datasets. The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An i"
W19-3801,W19-3814,0,0.0511122,"Missing"
W19-3801,N18-2003,0,0.139092,"traction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task wa"
W19-3801,W19-3815,0,0.0806747,"T in this capacity represented mention spans as input vectors to a neural structure (typically a linear structure, e.g. feed-forward network) that learned some sort of mention compatibility, via interaction or feature crossing. To derive mention-span representations from BERT subtoken encodings, Wang (2019) found that pooling using an attentionmediated process was more effective than simple mean-pooling; most teams pooled using AllenAI’s SelfAttentionSpanExtractor5 . An interesting finding was that certain BERT layers were more suitable for feature extraction than others (see Abzaliev (2019); Yang et al. (2019) for an exploration). The winning solution (Attree, 2019) used a Submissions In this section, we describe the diverse set of teams who competed in the shared task, and the systems they designed for the GAP challenge. We note effective use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, and ensembling. Despite very little modeling targeted at debiasing for gender, the submitted systems narrowed the gender gap to near parity at 0.99, while achieving remarkably strong performance. 3.1 Systems Teams We accepted ten system description papers, from 11 of the 263 teams"
