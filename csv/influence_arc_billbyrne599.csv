2020.acl-main.690,W19-6620,0,0.0171319,"iolence against women’, rather than gender-balanced professional entities. en-de en-es en-he Training 17.5M 10M 185K Gendered training 2.1M 1.1M 21.4K M:F 2.4 1.1 1.8 Test 3K 3K 1K Table 1: Parallel sentence counts. A gendered sentence pair has minimum one gendered stopword on the English side. M:F is ratio of male vs female gendered training sentences. For en-de and en-es we learn joint 32K BPE vocabularies on the training data (Sennrich et al., 2016). For en-he we use separate source and target vocabularies. The Hebrew vocabulary is a 2kmerge BPE vocabulary, following the recommendations of Ding et al. (2019) for smaller vocabularies when translating into lower-resource languages. For the en-he source vocabulary we experimented both with learning a new 32K vocabulary and with reusing the joint BPE vocabulary trained on the largest set – en-de – which lets us initialize the enhe system with the pre-trained en-de model. The latter resulted in higher BLEU and faster training. 3.2 Training and inference For all models we use a Transformer model (Vaswani et al., 2017) with the ‘base’ parameter settings given in Tensor2Tensor (Vaswani et al., 2018). We train baselines to validation set BLEU convergence"
2020.acl-main.690,W19-3821,0,0.1574,"Missing"
2020.acl-main.690,W17-4713,0,0.0321945,"ta augmentation where for each gendered sentence in the data a gender-swapped equivalent is added. Zhao et al. (2018) show improvement in coreference resolution for English using counterfactual data. Zmigrod et al. (2019) demonstrate a more complicated scheme for gender-inflected languages. However, their system focuses on words in isolation, and is difficult to apply to co-reference and conjunction situations with more than one term to swap, reducing its practicality for large MT datasets. Recent work recognizes that NMT can be adapted to domains with desired attributes using small datasets (Farajian et al., 2017; Michel and Neubig, 2018). Our choice of a small, trusted dataset for adaptation specifically to a debiased domain connects also to recent work in data selection by Wang et al. (2018), in which fine-tuning on less noisy data reduces translation noise. Similarly we propose fine-tuning on less biased data to reduce gender bias in translations. This is loosely the inverse of the approach described by Park et al. (2018) for monolingual abusive language detection, which pre-trains on a larger, less biased set. 2 Gender bias in machine translation We focus on translating coreference sentences conta"
2020.acl-main.690,W19-3621,0,0.0323574,"eotypical gender roles. For example, mentions of male doctors are more reliably translated than those of male nurses (Sun et al., 2019; Prates et al., 2019). Recent approaches to the bias problem in NLP have involved training from scratch on artificially gender-balanced versions of the original dataset (Zhao et al., 2018; Zmigrod et al., 2019) or with debiased embeddings (Escud´e Font and Costa-juss`a, 2019; Bolukbasi et al., 2016). While these approaches may be effective, training from scratch is inefficient and gender-balancing embeddings or large parallel datasets are challenging problems (Gonen and Goldberg, 2019). Instead we propose treating gender debiasing as a domain adaptation problem, since NMT models can very quickly adapt to a new domain (Freitag and Al-Onaizan, 2016). To the best of our knowledge this work is the first to attempt NMT bias reduction by fine-tuning, rather than retraining. We consider three aspects of this adaptation problem: creating less biased adaptation data, parameter adaptation using this data, and inference with the debiased models produced by adaptation. Regarding data, we suggest that a small, trusted gender-balanced set could allow more efficient and effective gender d"
2020.acl-main.690,P16-2096,0,0.10079,"tion. During inference we propose a latticerescoring scheme which outperforms all systems evaluated in Stanovsky et al. (2019) on WinoMT with no degradation of general test set BLEU, and we show this scheme can be applied to remove gender bias in the output of ‘black box‘ online commercial MT systems. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability. 1 Introduction As language processing tools become more prevalent concern has grown over their susceptibility to social biases and their potential to propagate bias (Hovy and Spruit, 2016; Sun et al., 2019). Natural language training data inevitably reflects biases present in our society. For example, gender bias manifests itself in training data which features more examples of men than of women. Tools trained on such data will then exhibit or even amplify the biases (Zhao et al., 2017). Gender bias is a particularly important problem for Neural Machine Translation (NMT) into genderinflected languages. An over-prevalence of some gendered forms in the training data leads to translations with identifiable errors (Stanovsky et al., 2019). Translations are better for sentences inv"
2020.acl-main.690,P18-2050,0,0.0253255,"or each gendered sentence in the data a gender-swapped equivalent is added. Zhao et al. (2018) show improvement in coreference resolution for English using counterfactual data. Zmigrod et al. (2019) demonstrate a more complicated scheme for gender-inflected languages. However, their system focuses on words in isolation, and is difficult to apply to co-reference and conjunction situations with more than one term to swap, reducing its practicality for large MT datasets. Recent work recognizes that NMT can be adapted to domains with desired attributes using small datasets (Farajian et al., 2017; Michel and Neubig, 2018). Our choice of a small, trusted dataset for adaptation specifically to a debiased domain connects also to recent work in data selection by Wang et al. (2018), in which fine-tuning on less noisy data reduces translation noise. Similarly we propose fine-tuning on less biased data to reduce gender bias in translations. This is loosely the inverse of the approach described by Park et al. (2018) for monolingual abusive language detection, which pre-trains on a larger, less biased set. 2 Gender bias in machine translation We focus on translating coreference sentences containing professions as a rep"
2020.acl-main.690,W19-3807,0,0.07444,"ior to training, and has the added benefit of preserving privacy, since no access to the original data or knowledge of its contents is required. As evidence, in section 3.4.5, we show this scheme can be applied to remove gender bias in the output of ‘black box‘ online commercial MT systems. 1.1 Related work Vanmassenhove et al. (2018) treat gender as a domain for machine translation, training from scratch by augmenting Europarl data with a tag indicating the speaker’s gender. This does not inherently remove gender bias from the system but allows control over the translation hypothesis gender. Moryossef et al. (2019) similarly prepend a short phrase at inference time which acts as a gender domain label for the entire sentence. These approaches are not directly applicable to text which may have more than one gendered entity per sentence, as in coreference resolution tasks. Escud´e Font and Costa-juss`a (2019) train NMT models from scratch with debiased word embeddings. They demonstrate improved performance on an English-Spanish occupations task with a single profession and pronoun per sentence. We assess our fine-tuning approaches on the WinoMT coreference set, with two entities to resolve per sentence. Fo"
2020.acl-main.690,P02-1040,0,0.107589,"Missing"
2020.acl-main.690,D18-1302,0,0.0280957,"than one term to swap, reducing its practicality for large MT datasets. Recent work recognizes that NMT can be adapted to domains with desired attributes using small datasets (Farajian et al., 2017; Michel and Neubig, 2018). Our choice of a small, trusted dataset for adaptation specifically to a debiased domain connects also to recent work in data selection by Wang et al. (2018), in which fine-tuning on less noisy data reduces translation noise. Similarly we propose fine-tuning on less biased data to reduce gender bias in translations. This is loosely the inverse of the approach described by Park et al. (2018) for monolingual abusive language detection, which pre-trains on a larger, less biased set. 2 Gender bias in machine translation We focus on translating coreference sentences containing professions as a representative subset of the gender bias problem. This follows much recent work on NLP gender bias (Rudinger et al., 2018; Zhao et al., 2018; Zmigrod et al., 2019) including the release of WinoMT, a relevant challenge set for NMT (Stanovsky et al., 2019). A sentence that highlights gender bias is: 7725 The doctor told the nurse that she had been busy. A human translator carrying out coreference"
2020.acl-main.690,W18-6319,0,0.0273626,". The latter resulted in higher BLEU and faster training. 3.2 Training and inference For all models we use a Transformer model (Vaswani et al., 2017) with the ‘base’ parameter settings given in Tensor2Tensor (Vaswani et al., 2018). We train baselines to validation set BLEU convergence on one GPU, delaying gradient updates by factor 4 to simulate 4 GPUs (Saunders et al., 2018). During fine-tuning training is continued without learning rate resetting. Normal and lattice-constrained decoding is via SGNMT3 with beam size 4. BLEU scores are calculated for cased, detokenized output using SacreBLEU (Post, 2018) 3.3 Lattice rescoring with debiased models For lattice rescoring we require a transducer T containing gender-inflected forms of words in the target vocabulary. To obtain the vocabulary for German we use all unique words in the full target training dataset. For Spanish and Hebrew, which have smaller and less diverse training sets, we use 2018 7729 3 https://github.com/ucam-smt/sgnmt OpenSubtitles word lists4 . We then use DEMorphy (Altinok, 2018) for German, spaCy (Honnibal and Montani, 2017) for Spanish and the small set of gendered suffixes for Hebrew (Schwarzwald, 1982) to approximately lem"
2020.acl-main.690,N18-2002,0,0.2088,"Missing"
2020.acl-main.690,P18-2051,1,0.91876,"Missing"
2020.acl-main.690,P19-1022,1,0.857698,"Missing"
2020.acl-main.690,P16-1162,0,0.0864622,"e balanced gender ratio than the other pairs, examining the data shows this stems largely from sections of the UNCorpus containing phrases like ‘empower women’ and ‘violence against women’, rather than gender-balanced professional entities. en-de en-es en-he Training 17.5M 10M 185K Gendered training 2.1M 1.1M 21.4K M:F 2.4 1.1 1.8 Test 3K 3K 1K Table 1: Parallel sentence counts. A gendered sentence pair has minimum one gendered stopword on the English side. M:F is ratio of male vs female gendered training sentences. For en-de and en-es we learn joint 32K BPE vocabularies on the training data (Sennrich et al., 2016). For en-he we use separate source and target vocabularies. The Hebrew vocabulary is a 2kmerge BPE vocabulary, following the recommendations of Ding et al. (2019) for smaller vocabularies when translating into lower-resource languages. For the en-he source vocabulary we experimented both with learning a new 32K vocabulary and with reusing the joint BPE vocabulary trained on the largest set – en-de – which lets us initialize the enhe system with the pre-trained en-de model. The latter resulted in higher BLEU and faster training. 3.2 Training and inference For all models we use a Transformer mod"
2020.acl-main.690,N19-1406,1,0.850044,"d swapping script are provided by the authors of Zhao et al. (2018) at https://github.com/ uclanlp/corefBias (c) Gender-inflected search space constructed from the biased hypothesis ‘der Arzt’. Projection of the composition YB ◦ T contains paths with differently-gendered inflections of the original biased hypothesis. This lattice can now be rescored by a debiased model. Figure 2: Finite State Transducers for lattice rescoring. An alternative approach for avoiding catastrophic forgetting takes inspiration from lattice rescoring for NMT (Stahlberg et al., 2016) and Grammatical Error Correction (Stahlberg et al., 2019). We assume we have two NMT models. With one we decode fluent translations which contain gender bias (B). For the one-best hypothesis we would translate: yB = argmaxy pB (y|x) (2) The other model has undergone debiasing (DB) at a cost to translation performance, producing: 7728 yDB = argmaxy pDB (y|x) (3) We construct a flower transducer T that maps each word in the target language’s vocabulary to itself, as well as to other forms of the same word with different gender inflections (Figure 2a). We also construct YB , a lattice with one path representing the biased but fluent hypothesis yB (Figu"
2020.acl-main.690,P16-2049,1,0.837483,"es for rescoring with debiased models The stopword list and swapping script are provided by the authors of Zhao et al. (2018) at https://github.com/ uclanlp/corefBias (c) Gender-inflected search space constructed from the biased hypothesis ‘der Arzt’. Projection of the composition YB ◦ T contains paths with differently-gendered inflections of the original biased hypothesis. This lattice can now be rescored by a debiased model. Figure 2: Finite State Transducers for lattice rescoring. An alternative approach for avoiding catastrophic forgetting takes inspiration from lattice rescoring for NMT (Stahlberg et al., 2016) and Grammatical Error Correction (Stahlberg et al., 2019). We assume we have two NMT models. With one we decode fluent translations which contain gender bias (B). For the one-best hypothesis we would translate: yB = argmaxy pB (y|x) (2) The other model has undergone debiasing (DB) at a cost to translation performance, producing: 7728 yDB = argmaxy pDB (y|x) (3) We construct a flower transducer T that maps each word in the target language’s vocabulary to itself, as well as to other forms of the same word with different gender inflections (Figure 2a). We also construct YB , a lattice with one p"
2020.acl-main.690,P19-1164,0,0.157794,"set, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address both in adaptation and in inference. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. During inference we propose a latticerescoring scheme which outperforms all systems evaluated in Stanovsky et al. (2019) on WinoMT with no degradation of general test set BLEU, and we show this scheme can be applied to remove gender bias in the output of ‘black box‘ online commercial MT systems. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability. 1 Introduction As language processing tools become more prevalent concern has grown over their susceptibility to social biases and their potential to propagate bias (Hovy and Spruit, 2016; Sun et al., 2019). Natural language training data inevitably reflects biases present in our society. Fo"
2020.acl-main.690,P19-1159,0,0.133974,"we propose a latticerescoring scheme which outperforms all systems evaluated in Stanovsky et al. (2019) on WinoMT with no degradation of general test set BLEU, and we show this scheme can be applied to remove gender bias in the output of ‘black box‘ online commercial MT systems. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability. 1 Introduction As language processing tools become more prevalent concern has grown over their susceptibility to social biases and their potential to propagate bias (Hovy and Spruit, 2016; Sun et al., 2019). Natural language training data inevitably reflects biases present in our society. For example, gender bias manifests itself in training data which features more examples of men than of women. Tools trained on such data will then exhibit or even amplify the biases (Zhao et al., 2017). Gender bias is a particularly important problem for Neural Machine Translation (NMT) into genderinflected languages. An over-prevalence of some gendered forms in the training data leads to translations with identifiable errors (Stanovsky et al., 2019). Translations are better for sentences involving men and for"
2020.acl-main.690,N19-1209,0,0.0226432,"s twice the size of the other counterfactual sets. Comparing performance in adaptation of FTrans swapped and FTrans original lets us distinguish between the effects of gender-swapping and of obtaining target sentences from forward-translation. 2.3 Regularized training Regularized training is a well-established approach for minimizing catastrophic forgetting during domain adaptation of machine translation (Barone et al., 2017). One effective form is Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) which in NMT has been shown to maintain or even improve original domain performance (Thompson et al., 2019; Saunders et al., 2019). In EWC a 2 (a) A subset of flower transducer T . T maps vocabulary to itself as well as to differently-gendered inflections. (b) Acceptor YB representing the biased first-pass translation yB for source fragment ’the doctor’. The German hypothesis has the male form. Debiasing while maintaining general translation performance Fine-tuning a converged neural network on data from a distinct domain typically leads to catastrophic forgetting of the original domain (French, 1999). We wish to adapt to the gender-balanced domain without losing general translation performance. T"
2020.acl-main.690,D18-1334,0,0.132082,"e source of bias in a dataset is both obvious and easily adjusted. We show that debiasing a full NMT dataset is difficult, and suggest alternative efficient and effective approaches for debiasing a model after it is trained. This avoids the need to identify and remove all possible biases prior to training, and has the added benefit of preserving privacy, since no access to the original data or knowledge of its contents is required. As evidence, in section 3.4.5, we show this scheme can be applied to remove gender bias in the output of ‘black box‘ online commercial MT systems. 1.1 Related work Vanmassenhove et al. (2018) treat gender as a domain for machine translation, training from scratch by augmenting Europarl data with a tag indicating the speaker’s gender. This does not inherently remove gender bias from the system but allows control over the translation hypothesis gender. Moryossef et al. (2019) similarly prepend a short phrase at inference time which acts as a gender domain label for the entire sentence. These approaches are not directly applicable to text which may have more than one gendered entity per sentence, as in coreference resolution tasks. Escud´e Font and Costa-juss`a (2019) train NMT model"
2020.acl-main.690,W18-1819,0,0.0298149,"a 2kmerge BPE vocabulary, following the recommendations of Ding et al. (2019) for smaller vocabularies when translating into lower-resource languages. For the en-he source vocabulary we experimented both with learning a new 32K vocabulary and with reusing the joint BPE vocabulary trained on the largest set – en-de – which lets us initialize the enhe system with the pre-trained en-de model. The latter resulted in higher BLEU and faster training. 3.2 Training and inference For all models we use a Transformer model (Vaswani et al., 2017) with the ‘base’ parameter settings given in Tensor2Tensor (Vaswani et al., 2018). We train baselines to validation set BLEU convergence on one GPU, delaying gradient updates by factor 4 to simulate 4 GPUs (Saunders et al., 2018). During fine-tuning training is continued without learning rate resetting. Normal and lattice-constrained decoding is via SGNMT3 with beam size 4. BLEU scores are calculated for cased, detokenized output using SacreBLEU (Post, 2018) 3.3 Lattice rescoring with debiased models For lattice rescoring we require a transducer T containing gender-inflected forms of words in the target vocabulary. To obtain the vocabulary for German we use all unique word"
2020.acl-main.690,W18-6314,0,0.0223459,"actual data. Zmigrod et al. (2019) demonstrate a more complicated scheme for gender-inflected languages. However, their system focuses on words in isolation, and is difficult to apply to co-reference and conjunction situations with more than one term to swap, reducing its practicality for large MT datasets. Recent work recognizes that NMT can be adapted to domains with desired attributes using small datasets (Farajian et al., 2017; Michel and Neubig, 2018). Our choice of a small, trusted dataset for adaptation specifically to a debiased domain connects also to recent work in data selection by Wang et al. (2018), in which fine-tuning on less noisy data reduces translation noise. Similarly we propose fine-tuning on less biased data to reduce gender bias in translations. This is loosely the inverse of the approach described by Park et al. (2018) for monolingual abusive language detection, which pre-trains on a larger, less biased set. 2 Gender bias in machine translation We focus on translating coreference sentences containing professions as a representative subset of the gender bias problem. This follows much recent work on NLP gender bias (Rudinger et al., 2018; Zhao et al., 2018; Zmigrod et al., 201"
2020.acl-main.690,D17-1323,0,0.229559,"emonstrate our approach translating from English into three languages with varied linguistic properties and data availability. 1 Introduction As language processing tools become more prevalent concern has grown over their susceptibility to social biases and their potential to propagate bias (Hovy and Spruit, 2016; Sun et al., 2019). Natural language training data inevitably reflects biases present in our society. For example, gender bias manifests itself in training data which features more examples of men than of women. Tools trained on such data will then exhibit or even amplify the biases (Zhao et al., 2017). Gender bias is a particularly important problem for Neural Machine Translation (NMT) into genderinflected languages. An over-prevalence of some gendered forms in the training data leads to translations with identifiable errors (Stanovsky et al., 2019). Translations are better for sentences involving men and for sentences containing stereotypical gender roles. For example, mentions of male doctors are more reliably translated than those of male nurses (Sun et al., 2019; Prates et al., 2019). Recent approaches to the bias problem in NLP have involved training from scratch on artificially gende"
2020.acl-main.690,N18-2003,0,0.307425,"blem for Neural Machine Translation (NMT) into genderinflected languages. An over-prevalence of some gendered forms in the training data leads to translations with identifiable errors (Stanovsky et al., 2019). Translations are better for sentences involving men and for sentences containing stereotypical gender roles. For example, mentions of male doctors are more reliably translated than those of male nurses (Sun et al., 2019; Prates et al., 2019). Recent approaches to the bias problem in NLP have involved training from scratch on artificially gender-balanced versions of the original dataset (Zhao et al., 2018; Zmigrod et al., 2019) or with debiased embeddings (Escud´e Font and Costa-juss`a, 2019; Bolukbasi et al., 2016). While these approaches may be effective, training from scratch is inefficient and gender-balancing embeddings or large parallel datasets are challenging problems (Gonen and Goldberg, 2019). Instead we propose treating gender debiasing as a domain adaptation problem, since NMT models can very quickly adapt to a new domain (Freitag and Al-Onaizan, 2016). To the best of our knowledge this work is the first to attempt NMT bias reduction by fine-tuning, rather than retraining. We consi"
2020.acl-main.690,L16-1561,0,0.0224238,"Missing"
2020.acl-main.690,P19-1161,0,0.365757,"hine Translation (NMT) into genderinflected languages. An over-prevalence of some gendered forms in the training data leads to translations with identifiable errors (Stanovsky et al., 2019). Translations are better for sentences involving men and for sentences containing stereotypical gender roles. For example, mentions of male doctors are more reliably translated than those of male nurses (Sun et al., 2019; Prates et al., 2019). Recent approaches to the bias problem in NLP have involved training from scratch on artificially gender-balanced versions of the original dataset (Zhao et al., 2018; Zmigrod et al., 2019) or with debiased embeddings (Escud´e Font and Costa-juss`a, 2019; Bolukbasi et al., 2016). While these approaches may be effective, training from scratch is inefficient and gender-balancing embeddings or large parallel datasets are challenging problems (Gonen and Goldberg, 2019). Instead we propose treating gender debiasing as a domain adaptation problem, since NMT models can very quickly adapt to a new domain (Freitag and Al-Onaizan, 2016). To the best of our knowledge this work is the first to attempt NMT bias reduction by fine-tuning, rather than retraining. We consider three aspects of th"
2020.acl-main.690,W19-5301,0,\N,Missing
2020.acl-main.693,W19-5301,0,0.0413186,"Missing"
2020.acl-main.693,N12-1067,0,0.234937,"o-documents where sentences are assigned randomly to a mini-batch, and true document context where all sentences in the batch are from the same document. We finally apply our scheme to supervised Grammatical Error Correction, for which using neural models is becoming increasingly popular (Xie et al., 2016; Sakaguchi et al., 2017; Stahlberg et al., 2019). 7764 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7764–7770 c July 5 - 10, 2020. 2020 Association for Computational Linguistics We show gains in GEC metrics GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). 1.1 Related Work Minimum Error Rate Training was introduced for phrase-based SMT with document-level BLEU (Och, 2003). Shen et al. (2016) extend these ideas to NMT, using expected minimum risk at the sequence level with an sBLEU cost for end-to-end NMT training. Edunov et al. (2018) explore random and beam sampling for NMT sequence-MRT, as well as other sequence-level training losses. Related developments in NMT include combined reinforcement-learning/cross-entropy approaches such as MIXER (Ranzato et al., 2016), which itself has origins in the REINFORCE algorithm described by Williams (1992"
2020.acl-main.693,W13-1703,0,0.0333022,"model never receives a strong enough signal for effective training. We fine-tune on old WMT news task test sets (2008-2016) in two settings. With random batches sentences from different documents are shuffled randomly into mini-batches. In this case doc-MRT metrics are over pseudo-documents. With document batches each batch contains only sentences from one document, and doc-MRT uses true document context. We use the same sampling temperatures and the same risk sharpness factors for both forms of MRT for each experiment. For Grammatical Error Correction (GEC) we train on sentences from NUCLE (Dahlmeier et al., 2013) and Lang-8 Learner English (Mizumoto et al., 2012) with at least one correction, a total of 660K sentences. We evaluate on the JFLEG (Napoles et al., 2017) and CoNLL 2014 (Ng et al., 2014) sets. For GEC experiments we use random batching only. For all models we use a Transformer model (Vaswani et al., 2017) with the ‘base’ Tensor2Tensor parameters (Vaswani et al., 2018). We train to validation set BLEU convergence on a single GPU. The batch size for baselines and MLE is 4096 tokens. For MRT, where each sentence in the batch is sampled N times, we reduce batch size by N while delaying gradient"
2020.acl-main.693,N18-1033,0,0.137499,"ics than with sequence metrics. We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations. 1 Introduction Neural Machine Translation (NMT) research has explored token-level likelihood functions (Sutskever et al., 2014; Bahdanau et al., 2015) and sequence-level objectives inspired by reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2016) or expected Minimum Risk Training (MRT) (Shen et al., 2016). A typical sequence objective in these cases is based on sentence-level BLEU (sBLEU) (Edunov et al., 2018). However ∗ Now at Google sBLEU, even if aggregated over sentences, is only an approximation of the desired metric, documentlevel BLEU. Beyond translation, many metrics for natural language tasks do not have robust sentencelevel approximations. A logical progression is the extension of sequence-level NMT training objectives to include context from outside the sentence. Document-based NMT, by contrast, aims to use out-of-sentence context to improve translation. Recent research explores lexical consistency by providing additional sentences during training (Maruf et al., 2019; Voita et al., 2018,"
2020.acl-main.693,W17-3207,0,0.0256224,"e similar and therefore give less noisy gradient estimates. Both seq-MRT and doc-MRT improve over the baseline with random sampling and N = 8. We also explore MRT at N = 4, with batch size adjusted as described in section 3 for the same effective batch size per update, and with fewer training steps such that the model ‘sees’ a similar proportion of the overall dataset. We do not report beam sampling results as early experiments indicate beam sampling gives similarly poor results for both seqMRT and doc-MRT. This may be because beam search produces insufficiently diverse samples for this task (Freitag and Al-Onaizan, 2017). Sequence-MRT gives a 0.8 BLEU gain over the baseline with both batching schemes using N = 8 samples, but starts to degrade the baseline with N = 4 samples. With document batches and N = 8 Doc-MRT (ordered) outperforms seq-MRT by a further 0.4 BLEU. With N = 4 doc-MRT (ordered) still achieves a 0.7 BLEU improvement over the baseline, or a 0.8 BLEU improvement over seq-MRT. We suggest therefore that doc-MRT (ordered) may be a computationally more efficient alternative to seq-MRT when large sample counts are not practical. For contrast with the ordered document sampling approach of Section 2.3,"
2020.acl-main.693,N19-1313,0,0.0218412,"evel BLEU (sBLEU) (Edunov et al., 2018). However ∗ Now at Google sBLEU, even if aggregated over sentences, is only an approximation of the desired metric, documentlevel BLEU. Beyond translation, many metrics for natural language tasks do not have robust sentencelevel approximations. A logical progression is the extension of sequence-level NMT training objectives to include context from outside the sentence. Document-based NMT, by contrast, aims to use out-of-sentence context to improve translation. Recent research explores lexical consistency by providing additional sentences during training (Maruf et al., 2019; Voita et al., 2018, 2019) or inference (Voita et al., 2019; Stahlberg et al., 2019), potentially with adjustments to model architecture. However, to the best of our knowledge, no attempt has been made to extend sequence-level neural training objectives to include document-level reward functions. This is despite document-level BLEU being arguably the most common NMT metric, and being the function originally optimised by Minimum Error Rate Training (MERT) for Statistical Machine Translation (SMT) (Och, 2003). We propose merging lines of research on training objectives and document-level transl"
2020.acl-main.693,C12-2084,0,0.067393,"Missing"
2020.acl-main.693,P15-2097,0,0.137544,"uments. We consider both pseudo-documents where sentences are assigned randomly to a mini-batch, and true document context where all sentences in the batch are from the same document. We finally apply our scheme to supervised Grammatical Error Correction, for which using neural models is becoming increasingly popular (Xie et al., 2016; Sakaguchi et al., 2017; Stahlberg et al., 2019). 7764 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7764–7770 c July 5 - 10, 2020. 2020 Association for Computational Linguistics We show gains in GEC metrics GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). 1.1 Related Work Minimum Error Rate Training was introduced for phrase-based SMT with document-level BLEU (Och, 2003). Shen et al. (2016) extend these ideas to NMT, using expected minimum risk at the sequence level with an sBLEU cost for end-to-end NMT training. Edunov et al. (2018) explore random and beam sampling for NMT sequence-MRT, as well as other sequence-level training losses. Related developments in NMT include combined reinforcement-learning/cross-entropy approaches such as MIXER (Ranzato et al., 2016), which itself has origins in the REINFORCE algor"
2020.acl-main.693,E17-2037,0,0.0144814,"hes sentences from different documents are shuffled randomly into mini-batches. In this case doc-MRT metrics are over pseudo-documents. With document batches each batch contains only sentences from one document, and doc-MRT uses true document context. We use the same sampling temperatures and the same risk sharpness factors for both forms of MRT for each experiment. For Grammatical Error Correction (GEC) we train on sentences from NUCLE (Dahlmeier et al., 2013) and Lang-8 Learner English (Mizumoto et al., 2012) with at least one correction, a total of 660K sentences. We evaluate on the JFLEG (Napoles et al., 2017) and CoNLL 2014 (Ng et al., 2014) sets. For GEC experiments we use random batching only. For all models we use a Transformer model (Vaswani et al., 2017) with the ‘base’ Tensor2Tensor parameters (Vaswani et al., 2018). We train to validation set BLEU convergence on a single GPU. The batch size for baselines and MLE is 4096 tokens. For MRT, where each sentence in the batch is sampled N times, we reduce batch size by N while delaying gradient updates by the same factor to keep the effective batch size constant (Saunders et al., 2018). At inference time we decode using beam size 4. All BLEU score"
2020.acl-main.693,W14-1701,0,0.0264131,"are shuffled randomly into mini-batches. In this case doc-MRT metrics are over pseudo-documents. With document batches each batch contains only sentences from one document, and doc-MRT uses true document context. We use the same sampling temperatures and the same risk sharpness factors for both forms of MRT for each experiment. For Grammatical Error Correction (GEC) we train on sentences from NUCLE (Dahlmeier et al., 2013) and Lang-8 Learner English (Mizumoto et al., 2012) with at least one correction, a total of 660K sentences. We evaluate on the JFLEG (Napoles et al., 2017) and CoNLL 2014 (Ng et al., 2014) sets. For GEC experiments we use random batching only. For all models we use a Transformer model (Vaswani et al., 2017) with the ‘base’ Tensor2Tensor parameters (Vaswani et al., 2018). We train to validation set BLEU convergence on a single GPU. The batch size for baselines and MLE is 4096 tokens. For MRT, where each sentence in the batch is sampled N times, we reduce batch size by N while delaying gradient updates by the same factor to keep the effective batch size constant (Saunders et al., 2018). At inference time we decode using beam size 4. All BLEU scores 7766 Model are for cased, detok"
2020.acl-main.693,P03-1021,0,0.370031,"xplores lexical consistency by providing additional sentences during training (Maruf et al., 2019; Voita et al., 2018, 2019) or inference (Voita et al., 2019; Stahlberg et al., 2019), potentially with adjustments to model architecture. However, to the best of our knowledge, no attempt has been made to extend sequence-level neural training objectives to include document-level reward functions. This is despite document-level BLEU being arguably the most common NMT metric, and being the function originally optimised by Minimum Error Rate Training (MERT) for Statistical Machine Translation (SMT) (Och, 2003). We propose merging lines of research on training objectives and document-level translation. We achieve this by presenting a document-level approach to sequence-level objectives which brings the training objective closer to the actual evaluation metric, using MRT as a representative example. We demonstrate MRT under document-level BLEU as well as Translation Edit Rate (TER) (Snover, 2006), which while decomposable to sentence level is less noisy when used over documents. We consider both pseudo-documents where sentences are assigned randomly to a mini-batch, and true document context where al"
2020.acl-main.693,W18-6319,0,0.0605853,"Missing"
2020.acl-main.693,C04-1072,0,0.0326349,"n practically experiment with, since previous work finds MRT performance increasing steadily with more samples per sentence (Shen et al., 2016). That we see improvements with so few samples is in contrast to previous work which finds BLEU gains only with 20 or more samples per sentence for sequence-MRT (Shen et al., 2016; Edunov et al., 2018). However, we find that document-MRT allows improvements with far fewer samples, perhaps because the aggregation of scores over sentences in a context increases robustness to variation in individual samples. Relatedly, we find that add-one BLEU smoothing (Lin and Och, 2004) is required for sequenceMRT as in Shen et al. (2016). However we find that doc-MRT can achieve good results without smoothing, perhaps because n-gram precisions are far less likely to be 0 when calculated over a document. 3.2 Baseline MLE Seq-MRT Doc-MRT (ordered) MRT for NMT Model Baseline MLE Seq-MRT Doc-MRT (random) Doc-MRT (ordered) Random batches Document batches 42.7 40.0 41.0 N =4 N =8 N =4 N =8 42.6 43.5 42.6 43.5 41.7∗ 43.1∗ 43.1 43.0 43.4 43.7 43.4 43.9 Table 1: BLEU on en-de after MLE and MRT under 1−sBLEU (seq-MRT) and 1−doc BLEU (doc-MRT). Results indicated by ∗ are averages over"
2020.acl-main.693,P18-2051,1,0.915092,"Missing"
2020.acl-main.693,P16-1162,0,0.0647833,"ultiple sampled documents, giving it undue weight. Instead we propose creating N documents by first ordering samples for each sentence (e.g. by sBLEU), then creating the nth sample document Yn by concatenating the nth sample from each sentence. This gives a set of N diverse documents sampled from N S possibilities. We expect the sampled documents to be diverse in contents, since a 3 Experiments We report on English-German NMT. We initialize with a baseline trained on 17.5M sentence pairs from WMT19 news task datasets (Barrault et al., 2019), on which we learn a 32K-merge joint BPE vocabulary (Sennrich et al., 2016). We validate on newstest2017, and evaluate on newstest2018. We apply MRT only during fine-tuning, following previous work (Edunov et al., 2018; Shen et al., 2016). In early experiments, we found that training from scratch with discriminative objectives (sequence- or document-based) is ineffective. We suspect samples produced early in training are so unlike the references that the model never receives a strong enough signal for effective training. We fine-tune on old WMT news task test sets (2008-2016) in two settings. With random batches sentences from different documents are shuffled randoml"
2020.acl-main.693,D19-1081,0,0.0779233,"Missing"
2020.acl-main.693,P18-1117,0,0.0251095,"dunov et al., 2018). However ∗ Now at Google sBLEU, even if aggregated over sentences, is only an approximation of the desired metric, documentlevel BLEU. Beyond translation, many metrics for natural language tasks do not have robust sentencelevel approximations. A logical progression is the extension of sequence-level NMT training objectives to include context from outside the sentence. Document-based NMT, by contrast, aims to use out-of-sentence context to improve translation. Recent research explores lexical consistency by providing additional sentences during training (Maruf et al., 2019; Voita et al., 2018, 2019) or inference (Voita et al., 2019; Stahlberg et al., 2019), potentially with adjustments to model architecture. However, to the best of our knowledge, no attempt has been made to extend sequence-level neural training objectives to include document-level reward functions. This is despite document-level BLEU being arguably the most common NMT metric, and being the function originally optimised by Minimum Error Rate Training (MERT) for Statistical Machine Translation (SMT) (Och, 2003). We propose merging lines of research on training objectives and document-level translation. We achieve th"
2020.acl-main.693,P16-1159,0,0.346796,"MRT and maximum-likelihood training. We demonstrate that training is more robust for document-level metrics than with sequence metrics. We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations. 1 Introduction Neural Machine Translation (NMT) research has explored token-level likelihood functions (Sutskever et al., 2014; Bahdanau et al., 2015) and sequence-level objectives inspired by reinforcement learning (Ranzato et al., 2016; Bahdanau et al., 2016) or expected Minimum Risk Training (MRT) (Shen et al., 2016). A typical sequence objective in these cases is based on sentence-level BLEU (sBLEU) (Edunov et al., 2018). However ∗ Now at Google sBLEU, even if aggregated over sentences, is only an approximation of the desired metric, documentlevel BLEU. Beyond translation, many metrics for natural language tasks do not have robust sentencelevel approximations. A logical progression is the extension of sequence-level NMT training objectives to include context from outside the sentence. Document-based NMT, by contrast, aims to use out-of-sentence context to improve translation. Recent research explores lex"
2020.acl-main.693,2006.amta-papers.25,0,0.0537986,"tions. This is despite document-level BLEU being arguably the most common NMT metric, and being the function originally optimised by Minimum Error Rate Training (MERT) for Statistical Machine Translation (SMT) (Och, 2003). We propose merging lines of research on training objectives and document-level translation. We achieve this by presenting a document-level approach to sequence-level objectives which brings the training objective closer to the actual evaluation metric, using MRT as a representative example. We demonstrate MRT under document-level BLEU as well as Translation Edit Rate (TER) (Snover, 2006), which while decomposable to sentence level is less noisy when used over documents. We consider both pseudo-documents where sentences are assigned randomly to a mini-batch, and true document context where all sentences in the batch are from the same document. We finally apply our scheme to supervised Grammatical Error Correction, for which using neural models is becoming increasingly popular (Xie et al., 2016; Sakaguchi et al., 2017; Stahlberg et al., 2019). 7764 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7764–7770 c July 5 - 10, 2020. 2020"
2020.acl-main.693,N19-1406,1,0.881406,"aggregated over sentences, is only an approximation of the desired metric, documentlevel BLEU. Beyond translation, many metrics for natural language tasks do not have robust sentencelevel approximations. A logical progression is the extension of sequence-level NMT training objectives to include context from outside the sentence. Document-based NMT, by contrast, aims to use out-of-sentence context to improve translation. Recent research explores lexical consistency by providing additional sentences during training (Maruf et al., 2019; Voita et al., 2018, 2019) or inference (Voita et al., 2019; Stahlberg et al., 2019), potentially with adjustments to model architecture. However, to the best of our knowledge, no attempt has been made to extend sequence-level neural training objectives to include document-level reward functions. This is despite document-level BLEU being arguably the most common NMT metric, and being the function originally optimised by Minimum Error Rate Training (MERT) for Statistical Machine Translation (SMT) (Och, 2003). We propose merging lines of research on training objectives and document-level translation. We achieve this by presenting a document-level approach to sequence-level obje"
2020.acl-main.693,W18-1819,0,0.034519,"oc-MRT uses true document context. We use the same sampling temperatures and the same risk sharpness factors for both forms of MRT for each experiment. For Grammatical Error Correction (GEC) we train on sentences from NUCLE (Dahlmeier et al., 2013) and Lang-8 Learner English (Mizumoto et al., 2012) with at least one correction, a total of 660K sentences. We evaluate on the JFLEG (Napoles et al., 2017) and CoNLL 2014 (Ng et al., 2014) sets. For GEC experiments we use random batching only. For all models we use a Transformer model (Vaswani et al., 2017) with the ‘base’ Tensor2Tensor parameters (Vaswani et al., 2018). We train to validation set BLEU convergence on a single GPU. The batch size for baselines and MLE is 4096 tokens. For MRT, where each sentence in the batch is sampled N times, we reduce batch size by N while delaying gradient updates by the same factor to keep the effective batch size constant (Saunders et al., 2018). At inference time we decode using beam size 4. All BLEU scores 7766 Model are for cased, detokenized output, calculated using SacreBLEU (Post, 2018). 3.1 Computation and sample count Our proposed document-MRT approach is more complex than sequence-MRT due to the additional scor"
2020.gebnlp-1.4,2020.winlp-1.25,0,0.302924,"Missing"
2020.gebnlp-1.4,2020.acl-main.418,0,0.0805607,"Missing"
2020.gebnlp-1.4,2020.lrec-1.502,0,0.0380865,"Missing"
2020.gebnlp-1.4,W19-3821,0,0.0769457,"Missing"
2020.gebnlp-1.4,2020.acl-main.154,0,0.160905,"er of all entities in a sentence. In some languages this grammatical gender is dependent on the social gender of human referents. For example, in the Spanish translation of the sentence ‘This is the doctor’, ‘the doctor’ would be either ‘el m´edico’, masculine, or ‘la m´edica’, feminine. Since the noun refers to a person the grammatical gender inflection should be correct for a given referent. In practice many NMT models struggle at generating such inflections correctly (Sun et al., 2019), often instead defaulting to gender-based social stereotypes (Prates et al., 2019) or masculine language (Hovy et al., 2020). For example, an NMT model might always translate ‘This is the doctor’ into a sentence with a masculine inflected noun: ‘Este es el m´edico’. Such behaviour can be viewed as translations exhibiting gender bias. By ‘bias’ we follow the definition from Friedman and Nissenbaum (1996) of behaviour which ‘systematically and unfairly discriminate[s] against certain individuals or groups of individuals in favor of others.’ Specifically, translation performance favors referents fitting into groups corresponding to social stereotypes, such as male doctors. Such systems propagate the representational h"
2020.gebnlp-1.4,W19-3807,0,0.0589035,"s, and produce a WinoMT set to assess translation of coreference sentences with gender-neutral entities. 1.1 Related work Variations on a gender tag or signal for machine translation have been proposed in several forms. Vanmassenhove et al. (2018) incorporate a ‘speaker gender’ tag into training data, allowing gender to be conveyed at the sentence level. However, this does not allow more fine-grained control, for example if there is more than one referent in a sentence. Similar approaches from Voita et al. (2018) and Basta et al. (2020) infer and use gender information from discourse context. Moryossef et al. (2019) also incorporate a single explicit gender feature for each sentence at inference. Miculicich Werlen and Popescu-Belis (2017) integrate coreference links into machine translation reranking to improve pronoun translation with cross-sentence context. Stanovsky et al. (2019) propose NMT gender bias reduction by ‘mixing signals’ with the addition of pro-stereotypical adjectives. Also related to our work is the very recent approach of Stafanoviˇcs et al. (2020), who train their NMT models from scratch with all source language words annotated with target language grammatical gender. In Saunders and"
2020.gebnlp-1.4,W18-6319,0,0.0520007,"Missing"
2020.gebnlp-1.4,N18-2002,0,0.181843,"Missing"
2020.gebnlp-1.4,2020.acl-main.690,1,0.856838,"et al. (2019) also incorporate a single explicit gender feature for each sentence at inference. Miculicich Werlen and Popescu-Belis (2017) integrate coreference links into machine translation reranking to improve pronoun translation with cross-sentence context. Stanovsky et al. (2019) propose NMT gender bias reduction by ‘mixing signals’ with the addition of pro-stereotypical adjectives. Also related to our work is the very recent approach of Stafanoviˇcs et al. (2020), who train their NMT models from scratch with all source language words annotated with target language grammatical gender. In Saunders and Byrne (2020) we treat gender bias as a domain adaptation problem by adapting to a small set of synthetic sentences with equal numbers of entities using masculine and feminine inflections. We also interpret this as a gender ‘tagging’ approach, since the gendered terms in the synthetic dataset give a strong signal to the model. In this work we extend the synthetic datasets from this work to explore this effect further. Other approaches to reducing gender bias effects involve adjusting the word embeddings either directly (Escud´e Font and Costa-juss`a, 2019) or by training with counterfactual data augmentati"
2020.gebnlp-1.4,2020.wmt-1.73,0,0.382617,"Missing"
2020.gebnlp-1.4,P19-1164,0,0.34483,"ender’ tag into training data, allowing gender to be conveyed at the sentence level. However, this does not allow more fine-grained control, for example if there is more than one referent in a sentence. Similar approaches from Voita et al. (2018) and Basta et al. (2020) infer and use gender information from discourse context. Moryossef et al. (2019) also incorporate a single explicit gender feature for each sentence at inference. Miculicich Werlen and Popescu-Belis (2017) integrate coreference links into machine translation reranking to improve pronoun translation with cross-sentence context. Stanovsky et al. (2019) propose NMT gender bias reduction by ‘mixing signals’ with the addition of pro-stereotypical adjectives. Also related to our work is the very recent approach of Stafanoviˇcs et al. (2020), who train their NMT models from scratch with all source language words annotated with target language grammatical gender. In Saunders and Byrne (2020) we treat gender bias as a domain adaptation problem by adapting to a small set of synthetic sentences with equal numbers of entities using masculine and feminine inflections. We also interpret this as a gender ‘tagging’ approach, since the gendered terms in t"
2020.gebnlp-1.4,P19-1159,0,0.0552645,"anguage. 1 Introduction Translation into languages with grammatical gender involves correctly inferring the grammatical gender of all entities in a sentence. In some languages this grammatical gender is dependent on the social gender of human referents. For example, in the Spanish translation of the sentence ‘This is the doctor’, ‘the doctor’ would be either ‘el m´edico’, masculine, or ‘la m´edica’, feminine. Since the noun refers to a person the grammatical gender inflection should be correct for a given referent. In practice many NMT models struggle at generating such inflections correctly (Sun et al., 2019), often instead defaulting to gender-based social stereotypes (Prates et al., 2019) or masculine language (Hovy et al., 2020). For example, an NMT model might always translate ‘This is the doctor’ into a sentence with a masculine inflected noun: ‘Este es el m´edico’. Such behaviour can be viewed as translations exhibiting gender bias. By ‘bias’ we follow the definition from Friedman and Nissenbaum (1996) of behaviour which ‘systematically and unfairly discriminate[s] against certain individuals or groups of individuals in favor of others.’ Specifically, translation performance favors referents"
2020.gebnlp-1.4,D18-1334,0,0.0617901,"d on the translation of sentences based on binary gender signals, such as exclusively male or female personal pronouns. This excludes and erases those who do not use binary gendered language, including but not limited to non-binary individuals (Zimman, 2017; Cao and Daum´e III, 2020). As part of this work we therefore explore applying tagging to indicate gender-neutral referents, and produce a WinoMT set to assess translation of coreference sentences with gender-neutral entities. 1.1 Related work Variations on a gender tag or signal for machine translation have been proposed in several forms. Vanmassenhove et al. (2018) incorporate a ‘speaker gender’ tag into training data, allowing gender to be conveyed at the sentence level. However, this does not allow more fine-grained control, for example if there is more than one referent in a sentence. Similar approaches from Voita et al. (2018) and Basta et al. (2020) infer and use gender information from discourse context. Moryossef et al. (2019) also incorporate a single explicit gender feature for each sentence at inference. Miculicich Werlen and Popescu-Belis (2017) integrate coreference links into machine translation reranking to improve pronoun translation with"
2020.gebnlp-1.4,P18-1117,0,0.0292215,"2020). As part of this work we therefore explore applying tagging to indicate gender-neutral referents, and produce a WinoMT set to assess translation of coreference sentences with gender-neutral entities. 1.1 Related work Variations on a gender tag or signal for machine translation have been proposed in several forms. Vanmassenhove et al. (2018) incorporate a ‘speaker gender’ tag into training data, allowing gender to be conveyed at the sentence level. However, this does not allow more fine-grained control, for example if there is more than one referent in a sentence. Similar approaches from Voita et al. (2018) and Basta et al. (2020) infer and use gender information from discourse context. Moryossef et al. (2019) also incorporate a single explicit gender feature for each sentence at inference. Miculicich Werlen and Popescu-Belis (2017) integrate coreference links into machine translation reranking to improve pronoun translation with cross-sentence context. Stanovsky et al. (2019) propose NMT gender bias reduction by ‘mixing signals’ with the addition of pro-stereotypical adjectives. Also related to our work is the very recent approach of Stafanoviˇcs et al. (2020), who train their NMT models from s"
2020.gebnlp-1.4,N18-2003,0,0.059657,"treat gender bias as a domain adaptation problem by adapting to a small set of synthetic sentences with equal numbers of entities using masculine and feminine inflections. We also interpret this as a gender ‘tagging’ approach, since the gendered terms in the synthetic dataset give a strong signal to the model. In this work we extend the synthetic datasets from this work to explore this effect further. Other approaches to reducing gender bias effects involve adjusting the word embeddings either directly (Escud´e Font and Costa-juss`a, 2019) or by training with counterfactual data augmentation (Zhao et al., 2018; Zmigrod et al., 2019). We view these approaches as orthogonal to our proposed scheme: they have similar goals but do not directly control inference-time gender inflection at the word or sentence level. 36 2 Assessing and controlling gender inflection We wish to investigate whether a system can translate into inflected languages correctly correctly given the reference gender label of a certain word. Our proposed approach involves fine-tuning a model on a very small, easily-constructed synthetic set of sentences which have gender tags. At test time we assign the reference gender label to the w"
2020.gebnlp-1.4,P19-1161,0,0.0367907,"s a domain adaptation problem by adapting to a small set of synthetic sentences with equal numbers of entities using masculine and feminine inflections. We also interpret this as a gender ‘tagging’ approach, since the gendered terms in the synthetic dataset give a strong signal to the model. In this work we extend the synthetic datasets from this work to explore this effect further. Other approaches to reducing gender bias effects involve adjusting the word embeddings either directly (Escud´e Font and Costa-juss`a, 2019) or by training with counterfactual data augmentation (Zhao et al., 2018; Zmigrod et al., 2019). We view these approaches as orthogonal to our proposed scheme: they have similar goals but do not directly control inference-time gender inflection at the word or sentence level. 36 2 Assessing and controlling gender inflection We wish to investigate whether a system can translate into inflected languages correctly correctly given the reference gender label of a certain word. Our proposed approach involves fine-tuning a model on a very small, easily-constructed synthetic set of sentences which have gender tags. At test time we assign the reference gender label to the words whose gender infle"
2020.nlp4call-1.2,W19-4406,0,0.0526347,"Missing"
2020.nlp4call-1.2,P17-1074,0,0.0328029,"Missing"
2020.nlp4call-1.2,P15-1068,0,0.028085,"each participant, we also provide grammatically corrected versions of the student turns. The teachers make errors too, which is interesting in itself, but the focus of teaching is on the students and therefore we economise effort by correcting student turns only. The process includes grammatical errors, typos, and improvements to lexical choice. This was done in a minimal fashion to stay as close to the original meaning as possible. In addition, there can often be many possible corrections for any one grammatical error, a known problem in corpus annotation and NLP work on grammatical errors (Bryant and Ng, 2015). The usual solution is to collect multiple annotations, which we have not yet done, but plan to. In the meantime, the error annotation is useful for grammatical error detection even if correction might be improved by more annotation. Sequence type: We indicate major and minor shifts in conversational sequences – sections of interaction with a particular purpose, even if that purpose is from time-to-time more social than it is educational. Borrowing key concepts from the CONVERSATION ANALYSIS (CA) approach (Sacks et al., 1974), we seek out groups of turns which together represent the building"
2020.nlp4call-1.2,W17-5547,0,0.0551408,"Missing"
2020.nlp4call-1.2,D18-1547,0,0.0565982,"Missing"
2020.nlp4call-1.2,N10-1020,0,0.0541221,"nd discourse functions (Csomay, 2012), the interaction of roles and goal-driven behaviour in academic discourse (Evison, 2013), and knowledge development at different stages of higher education learning (Atwood et al., 2010). On a larger scale, corpora such as the MultiDomain Wizard-of-Oz datasets (MultiWOZ) contain thousands of goal-directed dialogue collected through crowdsourcing and intended for the training of automated dialogue systems (Budzianowski et al., 2018; Eric et al., 2020). Other work has involved the collation of pre-existing conversations on the web, for example from Twitter (Ritter et al., 2010), Reddit (Schrading et al., 2015), and movie scripts (Danescu-Niculescu-Mizil and Lee, 2011). Such datasets are useful for training dialogue systems to respond to written inputs – so-called ‘chatbots’ – which in recent years have greatly improved in terms of presenting some kind of personality, empathy and world knowledge (Roller et al., 2020), where previously there had been relatively little of all three. The improvement in chatbots has caught the attention of, and in turn has been driven by, the technology industry, for they have clear commercial applications in customer service scenarios s"
2020.nlp4call-1.2,W03-0205,0,0.0609884,"Missing"
2020.nlp4call-1.2,W11-0609,0,0.0255946,"riven behaviour in academic discourse (Evison, 2013), and knowledge development at different stages of higher education learning (Atwood et al., 2010). On a larger scale, corpora such as the MultiDomain Wizard-of-Oz datasets (MultiWOZ) contain thousands of goal-directed dialogue collected through crowdsourcing and intended for the training of automated dialogue systems (Budzianowski et al., 2018; Eric et al., 2020). Other work has involved the collation of pre-existing conversations on the web, for example from Twitter (Ritter et al., 2010), Reddit (Schrading et al., 2015), and movie scripts (Danescu-Niculescu-Mizil and Lee, 2011). Such datasets are useful for training dialogue systems to respond to written inputs – so-called ‘chatbots’ – which in recent years have greatly improved in terms of presenting some kind of personality, empathy and world knowledge (Roller et al., 2020), where previously there had been relatively little of all three. The improvement in chatbots has caught the attention of, and in turn has been driven by, the technology industry, for they have clear commercial applications in customer service scenarios such as helplines and booking systems. 2 Corpus design We set out a design for the TSCC which"
2020.nlp4call-1.2,2020.bea-1.5,0,0.0361681,"Missing"
2020.nlp4call-1.2,P11-1019,1,0.81033,"Missing"
2020.wat-1.21,W19-5301,0,0.0232851,"Missing"
2020.wat-1.21,W17-4123,0,0.0576136,"Missing"
2020.wat-1.21,W17-3204,0,0.0214179,"dation compared to the baseline, especially for JapaneseEnglish. We note that our Japanese-English ASPEC decomposed-training score is similar to the result for the same set achieved by Zhang and Komachi (2018) with ideograph decomposition. However, our non-decomposed baseline is much stronger, and so we are not able to replicate their ﬁnding that training with sub-character decomposition is beneﬁcial to NMT from logographic languages to English. We suggest this degradation may be the result of training and inference with much longer sequences, which are wellestablished as challenging for NMT (Koehn and Knowles, 2017). Interestingly we ﬁnd that adding IDCs, which lengthen sequences, performs slightly better for the lower-resource than for higher-resource cases, especially for Chinese-English. A possible explanation is that the longer sequences regularize adaptation in these cases, avoiding overﬁtting to the highly speciﬁc lower-resource domains. However, these cases still show degradation relative to the baseline. On the unseen sets, training with sub-character decomposition outperforms the baseline in terms of BLEU for the ASPEC unseen set. However, this is not a consistent result, with the baseline perfo"
2020.wat-1.21,W17-4122,0,0.019163,"riminate subcharacter decomposition can harm unseen character translation, and also gives inconsistent performance on sentences with no unseen characters. • We instead propose a set of extremely straightforward inference-time sub-character decomposition schemes requiring no additional models or training. 1.1 Related work In NMT, applying radical decomposition before learning a Byte Pair Encoding (BPE, Sennrich Outside of translation, use of sub-character decomposition has been shown to improve learning of character embeddings for Chinese (Sun et al., 2014) and language modelling for Japanese (Nguyen et al., 2017). Sub-character decomposition has also been applied to sentiment classiﬁcation (Ke and Hagiwara, 2017), text classiﬁcation (Toyama et al., 2017) and word similarity tasks, the last with mixed results (Karpinska et al., 2018). We consider work on NMT with byte-level subwords (Costa-jussà et al., 2017; Wang et al., 2019) as complementary to this work. Representing text at the level of bytes allows any logographic character with a Unicode representation to be included in the model vocabulary. However, inclusion in the vocabulary does not guarantee that the model learns a good character representa"
2020.wat-1.21,W18-6319,0,0.0118287,"rce test sets. Baseline has no sub-character decomposition. Sub-character decomposition during training fails to improve general translation, and only improves unseen set translation for ASPEC. Japanese-English higher-resource domain models. For the lower resource domains we ﬁne-tune the trained models for 30K and 10K steps respectively. We conduct inference via beam search with beam size 4. For ASPEC evaluation we evaluate Moses tokenized English with the multi-bleu tool to correspond to the oﬃcial WAT evaluation. For all other results we report detokenized English using the SacreBLEU tool5 (Post, 2018). All BLEU is for truecased English. 3.3 Results We have two requirements when using subcharacter decomposition for unseen character translation: • Sets with few unseen characters (all general test sets except KFTT) should not experience performance degradation in terms of BLEU. • Translation performance on unseen characters should improve. Unseen character translation improvement may not be detectable by BLEU score, since the unseen character sets may only have one or two unseen characters per sentence. Moreover generating a hypernym, such as ‘ﬁsh’ instead of ‘sardine’ for 鰯, would not improv"
2020.wat-1.21,P16-1162,0,0.463419,"inconsistent results generally. We oﬀer a simple alternative based on decomposition before inference for unseen characters only. Our approach allows ﬂexible application, achieving translation adequacy improvements and requiring no additional models or training. 1 Introduction While Neural Machine Translation (NMT) has evolved rapidly in recent years, not all of its successful techniques are equally applicable to all language pairs. A particular example is the representation and translation of unseen tokens, which do not appear in the training data. With techniques like subword decomposition (Sennrich et al., 2016), an unseen word in an alphabetic language can in the worst case be represented as a sequence of characters. Since alphabetic languages usually have few unique characters, it is reasonable to assume that ∗ all of these ‘backoﬀ’ characters will be present in the limited model vocabulary. We focus instead on the translation of unseen Chinese and Japanese logographic characters into alphabetic languages, a task that remains a challenge for NMT. Logographic writing systems may have many thousands of logograms, each representing at least one word, morpheme or concept as well as conveying phonetic a"
2020.wat-1.21,D19-5214,0,0.0306693,"Missing"
2020.wat-1.21,W18-6303,0,0.349061,"e translation. In the worst case, the presence of a previously-unseen character at inference time may harm the translation quality. This is a particular concern for NMT in low-resource domains, when a model is less able to rely on lexical context. Many logographic characters share subcharacter components1 , which can carry semantic or phonetic meaning (Table 1). An intuitive approach to the logogram sparsity problem in NLP uses sub-character decompositions in place of characters. Sub-character work in NMT has focused on the use of shared sub-characters to improve Chinese-Japanese translation (Zhang and Komachi, 2018, 2019). In this approach all logograms are decomposed, and subword vocabularies are learned over sub-character sequences. We identify two motivations for using subNow at Amazon, work performed while at SDL 1 214 Kangxi Radicals are deﬁned as a block in Unicode as of version 3.0 (Consortium, 2000). In this paper we follow prior work in using shallower decompositions which can include non-radical sub-character units. 170 Proceedings of the 7th Workshop on Asian Translation, pages 170–177 c December 4, 2020. 2020 Association for Computational Linguistics Char 森 鰯 校 Meaning Forest Sardine School"
2020.wmt-1.94,W19-5403,0,0.156495,"contains the Introduction Neural Machine Translation (NMT) in the biomedical domain presents challenges in addition to general domain translation. Text often contains specialist vocabulary and follows specific stylistic conventions. For this task fine-tuning generic pretrained models on smaller amounts of biomedicalspecific data can lead to strong performance, as we found in our 2019 biomedical submission (Saunders et al., 2019). For our WMT 2020 submission we start with strong single models from that 2019 submission and fine-tune them exclusively on the small Medline abstracts training sets (Bawden et al., 2019). This allows fast training on very relevant training data, since the test set is also made up of Medline abstracts. However, fine-tuning on relevant but small corpora has pitfalls. The small number of training 862 Proceedings of the 5th Conference on Machine Translation (WMT), pages 862–869 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics English source Human translation MLE MRT English source Human translation MLE MRT [Associations of work-related strain with subjective sleep quality and individual daytime sleepiness]. [Zusammenhang von arbeitsbezogenen psychisc"
2020.wmt-1.94,P17-2061,0,0.0138659,"nce. In standard MRT (middle) each sample has a score, e.g. sBLEU. For doc-MRT (right) samples are sorted into minibatch-level ‘documents’, each with a combined score, e.g. document BLEU. Doc-MRT scores are less sensitive to individual samples, increasing robustness. 1.3 Related work Fine-tuning general models on domain-specific datasets has become common in NMT. Simple transfer learning on new data can adapt a general model to in-domain data (Luong and Manning, 2015). Mixed fine-tuning where some original data is combined with the new data avoids reduced performance on the original data-set (Chu et al., 2017). We are only interested in performance on one domain, so use simple transfer learning. For this task, we specifically fine-tune on a relatively small dataset. Adaptation to very small, carefully-chosen domains has been explored for speaker-personalized translation (Michel and Neubig, 2018) , and to reduce gender bias effects (Saunders and Byrne, 2020) while maintaining general domain performance. We wish to adapt to a very specific domain without need to maintain good general domain performance, but must avoid overfitting. Related approaches include finetuning a separate model for each test s"
2020.wmt-1.94,W17-4713,0,0.0141271,"n performance on one domain, so use simple transfer learning. For this task, we specifically fine-tune on a relatively small dataset. Adaptation to very small, carefully-chosen domains has been explored for speaker-personalized translation (Michel and Neubig, 2018) , and to reduce gender bias effects (Saunders and Byrne, 2020) while maintaining general domain performance. We wish to adapt to a very specific domain without need to maintain good general domain performance, but must avoid overfitting. Related approaches include finetuning a separate model for each test sentence (Li et al., 2018; Farajian et al., 2017) or test document (Xu et al., 2019; Kothur et al., 2018). We choose to train a single model for all test sentences in a language pair, but improve the robustness of that model to overfitting and exposure bias using MRT. Minimum Risk Training (MRT) aims to minimize the expected cost between N sampled target (s) sequences yn and the corresponding gold reference sequence y (s)∗ for the S sentence pairs in each minibatch. For translation MRT is usually applied using a sentence-level BLEU (sBLEU) score corresponding to cost function 1 − sBLEU, and sentence samples are generated by autoregressive sa"
2020.wmt-1.94,2020.acl-main.690,1,0.827335,"come common in NMT. Simple transfer learning on new data can adapt a general model to in-domain data (Luong and Manning, 2015). Mixed fine-tuning where some original data is combined with the new data avoids reduced performance on the original data-set (Chu et al., 2017). We are only interested in performance on one domain, so use simple transfer learning. For this task, we specifically fine-tune on a relatively small dataset. Adaptation to very small, carefully-chosen domains has been explored for speaker-personalized translation (Michel and Neubig, 2018) , and to reduce gender bias effects (Saunders and Byrne, 2020) while maintaining general domain performance. We wish to adapt to a very specific domain without need to maintain good general domain performance, but must avoid overfitting. Related approaches include finetuning a separate model for each test sentence (Li et al., 2018; Farajian et al., 2017) or test document (Xu et al., 2019; Kothur et al., 2018). We choose to train a single model for all test sentences in a language pair, but improve the robustness of that model to overfitting and exposure bias using MRT. Minimum Risk Training (MRT) aims to minimize the expected cost between N sampled targe"
2020.wmt-1.94,W16-2316,0,0.020544,"rom: MLE no-title (en-de) / baseline (en-es) de2en 41.1 41.4 41.3 41.9 en2de 32.2 31.8 33.0 32.6 es2en 48.9 49.0 en2es 47.7 47.2 Table 4: Validation BLEU developing models used in English-German and English-Spanish language pair submissions. Scores for averaged checkpoints. MLE fine-tuning with either dataset did not improve over the en-es baselines. For each approach we fine-tune on a single GPU, saving checkpoints every 1K updates, until fine-tuning validation set BLEU fails to improve for 3 consecutive checkpoints. Generally this took about 5K updates. We then perform checkpoint averaging (Junczys-Dowmunt et al., 2016) over the final 3 checkpoints to obtain the final model. 2.3 tion directions by up to 0.8 BLEU when comparing with or without checkpoint averaging. While checkpoint averaging slightly decreased validation set performance for en2de MLE, we use it in all cases since it reduces sensitivity to randomness in training (Popel and Bojar, 2018). In Table 4 we explore the impact of fine-tuning only on aggressively filtered ‘no-title’ data. This does noticeably improve performance for de2en, with a very small improvement for es2en. Since the added information in ‘title’ sentences is on the English side,"
2020.wmt-1.94,W19-5421,1,0.898442,"e. We identify a specific feature of the Medline abstract training data which caused noticeable translation errors. The data contains instances in which either the source or target sentence contains the Introduction Neural Machine Translation (NMT) in the biomedical domain presents challenges in addition to general domain translation. Text often contains specialist vocabulary and follows specific stylistic conventions. For this task fine-tuning generic pretrained models on smaller amounts of biomedicalspecific data can lead to strong performance, as we found in our 2019 biomedical submission (Saunders et al., 2019). For our WMT 2020 submission we start with strong single models from that 2019 submission and fine-tune them exclusively on the small Medline abstracts training sets (Bawden et al., 2019). This allows fast training on very relevant training data, since the test set is also made up of Medline abstracts. However, fine-tuning on relevant but small corpora has pitfalls. The small number of training 862 Proceedings of the 5th Conference on Machine Translation (WMT), pages 862–869 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics English source Human translation MLE MRT"
2020.wmt-1.94,W18-2708,0,0.0166138,"ng. For this task, we specifically fine-tune on a relatively small dataset. Adaptation to very small, carefully-chosen domains has been explored for speaker-personalized translation (Michel and Neubig, 2018) , and to reduce gender bias effects (Saunders and Byrne, 2020) while maintaining general domain performance. We wish to adapt to a very specific domain without need to maintain good general domain performance, but must avoid overfitting. Related approaches include finetuning a separate model for each test sentence (Li et al., 2018; Farajian et al., 2017) or test document (Xu et al., 2019; Kothur et al., 2018). We choose to train a single model for all test sentences in a language pair, but improve the robustness of that model to overfitting and exposure bias using MRT. Minimum Risk Training (MRT) aims to minimize the expected cost between N sampled target (s) sequences yn and the corresponding gold reference sequence y (s)∗ for the S sentence pairs in each minibatch. For translation MRT is usually applied using a sentence-level BLEU (sBLEU) score corresponding to cost function 1 − sBLEU, and sentence samples are generated by autoregressive sampling with temperature τ during training (Shen et al.,"
2020.wmt-1.94,2020.acl-main.693,1,0.927679,"e appears. Figure 1 gives a toy example of doc-MRT scoring samples in context. Document-level metrics aggregate scores across sentence samples, meaning a minibatch with some good samples and some poor samples will not have extreme score variation. Doc-MRT is therefore less sensitive than standard MRT to variation in individual samples. Doc-MRT has been shown to give better performance than standard MRT for small datasets with a risk of over-fitting, as well as improved robustness to small N . More discussion of these results and a derivation of the document-level loss function can be found in Saunders et al. (2020). Since we are attempting fine-tuning on small datasets and since N is a limiting factor for MRT on memory-intensive large models, the biomedical task is an appropriate application for doc-MRT. Document MRT Figure 1: Two MRT schemes with an S = 2 sentence minibatch and N = 3 samples / sentence. In standard MRT (middle) each sample has a score, e.g. sBLEU. For doc-MRT (right) samples are sorted into minibatch-level ‘documents’, each with a combined score, e.g. document BLEU. Doc-MRT scores are less sensitive to individual samples, increasing robustness. 1.3 Related work Fine-tuning general mode"
2020.wmt-1.94,L18-1146,0,0.0425397,"Missing"
2020.wmt-1.94,P18-2051,1,0.898959,"Missing"
2020.wmt-1.94,2015.iwslt-evaluation.11,0,0.0222349,"edical task is an appropriate application for doc-MRT. Document MRT Figure 1: Two MRT schemes with an S = 2 sentence minibatch and N = 3 samples / sentence. In standard MRT (middle) each sample has a score, e.g. sBLEU. For doc-MRT (right) samples are sorted into minibatch-level ‘documents’, each with a combined score, e.g. document BLEU. Doc-MRT scores are less sensitive to individual samples, increasing robustness. 1.3 Related work Fine-tuning general models on domain-specific datasets has become common in NMT. Simple transfer learning on new data can adapt a general model to in-domain data (Luong and Manning, 2015). Mixed fine-tuning where some original data is combined with the new data avoids reduced performance on the original data-set (Chu et al., 2017). We are only interested in performance on one domain, so use simple transfer learning. For this task, we specifically fine-tune on a relatively small dataset. Adaptation to very small, carefully-chosen domains has been explored for speaker-personalized translation (Michel and Neubig, 2018) , and to reduce gender bias effects (Saunders and Byrne, 2020) while maintaining general domain performance. We wish to adapt to a very specific domain without nee"
2020.wmt-1.94,P16-1162,0,0.047623,"t years (Shen et al., 2016; Neubig, 2016; Edunov et al., 2018). In particular, Wang and Sennrich (2020) recently highlighted the efficacy of MRT for reducing the effects of exposure bias. 2 2.1 Experimental setup Data We report on two language pairs: English-Spanish (en-es) and English-German (en-de). Table 2 lists the data used to train our biomedical domain evaluation systems. For each language pair we use the same training data in both directions, and preprocess all data with Moses tokenization, punctuation normalization and truecasing. We use a 32Kmerge joint source-target BPE vocabulary (Sennrich et al., 2016) learned on the pre-training data. All of our submitted approaches involve finetuning pre-trained models. We initialise finetuning with the strong biomedical domain models that formed our ‘run 1’ submission for the WMT19 biomedical translation task. Details of data preparation and training for these models are discussed in Saunders et al. (2019). We fine-tune these models on Medline abstracts data, validating on test sets from the 2019 Biomedical task. For these we concatenate the src-trg and trg-src 2019 test sets for each language pair, and select only the ‘OK’ aligned sentences as annotated"
2020.wmt-1.94,P18-2050,0,0.0129415,"Fine-tuning general models on domain-specific datasets has become common in NMT. Simple transfer learning on new data can adapt a general model to in-domain data (Luong and Manning, 2015). Mixed fine-tuning where some original data is combined with the new data avoids reduced performance on the original data-set (Chu et al., 2017). We are only interested in performance on one domain, so use simple transfer learning. For this task, we specifically fine-tune on a relatively small dataset. Adaptation to very small, carefully-chosen domains has been explored for speaker-personalized translation (Michel and Neubig, 2018) , and to reduce gender bias effects (Saunders and Byrne, 2020) while maintaining general domain performance. We wish to adapt to a very specific domain without need to maintain good general domain performance, but must avoid overfitting. Related approaches include finetuning a separate model for each test sentence (Li et al., 2018; Farajian et al., 2017) or test document (Xu et al., 2019; Kothur et al., 2018). We choose to train a single model for all test sentences in a language pair, but improve the robustness of that model to overfitting and exposure bias using MRT. Minimum Risk Training ("
2020.wmt-1.94,P16-1159,0,0.0333362,"et al., 2018). We choose to train a single model for all test sentences in a language pair, but improve the robustness of that model to overfitting and exposure bias using MRT. Minimum Risk Training (MRT) aims to minimize the expected cost between N sampled target (s) sequences yn and the corresponding gold reference sequence y (s)∗ for the S sentence pairs in each minibatch. For translation MRT is usually applied using a sentence-level BLEU (sBLEU) score corresponding to cost function 1 − sBLEU, and sentence samples are generated by autoregressive sampling with temperature τ during training (Shen et al., 2016). Hyperparameter α controls sharpness of the distribution over samples. While MRT permits training from scratch, in practice it is exclusively used to fine-tune models. Doc-MRT is a recently proposed MRT variant which changes sentence cost function to a document cost function, D(.) (Saunders et al., 2020). D measures costs between minibatch-level ‘documents’ Y ∗ and Yn . Y ∗ is formed of all S reference sentences in the minibatch, and Yn is one of N sample ‘documents’ each formed of one sample from each sentence pair (x(s) , y (s)∗ ). This permits MRT under document-level scores like BLEU, ins"
2020.wmt-1.94,W16-4610,0,0.0343971,"Missing"
2020.wmt-1.94,D17-2005,1,0.777183,"et training sentence quality impacts both MLE and MRT performance. However, removing these sentences entirely results in a noticeable performance decrease for the en2de and en2es models, demonstrating that they can be valuable training examples. Inference For the 2020 submissions, we additionally split any test lines containing multiple sentences before inference using the Python NLTK package7 , translate the split sentences separately, then remerged. We found this gave noticeable improvements in quality for the few sentences it applied to. In all cases we decode with beam size 4 using SGNMT (Stahlberg et al., 2017). Test scores are as provided by the organizers for ”OK” sentences using Moses tokenization and the multi-eval tool. Validation scores are for case-insensitive, detokenized text obtained using SacreBLEU8 (Post, 2018). 2.4 We submitted three runs to the WMT20 biomedical task for each language pair. For en-de run 1 was the baseline model fine-tuned on MLE with all data, while for en-es we submitted the checkpoint averaged baseline as MLE fine-tuning did not improve dev set performance. Run 2 was the run 1 model fine-tuned with doc-MRT on notitle data. Run 3 was the run 1 model fine-tuned with do"
2020.wmt-1.94,L16-1470,0,0.256374,"Missing"
2020.wmt-1.94,2020.acl-main.326,0,0.0174465,"models trained on English source sentences with titles can behave erratically when given sentences with square-bracketed titles at test time: an exposure bias effect. One possible approach to this problem is aggressively filtering sentences which may be poorly aligned. However, with such a small training set, this risks losing valuable examples of domainspecific source and target language. We hypothesise that such filtering is not the only way to reduce the effects during inference. Instead, we propose an approach in terms of the parameter fine-tuning scheme with Minimum Risk Training (MRT). Wang and Sennrich (2020) have recently shown MRT as effective for combating exposure bias in the context of domain shift – test sentences which are very different from the training data. We propose that MRT is also more robust against exposure to misaligned training data. The examples in Table 1 show the different behaviour of MLE and MRT in such cases. In the first example, the MLE hypothesis is unrelated to the source sentence, while the MRT output is relevant. In the second example, the MLE output is more plausible and therefore misleading, as it still misses the first clause which the MRT hypothesis covers. Both"
2021.acl-long.13,2020.acl-main.60,0,0.0372367,"Missing"
2021.acl-long.13,W14-4337,0,0.0701703,"Missing"
2021.acl-long.13,W19-5921,0,0.0283244,"ning Details The positive and negative RL rewards of Sec. 3 are tuned in the range [-5, 5] based on the dev set. The user goals employed for interaction during RL are taken from the training data without synthesizing new goals. Further training details can be found in Appendix A.1. Evaluation Metrics The proposed model is evaluated in terms of the inform rate (Info), the success rate (Succ), and BLEU.2 The inform rate measures whether the DS provides the correct entity matching the user goal, while the success rate further requires the system to answer all user questions correctly. Following (Mehri et al., 2019), the combined performance (Comb) is also reported, calculated as 0.5 ∗ (Info + Succ) + BLEU. Info 69.77 81.38 82.83 85.62 86.49 Info 70.0 76.3 84.4 85.5 84.9 77.4 80.6 83.2 Succ 58.0 60.4 70.1 72.9 74.9 66.7 69.4 73.5 BLEU 17.5 16.6 15.0 16.5 17.9 17.4 17.5 17.6 Comb 81.5 85.0 92.3 95.7 97.8 89.5 92.5 96.0 Table 2: Empirical comparison with state-of-the-art dialogue systems using the predicted belief state. ∗ indicates leveraging of pre-trained transfomer-based models. optimisation of both the US and the DS provides dialogues with higher success rate than only optimising the DS (c&e vs. b&d)."
2021.acl-long.13,W19-5912,0,0.071662,"duce natural language. As discussed above for DSs, the end-to-end architecture for the US also offers simplicity in transfer learning across domains. There are also potential advantages to continued joint training of the DS and the US. If a user model is less than perfectly optimised after supervised learning over a fixed training corpus, further learning through interaction between the two agents offers the US the opportunity to refine its behavior. Prior work has shown benefits from this approach to dialogue policy learning, with a higher success rate at dialogue level (Liu and Lane, 2017b; Papangelis et al., 2019; Takanobu et al., 2020), but there has not been previous work that addresses multi-domain end-to-end dialogue modelling for both agents. Takanobu et al. (2020) address refinement of the dialogue policy alone at the semantic level, but do not address end-to-end system architectures. Liu and Lane (2017b); Papangelis et al. (2019) address single-domain dialogues (Henderson et al., 2014), but not the more realistic and 152 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 152–16"
2021.acl-long.13,W17-5518,0,0.0555869,"Missing"
2021.acl-long.13,2020.acl-main.59,0,0.110666,"discussed above for DSs, the end-to-end architecture for the US also offers simplicity in transfer learning across domains. There are also potential advantages to continued joint training of the DS and the US. If a user model is less than perfectly optimised after supervised learning over a fixed training corpus, further learning through interaction between the two agents offers the US the opportunity to refine its behavior. Prior work has shown benefits from this approach to dialogue policy learning, with a higher success rate at dialogue level (Liu and Lane, 2017b; Papangelis et al., 2019; Takanobu et al., 2020), but there has not been previous work that addresses multi-domain end-to-end dialogue modelling for both agents. Takanobu et al. (2020) address refinement of the dialogue policy alone at the semantic level, but do not address end-to-end system architectures. Liu and Lane (2017b); Papangelis et al. (2019) address single-domain dialogues (Henderson et al., 2014), but not the more realistic and 152 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 152–166 August 1–6, 2021. ©202"
2021.acl-long.13,2020.acl-main.638,0,0.0355759,"Missing"
2021.acl-long.13,E17-1042,0,0.0662568,"Missing"
2021.acl-long.55,2020.tacl-1.36,0,0.023576,"onsidered by some to be better equipped to interact with external APIs (Sukhbaatar et al., 2015; Wen et al., 2017) 1 The term “end-to-end” is sometimes also used when describing parts of modular systems (Li et al., 2017; Wen et al., 2017) but it is fundamentally different from the single text-totext transformer model approach we present here. and therefore might be better suited for task-based dialogs. As mentioned above, we show that our single model-based approach can accurately generate both the appropriate response as well as predict the correct API call at the right time. Earlier work by Andreas et al. (2020) and Hosseini-Asl et al. (2020) employs a similar modeling approach to predict dialog state in task-based dialogs, which can be seen as a precursor to our API call prediction strategy. other words, carefully curated, annotated datasets that cover all the idiosyncrasies of a single task or transaction are a key factor in model performance. Concern about the cost and efficiency of creating these larger corpora has led some researchers to look for approaches that alleviate dependencies on annotated data (Budzianowski and Vuli´c, 2019; Wen et al., 2017). However, significant time and expense can b"
2021.acl-long.55,D19-5602,0,0.0411088,"Missing"
2021.acl-long.55,D18-1547,0,0.0444799,"Missing"
2021.acl-long.55,D19-1459,1,0.876702,"tion for even singledomain tasks has yet to be demonstrated. By first solving the problem for a single domain, we argue that replicating the process for multiple domains will be achievable by simply training on additional high-quality datasets labeled with the same APIfocused strategy. 2 2.1 has become a popular benchmarking corpus for state tracking. It has also undergone a series of subsequent refinements. MSR-E2E, featured in the Microsoft dialog challenge (Li et al., 2018), has 10,087 dialogues in three domains, movie-ticket booking, restaurant reservation, and taxi booking. Taskmaster-1 (Byrne et al., 2019) offers 13,215 dialogs in six domains and has been updated with a second installment, Taskmaster-2 (Byrne et al., 2020), which adds 17,289 more dialogs totalling over 30,000. The Schema Guided Dialogue dataset (Rastogi et al., 2020) has 22,825 dialogs in multiple domains. MetaLWOZ (Lee et al., 2019) has 37,884 dialogs in 227 domains and is aimed at helping models more accurately predict user responses in new domains. Both Schema and MetaLWOZ are used in DSTC8 (Kim et al., 2019). In addition to these, Serban et al. (2018) provides a thorough survey of dialog corpora released in previous years."
2021.acl-long.55,W13-4073,0,0.0340322,"the user’s task is focused on a very specific part of the exchange. This allows us to “fill holes” in the data quickly and cost effectively. That is, we can create large numbers of short, conversational examples that the model does not handle 673 adequately and then retrain for better results. 3.3 sent an end-to-end task based dialog system and ultimately achieve positive results. Annotation Dialog data annotation can be complex and time consuming even for trained linguists as it typically involves carefully and consistently labeling dialog states, user intents, and dialog acts, among others (Henderson et al., 2013; Wen et al., 2017; Budzianowski et al., 2018). The API-targeted approach is far more straightforward since only basic entities (e.g. name, time, number of tickets, theater, movie attributes, etc.) and API calls (e.g. to find theaters, movies, and show times, book tickets, etc.) are labeled. The task is therefore easier to learn, faster to complete, and cheaper to run. Moreover, as we discuss below, it fits well with the text-to-text format we use in our approach to transaction-based dialog systems. Fifteen workers performed the annotations using a web-based tool that allows for only well-form"
2021.acl-long.55,I17-1074,0,0.0615653,"Missing"
2021.acl-long.55,2020.emnlp-main.273,0,0.0296868,"Missing"
2021.acl-long.55,D18-1255,0,0.0224368,"has led some researchers to look for approaches that alleviate dependencies on annotated data (Budzianowski and Vuli´c, 2019; Wen et al., 2017). However, significant time and expense can be saved when assembling these corpora by simplifying the collection and annotation procedures. In addition, little to no training is required for workers to be able to perform consistently well. 3.2 Collection methodology Figure 2: Simplified end-to-end system 3 3.1 The TicketTalk dataset Overview The TicketTalk movie ticketing dataset was created using the self-dialog collection method (Krause et al., 2017; Moghe et al., 2018; Byrne et al., 2019) in which a paid crowd-sourced worker writes both sides of the dialog (i.e. both customer and ticketing agent turns) based on a particular scenario and set of instructions. Following the annotation strategy used for Taskmaster-1 (Byrne et al., 2019), labels are limited to basic entities and events (i.e. API calls). The dataset was created by over 4000 unique, native or near-native US English speakers. Further demographic information (e.g. gender, dialect, etc.) is not known, and no personal identifiable information was gathered. STAT TYPE Dialogs Total turns Unique tokens"
2021.acl-long.55,2020.emnlp-main.66,0,0.0140197,"from training individual models on labeled datasets (Wen et al., 2017; Young et al., 2013). This architecture is inherently more complex than the single-model end-to-end strategy we propose and can require significantly more design and engineering. Moreover, since each module requires its own supervised training dataset, it is harder to apply to different domains (Serban et al., 2015a). Figure 1: Traditional modular system Datasets Over the past few years the NLP community has responded to the lack of dialog data with larger, publicly released task-oriented datasets spanning multiple domains (Wu et al., 2020; Budzianowski and Vuli´c, 2019). This underscores the crucial role data plays in any approach to task-based dialog systems. MultiWOZ (Budzianowski et al., 2018) consists of 10,420 dialogs in multiple domains and 672 However, the separation of functions makes the modular approach more transparent and in some respects easier to debug. It has also been considered by some to be better equipped to interact with external APIs (Sukhbaatar et al., 2015; Wen et al., 2017) 1 The term “end-to-end” is sometimes also used when describing parts of modular systems (Li et al., 2017; Wen et al., 2017) but it"
2021.acl-long.55,E17-1042,0,0.0759199,"Missing"
2021.eancs-1.2,W10-4323,0,0.092939,"Missing"
2021.eancs-1.2,P19-3011,0,0.0513178,"Missing"
2021.eancs-1.2,2021.acl-short.111,0,0.0198301,"n contrast with previous approaches we measure the F-score at dialogue level and consider user and system behaviours to improve recall and precision estimation. We facilitate scores interpretation by providing a rich hierarchical structure with information about conversational patterns present in the test data and tools to efficiently query the conversations generated. We apply our framework to assess the performance and weaknesses of a Convlab2 user model1 . 1 Introduction Remarkable progress has been achieved in many dialogue systems research disciplines, from dialogue state tracking (DST) (Dai et al., 2021; Mehri et al., 2020) to policy- (Wang et al., 2020; Lubis et al., 2020) and end-to-end modelling (Peng et al., 2021; Yang et al., 2020). Progress is usually measured component-wise through task-specific metrics and improvements in the overall performance of the systems leveraging advances in component designs are seldom reported (Takanobu et al., 2020). Takanobu et al. (2020) empirically show that component-wise evaluation may not correlate well with the overall performance of the system. They recommend evaluating dialogue systems in an end-to-end, interactive, multi-turn setting to capture t"
2021.eancs-1.2,2020.coling-main.41,0,0.0365295,"Missing"
2021.eancs-1.2,N19-1423,0,0.00552806,"their search did not return results and offered an alternative. The system may also specify entity attributes which are not in the goal that the user may co-refer to in the next turn. Finally, since it does not know the user goal, the system may request the user to provide values for slots outside it. These patterns are detected by the evaluator and the false positive counts are adjusted accordingly. 4 4.1 Sample evaluation results Experimental setup System agent The system agent is a pipeline architecture implemented in the Convlab2 library (Zhu et al., 2020). It is comprised of a BERTbased (Devlin et al., 2019) natural language understanding (NLU) module, a handcrafted policy, a rule-based DST and a retrieval natural language generation (NLG) module. This model outperforms all other Convlab2 system configurations (Takanobu et al., 2020). User agent We evaluate the architecture employed by Takanobu et al. (2020) in their user model based evaluation study. It is comprised of an MILU-based (Hakkani-Tür et al., 2016) NLU module, an agenda-based handcrafted policy (Schatzmann et al., 2007) and a retrieval NLG module. Interaction setup We extend Convlab2 by driving the interaction between the two agents b"
2021.eancs-1.2,P02-1040,0,0.109878,"ll help developers improve their models. In summary, our contributions are: who ranks user models according to the Cramérvon Mises divergence between the system performance distributions measured in interaction with simulated user populations and real users. Callejas et al. (2012) suggest to overcome the lack of interpretability of these apporaches by using mutidimensional subspace clustering to graphically show the similarity between generated and real data, but their metric is susceptible to the choice of features and clustering algorithm. Jung et al. (2009) propose to adapt the BLEU score (Papineni et al., 2002) to capture &quot;dialogue level naturalness&quot; by considering a &quot;gram&quot; to be a user or system action and show that this metric correlates well with human judgement. One drawback to applying this metric to compare the sequences generated by user models with references is the arbitrary ordering of action sequences. The similarity of the simulated dialogues to the real data is assessed by evaluating the perplexity of the user model instead. However, this metric may not be a good indicator of the ability of the user model to predict a realistic response in an unknown dialogue situation, so it does not m"
2021.eancs-1.2,2005.sigdial-1.6,0,0.269467,"Missing"
2021.eancs-1.2,N07-2038,0,0.103844,"agent is a pipeline architecture implemented in the Convlab2 library (Zhu et al., 2020). It is comprised of a BERTbased (Devlin et al., 2019) natural language understanding (NLU) module, a handcrafted policy, a rule-based DST and a retrieval natural language generation (NLG) module. This model outperforms all other Convlab2 system configurations (Takanobu et al., 2020). User agent We evaluate the architecture employed by Takanobu et al. (2020) in their user model based evaluation study. It is comprised of an MILU-based (Hakkani-Tür et al., 2016) NLU module, an agenda-based handcrafted policy (Schatzmann et al., 2007) and a retrieval NLG module. Interaction setup We extend Convlab2 by driving the interaction between the two agents by the MultiWOZ 2.1 test set goals (Figure 1). This facilitates comparisons with future work and is Constraints repetitions Constraint repetitions occur due to user and system behaviours. For example, if a user search or booking fails, the user may repeat some already mentioned information when updating their criteria. The system may also ask some values to be repeated if uncertain about what was communicated. In addition, the user might repeat information when stating new constr"
2021.eancs-1.2,2020.sigdial-1.37,0,0.190762,"nversations generated. We apply our framework to assess the performance and weaknesses of a Convlab2 user model1 . 1 Introduction Remarkable progress has been achieved in many dialogue systems research disciplines, from dialogue state tracking (DST) (Dai et al., 2021; Mehri et al., 2020) to policy- (Wang et al., 2020; Lubis et al., 2020) and end-to-end modelling (Peng et al., 2021; Yang et al., 2020). Progress is usually measured component-wise through task-specific metrics and improvements in the overall performance of the systems leveraging advances in component designs are seldom reported (Takanobu et al., 2020). Takanobu et al. (2020) empirically show that component-wise evaluation may not correlate well with the overall performance of the system. They recommend evaluating dialogue systems in an end-to-end, interactive, multi-turn setting to capture the effect of 1 2 A constraint (e.g., price=cheap) is formed of a slot which constrains the search (price) and its value (cheap). Code available at https://bit.ly/3hVS55Q. 7 Proceedings of the First Workshop on Evaluations and Assessments of Neural Conversation Systems, pages 7–14 Nov 11, 2021. ©2021 Association for Computational Linguistics the same que"
2021.eancs-1.2,2020.acl-main.638,0,0.0154707,"F-score at dialogue level and consider user and system behaviours to improve recall and precision estimation. We facilitate scores interpretation by providing a rich hierarchical structure with information about conversational patterns present in the test data and tools to efficiently query the conversations generated. We apply our framework to assess the performance and weaknesses of a Convlab2 user model1 . 1 Introduction Remarkable progress has been achieved in many dialogue systems research disciplines, from dialogue state tracking (DST) (Dai et al., 2021; Mehri et al., 2020) to policy- (Wang et al., 2020; Lubis et al., 2020) and end-to-end modelling (Peng et al., 2021; Yang et al., 2020). Progress is usually measured component-wise through task-specific metrics and improvements in the overall performance of the systems leveraging advances in component designs are seldom reported (Takanobu et al., 2020). Takanobu et al. (2020) empirically show that component-wise evaluation may not correlate well with the overall performance of the system. They recommend evaluating dialogue systems in an end-to-end, interactive, multi-turn setting to capture the effect of 1 2 A constraint (e.g., price=cheap) i"
2021.eancs-1.2,2020.acl-demos.19,0,0.0759728,"r models perform and how to enhance them to improve system-wise evaluation accuracy. We propose a simple generalisation of the corpus-based, turn-level F1 score proposed by Schatzmann et al. (2005) as a measure of the similarity between the (semantic-level) simulated user response and the response provided by a real user given the same context. We believe this to be necessary since turn-level F1 favours models which are biased to a potentially restricted set of behaviours learned from a corpus whereas an optimal user model should exhibit a wider variety of behaviours. Similar to the Convlab2 (Zhu et al., 2020) evaluation, our metric is goal-driven. It evaluates, at dialogue level, the ability of the user model to express all the constraints2 (I-GCDF1) and request all the information (R-GDCDF1) prescribed by a goal when interacting with an arbitrary agent. In human-human conversation, repetition of constraints occurs due to co-reference, confirmation, emphasis and through other linguistic and conversational processes. Information requests may be specified at the same time with the search constraints and later repeated. Language understanding errors may see agents stuck in conversational loops where"
2021.emnlp-main.620,N19-1423,0,0.0277957,"871–7881 c November 7–11, 2021. 2021 Association for Computational Linguistics generation incorporating a causal attention mechanism. We note that Hosseini-Asl et al. (2020) have demonstrated that GPT-2 can identify slot values as a prediction task, with variable length token sequences produced sequentially with interspersed special tokens indicating slot boundaries. The ability to easily generate token sequences of arbitrary lengths is a valuable feature of the model, although it may come at the expense of modelling power relative to models with non-causal attention mechanisms, such as BERT (Devlin et al., 2019; Shan et al., 2020). In particular, GPT-2’s causality requires that the prediction of later slot values can depend explicitly on previously predicted slot values, but that the reverse is not possible. This can lead to decreased performance in predicting slot values that occur early on. We find that augmenting GPT-2 prediction with representations derived from GATs allows some sharing of information between slots prior to prediction to improve this GPT-2 limitation. igates a limitation of DSTs based on GPT-2 alone, associated with generating domain-slot values in a Left-to-Right manner. (3) We"
2021.emnlp-main.620,2020.sigdial-1.4,0,0.0409603,"Missing"
2021.emnlp-main.620,2020.findings-emnlp.95,0,0.387309,"irs, such as when the ‘star’ tiWOZ 2.0 against a strong GPT-2 baseline level of a booked hotel correlates with the price and investigate a simplified sparse training scerange of a reserved restaurant. nario in which DST models are trained only Graph Neural Networks (GNNs) have been proon session-level annotations but evaluated at the turn level. We further report detailed analposed to captures the interactions among slots and yses to demonstrate the effectiveness of graph values and to improve DST performance (Zhou and models in DST by showing that the proposed Small, 2019; Chen et al., 2020; Wu et al., 2020). graph modules capture inter-slot dependencies These relationships can be represented as edges and improve the predictions of values that are in graph-based models, where domains, slots, and common to multiple domains. values are nodes in the graphs. However previous work has not explored quantitatively or in depth 1 Introduction how graph models utilize the relationships they model. Chen et al. (2020) and Wu et al. (2020) This paper investigates two aspects of dialogue state tracking (DST) for multi-domain task- provide example cases where the predictions of correlated values were potentiall"
2021.emnlp-main.620,2020.acl-main.53,0,0.0711211,"Missing"
2021.emnlp-main.620,2020.acl-main.5,0,0.0760841,"Missing"
2021.emnlp-main.620,2020.acl-main.563,0,0.0322138,"–11, 2021. 2021 Association for Computational Linguistics generation incorporating a causal attention mechanism. We note that Hosseini-Asl et al. (2020) have demonstrated that GPT-2 can identify slot values as a prediction task, with variable length token sequences produced sequentially with interspersed special tokens indicating slot boundaries. The ability to easily generate token sequences of arbitrary lengths is a valuable feature of the model, although it may come at the expense of modelling power relative to models with non-causal attention mechanisms, such as BERT (Devlin et al., 2019; Shan et al., 2020). In particular, GPT-2’s causality requires that the prediction of later slot values can depend explicitly on previously predicted slot values, but that the reverse is not possible. This can lead to decreased performance in predicting slot values that occur early on. We find that augmenting GPT-2 prediction with representations derived from GATs allows some sharing of information between slots prior to prediction to improve this GPT-2 limitation. igates a limitation of DSTs based on GPT-2 alone, associated with generating domain-slot values in a Left-to-Right manner. (3) We investigate how kno"
2021.emnlp-main.620,P19-1078,0,0.150176,"associated with generating domain-slot values in a Left-to-Right manner. (3) We investigate how knowledge-aware models capture relationships between domain-slots and show how using graphs can improve prediction of inter-dependent slot values. While we do show DST accuracy improvements over a strong GPT-2 baseline, we emphasise that our aim is mainly to investigate and improve prediction of domain-slot values using relationships that otherwise are left unmodelled by the baseline. 2 Related Work Statistical DST prioritises general and extensible systems based on machine-learning architectures (Wu et al., 2019; Zhang et al., 2019; Huang et al., 2020; Lee et al., 2020). Systems must be able to predict slot values from domain-specific lists such as list of hotel names as well as from more open-ended categories such as days, prices, Capturing the relationships of slot values across and times. Recent trends are to combine several domains also offers the opportunity to make better strategies to deal differently with the two types of use of limited training data, particularly in sparsely values (Zhang et al., 2019; Zhou and Small, 2019; supervised and weakly supervised scenarios (Liang Heck et al., 2020)"
2021.emnlp-main.620,2020.findings-emnlp.68,0,0.331467,"odels, where domains, slots, and common to multiple domains. values are nodes in the graphs. However previous work has not explored quantitatively or in depth 1 Introduction how graph models utilize the relationships they model. Chen et al. (2020) and Wu et al. (2020) This paper investigates two aspects of dialogue state tracking (DST) for multi-domain task- provide example cases where the predictions of correlated values were potentially enhanced by oriented dialogue (Budzianowski et al., 2018). We their model, while Zhou and Small (2019) and present a novel hybrid architecture that augments Zhu et al. (2020) present ablation studies showGPT-2 (Radford et al., 2019) with dialogue act representations derived from Graph Attention Net- ing marginal improvements brought by their graph modules. Zhu et al. (2020) and Wu et al. (2020) furworks (GATs) (Veliˇckovi´c et al., 2018) in such a ther show joint accuracies over different dialogue way that allows causal, sequential prediction of slot turns, but there is more that can be said about how values while explicitly modelling the relationships between slots and values across domains. Our ap- GATs can improve DST. One of the aims of this paper is to more d"
2021.emnlp-main.666,W19-5301,0,0.061094,"Missing"
2021.emnlp-main.666,W17-4712,0,0.079498,"ing generic models on in- Consolidation (EWC) to preserve the generic perdomain data to yield a domain-specific model (Lu- formance of our model during adaptation (Kirkong and Manning, 2015; Freitag and Al-Onaizan, patrick et al., 2017). We corroborate the finding of 2016). When high quality output on more than one Thompson et al. (2019) and Saunders et al. (2019) target domain is required, multi-domain adaptation that EWC helps to reduce catastrophic forgetting in methods aim to produce a single system that per- machine translation adaptation. However, we find forms well on multiple domains (Britz et al., 2017; the quality trade-off in our multi-domain setting Pham et al., 2019; Currey et al., 2020). to be unfavourable: when preserving most of the ∗ Equal contributions. generic performance, the gains on the new domains 8470 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8470–8477 c November 7–11, 2021. 2021 Association for Computational Linguistics with EWC are limited. We further experiment with data mixing strategies to mitigate catastrophic forgetting and find that they are surprisingly effective. In summary, we make the following contributions: • W"
2021.emnlp-main.666,P18-1008,0,0.0299458,"e status-quo performance – frontier on this task. or generic domain performance – of our models which is easier to control in an adaptation setting. 1 Introduction In this paper, we explore the following research The quality of Neural Machine Translation (NMT) question: given a strong general-purpose model, has improved considerably in recent years, mostly how can we optimize the performance on multiple due to improvements in model architecture (Bah- new, diverse domains of interest without comprodanau et al., 2015; Cho et al., 2014; Vaswani et al., mising on generic domain performance? 2017; Chen et al., 2018). Training NMT models A naive strategy would be to fine-tune on the typically involves collecting parallel training data new domain data and stop as soon as performance from multiple sources to achieve high translation starts to decrease on the generic test set(s). Howquality and generalize well to unseen data (Barrault ever, this method allows for limited gains on et al., 2019). However, translation quality depends the new domains as we quickly start observing strongly on the relevance of the training data to the catastrophic forgetting: performance on previously input text, which is why perf"
2021.emnlp-main.666,D14-1179,0,0.0288256,"Missing"
2021.emnlp-main.666,P17-2061,0,0.0844354,"distinguish between the training domains and Pham et al. (2019) learn domain-specific word embeddings for the domains present in the training data. In contrast, we focus on the adaptation setting where additional domain data becomes available over time. Thompson et al. (2019) apply EWC for adaptation to a single new domain while Saunders et al. (2019) use it for sequentially adapting to two new domains. Both report positive results but at the same time show a performance trade-off which our work tries to address further. Mixing out-of-domain and in-domain data for fine-tuning was proposed by Chu et al. (2017) who use tags to distinguish domains at test time while our models are domain-agnostic. Data mixing is also related to work on Episodic Memories for continual learning. For example, Chaudhry et al. (2019) show that a random sample of previous task data can outperform EWC for image recognition. where θ is a set of model parameters, LB (θ) is the loss for task B and task A is represented by the ∗ and the diagonal of the Fisher inforparameters θA mation matrix F. The strength of the regularization is controlled by λ which can be used to balance the performance on task A versus task B. Intuitively"
2021.emnlp-main.666,2020.emnlp-main.364,0,0.138041,"ield a domain-specific model (Lu- formance of our model during adaptation (Kirkong and Manning, 2015; Freitag and Al-Onaizan, patrick et al., 2017). We corroborate the finding of 2016). When high quality output on more than one Thompson et al. (2019) and Saunders et al. (2019) target domain is required, multi-domain adaptation that EWC helps to reduce catastrophic forgetting in methods aim to produce a single system that per- machine translation adaptation. However, we find forms well on multiple domains (Britz et al., 2017; the quality trade-off in our multi-domain setting Pham et al., 2019; Currey et al., 2020). to be unfavourable: when preserving most of the ∗ Equal contributions. generic performance, the gains on the new domains 8470 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8470–8477 c November 7–11, 2021. 2021 Association for Computational Linguistics with EWC are limited. We further experiment with data mixing strategies to mitigate catastrophic forgetting and find that they are surprisingly effective. In summary, we make the following contributions: • We provide a thorough comparison between data mixing and EWC to prevent catastrophic forgett"
2021.emnlp-main.666,2020.amta-research.10,1,0.715575,"irkpatrick et al. (2017) study the problem of catastrophic forgetting in sequential machine learning settings. They propose EWC as a method to preserve model performance during sequential learning of task B by selectively slowing down learning on the weights that are important for the original task A learned by the model. This goal is achieved by adding a loss term to the training objective as shown in Equation 1: L = LB (θ) + Xλ i 2 2 ∗ 2 Fi (θi − θA,i ) , (1) Related work Most previous work on multi-domain adaptation focuses on a scenario with fixed training data. For example, Currey et al. (2020) use knowledge distillation to build a single model from expert models optimized for the training domains, Britz et al. (2017) train models that better distinguish between the training domains and Pham et al. (2019) learn domain-specific word embeddings for the domains present in the training data. In contrast, we focus on the adaptation setting where additional domain data becomes available over time. Thompson et al. (2019) apply EWC for adaptation to a single new domain while Saunders et al. (2019) use it for sequentially adapting to two new domains. Both report positive results but at the s"
2021.emnlp-main.666,W17-3204,0,0.410735,"s collecting parallel training data new domain data and stop as soon as performance from multiple sources to achieve high translation starts to decrease on the generic test set(s). Howquality and generalize well to unseen data (Barrault ever, this method allows for limited gains on et al., 2019). However, translation quality depends the new domains as we quickly start observing strongly on the relevance of the training data to the catastrophic forgetting: performance on previously input text, which is why performance varies across learned tasks degrades while increasing on the target domains (Koehn and Knowles, 2017a). newly learned tasks (Kirkpatrick et al., 2017). A popular method for domain adaptation of We therefore experiment with Elastic Weight NMT models is fine-tuning generic models on in- Consolidation (EWC) to preserve the generic perdomain data to yield a domain-specific model (Lu- formance of our model during adaptation (Kirkong and Manning, 2015; Freitag and Al-Onaizan, patrick et al., 2017). We corroborate the finding of 2016). When high quality output on more than one Thompson et al. (2019) and Saunders et al. (2019) target domain is required, multi-domain adaptation that EWC helps to redu"
2021.emnlp-main.666,P12-3005,0,0.0783098,"Missing"
2021.emnlp-main.666,2015.iwslt-evaluation.11,0,0.0861051,"Missing"
2021.emnlp-main.666,W18-6319,0,0.0249414,"Missing"
2021.emnlp-main.666,P19-1022,1,0.878198,"Missing"
2021.emnlp-main.666,P16-1162,0,0.160645,"Missing"
2021.emnlp-main.666,W15-3031,0,0.0749789,"Missing"
2021.emnlp-main.666,N19-1209,0,0.0795918,"ich is why performance varies across learned tasks degrades while increasing on the target domains (Koehn and Knowles, 2017a). newly learned tasks (Kirkpatrick et al., 2017). A popular method for domain adaptation of We therefore experiment with Elastic Weight NMT models is fine-tuning generic models on in- Consolidation (EWC) to preserve the generic perdomain data to yield a domain-specific model (Lu- formance of our model during adaptation (Kirkong and Manning, 2015; Freitag and Al-Onaizan, patrick et al., 2017). We corroborate the finding of 2016). When high quality output on more than one Thompson et al. (2019) and Saunders et al. (2019) target domain is required, multi-domain adaptation that EWC helps to reduce catastrophic forgetting in methods aim to produce a single system that per- machine translation adaptation. However, we find forms well on multiple domains (Britz et al., 2017; the quality trade-off in our multi-domain setting Pham et al., 2019; Currey et al., 2020). to be unfavourable: when preserving most of the ∗ Equal contributions. generic performance, the gains on the new domains 8470 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8470–847"
2021.emnlp-main.666,tiedemann-2012-parallel,0,0.107927,"Missing"
C14-1195,D07-1090,0,0.00939615,"The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All"
C14-1195,P05-1033,0,0.528215,"traction and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the GHKM method and use them to complement Hiero-style rules. All these rules are then employed to decode new sentences with source language parse trees. We experiment with our approach in a state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvements on the NIST newswire and web evaluation data of MT08 and MT12. 1 Introduction Synchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT), with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach. Hiero grammars are easily extracted from word-aligned parallel corpora and can capture complex nested translation relationships. Hiero grammars are formally syntactic, but rules are not constrained by source or target language syntax. This lack of constraint can lead to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminal"
C14-1195,P10-1146,0,0.0917775,"obustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Commons Attribution 4"
C14-1195,J10-3008,1,0.897734,"Missing"
C14-1195,P03-2041,0,0.455159,"Missing"
C14-1195,D12-1109,0,0.0143501,"ed in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment and source (or targe"
C14-1195,P06-1121,0,0.157895,"to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminals which are not allowed to be adjacent in the source language. These constraints can yield good performing translation systems, although at a sacrifice in the ability to model long-distance movement and complex reordering of multiple constituents. By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse on either the source or target language side to guide SCFG extraction and translation. The parse tree provides linguistically-motivated constraints both in grammar extraction and in translation. This allows for looser span constraints; rules need not be lexicalised; and rules can have more than two non-terminals to model complex reordering multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translatio"
C14-1195,W10-1761,0,0.0406427,"Missing"
C14-1195,P07-1019,0,0.0225341,"type 1 rules directly. For tree-to-string rules associated with type 2, we attempt to match rules to the source syntactic tree. If a match is found: the source span of the matching tree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted to a Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell. Once this process is finished, RTN construction, expansion, and language model composition proceeds as usual. Similar modifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011). We now elaborate on the rule matching strategy. Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exception that we impose no span limit on rule applications for source spans corresponding to constituents in the Chinese syntactic tree. Rule matching, the procedure that determines if a rule applies to a source span, is based on string matching (see Figure 4(a)). For example, the type 1 rule h9 in Figure 4(c) can be applied to spans (1,13) and (2,13) since both of them agree with tree constitu"
C14-1195,D10-1027,0,0.0168031,"an limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment"
C14-1195,2006.amta-papers.8,0,0.366008,"Missing"
C14-1195,E09-1044,1,0.894124,"Missing"
C14-1195,D11-1127,1,0.87318,"Missing"
C14-1195,N13-1060,0,0.0232239,"Missing"
C14-1195,P06-1077,0,0.614988,"Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingua"
C14-1195,P09-1063,0,0.365919,"Missing"
C14-1195,P09-1065,0,0.0321399,"Missing"
C14-1195,D08-1076,0,0.0276395,"sducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments a"
C14-1195,W06-1606,0,0.0321305,"ifference lies in that the tree-to-string rule indicator feature does not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities:"
C14-1195,P08-1114,0,0.143232,"multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We al"
C14-1195,D08-1022,0,0.0227804,"not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities: P Pphr (tr |sr ) = r00 :ϕ(sr00 )=ϕ(sr )∧tr00 =tr P P Pphr (sr |tr ) ="
C14-1195,P08-1023,0,0.02228,"re: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with"
C14-1195,P02-1040,0,0.0950826,"ror rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be 2069 Entry System exp01 exp02 exp03 exp04 exp05 exp06 exp07 exp08 exp09 exp10 baseline += no span limit += t-to-s rules += t-to-s features t-to-s baseline exp04 on spans &gt; 10 exp04 with null trans. exp04 + left binariz. exp04 + right binariz. exp04 + forest binariz. tune (1755) 35.84 36.05 36.63 36.82 34.63 36.17 36.10 37.11 36.58 37.03 mt08 (691) 35.85 36.08 36.51 36.49 34.44 36.11 36.03 37.46 36.56 37.2"
C14-1195,N07-1051,0,0.0249299,"track. Word alignments are obtained using MTTK (Deng and Byrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links. The data from newswire and web genres was used for tuning and test. The development sets contain 1,755 sentences and 2160 sentences for the two genres respectively. The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is app"
C14-1195,W13-2225,1,0.875894,"Missing"
C14-1195,2010.iwslt-papers.18,0,0.0179447,"inese syntactic structure indicating the reordered translations of NP and VP. However, such a rule would not normally be included in a Hiero grammar, as it would require consecutive source language non-terminals (see Figure 3). 3 The Proposed Approach Both the tree-to-string model and the hierarchical phrase-based model have their own strengths and weaknesses. For example, tree-to-string systems are good at modelling long distance reordering, while hierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences1 and free translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010). Here we present a method to enhance hierarchical phrase-based systems with tree-to-string rules and benefit from both models. The idea is simple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, and then use tree-to-string rules as additional rules in decoding with the SCFG. Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach. Our approach requires source language parse trees to be input in both rule extraction and decoding. In rule extraction, we acquire tree-to-string rules using the GHKM method and Hiero-sty"
C14-1195,J10-2004,0,0.0237385,"es the baseline result using the Hiero model (i.e., type 1 rules only). To investigate the effect of failed parse trees on system performance, we also report the BLEU score including null translations for which the parser fails. As shown in exp07, there are significantly lower BLEU scores when null translations are included. It indicates that our approach is more robust than standard treeto-string systems which would generate an empty translation if the source language parser fails. • Results on Binarization (exp08-10) Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010). exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina2070 Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ... Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ... GHKM+Hiero: After North Korea again demanded that U.S. promised concessions before the new round of six-nation talks , ... a Hiero rule X → h 在 X1 后, after X1 i is applied on span (1,15) Input: 北韩2 再度3 要求4 美国5 于6 新7 回合8 六9 国10 会谈11 前12 承诺13 让步"
C14-1195,P12-3004,1,0.88289,"guage model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of"
C14-1195,D11-1020,0,0.361476,"Missing"
C14-1195,P08-1064,0,0.375097,"Missing"
C14-1195,D08-1060,0,0.302527,"s. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Common"
C14-1195,W06-3119,0,0.264258,"rmed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licen"
C14-1195,2010.iwslt-keynotes.2,0,\N,Missing
C14-1195,J07-2003,0,\N,Missing
D15-1273,J07-2003,0,0.0602421,"al neural network language model (BiLM) of Devlin et al. (2014) to the output lattices of the CUED OpenMT12 Arabic-English hierarchical phrase-based translation system3 using HiFST (de Gispert et al., 2010). We use a development set mt0205tune (2075 sentences) and a validation set mt0205test (2040 sentences) from the NIST MT02 through MT05 evaluation sets. The edges in these WFSTs are of the form t:i/w, where t is the target word, i is the source sentence position t aligns to, and w contains the translation and language model score. HiFST outputs these WFSTs by using a standard hiero grammar (Chiang, 2007) augmented with target side heuristic alignments or affiliations to the source (Devlin et al., 2014). In a rule over source and target words X →&lt; s1 X s2 s3 , t1 X t2 &gt; / 2, 1 the feature ‘2, 1’ indicates that the target word t1 is aligned to source word s2 and that t2 aligns to s1 . As rules are applied in translation, this information can be used to link target words to absolute positions within the source sentence. Allowing for admissible pruning, all possible affiliation sequences under the grammar for every translation are available in the WFSTs; disambiguation keeps the best affiliation"
D15-1273,J10-3008,1,0.937855,"Missing"
D15-1273,D13-1140,0,0.111952,"Missing"
D15-1273,P14-1129,0,0.160875,"n We present a new disambiguation algorithm that can efficiently accomplish this. In Section 2 we describe how the tropical sparse tuple vector semiring can keep track of individual arcs in the original WFST as topological features during the mapping step (a). This allows us to describe in Section 3 an efficient expansion algorithm for step (c). We show in Section 4 empirical evidence that our algorithm is more efficient than Shafran et al. (2011) in their same PoS-tagging task. We also show how our method can be applied in rescoring translation lattices under a bilingual neuralnetwork model (Devlin et al., 2014), obtaining BLEU score gains consistent with the literature. Section 5 reviews related work and concludes. 2 Semiring Definitions A WFST T = (Σ, ∆, Q, I, F, E, ρ) over a semiring (K, ⊕, ⊗, 0, 1) has input and output alphabets Σ and ∆, a set of states Q, the initial state I ∈ Q, a set of final states F ⊂ Q, a set of transitions (edges) E ⊂ (Q × Σ × ∆ × K × Q), and a final state function ρ : F → K. We focus on extensions to the tropical semiring (R ± ∞, min, +, ∞, 0). the lattice, searches for the best output string for each input string, and converts the resulting sequences back into a WFST, wh"
D15-1273,W11-2123,0,0.0307226,"categorial method has only finished 1405. The slowest WFST to disambiguate takes 6700 seconds with the categorial procedure, which compares to 1000 seconds in our case. The BiLM model is trained with NPLM (Vaswani et al., 2013) with a context 3 See http://www.nist.gov/itl/iad/mig/openmt12results.cfm. mt0205tune 52.2 53.0 mt0205test 51.9 52.9 Table 1: Translation scores in lower-case BLEU. of 3 source and 4 target words. Lattice rescoring with this model requires a special variation of the standard WFST composition which looks at both input and output labels on a transducer arc; we use KenLM (Heafield, 2011) to retrieve neural network scores for on-the-fly composition. We retune the parameters with Lattice MERT (Macherey et al., 2008) . Results are shown in Table 1. Acknowledging the task differences with respect to (Devlin et al., 2014), we find BLEU gains consistent with rescoring results reported in their Table 5. 5 Conclusions and Related Work We have described a tagging disambiguation algorithm that supports non-functional WFSTs, which cannot be handled directly by neither WFST determinization (Mohri, 1997) nor WFST disambiguation (Mohri, 2012). We show it is faster than the implementation w"
D15-1273,D08-1076,0,0.100196,"re, which compares to 1000 seconds in our case. The BiLM model is trained with NPLM (Vaswani et al., 2013) with a context 3 See http://www.nist.gov/itl/iad/mig/openmt12results.cfm. mt0205tune 52.2 53.0 mt0205test 51.9 52.9 Table 1: Translation scores in lower-case BLEU. of 3 source and 4 target words. Lattice rescoring with this model requires a special variation of the standard WFST composition which looks at both input and output labels on a transducer arc; we use KenLM (Heafield, 2011) to retrieve neural network scores for on-the-fly composition. We retune the parameters with Lattice MERT (Macherey et al., 2008) . Results are shown in Table 1. Acknowledging the task differences with respect to (Devlin et al., 2014), we find BLEU gains consistent with rescoring results reported in their Table 5. 5 Conclusions and Related Work We have described a tagging disambiguation algorithm that supports non-functional WFSTs, which cannot be handled directly by neither WFST determinization (Mohri, 1997) nor WFST disambiguation (Mohri, 2012). We show it is faster than the implementation with a lexicographic-tropicalcategorial semiring described by Shafran et al. (2011) and describe a use case in a practical rescori"
D15-1273,J97-2003,0,0.0723358,"ion which looks at both input and output labels on a transducer arc; we use KenLM (Heafield, 2011) to retrieve neural network scores for on-the-fly composition. We retune the parameters with Lattice MERT (Macherey et al., 2008) . Results are shown in Table 1. Acknowledging the task differences with respect to (Devlin et al., 2014), we find BLEU gains consistent with rescoring results reported in their Table 5. 5 Conclusions and Related Work We have described a tagging disambiguation algorithm that supports non-functional WFSTs, which cannot be handled directly by neither WFST determinization (Mohri, 1997) nor WFST disambiguation (Mohri, 2012). We show it is faster than the implementation with a lexicographic-tropicalcategorial semiring described by Shafran et al. (2011) and describe a use case in a practical rescoring task of an MT system with bilingual neural networks that yield 1.0 BLEU gain. Povey et al. (2012) also use a special semiring that allows to map non-functional WFSTs into WFSAs by inserting the tag into a string weight. However, in contrast to our implementation and that of Shafran et al (2011), no expansion into an WFST with aligned input/output is described. Lexicographic semir"
D15-1273,P11-2001,0,0.0720733,"Missing"
D15-1273,J14-3008,1,\N,Missing
D15-1273,J14-4002,0,\N,Missing
D15-1273,2010.iwslt-keynotes.2,0,\N,Missing
D17-1208,N16-1100,1,0.889403,"Missing"
D17-1208,D14-1179,0,0.021607,"Missing"
D17-1208,P16-1160,0,0.201878,"e ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system. 1 Introduction The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017). Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT. The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017). The ensemble decoder computes predictions from each of the individual models which are then combine"
D17-1208,W16-4616,0,0.0193121,"dy improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system. 1 Introduction The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017). Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT. The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017). The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014)"
D17-1208,J82-2005,0,0.656201,"Missing"
D17-1208,W16-2316,0,0.021939,"this network afterwards. Another approach, known as knowledge distillation, uses the large model (the teacher) to generate soft training labels for the smaller student network (Bucilu et al., 2006; Hinton et al., 2014). The student network is trained by minimizing the cross-entropy to the teacher. This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017). Our approach can be computationally more efficient as the training set does not have to be decoded by the large teacher network. Junczys-Dowmunt et al. (2016a; 2016b) reported gains from averaging the weight matrices of multiple checkpoints of the same training run. However, our attempts to replicate their approach were not successful. Averaging might work well when the behaviour of corresponding units is similar across networks, but that cannot be guaranteed when networks are trained independently. 6 Conclusion We have described a generic method for improving the decoding speed and BLEU score of single system NMT. Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by remov"
D17-1208,D16-1139,0,0.207117,"h of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014) or the geometric average (Cromieres et al., 2016). Ensembling consistently outperforms single NMT by a large margin. However, the decoding speed is significantly worse since the decoder needs to apply K NMT models rather than only one. Therefore, a recent line of research transfers the idea of knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014) to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system (the teacher) (Kim and Rush, 2016; Freitag et al., 2017). This paper presents an alternative to knowledge distillation as we aim to speed up decoding to be comparable to single NMT while retaining the boost in translation accuracy from the ensemble. In a first step, we describe how to construct a single large neural network which imitates the output of an ensemble of multiple networks with the same topology. We will refer to this process as unfolding. GPU-based decoding with the unfolded network is often much faster than ensemble decoding since more work can be done on the GPU. In a second step, we explore methods to reduce t"
D17-1208,W16-4610,0,0.0860138,"e show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system. 1 Introduction The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017). Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT. The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017). The ensemble decoder computes predictions from each of the individual models which are then combined using the ar"
D17-1208,W15-5003,0,0.0579499,"hdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers. We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016). We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. (2015). We report cased BLEU scores calculated with Moses’ multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix siz"
D17-1208,W16-2323,0,0.229104,"mitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system. 1 Introduction The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017). Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT. The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017). The ensemble decoder computes predictions from each of the individual models w"
D17-1208,P16-1162,0,0.638582,"mitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system. 1 Introduction The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017). Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT. The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017). The ensemble decoder computes predictions from each of the individual models w"
D17-1208,D17-2005,1,0.837089,"NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta (Zeiler, 2012) on the Blocks/Theano implementation (van Merri¨enboer et al., 2015; Bastien et al., 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers. We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016). We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. (2015). We report cased BLEU scores calculated with Moses’ multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are"
D17-1208,1983.tc-1.13,0,0.288229,"Missing"
D17-1208,K16-1029,0,0.0220111,"ls on En-De. ments in BLEU compared to Single with about the same decoding speed. 5 Related Work The idea of pruning neural networks to improve the compactness of the models dates back more than 25 years (LeCun et al., 1989). The literature is therefore vast (Augasta and Kathirvalavakumar, 2013). One line of research aims to remove unimportant network connections. The connections can be selected for deletion based on the secondderivative of the training error with respect to the weight (LeCun et al., 1989; Hassibi et al., 1993), or by a threshold criterion on its magnitude (Han et al., 2015). See et al. (2016) confirmed a high degree of weight redundancy in NMT networks. In this work we are interested in removing neurons rather than single connections since we strive to shrink the unfolded network such that it resembles the layout of an individual model. We argued in Sec. 4 that removing neurons rather than connections does not only improve the model size but also the memory footprint and decoding speed. As explained in Sec. 3.1, our data-free method is an extension of the approach by Srinivas and Babu (2015); our extension performs significantly better on NMT networks. Our data-bound method (Sec."
D17-1220,P15-2073,0,0.0127532,"0.15. 6 Related Work Work on modeling of social annotations has mainly focused on the use of topic models (Iwata et al., 2009; Das et al., 2014) in which annotations are assumed to originate from topics. They can be used as a preprocessing step in machine learning tasks such as text classification and image recognition but do not generate language as required in our ALA task. Text simplification and paraphrase generation have been widely studied. Recent work has highlighted the need for large text collections (Xu et al., 2015) as well as more appropriate evaluation measures (Xu et al., 2016; Galley et al., 2015). They indicated that especially informal language, Conclusion and Future Work We presented and released the Genius dataset to study the task of Automated Lyric Annotation. As a first investigation, we studied automatic generation of context independent annotations as machine translation and information retrieval. Our baseline system tests indicate that our corpus is suitable to train machine translation systems. Standard SMT models are capable of rephrasing and simplifying song lyrics but tend to keep close to the structure of the song lyric. Seq2Seq models demonstrated potential to generate"
D17-1220,J03-1002,0,0.00750271,"o generate large quantities of parallel text from Genius, users operate without a single, predefined and shared global goal other than to maximize their own IQ points. As such, there is no motivation to provide annotations for a song in its entirety, or independent of previous annotations. 2075 • Statistical Machine Translation (SMT): One approach is to treat the task as one of translation, and to use established statistical machine translation (SMT) methods (Quirk et al., 2004) to produce them. We train a standard phrase-based SMT model to translate lyrics to annotations, using GIZA++ (Josef Och and Ney, 2003) for word alignment and Moses (Koehn et al., 2007) for phrasal alignment, training, and decoding. • Seq2Seq: Sequence-to-sequence models (Sutskever et al., 2014) offer an alternative to SMT systems, and have been applied successfully to a variety of tasks including machine translation. In Seq2Seq, a recurrent neural network (RNN) encodes the source sequence to a single vector representation. A separate decoder RNN generates the translation conditioned on this representation of the source sequence’s semantics. We utilize Seq2Seq with attention (Bahdanau et al., 2014), which allows the model to"
D17-1220,P17-4012,0,0.0234614,"Missing"
D17-1220,P07-2045,0,0.0106051,"Missing"
D17-1220,P02-1040,0,0.103714,"the same lyric, resulting in a total of 1,813,350 sentence pairs. We use this collection of sentence pairs (denoted as sent. in results) to train the SMT model. Seq2Seq models are trained using sentence pairs as well as full annotations. Interestingly, techniques encouraging alignment by matching length and thresholding cosine distance between lyric and annotation did not improve performance during development. 4.2 Measures For automated evaluation, we use measures commonly used to evaluate translation systems (BLEU, METEOR), paraphrase generation (iBLEU) and text simplification (SARI). BLEU (Papineni et al., 2002) uses a modified form of precision to compare generated annotations against references from Genius. METEOR (Denkowski and Lavie, 2011) is based on the harmonic mean of precision and recall and, along with exact word matching, includes stemming and synonymy matching. iBLEU (Sun and Zhou, 2012) is an extension of the BLEU metric to measure diversity as well as adequacy of the annotation, iBLEU = 0.9 × BLEU(Annotation, Reference) − 0.1 × BLEU(Annotation, Lyric). SARI (Xu et al., 2016) measures precision and recall of words that are added, kept, or deleted separately and averages their arithmetic"
D17-1220,D15-1221,0,0.0201258,"btain higher iBLEU 2077 Figure 2: Attention visualization of Seq2Seq models for ALA. with its high degree of lexical variation, e.g., as used in social media or lyrics, poses serious challenges (Xu et al., 2013). Text generation for artistic purposes, such as poetry and lyrics, has been explored most commonly using templates and constraints (Barbieri et al., 2012). In regard to rap lyrics, Wu et al. (2013) present a system for rap lyric generation that produces a single line of lyrics that is meant to be a response to a single line of input. Most recent work is that of Zhang et al. (2014) and Potash et al. (2015), who show the effectiveness of RNNs for the generation of poetry and lyrics. The task of annotating song lyrics is also related to metaphor processing. As annotators often explain metaphors used in song lyrics, the Genius dataset can serve as a resource to study computational modeling of metaphors (Shutova and Teufel, 2010). 7 and Information scores. To visualize some of the alignments learned by the translation models, Fig. 2 shows word-by-word attention scores for a translation by the Seq2Seq model. While the retrieval model obtains quality annotations when test lyrics are highly similar to"
D17-1220,shutova-teufel-2010-metaphor,0,0.0221753,"lates and constraints (Barbieri et al., 2012). In regard to rap lyrics, Wu et al. (2013) present a system for rap lyric generation that produces a single line of lyrics that is meant to be a response to a single line of input. Most recent work is that of Zhang et al. (2014) and Potash et al. (2015), who show the effectiveness of RNNs for the generation of poetry and lyrics. The task of annotating song lyrics is also related to metaphor processing. As annotators often explain metaphors used in song lyrics, the Genius dataset can serve as a resource to study computational modeling of metaphors (Shutova and Teufel, 2010). 7 and Information scores. To visualize some of the alignments learned by the translation models, Fig. 2 shows word-by-word attention scores for a translation by the Seq2Seq model. While the retrieval model obtains quality annotations when test lyrics are highly similar to lyrics from the training set, retrieved annotations are often unrelated to the test lyric or specific to the song lyric it is retrieved from. Out of the unsupervised metrics, METEOR obtained the highest Pearson correlation (Pearson, 1895) with human ratings for Information with a coefficient of 0.15. 6 Related Work Work on"
D17-1220,P12-2008,0,0.0137595,"g length and thresholding cosine distance between lyric and annotation did not improve performance during development. 4.2 Measures For automated evaluation, we use measures commonly used to evaluate translation systems (BLEU, METEOR), paraphrase generation (iBLEU) and text simplification (SARI). BLEU (Papineni et al., 2002) uses a modified form of precision to compare generated annotations against references from Genius. METEOR (Denkowski and Lavie, 2011) is based on the harmonic mean of precision and recall and, along with exact word matching, includes stemming and synonymy matching. iBLEU (Sun and Zhou, 2012) is an extension of the BLEU metric to measure diversity as well as adequacy of the annotation, iBLEU = 0.9 × BLEU(Annotation, Reference) − 0.1 × BLEU(Annotation, Lyric). SARI (Xu et al., 2016) measures precision and recall of words that are added, kept, or deleted separately and averages their arithmetic means. We also measure quality by crowdsourcing ratings via the online platform CrowdFlower.4 We present collaborators with a song lyric excerpt annotated with output from the annotation generators as well as a reference annotation from Genius. Collaborators assign a 5-point rating for Fluenc"
D17-1220,D13-1011,0,0.0145589,"r tuning the decoder which promotes lexical dissimilarity as done for paraphrase generation, would be beneficial for this approach. Seq2Seq models generate annotations more dissimilar to the song lyric and obtain higher iBLEU 2077 Figure 2: Attention visualization of Seq2Seq models for ALA. with its high degree of lexical variation, e.g., as used in social media or lyrics, poses serious challenges (Xu et al., 2013). Text generation for artistic purposes, such as poetry and lyrics, has been explored most commonly using templates and constraints (Barbieri et al., 2012). In regard to rap lyrics, Wu et al. (2013) present a system for rap lyric generation that produces a single line of lyrics that is meant to be a response to a single line of input. Most recent work is that of Zhang et al. (2014) and Potash et al. (2015), who show the effectiveness of RNNs for the generation of poetry and lyrics. The task of annotating song lyrics is also related to metaphor processing. As annotators often explain metaphors used in song lyrics, the Genius dataset can serve as a resource to study computational modeling of metaphors (Shutova and Teufel, 2010). 7 and Information scores. To visualize some of the alignments"
D17-1220,Q15-1021,0,0.0262616,"orrelation (Pearson, 1895) with human ratings for Information with a coefficient of 0.15. 6 Related Work Work on modeling of social annotations has mainly focused on the use of topic models (Iwata et al., 2009; Das et al., 2014) in which annotations are assumed to originate from topics. They can be used as a preprocessing step in machine learning tasks such as text classification and image recognition but do not generate language as required in our ALA task. Text simplification and paraphrase generation have been widely studied. Recent work has highlighted the need for large text collections (Xu et al., 2015) as well as more appropriate evaluation measures (Xu et al., 2016; Galley et al., 2015). They indicated that especially informal language, Conclusion and Future Work We presented and released the Genius dataset to study the task of Automated Lyric Annotation. As a first investigation, we studied automatic generation of context independent annotations as machine translation and information retrieval. Our baseline system tests indicate that our corpus is suitable to train machine translation systems. Standard SMT models are capable of rephrasing and simplifying song lyrics but tend to keep close"
D17-1220,Q16-1029,0,0.206258,"distance as similarity measure between lyrics’ excerpts. 4 4.1 Evaluation Data We evaluate automatic annotators on a selection of 354 CI annotations and partition the rest of the annotations into 2,000 instances for development and the full remainder for training. It is important to note that the annotations used for training and development include CI as well as CS annotations. Annotations often include multiple sentences or even paragraphs for a single lyrics excerpt (which does not include end marks), while machine translation models need aligned corpora at sentence level to perform well (Xu et al., 2016). We therefore transform training data by including each sentence from the annotation as a single training instance with the same lyric, resulting in a total of 1,813,350 sentence pairs. We use this collection of sentence pairs (denoted as sent. in results) to train the SMT model. Seq2Seq models are trained using sentence pairs as well as full annotations. Interestingly, techniques encouraging alignment by matching length and thresholding cosine distance between lyric and annotation did not improve performance during development. 4.2 Measures For automated evaluation, we use measures commonly"
D17-1220,D17-1197,0,0.0230723,"Missing"
D17-1220,D14-1074,0,0.0244732,"Missing"
D17-1220,W11-2100,0,\N,Missing
D17-2005,J14-3008,1,0.89864,"Missing"
D17-2005,P16-1162,0,0.142015,"or (i.e. system) rather than only one as in normal beam search. We report a moderate gain of 0.5 BLEU over a single system on the Japanese-English ASPEC test set (Nakazawa et al., 2016) by combining three BPE-based NMT models from Stahlberg et al. (2017) using the sepbeam decoder. Ensembling with models at multiple tokenization levels SGNMT allows masking predictors with alternative sets of modelling units. The conversion between the tokenization schemes of different predictors is defined with FSTs. This makes it possible to decode by combining scores from both a subword-unit (BPE) based NMT (Sennrich et al., 2016) and a word-based NMT model with character-based NMT, masking the BPE-based and word-based NMT predictors with FSTs which transduce character sequences to BPE or word sequences. Masking is transparent to the decoding strategy as predictors are replaced by a special wrapper (fsttok) that uses the masking FST to translate predict next() and consume() calls to (a series of) predictor calls with alternative tokens. The syncbeam variation of beam search compares competing hypotheses only after consuming a special word boundary symbol rather than after each token. This allows combining scores at the"
D17-2005,D13-1110,0,0.0309801,"rministic lattices. Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion. n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit. Neural n-gram language models based on NPLM (Vaswani et al., 2013). Integrates RNN language models with TensorFlow as described by Zaremba et al. (2014). Forced decoding with a single reference. n-best list rescoring. Restricts the search space to a bag of words with or without repetition (Hasler et al., 2017). Experimental implementation of leftto-right Hiero (Siahbani et al., 2013) for small grammars. Number of words feature. Applies a Poisson model for the number of UNKs in the output. Integrates external n-gram posteriors, e.g. for MBR-based NMT according Stahlberg et al. (2017). Target sentence length model using simple source sentence features. Table 2: Currently implemented predictors. 26 Decoder greedy beam • consume(token) Update the internal predictor state by adding token to the current history. dfs The structure of the predictor state and the implementations of these methods differ substantially between predictors. Tab. 2 lists all predictors which are current"
D17-2005,E17-2058,1,0.87293,"Missing"
D17-2005,P16-2049,1,0.948596,"d forcedlst bow lrhiero • initialize(src sentence) Initialize the predictor state using the source sentence. wc unkc • get state() Get the internal predictor state. ngramc • set state(state) Set the internal predictor state. length • predict next() Given the internal predictor state, produce the posterior over target tokens for the next position. Description Attention-based neural machine translation following Bahdanau et al. (2015). Supports Blocks/Theano (Bastien et al., 2012; van Merri¨enboer et al., 2015) and TensorFlow (Abadi et al., 2016). Predictor for rescoring deterministic lattices (Stahlberg et al., 2016). Predictor for rescoring nondeterministic lattices. Rescoring recurrent transition networks (RTNs) as created by HiFST (Allauzen et al., 2014) with late expansion. n-gram Kneser-Ney language model using the SRILM (Heafield et al., 2013; Stolcke et al., 2002) toolkit. Neural n-gram language models based on NPLM (Vaswani et al., 2013). Integrates RNN language models with TensorFlow as described by Zaremba et al. (2014). Forced decoding with a single reference. n-best list rescoring. Restricts the search space to a bag of words with or without repetition (Hasler et al., 2017). Experimental imple"
D17-2005,D13-1140,0,0.376701,"ch ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group. 1 Introduction We are developing an open source decoding framework called SGNMT, short for Syntactically Guided Neural Machine Translation.1 The software package supports a number of well-known frameworks, including TensorFlow2 (Abadi et al., 2016), OpenFST (Allauzen et al., 2007), Blocks/Theano (Bastien et al., 2012; van Merri¨enboer et al., 2015), and NPLM (Vaswani et al., 2013). The two central concepts in the 1 http://ucam-smt.github.io/sgnmt/html/ SGNMT relies on the TensorFlow fork available at https://github.com/ehasler/tensorflow 2 3 http://www.mlsalt.eng.cam.ac.uk/Main/ CurrentMPhils 25 Proceedings of the 2017 EMNLP System Demonstrations, pages 25–30 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Predictor NMT Predictor state State vector in the GRU or LSTM layer of the decoder network and current context vector. initialize(·) Run encoder network to compute annotations. FST ID of the current node in the FST. Load FS"
D17-2005,W17-3531,1,0.892018,"Missing"
D17-2005,P13-2121,0,0.0588947,"Missing"
D19-1125,W17-5506,0,0.0356549,"process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to learn Dialogue State Tracking (DST) jointly with the text generation component (Eric et al., 2017; Wu et al., 2019), the most effective models use it as an intermediate signal (Wen et al., 2017; Lei et al., 2018). The difficulty of state tracking has made this task a driving force behind most of the Dialog System Technology Challenges in recent years (Henderson et al., 2014; Kim et al., 2017). In this paper, we reduce the reliance of taskoriented dialogue systems on data collection by leveraging semi-supervised training (Chapelle et al., 2009). Two approaches are investigated and evaluated for providing an improved training signal to the dialogue state tracking component in an end-to-end"
D19-1125,W10-4334,0,0.0889143,"Missing"
D19-1125,P18-1133,0,0.0505559,"en et al., 2015). Recent progress in sequence-to-sequence (seq2seq) modelling has enabled the development of fully neural end-to-end architectures, allowing for different components to be optimized jointly in order to share information (Wen et al., 2017; Zhao et al., 2017; Budzianowski and Vuli´c, 2019). Dialogue systems benefit greatly from optimizing on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to le"
D19-1125,W15-4640,0,0.030933,"on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are several orders of magnitude smaller compared to general open-domain dialogue corpora (Lowe et al., 2015; Henderson et al., 2019). Arguably, one of the most challenging parts of dialogue modelling is maintaining an interpretable internal memory over crucial domain concepts (Williams et al., 2016). Although there is increasing research effort to learn Dialogue State Tracking (DST) jointly with the text generation component (Eric et al., 2017; Wu et al., 2019), the most effective models use it as an intermediate signal (Wen et al., 2017; Lei et al., 2018). The difficulty of state tracking has made this task a driving force behind most of the Dialog System Technology Challenges in recent years (Hen"
D19-1125,D15-1166,0,0.00765738,"rrectly recognize the mentioned slot-value pairs in the user utterance and to maintain the updated dialogue (belief) state. Let i, j and k denote the index of domain, slot and value. As depicted at the top of Figure 1, the user utterance w1 :wL at turn t is first encoded by the BiLSTM to obtain the hidden states ht1:L . The encoding of the slot-value pair svkij is the output of the affine layer that takes the concatenation of the embeddings of domain i, slot j and value k as the input. The context vector aij k is then computed by the attention mechanism, denoted as attn in Figure 1, following Luong et al. (2015): el = sim(hl , svkij ) aij k = L ∑ el hl , (1) (2) l=1 where l is the word index of the user utterance and sim denotes any function that calculates the similarity of two vectors. We adopt here the dot product function, following Mrkši´c et al. (2017); Zhong et al. (2018); Ramadan et al. (2018). The ij ij similarity score sij k between ak and svk is then computed to see whether the slot-value pair svkij is mentioned in the utterance. The mentioned pair should have higher similarity score to its context vector than those which are not mentioned. The softmax layer is then applied to form the pro"
D19-1125,P17-1163,0,0.0997314,"Missing"
D19-1125,P18-2069,1,0.913302,"Missing"
D19-1125,P17-1061,0,0.0312897,"ts or providing information to visitors in a new city (Raux et al., 2005). Most current industry-oriented systems rely on modular, domain-focused frameworks (Young et al., 2013; Sarikaya et al., 2016), with separate components for user understanding (Henderson et al., 2014), decision making (Gaši´c et al., 2010) and system answer generation (Wen et al., 2015). Recent progress in sequence-to-sequence (seq2seq) modelling has enabled the development of fully neural end-to-end architectures, allowing for different components to be optimized jointly in order to share information (Wen et al., 2017; Zhao et al., 2017; Budzianowski and Vuli´c, 2019). Dialogue systems benefit greatly from optimizing on detailed annotations, such as turn-level dialogue state labels (Henderson et al., 2014) or dialogue actions (Rieser and Lemon, 2011), with end-to-end architectures still relying on intermediate labels in order to obtain satisfactory results (Lei et al., 2018). Collecting these labels is often the bottleneck in dataset creation, as the process is expensive and time-consuming, requiring domain and expert knowledge (Asri et al., 2017). Due to this restriction, existing datasets for task-oriented dialogue are sev"
D19-1125,P18-1135,0,0.0479066,"by the BiLSTM to obtain the hidden states ht1:L . The encoding of the slot-value pair svkij is the output of the affine layer that takes the concatenation of the embeddings of domain i, slot j and value k as the input. The context vector aij k is then computed by the attention mechanism, denoted as attn in Figure 1, following Luong et al. (2015): el = sim(hl , svkij ) aij k = L ∑ el hl , (1) (2) l=1 where l is the word index of the user utterance and sim denotes any function that calculates the similarity of two vectors. We adopt here the dot product function, following Mrkši´c et al. (2017); Zhong et al. (2018); Ramadan et al. (2018). The ij ij similarity score sij k between ak and svk is then computed to see whether the slot-value pair svkij is mentioned in the utterance. The mentioned pair should have higher similarity score to its context vector than those which are not mentioned. The softmax layer is then applied to form the probability distribution pinf ij for each informable slot sinf ij , where the predicted value is the value with 1274 the highest probability. The same attention mechanism is used for each requestable slot reqr to decide whether the user has asked for the slot in the current"
D19-1125,D15-1199,0,0.0220238,"Missing"
D19-1125,E17-1042,0,0.0922133,"Missing"
D19-1331,N18-1205,0,0.0263457,"the locally normalized maximum likelihood training objective in NMT that underestimates the margin between the correct translation and shorter ones if trained with regularization and finite data. A similar argument was made by Murray and Chiang (2018) who pointed out the difficulty for a locally normalized model to estimate the “budget” for all remaining (longer) translations. Kumar and Sarawagi (2019) demonstrated that NMT models are often poorly calibrated, and that that can cause the length deficiency. Ott et al. (2018) argued that uncertainty caused by noisy training data may play a role. Chen et al. (2018) showed that the consistent best string problem for RNNs is decidable. We provide an alternative DFS algorithm that relies on the monotonic nature of model scores rather than consistency, and that often converges in practice. To the best of our knowledge, this is the first work that reports the exact number of search errors in NMT as prior work often relied on approximations, e.g. via n-best lists (Niehues et al., 2017) or constraints (Stahlberg et al., 2018b). 6 Conclusion We have presented an exact inference scheme for NMT. Exact search may not be practical, but it allowed us to discover def"
D19-1331,D17-1227,0,0.0662159,"han the score of any of its translation prefixes. While this monotonicity condition is true for vanilla NMT (Eq. 3), it does not hold for methods like length normalization (Jean et al., 2015; Boulanger-Lewandowski et al., 2013; Wu et al., 2016) or word rewards (He et al., 2016): Length normalization gives an advantage to longer hypotheses by dividing the score by the sentence length, while a word reward directly violates monotonicity as it rewards each word with a positive value. In Sec. 4 we show how our exact search can be extended to handle arbitrary length models (Murray and Chiang, 2018; Huang et al., 2017; Yang et al., 2018) by introducing length dependent lower bounds γk and report initial findings on exact search under length normalization. However, despite being of practical use, methods like length normalization and word penalties are rather heuristic as they do not have any justification from a probabilistic perspective. They also do not generalize well as (without retuning) they often work only for a specific beam size. It would be much more desirable to fix the length bias in the NMT model itself. Search Greedy Beam-10 Exact Log-likelihood Alg. 2 specifies the DFS algorithm formally. An"
D19-1331,W15-3014,0,0.092067,"Missing"
D19-1331,J82-2005,0,0.569288,"Missing"
D19-1331,D13-1176,0,0.05451,"s global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation. 1 ˆ = arg max P (y|x). y Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015, NMT) assigns the probability P (y|x) of a translation y = y1J ∈ T J of length J over the target language vocabulary T for a source sentence x ∈ S I of length I over the source language vocabulary S via a left-to-right factorization using the chain rule: log P (y|x) = J X log P (yj |y1j−1 , x). (1) j=1 ˆ∈ The task of finding the most likely translation y ∗ T for a given source sentence x is known as the * Now at Google. (2) y∈T ∗ The NMT search space is vast as it grows exponentially with the sequence length. For example, for a common vocabulary"
D19-1331,W17-3204,0,0.0612282,"1 (4) Exact search under length normalization does not suffer from the length deficiency anymore (last row in Tab. 4), but it is not able to match our best BLEU score under Beam-10 search. This suggests that while length normalization biases search towards translations of roughly the correct length, it does not fix the fundamental modelling problem. 7 Available in our SGNMT decoder (Stahlberg et al., 2017, 2018b) as simplelendfs strategy. 8 We add 1 to the lengths to avoid division by zero errors. Related Work Other researchers have also noted that large beam sizes yield shorter translations (Koehn and Knowles, 2017). Sountsov and Sarawagi (2016) argue that this model error is due to the locally normalized maximum likelihood training objective in NMT that underestimates the margin between the correct translation and shorter ones if trained with regularization and finite data. A similar argument was made by Murray and Chiang (2018) who pointed out the difficulty for a locally normalized model to estimate the “budget” for all remaining (longer) translations. Kumar and Sarawagi (2019) demonstrated that NMT models are often poorly calibrated, and that that can cause the length deficiency. Ott et al. (2018) ar"
D19-1331,W18-6322,0,0.362594,"othesis is always lower than the score of any of its translation prefixes. While this monotonicity condition is true for vanilla NMT (Eq. 3), it does not hold for methods like length normalization (Jean et al., 2015; Boulanger-Lewandowski et al., 2013; Wu et al., 2016) or word rewards (He et al., 2016): Length normalization gives an advantage to longer hypotheses by dividing the score by the sentence length, while a word reward directly violates monotonicity as it rewards each word with a positive value. In Sec. 4 we show how our exact search can be extended to handle arbitrary length models (Murray and Chiang, 2018; Huang et al., 2017; Yang et al., 2018) by introducing length dependent lower bounds γk and report initial findings on exact search under length normalization. However, despite being of practical use, methods like length normalization and word penalties are rather heuristic as they do not have any justification from a probabilistic perspective. They also do not generalize well as (without retuning) they often work only for a specific beam size. It would be much more desirable to fix the length bias in the NMT model itself. Search Greedy Beam-10 Exact Log-likelihood Alg. 2 specifies the DFS al"
D19-1331,W17-3202,0,0.0288412,"Missing"
D19-1331,P16-1162,0,0.236651,"Missing"
D19-1331,D16-1158,0,0.140048,"ength normalization does not suffer from the length deficiency anymore (last row in Tab. 4), but it is not able to match our best BLEU score under Beam-10 search. This suggests that while length normalization biases search towards translations of roughly the correct length, it does not fix the fundamental modelling problem. 7 Available in our SGNMT decoder (Stahlberg et al., 2017, 2018b) as simplelendfs strategy. 8 We add 1 to the lengths to avoid division by zero errors. Related Work Other researchers have also noted that large beam sizes yield shorter translations (Koehn and Knowles, 2017). Sountsov and Sarawagi (2016) argue that this model error is due to the locally normalized maximum likelihood training objective in NMT that underestimates the margin between the correct translation and shorter ones if trained with regularization and finite data. A similar argument was made by Murray and Chiang (2018) who pointed out the difficulty for a locally normalized model to estimate the “budget” for all remaining (longer) translations. Kumar and Sarawagi (2019) demonstrated that NMT models are often poorly calibrated, and that that can cause the length deficiency. Ott et al. (2018) argued that uncertainty caused b"
D19-1331,W18-6427,1,0.886695,"Missing"
D19-1331,D17-2005,1,0.850949,"lation length k in a certain range (e.g. zero to 1.2 times the source sentence length). The initial lower bounds are derived from the Beam-10 hypothesis ybeam as follows:8 γk = (k + 1) log P (ybeam |x) . |ybeam |+ 1 (4) Exact search under length normalization does not suffer from the length deficiency anymore (last row in Tab. 4), but it is not able to match our best BLEU score under Beam-10 search. This suggests that while length normalization biases search towards translations of roughly the correct length, it does not fix the fundamental modelling problem. 7 Available in our SGNMT decoder (Stahlberg et al., 2017, 2018b) as simplelendfs strategy. 8 We add 1 to the lengths to avoid division by zero errors. Related Work Other researchers have also noted that large beam sizes yield shorter translations (Koehn and Knowles, 2017). Sountsov and Sarawagi (2016) argue that this model error is due to the locally normalized maximum likelihood training objective in NMT that underestimates the margin between the correct translation and shorter ones if trained with regularization and finite data. A similar argument was made by Murray and Chiang (2018) who pointed out the difficulty for a locally normalized model t"
D19-1331,W18-1821,1,0.924795,"1j−1 , x). (1) j=1 ˆ∈ The task of finding the most likely translation y ∗ T for a given source sentence x is known as the * Now at Google. (2) y∈T ∗ The NMT search space is vast as it grows exponentially with the sequence length. For example, for a common vocabulary size of |T |= 32, 000, there are already more possible translations with 20 words or less than atoms in the observable universe (32, 00020  1082 ). Thus, complete enumeration of the search space is impossible. The size of the NMT search space is perhaps the main reason why – besides some preliminary studies (Niehues et al., 2017; Stahlberg et al., 2018b; Ott et al., 2018) – analyzing search errors in NMT has received only limited attention. To the best of our knowledge, none of the previous studies were able to quantify the number of search errors in unconstrained NMT due to the lack of an exact inference scheme that – although too slow for practical MT – guarantees to find the global best model score for analysis purposes. In this work we propose such an exact decoding algorithm for NMT that exploits the monotonicity of NMT scores: Since the conditional log-probabilities in Eq. 1 are always negative, partial hypotheses can be safely discar"
D19-1331,W18-1819,0,0.0529152,"Missing"
D19-1331,1983.tc-1.13,0,0.720501,"Missing"
D19-1331,D18-1342,0,0.573507,"Missing"
D19-1459,D18-1547,0,0.414529,"ve they were interacting with an automated system while it was in fact a human, allowing them to express their turns in natural ways but in the context of an automated interface. We refer to this spoken dialog type as “two-person dialogs”. For the written dialogs, we engaged crowdsourced workers to write the full conversation themselves based on scenarios outlined for each task, thereby playing roles of both the user and assistant. We refer to this written dialog type as “self-dialogs”. In a departure from traditional annotation techniques (Henderson et al., 2013; Rojas-Barahona et al., 2017; Budzianowski et al., 2018), dialogs are labeled with simple API calls and arguments. This technique is much easier for annotators to learn and simpler to apply. As such it is more cost effective and, in addition, the same model can be used for multiple service providers. Taskmaster-1 has richer and more diverse language than the current popular benchmark in taskoriented dialog, MultiWOZ (Budzianowski et al., 2018). Table 1 shows that Taskmaster-1 has more unique words and is more difficult for language models to fit. We also find that Taskmaster-1 is Statistic # unique words # unique named entities # utterances # dialo"
D19-1459,W15-4640,0,0.0212591,"stics system to hold a coherent task-based conversation with a human remains one of computer science’s most complex and intriguing unsolved problems (Weizenbaum, 1966). In contrast to more traditional NLP efforts, interest in statistical approaches to dialog understanding and generation aided by machine learning has grown considerably in the last couple of years (Rojas-Barahona et al., 2017; Bordes et al., 2017; Henderson et al., 2013). However, the dearth of high quality, goal-oriented dialog data is considered a major hindrance to more significant progress in this area (Bordes et al., 2017; Lowe et al., 2015). To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations. For the spoken dialogs, we created a Wizard of Oz (WOz) system (Kelley, 1984) to collect two-person, spoken conversations. Crowdsourced workers playing the “user” interacted with human operators playing the"
D19-1459,D18-1255,0,0.0223468,"tickets for the 7:30? One moment. Okay so that’s 2 tickets for 7:30 at the AMC Neshaminy 24? Yes. It’ll be twenty-four ninety-nine for your tickets. That sounds great. I’ve confirmed your tickets, they’ll arrive via text shortly. Did you need any other information? No, that was it. Thank you so much for your help. Great, no problem. I hope you have fun. I hope so, too. Thank you so much. Figure 1: Sample Taskmaster-1 two-person dialog remaining 7,708 conversations in Taskmaster-1 are self-dialogs, in which crowdsourced workers write the entire conversation themselves. As Krause et al. (2017); Moghe et al. (2018) show, self dialogs are surprisingly rich in content. 3 3.1 The Taskmaster Corpus Overview There are several key attributes that make Taskmaster-1 both unique and effective for datadriven approaches to building dialog systems and for other research. Spoken and written dialogs: While the spoken sources more closely reflect conversational language (Chafe and Tannen, 1987), written dialogs are significantly cheaper and easier to gather. This allows for a significant increase in the size of the corpus and in speaker diversity. Goal-oriented dialogs: All dialogs are based on one of six tasks: order"
D19-1459,N19-4009,0,0.0219544,"t−1 . Each utterance Ui itself is comprised of a sequence of words wi1 , wi2 . . . wik . The overall conditional probability is factorized autoregressively as Pθ (Ut |U1:t−1 ) = n Y Pθ (wti |wt1:i−1 , U1:t−1 ) i=1 Pθ , in this work, is parameterized by a recurrent, convolution or Transformer-based seq2seq model. n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model. Convolution: We use the fconv architecture (Gehring et al., 2017) and default hyperparameters from the fairseq (Ott et al., 2019) framework.2 We train the network with ADAM optimizer (Kingma and Ba, 2015) with learning rate of 0.25 and dropout probability set to 0.2. LSTM: We consider LSTM models (Hochreiter and Schmidhuber, 1997) with and without attention (Bahdanau et al., 2015) and use the tensor2tensor (Vaswani et al., 2018) framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors. Transformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. 4522 2 https://github.com/pytorch/fairseq Baseline Models PPL BLEU"
D19-1459,P16-1154,0,0.0455714,"Missing"
D19-1459,W13-4073,0,0.226819,"Processing and the 9th International Joint Conference on Natural Language Processing, pages 4516–4525, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics system to hold a coherent task-based conversation with a human remains one of computer science’s most complex and intriguing unsolved problems (Weizenbaum, 1966). In contrast to more traditional NLP efforts, interest in statistical approaches to dialog understanding and generation aided by machine learning has grown considerably in the last couple of years (Rojas-Barahona et al., 2017; Bordes et al., 2017; Henderson et al., 2013). However, the dearth of high quality, goal-oriented dialog data is considered a major hindrance to more significant progress in this area (Bordes et al., 2017; Lowe et al., 2015). To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations. For the spoken dialogs, we c"
D19-1459,E17-1042,0,0.350255,"nference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4516–4525, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics system to hold a coherent task-based conversation with a human remains one of computer science’s most complex and intriguing unsolved problems (Weizenbaum, 1966). In contrast to more traditional NLP efforts, interest in statistical approaches to dialog understanding and generation aided by machine learning has grown considerably in the last couple of years (Rojas-Barahona et al., 2017; Bordes et al., 2017; Henderson et al., 2013). However, the dearth of high quality, goal-oriented dialog data is considered a major hindrance to more significant progress in this area (Bordes et al., 2017; Lowe et al., 2015). To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaur"
D19-1459,W18-1819,0,0.0255079,"seq2seq model. n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model. Convolution: We use the fconv architecture (Gehring et al., 2017) and default hyperparameters from the fairseq (Ott et al., 2019) framework.2 We train the network with ADAM optimizer (Kingma and Ba, 2015) with learning rate of 0.25 and dropout probability set to 0.2. LSTM: We consider LSTM models (Hochreiter and Schmidhuber, 1997) with and without attention (Bahdanau et al., 2015) and use the tensor2tensor (Vaswani et al., 2018) framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors. Transformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. 4522 2 https://github.com/pytorch/fairseq Baseline Models PPL BLEU Ratings (LIKERT) Rank GPT-2 (117M) - 0.26 - - 3-gram 4-gram LSTM Convolution LSTM-attention Transformer 38.12 34.49 25.73 21.25 20.05 18.19 0.20 0.21 4.45 5.09 5.12 6.11 2.89 3.51 3.22 3 2 1 Evalation method Inter-Annotator Reliability (Krippendorfs Alpha) Rating (1-5 LIKERT) Ranking 0.21 0.29 Table 4"
E14-1026,E09-1011,0,0.0143201,"ple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word align"
E14-1026,C10-1071,0,0.0144243,"ated work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source t"
E14-1026,D13-1049,0,0.65874,"ply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for Fr"
E14-1026,P05-1066,0,0.434988,"exical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neit"
E14-1026,P07-1091,0,0.146309,"y-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with mul"
E14-1026,W06-1609,0,0.47243,"Missing"
E14-1026,2006.amta-papers.4,0,0.0474153,"Missing"
E14-1026,D12-1077,0,0.719881,"h is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that automatically learn preor"
E14-1026,D11-1018,0,0.709824,"alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that aut"
E14-1026,E99-1010,0,0.0472819,"ng two nodes a and b in the given order is dst gap The dependency labels of each node The part-of-speech tags of each node. The head words and classes of each node. The left-most and right-most words and classes of a node. The distances between each node and the head. If there is a gap between nodes, the left-most and right-most words and classes in the gap. In order to keep the size of our feature space manageable, we only consider features which occur at least 5 times1 . For the lexical features, we use the top 100 vocabulary items from our training data, and 51 clusters generated by mkcls (Och, 1999). Similarly to previous work (Genzel, 2010; Yang et al., 2012), we also explore feature conjunctions. For the tag and label classes, we generate all possible combinations up to a given size. For the lexical and distance features, we explicitly specify conjunctions with the tag and label features. Results for various feature configurations are discussed in Section 4.3.1. cs(a, b) := |{(i, j) ∈ Aa × Ab : i > j}| where Aa and Ab are the target-side positions to which the words spanned by a and b are aligned. The label is then given as y(a, b) = ... Figure 2: Branch-and-bound search: Partial searc"
E14-1026,D08-1089,0,0.0724942,"ic. Source sentences were extracted from the web and one target reference was produced by a bilingual speaker. These sentences were chosen to evenly represent 10 domains, including world news, chat/SMS, health, sport, science, business, and others. The dev/test sets contain 602/903 sentences and 14K/20K words each. We do English part-of-speech tagging using SVMTool (Gim´enez and M`arquez, 2004) and dependency parsing using MaltParser (Nivre et al., 2007). For translation experiments, we use a phrasebased decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008) with weights tuned using MERT to optimize the character-based BLEU score on the dev set. The Japanese and Korean language models are 5-grams estimated on > 350M words of generic web text. For training the logistic regression model, we automatically align the parallel training data and intersect the source-to-target and target-to-source alignments. We reserve a random 5K-sentence j∈V |i>j Y Experiments 1 − p(i, j) j∈V |i&lt;j where V = {1, ..., k}(π 0 · hii) is the set of source child positions that have not yet been visited. Observe that the nodes at search depth k correspond exactly to the set"
E14-1026,C10-1043,0,0.162365,"ter and better translations, if preordering is done well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang"
E14-1026,P09-1090,0,0.0142626,"rforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cas"
E14-1026,gimenez-marquez-2004-svmtool,0,0.0424559,"Missing"
E14-1026,2007.tmi-papers.21,0,0.0930351,"alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexi"
E14-1026,2007.mtsummit-papers.29,0,0.121175,"s word movement needs to be considered, which results in faster and better translations, if preordering is done well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat"
E14-1026,D09-1105,0,0.25904,"mon criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering"
E14-1026,P09-2059,0,0.0206331,"ch and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, whe"
E14-1026,C10-1126,0,0.0572432,"t needs to be considered, which results in faster and better translations, if preordering is done well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a"
E14-1026,W10-1736,0,0.175411,"ul way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered versio"
E14-1026,P13-1125,0,0.0145624,"system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orthogonal to the gains obtained when incorporating a lexicalised reordering model (LRM). In fact, preordering gains are slightly larger with LRM, suggesting that this reordering model can be better estimated with preordered text. This echoes the notion that reordering models are particularly sensitive to alignment noise (DeNero and Uszkoreit, 2011; Neubig et al., 2012; Visweswariah et al., 2013), and that a ‘more monotonic’ training corpus leads to better translation models. Finally, ‘df-bnb’ outperforms all other preordering approaches, and achieves an extra 0.5–0.8 BLEU over the rule-based one even at zero distortion limit. This is consistent with the substantial crossing score reductions reported in Section 4.3. We argue that these improvements are due to the usage of lexical features to facilitate finergrained ordering decisions, and to our better search through the children permutation space which is not restricted by sliding windows, does Translation performance Table 3 reports"
E14-1026,D07-1077,0,0.0736374,"o adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntac"
E14-1026,I11-1004,0,0.0125726,"one well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classificatio"
E14-1026,C04-1073,0,0.324679,"pealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, ther"
E14-1026,N09-1028,0,0.691977,"d of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the rel"
E14-1026,P12-1096,0,0.523998,"2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse t"
E14-1026,D11-1045,0,\N,Missing
E14-1028,P04-1051,0,0.194587,"We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω"
E14-1028,C00-1007,0,0.200795,"Missing"
E14-1028,N12-1017,0,0.0209902,"Missing"
E14-1028,W11-2832,0,0.0715511,"y in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be us"
E14-1028,D10-1055,0,0.211617,"contain at most 12 words. By contrast, Pruned Expansion (Algorithm 2) with β = 10 is feasible for bags of up to 18 words. For 3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al., 2012). By means of extracted CCG rules, the CCG system searches for an optimal parse guided by large-margin training. Each partial hypothesis (or ‘edge’) is scored using the syntax model and a 4gram LM trained similarly on one billion words of English Gigaword data. Both systems are evaluated using BLEU (Papineni et al., 2002; Espinosa et al., 2010). For GYRO, we use the pruned parsing algorithm of Section 2.4 with r = 6 and β = 10 and a memory usage limit of 20G. The phrasebased rules of the grammar contain only 3-grams, 263 LM 4g 3g 4g System CCG GYRO GYRO +3g GYRO +4g 100-best oracle lattice oracle MT08-nw 48.0 59.0 63.0 65.5 76.1 80.4 MT09-nw 48.8 58.4 64.1 65.9 76.1 80.2 provement from rescoring, in that even for small 100-best lists the improvement found by the Oracle can exceed 10 BLEU points; and (b) that the output lattices are not perfect in that the Oracle score is not 100. 3.2.1 Table 1: CCG and GYRO BLEU scores. Rescoring GY"
E14-1028,C10-1043,0,0.0300121,"ge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Conference of the Eu"
E14-1028,W12-1525,0,0.0160275,"ng for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems"
E14-1028,D10-1051,0,0.0609237,"Missing"
E14-1028,P10-2006,1,0.898233,"Missing"
E14-1028,C10-1009,1,0.840911,"Missing"
E14-1028,D11-1127,1,0.897552,"Missing"
E14-1028,W11-2835,0,0.0501234,"before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these prob"
E14-1028,J99-4005,0,0.240416,"ures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabet"
E14-1028,P98-1116,0,0.464961,"Missing"
E14-1028,D07-1090,0,0.0922874,"Missing"
E14-1028,J07-2003,0,0.639668,"rder Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consisting of words from the bag. Strings are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram language model (LM). The motivation is that it is easier to make fluent sentences from phrases (snippets of fluent text) than from words in isolation. GYRO builds on approaches developed for syntactic SMT (Chiang, 2007; de Gispert et al., 2010; Iglesias et al., 2011). The system generates strings in the form of weighted automata which can be rescored using higher-order n-gram LMs, dependency LMs (Shen et al., 2010), and Minimum Bayes Risk decoding, either using posterior probabilities obtained from GYRO or SMT systems. We report extensive experiments using BLEU and conclude with human assessments. We show that despite its relatively simple formulation, GYRO gives BLEU scores over 20 points higher than the best previously reported results, generated by a syntax-based ordering system. Human fluency assessment"
E14-1028,N09-2019,1,0.875164,"Missing"
E14-1028,P02-1038,0,0.0746264,"as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems, as described in this paper, which do not require rich input representations. We plan to investigate these applications in future work. There is much opportunity for future development. To improve coverage, the grammars of Section 2.1 could perform generation with overlapping, rather than concatenated, n-grams; and features could be included to define tuneable loglinear rule probabilities (Och and Ney, 2002; Chiang, 2007). The GYRO grammar could be extended using techniques from string-to-tree SMT, in particular by modifying the grammar so that output derivations respect dependencies (Shen et al., 2010); this will make it easier to integrate dependency LMs into GYRO. Finally, it would be interesting to couple the GYRO architecture with automata-based models of poetry and rhythmic text (Greene et al., 2010). Acknowledgement The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement number 247762, the FAUST"
E14-1028,J10-3008,1,0.902259,"Missing"
E14-1028,P02-1040,0,0.0911806,"feasible for bags that contain at most 12 words. By contrast, Pruned Expansion (Algorithm 2) with β = 10 is feasible for bags of up to 18 words. For 3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al., 2012). By means of extracted CCG rules, the CCG system searches for an optimal parse guided by large-margin training. Each partial hypothesis (or ‘edge’) is scored using the syntax model and a 4gram LM trained similarly on one billion words of English Gigaword data. Both systems are evaluated using BLEU (Papineni et al., 2002; Espinosa et al., 2010). For GYRO, we use the pruned parsing algorithm of Section 2.4 with r = 6 and β = 10 and a memory usage limit of 20G. The phrasebased rules of the grammar contain only 3-grams, 263 LM 4g 3g 4g System CCG GYRO GYRO +3g GYRO +4g 100-best oracle lattice oracle MT08-nw 48.0 59.0 63.0 65.5 76.1 80.4 MT09-nw 48.8 58.4 64.1 65.9 76.1 80.2 provement from rescoring, in that even for small 100-best lists the improvement found by the Oracle can exceed 10 BLEU points; and (b) that the output lattices are not perfect in that the Oracle score is not 100. 3.2.1 Table 1: CCG and GYRO B"
E14-1028,N10-1141,0,0.0161358,"0 40 65 30 60 20 55 10 50 0 1-5 6-10 11-15 16-20 21-25 26+ all sizes Size of the bag of words terior probabilities extracted from (a) the same generation lattice, and (b) from lattices produced by an Arabic-to-English hierarchical-phrase based MT system developed for the NIST 2012 OpenMT Evaluation. As noted, LMBR relies on a posterior distribution over n-grams as part of its computation or risk. Here, we use LMBR with a posterior of the form α pGYRO + (1–α) pMT . This is effectively performing a system combination between the GYRO generation system and the MT system (de Gispert et al., 2009; DeNero et al., 2010) but restricting the hypothesis space to be that of the GYRO lattice (Blackwood et al., 2010b). Results are reported in the last two rows of Table 2. Relative to 5-gram LM rescoring alone, we see gains in BLEU of 2.3 and 4.4 in MT08-nw and MT09nw, suggesting that posterior distributions over ngrams provided by SMT systems can give good guidance in generation. These results also suggest that if we knew what words to use, we could generate very good quality translation output. 3.3 90 85 Sentence Precision Rate (SPR) MT08-nw 68.5 68.7 ? 68.7 68.6 70.8 ? 70.8 BLEU score 4g GYRO rescoring: +5g +5g"
E14-1028,A00-2026,0,0.187782,"Missing"
E14-1028,J10-4005,0,0.201998,"s are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram language model (LM). The motivation is that it is easier to make fluent sentences from phrases (snippets of fluent text) than from words in isolation. GYRO builds on approaches developed for syntactic SMT (Chiang, 2007; de Gispert et al., 2010; Iglesias et al., 2011). The system generates strings in the form of weighted automata which can be rescored using higher-order n-gram LMs, dependency LMs (Shen et al., 2010), and Minimum Bayes Risk decoding, either using posterior probabilities obtained from GYRO or SMT systems. We report extensive experiments using BLEU and conclude with human assessments. We show that despite its relatively simple formulation, GYRO gives BLEU scores over 20 points higher than the best previously reported results, generated by a syntax-based ordering system. Human fluency assessments confirm these substantial improvements. We describe an approach to word ordering using modelling techniques from statistical machine translation. The system incorporates a phrase-based model of stri"
E14-1028,E12-1013,0,0.0389721,"Missing"
E14-1028,P05-1009,0,0.420309,"R EPLACE(FN,1 ) 5 return L ← F ◦ G 6 function P RUNE S -ROW(x) : 7 F ← y Fx,y 8 F ← FSA-R EPLACE(F ) 9 F ←F ◦G 10 F ← FSA-P RUNE(F, β)  11 for each cell y = 1, . . . , N x 12 Fx,y ← Fx,y · F 13 return Figure 4: Pseudocode for Algorithm 1 (excluding lines 2-3) and Algorithm 2 (including all lines). the LM during parsing. The pseudocode is identical to that of Algorithm 1 except for the following changes: in parsing (Figure 2) we pass G as input and we call the row pruning function of Figure 4 after line 5 if x ≥ r. We note that there is a strong connection between GYRO and the IDL approach of Soricut and Marcu (2005; 2006). Our bag of words parser could be cast in the IDL-formalism, and the FSA ‘Replace’ operation would be expressed by an IDL ‘Unfold’ operation. However, whereas their work applies pruning in the creation of the IDLexpression prior to LM application, GYRO uses unweighted phrase constraints so the LM must be considered for pruning while parsing. Algorithm 3: Pruned Parsing and Generation 3 The two generation algorithms presented above rely on a completed initial parsing step. However, given that the complexity of the parsing stage is O(2N · K), this may not be achievable in practice. Leavi"
E14-1028,P06-1139,0,0.097572,"f greater than 85. Performance is 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 REF (a) (b) (c) (d) REF (a-c) (d) REF (a) (b) (c) (d) Hypothesis a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . critics of bush ’s policy in iraq joins the list of a third republican senator . critics of bush ’s iraq policy in a list of republican senator joins the third . the list of critics of bush ’s policy in iraq a third republican senator joins . it added that these messages were sent to president bashar al-asad through tu"
E14-1028,D09-1105,0,0.0241349,"riven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Confe"
E14-1028,D08-1065,0,0.0748302,"Missing"
E14-1028,E09-1097,0,0.334788,"age models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259–268, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Compu"
E14-1028,2007.mtsummit-ucnlg.4,0,0.0470482,"Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 REF (a) (b) (c) (d) REF (a-c) (d) REF (a) (b) (c) (d) Hypothesis a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . critics of bush ’s policy in iraq joins the list of a third republican senator . critics of bush ’s iraq policy in a list of republican senator joins the third . the list of critics of bush ’s policy in iraq a third republican senator joins . it added that these messages were sent to president bashar al-asad through turkish and german officials . it added that pres"
E14-1028,N07-1022,0,0.0571414,"enator has joined the ranks of critics of george bush ’s policy in iraq , just days before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but whic"
E14-1028,D11-1106,0,0.247024,"1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259–268, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics are extracted from large text collections, and contain onl"
E14-1028,C08-1038,0,\N,Missing
E14-1028,E12-1075,0,\N,Missing
E14-1028,C98-1112,0,\N,Missing
E14-1028,2010.iwslt-keynotes.2,0,\N,Missing
E14-1028,N04-1022,0,\N,Missing
E17-2058,P13-2121,0,0.0327812,"Missing"
E17-2058,D16-1162,0,0.143651,"Missing"
E17-2058,N04-1022,0,0.0944175,"t avoids using NMT scores for risk calculation. We show how to reformulate the original LMBR decision rule for using it in a word-based NMT decoder which is not restricted to an n-best list or a lattice. Our hybrid system outperforms lattice rescoring on multiple data sets for EnglishGerman and Japanese-English. We report similar gains from applying our method to subword-unitbased NMT rather than word-based NMT. Introduction Lattice minimum Bayes-risk (LMBR) decoding has been applied successfully to translation lattices in traditional SMT to improve translation performance of a single system (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). However, minimum Bayes-risk (MBR) decoding is also a very powerful framework for combining diverse systems (Sim et al., 2007; de Gispert et al., 2009). Therefore, we study combining traditional SMT and NMT in a hybrid decoding scheme based on MBR. We argue that MBR-based methods in their present form are not well-suited for NMT because of the following reasons: 2 Combining NMT and SMT by Minimising the Lattice Bayes-risk We propose to collect statistics for MBR from a potentially large translation lattice generated with SMT, and use the n-gram p"
E17-2058,P10-2006,1,0.928326,"Missing"
E17-2058,D15-1249,0,0.0130546,"the Blocks and Theano frameworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-bas"
E17-2058,P16-1100,0,0.0118496,"ien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice rescoring and MBR. First, we construct a fin"
E17-2058,P16-1160,0,0.0129826,"r et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice rescoring and MBR."
E17-2058,P15-1002,0,0.0493514,"Missing"
E17-2058,P16-2058,0,0.0228423,"in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice rescoring and MBR. First, we construct a finite state 2 3 364 Comparable to ht"
E17-2058,D08-1076,0,0.139487,"Missing"
E17-2058,N09-2019,1,0.830192,"Missing"
E17-2058,J10-3008,1,0.885453,"Missing"
E17-2058,P14-2024,0,0.0203616,"Missing"
E17-2058,W15-5003,0,0.252697,"Missing"
E17-2058,P13-4016,0,0.0726014,"Missing"
E17-2058,N15-3009,0,0.0363475,"Missing"
E17-2058,P16-1162,0,0.0971626,"eworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice"
E17-2058,W16-2324,1,0.873055,"hese n-grams u, we smooth P (u|Ye ) by mixing it with the uniform distribution to flatten the distribution and increase the offset to n-grams which are not in the lattice. 5 6 Experimental Setup We test our approach on English-German (En-De) and Japanese-English (Ja-En). For En-De, we use the WMT news-test2014 (the filtered version) as a development set, and keep news-test2015 and news-test2016 as test sets. For Ja-En, we use the ASPEC corpus (Nakazawa et al., 2016) to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). The NMT systems are as described by Stahlberg et al. (2016b) using the Blocks and Theano frameworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-ba"
E17-2058,P16-2049,1,0.867259,"hese n-grams u, we smooth P (u|Ye ) by mixing it with the uniform distribution to flatten the distribution and increase the offset to n-grams which are not in the lattice. 5 6 Experimental Setup We test our approach on English-German (En-De) and Japanese-English (Ja-En). For En-De, we use the WMT news-test2014 (the filtered version) as a development set, and keep news-test2015 and news-test2016 as test sets. For Ja-En, we use the ASPEC corpus (Nakazawa et al., 2016) to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). The NMT systems are as described by Stahlberg et al. (2016b) using the Blocks and Theano frameworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-ba"
E17-2058,D08-1065,0,0.158605,"es for risk calculation. We show how to reformulate the original LMBR decision rule for using it in a word-based NMT decoder which is not restricted to an n-best list or a lattice. Our hybrid system outperforms lattice rescoring on multiple data sets for EnglishGerman and Japanese-English. We report similar gains from applying our method to subword-unitbased NMT rather than word-based NMT. Introduction Lattice minimum Bayes-risk (LMBR) decoding has been applied successfully to translation lattices in traditional SMT to improve translation performance of a single system (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). However, minimum Bayes-risk (MBR) decoding is also a very powerful framework for combining diverse systems (Sim et al., 2007; de Gispert et al., 2009). Therefore, we study combining traditional SMT and NMT in a hybrid decoding scheme based on MBR. We argue that MBR-based methods in their present form are not well-suited for NMT because of the following reasons: 2 Combining NMT and SMT by Minimising the Lattice Bayes-risk We propose to collect statistics for MBR from a potentially large translation lattice generated with SMT, and use the n-gram posteriors as additiona"
E17-2058,D13-1140,0,0.0608796,"g NMT with traditional SMT by biasing NMT scores towards translations with low Bayes-risk with respect to the SMT lattice. We reported significant improvements of the new method over lattice rescoring on Japanese-English and EnglishGerman and showed that it can make good use even of very small lattices and n-best lists. In this work, we calculated the Bayes-risk over non-neural SMT lattices. In the future, we are planning to introduce neural models to the risk estimation while keeping the computational complexity under control, e.g. by using neural n-gram language models (Bengio et al., 2003; Vaswani et al., 2013) or approximations of NMT scores (Lecorv´e and Motlicek, 2012; Liu et al., 2016) for n-gram posterior calculation. Related Work Combining the advantages of NMT and traditional SMT has received some attention in current research. A recent line of research attempts to integrate SMT-style translation tables into the NMT system (Zhang and Zong, 2016; Arthur et al., 2016; He et al., 2016). Wang et al. (2016) interpolated NMT posteriors with word recommendations from SMT and jointly trained NMT together with a gating function which assigns the weight between SMT and NMT scores dynamically. Neu7 Conc"
E17-2058,P16-5005,0,0.0171812,"Missing"
J14-3008,P10-2006,1,0.883775,"Missing"
J14-3008,D07-1090,0,0.0385823,"m, bigram, or unigram model. For both the Katz and the Kneser-Ney 4-gram language models: at θ = 7.5E − 05 the number of 4-grams in the LM is effectively reduced to zero; at θ = 7.5E − 4 the number of 3-grams is effectively 0; and at θ = 7.5E − 3, only unigrams remain. Development set perplexities increase as entropy pruning becomes more aggressive, with the Katz smoothed model performing better under pruning (Chelba et al. 2010; Roark, Allauzen, and Riley 2013). We will also use a larger language model, denoted M2 , obtained by interpolating M1 with a zero-cutoff stupid-backoff 5-gram model (Brants et al. 2007) estimated over 6.6B words of English newswire text; M2 is estimated as needed for the n-grams required for the test sets. 8 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl. 708 Allauzen et al. Pushdown Automata in Statistical Machine Translation Table 3 Success in finding the 1-best translation under G with various M1θ under a memory size limit of 10GB as measured over tune-nw (1,755 sentences). We note which operations in translation exceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection and Shortest Path operation for HiPDT. Decoding with G + M1θ u"
J14-3008,D11-1003,0,0.0199507,"via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after noting that with t"
J14-3008,J07-2003,0,0.451126,"k, NY 10011. E-mail: {allauzen,riley}@google.com. ∗∗ University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K. and SDL Research, Cambridge U.K. E-mail: {wjb31,ad465,gi212}@eng.cam.ac.uk. Submission received: 6 August 2012; revised version received: 20 February 2013; accepted for publication: 2 December 2013. doi:10.1162/COLI a 00197 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 1. Introduction Synchronous context-free grammars (SCFGs) are now widely used in statistical machine translation, with Hiero as the preeminent example (Chiang 2007). Given an SCFG and an n-gram language model, the challenge is to decode with them, that is, to apply them to source text to generate a target translation. Decoding is complex in practice, but it can be described simply and exactly in terms of the formal languages and relations involved. We will use this description to introduce and analyze pushdown automata (PDAs) for machine translation. This formal description will allow close comparison of PDAs to existing decoders which are based on other forms of automata. Decoding can be described in terms of the following steps: 1. Translation: T = Π2"
J14-3008,J10-3008,1,0.943912,"Missing"
J14-3008,N10-1033,0,0.175195,"utomata that accepts ‘t1 t2 t3 t6 t7 ’, Step 2 will yield all derivations that yield this string 691 Computational Linguistics 0 1 X 2 t2 t1 Volume 40, Number 3 4 t4 t3 t6 X 3 6 t7 7 t2 0 5 t3 1 S 2 X (a) Optimized RTN t1 0 1 t2 2 t2 t3 4 6 t4 t3 8 t6 t2 3 t3 5 t7 9 7 (b) Optimized FSA 2 t2 0 t1 1 t3 [ 4 ) ( 8 t2 9 t3 10 t4 t6 ] 6 t7 7 5 3 (c) Optimized PDA Figure 3 Optimized representations of the regular language of possible translation candidates. as a translation of the source string. This is the approach taken in Iglesias et al. (2009a) and de Gispert et al. (2010) for the RTN/FSA and in Dyer (2010b) for hypergraphs. In Section 4 we analyze how PDAs can be used for alignment. 1.2 Goals We summarize here the aims of this article. We will show how PDAs can be used as compact representations of the space T of candidate translations generated by a hierarchical phrase-based SCFG when applied to an input sentence s and intersected with a language model M. We have described the architecture of HiPDT, a hierarchical phrase-based decoder based on PDAs, and have identified the general-purpose algorithms needed [ 2 t2 0 t1 1 ( [ 3 t3 ] 5 X→hs1 , t3 t4 i X→hs3 , t5 t6 i S→hX1 s2 X2 , t1 X1 X2 i S→h"
J14-3008,N04-1035,0,0.065967,"ially larger than the RTN/PDT optimized as described. Although our interest is primarily in Hiero-style translation grammars, which have rank 2 and a relatively small number of nonterminals, this complexity analysis can be extended to other grammars. For SCFGs of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this art"
J14-3008,D10-1063,0,0.0163841,"complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this article (except when stated otherwise). In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars. 4.1 Translation Grammars and Language Models Translation grammars are extracted from a subset of the"
J14-3008,C08-5001,0,0.014298,"lations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. W"
J14-3008,P07-1019,0,0.0286452,"h a compromise between decoding speed and final performance of our HiPDT system. For instance, with θ = 7.5 × 10−7 and β = 12, for which we decode at a rate of 3 words/sec as seen in Figure 16, we are losing only 0.5 BLEU after LMBR compared to θ = 7.5 × 10−7 and β = 15. 6. Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits the left-to-right nature of n-gram language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that"
J14-3008,D10-1027,0,0.0185477,"For instance, with θ = 7.5 × 10−7 and β = 12, for which we decode at a rate of 3 words/sec as seen in Figure 16, we are losing only 0.5 BLEU after LMBR compared to θ = 7.5 × 10−7 and β = 15. 6. Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits the left-to-right nature of n-gram language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an"
J14-3008,W05-1507,0,0.0768257,"Missing"
J14-3008,N09-1049,1,0.939348,"Missing"
J14-3008,E09-1044,1,0.880685,"Missing"
J14-3008,D10-1125,0,0.0345495,"ias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a"
J14-3008,N04-1022,0,0.0588184,"Does the HiPDT two-pass decoding generate lattices that can be useful in rescoring? We now report on rescoring experiments using WFSAs produced by the two-pass HiPDT translation system under the large translation grammar G. We demonstrate that HiPDT can be used to generate large, compact representations of the translation space that are suitable for rescoring with large language models or by alternative decoding procedures. We investigate translation performance by applying versions of the language model M2 estimated with stupid backoff. We also investigate minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited for the large WFSAs that the system can generate; we use the implementation described by Blackwood, de Gispert, & Byrne (2010). There are two parameters to be tuned: a scaling parameter to normalize the evidence scores and a word penalty applied to the hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in Figure 18. We note first that rescoring with the large language model M2 , which is effectively interpolated with M1 , gives con"
J14-3008,H05-1021,0,0.0389465,"ork involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after noting that with the space of translations represented as WFSAs, alignment can be performed using operations over WFSTs (Kumar and Byrne 2005). Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al. 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widely used in automatic speech recognition (Ljolje, Pereira, and Riley 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs. 7. Conclusion In this article, we have described a novel approach to hierarchical machine translation using pushdown automata. We have pre"
J14-3008,W03-3016,0,0.0431321,"whose size is always linear in the size of R. In this article, we assume this optimization is always performed. We note here that RTNs can be defined and the replacement operation can be applied in any semiring. 3.3 Composition Once we have created the PDA with translation scores, Step 2 in Section 1.1 applies the language model scores to the translation space. This is done by composition with an FSA containing the relevant language model weights. The class of weighted pushdown transducers is closed under composition with weighted finite-state transducers (Bar-Hillel, Perles, and Shamir 1964; Nederhof and Satta 2003). OpenFST supports composition between automata T1 and T2 , where T1 is a weighted pushdown transducer and T2 is a weighted finite-state transducer. If both T1 and T2 are acceptors, rather than transducers, the composition of a PDA and an FSA produces a PDA containing their intersection, and so no separate intersection algorithm is required for these automata. Given this, we describe only the simpler, special case of intersection between a PDA and an FSA, as this is sufficient for most of the translation applications described in this article. The alignment experiments of RTN R PDT T X1 a 2 1"
J14-3008,P03-1021,0,0.00584984,"une-nw reference translations are also reported. The Kneser-Ney and Katz 4-gram LM have 416,190 unigrams, which are not removed by pruning. θ 0 7.5 × 10−9 7.5 × 10−8 7.5 × 10−7 7.5 × 10−6 7.5 × 10−5 7.5 × 10−4 7.5 × 10−3 KN 2-grams 3-grams 4-grams perplexity 28M 61M 117M 98.1 10M 6M 3M 122.2 2.5M 969K 219K 171.5 442K 74K 5K 290.4 37K 2.7K 44 605.1 1.3K 38 0 1270.2 21 0 0 1883.6 0 0 0 2200.0 KATZ 2-grams 3-grams 4-grams perplexity 28M 64M 117M 106.7 7M 10M 4.6M 120.4 2M 1.5M 398K 146.9 391K 148K 19K 210.5 52K 8.4K 510 336.6 4K 197 1 596.5 117 1 0 905.0 1 0 0 1046.1 In tuning the systems, MERT (Och 2003) iterative parameter estimation under IBM BLEU8 is performed on the development set. The parallel corpus is aligned using MTTK (Deng and Byrne 2008) in both sourceto-target and target-to-source directions. We then follow published procedures (Chiang 2007; Iglesias et al. 2009b) to extract hierarchical phrases from the union of the directional word alignments. We call a translation grammar (G) the set of rules extracted from this process. For reference, the number of rules in G that can apply to the tune-nw is 1.1M, of which 593K are standard non-hierarchical phrases and 511K are strictly hiera"
J14-3008,P13-1005,1,0.88019,"Missing"
J14-3008,P11-2001,0,0.112438,"Missing"
J14-3008,P11-1008,0,0.015885,"at cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after noting that with the space of translations represented as WFSAs, alignment can be performed using operation"
J14-3008,H05-1101,0,0.0267061,"rable to ITG alignment (Wu 1997) and the intersection algorithm of Dyer (2010b). Our experimental results support the complexity analysis summarized in Table 1. HiPDT is more efficient in ITG alignment and this is consistent with its linear dependence on the grammar size, whereas HiFST suffers from its exponential dependence. This use of PDAs in alignment does not rely on properties specific either to Hiero or to ITGs. We expect that the approach should be applicable with other types of SCFGs, although we note that alignment under SCFGs with an arbitrary number of nonterminals can be NP-hard (Satta and Peserico 2005). 5. HiPDT Two-Pass Translation Architecture and Experiments The previous complexity analysis suggests that PDAs should excel when used with large translation grammars and relatively small n-gram language models. In hierarchical phrase-based translation, this is a somewhat unusual scenario: It is far more typical that translation tasks requiring a large translation grammar also require large language models. To accommodate these requirements we have developed a twopass decoding strategy in which a weak version of a large language model is applied prior to the expansion of the PDA, after which"
J14-3008,J95-2002,0,0.458154,"Missing"
J14-3008,D08-1065,0,0.0141347,"experiments using WFSAs produced by the two-pass HiPDT translation system under the large translation grammar G. We demonstrate that HiPDT can be used to generate large, compact representations of the translation space that are suitable for rescoring with large language models or by alternative decoding procedures. We investigate translation performance by applying versions of the language model M2 estimated with stupid backoff. We also investigate minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited for the large WFSAs that the system can generate; we use the implementation described by Blackwood, de Gispert, & Byrne (2010). There are two parameters to be tuned: a scaling parameter to normalize the evidence scores and a word penalty applied to the hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in Figure 18. We note first that rescoring with the large language model M2 , which is effectively interpolated with M1 , gives consistent gains over initial results obtained with M1 alone. After 5-gram rescoring there is already +0.5 BLEU i"
J14-3008,J97-3002,0,0.556067,"es successfully. The subsequent shortest path (or pruned expansion) operation is prone to failure, but the risk of this can be greatly reduced by using smaller language models. In the next section we contrast both HiPDT and HiFST for alignment. 4.3 Alignment with Inversion Transduction Grammars We continue to explore applications characterized by large translation grammars G and small language models M. As an extreme instance of a problem involving a large translation grammar and a simple target language model, we consider parallel text alignment under an Inversion Transduction Grammar (ITG) (Wu 1997). This task, or something like it, is often done in translation grammar induction. The process should yield the set of derivations, with scores, that generate the target sentence as a translation 9 We use the UNIX ulimit command. The experiment was carried out over machines with different configurations and loads, so these numbers should be considered as approximate values. 709 Computational Linguistics Volume 40, Number 3 of the source sentence. In alignment the target language model is extremely simple: It is simply an acceptor for the target language sentence so that |M |is linear in the le"
J14-3008,D09-1038,0,0.0192808,"of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this article (except when stated otherwise). In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars. 4.1 Translation Grammars and Language Models Translatio"
J14-3008,2008.iwslt-papers.8,0,0.027566,"g exact translations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. W"
J14-3008,N06-1033,0,0.0299036,"grammars. For SCFGs of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this article (except when stated otherwise). In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars. 4.1 Translation Grammars and Languag"
J14-3008,W06-3119,0,0.0418105,"or CFG representation can be exponentially larger than the RTN/PDT optimized as described. Although our interest is primarily in Hiero-style translation grammars, which have rank 2 and a relatively small number of nonterminals, this complexity analysis can be extended to other grammars. For SCFGs of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used through"
J14-3008,2010.iwslt-keynotes.2,0,\N,Missing
N15-1041,W08-0304,0,0.32659,"through the use of high dimensional, sparse features? We believe that the explanation is in feasibility. If the oracle index vector ˆi is feasible then all training methods will find very similar solutions. Our belief is that as the feature dimension increases, the chance of an oracle index vector being feasible also increases. 3 Convex Geometry We now build on the description of LP-MERT to give a geometric interpretation to training linear models. We first give a concise summary of the fundamentals of convex geometry as presented by (Ziegler, 1995) after which we work through the example in Cer et al. (2008) to provide an intuition behind these concepts. 3.1 Convex Geometry Fundamentals In this section we reference definitions from convex geometry (Ziegler, 1995) in a form that allows us to describe SMT model parameter optimisation. Vector Space The real valued vector space RD represents the space of all finite D-dimensional feature vectors. Dual Vector Space The dual vector space (RD )∗ are the real linear functions RD → R. Polytope The polytope Hs ⊆ RD is the convex hull of the finite set of feature vectors associated with the K hypotheses for the sth sentence, i.e. Hs = conv(hs,1 , . . . , hs,"
N15-1041,N12-1047,0,0.0872564,"ror function, a set of S oracle hypotheses are indexed with the vector ˆi: ˆis = argmin E(es,i , rs ) for 1 ≤ s ≤ S i For a given s the objective at iteration n + 1 is : K X 1 minimise kw(n+1) − w(n) k2 + C ξj 2 w(n+1) (7) j=1 subject to ξj ≥ 0 and for 1 ≤ j ≤ K, ˆis 6= j : w(n+1) (hs,j − hs,bis ) + ∆(es,bis , es,j ) − ξj ≤ 0 where {ξ} are slack variables added to allow infeasible solutions, and C controls the trade-off between error minimisation and margin maximisation. The online nature of the optimiser results in complex implementations, therefore batch versions of MIRA have been proposed (Cherry and Foster, 2012; Gimpel and Smith, 2012). Although MERT, LP-MERT, PRO, and MIRA carry out their search in very different ways, we can compare them in terms of the constraints they are attempting to satisfy. A feasible solution for LPMERT is also an optimal solution for MERT, and vice versa. The constraints (Eqn. (5)) that define LP-MERT are a subset of the constraints (Eqn. (6)) that define PRO and so a feasible solution for PRO will also be feasible for LP-MERT; however the converse is not necessarily true. The constraints that define MIRA (Eqn. (7)) are similar to the LP-MERT constraints (5), although with"
N15-1041,D08-1024,0,0.0786097,"Missing"
N15-1041,N09-1025,0,0.0604483,"Missing"
N15-1041,D14-1179,0,0.0201411,"Missing"
N15-1041,W02-1001,0,0.0876056,"solutions of a linear model in polynomial time using this description. The immediate conclusion from this work is that the current methods for estimating linear models as done in SMT works best for low dimensional feature vectors. We can consider the SMT linear model as a member of a family of linear models where the output values are highly structured, and where each input yields a candidate space of possible output values. We have already noted that the constraints in (13) are shared with the structured-SVM (Tsochantaridis et al., 2005), and we can also see the same constraints in Eqn. 3 of Collins (2002). It is our belief that our analysis is applicable to all models in this family and extends far beyond the discussion of SMT here. We note that the upper bound on feasible solutions increases polynomially in training set size S, whereas the number of possible solutions increases exponentially in S. The result is that the ratio of feasible to possible solutions decreases with S. Our analysis suggests that inherent regularisation should be improved by increasing training set size. This confirms most researchers intuition, with perhaps even larger training sets needed than previously believed. An"
N15-1041,P14-1129,0,0.0438082,"Missing"
N15-1041,N15-1106,0,0.0110651,",j ) ≥ 0 (6) where ∆ computes the difference in error between two hypotheses. The difference vectors (hs,j − hs,i ) associated with each constraint can be used as input vectors for a binary classification problem in which the aim is to predict whether the the difference in error ∆(es,i , es,j ) is positive or negative. Hopkins and May (2011) call this algorithm Pairwise Ranking Optimisation (PRO). Because there are SK 2 difference vectors across all source sentences, a subset of constraints is sampled in the original formulation; with effcient calculation of rankings, sampling can be avoided (Dreyer and Dong, 2015). The online error based training algorithm MIRA (Crammer et al., 2006) is also used for SMT (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012). Using a sentence-level error function, a set of S oracle hypotheses are indexed with the vector ˆi: ˆis = argmin E(es,i , rs ) for 1 ≤ s ≤ S i For a given s the objective at iteration n + 1 is : K X 1 minimise kw(n+1) − w(n) k2 + C ξj 2 w(n+1) (7) j=1 subject to ξj ≥ 0 and for 1 ≤ j ≤ K, ˆis 6= j : w(n+1) (hs,j − hs,bis ) + ∆(es,bis , es,j ) − ξj ≤ 0 where {ξ} are slack variables added to allow infeasible solutions, and C controls the trade-of"
N15-1041,N13-1025,0,0.0274462,"Missing"
N15-1041,D11-1004,0,0.34909,"hat severe overtraining is a problem in many current linear model formulations due to this lack of robustness. In the process of building this geometric representation of linear models we discuss algorithms such as the Minkowski sum algorithm (Fukuda, 2004) and projected MERT (Section 4.2) that could be useful for designing new and more robust training algorithms for SMT and other natural language processing problems. Using this geometric representation of MERT we investigate whether the optimisation of linear models is tractable in general. Previous work on finding optimal solutions in MERT (Galley and Quirk, 2011) established a worstcase complexity that was exponential in the number of sentences, in contrast we show that exponential dependence in the worst-case complexity is mainly in the number of features. Although our work is framed with respect to MERT, the convex geometric description is also applicable to other error-based training methods for linear models. We believe our analysis has important ramifications because it suggests that the current trend in building statistical machine translation systems by introducing a very large number of sparse features is inherently not robust. 1 (1) 2 Introdu"
N15-1041,D13-1201,0,0.169092,"e solutions. However, if a feasible solution is available for MIRA, then these extra quantities are unnecessary. With these quantities removed, then we recover a ‘hard-margin’ optimiser, which utilises the same constraint set as in LP-MERT. In the feasible case, the solution found by MIRA is also a solution for LP-MERT. 2.1 Survey of Recent Work One avenue of SMT research has been to add as many features as possible to the linear model, especially in the form of sparse features (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012; Flanigan et al., 2013; Galley et al., 2013; Green et al., 2013). The assumption is that the addition of new features will improve translation performance. It is interesting to read the justification for many of these works as stated in their abstracts. For example Hopkins and May (2011) state that: We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems Cherry and Foster (2012) state: Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online. Along similar lines Gimpel and Smi"
N15-1041,N12-1023,0,0.0615529,"oracle hypotheses are indexed with the vector ˆi: ˆis = argmin E(es,i , rs ) for 1 ≤ s ≤ S i For a given s the objective at iteration n + 1 is : K X 1 minimise kw(n+1) − w(n) k2 + C ξj 2 w(n+1) (7) j=1 subject to ξj ≥ 0 and for 1 ≤ j ≤ K, ˆis 6= j : w(n+1) (hs,j − hs,bis ) + ∆(es,bis , es,j ) − ξj ≤ 0 where {ξ} are slack variables added to allow infeasible solutions, and C controls the trade-off between error minimisation and margin maximisation. The online nature of the optimiser results in complex implementations, therefore batch versions of MIRA have been proposed (Cherry and Foster, 2012; Gimpel and Smith, 2012). Although MERT, LP-MERT, PRO, and MIRA carry out their search in very different ways, we can compare them in terms of the constraints they are attempting to satisfy. A feasible solution for LPMERT is also an optimal solution for MERT, and vice versa. The constraints (Eqn. (5)) that define LP-MERT are a subset of the constraints (Eqn. (6)) that define PRO and so a feasible solution for PRO will also be feasible for LP-MERT; however the converse is not necessarily true. The constraints that define MIRA (Eqn. (7)) are similar to the LP-MERT constraints (5), although with the addition of slack va"
N15-1041,P13-1031,0,0.0639487,"if a feasible solution is available for MIRA, then these extra quantities are unnecessary. With these quantities removed, then we recover a ‘hard-margin’ optimiser, which utilises the same constraint set as in LP-MERT. In the feasible case, the solution found by MIRA is also a solution for LP-MERT. 2.1 Survey of Recent Work One avenue of SMT research has been to add as many features as possible to the linear model, especially in the form of sparse features (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012; Flanigan et al., 2013; Galley et al., 2013; Green et al., 2013). The assumption is that the addition of new features will improve translation performance. It is interesting to read the justification for many of these works as stated in their abstracts. For example Hopkins and May (2011) state that: We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems Cherry and Foster (2012) state: Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online. Along similar lines Gimpel and Smith (2012) state: [We]"
N15-1041,D11-1125,0,0.359897,"e over a plane embedded in parameter space. The description of this algorithm relies on convex geometry, which is the mathematics of polytopes and their faces. ˆ(f ; w) = argmax{wh(e, f )} e e where translation scores are a linear combination of the D × 1 feature vector h(e, f ) ∈ RD under the 1 × D model parameter vector w. Convex geometry (Ziegler, 1995) is the mathematics of such linear equations presented as the study of convex polytopes. We use convex geometry to show that the behaviour of training methods such as MERT (Och, 2003; Macherey et al., 2008), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and others converge with a high feature dimension. In particular we analyse how robustness decreases in linear models as feature dimension increases. We believe that severe overtraining is a problem in many current linear model formulations due to this lack of robustness. In the process of building this geometric representation of linear models we discuss algorithms such as the Minkowski sum algorithm (Fukuda, 2004) and projected MERT (Section 4.2) that could be useful for designing new and more robust training algorithms for SMT and other natural language processing problems. Using this geo"
N15-1041,D13-1176,0,0.0412785,"ce of w(1) or w(2) , which are equidistant from w(0) . In this affine projection of parameter space it is unclear which one is the optimum. However, if we consider the normal fan as a whole we can clearly see that ˆ ∈ N{hi } is the optimal point under the regularw isation. However, it is not obvious in the projected ˆ is the better choice. This parameter space that w analysis suggests that direct intervention, e.g. monitoring BLEU on a held-out set, may be more effective in avoiding overtraining. tors. We also note that non-linear models methods, such as neural networks (Schwenk et al., 2006; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Cho et al., 2014) and decision forests (Criminisi et al., 2011) are not bound by these analyses. In particular neural networks are non-linear functions of the features, and decision forests actively reduce the number of features for individual trees in the forrest. From the perspective of this paper, the recent improvements in SMT due to neural networks are well motivated. 6 Acknowledgments Discussion The main contribution of this work is to present a novel geometric description of MERT. We show that it is possible to enumerate all the feasible solutions of a linear mode"
N15-1041,D08-1076,0,0.404611,"sation of line optimisation that computes the error surface over a plane embedded in parameter space. The description of this algorithm relies on convex geometry, which is the mathematics of polytopes and their faces. ˆ(f ; w) = argmax{wh(e, f )} e e where translation scores are a linear combination of the D × 1 feature vector h(e, f ) ∈ RD under the 1 × D model parameter vector w. Convex geometry (Ziegler, 1995) is the mathematics of such linear equations presented as the study of convex polytopes. We use convex geometry to show that the behaviour of training methods such as MERT (Och, 2003; Macherey et al., 2008), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and others converge with a high feature dimension. In particular we analyse how robustness decreases in linear models as feature dimension increases. We believe that severe overtraining is a problem in many current linear model formulations due to this lack of robustness. In the process of building this geometric representation of linear models we discuss algorithms such as the Minkowski sum algorithm (Fukuda, 2004) and projected MERT (Section 4.2) that could be useful for designing new and more robust training algorithms for SMT and"
N15-1041,P02-1038,0,0.0471227,"umber of sentences, in contrast we show that exponential dependence in the worst-case complexity is mainly in the number of features. Although our work is framed with respect to MERT, the convex geometric description is also applicable to other error-based training methods for linear models. We believe our analysis has important ramifications because it suggests that the current trend in building statistical machine translation systems by introducing a very large number of sparse features is inherently not robust. 1 (1) 2 Introduction The linear model of Statistical Machine Translation (SMT) (Och and Ney, 2002) casts translation as a Training Linear Models Let f1 . . . fS be a set of S source language sentences with reference translations r1 . . . rS . The goal is to estimate the model parameter vector w so as to minimize an error count based on an automated metric, such as BLEU (Papineni et al., 2002), assumed to be 376 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 376–386, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics additive over sentences: ˆ = argmin w w S X E(ˆ e(fs ; w), rs ) (2) s=1 Optimisa"
N15-1041,P03-1021,0,0.289722,"me generalisation of line optimisation that computes the error surface over a plane embedded in parameter space. The description of this algorithm relies on convex geometry, which is the mathematics of polytopes and their faces. ˆ(f ; w) = argmax{wh(e, f )} e e where translation scores are a linear combination of the D × 1 feature vector h(e, f ) ∈ RD under the 1 × D model parameter vector w. Convex geometry (Ziegler, 1995) is the mathematics of such linear equations presented as the study of convex polytopes. We use convex geometry to show that the behaviour of training methods such as MERT (Och, 2003; Macherey et al., 2008), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and others converge with a high feature dimension. In particular we analyse how robustness decreases in linear models as feature dimension increases. We believe that severe overtraining is a problem in many current linear model formulations due to this lack of robustness. In the process of building this geometric representation of linear models we discuss algorithms such as the Minkowski sum algorithm (Fukuda, 2004) and projected MERT (Section 4.2) that could be useful for designing new and more robust training"
N15-1041,P02-1040,0,0.0932049,"We believe our analysis has important ramifications because it suggests that the current trend in building statistical machine translation systems by introducing a very large number of sparse features is inherently not robust. 1 (1) 2 Introduction The linear model of Statistical Machine Translation (SMT) (Och and Ney, 2002) casts translation as a Training Linear Models Let f1 . . . fS be a set of S source language sentences with reference translations r1 . . . rS . The goal is to estimate the model parameter vector w so as to minimize an error count based on an automated metric, such as BLEU (Papineni et al., 2002), assumed to be 376 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 376–386, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics additive over sentences: ˆ = argmin w w S X E(ˆ e(fs ; w), rs ) (2) s=1 Optimisation can be made tractable by restricting the search to rescoring of K-best lists of translation hypotheses, {es,i , 1 ≤ i ≤ K}Ss=1 . For fs , let hs,i = h(es,i , fs ) be the feature vector associated with hypothesis es,i . Restricted to these lists, the general decoder of Eqn. 1 becomes ˆ(fs ; w"
N15-1041,W13-2225,1,0.904832,"Missing"
N15-1041,P06-2093,0,0.0386512,"(2013) we have a choice of w(1) or w(2) , which are equidistant from w(0) . In this affine projection of parameter space it is unclear which one is the optimum. However, if we consider the normal fan as a whole we can clearly see that ˆ ∈ N{hi } is the optimal point under the regularw isation. However, it is not obvious in the projected ˆ is the better choice. This parameter space that w analysis suggests that direct intervention, e.g. monitoring BLEU on a held-out set, may be more effective in avoiding overtraining. tors. We also note that non-linear models methods, such as neural networks (Schwenk et al., 2006; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Cho et al., 2014) and decision forests (Criminisi et al., 2011) are not bound by these analyses. In particular neural networks are non-linear functions of the features, and decision forests actively reduce the number of features for individual trees in the forrest. From the perspective of this paper, the recent improvements in SMT due to neural networks are well motivated. 6 Acknowledgments Discussion The main contribution of this work is to present a novel geometric description of MERT. We show that it is possible to enumerate all the fea"
N15-1041,D14-2005,0,0.0423379,"n with the O(K S ) for LP-MERT the worst case complexity of the reverse search algorithm is linear with respect to the size of vert(H). 4.2 Two Dimensional Projected MERT We now explore whether the reverse search algorithm is a practical method for performing MERT using an open source implementation of the algorithm (Weibel, 2010). For reasons discussed in the next section, we wish to reduce the feature dimension. For M < D, we can define a projection matrix AM +1,D that maps hi ∈ RD into RM +1 as ˜ i, h ˜ i ∈ RM +1 . There are techAM +1,D hi = h nical constraints to be observed, discussed in Waite (2014). We note that when M = 1 we obtain Eqn. (4). 382 For our demonstration, we plot the error count over a plane in (RD )∗ . Using the CUED Russian-toEnglish (Pino et al., 2013) entry to WMT’13 (Bojar et al., 2013) we build a tune set of 1502 sentences. The system uses 12 features which we initially tune with lattice MERT (Macherey et al., 2008) to get a parameter w(0) . Using this parameter we generate 1000-best lists. We then project the feature functions in the 1000-best lists to a 3-dimensional representation that includes the source-to-target phrase probability (UtoV), the word insertion pen"
N15-1041,D07-1080,0,0.0363496,"associated with each constraint can be used as input vectors for a binary classification problem in which the aim is to predict whether the the difference in error ∆(es,i , es,j ) is positive or negative. Hopkins and May (2011) call this algorithm Pairwise Ranking Optimisation (PRO). Because there are SK 2 difference vectors across all source sentences, a subset of constraints is sampled in the original formulation; with effcient calculation of rankings, sampling can be avoided (Dreyer and Dong, 2015). The online error based training algorithm MIRA (Crammer et al., 2006) is also used for SMT (Watanabe et al., 2007; Chiang et al., 2008; Chiang, 2012). Using a sentence-level error function, a set of S oracle hypotheses are indexed with the vector ˆi: ˆis = argmin E(es,i , rs ) for 1 ≤ s ≤ S i For a given s the objective at iteration n + 1 is : K X 1 minimise kw(n+1) − w(n) k2 + C ξj 2 w(n+1) (7) j=1 subject to ξj ≥ 0 and for 1 ≤ j ≤ K, ˆis 6= j : w(n+1) (hs,j − hs,bis ) + ∆(es,bis , es,j ) − ξj ≤ 0 where {ξ} are slack variables added to allow infeasible solutions, and C controls the trade-off between error minimisation and margin maximisation. The online nature of the optimiser results in complex impleme"
N15-1041,W06-1626,0,\N,Missing
N15-1041,W02-1021,0,\N,Missing
N15-1041,C96-2141,0,\N,Missing
N15-1041,N03-1031,0,\N,Missing
N15-1041,D07-1055,0,\N,Missing
N15-1041,N07-1062,0,\N,Missing
N15-1041,N04-1022,0,\N,Missing
N15-1041,W12-6219,1,\N,Missing
N15-1041,2005.eamt-1.39,0,\N,Missing
N15-1041,W13-2201,0,\N,Missing
N15-1041,W11-2160,0,\N,Missing
N15-1105,D14-1082,0,0.195659,"g to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SM"
N15-1105,P05-1066,0,0.242204,"equired. Preordering schemes can be automatically learnt from source-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NL"
N15-1105,D11-1018,0,0.063682,"omatically learnt from source-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including languag"
N15-1105,P14-1129,0,0.0562621,"Missing"
N15-1105,D08-1089,0,0.0440634,"the faster decoding will run as less distortion will be needed. Normalizing over the number of source words allows us to compare this metric across language pairs, and so the potential impact of preordering in translation performance becomes apparent. See Figure 2 for results across several language pairs. In all cases our proposed NN-based preorderer achieves the lowest normalized crossing score among all preordering schemes. 3.3 Translation performance For translation experiments, we use a phrase-based decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008). The decoder stack size is set to 1000. Weights are tuned using MERT to optimize BLEU on the dev set. In English-to-Japanese and Chinese we use character-BLEU instead. To minimise optimization noise, we tune all our systems from flat parameters three times and report average BLEU score and standard deviation on the test set. Table 1 contrasts the performance obtained by the system when using no preordering capabilities (baseline), and when using three alternative preordering schemes: the rule-based approach of Genzel (2010), the linear-model logistic-regression approach of Jehl et al (2014) a"
N15-1105,C10-1043,0,0.451565,"dicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denve"
N15-1105,gimenez-marquez-2004-svmtool,0,0.0508697,"Missing"
N15-1105,E14-1026,1,0.431273,"Missing"
N15-1105,D13-1176,0,0.0490322,"North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage of NNs in preordering for SMT. 2 Preordering as node-pair swapping Jehl et al (2014) describe a preordering scheme based on a logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted in order to have a more"
N15-1105,D13-1049,0,0.175053,"st ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved wor"
N15-1105,D13-1054,0,0.0234436,"increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage of NNs in preordering for SMT. 2 Preordering as node-pair sw"
N15-1105,D12-1077,0,0.0601436,"e-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et"
N15-1105,D14-1197,0,0.0255573,"In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage"
N15-1105,D14-1003,0,0.0228415,"ding language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage of NNs in preordering for SMT. 2 Preordering as node-pair swapping Jehl et al (2014) describe a preordering scheme based on a logistic regression model t"
N15-1105,P14-1138,0,0.0137744,"al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first"
N15-1105,D13-1140,0,0.0374055,"nother logistic regression model or a linear regression model” (Murphy, 2012). Given this, we propose a straightforward alternative to the above framework: replace the linear logistic regression model by a neural network (NN). This way a superior modeling performance of the nodeswapping phenomenon is to be expected. Additionally, feature combination need not be engineered anymore because that is learnt by the NN in training (line 6 in Figure 1 is skipped). Training the neural network requires the same labeled samples that were used by Jehl et al (2014). We use the NPLM toolkit out-of-the-box (Vaswani et al., 2013). The architecture is a feed-forward neural network (Bengio et al., 2003) with four layers. The first layer i contains the input embeddings. The next two hidden layers (h1 , h2 ) use rectified linear units; the last one is the softmax layer (o). We did not experiment with deeper NNs. For our purposes, the input vocabulary of the NN is the set of all possible feature indicator names that are used for preordering1 . There are no OOVs. Given the sequence of ∼ 20 features seen by the 1 Using a vocabulary of the 5K top-frequency English words, 50 word classes, approximately 40 POS tags and 50 depen"
N15-1105,N09-1028,0,0.0389467,"chemes can be automatically learnt from source-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been"
N15-1105,P12-1096,0,0.04679,"a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In t"
N15-1105,P13-1017,0,0.0293528,"regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowl"
N15-1105,bojar-etal-2014-hindencorp,0,\N,Missing
N16-1100,W12-3159,0,0.0167268,"ed and memory constraints on neural network trainings and report better results compared to those of naive models which explicitly put high costs on regions that violate constraints. A different approach based on augmented Lagrangians is proposed by Gramacy et al. (2014). The authors apply BO in a water decontamination setting where the goal is to find the optimal pump positioning subject to restrictions on water and contaminant flows. All these previous work in constrained BO use GPs as the prior model. Optimizing decoding parameters for speed is an understudied problem in the MT literature. Chung and Galley (2012) propose direct search methods to optimize feature weights and decoder parameters jointly but aiming at the traditional goal of maximizing translation quality. To enable search parameter optimization they enforce a deterministic time penalty on BLEU scores, which is not ideal due to the stochastic nature of time measurements shown 864 on Section 3.2 (this issue is also cited by the authors in their manuscript). It would be interesting to incorporate their approach into BO for optimizing translation quality under speed constraints. 5 Conclusion We have shown that Bayesian Optimisation performs"
N16-1100,D08-1089,0,0.0136634,"ish-to-German we use mixed-domain tuning/test sets, which have about 1K sentences each and were created to evenly represent different domains, including world news, health, sport, science and others. For Chinese-toEnglish we use in-domain sets (2K sentences) created by randomly extracting unique parallel sentences from in-house parallel text collections; this in-domain data leads to higher BLEU scores than in the other tasks, as will be reported later. In all cases we have one reference translation. We use an in-house implementation of a phrase-based decoder with lexicalized reordering model (Galley and Manning, 2008). The system uses 21 features, whose weights are optimized for BLEU via MERT (Och and Ney, 2004) at very slow decoder parameter settings in order to minimize search errors in tuning. The feature weights remain fixed during the speed tuning process. 3.1 n: number of translations. The maximum number of alternative translations per source phrase considered in decoding. 3.2 Measuring Decoding Speed To get a better understanding of the speed measurements we decode the English-German tuning set 100 times with a slow decoder parameter setting, i.e. θ = (5, 100, 100), and repeat for a fast setting wit"
N16-1100,2001.mtsummit-papers.68,0,0.0233509,"ically finding the decoder parameters and feature weights that yield the best BLEU at a specified minimum decoding speed. This is potentially very expensive because each change in a decoder parameter requires re-decoding to assess both BLEU and translation speed. This is under-studied in the literature, despite its importance for real-life commercial SMT engines whose speed and latency can be as significant for user satisfaction as overall translation quality. Introduction Research in Statistical Machine Translation (SMT) aims to improve translation quality, typically measured by BLEU scores (Papineni et al., 2001), over a baseline system. Given a task defined by a language pair and its corpora, the quality of a system is assessed by contrasting choices made in rule/phrase extraction criteria, feature functions, decoding algorithms and parameter optimization techniques. Some of these choices result in systems with significant differences in performance. For example, in phrase-based translation (PBMT) (Koehn et al., ∗ This work was done during an internship of the first author at SDL Research, Cambridge. We propose to use Bayesian Optimization (Brochu et al., 2010b; Shahriari et al., 2015) for this const"
N16-1100,W10-1748,0,0.020055,"andomized sets of sentences. Warped GPs (Snelson et al., 2003) could be a more accurate model as they can learn transformations for heteroscedastic data without relying on a fixed transformation, as we do with log speed measurements. Modelling of the objective function could also be improved. In our experiments we used a GP with a Mat`ern52 kernel, but this assumes f is doubly-differentiable and exhibits Lipschitzcontinuity (Brochu et al., 2010b). Since that does not hold for the BLEU score, using alternative smoother metrics such as linear corpus BLEU (Tromble et al., 2008) or expected BLEU (Rosti et al., 2010) could yield better results. Other recent developments in Bayesian Optimisation could be applied to our settings, like multi-task optimization (Swersky et al., 2013) or freeze-thaw optimization (Swersky et al., 2014). In our application we treat Bayesian Optimisation as a sequential model. Parallel approaches do exist (Snoek et al., 2012; Gonz´alez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements. However for more complex optimisation scenarios or for problems that require lengthy searc"
N16-1100,D08-1065,0,0.0262866,"sets, one possibility would be to use randomized sets of sentences. Warped GPs (Snelson et al., 2003) could be a more accurate model as they can learn transformations for heteroscedastic data without relying on a fixed transformation, as we do with log speed measurements. Modelling of the objective function could also be improved. In our experiments we used a GP with a Mat`ern52 kernel, but this assumes f is doubly-differentiable and exhibits Lipschitzcontinuity (Brochu et al., 2010b). Since that does not hold for the BLEU score, using alternative smoother metrics such as linear corpus BLEU (Tromble et al., 2008) or expected BLEU (Rosti et al., 2010) could yield better results. Other recent developments in Bayesian Optimisation could be applied to our settings, like multi-task optimization (Swersky et al., 2013) or freeze-thaw optimization (Swersky et al., 2014). In our application we treat Bayesian Optimisation as a sequential model. Parallel approaches do exist (Snoek et al., 2012; Gonz´alez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements. However for more complex optimisation scenarios or f"
N16-1100,D15-1253,0,0.0308686,"ng procedure of Miao et al. (2014) but obtained mixed results on our test sets. Effective ways to combine BO with well-established feature tuning algorithms such as MERT could be a promising research direction. 4 Related Work Bayesian Optimization has been previously used for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system. In SMT,"
N16-1100,D15-1251,0,0.0165713,"for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system. In SMT, Miao et al. (2014) use BO for feature weight tuning and report better results in some language pairs when compared to traditional tuning algorithms. Our approach is heavily based on the work of Gelbart et al. (2014) and Hern´andez-Lobato et al. (2015) which uses BO in the pr"
N16-1100,P02-1040,0,\N,Missing
N16-1100,J04-4002,0,\N,Missing
N16-1100,N03-1017,0,\N,Missing
N18-2081,P17-1141,0,0.0649765,"y (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals and is often more important than translation quality alone. Although domain adaptation and guided decoding can help to reduce errors in these use cases, they do not provide reliable solutions. Another recent line of work strictly enforces a given set of words in the output (Anderson et al., 2017; Hokamp and Liu, 2017; Crego et al., 2016). 2 Constrained Beam Search A naive approach to decoding with constraints would be to use a large beam size and select from the set of complete hypotheses the best that satis506 Proceedings of NAACL-HLT 2018, pages 506–512 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics fies all constraints. However, this is infeasible in practice because it would require searching a potentially very large space to ensure that even hypotheses with low model score due to the inclusion of a constraint would be part of the set of outputs. A better st"
N18-2081,W17-4711,0,0.0535327,"tives and encoded as different arcs connecting the same states. When alternatives consist of multiple tokens, the alternative paths will contain intermediate states. Figure 1 shows an FSA with constraints C1 and C2 where C1 is a phrase (yielding intermediate states s1 , s4 ) and C2 consists of two single-token alternatives. Both permutations C1 C2 and C2 C1 lead to final state s5 with both constraints satisfied. 2.2 a s0 V - {a} - C2 s0 C [i,j) s1 j &gt; i, 0 ≤ i, j ≤ |S| Because the attention weights in attention-based decoders function as soft alignments from the target to the source sentence (Alkhouli and Ney, 2017), we use them to decide at which position a constraint should be inserted in the output. At each time step in a hypothesis, we determine the source position with the maximum attention. If it falls into a constrained source span and this span matches an outgoing arc in the current acceptor state, we extend the current hypothesis with the arc label. Thus, the outgoing arcs in non-intermediate states are active or inactive depending on the current attentions. This reduces the complexity from O(tk2c ) to O(tkc) by ignoring all but one constraint permutation and in practice, disabling vocabulary lo"
N18-2081,N18-3013,1,0.878414,"Missing"
N18-2081,D17-1098,0,0.4558,"tions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals and is often more important than translation quality alone. Although domain adaptation and guided decoding can help to reduce errors in these use cases, they do not provide reliable solutions. Another recent line of work strictly enforces a given set of words in the output (Anderson et al., 2017; Hokamp and Liu, 2017; Crego et al., 2016). 2 Constrained Beam Search A naive approach to decoding with constraints would be to use a large beam size and select from the set of complete hypotheses the best that satis506 Proceedings of NAACL-HLT 2018, pages 506–512 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics fies all constraints. However, this is infeasible in practice because it would require searching a potentially very large space to ensure that even hypotheses with low model score due to the inclusion of a constraint would be part of the set o"
N18-2081,Q17-1024,0,0.0992256,"Missing"
N18-2081,2015.iwslt-evaluation.11,0,0.0610273,"en problem. We describe our approach to constrained neural decoding based on finite-state machines and multistack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints. 1 Introduction Adapting an NMT system with domain-specific data is one way to adjust its output vocabulary to better match the target domain (Luong and Manning, 2015; Sennrich et al., 2016). Another way to encourage the beam decoder to produce certain words in the output is to explicitly reward n-grams provided by an SMT system (Stahlberg et al., 2017) or language model (Gulcehre et al., 2017) or to modify the vocabulary distribution of the decoder with suggestions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating"
N18-2081,P16-1009,0,0.272321,"ur approach to constrained neural decoding based on finite-state machines and multistack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints. 1 Introduction Adapting an NMT system with domain-specific data is one way to adjust its output vocabulary to better match the target domain (Luong and Manning, 2015; Sennrich et al., 2016). Another way to encourage the beam decoder to produce certain words in the output is to explicitly reward n-grams provided by an SMT system (Stahlberg et al., 2017) or language model (Gulcehre et al., 2017) or to modify the vocabulary distribution of the decoder with suggestions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals"
N18-2081,W17-4716,0,0.057497,"coding with attentions as a means of reducing misplacement and duplication when translating user constraints. 1 Introduction Adapting an NMT system with domain-specific data is one way to adjust its output vocabulary to better match the target domain (Luong and Manning, 2015; Sennrich et al., 2016). Another way to encourage the beam decoder to produce certain words in the output is to explicitly reward n-grams provided by an SMT system (Stahlberg et al., 2017) or language model (Gulcehre et al., 2017) or to modify the vocabulary distribution of the decoder with suggestions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals and is often more important than translation quality alone. Although domain adaptation and guided decoding can help to reduce errors in these use cases, they do not provide reliable solutions. Another recent line of work strictly enforces a given set of words in the output (Anderson et al., 2017; Hokamp and Liu, 2017; Crego"
N18-2081,1981.tc-1.3,0,0.819746,"Missing"
N18-2081,E17-2058,1,0.891813,"Missing"
N18-3013,W17-4716,0,0.0641777,"ng without GPUs, and still an avenue for research (Devlin, 2017). Great speeds have been reported by Junczys-Dowmunt et al. (2016) on GPUs, for which batching queries to the neural model is essential. Disk usage and memory footprint of pure neural systems are certainly lower than that of SMT systems, but at the same time GPU memory is limited and high-end GPUs are expensive. Further to that, consumers still need the ability to constrain translations; in particular, brandrelated information is often as important for companies as translation quality itself, and is currently under investigation (Chatterjee et al., 2017; Hokamp and Liu, 2017; Hasler et al., 2018). It is also well known that pure neural systems 2 Neural Machine Translation and LMBR Given a source sentence x, a sequence-tosequence NMT model scores a candidate translation sentence y = y1T with T words as: PN M T (y1T |x) = T Y t=1 PN M T (yt |y1t−1 , x) (1) where PN M T (yt |y1t−1 , x) uses a neural function fN M T (·). To account for batching B neu106 Proceedings of NAACL-HLT 2018, pages 106–113 c New Orleans, Louisiana, June 1 - 6, 2018. 2017 Association for Computational Linguistics ral queries together, our abstract function takes the form"
N18-3013,2006.iwslt-papers.6,0,0.0478008,"o particular constraint on speed for the research systems reported in Table 1. We now address the question of deploying NMT systems so that MT users get the best quality improvements at real-time speed and with acceptable memory footprint. As an example, we analyse in detail the English-German FNMT and LNMT case and discuss the main trade-offs if one wanted to accelerate them. Although the actual measurements vary across all our productised NMT engines, the trends are similar to the ones reported here. In this particular case we specify a beam width of 0.01 for early pruning (Wu et al., 2016; Delaney et al., 2006) and reduce the beam size to 4. We also shrink the ensemble into one single big model5 using the data-free shrinking method described by Stahlberg and Byrne (2017), an inexpensive way to improve both speed and GPU memory footprint. In the process, both accelerated systems have lost 0.9 BLEU relative to the baseline. As an example, let us break down the effects of accelerating the LNMT system: using only 200-best hypotheses from the phrase-based translation lattice reduces 0.3 BLEU. Replacing the ensemble with a data-free shrunken model reduces another 0.2 BLEU and decreasing the beam size redu"
N18-3013,D17-1300,0,0.0784178,"n effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems, for five translation tasks. Finally, we discuss how to prepare (LMBR-based) NMT systems for deployment, and how our batching algorithm performs in terms of memory and speed. We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently"
N18-3013,C16-1290,0,0.0222591,"gispert|ehasler|bbyrne}@sdl.com † ‡ Department of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems,"
N18-3013,P17-1141,0,0.0503143,"l an avenue for research (Devlin, 2017). Great speeds have been reported by Junczys-Dowmunt et al. (2016) on GPUs, for which batching queries to the neural model is essential. Disk usage and memory footprint of pure neural systems are certainly lower than that of SMT systems, but at the same time GPU memory is limited and high-end GPUs are expensive. Further to that, consumers still need the ability to constrain translations; in particular, brandrelated information is often as important for companies as translation quality itself, and is currently under investigation (Chatterjee et al., 2017; Hokamp and Liu, 2017; Hasler et al., 2018). It is also well known that pure neural systems 2 Neural Machine Translation and LMBR Given a source sentence x, a sequence-tosequence NMT model scores a candidate translation sentence y = y1T with T words as: PN M T (y1T |x) = T Y t=1 PN M T (yt |y1t−1 , x) (1) where PN M T (yt |y1t−1 , x) uses a neural function fN M T (·). To account for batching B neu106 Proceedings of NAACL-HLT 2018, pages 106–113 c New Orleans, Louisiana, June 1 - 6, 2018. 2017 Association for Computational Linguistics ral queries together, our abstract function takes the form of fN M T (St−1 , yt−1"
N18-3013,W17-3204,0,0.0781637,"Missing"
N18-3013,P10-2006,1,0.911633,"to focus on, informed by A and St−1 . Bahdanau et al. (2015) use recurrent layers to both compute A and the next target word yt . Gehring et al. (2017) use convolutional layers instead, and Vaswani et al. (2017) prescind from GRU or LSTM layers, relying heavily on multi-layered attention mechanisms, stateful only on the translation side. Finally, this function can also represent an ensemble of neural models. Lattice Minimum Bayes Risk decoding computes n-gram posterior probabilities from an evidence space and uses them to score a hypothesis space (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). It improves single SMT systems, and also lends itself quite nicely to system combination (Sim et al., 2007; de Gispert et al., 2009). Stahlberg et al. (2017) have recently shown a way to use it with NMT decoding: a traditional SMT system is first used to create an evidence space ϕe , and the NMT space is then scored left-to-right with both the NMT model(s) and the n-gram posteriors gathered from ϕe . More formally: z it turns into a dense matrix with the summation of Θ0 . Both sparse and dense operations can be performed on the GPU. We have found it more efficient to compute first all the sp"
N18-3013,N04-1022,0,0.307153,"n mechanism that determines which source word to focus on, informed by A and St−1 . Bahdanau et al. (2015) use recurrent layers to both compute A and the next target word yt . Gehring et al. (2017) use convolutional layers instead, and Vaswani et al. (2017) prescind from GRU or LSTM layers, relying heavily on multi-layered attention mechanisms, stateful only on the translation side. Finally, this function can also represent an ensemble of neural models. Lattice Minimum Bayes Risk decoding computes n-gram posterior probabilities from an evidence space and uses them to score a hypothesis space (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). It improves single SMT systems, and also lends itself quite nicely to system combination (Sim et al., 2007; de Gispert et al., 2009). Stahlberg et al. (2017) have recently shown a way to use it with NMT decoding: a traditional SMT system is first used to create an evidence space ϕe , and the NMT space is then scored left-to-right with both the NMT model(s) and the n-gram posteriors gathered from ϕe . More formally: z it turns into a dense matrix with the summation of Θ0 . Both sparse and dense operations can be performed on the GPU. We have foun"
N18-3013,C16-1205,0,0.0245164,"rne}@sdl.com † ‡ Department of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems, for five translatio"
N18-3013,D17-1151,0,0.0226232,"(2017), an inexpensive way to improve both speed and GPU memory footprint. In the process, both accelerated systems have lost 0.9 BLEU relative to the baseline. As an example, let us break down the effects of accelerating the LNMT system: using only 200-best hypotheses from the phrase-based translation lattice reduces 0.3 BLEU. Replacing the ensemble with a data-free shrunken model reduces another 0.2 BLEU and decreasing the beam size reduces 0.4 BLEU. The impact of reducing the beam size varies from system to system, although often does not result in substantial quality loss for NMT models (Britz et al., 2017). It is worth noting that these two systems share exactly the same neural model and parameter values. However, LNMT runs 4500 words per minute (wpm) slower than FNMT. Figure 1 breaks down the decoding times for both the accelerated FNMT and LNMT systems. The LNMT pipeline also requires a phrase-based decoder and the extra component to compute the n-gram posterior probabil5 The file size of each 3 individual models of the ensemble is 510MB; the size of the shrunken model is 1.2GB. 110 Figure 2: Batch beam decoder speed measured over newstest-2017 test set, using the accelerated FNMT system (25."
N18-3013,Q17-1007,0,0.0470511,"Missing"
N18-3013,P16-1008,0,0.0228724,"partment of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems, for five translation tasks. Finally,"
N18-3013,W17-3208,0,0.0387818,"Missing"
N18-3013,P03-1021,0,0.0195144,"1 WMT17 eng-ger chi-eng 19.6 15.8 26.1 20.8 26.6 22.0 28.9 24.8 29.2 25.4 28.3 26.4 WAT eng-jpn jpn-eng 33.4 18.0 39.1 25.3 40.4 26.1 44.6 29.4 44.9 30.2 43.3 28.4 Table 1: Quality assessment of our NMT systems with and without LMBR posteriors for GRU-based (FNMT, LNMT) and Transformer models (TNMT, LTNMT). Cased BLEU scores reported on 5 translation tasks.The exact PBMT systems used to compute n-gram posteriors for LNMT and LTNMT systems are also reported. The last row shows scores for the best official submissions to each task. model (Heafield et al., 2013), and is tuned with standard MERT (Och, 2003); n-gram posterior probabilities are computed on-the-fly over rich translation lattices, with size bounded by the PBMT stack and distortion limits. The parameter λ in Equation 2 is set as 0.5 divided by the number of models in the ensemble. Empirically we have found this to be a good setting in many tasks. Unless noted otherwise, the beam size is set to 12 and the NMT beam decoder always batches queries to the neural model. The beam decoder relies on an early preview of ArrayFire 3.6 (Yalamanchili et al., 2015)3 , compiled with CUDA 8.0 libraries. For speed measurements, the decoder uses one s"
N18-3013,P02-1040,0,0.102076,"limits. The parameter λ in Equation 2 is set as 0.5 divided by the number of models in the ensemble. Empirically we have found this to be a good setting in many tasks. Unless noted otherwise, the beam size is set to 12 and the NMT beam decoder always batches queries to the neural model. The beam decoder relies on an early preview of ArrayFire 3.6 (Yalamanchili et al., 2015)3 , compiled with CUDA 8.0 libraries. For speed measurements, the decoder uses one single CPU thread. For hardware, we use an Intel Xeon CPU E5-2640 at 2.60GHz. The GPU is a GeForce GTX 1080Ti. We report cased BLEU scores (Papineni et al., 2002), strictly comparable to the official scores in each task4 . pairs for the WMT17 task, and Japanese-English and English-Japanese for the WAT task. For the German tasks we use news-test2013 as a development set, and news-test2017 as a test set; for Chinese-English, we use news-dev2017 as a development set, and news-test2017 as a test set. For Japanese tasks we use the ASPEC corpus (Nakazawa et al., 2016). We use all available data in each task for training. In addition, for German we use backtranslation data (Sennrich et al., 2016a). All training data for neural models is preprocessed with the"
N18-3013,P16-1009,0,0.374586,"prepare (LMBR-based) NMT systems for deployment, and how our batching algorithm performs in terms of memory and speed. We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed. 1 Introduction The advent of Neural Machine Translation (NMT) has revolutionized the market. Objective improvements (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Gehring et al., 2017; Vaswani et al., 2017) and a fair amount of neural hype have increased the pressure on companies offering Machine Translation services to shift as quickly as possible to this new paradigm. Such a radical change entails non-trivial challenges for deployment; consumers certainly look forward to better translation quality, but do not want to lose all the good features that have been developed over the years along with SMT technology. With NMT, real time decoding is challenging without GPUs, and still an avenue for research (Devlin, 2017). Great speeds have been reported by"
N18-3013,P16-1162,0,0.380163,"prepare (LMBR-based) NMT systems for deployment, and how our batching algorithm performs in terms of memory and speed. We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed. 1 Introduction The advent of Neural Machine Translation (NMT) has revolutionized the market. Objective improvements (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Gehring et al., 2017; Vaswani et al., 2017) and a fair amount of neural hype have increased the pressure on companies offering Machine Translation services to shift as quickly as possible to this new paradigm. Such a radical change entails non-trivial challenges for deployment; consumers certainly look forward to better translation quality, but do not want to lose all the good features that have been developed over the years along with SMT technology. With NMT, real time decoding is challenging without GPUs, and still an avenue for research (Devlin, 2017). Great speeds have been reported by"
N18-3013,I17-1016,0,0.0328748,"Missing"
N18-3013,D17-1208,1,0.787244,"est quality improvements at real-time speed and with acceptable memory footprint. As an example, we analyse in detail the English-German FNMT and LNMT case and discuss the main trade-offs if one wanted to accelerate them. Although the actual measurements vary across all our productised NMT engines, the trends are similar to the ones reported here. In this particular case we specify a beam width of 0.01 for early pruning (Wu et al., 2016; Delaney et al., 2006) and reduce the beam size to 4. We also shrink the ensemble into one single big model5 using the data-free shrinking method described by Stahlberg and Byrne (2017), an inexpensive way to improve both speed and GPU memory footprint. In the process, both accelerated systems have lost 0.9 BLEU relative to the baseline. As an example, let us break down the effects of accelerating the LNMT system: using only 200-best hypotheses from the phrase-based translation lattice reduces 0.3 BLEU. Replacing the ensemble with a data-free shrunken model reduces another 0.2 BLEU and decreasing the beam size reduces 0.4 BLEU. The impact of reducing the beam size varies from system to system, although often does not result in substantial quality loss for NMT models (Britz e"
N18-3013,E17-2058,1,0.780067,"Missing"
N18-3013,P16-2049,1,0.838436,"es to improve on. In addition, for LNMT systems we tune phrasebased decoder parameters such as the distortion limit, the number of translations per source phrase and the stack limit. To compute n-gram posteriors we now only take a 200-best from the phrasebased translation lattice. Table 2 shows a contrast of our English-German WMT17 research systems versus the respective accelerated ones. 3. Further, applying LMBR posteriors along with the Transformer model yields gains in all tasks (LTNMT vs TNMT), up to +0.8BLEU in Japanese-English. Interestingly, while we find that rescoring PBMT lattices (Stahlberg et al., 2016) with GRU models yields similar improvements to those reported by Stahlberg et al. (2017), we did not find gains when rescoring with the stronger TNMT models instead. 4.3 FNMT LNMT Research BLEU speed 26.1 2207 26.6 263 Accelerated BLEU speed 25.2 9449 25.7 4927 Table 2: Cased BLEU scores for research vs accelerated English-to-German WMT17 systems. Speed reported in words per minute. Accelerating FNMT and LNMT systems for deployment There is no particular constraint on speed for the research systems reported in Table 1. We now address the question of deploying NMT systems so that MT users get"
N18-3013,D08-1065,0,0.240919,"as† William Tambellini† Adri`a De Gispert†‡ Eva Hasler† Bill Byrne†‡ SDL Research {giglesias|wtambellini|agispert|ehasler|bbyrne}@sdl.com † ‡ Department of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking in"
N19-1406,W18-0529,1,0.874299,"ns tend to be very local, and lexical choices are usually limited. Finite state transducers (FSTs) are an efficient way to represent large structured search spaces. In this paper, we propose to construct a hypothesis space using standard FST operations like composition, and then constrain the output of a neural GEC system to that space. We study two different scenarios: In the first scenario, we do not have access to annotated training data, and only use a small development set for tuning. In this scenario, we construct the hypothesis space using word-level context-independent confusion sets (Bryant and Briscoe, 2018) based on spell checkers and morphology databases, and rescore it with count-based and neural language models (NLMs). In the second scenario, we assume to have enough training data available to train SMT and neural machine translation (NMT) systems. In this case, we make additional use of the SMT lattice and rescore with an NLM-NMT ensemble. Our contributions are: Grammatical error correction (GEC) is one of the areas in natural language processing in which purely neural models have not yet superseded more traditional symbolic models. Hybrid systems combining phrase-based statistical machine t"
N19-1406,P17-1074,1,0.778309,"by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning. 4 https://github.com/grammatical/ baselines-emnlp2016 Saunders et al. (2018). We decode with beam size 12 using the SGNMT decoder (Stahlberg et al., 2017). We evaluate on CoNLL-2014 (Ng et al., 2014) and JFLEG-Test (Napoles et al., 2017), using CoNLL-2013 (Ng et al., 2013) and JFLEGDev as development sets. Our evaluation metrics are GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). We generated M2 files using ERRANT (Bryant et al., 2017) for JFLEG and Tab. 1 to be comparable to Bryant and Briscoe (2018), but used the official M2 files in Tab. 2 to be comparable to Grundkiewicz and JunczysDowmunt (2018). Results Our LM-based GEC results without using annotated training data are summarized in Tab. 1. Even when we use the same resources (same LM and same confusion sets) as Bryant and Briscoe (2018), we see gains on JFLEG (rows 1 vs. 2), probably because we avoid search errors in our FST-based scheme. Adding an NLM yields significant gains across the board. Tab. 2 shows that adding confusion sets to SMT lattices is effective even"
N19-1406,W17-5037,0,0.0410758,"(2018) which allows exact inference, and does not require annotated training data. We report large gains from rescoring with a neural language model. Introduction Grammatical error correction (GEC) is the task of automatically correcting all types of errors in text; e.g. [In a such situaction → In such a situation]. Using neural models for GEC is becoming increasingly popular (Xie et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Ge et al., 2018a,b), possibly combined with phrase-based SMT (Chollampatt et al., 2016; Chollampatt and Ng, 2017; Grundkiewicz and Junczys-Dowmunt, 2018). A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence. GEC is – compared to machine translation – a highly constrained prob• Our technique beats the best published result with comparable amounts of training data on the CoNLL-2014 (Ng et al., 2014) test set when applied to SMT lattices. Our combination strategy yields larger gains over the SMT baselines than simpler rescoring or pipelining used in prior work on hybrid systems (Grundkiewicz and Junczys-Dowmunt, 2018). 2"
N19-1406,N12-1067,0,0.314493,"We delay SGD updates by 2 on four physical GPUs as suggested by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning. 4 https://github.com/grammatical/ baselines-emnlp2016 Saunders et al. (2018). We decode with beam size 12 using the SGNMT decoder (Stahlberg et al., 2017). We evaluate on CoNLL-2014 (Ng et al., 2014) and JFLEG-Test (Napoles et al., 2017), using CoNLL-2013 (Ng et al., 2013) and JFLEGDev as development sets. Our evaluation metrics are GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). We generated M2 files using ERRANT (Bryant et al., 2017) for JFLEG and Tab. 1 to be comparable to Bryant and Briscoe (2018), but used the official M2 files in Tab. 2 to be comparable to Grundkiewicz and JunczysDowmunt (2018). Results Our LM-based GEC results without using annotated training data are summarized in Tab. 1. Even when we use the same resources (same LM and same confusion sets) as Bryant and Briscoe (2018), we see gains on JFLEG (rows 1 vs. 2), probably because we avoid search errors in our FST-based scheme. Adding an NLM yields significant gains across the board. Tab. 2 shows th"
N19-1406,W13-1703,0,0.187449,"pment sets using Powell search (Powell, 1964).3 3 Experiments Experimental setup In our experiments with annotated training data we use the SMT system of Junczys-Dowmunt and Grundkiewicz (2016)4 to create 1000-best lists from which we derive the input lattices I. All our LMs are trained on the One Billion Word Benchmark dataset (Chelba et al., 2014). Our neural LM is a Transformer decoder architecture in the transformer base configuration trained with Tensor2Tensor (Vaswani et al., 2018). Our NMT model is a Transformer model (transformer base) trained on the concatenation of the NUCLE corpus (Dahlmeier et al., 2013) and the Lang-8 Corpus of Learner English v1.0 (Mizumoto et al., 2012). We only keep sentences with at least one correction (659K sentences in total). Both NMT and NLM models use byte pair encoding (Sennrich et al., 2016, BPE) with 32K merge operations. We delay SGD updates by 2 on four physical GPUs as suggested by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning. 4 https://github.com/grammatical/ baselines-emnlp2016 Saunders et al. (2018). We decode with beam size 12 using the S"
N19-1406,P18-1097,0,0.146099,"ybrid systems. 1 • We present an FST-based adaptation of the work of Bryant and Briscoe (2018) which allows exact inference, and does not require annotated training data. We report large gains from rescoring with a neural language model. Introduction Grammatical error correction (GEC) is the task of automatically correcting all types of errors in text; e.g. [In a such situaction → In such a situation]. Using neural models for GEC is becoming increasingly popular (Xie et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Ge et al., 2018a,b), possibly combined with phrase-based SMT (Chollampatt et al., 2016; Chollampatt and Ng, 2017; Grundkiewicz and Junczys-Dowmunt, 2018). A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence. GEC is – compared to machine translation – a highly constrained prob• Our technique beats the best published result with comparable amounts of training data on the CoNLL-2014 (Ng et al., 2014) test set when applied to SMT lattices. Our combination strategy yields larger gains over the SMT baselines than simpler rescorin"
N19-1406,N18-2046,0,0.352897,"inference, and does not require annotated training data. We report large gains from rescoring with a neural language model. Introduction Grammatical error correction (GEC) is the task of automatically correcting all types of errors in text; e.g. [In a such situaction → In such a situation]. Using neural models for GEC is becoming increasingly popular (Xie et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Ge et al., 2018a,b), possibly combined with phrase-based SMT (Chollampatt et al., 2016; Chollampatt and Ng, 2017; Grundkiewicz and Junczys-Dowmunt, 2018). A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence. GEC is – compared to machine translation – a highly constrained prob• Our technique beats the best published result with comparable amounts of training data on the CoNLL-2014 (Ng et al., 2014) test set when applied to SMT lattices. Our combination strategy yields larger gains over the SMT baselines than simpler rescoring or pipelining used in prior work on hybrid systems (Grundkiewicz and Junczys-Dowmunt, 2018). 2 Constructing the Hypothesis Space Constru"
N19-1406,W11-2123,0,0.0573916,"orr> and &lt;corr> tokens with two further parameters, λmcorr and λcorr , by composing B with the penalization transducer P shown in Fig. 3.2 The λmcorr and λcorr parameters control the trade-off between the number and quality of the proposed corrections since high values bias towards fewer corrections. To incorporate word-level language model scores we train a 5-gram count-based LM with 2 Rather than using &lt;mcorr> and &lt;corr> tokens and the transducer P we could directly incorporate the costs in the transducers I and E, respectively. We chose to use explicit correction tokens for clarity. KenLM (Heafield, 2011) on the One Billion Word Benchmark dataset (Chelba et al., 2014), and convert it to an FST L using the OpenGrm NGram Library (Roark et al., 2012). For tuning purposes we scale weights in L with λKenLM : [[L]](y) = −λKenLM log PKenLM (y). (4) Our combined word-level scores can be expressed with the following transducer: Hword = B ◦ P ◦ L. (5) Since we operate in the tropical semiring, path scores in Hword are linear combinations of correction penalties, LM scores, and, if applicable, SMT scores, weighted with the λ-parameters. Note that exact inference in Hword is possible using FST shortest pa"
N19-1406,P17-1070,0,0.066502,"set, and achieves far better relative improvements over the SMT baselines than previous hybrid systems. 1 • We present an FST-based adaptation of the work of Bryant and Briscoe (2018) which allows exact inference, and does not require annotated training data. We report large gains from rescoring with a neural language model. Introduction Grammatical error correction (GEC) is the task of automatically correcting all types of errors in text; e.g. [In a such situaction → In such a situation]. Using neural models for GEC is becoming increasingly popular (Xie et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Ge et al., 2018a,b), possibly combined with phrase-based SMT (Chollampatt et al., 2016; Chollampatt and Ng, 2017; Grundkiewicz and Junczys-Dowmunt, 2018). A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence. GEC is – compared to machine translation – a highly constrained prob• Our technique beats the best published result with comparable amounts of training data on the CoNLL-2014 (Ng et al., 2014) test set when applied to SMT lattices."
N19-1406,D16-1161,0,0.105548,"tems are tuned with respect to the metric highlighted in gray. Input lattices I are derived from the Moses 1000-best list as in Fig. 1(c). Row 3 is the SMT baseline. We have introduced three λ-parameters λcorr , λKenLM , and λNLM , and three additional parameters λSMT , λmcorr , and λNMT if we make use of annotated training data. We also use a word insertion penalty λwc for our SMT-based experiments. We tune all these parameters on the development sets using Powell search (Powell, 1964).3 3 Experiments Experimental setup In our experiments with annotated training data we use the SMT system of Junczys-Dowmunt and Grundkiewicz (2016)4 to create 1000-best lists from which we derive the input lattices I. All our LMs are trained on the One Billion Word Benchmark dataset (Chelba et al., 2014). Our neural LM is a Transformer decoder architecture in the transformer base configuration trained with Tensor2Tensor (Vaswani et al., 2018). Our NMT model is a Transformer model (transformer base) trained on the concatenation of the NUCLE corpus (Dahlmeier et al., 2013) and the Lang-8 Corpus of Learner English v1.0 (Mizumoto et al., 2012). We only keep sentences with at least one correction (659K sentences in total). Both NMT and NLM mo"
N19-1406,C12-2084,0,0.275075,"Missing"
N19-1406,J97-2003,0,0.0485351,"selected correction options greedily. Our ultimate goal, however, is to rescore Hword with neural models such as an NLM and – if annotated training data is available – an NMT model. Since our neural models use subword units (Sennrich et al., 2016, BPEs), we compose Hword with a transducer T which maps word sequences to BPE sequences. Our final transducer HBPE which we use to constrain the neural beam decoder can be written as: HBPE = Πoutput (Hword ◦ T ) = Πoutput (I ◦ E ◦ P ◦ L ◦ T ). (6) To help downstream beam decoding we apply -removal, determinization, minimization, and weight pushing (Mohri, 1997; Mohri and Riley, 2001) to HBPE . We search for the best hypothe∗ sis yBPE with beam search using a combined score of word-level symbolic models (represented by HBPE ) and subword unit based neural models:  ∗ yBPE = arg max − [[HBPE ]](yBPE ) yBPE + λNLM log PNLM (yBPE )  + λNMT log PNMT (yBPE |xBPE ) (7) The final decoding pass can be seen as an ensemble of a neural LM and an NMT model which is constrained and scored at each time step by the set of possible tokens in HBPE . 4035 1 2 3 4 Uses 5-gram NLM E FST-LM (BPE) Best published (B&B, 2018) X X X X X X X X P 40.56 40.62 54.43 53.64 CoNL"
N19-1406,P15-2097,0,0.111226,"E) with 32K merge operations. We delay SGD updates by 2 on four physical GPUs as suggested by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning. 4 https://github.com/grammatical/ baselines-emnlp2016 Saunders et al. (2018). We decode with beam size 12 using the SGNMT decoder (Stahlberg et al., 2017). We evaluate on CoNLL-2014 (Ng et al., 2014) and JFLEG-Test (Napoles et al., 2017), using CoNLL-2013 (Ng et al., 2013) and JFLEGDev as development sets. Our evaluation metrics are GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). We generated M2 files using ERRANT (Bryant et al., 2017) for JFLEG and Tab. 1 to be comparable to Bryant and Briscoe (2018), but used the official M2 files in Tab. 2 to be comparable to Grundkiewicz and JunczysDowmunt (2018). Results Our LM-based GEC results without using annotated training data are summarized in Tab. 1. Even when we use the same resources (same LM and same confusion sets) as Bryant and Briscoe (2018), we see gains on JFLEG (rows 1 vs. 2), probably because we avoid search errors in our FST-based scheme. Adding an NLM yields significant gains a"
N19-1406,E17-2037,0,0.369251,"at least one correction (659K sentences in total). Both NMT and NLM models use byte pair encoding (Sennrich et al., 2016, BPE) with 32K merge operations. We delay SGD updates by 2 on four physical GPUs as suggested by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning. 4 https://github.com/grammatical/ baselines-emnlp2016 Saunders et al. (2018). We decode with beam size 12 using the SGNMT decoder (Stahlberg et al., 2017). We evaluate on CoNLL-2014 (Ng et al., 2014) and JFLEG-Test (Napoles et al., 2017), using CoNLL-2013 (Ng et al., 2013) and JFLEGDev as development sets. Our evaluation metrics are GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). We generated M2 files using ERRANT (Bryant et al., 2017) for JFLEG and Tab. 1 to be comparable to Bryant and Briscoe (2018), but used the official M2 files in Tab. 2 to be comparable to Grundkiewicz and JunczysDowmunt (2018). Results Our LM-based GEC results without using annotated training data are summarized in Tab. 1. Even when we use the same resources (same LM and same confusion sets) as Bryant and Briscoe (2018), we see gains on JF"
N19-1406,W14-1701,1,0.961939,"e et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Ge et al., 2018a,b), possibly combined with phrase-based SMT (Chollampatt et al., 2016; Chollampatt and Ng, 2017; Grundkiewicz and Junczys-Dowmunt, 2018). A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence. GEC is – compared to machine translation – a highly constrained prob• Our technique beats the best published result with comparable amounts of training data on the CoNLL-2014 (Ng et al., 2014) test set when applied to SMT lattices. Our combination strategy yields larger gains over the SMT baselines than simpler rescoring or pipelining used in prior work on hybrid systems (Grundkiewicz and Junczys-Dowmunt, 2018). 2 Constructing the Hypothesis Space Constructing the set of hypotheses The core idea of our approach is to first construct a 4033 Proceedings of NAACL-HLT 2019, pages 4033–4039 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (a) The input lattice I without SMT (no annotated training data). (b) The base lattice B without SMT. ("
N19-1406,D17-1298,0,0.130768,"provements over the SMT baselines than previous hybrid systems. 1 • We present an FST-based adaptation of the work of Bryant and Briscoe (2018) which allows exact inference, and does not require annotated training data. We report large gains from rescoring with a neural language model. Introduction Grammatical error correction (GEC) is the task of automatically correcting all types of errors in text; e.g. [In a such situaction → In such a situation]. Using neural models for GEC is becoming increasingly popular (Xie et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Ge et al., 2018a,b), possibly combined with phrase-based SMT (Chollampatt et al., 2016; Chollampatt and Ng, 2017; Grundkiewicz and Junczys-Dowmunt, 2018). A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence. GEC is – compared to machine translation – a highly constrained prob• Our technique beats the best published result with comparable amounts of training data on the CoNLL-2014 (Ng et al., 2014) test set when applied to SMT lattices. Our combination strategy yields larger gains o"
N19-1406,P16-1162,0,0.444544,"ng transducer: Hword = B ◦ P ◦ L. (5) Since we operate in the tropical semiring, path scores in Hword are linear combinations of correction penalties, LM scores, and, if applicable, SMT scores, weighted with the λ-parameters. Note that exact inference in Hword is possible using FST shortest path search. This is an improvement over the work of Bryant and Briscoe (2018) who selected correction options greedily. Our ultimate goal, however, is to rescore Hword with neural models such as an NLM and – if annotated training data is available – an NMT model. Since our neural models use subword units (Sennrich et al., 2016, BPEs), we compose Hword with a transducer T which maps word sequences to BPE sequences. Our final transducer HBPE which we use to constrain the neural beam decoder can be written as: HBPE = Πoutput (Hword ◦ T ) = Πoutput (I ◦ E ◦ P ◦ L ◦ T ). (6) To help downstream beam decoding we apply -removal, determinization, minimization, and weight pushing (Mohri, 1997; Mohri and Riley, 2001) to HBPE . We search for the best hypothe∗ sis yBPE with beam search using a combined score of word-level symbolic models (represented by HBPE ) and subword unit based neural models:  ∗ yBPE = arg max − [[HBPE ]"
N19-1406,D17-2005,1,0.851147,"8 Corpus of Learner English v1.0 (Mizumoto et al., 2012). We only keep sentences with at least one correction (659K sentences in total). Both NMT and NLM models use byte pair encoding (Sennrich et al., 2016, BPE) with 32K merge operations. We delay SGD updates by 2 on four physical GPUs as suggested by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning. 4 https://github.com/grammatical/ baselines-emnlp2016 Saunders et al. (2018). We decode with beam size 12 using the SGNMT decoder (Stahlberg et al., 2017). We evaluate on CoNLL-2014 (Ng et al., 2014) and JFLEG-Test (Napoles et al., 2017), using CoNLL-2013 (Ng et al., 2013) and JFLEGDev as development sets. Our evaluation metrics are GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). We generated M2 files using ERRANT (Bryant et al., 2017) for JFLEG and Tab. 1 to be comparable to Bryant and Briscoe (2018), but used the official M2 files in Tab. 2 to be comparable to Grundkiewicz and JunczysDowmunt (2018). Results Our LM-based GEC results without using annotated training data are summarized in Tab. 1. Even when we use the same resources"
N19-1406,W13-3601,0,0.150838,"in total). Both NMT and NLM models use byte pair encoding (Sennrich et al., 2016, BPE) with 32K merge operations. We delay SGD updates by 2 on four physical GPUs as suggested by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentences for tuning. 4 https://github.com/grammatical/ baselines-emnlp2016 Saunders et al. (2018). We decode with beam size 12 using the SGNMT decoder (Stahlberg et al., 2017). We evaluate on CoNLL-2014 (Ng et al., 2014) and JFLEG-Test (Napoles et al., 2017), using CoNLL-2013 (Ng et al., 2013) and JFLEGDev as development sets. Our evaluation metrics are GLEU (Napoles et al., 2015) and M2 (Dahlmeier and Ng, 2012). We generated M2 files using ERRANT (Bryant et al., 2017) for JFLEG and Tab. 1 to be comparable to Bryant and Briscoe (2018), but used the official M2 files in Tab. 2 to be comparable to Grundkiewicz and JunczysDowmunt (2018). Results Our LM-based GEC results without using annotated training data are summarized in Tab. 1. Even when we use the same resources (same LM and same confusion sets) as Bryant and Briscoe (2018), we see gains on JFLEG (rows 1 vs. 2), probably because"
N19-1406,W18-1819,0,0.030632,"training data. We also use a word insertion penalty λwc for our SMT-based experiments. We tune all these parameters on the development sets using Powell search (Powell, 1964).3 3 Experiments Experimental setup In our experiments with annotated training data we use the SMT system of Junczys-Dowmunt and Grundkiewicz (2016)4 to create 1000-best lists from which we derive the input lattices I. All our LMs are trained on the One Billion Word Benchmark dataset (Chelba et al., 2014). Our neural LM is a Transformer decoder architecture in the transformer base configuration trained with Tensor2Tensor (Vaswani et al., 2018). Our NMT model is a Transformer model (transformer base) trained on the concatenation of the NUCLE corpus (Dahlmeier et al., 2013) and the Lang-8 Corpus of Learner English v1.0 (Mizumoto et al., 2012). We only keep sentences with at least one correction (659K sentences in total). Both NMT and NLM models use byte pair encoding (Sennrich et al., 2016, BPE) with 32K merge operations. We delay SGD updates by 2 on four physical GPUs as suggested by 3 Similarly to Bryant and Briscoe (2018), even in our experiments without annotated training data, we do need a very small amount of annotated sentence"
N19-1406,P12-3011,0,0.0189707,"e λmcorr and λcorr parameters control the trade-off between the number and quality of the proposed corrections since high values bias towards fewer corrections. To incorporate word-level language model scores we train a 5-gram count-based LM with 2 Rather than using &lt;mcorr> and &lt;corr> tokens and the transducer P we could directly incorporate the costs in the transducers I and E, respectively. We chose to use explicit correction tokens for clarity. KenLM (Heafield, 2011) on the One Billion Word Benchmark dataset (Chelba et al., 2014), and convert it to an FST L using the OpenGrm NGram Library (Roark et al., 2012). For tuning purposes we scale weights in L with λKenLM : [[L]](y) = −λKenLM log PKenLM (y). (4) Our combined word-level scores can be expressed with the following transducer: Hword = B ◦ P ◦ L. (5) Since we operate in the tropical semiring, path scores in Hword are linear combinations of correction penalties, LM scores, and, if applicable, SMT scores, weighted with the λ-parameters. Note that exact inference in Hword is possible using FST shortest path search. This is an improvement over the work of Bryant and Briscoe (2018) who selected correction options greedily. Our ultimate goal, however"
N19-1406,N16-1042,0,0.0405848,"t on the CoNLL-2014 test set, and achieves far better relative improvements over the SMT baselines than previous hybrid systems. 1 • We present an FST-based adaptation of the work of Bryant and Briscoe (2018) which allows exact inference, and does not require annotated training data. We report large gains from rescoring with a neural language model. Introduction Grammatical error correction (GEC) is the task of automatically correcting all types of errors in text; e.g. [In a such situaction → In such a situation]. Using neural models for GEC is becoming increasingly popular (Xie et al., 2016; Yuan and Briscoe, 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018; Ge et al., 2018a,b), possibly combined with phrase-based SMT (Chollampatt et al., 2016; Chollampatt and Ng, 2017; Grundkiewicz and Junczys-Dowmunt, 2018). A potential challenge for purely neural GEC models is their vast output space since they assign non-zero probability mass to any sequence. GEC is – compared to machine translation – a highly constrained prob• Our technique beats the best published result with comparable amounts of training data on the CoNLL-2014 (Ng et al., 2014) test set when applied"
N19-1406,I17-2062,0,0.0594355,"Missing"
N19-1406,P18-2051,1,0.85042,"Missing"
P16-2049,D11-1127,0,0.0392114,"Missing"
P16-2049,P15-1001,0,0.0569091,"o a unique Hiero translation prefix. Suppose a path to a state accepts the translation prefix y1t−1 . An outgoing arc from that state with symbol y has a weight that corresponds to the (negative log of the) conditional probability 2 Syntactically Guided NMT (SGNMT) 2.1 NMT and Hiero also differ in their internal representations. The NMT continuous representation captures morphological, syntactic and semantic similarity (Collobert and Weston, 2008) across words and phrases. However, extending these representations to the large vocabularies needed for open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015). By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010). Moreover, words and phrases can be easily added to a fully-trained symbolic MT system. This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain. Adding new words and phrases to an NMT system is not as straightforward, and it is not clear that the advanta"
P16-2049,J14-3008,1,0.858953,"’ (Bahdanau et al., 2015). Search looks only one word ahead and no deeper than the beam. Hiero defines a synchronous context-free grammar (SCFG) with rules: X → hα, γi, where α and γ are strings of terminals and non-terminals in the source and target languages. A target language sentence y can be a translation of a source language sentence x if there is a derivation D in the grammar which yields both y and x: y = y(D), x = x(D). This defines a regular language Y over strings in the target language via a projection of the sentence to be translated: Y = {y(D) : x(D) = x} (Iglesias et al., 2011; Allauzen et al., 2014). Scores are defined over derivations via a log-linear model with features {φi } and weights λ. The decoder searches for the translation y(D) in Y with the highest derivation score S(D) (Chiang, 2007, Eq. 24) : We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full ngram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find"
P16-2049,W15-3014,0,0.0222999,"o a unique Hiero translation prefix. Suppose a path to a state accepts the translation prefix y1t−1 . An outgoing arc from that state with symbol y has a weight that corresponds to the (negative log of the) conditional probability 2 Syntactically Guided NMT (SGNMT) 2.1 NMT and Hiero also differ in their internal representations. The NMT continuous representation captures morphological, syntactic and semantic similarity (Collobert and Weston, 2008) across words and phrases. However, extending these representations to the large vocabularies needed for open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015). By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010). Moreover, words and phrases can be easily added to a fully-trained symbolic MT system. This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain. Adding new words and phrases to an NMT system is not as straightforward, and it is not clear that the advanta"
P16-2049,D13-1176,0,0.0859856,"r complete translation hypotheses, with the full translation grammar score and full ngram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies. 1 Introduction We report on investigations motivated by the idea that the structured search spaces defined by syntactic machine translation approaches such as Hiero (Chiang, 2007) can be used to guide Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). NMT and Hiero have complementary strengths and weaknesses and differ markedly in how they define probability distributions over translations and what search procedures they use. The NMT encoder-decoder formalism provides a probability distribution over translations y = y1T of a source sentence x as (Bahdanau et al., 2015) P (y1T |x) = T Y t=1 P (yt |y1t−1 , x) = T Y     ˆ = y  argmax PG (D)PLM (y(D))λLM  (2) y | {z } D:x(D)=x S(D) g(yt−1 , st , ct ) t=1 (1) where st = f (st−1 , yt−1 , ct ) is a decoder state variable and"
P16-2049,N09-1025,0,0.0103414,"l., 2009). These procedures search over a vast space of translations, much larger than is considered by the NMT beam search. However the Hiero context-free grammars that make efficient search possible are weak models of translation. The basic Hiero formalism can be extended through ‘soft syntactic constraints’ (Venugopal et al., 2009; Marton and Resnik, 2008) or by 299 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 299–305, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics adding very high dimensional features (Chiang et al., 2009), however the translation score assigned by the grammar is still only the product of probabilities of individual rules. From the modelling perspective, this is an overly strong conditional independence assumption. NMT clearly has the potential advantage in incorporating long-term context into translation scores. NMT decoder. Our approach addresses the limited vocabulary issue in NMT as we replace NMT OOVs with lattice words from the much larger Hiero vocabulary. We also find good gains from neural and Kneser-Ney n-gram language models. NMT and Hiero differ in how they ‘consume’ source words. H"
P16-2049,P15-1002,0,0.176189,"nslation prefix. Suppose a path to a state accepts the translation prefix y1t−1 . An outgoing arc from that state with symbol y has a weight that corresponds to the (negative log of the) conditional probability 2 Syntactically Guided NMT (SGNMT) 2.1 NMT and Hiero also differ in their internal representations. The NMT continuous representation captures morphological, syntactic and semantic similarity (Collobert and Weston, 2008) across words and phrases. However, extending these representations to the large vocabularies needed for open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015). By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010). Moreover, words and phrases can be easily added to a fully-trained symbolic MT system. This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain. Adding new words and phrases to an NMT system is not as straightforward, and it is not clear that the advantages of the continuous"
P16-2049,N10-4001,0,0.0111107,"Guided NMT (SGNMT) 2.1 NMT and Hiero also differ in their internal representations. The NMT continuous representation captures morphological, syntactic and semantic similarity (Collobert and Weston, 2008) across words and phrases. However, extending these representations to the large vocabularies needed for open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015). By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010). Moreover, words and phrases can be easily added to a fully-trained symbolic MT system. This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain. Adding new words and phrases to an NMT system is not as straightforward, and it is not clear that the advantages of the continuous representation can be extended to the new additions to the vocabularies. Hiero Predictive Posteriors PHiero (yt = y|y1t−1 , x). (3) This conditional probability is such that for a Hiero translation y1T = y(D) accepted by the WF"
P16-2049,J07-2003,0,0.0456895,"nd non-terminals in the source and target languages. A target language sentence y can be a translation of a source language sentence x if there is a derivation D in the grammar which yields both y and x: y = y(D), x = x(D). This defines a regular language Y over strings in the target language via a projection of the sentence to be translated: Y = {y(D) : x(D) = x} (Iglesias et al., 2011; Allauzen et al., 2014). Scores are defined over derivations via a log-linear model with features {φi } and weights λ. The decoder searches for the translation y(D) in Y with the highest derivation score S(D) (Chiang, 2007, Eq. 24) : We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full ngram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies. 1 Introduction We report on investigations motivated by the idea"
P16-2049,D08-1076,0,0.00975529,"we find that our BASIC NMT system performs similarly (within 0.5 BLEU) to previously published results (16.31 vs. 16.46 and 30.42 vs. 29.97). In NMT-H IERO, decoding is as described in Sec. 2.2, but with λHiero = 0. The decoder searches through the Hiero lattice, ignoring the Hiero scores, but using Hiero word hypotheses in place of any UNKs that might have been produced by NMT. The results show that NMT-H IERO is much more effective in fixing NMT OOVs than the ‘UNK Replace’ technique (Luong et al., 2015); this holds in both En-De and En-Fr. For the NMT-H IERO +T UNING systems, lattice MERT (Macherey et al., 2008) is used to optimise λHiero and λN M T on the tuning sets. This yields further gains in both En-Fr and En-De, suggesting 1 2 http://ucam-smt.github.io/sgnmt/html/ https://pyfst.github.io/ 3 302 http://matrix.statmt.org/matrix/systems list/1774 Figure 2: SGNMT performance over lattice size on English-German news-test2015. 8,510 nodes per lattice corresponds to row 14 in Tab. 3. Figure 1: Performance with NPLM over beam size on English-German news-test2015. A beam of 12 corresponds to row 15 in Tab. 3. Determinisation X X X Minimisation Weight pushing X X X Local Softmax In SGNMT decoding we hav"
P16-2049,D15-1249,0,0.0367476,"accepts the translation prefix y1t−1 . An outgoing arc from that state with symbol y has a weight that corresponds to the (negative log of the) conditional probability 2 Syntactically Guided NMT (SGNMT) 2.1 NMT and Hiero also differ in their internal representations. The NMT continuous representation captures morphological, syntactic and semantic similarity (Collobert and Weston, 2008) across words and phrases. However, extending these representations to the large vocabularies needed for open-domain MT is an open area of research (Jean et al., 2015a; Luong et al., 2015; Sennrich et al., 2015; Chitnis and DeNero, 2015). By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies (Heafield et al., 2013; Lin and Dyer, 2010). Moreover, words and phrases can be easily added to a fully-trained symbolic MT system. This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain. Adding new words and phrases to an NMT system is not as straightforward, and it is not clear that the advantages of the continuous representation can be extended to the new additio"
P16-2049,P08-1114,0,0.0739457,"asily lend itself where PLM Q is an n-gram Q language model λand PG (D) ∝ (X→hγ,αi)∈D i φi (X → hγ, αi) i . Hiero decoders attempt to avoid search errors when combining the translation and language model for the translation hypotheses (Chiang, 2007; Iglesias et al., 2009). These procedures search over a vast space of translations, much larger than is considered by the NMT beam search. However the Hiero context-free grammars that make efficient search possible are weak models of translation. The basic Hiero formalism can be extended through ‘soft syntactic constraints’ (Venugopal et al., 2009; Marton and Resnik, 2008) or by 299 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 299–305, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics adding very high dimensional features (Chiang et al., 2009), however the translation score assigned by the grammar is still only the product of probabilities of individual rules. From the modelling perspective, this is an overly strong conditional independence assumption. NMT clearly has the potential advantage in incorporating long-term context into translation scores. NMT decoder. Our approach a"
P16-2049,J10-3008,0,0.172625,"Missing"
P16-2049,W15-5003,0,0.0257021,"provement of +0.8 BLEU for single NMT. If we allow Hiero to fix NMT UNKs, we see a further +2.7 BLEU gain (row 11). The majority of gains come from fixing UNKs, but there is still improvement from the constrained search space for single NMT. We next investigate the contribution of the Hiero system scores. We see that, once lattices are generated, the KN-LM contributes more to rescoring than the Hiero grammar scores (rows 1214). Further gains can be achieved by adding a feed-forward neural language model with NPLM (Vaswani et al., 2013) (row 15). We observe that n-best list rescoring with NMT (Neubig et al., 2015) also outperforms both the Hiero and NMT SGNMT Performance Tab. 2 compares our combined NMT+Hiero decoding with NMT results in the literature. We use a beam size of 12. In En-De and in En-Fr, we find that our BASIC NMT system performs similarly (within 0.5 BLEU) to previously published results (16.31 vs. 16.46 and 30.42 vs. 29.97). In NMT-H IERO, decoding is as described in Sec. 2.2, but with λHiero = 0. The decoder searches through the Hiero lattice, ignoring the Hiero scores, but using Hiero word hypotheses in place of any UNKs that might have been produced by NMT. The results show that NMT-"
P16-2049,P13-2121,0,0.0420481,"Missing"
P16-2049,N09-1049,0,0.0569737,"Missing"
P16-2049,P16-5005,0,0.0172126,"ses the limited vocabulary issue in NMT as we replace NMT OOVs with lattice words from the much larger Hiero vocabulary. We also find good gains from neural and Kneser-Ney n-gram language models. NMT and Hiero differ in how they ‘consume’ source words. Hiero applies the translation rules to the source sentence via the CYK algorithm, with each derivation yielding a complete and unambiguous translation of the source words. The NMT beam decoder does not have an explicit mechanism for tracking source coverage, and there is evidence that may lead to both ‘over-translation’ and ‘under-translation’ (Tu et al., 2016). The Hiero decoder generates translation hypotheses as weighted finite state acceptors (WFSAs), or lattices, with weights in the tropical semiring. For a translation hypothesis y(D) arising from the Hiero derivation D, the path weight in the WFSA is − log S(D), after Eq. 2. While this representation is correct with respect to the Hiero translation grammar and language model scores, having Hiero scores at the path level is not convenient for working with the NMT system. What we need are predictive probabilities in the form of Eq. 1. The Hiero WFSAs are determinised and minimised with epsilon r"
P16-2049,D13-1140,0,0.0148886,"constraining NMT to the search space defined by the Hiero lattices yields an improvement of +0.8 BLEU for single NMT. If we allow Hiero to fix NMT UNKs, we see a further +2.7 BLEU gain (row 11). The majority of gains come from fixing UNKs, but there is still improvement from the constrained search space for single NMT. We next investigate the contribution of the Hiero system scores. We see that, once lattices are generated, the KN-LM contributes more to rescoring than the Hiero grammar scores (rows 1214). Further gains can be achieved by adding a feed-forward neural language model with NPLM (Vaswani et al., 2013) (row 15). We observe that n-best list rescoring with NMT (Neubig et al., 2015) also outperforms both the Hiero and NMT SGNMT Performance Tab. 2 compares our combined NMT+Hiero decoding with NMT results in the literature. We use a beam size of 12. In En-De and in En-Fr, we find that our BASIC NMT system performs similarly (within 0.5 BLEU) to previously published results (16.31 vs. 16.46 and 30.42 vs. 29.97). In NMT-H IERO, decoding is as described in Sec. 2.2, but with λHiero = 0. The decoder searches through the Hiero lattice, ignoring the Hiero scores, but using Hiero word hypotheses in pla"
P16-2049,N09-1027,0,0.0277941,"l, however it does not easily lend itself where PLM Q is an n-gram Q language model λand PG (D) ∝ (X→hγ,αi)∈D i φi (X → hγ, αi) i . Hiero decoders attempt to avoid search errors when combining the translation and language model for the translation hypotheses (Chiang, 2007; Iglesias et al., 2009). These procedures search over a vast space of translations, much larger than is considered by the NMT beam search. However the Hiero context-free grammars that make efficient search possible are weak models of translation. The basic Hiero formalism can be extended through ‘soft syntactic constraints’ (Venugopal et al., 2009; Marton and Resnik, 2008) or by 299 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 299–305, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics adding very high dimensional features (Chiang et al., 2009), however the translation score assigned by the grammar is still only the product of probabilities of individual rules. From the modelling perspective, this is an overly strong conditional independence assumption. NMT clearly has the potential advantage in incorporating long-term context into translation scores. N"
P16-2049,2010.iwslt-keynotes.2,0,\N,Missing
P16-2049,P16-1162,0,\N,Missing
P18-2051,P17-2021,0,0.194483,"focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models producing different sentence representations are necessarily synchronized to enable this. We propose an approach to decoding ensembles of models generat"
P18-2051,N16-1024,0,0.0329883,"erform NMT with syntax annotation in the form of Combinatory Categorial Grammar (CCG) supertags. Aharoni and Goldberg (2017) translate from source BPE into target linearized parse trees, 319 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituenc"
P18-2051,P17-2012,0,0.26068,"containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models producing different sentence representations are necessarily synchronized to enable this. We propose an approach to decoding ensembles of models generating different representa"
P18-2051,C16-1133,0,0.0721567,"‡ SDL Research, Cambridge, UK Abstract We explore strategies for incorporating target syntax into Neural Machine Translation. We specifically focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models produc"
P18-2051,W17-4775,0,0.0250167,"mputational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model. We therefore propose a derivation-based representation which is much more compact than a linearized parse tree (examples in Table 1). Our linearized derivation representation"
P18-2051,P07-2045,0,0.00436258,"ed derivation Linearized derivation 28.8 Linearized tree Plain BPE 28.9 It is also possible to constrain decoding Plain BPE Linearized derivation 28.8 of linearized trees and derivations to wellLinearized derivation Plain BPE 29.4† † formed outputs. However, we found that this POS/BPE Plain BPE 29.3 Plain BPE POS/BPE 29.4† gives little improvement in BLEU over unconstrained decoding although it remains an interTable 5: Ja-En Transformer ensembles: † esting line of research. marks significant improvement on plain BPE baseline shown in Table 4 (p < 0.05 using 4 Conclusions bootstrap resampling (Koehn et al., 2007)). 3 indicates that large batch training is instrumental in this. We find that RNN-based syntax models can equal plain BPE models as in Aharoni and Goldberg (2017); Eriguchi et al. (2017) use syntax for a 1 BLEU improvement on this dataset, but over a much lower baseline. Our plain BPE Transformer outperforms all syntax models except POS/BPE. More compact syntax representations perform better, with POS/BPE outperforming linearized derivations, which outperform linearized trees. Ensembles of two identical models trained with different seeds only slightly improve over the single model (Table 5)."
P18-2051,W17-5706,0,0.014417,"the linear derivation model. It has been suggested that decaying the learning rate can have a similar effect to large batch training (Smith et al., 2017), but reducing the initial learning rate by a factor of 8 alone did not give the same improvements. Representation Plain BPE Linearized derivation Batches / update 1 1 8 1 1 8 Learning rate 0.025 0.2 0.2 0.025 0.2 0.2 Test BLEU 27.5 27.2 28.9 25.6 25.6 28.7 Table 3: Single Transformers trained to convergence on 1M WAT Ja-En, batch size 4096 Our plain BPE baseline (Table 4) outperforms the current best system on WAT Ja-En, an 8-model ensemble (Morishita et al., 2017). Our syntax models achieve similar results despite producing much longer sequences. Table 322 Architecture Representation Dev Test BLEU BLEU 28.4 By ensembling syntax and plain-text we hope to benefit from their complementary Seq2seq Best WAT17 result (8-model (Morishita et al., strengths. To highlight these, we examine hyensemble) 2017) potheses generated by the plain BPE and linPlain BPE 21.6 21.2 Seq2seq earized derivation models. We find that the Linearized derivation 21.9 21.2 Plain BPE 28.0 28.9 syntax model is often more grammatical, even Linearized tree 28.2 28.4 when the plain BPE mo"
P18-2051,W17-4707,0,0.0198566,"especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models producing different sentence representations are necessarily synchronized to enable this. We propose an approach to decoding ensembles of models generating different representations, focusing on models generating syntax. As part of our investigation we suggest strategies for practical NMT with very long target sequences. These long sequen"
P18-2051,W17-5708,0,0.0383692,"nal GPU memory. The Tensor2Tensor framework (Vaswani et al., 2018) defines batch size as the number of tokens per batch, so batches will contain fewer sequences if their average length increases. During NMT training, by default, the gradients used to update model parameters are calculated over individual batches. A possible consequence is that batches containing fewer sequences per update may have ‘noisier’ estimated gradients than batches with more sequences. Previous research has used very large batches to improve training convergence while requiring fewer model updates (Smith et al., 2017; Neishi et al., 2017). However, with such large batches the model size may exceed available GPU memory. Training on multiple GPUs is one way to increase the amount of data used to estimate gradients, but it requires significant resources. Our strategy avoids this problem by using delayed SGD updates. We accumulate gradients over a fixed number of batches before using the accumulated gradients to update the model1 . This lets us effectively use very large batch sizes without requiring multiple GPUs. 2.2 Ensembling Representations Table 1 shows several different representations of the same hypothesis. To formulate a"
P18-2051,W04-0308,0,0.0215238,"ized parse trees, 319 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model. We therefore propose a derivation-based representation which is much more compact than a"
P18-2051,N15-3009,0,0.0282745,"Missing"
P18-2051,P16-1162,0,0.31982,"Missing"
P18-2051,D17-2005,1,0.518257,"ulated updates every 8 batches. To compare target representations we train Transformer models with target representations (1), (2), (4) and (5) shown in Table 1, using delayed SGD updates every 8 batches. We decode with individual models and two-model ensembles, comparing results for single-representation and multi-representation ensembles. Each multirepresentation ensemble consists of the plain BPE model and one other individual model. All Transformer architectures are Tensor2Tensor’s base Transformer model (Vaswani et al., 2018) with a batch size of 4096. In all cases we decode using SGNMT (Stahlberg et al., 2017) with beam size 4, using the average of the final 20 checkpoints. For comparison with earlier target syntax work, we also train two RNN attention-based seq2seq models (Bahdanau et al., 2015) with normal SGD to produce plain BPE sequences and linearized derivations. For these models we use embedding size 400, a single BiLSTM layer of size 750, and batch size 80. We report all experiments for JapaneseEnglish, using the first 1M training sentences of the Japanese-English ASPEC data (Nakazawa et al., 2016). All models use plain BPE Japanese source sentences. English constituency trees are obtained"
P18-2051,W18-1819,0,0.25904,"ntation ((4) in Table 1) consists of the derivation’s right-hand side tokens with an end-of-rule marker, </R> , marking the last non-terminal in each rule. The original tree can be directly reproduced from the sequence, so that structure information is maintained. We map words to subwords as described in Section 3. 2.1 Delayed SGD Update Training for Long Sequences We suggest a training strategy for the Transformer model (Vaswani et al., 2017) which gives improved performance for long sequences, like syntax representations, without requiring additional GPU memory. The Tensor2Tensor framework (Vaswani et al., 2018) defines batch size as the number of tokens per batch, so batches will contain fewer sequences if their average length increases. During NMT training, by default, the gradients used to update model parameters are calculated over individual batches. A possible consequence is that batches containing fewer sequences per update may have ‘noisier’ estimated gradients than batches with more sequences. Previous research has used very large batches to improve training convergence while requiring fewer model updates (Smith et al., 2017; Neishi et al., 2017). However, with such large batches the model s"
P18-2051,P17-1065,0,0.0188723,"pertags. Aharoni and Goldberg (2017) translate from source BPE into target linearized parse trees, 319 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model. We therefor"
P19-1022,D17-1156,0,0.129197,"s t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan (2016) use uniform ensembles of general and noreg fine-tuned models. Figure 1: Adaptively adjusting ensemble model weights Wk,i (Eq. 6) during decoding with BI We propose a simpler approach based on the source language n-gram language models from Eq. 9. We assume that each Gt is also a language m"
P19-1022,L16-1470,0,0.0841703,"Missing"
P19-1022,W18-6319,0,0.024261,"training sentences for minimum three tokens and maximum 120 tokens, and remove sentence pairs with length ratios higher than 4.5:1 or lower than 1:4.5. Table 2 shows filtered training sentence counts. Each language pair uses a 32K-merge source-target BPE vocabulary trained on the general domain (Sennrich et al., 2016b). We implement in Tensor2Tensor (Vaswani et al., 2018) and use its base Transformer model (Vaswani et al., 2017) for all NMT models. At inference time we decode with beam size 4 in SGNMT (Stahlberg et al., 2017) and evaluate with case-sensitive detokenized BLEU using SacreBLEU (Post, 2018). For BI, we use 4-gram KENLM models (Heafield, 2011). λk,t can be interpreted as the probability that task t contains sentences x drawn from domain k as estimated over the Vt . Figure 1 demonstrates this adaptive decoding scheme when weighting a biomedical and a general (news) domain model to produce a biomedical sentence under BI. The model weights Wk,i are even until biomedical-specific vocabulary is produced, at which point the in-domain model dominates. Decoder Uniform IS Identity-BI BI BI+IS Related Work δk (t) Eq. 10 Eq. 10 Table 1: Setting task posterior p(t|x) and domain-task weight λ"
P19-1022,E17-2045,0,0.132289,"for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Mon"
P19-1022,P16-1009,0,0.17909,"age models to estimate p(t = k|x) in Eq. 8 for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain ada"
P19-1022,C16-1133,0,0.0161946,"d et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan (2016) use uniform ensembles of general and noreg fine-tuned models. Figure 1: Adaptively adjusting ensemble model weights Wk,i (Eq. 6) during decoding with BI We propose a simpler approach based on the source language n-gram language models from Eq. 9. We assume that each Gt is also a language model P for its corresponding domain k. With Gk,t = x∈Vt Gk (x), we take: 2 Gk,t k0 Gk0 ,t λk,t = P (10) 1.2.3 Summary We summarize our approaches to decoding in Table 1. Static Adaptive p(t|x) λk,t 1 T 1 T δk"
P19-1022,P16-1162,0,0.538171,"age models to estimate p(t = k|x) in Eq. 8 for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain ada"
P19-1022,W11-2123,0,0.0649947,"maximum 120 tokens, and remove sentence pairs with length ratios higher than 4.5:1 or lower than 1:4.5. Table 2 shows filtered training sentence counts. Each language pair uses a 32K-merge source-target BPE vocabulary trained on the general domain (Sennrich et al., 2016b). We implement in Tensor2Tensor (Vaswani et al., 2018) and use its base Transformer model (Vaswani et al., 2017) for all NMT models. At inference time we decode with beam size 4 in SGNMT (Stahlberg et al., 2017) and evaluate with case-sensitive detokenized BLEU using SacreBLEU (Post, 2018). For BI, we use 4-gram KENLM models (Heafield, 2011). λk,t can be interpreted as the probability that task t contains sentences x drawn from domain k as estimated over the Vt . Figure 1 demonstrates this adaptive decoding scheme when weighting a biomedical and a general (news) domain model to produce a biomedical sentence under BI. The model weights Wk,i are even until biomedical-specific vocabulary is produced, at which point the in-domain model dominates. Decoder Uniform IS Identity-BI BI BI+IS Related Work δk (t) Eq. 10 Eq. 10 Table 1: Setting task posterior p(t|x) and domain-task weight λk,t for T tasks under decoding schemes in this work."
P19-1022,D17-2005,1,0.859979,"al., 2016), and then sequentially to the APE 2017 IT task (Turchi et al., 2017). We filter training sentences for minimum three tokens and maximum 120 tokens, and remove sentence pairs with length ratios higher than 4.5:1 or lower than 1:4.5. Table 2 shows filtered training sentence counts. Each language pair uses a 32K-merge source-target BPE vocabulary trained on the general domain (Sennrich et al., 2016b). We implement in Tensor2Tensor (Vaswani et al., 2018) and use its base Transformer model (Vaswani et al., 2017) for all NMT models. At inference time we decode with beam size 4 in SGNMT (Stahlberg et al., 2017) and evaluate with case-sensitive detokenized BLEU using SacreBLEU (Post, 2018). For BI, we use 4-gram KENLM models (Heafield, 2011). λk,t can be interpreted as the probability that task t contains sentences x drawn from domain k as estimated over the Vt . Figure 1 demonstrates this adaptive decoding scheme when weighting a biomedical and a general (news) domain model to produce a biomedical sentence under BI. The model weights Wk,i are even until biomedical-specific vocabulary is produced, at which point the in-domain model dominates. Decoder Uniform IS Identity-BI BI BI+IS Related Work δk (t"
P19-1022,W18-2705,0,0.15007,"Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag"
P19-1022,N19-1209,0,0.257037,"tation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan (2016) use uniform ensembles of general and noreg fine-tuned models. Figure 1: Adaptively adjusting ensemble model weights Wk,i (Eq. 6) during decoding with BI We propose a simpler approach based on the source language n-gram language models from Eq. 9. We assume that each Gt is also a language model P for its corresponding domain k. With Gk,t = x∈Vt Gk (x), we take: 2 Gk,t k0 Gk0 ,t λk,t = P (10) 1."
P19-1022,W17-3204,0,0.0269885,"ing a large generaldomain corpus (Luong and Manning, 2015). The ∗ are fine-tuned on task optimized parameters θA B, a new domain. Without regularization, catastrophic forgetting can occur: performance on task A degrades as parameters adjust to the new objective. A regularized objective is: X ∗ L(θ) = LB (θ) + Λ Fj (θj − θA,j )2 (1) Introduction Neural Machine Translation (NMT) models are effective when trained on broad domains with large datasets, such as news translation (Bojar et al., 2017). However, test data may be drawn from a different domain, on which general models can perform poorly (Koehn and Knowles, 2017). We address the problem of adapting to one or more domains while maintaining good performance across all domains. Crucially, we assume the realistic scenario where the domain is unknown at inference time. One solution is ensembling models trained on different domains (Freitag and Al-Onaizan, 2016). This approach has two main drawbacks. Firstly, obtaining models for each domain is challenging. Training from scratch on each new domain is impractical, while continuing training on a new domain can cause catastrophic forgetting of previous tasks (French, 1999), even in an ensemble (Freitag and Al-"
P19-1022,N18-2080,0,0.0223401,"t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan"
P19-1022,P17-2089,0,0.0311959,"(t = k|x) in Eq. 8 for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During infe"
W15-0116,J14-3008,1,0.901539,"Missing"
W15-0116,P06-1130,0,0.0456936,"Missing"
W15-0116,I05-1015,0,0.0296606,"imal Recursion Semantics representations. The approach treats realization as a translation problem, transforming the Dependency MRS graph representation to a surface string. Translation is based on a Synchronous Context-Free Grammar that is automatically extracted from a large corpus of parsed sentences. We have evaluated the new approach on the Wikiwoods corpus, where it shows promising results.1 1 Introduction Realization from Minimal Recursion Semantics (MRS) representations has traditionally used a chartbased approach governed by a resource grammar. Introduced by Carroll et al. (1999) and Carroll and Oepen (2005), the chart-based approach is lexically-driven and is able to produce a large number of candidate surface strings which may be ranked using an N-gram language model or using discriminative machine learning (Velldal and Oepen, 2005; Velldal, 2009). As the chart-based realization relies on a resource grammar, it tends to perform well when realizing from MRS representations that were created by a parser using the same resource grammar. However, the chart-based approach may fail to produce any output when the MRS representation has missing or incorrect parts. This is a significant issue for the MR"
W15-0116,P05-1033,0,0.490564,"mantic transfer translation systems such as LOGON (Lø nning et al., 2004) due to the difficulty of the translation problem. Consequently, the realization component is unable to produce any output and, in turn, translation fails. In this paper we describe a first attempt at statistical realization from MRS representations. The approach treats realization as a translation problem, transforming the Dependency MRS graph representation to a surface string. The approach draws inspiration from Statistical Machine Translation, namely the hierarchical phrase-based approach to translation introduced by Chiang (2005, 2007). We will refer to the new approach as Hierarchical Statistical Semantic Realization or HSSR. As part of the HSSR system, we present an approach for the automatic extraction of salient hierarchical rules for realization. The approach creates rules by considering DMRS subgraphs and corresponding surface substrings. The rules are created in two stages, first creating terminal rules, followed by nonterminal rules. The latter are created by ‘subtracting’ terminal rules from each other. The realization rules are extracted from a large parsed corpus to form a Synchronous Context-Free Grammar"
W15-0116,J07-2003,0,0.0502277,") introducing glue rules which would combine realized parts of subgraphs despite missing connecting predicates; (3) increasing the size of the grammar in hope that such rules would be extracted from examples. We find the second preferable to others as it also enables realization in other instances of missing rules (not to mention that there are instances of predicates with four arguments). However, introduction of glue rules is nontrivial as it can create spurious ambiguity in the decoder - a situation where many distinct derivations with the same model features and realizations are produced (Chiang, 2007). An increase in spurious ambiguity would affect the decoder efficiency and cause problems for advanced tuning procedures depending on n-best lists, such as MERT. The fifth and final example realization demonstrates the variability of output that the realization system is able to produce. This highlights the deficiencies of using N-gram precision measures such as BLEU for evaluating realization (and translation) output. 5 Related Work In this paper we described a first attempt at statistical realization from MRS representations using synchronous context-free grammar. In this section we discuss"
W15-0116,W07-1210,1,0.808066,"f the trees corresponding to the different scopes, maintaining the constraints between the elements via qeq constraints (=q , equality modulo quantifier) between hole arguments and labels. Intuitively, a qeq constraint, h =q l, enables one or more quantifiers to float between the label l and handle h but we will not explain the details here. Replacing the equalities with qeq constraints in the example above underspecifies scope, giving the MRS shown in (2). Robust Minimal Recursion Semantics (RMRS) is a modified MRS representation that also allows underspecification of relational information (Copestake, 2007). The transformation process between MRS and RMRS splits off most of arguments of elementary predicates and refers to them using anchors (a). e.g., in (4), l8: _accept(e2, _, x) becomes l8:a4:_accept(e2), ARG2(a4, x). Dependency MRS (DMRS) (Copestake, 2009) is an alternative representation interconvertible with MRS or RMRS. It has minimal redundancy in its structure and was developed for the purpose of readability and ease of use for both humans and computational applications. A DMRS is a directed graph with elementary predicates as nodes. It is constructed from a RMRS representation by combin"
W15-0116,E09-1001,1,0.889159,"rs to float between the label l and handle h but we will not explain the details here. Replacing the equalities with qeq constraints in the example above underspecifies scope, giving the MRS shown in (2). Robust Minimal Recursion Semantics (RMRS) is a modified MRS representation that also allows underspecification of relational information (Copestake, 2007). The transformation process between MRS and RMRS splits off most of arguments of elementary predicates and refers to them using anchors (a). e.g., in (4), l8: _accept(e2, _, x) becomes l8:a4:_accept(e2), ARG2(a4, x). Dependency MRS (DMRS) (Copestake, 2009) is an alternative representation interconvertible with MRS or RMRS. It has minimal redundancy in its structure and was developed for the purpose of readability and ease of use for both humans and computational applications. A DMRS is a directed graph with elementary predicates as nodes. It is constructed from a RMRS representation by combining 3 subgraphs: (1) Label equality graph, connecting EPs with shared labels; (2) Handle-to-label qeq graph, connecting handles and labels; (3) Variable graph, connecting EPs with their arguments. Upon merging the three subgraphs, the redundant links are de"
W15-0116,1995.tmi-1.2,1,0.574164,"ted its performance on the Wikiwoods corpus (Flickinger et al., 2010), a large deep parsed corpus of English Wikipedia which provides a large collection of MRS representations aligned with surface realizations that are suitable for learning the realization grammar. We measure the performance of the HSSR system using BLEU and discuss its strengths and weakness using example output. The system shows promising results, with the main issues stemming from the lack of efficiency. 2 Minimal Recursion Semantics Minimal Recursion Semantics (MRS) is a framework for computational semantics introduced by Copestake et al. (1995) and formally described in Copestake et al. (2005). As discussed there, MRS is a metalanguage for describing semantic structures in some underlying object language: the object language usually discussed is predicate calculus with generalized quantifiers. MRS was designed to be a tractable representation for large-scale parsing and generation, while not sacrificing expressiveness. It provides flat semantic representations that enable underspecification and can be integrated with grammatical representation in a number of frameworks. While MRS has been used in a wide variety of grammars, we conce"
W15-0116,E14-1028,1,0.896541,"Missing"
W15-0116,D10-1055,0,0.0255589,"shortest path through the WFSA, N-shortest paths can be extracted to produce an N-best list of realizations. An N-best list of realizations can be re-ranked using various strategies to improve the performance of the realizer. The strategies include re-ranking with a stronger language model and reranking using discriminative machine learning with a larger set of features. 4 Evaluation We evaluated the HSSR approach by realizing a set of parsed MRS representations and comparing the realized surface strings against the original sentences. We use the BLEU metric for comparison of surface strings. Espinosa et al. (2010) have investigated the use of various automatic evaluation metrics to measure the quality of realization output. They have found that several standard Statistical Machine Translation evaluation metrics, including BLEU, correlate moderately well with human judgment of adequacy and fluency for the string realization task. The authors conclude that these metrics are useful for measuring incremental progress of a realization system, but advise caution when comparing different realization systems. 4.1 Experimental setup We trained and evaluated the HSSR system on a subset of the Wikiwoods corpus. T"
W15-0116,flickinger-etal-2010-wikiwoods,0,0.369996,"Missing"
W15-0116,W11-2123,0,0.0177643,"age, we performed general predicate node filtering from DMRS graphs to remove nodes that would introduce unnecessary complexity in rule extraction and decoding. On the other hand, we augmented the DMRS representations with explicit punctuation nodes and links, as the graphs otherwise do not contain information regarding punctuation. We evaluated the system using 2-gram, 3-gram, and 4-gram language models. We estimated the language models on the entire Wikiwoods corpus, consisting of 800 million words (excluding tuning and training sets). The language models were estimated using KenLM toolkit (Heafield, 2011) with interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). Rule extraction on the training set of 1 million DMRS graph-surface string pairs produced 7.3 million realization rules. Practical limitations mentioned above apply: we extracted rules with at most 2 nonterminals, and the size of source side is at most five nodes. We tuned the log-linear model weights using simple grid search over several iterations against the BLEU evaluation metric (Papineni et al., 2002). A mature system could instead be optimized using standard tuning approaches from SMT, for example MERT (Och, 2003"
W15-0116,E14-3010,1,0.833728,"using N-gram precision measures such as BLEU for evaluating realization (and translation) output. 5 Related Work In this paper we described a first attempt at statistical realization from MRS representations using synchronous context-free grammar. In this section we discuss similar approaches to realization. One way of categorizing realization systems is according to the type of input assumed. Some authors have worked on the problem of word ordering, where the input is a bag of words, possibly combined with partial ordering information (e.g., Zhang and Clark (2011), de Gispert et al. (2014), Horvat and Byrne (2014)). Other systems take as input some form of meaning representation designed for a limited domain: such systems may be tested on the GEOQUERY corpus, for instance. Of particular relevance to us is Wong and Mooney (2007) which investigates SMT based techniques: they experiment with a phrase-based SMT method, a system that inverts an SMT-based semantic parser and a third approach which is a hybrid of these. Other systems take as input a structured representation which is intended to be flexible enough to cover general language: most such systems are associated with bidirectional approaches, and t"
W15-0116,N09-1049,0,0.0603197,"Missing"
W15-0116,D08-1076,0,0.0263776,"lated modified Kneser-Ney smoothing (Chen and Goodman, 1998). Rule extraction on the training set of 1 million DMRS graph-surface string pairs produced 7.3 million realization rules. Practical limitations mentioned above apply: we extracted rules with at most 2 nonterminals, and the size of source side is at most five nodes. We tuned the log-linear model weights using simple grid search over several iterations against the BLEU evaluation metric (Papineni et al., 2002). A mature system could instead be optimized using standard tuning approaches from SMT, for example MERT (Och, 2003) and LMERT (Macherey et al., 2008; Waite et al., 2011). The decoding times of the current system implementation can be relatively long. We enforced reasonable computation time by terminating decoding of a DMRS graph after 300 seconds. This occurred for 96/1000 examples in the test set. As BLEU significantly penalizes short or omitted output using the brevity penalty, we computed the BLEU scores only on decoded examples. The final evaluation example set is the same between all systems in order to keep the scores comparable between them. 4.2 Results and discussion We obtain the following results on decoded DMRS graphs of the te"
W15-0116,P03-1021,0,0.0106901,"ld, 2011) with interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). Rule extraction on the training set of 1 million DMRS graph-surface string pairs produced 7.3 million realization rules. Practical limitations mentioned above apply: we extracted rules with at most 2 nonterminals, and the size of source side is at most five nodes. We tuned the log-linear model weights using simple grid search over several iterations against the BLEU evaluation metric (Papineni et al., 2002). A mature system could instead be optimized using standard tuning approaches from SMT, for example MERT (Och, 2003) and LMERT (Macherey et al., 2008; Waite et al., 2011). The decoding times of the current system implementation can be relatively long. We enforced reasonable computation time by terminating decoding of a DMRS graph after 300 seconds. This occurred for 96/1000 examples in the test set. As BLEU significantly penalizes short or omitted output using the brevity penalty, we computed the BLEU scores only on decoded examples. The final evaluation example set is the same between all systems in order to keep the scores comparable between them. 4.2 Results and discussion We obtain the following results"
W15-0116,P02-1040,0,0.0925743,"ar model over derivations D: P (D) ∝ Y θi (D)λi (2) i where θi are features defined over rules used in derivation D and λi are feature weights. We define four features to aid realization: bidirectional conditional translation probabilities P (source|target) and P (target|source), N-gram language model probability, and word insertion penalty. The bidirectional probability features are trained by performing rule extraction and using rule frequency counts to estimate the probabilities. The feature weights λi of the log-linear model are tuned using grid search over the parameter space using BLEU (Papineni et al., 2002) as the measure of performance. 3.4 Decoder The task of the decoder is to generate a string realization for a (previously unseen) MRS representation. The decoder uses a grammar estimated on a training corpus as the source of translation rules. We base the HSSR decoder on the ideas behind the HiFST hierarchical phrase-based translation system, presented in Iglesias et al. (2009). Following the description in Allauzen et al. (2014), our decoder operates in three stages: 1. Realization: The decoder constructs a Weighted Finite State Acceptor (WFSA) encoding all possible realizations under a given"
W15-0116,2005.mtsummit-papers.15,0,0.0124032,"r that is automatically extracted from a large corpus of parsed sentences. We have evaluated the new approach on the Wikiwoods corpus, where it shows promising results.1 1 Introduction Realization from Minimal Recursion Semantics (MRS) representations has traditionally used a chartbased approach governed by a resource grammar. Introduced by Carroll et al. (1999) and Carroll and Oepen (2005), the chart-based approach is lexically-driven and is able to produce a large number of candidate surface strings which may be ranked using an N-gram language model or using discriminative machine learning (Velldal and Oepen, 2005; Velldal, 2009). As the chart-based realization relies on a resource grammar, it tends to perform well when realizing from MRS representations that were created by a parser using the same resource grammar. However, the chart-based approach may fail to produce any output when the MRS representation has missing or incorrect parts. This is a significant issue for the MRS representations produced as a result of semantic transfer in semantic transfer translation systems such as LOGON (Lø nning et al., 2004) due to the difficulty of the translation problem. Consequently, the realization component i"
W15-0116,W11-2827,0,0.0187123,"is quite unlike a conventional 115 logic. The earlier work on MRS (i.e., Carroll et al. (1999) and subsequent papers listed in the introduction) used the flatness of its structure to facilitate the use of a chart generation approach, while in our work on generation from DMRS, the input is a graph. In terms of methodology, our work is perhaps closest to (Cahill and van Genabith, 2006) whose PCFG system for robust probabilistic generation is based on approximations to a LFG automatically extracted from a treebank. However, the nature of the input and the techniques employed are very different. White (2011) investigates an approach which has some similarities with ours, using MT-style glue rules for robustness in conjunction with a realizer based on CCG, but his approach is directed at patching up failed realizations. 6 Conclusions and Future Work In this paper, we presented a first attempt at statistical realization from MRS representations. The approach treats realization as a translation problem, transforming the Dependency MRS graph representation to a surface string. The HSSR approach draws inspiration from Statistical Machine Translation. We evaluated the performance of the new approach on"
W15-0116,N07-1022,0,0.0280931,"hronous context-free grammar. In this section we discuss similar approaches to realization. One way of categorizing realization systems is according to the type of input assumed. Some authors have worked on the problem of word ordering, where the input is a bag of words, possibly combined with partial ordering information (e.g., Zhang and Clark (2011), de Gispert et al. (2014), Horvat and Byrne (2014)). Other systems take as input some form of meaning representation designed for a limited domain: such systems may be tested on the GEOQUERY corpus, for instance. Of particular relevance to us is Wong and Mooney (2007) which investigates SMT based techniques: they experiment with a phrase-based SMT method, a system that inverts an SMT-based semantic parser and a third approach which is a hybrid of these. Other systems take as input a structured representation which is intended to be flexible enough to cover general language: most such systems are associated with bidirectional approaches, and they have generally been tested with representations produced by parsers or with trees from manually-annotated treebanks. For instance, a number of systems have been built that take LFG fstructures as input (Cahill and"
W15-0116,D11-1106,0,0.0145421,"le to produce. This highlights the deficiencies of using N-gram precision measures such as BLEU for evaluating realization (and translation) output. 5 Related Work In this paper we described a first attempt at statistical realization from MRS representations using synchronous context-free grammar. In this section we discuss similar approaches to realization. One way of categorizing realization systems is according to the type of input assumed. Some authors have worked on the problem of word ordering, where the input is a bag of words, possibly combined with partial ordering information (e.g., Zhang and Clark (2011), de Gispert et al. (2014), Horvat and Byrne (2014)). Other systems take as input some form of meaning representation designed for a limited domain: such systems may be tested on the GEOQUERY corpus, for instance. Of particular relevance to us is Wong and Mooney (2007) which investigates SMT based techniques: they experiment with a phrase-based SMT method, a system that inverts an SMT-based semantic parser and a third approach which is a hybrid of these. Other systems take as input a structured representation which is intended to be flexible enough to cover general language: most such systems"
W15-0116,W12-6219,0,\N,Missing
W16-2324,P13-2121,0,0.0349981,"Missing"
W16-2324,D15-1273,1,0.890855,"Missing"
W16-2324,D13-1176,0,0.0584509,"bination schemes. Starting out with a simple neural lattice rescoring approach, we show that the Hiero lattices are often too narrow for NMT ensembles. Therefore, instead of a hard restriction of the NMT search space to the lattice, we propose to loosely couple NMT and Hiero by composition with a modified version of the edit distance transducer. The loose combination outperforms lattice rescoring, especially when using multiple NMT systems in an ensemble. 1 Introduction Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al., 2015; Stahlberg et al., 2016). Recent attempts to combine syntactic SMT and NMT report large gains over both baselines. Authors in (Neubig et al., 2015) used NMT to rescore n-best lists which were generated with a syntax-based system. They report that even with 1000-best lists, the gains of using the NMT rescorer often do not saturate. Syntactically Guided NMT (Stahlberg et al., 2016, SGNMT) constrains the NMT search space to Hiero translation la"
W16-2324,N04-1022,0,0.178921,"bridge, UK Abstract beam decoder with a relatively small beam can explore spaces much larger than n-best lists, yielding BLEU score improvements with far fewer expensive NMT evaluations. However, these rescoring approaches enforce an exact match between the NMT and syntactic decoders. In general, this kind of hard restriction is best avoided when combining diverse systems (Liu et al., 2009; Frederking et al., 1994). For example, in speech recognition, ROVER (Fiscus, 1997) is a system combination approach based on a soft voting scheme. In machine translation, minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) can be used to combine multiple systems (de Gispert et al., 2009). MBR also does not enforce exact agreement between systems as it distinguishes between the hypothesis space and the evidence space (Goel and Byrne, 2000; Tromble et al., 2008). We find that Hiero lattices generated by grammars extracted with the usual heuristics (Chiang, 2007) do not provide enough variety to explore the full potential of neural models, especially when using NMT ensembles. Therefore, we present a “soft” lattice-based combination scheme which uses standard operations on finite state transducers such as compositi"
W16-2324,J07-2003,0,0.280309,"ining diverse systems (Liu et al., 2009; Frederking et al., 1994). For example, in speech recognition, ROVER (Fiscus, 1997) is a system combination approach based on a soft voting scheme. In machine translation, minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) can be used to combine multiple systems (de Gispert et al., 2009). MBR also does not enforce exact agreement between systems as it distinguishes between the hypothesis space and the evidence space (Goel and Byrne, 2000; Tromble et al., 2008). We find that Hiero lattices generated by grammars extracted with the usual heuristics (Chiang, 2007) do not provide enough variety to explore the full potential of neural models, especially when using NMT ensembles. Therefore, we present a “soft” lattice-based combination scheme which uses standard operations on finite state transducers such as composition. Our method replaces the hard combination in previous methods with a similarity measure based on the edit distance, and gives the NMT decoder more freedom to diverge from the Hiero translations. We find that this loose coupling scheme is especially useful when using NMT ensembles. This paper presents the University of Cambridge submission"
W16-2324,D15-1249,0,0.0549874,"ncreasing the vocabulary size from 50k to 60k. Figure 5: BLEU score over the number of systems in the ensemble on news-test2016. 5 Conclusion and Future Work We have presented a method based on the edit distance that is effective in combining Hiero SMT systems with NMT ensembles. Our approach makes use of standard WFST operations, and we showed the effectiveness of the approach with a successful WMT’16 submission for EnglishGerman. In the future, we are planning to add back-translation (Sennrich et al., 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework. One important practical issue for system building is the number of systems to be ensembled as training each individual NMT system takes a significant amount of time. Fig. 5 indicates that even for 8-ensembles the gains for pure NMT do not seem to saturate. The combination with Hiero via edit distance transducer also greatly benefits from using ensembles, but most of the gains are gotten with fewer systems. Acknowledgements This work was supported by the U.K. Engineering and Physical Sciences Res"
W16-2324,P09-1065,0,0.0681278,"Missing"
W16-2324,P16-1100,0,0.0618143,"Missing"
W16-2324,P16-1160,0,0.031643,". Figure 5: BLEU score over the number of systems in the ensemble on news-test2016. 5 Conclusion and Future Work We have presented a method based on the edit distance that is effective in combining Hiero SMT systems with NMT ensembles. Our approach makes use of standard WFST operations, and we showed the effectiveness of the approach with a successful WMT’16 submission for EnglishGerman. In the future, we are planning to add back-translation (Sennrich et al., 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework. One important practical issue for system building is the number of systems to be ensembled as training each individual NMT system takes a significant amount of time. Fig. 5 indicates that even for 8-ensembles the gains for pure NMT do not seem to saturate. The combination with Hiero via edit distance transducer also greatly benefits from using ensembles, but most of the gains are gotten with fewer systems. Acknowledgements This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1"
W16-2324,D08-1076,0,0.0943159,"H. Weights in N and H are scaled by λnmt and λhiero , respectively. The similarity measure between NMT and Hiero translations is parametrized with λins , λedit , and λsub . We keep the various costs separated by using transducers with tropical sparse tuple vector semirings (Iglesias et al., 2015). Instead of single real-valued arc weights, this semiring uses vectors which can hold multiple features. The inner product of these vectors with a constant parameter vector determines the final weights on the arcs1 . The sparse tuple vector semiring enables us to optimize the λparameters with LMERT (Macherey et al., 2008) on a development set. We will make extensive use of this operation as tool for building complex automata which make use of both the NMT and Hiero translation lattices. 2.2 Loose Coupling of Hiero and NMT The Edit Distance Transducer Composition can be used together with a “flower automaton” to calculate the edit distance between two sequences (Mohri, 2003). The edit distance transducer shown in Fig. 1(a) transduces a sequence x to another sequence y over the alphabet {a, b} and accumulates the number of edit operations via the transitions with cost 1. In our case, x corresponds to an NMT hypo"
W16-2324,N09-2019,0,0.102039,"Missing"
W16-2324,J10-3008,0,0.284617,"Missing"
W16-2324,W15-5003,0,0.126933,"stead of a hard restriction of the NMT search space to the lattice, we propose to loosely couple NMT and Hiero by composition with a modified version of the edit distance transducer. The loose combination outperforms lattice rescoring, especially when using multiple NMT systems in an ensemble. 1 Introduction Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al., 2015; Stahlberg et al., 2016). Recent attempts to combine syntactic SMT and NMT report large gains over both baselines. Authors in (Neubig et al., 2015) used NMT to rescore n-best lists which were generated with a syntax-based system. They report that even with 1000-best lists, the gains of using the NMT rescorer often do not saturate. Syntactically Guided NMT (Stahlberg et al., 2016, SGNMT) constrains the NMT search space to Hiero translation lattices which contain significantly more hypotheses than n-best lists. In SGNMT, an NMT 2 Combining Hiero and NMT via Edit Distance Transducer In contrast"
W16-2324,1994.amta-1.10,0,0.409931,"Missing"
W16-2324,P16-1009,0,0.0347426,"MT) outperforms both NMT and Hiero baselines significantly on all test sets. For SGNMT, we see further improvements of between +0.7 BLEU (newstest2014) and +1.1 BLEU (news-test2015) by using NMT ensembles rather than single NMT. However, these gains are rather small considering the improvements from using ensembles for the (pure) NMT baseline (between +1.9 BLEU and +2.2 BLEU). Our combination scheme makes better use of the ensembles. We report 31.3 BLEU on news-test2016, which in the EnglishGerman WMT’16 evaluation is among the best systems (within 0.1 BLEU) which do not use back-translation (Sennrich et al., 2016a). Backtranslation is a technique for making use of monolingual data in NMT training, and we expect our system could benefit from back-translation, although we leave this analysis to future work. The combination procedure we propose is nontrivial. It is not immediately clear how the gains arise as the final scores are mixtures between edit distance costs, NMT scores, and Hiero scores. In the remainder we will try to provide some insight. Unless stated otherwise, we report investigations 4 http://matrix.statmt.org/ The code we used for SGNMT and ensembling is available at http://ucam-smt.githu"
W16-2324,P16-1162,0,0.0705787,"MT) outperforms both NMT and Hiero baselines significantly on all test sets. For SGNMT, we see further improvements of between +0.7 BLEU (newstest2014) and +1.1 BLEU (news-test2015) by using NMT ensembles rather than single NMT. However, these gains are rather small considering the improvements from using ensembles for the (pure) NMT baseline (between +1.9 BLEU and +2.2 BLEU). Our combination scheme makes better use of the ensembles. We report 31.3 BLEU on news-test2016, which in the EnglishGerman WMT’16 evaluation is among the best systems (within 0.1 BLEU) which do not use back-translation (Sennrich et al., 2016a). Backtranslation is a technique for making use of monolingual data in NMT training, and we expect our system could benefit from back-translation, although we leave this analysis to future work. The combination procedure we propose is nontrivial. It is not immediately clear how the gains arise as the final scores are mixtures between edit distance costs, NMT scores, and Hiero scores. In the remainder we will try to provide some insight. Unless stated otherwise, we report investigations 4 http://matrix.statmt.org/ The code we used for SGNMT and ensembling is available at http://ucam-smt.githu"
W16-2324,P16-2049,1,0.834584,"iction of the NMT search space to the lattice, we propose to loosely couple NMT and Hiero by composition with a modified version of the edit distance transducer. The loose combination outperforms lattice rescoring, especially when using multiple NMT systems in an ensemble. 1 Introduction Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al., 2015; Stahlberg et al., 2016). Recent attempts to combine syntactic SMT and NMT report large gains over both baselines. Authors in (Neubig et al., 2015) used NMT to rescore n-best lists which were generated with a syntax-based system. They report that even with 1000-best lists, the gains of using the NMT rescorer often do not saturate. Syntactically Guided NMT (Stahlberg et al., 2016, SGNMT) constrains the NMT search space to Hiero translation lattices which contain significantly more hypotheses than n-best lists. In SGNMT, an NMT 2 Combining Hiero and NMT via Edit Distance Transducer In contrast to the strict coupling in"
W16-2324,D08-1065,0,0.0850713,"ch between the NMT and syntactic decoders. In general, this kind of hard restriction is best avoided when combining diverse systems (Liu et al., 2009; Frederking et al., 1994). For example, in speech recognition, ROVER (Fiscus, 1997) is a system combination approach based on a soft voting scheme. In machine translation, minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) can be used to combine multiple systems (de Gispert et al., 2009). MBR also does not enforce exact agreement between systems as it distinguishes between the hypothesis space and the evidence space (Goel and Byrne, 2000; Tromble et al., 2008). We find that Hiero lattices generated by grammars extracted with the usual heuristics (Chiang, 2007) do not provide enough variety to explore the full potential of neural models, especially when using NMT ensembles. Therefore, we present a “soft” lattice-based combination scheme which uses standard operations on finite state transducers such as composition. Our method replaces the hard combination in previous methods with a similarity measure based on the edit distance, and gives the NMT decoder more freedom to diverge from the Hiero translations. We find that this loose coupling scheme is e"
W16-2324,2010.iwslt-keynotes.2,0,\N,Missing
W17-3531,W15-3001,0,0.0185739,"al hypothesis with the product of heuristic estimates of its words. This is especially useful for model combinations since all models are taken into account. We also implement hypothesis recombination to further reduce the number of search errors. More formally, at each time step t our beam search keeps the n best hypotheses according to scoring function S(·) using partial model score s(·) and estimates g(·): S(w1t ) = s(w1t ) − g(w1t ) s(w1t ) = log P (w1t |B OW(w)) X g(w1t ) = log Pˆ (w0 ) (4) w0 ∈w1t 4 Experimental Setup We evaluate using data from the English-German news translation task (Bojar et al., 2015, WMT) and using the English Penn Treebank data (Marcus et al., 1993, PTB). Since additional knowledge sources are often available in practice, such as access to the source sentence in a translation scenario, we also report on bilingual experiments for the WMT task. 4.1 Data and evaluation The WMT parallel training data includes Europarl v7, Common Crawl, and News Commentary v10. We use news-test2013 for tuning model combinations and news-test2015 for testing. All monolingual models for the WMT task were trained on the German news2015 corpus (∼51.3M sentences). For PTB, we use preprocessed dat"
W17-3531,E14-1028,1,0.921133,"Missing"
W17-3531,N15-1012,0,0.061208,"text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the following contribution"
W17-3531,J93-2004,0,0.0575649,"This is especially useful for model combinations since all models are taken into account. We also implement hypothesis recombination to further reduce the number of search errors. More formally, at each time step t our beam search keeps the n best hypotheses according to scoring function S(·) using partial model score s(·) and estimates g(·): S(w1t ) = s(w1t ) − g(w1t ) s(w1t ) = log P (w1t |B OW(w)) X g(w1t ) = log Pˆ (w0 ) (4) w0 ∈w1t 4 Experimental Setup We evaluate using data from the English-German news translation task (Bojar et al., 2015, WMT) and using the English Penn Treebank data (Marcus et al., 1993, PTB). Since additional knowledge sources are often available in practice, such as access to the source sentence in a translation scenario, we also report on bilingual experiments for the WMT task. 4.1 Data and evaluation The WMT parallel training data includes Europarl v7, Common Crawl, and News Commentary v10. We use news-test2013 for tuning model combinations and news-test2015 for testing. All monolingual models for the WMT task were trained on the German news2015 corpus (∼51.3M sentences). For PTB, we use preprocessed data by Schmaltz et al. (2016) for a fair comparison (∼40k sentences fo"
W17-3531,N16-1058,0,0.0177344,"as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the following contributions. We compare several lang"
W17-3531,D16-1255,0,0.750893,"or all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the following contributions. We compare several language models on the word-ordering task and propose a bag-to-sequence neural architecture that equips an LSTM decoder with explicit context of the bag-ofwords (B OW) to be ordered. This model performs particularly strongly on WMT"
W17-3531,W16-2323,0,0.0236092,"otations of the same size as the hidden units in the decoder. The R NNLM is based on the “large” setup of Zaremba et al. (2014) which uses an L STM. N PLM, a 5-gram neural feedforward language model, was trained for 10 epochs with a vocabulary size of 100k, 150 input and output units, 750 hidden units and 100 noise samples (Vaswani et al., 2013). The n-gram language model is a 5-gram model estimated with S RILM (Kneser and Ney, 1995). For the bilingual setting, we implemented a seq2seq NMT system following Bahdanau et al. (2015) using a beam size of 12 in line with recent NMT systems for WMT (Sennrich et al., 2016). R NNLM, bag2seq and seq2seq were implemented using TensorFlow (Abadi et al., 2015)2 and we used sgnmt for beam decoding3 . Following Schmaltz et al. (2016), our neural models for PTB have a vocabulary of 16,161 incl. two different unk tokens and the R NNLM is based on the “medium” setup of Zaremba et al. (2014). bag2seq uses 300 dimensional word embeddings and 500 hidden units in the decoder L STM. We also compare to G YRO (de Gispert et al., 2014) which explicitly targets the word-ordering problem. We extracted 1-gram to 5-gram phrase rules from the PTB train1 We thank the authors for help"
W17-3531,D13-1140,0,0.0200824,"odel settings For WMT, the bag2seq parameter settings follow the recent NMT systems trained on WMT data. We use a 50k vocabulary, 620 dimensional word embeddings and 1000 hidden units in the decoder L STM cells. On the encoder side, the input tokens are embedded to form annotations of the same size as the hidden units in the decoder. The R NNLM is based on the “large” setup of Zaremba et al. (2014) which uses an L STM. N PLM, a 5-gram neural feedforward language model, was trained for 10 epochs with a vocabulary size of 100k, 150 input and output units, 750 hidden units and 100 noise samples (Vaswani et al., 2013). The n-gram language model is a 5-gram model estimated with S RILM (Kneser and Ney, 1995). For the bilingual setting, we implemented a seq2seq NMT system following Bahdanau et al. (2015) using a beam size of 12 in line with recent NMT systems for WMT (Sennrich et al., 2016). R NNLM, bag2seq and seq2seq were implemented using TensorFlow (Abadi et al., 2015)2 and we used sgnmt for beam decoding3 . Following Schmaltz et al. (2016), our neural models for PTB have a vocabulary of 16,161 incl. two different unk tokens and the R NNLM is based on the “medium” setup of Zaremba et al. (2014). bag2seq u"
W17-3531,D11-1106,0,0.0982226,"for studying and comparing different kinds of models that produce text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We c"
W17-3531,J15-3005,0,0.0725805,"of models that produce text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the foll"
W17-3531,E12-1075,0,0.133967,"ing different kinds of models that produce text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of"
W18-1821,P16-1160,0,0.0200825,"esearch work of the Cambridge MT group is based on SGNMT, and (3) technology transfer as we show how SGNMT is helping to transfer research ﬁndings from the laboratory to the industry, eg. into a product of SDL plc. 1 Introduction The rate of innovation in machine translation (MT) has gathered impressive momentum over the recent years. The discovery and maturation of the neural machine translation (NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Fig. 1 shows that this progress is often driven by signiﬁcant changes in the network architecture. This volatility poses major challenges in MT-related research, teaching, and industry. Researchers potentially spend a lot of time implementing to keep their setups up-to-date with the latest models, teaching needs to identify suitable material in a changing environment, and the industry faces demanding speed requirements on its deployment processes. Another practical challenge many researchers are struggling with is the large number"
W18-1821,E17-3017,0,0.0348469,"oses major challenges in MT-related research, teaching, and industry. Researchers potentially spend a lot of time implementing to keep their setups up-to-date with the latest models, teaching needs to identify suitable material in a changing environment, and the industry faces demanding speed requirements on its deployment processes. Another practical challenge many researchers are struggling with is the large number of available NMT tools (van Merri¨enboer et al., 2015; Junczys-Dowmunt et al., 2016; Klein et al., 2017; Sennrich et al., 2017; Helcl and Libovick´y, 2017; Bertoldi et al., 2017; Hieber et al., 2017).1 Committing to one particular NMT tool bears the risk of being outdated soon, as keeping up with the pace of research is especially costly for NMT software developers. The open-source SGNMT (Syntactically Guided Neural Machine Translation) decoder2 (Stahlberg et al., 2017b) is our attempt to mediate the effects of the rapid progress in 1 See 2 Full https://github.com/jonsafari/nmt-list for a complete list of NMT software. documentation available at http://ucam-smt.github.io/sgnmt/html/. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 208 Figure 1: Best s"
W18-1821,D15-1273,1,0.9093,"Missing"
W18-1821,P15-1001,0,0.0383337,"ambridge, (2) research as most of the research work of the Cambridge MT group is based on SGNMT, and (3) technology transfer as we show how SGNMT is helping to transfer research ﬁndings from the laboratory to the industry, eg. into a product of SDL plc. 1 Introduction The rate of innovation in machine translation (MT) has gathered impressive momentum over the recent years. The discovery and maturation of the neural machine translation (NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Fig. 1 shows that this progress is often driven by signiﬁcant changes in the network architecture. This volatility poses major challenges in MT-related research, teaching, and industry. Researchers potentially spend a lot of time implementing to keep their setups up-to-date with the latest models, teaching needs to identify suitable material in a changing environment, and the industry faces demanding speed requirements on its deployment processes. Another practical challenge many researchers"
W18-1821,J82-2005,0,0.816506,"Missing"
W18-1821,P17-4012,0,0.0309416,"progress is often driven by signiﬁcant changes in the network architecture. This volatility poses major challenges in MT-related research, teaching, and industry. Researchers potentially spend a lot of time implementing to keep their setups up-to-date with the latest models, teaching needs to identify suitable material in a changing environment, and the industry faces demanding speed requirements on its deployment processes. Another practical challenge many researchers are struggling with is the large number of available NMT tools (van Merri¨enboer et al., 2015; Junczys-Dowmunt et al., 2016; Klein et al., 2017; Sennrich et al., 2017; Helcl and Libovick´y, 2017; Bertoldi et al., 2017; Hieber et al., 2017).1 Committing to one particular NMT tool bears the risk of being outdated soon, as keeping up with the pace of research is especially costly for NMT software developers. The open-source SGNMT (Syntactically Guided Neural Machine Translation) decoder2 (Stahlberg et al., 2017b) is our attempt to mediate the effects of the rapid progress in 1 See 2 Full https://github.com/jonsafari/nmt-list for a complete list of NMT software. documentation available at http://ucam-smt.github.io/sgnmt/html/. Proceeding"
W18-1821,N04-1022,0,0.183206,"T back ends on the complete KFTT test set (Neubig, 2011) computed with multi-bleu.pl. All neural systems are BPE-based (Sennrich et al., 2016) with vocabulary sizes of 30K. The SMT baseline achieves 18.1 BLEU. 4 Output Formats SGNMT supports ﬁve different output formats. • text: Plain text ﬁle with ﬁrst best translations. • nbest: n-best list of translation hypotheses. • sfst: Lattice generation in OpenFST (Allauzen et al., 2007) format with standard arcs. • fst: Lattices with sparse tuple arcs (Iglesias et al., 2015) which keep predictor scores separate. • ngram: MBR-style n-gram posteriors (Kumar and Byrne, 2004; Tromble et al., 2008) as used by Stahlberg et al. (2017a) for NMT. 5 SGNMT for Research SGNMT is designed for environments in which implementation time is far more valuable than computation time. This basic design decision is strongly reﬂected by the software architecture which accepts degradations in runtime in favor of extendibility and ﬂexibility. We designed SGNMT that way because training models and coding usually take the most time in our day-today work. Decoding, however, usually takes a small fraction of that time. Therefore, reducing the implementation time has a much larger impact"
W18-1821,D15-1166,0,0.0638438,"rch as most of the research work of the Cambridge MT group is based on SGNMT, and (3) technology transfer as we show how SGNMT is helping to transfer research ﬁndings from the laboratory to the industry, eg. into a product of SDL plc. 1 Introduction The rate of innovation in machine translation (MT) has gathered impressive momentum over the recent years. The discovery and maturation of the neural machine translation (NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Fig. 1 shows that this progress is often driven by signiﬁcant changes in the network architecture. This volatility poses major challenges in MT-related research, teaching, and industry. Researchers potentially spend a lot of time implementing to keep their setups up-to-date with the latest models, teaching needs to identify suitable material in a changing environment, and the industry faces demanding speed requirements on its deployment processes. Another practical challenge many researchers are struggling with"
W18-1821,W15-5003,0,0.10719,"chitecture is designed to facilitate the implementation of new predictors. Therefore, SGNMT can be extended to a new model or tool with very limited coding effort because rather than reimplementing models it is often enough to access APIs within an adapter predictor.3 Software packages which are not written in Python can be exposed in SGNMT if they have a Python interface.4 Once a new predictor is implemented, it can be directly combined with all other predictors which are already available in SGNMT. Therefore, general techniques like lattice and n-best list rescoring (Stahlberg et al., 2016; Neubig et al., 2015), ensembling, MBR-based NMT (Stahlberg et al., 2017a), etc. only need to be implemented once (as predictor), and are automatically available for all models. This does not only speed up the transition to a new NMT toolkit, it also allows the combination of different NMT implementations, eg. ensembling a Theano-based NMT model (van Merri¨enboer et al., 2015) with a TensorFlow-based Tensor2Tensor (Google, 2017) model. Hasler et al. (2017) demonstrated the versatility of SGNMT by combining ﬁve very different models (RNN LM, feedforward NPLM, Kneser-Ney LM, bag-to-seq model, seq-to-seq model) and a"
W18-1821,P16-1162,0,0.314288,"Missing"
W18-1821,D17-1208,1,0.833328,"bringing state-of-the-art MT products to the market9 while pushing the boundaries of MT technology via innovation and quick experimental research. In this context, it is highly desirable to use versatile tools that can be easily extended to support and combine new models, allowing for quick and painless experimentation. SDL Research chose SGNMT over all other existing tools for rapid prototyping and assessment of new research avenues. Among other Neural MT innovations, SDL Research used SGNMT to prototype and assess attention-based Neural MT (Bahdanau et al., 2015), Neural MT model shrinking (Stahlberg and Byrne, 2017) and the recent Transformer model (Vaswani et al., 2017). As described in Sec. 5, the Transformer model is trivially supported by the SGNMT decoder through its predictor framework, and is easy to combine with other predictors. It is worth noting that at the time of writing this paper, Transformer ensembles are not natively supported by the Tensor2Tensor decoder (Google, 2017). Although SDL Research’s decoder is homegrown, the SGNMT decoder is still a valuable reference tool for side-by-side comparison between state-of-the-art Neural MT research and the Neural MT product. 8 http://ucam-smt.gith"
W18-1821,E17-2058,1,0.885126,"Missing"
W18-1821,D17-2005,1,0.836553,"faces demanding speed requirements on its deployment processes. Another practical challenge many researchers are struggling with is the large number of available NMT tools (van Merri¨enboer et al., 2015; Junczys-Dowmunt et al., 2016; Klein et al., 2017; Sennrich et al., 2017; Helcl and Libovick´y, 2017; Bertoldi et al., 2017; Hieber et al., 2017).1 Committing to one particular NMT tool bears the risk of being outdated soon, as keeping up with the pace of research is especially costly for NMT software developers. The open-source SGNMT (Syntactically Guided Neural Machine Translation) decoder2 (Stahlberg et al., 2017b) is our attempt to mediate the effects of the rapid progress in 1 See 2 Full https://github.com/jonsafari/nmt-list for a complete list of NMT software. documentation available at http://ucam-smt.github.io/sgnmt/html/. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 208 Figure 1: Best systems on the English-German WMT news-test2014 test set over the years (BLEU script: Moses’ multi-bleu.pl). MT and the diversity of available NMT software. SGNMT introduces the concept of predictors as abstract scoring modules with left-to-right semantics. We can think of a"
W18-1821,P16-2049,1,0.954302,"dictors. Our software architecture is designed to facilitate the implementation of new predictors. Therefore, SGNMT can be extended to a new model or tool with very limited coding effort because rather than reimplementing models it is often enough to access APIs within an adapter predictor.3 Software packages which are not written in Python can be exposed in SGNMT if they have a Python interface.4 Once a new predictor is implemented, it can be directly combined with all other predictors which are already available in SGNMT. Therefore, general techniques like lattice and n-best list rescoring (Stahlberg et al., 2016; Neubig et al., 2015), ensembling, MBR-based NMT (Stahlberg et al., 2017a), etc. only need to be implemented once (as predictor), and are automatically available for all models. This does not only speed up the transition to a new NMT toolkit, it also allows the combination of different NMT implementations, eg. ensembling a Theano-based NMT model (van Merri¨enboer et al., 2015) with a TensorFlow-based Tensor2Tensor (Google, 2017) model. Hasler et al. (2017) demonstrated the versatility of SGNMT by combining ﬁve very different models (RNN LM, feedforward NPLM, Kneser-Ney LM, bag-to-seq model, s"
W18-1821,D08-1065,0,0.200612,"Missing"
W18-1821,D13-1140,0,0.0263605,"new model or toolkit. Secs. 2 to 4 describe central concepts in SGNMT like predictors and decoders brieﬂy and outline some common use cases. Sec. 5 shows that the SGNMT software architecture has proven to be very well suited for our research as new directions can be quickly prototyped, and new NMT toolkits can be introduced without breaking old code. Sec. 6 and Sec. 7 discuss the beneﬁts of SGNMT in teaching and industry, respectively. 3 Making all models of the T2T library (Google, 2017) available to SGNMT took less than 200 lines of code. example, the neural language modeling software NPLM (Vaswani et al., 2013) is written in C++, but can be accessed in SGNMT via its Python interface. 4 For Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 209 Figure 2: Greedy decoding with the predictor constellation nmt,fst for lattice rescoring. 2 The Predictor Interface Predictors in SGNMT provide a uniform interface for models and constraints. Since predictors are decoupled from each other, any predictor can be combined with any other predictor in a linear model. One predictor usually has a single responsibility as it represents a single model or type of constraint. Predictors"
W18-5420,P17-1183,0,0.0220111,"y closely: at each SRC POP operation the attention shifts by behavior of clones pennisetum subjected to periods restriction water controlled The word-by-word translation back to Portugese is: comportamento de clones pennisetum submetidos a per´ıodos restric¸a˜ o h´ıdrica controlada 182 ence to our OSNMT is that we have adapted the set of operations for neural models and are able to use it as stand-alone system, and not on top of a phrase-based system. Our operation sequence model has some similarities with transition-based models used in other areas of NLP (Stenetorp, 2013; Dyer et al., 2015; Aharoni and Goldberg, 2017). In particular, our POP SRC operation is very similar to the step action of the hard alignment model of Aharoni and Goldberg (2017). However, Aharoni and Goldberg (2017) investigated monotonic alignments for morphological inflections whereas we use a larger operation/action set to model complex word reorderings in machine translation. one to the next source token. Other attention heads have learned to take other responsibilities. For instance, head 3 in layer 2 (Fig. 4b) attends to the trigram right of the source head. 5 Related Work Explainable and interpretable machine learning is attractin"
W18-5420,W16-2206,0,0.052827,"l an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control for reading source tokens and writing target tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links during decoding. While our motivation is very similar to Alkhouli et al. (2016), our approach is very different as we represent the alignment as operation sequence, and we do not use separate models for reordering and lexical translation. The operation sequence model for SMT (Durrani et al., 2011, 2015) has been used in a number of MT evaluation systems (Durrani et al., 2014; Peter et al., 2016; Durrani et al., 2016) and for post-editing (Pal et al., 2016), often in combination with a phrase-"
W18-5420,W17-4711,0,0.0637857,"st, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on the output (Hasler et al., 2018) or to preserve text formatting, introducing explicit alignment information to NMT is still an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control for reading source tokens and writing target tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links during decoding. While our motivation is very similar to Alkho"
W18-5420,D17-1042,0,0.0180919,"s for morphological inflections whereas we use a larger operation/action set to model complex word reorderings in machine translation. one to the next source token. Other attention heads have learned to take other responsibilities. For instance, head 3 in layer 2 (Fig. 4b) attends to the trigram right of the source head. 5 Related Work Explainable and interpretable machine learning is attracting more and more attention in the research community (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017), particularly in the context of natural language processing (Karpathy et al., 2015; Li et al., 2016; Alvarez-Melis and Jaakkola, 2017; Ding et al., 2017; Feng et al., 2018). These approaches aim to explain (the predictions of) an existing model. In contrast, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on the output (Hasler et al., 2018) or to preserve text formatting, introducing explicit alignment information to NMT is still an open research problem. Word alignment"
W18-5420,W14-3309,0,0.024146,"arget tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links during decoding. While our motivation is very similar to Alkhouli et al. (2016), our approach is very different as we represent the alignment as operation sequence, and we do not use separate models for reordering and lexical translation. The operation sequence model for SMT (Durrani et al., 2011, 2015) has been used in a number of MT evaluation systems (Durrani et al., 2014; Peter et al., 2016; Durrani et al., 2016) and for post-editing (Pal et al., 2016), often in combination with a phrase-based model. The main differ6 Conclusion We have presented a way to use standard seq2seq models to generate a translation together with an alignment as linear sequence of operations. This greatly improves the interpretability of the model output as it establishes explicit alignment links between source and target tokens. However, the neural architecture we used in this paper is representation-agnostic, i.e. we did not explicitly incorporate the alignments induced by an operat"
W18-5420,W09-0437,0,0.0150577,"mponent, our scoring model for G is more powerful as it conditions on the OSNMT history which potentially contains context information. Note that OSNMT is deficient (Brown et al., 1993) as it assigns nonzero probability mass to any operation sequence, not only those with derivation in G. We further note that subword-based OSNMT can potentially represent any alignment to any target sentence as long as the alignment does not violate the 1:n restriction. This is in contrast to phrase-based SMT where reference translations often do not have a derivation in the SMT system due to coverage problems (Auli et al., 2009). 2.3 Comparison to the OSM for SMT Our OSNMT set of operations (POP SRC, SET MARKER, JMP FWD, JMP BWD, and INSERT(t)) is inspired by the original OSM for SMT (Durrani et al., 2011) as it also represents the translation process as linear sequence of operations. However, there are significant differences which make OSNMT more suitable for neural models. First, OSNMT is monotone on the source side, and allows jumps on the target side. SMT-OSM operations jump in the source sentence. We argue that source side monotonicity potentially mitigates coverage issues of neural models (over- and under-tran"
W18-5420,P11-1105,0,0.528898,"n in the network is represented by real-valued vectors or matrices (Ding et al., 2017). In contrast, the translation process in SMT is ‘transparent’ as it can identify the source word which caused a target word through word alignment. Most NMT models do not use the concept of word alignment. It is tempting to interpret encoder-decoder attention matrices (Bahdanau et al., 2015) in neural models as (soft) alignments, but previous work has found 2 A Neural Operation Sequence Model Our operation sequence neural machine translation (OSNMT) model is inspired by the operation sequence model for SMT (Durrani et al., 2011), but changes the set of operations to be more appropriate for neural sequence models. OSNMT is not restricted to a particular architecture, i.e. any seq2seq model such as RNN-based, convolutional, or self-attention-based models (Bahdanau et al., 175 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 175–186 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 2015; Vaswani et al., 2017; Gehring et al., 2017) could be used. In this paper, we use the recent Transformer model architecture (Vaswani et al.,"
W18-5420,J93-2003,0,0.13926,"generalization of SCFGs to more than two output streams (Melamed, 2003; Melamed et al., 2004). We find that an OSNMT sequence can be interpreted as sequence of rules of a tertiary MTG G which generates 1.) the source sentence, 2.) the target sentence, and 3.) the position of the target side write head. The start symbol of G is 2.1 OSNMT Represents Alignments The word alignment can be derived from the operation sequence by looking up the position of the read head for each generated target token. The alignment for the example in Tab. 1 is shown in Fig. 1. Note that similarly to the IBM models (Brown et al., 1993) and the OSM for SMT (Durrani et al., 2011), our OSNMT can only represent 1:n alignments. Thus, each target token is aligned to exactly one source token, but a source token can generate any number of (possibly nonconsecutive) target tokens. 2.2 • SET MARKER: Insert a marker symbol into the target sentence at the position of the write head. • JMP FWD: Move the write head to the nearest marker right of the current head position in the target sentence. • JMP BWD: Move the write head to the nearest marker left of the current head position in the target sentence. • INSERT(t): Insert a target token"
W18-5420,J15-2001,0,0.207008,"Missing"
W18-5420,2016.amta-researchers.10,0,0.0410905,"ions of) an existing model. In contrast, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on the output (Hasler et al., 2018) or to preserve text formatting, introducing explicit alignment information to NMT is still an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control for reading source tokens and writing target tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links during decoding. Wh"
W18-5420,P15-1033,0,0.0168131,"d head position very closely: at each SRC POP operation the attention shifts by behavior of clones pennisetum subjected to periods restriction water controlled The word-by-word translation back to Portugese is: comportamento de clones pennisetum submetidos a per´ıodos restric¸a˜ o h´ıdrica controlada 182 ence to our OSNMT is that we have adapted the set of operations for neural models and are able to use it as stand-alone system, and not on top of a phrase-based system. Our operation sequence model has some similarities with transition-based models used in other areas of NLP (Stenetorp, 2013; Dyer et al., 2015; Aharoni and Goldberg, 2017). In particular, our POP SRC operation is very similar to the step action of the hard alignment model of Aharoni and Goldberg (2017). However, Aharoni and Goldberg (2017) investigated monotonic alignments for morphological inflections whereas we use a larger operation/action set to model complex word reorderings in machine translation. one to the next source token. Other attention heads have learned to take other responsibilities. For instance, head 3 in layer 2 (Fig. 4b) attends to the trigram right of the source head. 5 Related Work Explainable and interpretable"
W18-5420,1983.tc-1.13,0,0.74511,"Missing"
W18-5420,D18-1407,0,0.0482647,"Missing"
W18-5420,P05-1033,0,0.411388,"ments from the target words to the source words that generate them. The operation sequence is “selfexplanatory”; it does not explain an underlying NMT system but is rather a single representation produced by the NMT system that can be used to generate translations along with an accompanying explanatory alignment to the source sentence. We report competitive results of our method on Spanish-English, Portuguese-English, and Japanese-English, with the benefit of producing hard alignments for better interpretability. We discuss the theoretical connection between our approach and hierarchical SMT (Chiang, 2005) by showing that an operation sequence can be seen as a derivation in a formal grammar. We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often cruc"
W18-5420,I17-1004,0,0.0639784,"Missing"
W18-5420,E17-1099,0,0.0208944,"ion to NMT is still an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control for reading source tokens and writing target tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links during decoding. While our motivation is very similar to Alkhouli et al. (2016), our approach is very different as we represent the alignment as operation sequence, and we do not use separate models for reordering and lexical translation. The operation sequence model for SMT (Durrani et al., 2011, 2015) has been used in a number of MT evaluation systems (Durrani et al., 2014; Peter et al., 2016; Durrani et al., 2016) and for post-editing (Pal et al., 2016), often in co"
W18-5420,N18-2081,1,0.778525,"Missing"
W18-5420,W16-2379,0,0.169096,"Missing"
W18-5420,W17-3204,0,0.0350425,"e attention in the research community (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017), particularly in the context of natural language processing (Karpathy et al., 2015; Li et al., 2016; Alvarez-Melis and Jaakkola, 2017; Ding et al., 2017; Feng et al., 2018). These approaches aim to explain (the predictions of) an existing model. In contrast, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on the output (Hasler et al., 2018) or to preserve text formatting, introducing explicit alignment information to NMT is still an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control fo"
W18-5420,N16-1082,0,0.0517412,"notonic alignments for morphological inflections whereas we use a larger operation/action set to model complex word reorderings in machine translation. one to the next source token. Other attention heads have learned to take other responsibilities. For instance, head 3 in layer 2 (Fig. 4b) attends to the trigram right of the source head. 5 Related Work Explainable and interpretable machine learning is attracting more and more attention in the research community (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017), particularly in the context of natural language processing (Karpathy et al., 2015; Li et al., 2016; Alvarez-Melis and Jaakkola, 2017; Ding et al., 2017; Feng et al., 2018). These approaches aim to explain (the predictions of) an existing model. In contrast, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on the output (Hasler et al., 2018) or to preserve text formatting, introducing explicit alignment information to NMT is still an ope"
W18-5420,N16-3020,0,0.0545398,"r to the step action of the hard alignment model of Aharoni and Goldberg (2017). However, Aharoni and Goldberg (2017) investigated monotonic alignments for morphological inflections whereas we use a larger operation/action set to model complex word reorderings in machine translation. one to the next source token. Other attention heads have learned to take other responsibilities. For instance, head 3 in layer 2 (Fig. 4b) attends to the trigram right of the source head. 5 Related Work Explainable and interpretable machine learning is attracting more and more attention in the research community (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017), particularly in the context of natural language processing (Karpathy et al., 2015; Li et al., 2016; Alvarez-Melis and Jaakkola, 2017; Ding et al., 2017; Feng et al., 2018). These approaches aim to explain (the predictions of) an existing model. In contrast, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on th"
W18-5420,C16-1291,0,0.0571906,"g model. In contrast, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on the output (Hasler et al., 2018) or to preserve text formatting, introducing explicit alignment information to NMT is still an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control for reading source tokens and writing target tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links during decoding. While our motivation"
W18-5420,P18-2051,1,0.853138,"Missing"
W18-5420,N03-1021,0,0.107047,"ructure from the operation sequence in Tab. 1 (Fig. 2) in which each marker is represented by a nonterminal node with outgoing arcs to symbols inserted at that marker. The target sentence can be read off the tree by depth-first search traversal (post-order). More formally, synchronous context-free grammars (SCFGs) generate pairs of strings by pairing two context-free grammars. Phrase-based hierarchical SMT (Chiang, 2005) uses SCFGs to model the relation between the source sentence and the target sentence. Multitext grammars (MTGs) are a generalization of SCFGs to more than two output streams (Melamed, 2003; Melamed et al., 2004). We find that an OSNMT sequence can be interpreted as sequence of rules of a tertiary MTG G which generates 1.) the source sentence, 2.) the target sentence, and 3.) the position of the target side write head. The start symbol of G is 2.1 OSNMT Represents Alignments The word alignment can be derived from the operation sequence by looking up the position of the read head for each generated target token. The alignment for the example in Tab. 1 is shown in Fig. 1. Note that similarly to the IBM models (Brown et al., 1993) and the OSM for SMT (Durrani et al., 2011), our OSN"
W18-5420,P16-1162,0,0.064642,"time. However, there is spurious ambiguity as one alignment can be represented by different OSNMT sequences. For instance, simply adding a SET MARKER operation at the end of an OSNMT sequence does not change the alignment represented by it. 4 Results We evaluate on three language pairs: JapaneseEnglish (ja-en), Spanish-English (es-en), and Portuguese-English (pt-en). We use the ASPEC corpus (Nakazawa et al., 2016) for ja-en and the health science portion of the Scielo corpus (Neves and N´ev´eol, 2016) for es-en and pt-en. Training set sizes are summarized in Tab. 3. We use byte pair encoding (Sennrich et al., 2016) with 32K merge operations for all systems (joint encoding models for es-en and pt-en and separate source/target models for ja-en). We trained Transformer models (Vaswani et al., 2017)2 until convergence (250K steps for plain text, 350K steps for OSNMT) on a single GPU using Tensor2Tensor (Vaswani et al., 2018) after removing sentences with more than 250 tokens. Batches contain around 4K source and 4K target tokens. Transformer training is very sensitive to the batch size and the number of GPUs (Popel and Bojar, 2018). Therefore, we delay SGD updates (Saunders et al., 2018) to every 8 steps to"
W18-5420,P04-1084,0,0.299542,"e operation sequence in Tab. 1 (Fig. 2) in which each marker is represented by a nonterminal node with outgoing arcs to symbols inserted at that marker. The target sentence can be read off the tree by depth-first search traversal (post-order). More formally, synchronous context-free grammars (SCFGs) generate pairs of strings by pairing two context-free grammars. Phrase-based hierarchical SMT (Chiang, 2005) uses SCFGs to model the relation between the source sentence and the target sentence. Multitext grammars (MTGs) are a generalization of SCFGs to more than two output streams (Melamed, 2003; Melamed et al., 2004). We find that an OSNMT sequence can be interpreted as sequence of rules of a tertiary MTG G which generates 1.) the source sentence, 2.) the target sentence, and 3.) the position of the target side write head. The start symbol of G is 2.1 OSNMT Represents Alignments The word alignment can be derived from the operation sequence by looking up the position of the read head for each generated target token. The alignment for the example in Tab. 1 is shown in Fig. 1. Note that similarly to the IBM models (Brown et al., 1993) and the OSM for SMT (Durrani et al., 2011), our OSNMT can only represent 1"
W18-5420,D17-2005,1,0.845378,"n text, 350K steps for OSNMT) on a single GPU using Tensor2Tensor (Vaswani et al., 2018) after removing sentences with more than 250 tokens. Batches contain around 4K source and 4K target tokens. Transformer training is very sensitive to the batch size and the number of GPUs (Popel and Bojar, 2018). Therefore, we delay SGD updates (Saunders et al., 2018) to every 8 steps to simulate 8 GPU training as recommended by Vaswani et al. (2017). Based on the performance on the ja-en dev set we decode the plain text systems with a beam size of 4 and OSNMT with a beam size of 8 using our SGNMT decoder (Stahlberg et al., 2017). We use length normalization for ja-en but not for esen or pt-en. We report cased multi-bleu.pl BLEU scores on the tokenized text to be comparable with the WAT evaluation campaign on ja-en.3 . We train our Transformer model as usual by minimising the negative log-likelihood of the target sequence. However, in contrast to plain text NMT, the target sequence is not a plain sequence of subword or word tokens but a sequence of operations. Consequently, we need to map the target sentences in the training corpus to OSNMT representations. We first run a statistical word aligner like Giza++ (Och and"
W18-5420,D16-1249,0,0.0183256,"lain (the predictions of) an existing model. In contrast, we change the target representation such that the generated sequences themselves convey important information about the translation process such as the word alignments. Despite considerable consensus about the importance of word alignments in practice (Koehn and Knowles, 2017), e.g. to enforce constraints on the output (Hasler et al., 2018) or to preserve text formatting, introducing explicit alignment information to NMT is still an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control for reading source tokens and writing target tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links"
W18-5420,L16-1470,0,0.0221992,"Missing"
W18-5420,P16-1008,0,0.0303985,"parison to the OSM for SMT Our OSNMT set of operations (POP SRC, SET MARKER, JMP FWD, JMP BWD, and INSERT(t)) is inspired by the original OSM for SMT (Durrani et al., 2011) as it also represents the translation process as linear sequence of operations. However, there are significant differences which make OSNMT more suitable for neural models. First, OSNMT is monotone on the source side, and allows jumps on the target side. SMT-OSM operations jump in the source sentence. We argue that source side monotonicity potentially mitigates coverage issues of neural models (over- and under-translation (Tu et al., 2016)) as the attention can learn to scan the source sentence from left to right. Another major difference is that we use markers rather than gaps, and do not close a gap/marker after jumping to it. This is an implication of OSNMT jumps being defined on the target side since the size of a span is unknown at inference time. Corpus Scielo Scielo WAT Algorithm 1 Align2OSNMT(a, x, y) 26: holes ← {(0, ∞)} ops ← hi {Initialize with empty list} head ← 0 for i ← 1 to |x |do for all j ∈ {j|aj = i} do hole idx ← holes.find(j) d ← hole idx − head if d < 0 then ops.extend(JMP BWD.repeat(−d)) end if if d > 0 th"
W18-5420,D16-1138,0,0.0286848,"icit alignment information to NMT is still an open research problem. Word alignments have been used as supervision signal for the NMT attention model (Mi et al., 2016; Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). Cohn et al. (2016) showed how to reintroduce concepts known from traditional statistical alignment models (Brown et al., 1993) like fertility and agreement over translation direction to NMT. Some approaches to simultaneous translation explicitly control for reading source tokens and writing target tokens and thereby generate monotonic alignments on the segment level (Yu et al., 2016, 2017; Gu et al., 2017). Alkhouli et al. (2016) used separate alignment and lexical models and thus were able to hypothesize explicit alignment links during decoding. While our motivation is very similar to Alkhouli et al. (2016), our approach is very different as we represent the alignment as operation sequence, and we do not use separate models for reordering and lexical translation. The operation sequence model for SMT (Durrani et al., 2011, 2015) has been used in a number of MT evaluation systems (Durrani et al., 2014; Peter et al., 2016; Durrani et al., 2016) and for post-editing (Pal et"
W18-5420,W10-1711,0,\N,Missing
W18-5420,2010.iwslt-evaluation.22,0,\N,Missing
W18-5420,J03-1002,0,\N,Missing
W18-5420,P17-1106,0,\N,Missing
W18-6427,D17-1148,0,0.0356724,"NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very different architectures (recurrent, convolutional, and self-attention based) in two different ways (full posterior and MBR-based), and find that combination with MBR-based n-gram scores is superior. 7 2016. Deeper machine translation and evaluation for German. In Proceedi"
W18-6427,W18-2202,0,0.018873,"nts worse than our best single neural model. We list the performance of our submitted systems on all test sets in Tab. 8. Test set news-test14 news-test15 news-test16 news-test17 news-test18 news-test14 news-test15 news-test16 news-test17 news-test18 news-dev17 news-test17 news-test18 BLEU 31.6 32.6 38.5 31.7 46.6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and"
W18-6427,I17-2004,0,0.0390255,"ble 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multip"
W18-6427,N18-2046,0,0.033061,"submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In t"
W18-6427,W17-3204,0,0.0290138,"e PBMT baseline is usally more than 10 BLEU points worse than our best single neural model. We list the performance of our submitted systems on all test sets in Tab. 8. Test set news-test14 news-test15 news-test16 news-test17 news-test18 news-test14 news-test15 news-test16 news-test17 news-test18 news-dev17 news-test17 news-test18 BLEU 31.6 32.6 38.5 31.7 46.6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysD"
W18-6427,N04-1022,0,0.0969931,"n-gram posteriors from the R2L model, reverse them, and use them for system combination as described in Sec. 2. λi log P (yt |y1t−1 , x, Mi ) + q X | gorithm similar to golden-section search (Kiefer, 1953).  (2) where λ1 , . . . , λq are interpolation weights. Eq. 2 also describes how to use beam search in this framework as hypotheses can be built up from left to right due to the outer sum over time steps. The MBR-based models contribute via the probabilt |x, M ) of an n-gram y t ity P (yt−n j t−n given the source sentence x. Posteriors in this form are commonly used for MBR decoding in SMT (Kumar and Byrne, 2004; Tromble et al., 2008), and can be extracted efficiently from translation lattices using counting transducers (Blackwood et al., 2010). For our neural models we run beam search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performa"
W18-6427,N18-3013,1,0.880265,"Missing"
W18-6427,D13-1054,0,0.0282011,"ission ranks second in terms of BLEU score in the WMT18 evaluation campaign on English-German and German-English, and outperforms all other systems on a variety of linguistic phenomena on German-English (Avramidis et al., 2018). Introduction 2 Encoder-decoder networks (Pollack, 1990; Chris˜ man, 1991; Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments s"
W18-6427,W16-2316,0,0.025543,"onger LSTM training. We use news-test2017 as development set on all language pairs to tune the model interpolation weights λ (Eq. 2) and the scaling factor for length normalization. We train vanilla phrase-based SMT systems5 and extract 1000-best lists of unique translations candidates, from which n-gram posteriors are calculated. All neural models were trained with the Adam optimizer (Kingma and Ba, 2015), dropout (Srivastava et al., 2014), and label smoothing (Szegedy et al., 2016) using the Tensor2Tensor (Vaswani et al., 2018) library. We decode with the average of the last 40 checkpoints (Junczys-Dowmunt et al., 2016a). We make extensive use of the delayed SGD updates technique we already applied successfully to syntax-based NMT (Saunders et al., 2018). Delaying SGD updates allows to arbitrarily choose the effective batch size even on limited GPU hardware. Large batch training has received some attention in recent research (Smith et al., 2017; Neishi et al., 2017) and has been shown particularly useful for training the Transformer architecture with the Tensor2Tensor framework (Popel and Bojar, 2018). We support these findings in Tab. 4.6 Our technical infrastructure7 allows us to train on four P100 GPUs s"
W18-6427,N16-1046,0,0.0667258,"Missing"
W18-6427,J82-2005,0,0.814358,"Missing"
W18-6427,W16-4602,0,0.0505981,"uperior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very different architectures (recurrent, convolutional, and self-attention based) in two different ways (full posterior and MBR-based), and find that combination with MBR-based n-gram scores is superior. 7 2016. Deeper machine translat"
W18-6427,D13-1176,0,0.0621467,"he batch size on limited GPU hardware. Furthermore, we also report gains from MBR-based combination with a phrase-based SMT system. We found this particularly striking as the SMT baselines are often more than 10 BLEU points below our strongest neural models. Our final submission ranks second in terms of BLEU score in the WMT18 evaluation campaign on English-German and German-English, and outperforms all other systems on a variety of linguistic phenomena on German-English (Avramidis et al., 2018). Introduction 2 Encoder-decoder networks (Pollack, 1990; Chris˜ man, 1991; Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw e"
W18-6427,D15-1166,0,0.0576362,"German-English, and outperforms all other systems on a variety of linguistic phenomena on German-English (Avramidis et al., 2018). Introduction 2 Encoder-decoder networks (Pollack, 1990; Chris˜ man, 1991; Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments suggest that self-attention is the superior architecture on the tested language pairs, but it can st"
W18-6427,P18-2051,1,0.876707,"Missing"
W18-6427,D08-1076,0,0.0471058,"m search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performance of our system combinations depends on the correct calibration of the interpolation weights λ1 , . . . , λq . We first tried to use nbest or lattice MERT (Och, 2003; Macherey et al., 2008) to find interpolation weights, but these techniques were not effective in our setting, possibly due to the lack of diversity and depth in nbest lists from standard beam search. Therefore, we tune on the first best translation using Powell’s method (Powell, 1964) with a line search al4 Experimental Setup 4.1 Data Selection We ran language detection (Nakatani, 2010) and gentle length filtering based on the number of characters and words in a sentence on all available monolingual and parallel data in English, German, and Chinese. Due to the high level of noise in the ParaCrawl corpus and its lar"
W18-6427,C16-1160,0,0.0235827,"her small, but we still found them surprising given that the PBMT baseline is usally more than 10 BLEU points worse than our best single neural model. We list the performance of our submitted systems on all test sets in Tab. 8. Test set news-test14 news-test15 news-test16 news-test17 news-test18 news-test14 news-test15 news-test16 news-test17 news-test18 news-dev17 news-test17 news-test18 BLEU 31.6 32.6 38.5 31.7 46.6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 201"
W18-6427,W18-1811,0,0.0507167,"large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very diff"
W18-6427,W17-4739,0,0.159119,"rior scores log-linearly, and bias the combined score S(y|x) towards low-risk hypotheses with respect to the MBR-based group as suggested by Stahlberg et al. (2017a, Eq. 4):1 S(y|x) = p T X X t=1 |i=1 3 {z } {z } Full posterior j=p+1 λj 4 X n=1 t P (yt−n |x, Mj ) MBR-based n-gram scores Right-to-left Translation Models Standard NMT models generate the translation from left to right on the target side. Recent work has shown that incorporating models which generate the target sentence in reverse order (i.e. from right to left) can improve translation quality (Liu et al., 2016; Li et al., 2017; Sennrich et al., 2017; Hassan et al., 2018). Right-to-left models are often used to rescore n-best lists from left-to-right models. However, we could not find improvements from rescoring in our setting. Instead, we extract n-gram posteriors from the R2L model, reverse them, and use them for system combination as described in Sec. 2. λi log P (yt |y1t−1 , x, Mi ) + q X | gorithm similar to golden-section search (Kiefer, 1953).  (2) where λ1 , . . . , λq are interpolation weights. Eq. 2 also describes how to use beam search in this framework as hypotheses can be built up from left to right due to the outer sum over"
W18-6427,P16-1009,0,0.286025,"Crawl more aggressively with the following rules: • No words contain more than 40 characters. • Sentences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. This additional filtering reduced the size of ParaCrawl from originally 36M sentences to 19M sentences after language detection, and to 11M sentences after applying the more aggressive rules. For backtranslation (Sennrich et al., 2016a) we selected 20M sentences from News Crawl 2017. We used a single Transformer (Vaswani et al., 2017) model in Tensor2Tensor’s (Vaswani et al., 2018) transformer base configuration 1 Eq. 2 differs from Eq. 4 of Stahlberg et al. (2017a) in that we do not use a word penalty Θ0 here, and we do not tune weights for different order n-grams separately (Θ1 , . . . Θ4 ). Both did not improve translation quality in our setting. 505 Corpus Common Crawl Europarl v7 News Commentary v13 Rapid 2016 ParaCrawl Synthetic (news-2017) Total Over-sampling 2x 2x 2x 2x 1x 1x Architecture LSTM SliceNet Transformer"
W18-6427,W17-5708,0,0.0244824,"d with the Adam optimizer (Kingma and Ba, 2015), dropout (Srivastava et al., 2014), and label smoothing (Szegedy et al., 2016) using the Tensor2Tensor (Vaswani et al., 2018) library. We decode with the average of the last 40 checkpoints (Junczys-Dowmunt et al., 2016a). We make extensive use of the delayed SGD updates technique we already applied successfully to syntax-based NMT (Saunders et al., 2018). Delaying SGD updates allows to arbitrarily choose the effective batch size even on limited GPU hardware. Large batch training has received some attention in recent research (Smith et al., 2017; Neishi et al., 2017) and has been shown particularly useful for training the Transformer architecture with the Tensor2Tensor framework (Popel and Bojar, 2018). We support these findings in Tab. 4.6 Our technical infrastructure7 allows us to train on four P100 GPUs simultaneously, which limits the number of physical GPUs to g = 4 and the batch size8 to b = 2048 due to the GPU memory. Thus, the maximum possible effective batch size without delaying SGD updates is b0 = 8192. Training with delay factor d accumulates gradients over d batches and applies the optimizer update rule on the accumulated gradients. This allo"
W18-6427,P16-1162,0,0.545743,"Crawl more aggressively with the following rules: • No words contain more than 40 characters. • Sentences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. This additional filtering reduced the size of ParaCrawl from originally 36M sentences to 19M sentences after language detection, and to 11M sentences after applying the more aggressive rules. For backtranslation (Sennrich et al., 2016a) we selected 20M sentences from News Crawl 2017. We used a single Transformer (Vaswani et al., 2017) model in Tensor2Tensor’s (Vaswani et al., 2018) transformer base configuration 1 Eq. 2 differs from Eq. 4 of Stahlberg et al. (2017a) in that we do not use a word penalty Θ0 here, and we do not tune weights for different order n-grams separately (Θ1 , . . . Θ4 ). Both did not improve translation quality in our setting. 505 Corpus Common Crawl Europarl v7 News Commentary v13 Rapid 2016 ParaCrawl Synthetic (news-2017) Total Over-sampling 2x 2x 2x 2x 1x 1x Architecture LSTM SliceNet Transformer"
W18-6427,W15-5003,0,0.0426753,".6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performa"
W18-6427,N18-2074,0,0.178466,"2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments suggest that self-attention is the superior architecture on the tested language pairs, but it can still benefit from model combination with the other two. We show that using large batch sizes is crucial to Transformer training, and that the delayed SGD updates technique (Saunders et al., 2018) is useful to increase System Combination Stahlberg et al. (2"
W18-6427,C16-1172,0,0.0864616,"Missing"
W18-6427,P03-1021,0,0.112904,"we run beam search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performance of our system combinations depends on the correct calibration of the interpolation weights λ1 , . . . , λq . We first tried to use nbest or lattice MERT (Och, 2003; Macherey et al., 2008) to find interpolation weights, but these techniques were not effective in our setting, possibly due to the lack of diversity and depth in nbest lists from standard beam search. Therefore, we tune on the first best translation using Powell’s method (Powell, 1964) with a line search al4 Experimental Setup 4.1 Data Selection We ran language detection (Nakatani, 2010) and gentle length filtering based on the number of characters and words in a sentence on all available monolingual and parallel data in English, German, and Chinese. Due to the high level of noise in the Para"
W18-6427,D11-1014,0,0.080721,"Missing"
W18-6427,W18-1819,0,0.0616536,"Missing"
W18-6427,E17-2058,1,0.895446,"Missing"
W18-6427,D17-2005,1,0.600459,"Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments suggest that self-attention is the superior architecture on the tested language pairs, but it can still benefit from model combination with the other two. We show that using large batch sizes is crucial to Transformer training, and that the delayed SGD updates technique (Saunders et al., 2018) is useful to increase System Combination Stahlberg et al. (2017a) combined SMT and NMT in a hybrid system with a minimum Bayes-risk (MBR) formulation which has been proven useful even for practical industry-level MT (Iglesias et al., 2018). Our system combination scheme is a generalization of this approach to more than two systems. Suppose we want to combine q models M1 , . . . , Mq . We first divide the models into two groups by selecting a p with 1 ≤ p ≤ q. We refer to scores from the first group M1 , . . . , Mp as full posterior scores and from the second group Mp+1 , . . . , Mq as MBR-based scores. Full posterior models contribute to the combined scor"
W18-6427,1983.tc-1.13,0,0.701411,"Missing"
W18-6427,P16-2049,1,0.827016,"7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles"
W18-6427,P17-2060,0,0.0274836,"tings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very different architectures (recurrent, convolutional, and self-attention based) in two different ways (full posterior and MBR-based), and find that combination with MBR-based n-gram scores is superior. 7 2016. Deeper machine translation and evaluation for German. In Proceedings of the 2nd Deep"
W18-6427,W18-1821,1,0.783781,"Missing"
W18-6427,E17-1100,0,0.0450447,"Missing"
W18-6427,D08-1065,0,0.0405232,"the R2L model, reverse them, and use them for system combination as described in Sec. 2. λi log P (yt |y1t−1 , x, Mi ) + q X | gorithm similar to golden-section search (Kiefer, 1953).  (2) where λ1 , . . . , λq are interpolation weights. Eq. 2 also describes how to use beam search in this framework as hypotheses can be built up from left to right due to the outer sum over time steps. The MBR-based models contribute via the probabilt |x, M ) of an n-gram y t ity P (yt−n j t−n given the source sentence x. Posteriors in this form are commonly used for MBR decoding in SMT (Kumar and Byrne, 2004; Tromble et al., 2008), and can be extracted efficiently from translation lattices using counting transducers (Blackwood et al., 2010). For our neural models we run beam search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performance of our system combi"
W18-6427,W18-6436,0,\N,Missing
W18-6427,P10-2006,1,\N,Missing
W19-4417,D18-1045,0,0.0175636,"l in practice. Back-translation Back-translation (Sennrich et al., 2016b) has become the most widely used technique to use monolingual data in neural machine translation. Back-translation extends the existing parallel training set by additional training samples with real English target sentences but synthetic source sentences. Different methods have been proposed to synthesize the source sentence such as using dummy tokens (Sennrich et al., 2016b), copying the target sentence (Currey et al., 2017), or sampling from or decoding with a reverse sequence-to-sequence model (Sennrich et al., 2016b; Edunov et al., 2018; Kasewa et al., Over-sampling Rate (Real Data) 1x 1x 1x 1x 3x 6x Number of Synthetic Sentences 0 1M 3M 5M 3M 5M Fine-tuning As explained previously, we oversample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fineRatio 1:1.6 1:4.8 1:7.9 1:1.6 1:1.3 CoNLL-2014 P R M2 53.34 28.83 45.59 56.17 31.30 48.47 61.40 34.29 53.02 64.18 34.27 54.64 57.12 32.55 49.63 59.15 33.99 51.52 P 33.04 37.79 42.62 44.69 40.08 41.52 BEA-2019 Dev R ERRANT 23.14 30.44 23.86 33.84 25.30 37.4"
W19-4417,N18-2046,0,0.0395266,"6b; Poncelas et al., 2018; Sennrich et al., 2016a). Over-sampling the real data is a common practice to rectify that ratio if large amounts of synthetic data are available. Interestingly, over-sampling real data in GEC hurts performance (row 3 vs. 5 in Tab. 8), and it is possible to mix real and synthetic sentences at a ratio of 1:7.9 (last three rows in Tab. 8). We will proceed with the 5M setup for the remainder of this paper. Table 7: Impact of identity removal on BASE models. (i.e. source and target sentences are equal) from the training corpora (Stahlberg et al., 2019; Zhao et al., 2019; Grundkiewicz and Junczys-Dowmunt, 2018). We note that removing these identity mappings can be seen as measure to control the balance between precision and recall. As shown in Tab. 7, removing identities encourages the model to make more corrections and thus leads to higher recall but lower precision. It depends on the test set whether this results in an improvement in F0.5 score. For the subsequent experiments we found that removing identities in the parallel training corpora but not in the back-translated synthetic data works well in practice. Back-translation Back-translation (Sennrich et al., 2016b) has become the most widely us"
W19-4417,D18-1541,0,0.102758,"I+LOCNESS under BASE models. The second column contains the ratio of W&I+LOCNESS samples to training samples from the other corpora. Identity CoNLL-2014 BEA-2019 Dev Removal P R M2 P R ERR. × 59.16 17.20 39.76 40.40 16.67 31.44 X 53.34 28.83 45.59 33.04 23.14 30.44 2018). The most popular approach is to generate the synthetic source sentences with a reverse model that is trained to transform target to source sentences using beam search. In GEC, this means that the reverse model learns to introduce errors into a correct English sentence. Back-translation has been applied successfully to GEC by Kasewa et al. (2018). We confirm the effectiveness of back-translation in GEC and discuss some of the differences between applying this technique to grammatical error correction and machine translation. Our experiments with back-translation are summarized in Tab. 8. Adding 1M synthetic sentences to the training data already yields very substantial gains on both test sets. We achieve our best results with 5M synthetic sentences (+8.44 on BEA2019 Dev). In machine translation, it is important to maintain a balance between authentic and synthetic data (Sennrich et al., 2016b; Poncelas et al., 2018; Sennrich et al., 2"
W19-4417,W18-0529,0,0.0686281,"it to itself at no cost. on FSTs and supported efficiently by FST toolkits such as OpenFST (Allauzen et al., 2007). We construct the hypothesis space as follows:2 1. We compose the input I with the deletion transducer D in Fig. 2. D allows to delete tokens on the short list shown in Tab. 1 at a cost λdel . We selected R by looking up all tokens which have been deleted in the dev set more than five times and manually filtered that list slightly. We did not use the full list of dev set deletions to avoid under-estimating λdel in tuning. Like Stahlberg et al. (2019), we use the confusion sets of Bryant and Briscoe (2018) based on CyHunspell for spell checking (Rodriguez and Seal, 2014), the AGID morphology database for morphology errors (Atkinson, 2011), and manually defined corrections for determiner and preposition errors to construct E. Additionally, we extracted all substitution errors from the BEA-2019 dev set which occurred more than five times, and added a small number of manually defined rules that fix tokenization around punctuation symbols. 2. In a next step, we compose the transducer from step 1 with the edit transducer E in Fig. 3. This step addresses substitution errors such as spelling or morpho"
W19-4417,J97-2003,0,0.0342667,".57 points. We report further gains on both test sets by ensembling two language models and increasing the beam size. In a more condensed form, we can describe the final transducer as: I ◦D◦E◦A◦T (1) with D for deletions, E for substitutions, A for insertions, and T for converting words to BPE tokens. Path scores in the FST in Eq. 1 are the accumulated penalties λdel , λsub , and λins . The λ-parameters are tuned on the dev set using a variant of Powell search (Powell, 1964). We apply standard FST operations like output projection, -removal, determinization, minimization, and weight pushing (Mohri, 1997; Mohri and Riley, 2001) to help downstream decoding. Following Stahlberg et al. (2019) we then use the resulting transducer to constrain a neural LM beam decoder. 2.2 Results 2.4 Experimental Setup Differences Between CoNLL-2014 and BEA-2019 Dev Our results in Tab. 2 differ significantly between the CoNLL-2014 test set and the BEA-2019 dev set. Allowing insertions is beneficial on BEA2019 Dev but decreases the M2 score on CoNLL2014. Increasing the beam size improves our system by 3.28 points on CoNLL-2014 while the imOur LMs are Transformer (Vaswani et al., 2017) decoders (transformer big) tr"
W19-4417,W19-4406,0,0.34088,"ng – without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab. 1 Introduction The automatic correction of errors in text [In a such situaction → In such a situation] is receiving more and more attention from the natural language processing community. A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task (Ng et al., 2013), the CoNLL-2014 shared task (Ng et al., 2014), and finally the BEA 2019 shared task (Bryant et al., 2019). This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop. We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC (Stahlberg et al., 2019) to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, th"
W19-4417,P17-1074,0,0.145778,"r decoding, we use our SGNMT tool (Stahlberg et al., 2017b, 2018b) with OpenFST backend (Allauzen et al., 2007). 4. Finally, we map the word-level FSTs to the subword-level by composition with a mapping transducer T that applies byte pair encoding (Sennrich et al., 2016c, BPE) to the full words. Word-to-BPE mapping transducers have been used in prior work to combine word-level models with subword-level neural sequence models (Stahlberg et al., 2019, 2017b, 2018b, 2017a). 2.3 We report M2 (Dahlmeier and Ng, 2012) scores on the CoNLL-2014 test set (Ng et al., 2014) and span-based ERRANT scores (Bryant et al., 2017) on the BEA-2019 dev set (Bryant et al., 2019). On CoNLL-2014 we compare with the best published results with comparable amount of parallel training data. We refer to (Bryant et al., 2019) for a full comparison of BEA-2019 systems. We tune our systems on BEA-2019 and only report the performance on CoNLL-2014 for comparison to prior work. Tab. 2 summarizes our low-resource experiments. Our substitution-only system already outperforms the prior work of Stahlberg et al. (2019). Allowing for deletions and insertions improves the ERRANT score on BEA-2019 Dev by 2.57 points. We report further gains"
W19-4417,W14-1701,0,0.398712,"and a combination of checkpoint averaging and fine-tuning – without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab. 1 Introduction The automatic correction of errors in text [In a such situaction → In such a situation] is receiving more and more attention from the natural language processing community. A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task (Ng et al., 2013), the CoNLL-2014 shared task (Ng et al., 2014), and finally the BEA 2019 shared task (Bryant et al., 2019). This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop. We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC (Stahlberg et al., 2019) to handle new error types such as punctuation errors as well as insertions and deletions of a"
W19-4417,W17-4715,0,0.019706,"ound that removing identities in the parallel training corpora but not in the back-translated synthetic data works well in practice. Back-translation Back-translation (Sennrich et al., 2016b) has become the most widely used technique to use monolingual data in neural machine translation. Back-translation extends the existing parallel training set by additional training samples with real English target sentences but synthetic source sentences. Different methods have been proposed to synthesize the source sentence such as using dummy tokens (Sennrich et al., 2016b), copying the target sentence (Currey et al., 2017), or sampling from or decoding with a reverse sequence-to-sequence model (Sennrich et al., 2016b; Edunov et al., 2018; Kasewa et al., Over-sampling Rate (Real Data) 1x 1x 1x 1x 3x 6x Number of Synthetic Sentences 0 1M 3M 5M 3M 5M Fine-tuning As explained previously, we oversample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fineRatio 1:1.6 1:4.8 1:7.9 1:1.6 1:1.3 CoNLL-2014 P R M2 53.34 28.83 45.59 56.17 31.30 48.47 61.40 34.29 53.02 64.18 34.27 54.64 57.12 32.55 4"
W19-4417,W13-3601,0,0.189568,"ranslation models trained with backtranslation and a combination of checkpoint averaging and fine-tuning – without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab. 1 Introduction The automatic correction of errors in text [In a such situaction → In such a situation] is receiving more and more attention from the natural language processing community. A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task (Ng et al., 2013), the CoNLL-2014 shared task (Ng et al., 2014), and finally the BEA 2019 shared task (Bryant et al., 2019). This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop. We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC (Stahlberg et al., 2019) to handle new error types such as punctuation er"
W19-4417,J82-2005,0,0.504352,"Missing"
W19-4417,D17-2005,1,0.946482,"predictable words like proper names. We therefore restrict insertions to the three tokens “,”, “-”, and “’s” and allow only one insertion per sentence. We achieve this by adding the transducer A in Fig. 4 to our composition cascade. source and target tokens. Our LM training set comprises the monolingual news2015-news2018 English training sets3 from the WMT evaluation campaigns (Bojar et al., 2018) after language detection (Nakatani, 2010) (138M sentences) and subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. For decoding, we use our SGNMT tool (Stahlberg et al., 2017b, 2018b) with OpenFST backend (Allauzen et al., 2007). 4. Finally, we map the word-level FSTs to the subword-level by composition with a mapping transducer T that applies byte pair encoding (Sennrich et al., 2016c, BPE) to the full words. Word-to-BPE mapping transducers have been used in prior work to combine word-level models with subword-level neural sequence models (Stahlberg et al., 2019, 2017b, 2018b, 2017a). 2.3 We report M2 (Dahlmeier and Ng, 2012) scores on the CoNLL-2014 test set (Ng et al., 2014) and span-based ERRANT scores (Bryant et al., 2017) on the BEA-2019 dev set (Bryant et a"
W19-4417,W18-1821,1,0.885973,"Missing"
W19-4417,P18-2051,1,0.901489,"Missing"
W19-4417,W18-1819,0,0.0442782,"s the differences between both setups. Tab. 5 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints (Junczys-Dowmunt et al., 2016). We use the SGNMT decoder (Stahlberg et al., 2017b, 2018b) in all our experiments. Restricted Track Submission In contrast to our low-resource submission, our restricted system entirely relies on neural models and does not use any external NLP tools, spell checkers, or hand-crafted confusion sets. For simplicity, we also chose to use standard implementations (Vaswani et al., 2018) of standard Transformer (Vaswani et al., 2017) models with standard hyper-parameters. This makes our final system easy to deploy as it is a simple ensemble of standard neural models with minimal preprocessing (subword segmentation). Our contributions on this track focus on NMT training techniques such as over-sampling, back-translation, and fine-tuning. We show that over-sampling effectively reduces domain mismatch. We found back-translation (Sennrich et al., 2016b) to be a very effective technique to utilize unannotated training data. However, while over-sampling is commonly used in machine"
W19-4417,W16-2323,0,0.372496,"ults on the low-resource track. The λ-parameters are tuned on the BEA-2019 dev set. less predictable words like proper names. We therefore restrict insertions to the three tokens “,”, “-”, and “’s” and allow only one insertion per sentence. We achieve this by adding the transducer A in Fig. 4 to our composition cascade. source and target tokens. Our LM training set comprises the monolingual news2015-news2018 English training sets3 from the WMT evaluation campaigns (Bojar et al., 2018) after language detection (Nakatani, 2010) (138M sentences) and subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. For decoding, we use our SGNMT tool (Stahlberg et al., 2017b, 2018b) with OpenFST backend (Allauzen et al., 2007). 4. Finally, we map the word-level FSTs to the subword-level by composition with a mapping transducer T that applies byte pair encoding (Sennrich et al., 2016c, BPE) to the full words. Word-to-BPE mapping transducers have been used in prior work to combine word-level models with subword-level neural sequence models (Stahlberg et al., 2019, 2017b, 2018b, 2017a). 2.3 We report M2 (Dahlmeier and Ng, 2012) scores on the CoNLL-2014 test set (Ng et al., 2014"
W19-4417,P16-1009,0,0.487188,"ults on the low-resource track. The λ-parameters are tuned on the BEA-2019 dev set. less predictable words like proper names. We therefore restrict insertions to the three tokens “,”, “-”, and “’s” and allow only one insertion per sentence. We achieve this by adding the transducer A in Fig. 4 to our composition cascade. source and target tokens. Our LM training set comprises the monolingual news2015-news2018 English training sets3 from the WMT evaluation campaigns (Bojar et al., 2018) after language detection (Nakatani, 2010) (138M sentences) and subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. For decoding, we use our SGNMT tool (Stahlberg et al., 2017b, 2018b) with OpenFST backend (Allauzen et al., 2007). 4. Finally, we map the word-level FSTs to the subword-level by composition with a mapping transducer T that applies byte pair encoding (Sennrich et al., 2016c, BPE) to the full words. Word-to-BPE mapping transducers have been used in prior work to combine word-level models with subword-level neural sequence models (Stahlberg et al., 2019, 2017b, 2018b, 2017a). 2.3 We report M2 (Dahlmeier and Ng, 2012) scores on the CoNLL-2014 test set (Ng et al., 2014"
W19-4417,W19-4424,1,0.868009,"Sennrich et al., 2016b) to GEC – a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture (Vaswani et al., 2017). Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by Yuan et al. (2019). We describe two entries from the Cambridge University Engineering Department to the BEA 2019 Shared Task on grammatical error correction. Our submission to the lowresource track is based on prior work on using finite state transducers together with strong neural language models. Our system for the restricted track is a purely neural system consisting of neural language models and neural machine translation models trained with backtranslation and a combination of checkpoint averaging and fine-tuning – without the help of any additional tools like spell checkers. The latter system has been use"
W19-4417,P16-1162,0,0.77637,"ults on the low-resource track. The λ-parameters are tuned on the BEA-2019 dev set. less predictable words like proper names. We therefore restrict insertions to the three tokens “,”, “-”, and “’s” and allow only one insertion per sentence. We achieve this by adding the transducer A in Fig. 4 to our composition cascade. source and target tokens. Our LM training set comprises the monolingual news2015-news2018 English training sets3 from the WMT evaluation campaigns (Bojar et al., 2018) after language detection (Nakatani, 2010) (138M sentences) and subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. For decoding, we use our SGNMT tool (Stahlberg et al., 2017b, 2018b) with OpenFST backend (Allauzen et al., 2007). 4. Finally, we map the word-level FSTs to the subword-level by composition with a mapping transducer T that applies byte pair encoding (Sennrich et al., 2016c, BPE) to the full words. Word-to-BPE mapping transducers have been used in prior work to combine word-level models with subword-level neural sequence models (Stahlberg et al., 2019, 2017b, 2018b, 2017a). 2.3 We report M2 (Dahlmeier and Ng, 2012) scores on the CoNLL-2014 test set (Ng et al., 2014"
W19-4417,N19-1014,0,0.256705,"ennrich et al., 2016b; Poncelas et al., 2018; Sennrich et al., 2016a). Over-sampling the real data is a common practice to rectify that ratio if large amounts of synthetic data are available. Interestingly, over-sampling real data in GEC hurts performance (row 3 vs. 5 in Tab. 8), and it is possible to mix real and synthetic sentences at a ratio of 1:7.9 (last three rows in Tab. 8). We will proceed with the 5M setup for the remainder of this paper. Table 7: Impact of identity removal on BASE models. (i.e. source and target sentences are equal) from the training corpora (Stahlberg et al., 2019; Zhao et al., 2019; Grundkiewicz and Junczys-Dowmunt, 2018). We note that removing these identity mappings can be seen as measure to control the balance between precision and recall. As shown in Tab. 7, removing identities encourages the model to make more corrections and thus leads to higher recall but lower precision. It depends on the test set whether this results in an improvement in F0.5 score. For the subsequent experiments we found that removing identities in the parallel training corpora but not in the back-translated synthetic data works well in practice. Back-translation Back-translation (Sennrich et"
W19-4417,N19-1406,1,0.687972,"cal error correction (GEC): the CoNLL-2013 shared task (Ng et al., 2013), the CoNLL-2014 shared task (Ng et al., 2014), and finally the BEA 2019 shared task (Bryant et al., 2019). This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop. We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC (Stahlberg et al., 2019) to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M 2 2.1 Low-resource Track Submission FST-based Grammatical Error Correction Stahlberg et al. (2019) investigated the use of finite state transducers (FSTs) for neural grammatical error correction. They proposed a cascade of FST compositions to construct a hypothesis space which is then rescored with a neural language model. We will outline this approach and explain our modifications in this section. For more details"
W19-4417,W18-6427,1,0.863494,"Missing"
W19-4417,E17-2058,1,0.903829,"Missing"
W19-4417,N12-1067,0,\N,Missing
W19-4417,W18-6401,0,\N,Missing
W19-4424,D14-1082,0,0.0413013,"composition, we re-score the enriched input lattice I with the system described in Section 2.2. The FST-based system combination uses 7 different features: the convolutional system score, the LM and NMT scores from the Transformer• Two binary features indicating whether two publicly available spell-checkers – HunSpell2 and JamSpell3 – identify the target word as a spelling mistake. 1 https://github.com/marekrei/ sequence-labeler 2 http://hunspell.github.io/ 3 https://github.com/bakwc/JamSpell 231 • The POS tag, NER label and dependency relation of the target word based on the Stanford parser (Chen and Manning, 2014). false positives (FP) on token-level error detection by treating the error detection model as the “gold standard”. Specifically, we count how many times the candidate FST hypothesis disagrees with the detection model on the tokens identified as incorrect, and use as a 1.0 feature the following: FP+1.0 • The number of times the unigram, bigram, or trigram context of the target word appears in the BNC (Burnard, 2007) and in ukWaC (Ferraresi et al., 2008). We use a linear combination of the above three features together with the original score given by the FST system for each candidate hypothesi"
W19-4424,D14-1179,0,0.0326186,"Missing"
W19-4424,Q17-1010,0,0.00763353,"2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring et al., 2017), which employs convolutional neural networks (CNNs) to compute intermediate encoder and decoder states. The parameter settings follow Chollampatt and Ng (2018) and Ge et al. (2018). The source and target word embeddings have size 500, and are initialised with fastText embeddings (Bojanowski et al., 2017) trained on the native English Wikipedia corpus (2, 405, 972, 890 tokens). Each of the encoder and decoder is made up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybri"
W19-4424,W18-0529,0,0.105303,"max layer at the output. In addition to neural text representations, we also include several external features into the model, designed to help it learn more accurate error detection patterns from the limited amounts of training data available: Stahlberg et al. (2019) demonstrated the usefulness of FSTs for grammatical error correction. Their method starts with an input lattice I which is generated with a phrase-based statistical machine translation (SMT) system. The lattice I is composed with a number of FSTs that aim to enrich the search space with further possible corrections. Similarly to Bryant and Briscoe (2018), they rely on external knowledge sources like spell checkers and morphological databases to generate additional correction options for the input sentence. The enriched lattice is then mapped to the subword level by composition with a mapping transducer, and re-scored with neural machine translation models and neural LMs. In this work, rather than combining SMT and neural models, we use the framework of Stahlberg et al. (2019) to combine and enrich the outputs of two neural systems. The input lattice I is now the union of two n-best lists – one from the convolutional system (Section 2.1), and"
W19-4424,W19-4406,0,0.0263852,"as a feature the LD between those binary representations. Specifically, we would like to select the candidate FST sentence that has the smallest LD from the binary sequence created by the detection model, and therefore 1.0 use as a feature the following: LD+1.0 • Cambridge English W&I corpus Cambridge English Write & Improve (W&I)5 (Yannakoudakis et al., 2018) is an online web platform that assists non-native English learners with their writing. Learners from around the world submit letters, stories, articles and essays for automated assessment in response to various prompts. The W&I corpus (Bryant et al., 2019) contains 3, 600 annotated submissions across 3 different CEFR6 levels: A (beginner), B (intermediate), and C (advanced). The data has been 4 We note that there are no restrictions on the use of NLP tools (e.g., POS taggers, parsers, spellcheckers, etc.), nor on the amount of unannotated data that can be used, so long as such resources are publicly available. 5 https://writeandimprove.com/ 6 https://www.cambridgeenglish.org/ exams-and-tests/cefr/ 3. False positives: using the binary sequences described above, we count the number of 232 split into training (3, 000 essays), development (200 essa"
W19-4424,W13-1703,0,0.0241772,"atical error detection system was optimized separately as a sequence labeling model. Word embeddings were set to size 300 and initialized with pre-trained Glove embedding (Pennington et al., 2014). The bi-LSTM has 300dimensional hidden layers for each direction. Dropout was applied to word embeddings and LSTM outputs with probability 0.5. The model was optimized with Adam (Kingma and Ba, 2015), using a default learning rate 0.001. Training was stopped when performance on the development set did not improved over 7 epochs. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) contains 1, 400 essays written by undergraduate students at the National University of Singapore who are non-native English speakers. • Lang-8 Corpus of Learner English Lang-88 is an online language learning website which encourages users to correct each other’s grammar. The Lang-8 Corpus of Learner English (Mizumoto et al., 2011; Tajiri et al., 2012) refers to an English subsection of this website (can be quite noisy). Additional resources used in our system include: • English Wikipedia corpus The English Wikipedia corpus (2, 405, 972, 890 tokens in 110, 698, 467 sentences) is used to pre-tr"
W19-4424,P17-1074,0,0.263796,"tion 3 is a threshold used to filter out sentence pairs with unnecessary changes; e.g., [I look forward to hearing from you. → I am looking forward to hearing from you.]. It is an avA binary classification task is also introduced to predict whether the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learne"
W19-4424,W12-2006,0,0.021788,"rrection (GEC) is the task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring e"
W19-4424,W11-2838,0,0.018545,"uction Grammatical error correction (GEC) is the task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decod"
W19-4424,P17-1070,0,0.0839707,"Missing"
W19-4424,W14-1702,1,0.843944,"der and decoder is made up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and pr"
W19-4424,N18-1055,0,0.0833184,"Missing"
W19-4424,D18-1541,0,0.18577,"ed by learners. Some researchers have investigated ways of incorporating task-specific knowledge, either by directly modifying the training objectives (Schmaltz et al., 2017; Sakaguchi et al., 2017; JunczysDowmunt et al., 2018) or by re-ranking machinetranslation-system correction hypotheses (Yannakoudakis et al., 2017; Chollampatt and Ng, 2018). To ameliorate the lack of large amounts of error-annotated learner data, various approaches have proposed to leverage unlabelled native data within a number of frameworks, including artificial error generation with back translation (Rei et al., 2017; Kasewa et al., 2018), fluency boost learning (Ge et al., 2018), and pre-training with denoising autoencoders (Zhao et al., 2019). Previous work has shown that a GEC system In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly available data, along"
W19-4424,I11-1017,0,0.118142,"Missing"
W19-4424,N18-2046,0,0.036794,"l layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the shared task development set; Section 4 pre"
W19-4424,W14-1701,0,0.340725,"Missing"
W19-4424,P16-1154,0,0.0209614,"and target side of the parallel training data respectively. The same BPE operation is applied to the Wikipedia data before being used for training of our word embeddings. Approach We approach the error correction task using a pipeline of systems, as presented in Figure 1. In the following sections, we describe each of these components in detail. 2.1 p(yt |{y1 , ..., yt−1 }, x) t=1 Transformer 2 m Y Copying mechanism is a technique that has led to performance improvement on various monolingual sequence-to-sequence tasks, such as text summarisation, dialogue systems, and paraphrase generation (Gu et al., 2016; Cao et al., 2017). The idea is to allow the decoder to choose between simply copying an original input word and outputting a translation word. Since the source and target sentences are both in the same language (i.e., monolingual translation) and most words in The convolutional neural network (CNN) system We use a neural sequence-to-sequence model and an encoder–decoder architecture (Cho et al., 2014; Sutskever et al., 2014). An encoder first reads and encodes an entire input sequence x = (x1 , x2 , ..., xn ) into hidden state representations. A decoder then generates an output sequence y ="
W19-4424,W13-3601,0,0.0864338,"he task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring et al., 2017), whi"
W19-4424,W11-2123,0,0.0115977,"providing error detection labels. Instead of only generating a corrected sentence, we extend the system to additionally predict whether a token in the source sentence is correct or incorrect. f (y) ≤σ f (yok ) (3) where f (y) is the normalised log probability of y: Pm f (y) = • Sentence-level labelling t=1 log(P (yt |y&lt;t )) m (4) This ensures that the quality of the artificially generated sentence, as estimated by a language model, is lower compared to the original sentence. We use a 5-gram language model (LM) trained on the One Billion Word Benchmark dataset (Chelba et al., 2014) with KenLM (Heafield, 2011) to compute P (yt |y&lt;t ). The σ in Equation 3 is a threshold used to filter out sentence pairs with unnecessary changes; e.g., [I look forward to hearing from you. → I am looking forward to hearing from you.]. It is an avA binary classification task is also introduced to predict whether the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens usin"
W19-4424,D14-1162,0,0.0842984,"sterov’s Accelerated Gradient Descent (NAG) with a simplified formulation for Nesterov’s momentum (Bengio et al., 2013). The initial learning rate is set to 0.25, with a decaying factor of 0.1 and a momentum value of 0.99. We perform validation after every epoch, and select the best model based on the performance on the development set. During beam search, we keep a beam size of 12 and discard all other hypotheses. The grammatical error detection system was optimized separately as a sequence labeling model. Word embeddings were set to size 300 and initialized with pre-trained Glove embedding (Pennington et al., 2014). The bi-LSTM has 300dimensional hidden layers for each direction. Dropout was applied to word embeddings and LSTM outputs with probability 0.5. The model was optimized with Adam (Kingma and Ba, 2015), using a default learning rate 0.001. Training was stopped when performance on the development set did not improved over 7 epochs. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) contains 1, 400 essays written by undergraduate students at the National University of Singapore who are non-native English speakers. • Lang-8 Corpus of Learner English Lan"
W19-4424,N18-1202,0,0.00953368,"grees with the detection model on the tokens identified as incorrect, and use as a 1.0 feature the following: FP+1.0 • The number of times the unigram, bigram, or trigram context of the target word appears in the BNC (Burnard, 2007) and in ukWaC (Ferraresi et al., 2008). We use a linear combination of the above three features together with the original score given by the FST system for each candidate hypothesis to re-rank the FST system’s 8-best list in an unsupervised way. The new 1-best correction hypothesis c∗ is then the one that maximises: • Contextualized word representations from ELMo (Peters et al., 2018). The discrete features are represented as 10dimensional embeddings and, together with the continuous features, concatenated to each word representation in the model. The overall architecture is optimized for error detection using crossentropy. Once trained, the model returns the predicted probabilities of each token in a sentence being correct or incorrect. c∗ = arg max c K X λi hi (c) (6) i=1 where h represents the score assigned to candidate hypothesis c according to feature i; λ is a weighting parameter that controls the effect feature i has on the final ranking; and K = 4 as we use a tota"
W19-4424,P16-1162,0,0.339169,"of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the shared task development set; Section 4 presents our official results on the shared task test set, including a detailed analysis of the performance of our final system; and, finally, Section 5 concludes the paper and provides an overview of our findings. BPE is introduced to alleviate the rare-word problem, and rare and unknown words are split into multiple frequent subword tokens (Sennrich et al., 2016b). NMT systems often limit vocabulary size on both source and target sides due to the computational complexity during training. Therefore, they are unable to translate out-of-vocabulary (OOV) words, which are treated as unknown tokens, resulting in poor translation quality. As noted by Yuan and Briscoe (2016), this problem is more serious for GEC as non-native text contains, not only rare words (e.g., proper nouns), but also misspelled words (i.e., spelling errors). In our model, each of the source and target vocabularies consist of the 30K most frequent BPE tokens from the source and target"
W19-4424,P17-1194,1,0.930878,"ing (NLP) (Collobert and Weston, 2008) and error generation system using the same network speech recognition (Deng et al., 2013) to computer architecture as the one described here, with errorvision (Girshick, 2015). Multi-task learning alcorrected sentences as the source and their correlows systems to use information from related tasks sponding uncorrected counterparts written by lanand learn from multiple objectives, which leads guage learners as the target. The system is then to performance improvement on individual tasks. used to collect the n-best outputs: yo1 , yo2 , ..., yon , Recently, Rei (2017) and Rei and Yannakoudakis for a given error-free native and/or learner sen(2017) investigated the use of different auxiliary tence y. Since there is no guarantee that the error objectives for the task of error detection in learner generation system will inject errors into the input writing. sentence y to make it less grammatically correct, In addition to our primary error correction task, we apply “quality control”. A pair of artificially we propose two related auxiliary objectives to generated sentences (yok , y), for k ∈ {1, 2, ..., n}, boost model performance: will be added to the training"
W19-4424,N19-1406,1,0.911367,"the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previo"
W19-4424,W17-5032,1,0.94333,"andard language used by learners. Some researchers have investigated ways of incorporating task-specific knowledge, either by directly modifying the training objectives (Schmaltz et al., 2017; Sakaguchi et al., 2017; JunczysDowmunt et al., 2018) or by re-ranking machinetranslation-system correction hypotheses (Yannakoudakis et al., 2017; Chollampatt and Ng, 2018). To ameliorate the lack of large amounts of error-annotated learner data, various approaches have proposed to leverage unlabelled native data within a number of frameworks, including artificial error generation with back translation (Rei et al., 2017; Kasewa et al., 2018), fluency boost learning (Ge et al., 2018), and pre-training with denoising autoencoders (Zhao et al., 2019). Previous work has shown that a GEC system In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly"
W19-4424,W19-4417,1,0.757125,"ence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the"
W19-4424,P16-1112,1,0.857116,"n SMT system based on error detection predictions. Following this work, we also deploy a re-ranking component which re-ranks the n-best correction hypotheses of the FST system (Section 2.3) based on error detection predictions output by an error detection system. FST-based system combination Error detection. Our system for grammatical error detection is based on the model described by Rei (2017).1 The task is formulated as a sequence labeling problem – given a sentence, the model assigns a probability to each token, indicating the likelihood of that token being incorrect in the given context (Rei and Yannakoudakis, 2016). The architecture maps words to distributed embeddings, while also constructing character-based representations for each word with a neural component. These are then passed through a bidirectional LSTM, followed by a feed-forward layer and a softmax layer at the output. In addition to neural text representations, we also include several external features into the model, designed to help it learn more accurate error detection patterns from the limited amounts of training data available: Stahlberg et al. (2019) demonstrated the usefulness of FSTs for grammatical error correction. Their method s"
W19-4424,D17-2005,1,0.86093,"ing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT mode"
W19-4424,W17-5004,1,0.919341,"Missing"
W19-4424,W18-1821,1,0.843308,"Missing"
W19-4424,P16-1208,0,0.0167945,"de up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the s"
W19-4424,I17-2062,0,0.0663944,"Missing"
W19-4424,P12-2039,0,0.20414,"Missing"
W19-4424,D17-1298,0,0.0244066,"Missing"
W19-4424,W18-1819,0,0.0256159,"for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT models are trained with backtranslation (Sennrich et al., 2016a; Rei et al., 2017; Kasewa et al., 2018) and fine-tuning through continued training. For a detailed description of this system we refer the reader to Stahlberg and Byrne (2019). 2.3 2.4 Re-ranking FST output Yannakoudakis et al. (2017) found that grammatical error detection systems can be used to improve error correction outputs. Specifically, they re-rank the n-best correction hypotheses of an SMT system based on error detection predictions. Following this work, we also deploy a re-ranking c"
W19-4424,P11-1019,1,0.784266,"section of 100 essays has been manually annotated, and equally partitioned into development and test sets. In order to cover the full range of English levels and abilities, the official development set consists of 300 essays from W&I (A: 130, B:100, and C:70) and 50 essays from LOCNESS (86, 973 tokens in 4, 384 sentences). The ERRANT scorer (Bryant et al., 2017) is used as the official scorer for the shared task. System performance is evaluated in terms of spanlevel correction using F0.5 , which emphasises precision twice as much as recall. • FCE The First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) is a subset of the Cambridge Learner Corpus (CLC) that consists of 1, 244 exam scripts written by learners of English sitting the FCE exam. 3.2 • NUCLE Training details The convolutional NMT model is trained with a hidden layer size of 1, 024 for both the encoder and the decoder. Dropout at a rate of 0.2 is applied to the embedding layers, convolutional layers and decoder output. The model is optimized using Nesterov’s Accelerated Gradient Descent (NAG) with a simplified formulation for Nesterov’s momentum (Bengio et al., 2013). The initial learning rate is set to 0.25, with a decaying factor"
W19-4424,D17-1297,1,0.858113,"ystem Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT models are trained with backtranslation (Sennrich et al., 2016a; Rei et al., 2017; Kasewa et al., 2018) and fine-tuning through continued training. For a detailed description of this system we refer the reader to Stahlberg and Byrne (2019). 2.3 2.4 Re-ranking FST output Yannakoudakis et al. (2017) found that grammatical error detection systems can be used to improve error correction outputs. Specifically, they re-rank the n-best correction hypotheses of an SMT system based on error detection predictions. Following this work, we also deploy a re-ranking component which re-ranks the n-best correction hypotheses of the FST system (Section 2.3) based on error detection predictions output by an error detection system. FST-based system combination Error detection. Our system for grammatical error detection is based on the model described by Rei (2017).1 The task is formulated as a sequence l"
W19-4424,N16-1042,1,0.819082,"set, including a detailed analysis of the performance of our final system; and, finally, Section 5 concludes the paper and provides an overview of our findings. BPE is introduced to alleviate the rare-word problem, and rare and unknown words are split into multiple frequent subword tokens (Sennrich et al., 2016b). NMT systems often limit vocabulary size on both source and target sides due to the computational complexity during training. Therefore, they are unable to translate out-of-vocabulary (OOV) words, which are treated as unknown tokens, resulting in poor translation quality. As noted by Yuan and Briscoe (2016), this problem is more serious for GEC as non-native text contains, not only rare words (e.g., proper nouns), but also misspelled words (i.e., spelling errors). In our model, each of the source and target vocabularies consist of the 30K most frequent BPE tokens from the source and target side of the parallel training data respectively. The same BPE operation is applied to the Wikipedia data before being used for training of our word embeddings. Approach We approach the error correction task using a pipeline of systems, as presented in Figure 1. In the following sections, we describe each of th"
W19-4424,N19-1014,0,0.20593,"Missing"
W19-5340,W16-6404,0,0.0223852,"bmissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations fro"
W19-5340,D17-1156,0,0.0195049,"vs. 6 in Tab. 4). Gains are generally smaller on German-English. 4.3 et al., 2017) and checkpoint averaging (JunczysDowmunt et al., 2016b,a). In our context, both methods aim to avoid catastrophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general domain when used for fine-tuning on a related domain. Language modelling We introduced our new Intra-Inter Transformer architecture for document-level language modelling in Sec. 2. Tab. 5 shows that our architecture achie"
W19-5340,N18-1118,0,0.119742,"Extracting n-gram probabilities from traditional PBSMT lattices as described by Stahlberg et al. (2017a) and using them as source-conditioned n-gram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... c"
W19-5340,W18-6478,0,0.304334,"ntences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. Tab. 2 indicates that our systems benefit from ParaCrawl even without filtering (rows 1 vs. 2). Our best ‘Base’ model uses both dual and naive filtering. However, the difference between filtering techniques diminishes under stronger ‘Big’ models with back-translation (rows 6 and 7). ParaCrawl Corpus Filtering Junczys-Dowmunt (2018a,b) reported large gains from filtering the ParaCrawl corpus. This year, the WMT organizers made version 3 of the ParaCrawl corpus available. We compared two different filtering approaches on the new data set. First, we implemented dual cross-entropy filtering (Junczys-Dowmunt, 2018a,b), a sophisticated data selection criterion based on neural 4 4.1 Results Back-translation Back-translation (Sennrich et al., 2016b) is a wellestablished technique to use monolingual target language data for NMT. The idea is to automatically generate translations into the source language with an inverse translat"
W19-5340,D17-1148,0,0.0202572,"ure of the core NMT system. German-English Team BLEU MSRA 42.8 Facebook FAIR 40.8 NEU 40.5 UCAM 39.7 RWTH 39.6 MLLP-UPV 39.3 DFKI 38.8 4 more... Table 7: English-German and German-English primary submissions to the WMT19 shared task. Year 2017 2018 2019 Best in competition 28.3 48.3 44.9 This work ∆ 32.8 49.3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distan"
W19-5340,W18-6415,0,0.254207,"ntences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. Tab. 2 indicates that our systems benefit from ParaCrawl even without filtering (rows 1 vs. 2). Our best ‘Base’ model uses both dual and naive filtering. However, the difference between filtering techniques diminishes under stronger ‘Big’ models with back-translation (rows 6 and 7). ParaCrawl Corpus Filtering Junczys-Dowmunt (2018a,b) reported large gains from filtering the ParaCrawl corpus. This year, the WMT organizers made version 3 of the ParaCrawl corpus available. We compared two different filtering approaches on the new data set. First, we implemented dual cross-entropy filtering (Junczys-Dowmunt, 2018a,b), a sophisticated data selection criterion based on neural 4 4.1 Results Back-translation Back-translation (Sennrich et al., 2016b) is a wellestablished technique to use monolingual target language data for NMT. The idea is to automatically generate translations into the source language with an inverse translat"
W19-5340,J82-2005,0,0.725998,"Missing"
W19-5340,W16-2316,0,0.0565057,"Missing"
W19-5340,D18-1045,0,0.127109,"itional cross-entropy filtering (JunczysDowmunt, 2018a,b). • Elastic weight consolidation (Kirkpatrick et al., 2017, EWC) is a domain adaptation technique that aims to avoid degradation in performance on the original domain. We report large gains from fine-tuning our models on former English-German WMT test sets with EWC. We find that combining finetuning with checkpoint averaging (JunczysDowmunt et al., 2016b,a) yields further significant gains. Fine-tuning is less effective for German-English. • We confirm the effectiveness of source-side noise for scaling up back-translation as proposed by Edunov et al. (2018). 364 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 364–373 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Document-level Language Modelling MT systems usually translate sentences in isolation. However, there is evidence that humans also take context into account, and judge translations from humans with access to the full document higher than the output of a state-of-the-art sentence-level machine translation system (L¨aubli et al., 2018). Common examples of ambiguity which can be resolved wi"
W19-5340,2015.iwslt-evaluation.11,0,0.128742,"rate document-level context in a light-weight fashion, we propose a modification to the Transformer (Vaswani et al., 2017) that has separate attention layers for inter- and intra-sentential context. We report large perplexity reductions compared to sentence-level LMs under the new architecture. Our document-level LM yields small BLEU gains on top of strong NMT ensembles, and we hope to benefit even more from it in document-level human evaluation. Introduction Both fine-tuning and language modelling are techniques widely used for NMT. Fine-tuning is often used to adapt a model to a new domain (Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a,"
W19-5340,I17-2004,0,0.0190888,"le 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b,"
W19-5340,W18-1811,0,0.0432,"two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations from SMT and jointly traine"
W19-5340,W18-2705,0,0.0205868,"Fi = E ∇2 LA (θi ) is an estimate of task A Fisher information, which represents the importance of parameter θi to A. On English-German, fine-tuning with EWC and checkpoint averaging yields an 1.1 BLEU improvement (rows 1 vs. 6 in Tab. 4). Gains are generally smaller on German-English. 4.3 et al., 2017) and checkpoint averaging (JunczysDowmunt et al., 2016b,a). In our context, both methods aim to avoid catastrophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general dom"
W19-5340,P18-1118,0,0.0256459,"ram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source"
W19-5340,D16-1139,0,0.0312813,"improvement (rows 1 vs. 6 in Tab. 4). Gains are generally smaller on German-English. 4.3 et al., 2017) and checkpoint averaging (JunczysDowmunt et al., 2016b,a). In our context, both methods aim to avoid catastrophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general domain when used for fine-tuning on a related domain. Language modelling We introduced our new Intra-Inter Transformer architecture for document-level language modelling in Sec. 2. Tab. 5 shows that"
W19-5340,N19-1313,0,0.0123114,"r input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Scherrer, 2017; Maruf et"
W19-5340,D18-1325,0,0.0205331,"17), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Sch"
W19-5340,W18-6417,0,0.0924966,"18a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Koehn et al., 2018b; JunczysDowmunt, 2018b), in our experiments this year we found that a very simple filtering approach based on a small number of crude heuristics can perform as well as dual conditional cross-entropy filtering (JunczysDowmunt, 2018a,b). • Elastic weight consolidation (Kirkpatrick et al., 2017, EWC) is a domain adaptation technique that aims to avoid degradation in performance on the original domain. We report large gains from fine-tuning our models on former English-German WMT test sets with EWC. We find that combining finetuning with checkpoint averaging (JunczysDowmunt et al., 2016b,a) yiel"
W19-5340,W18-6307,0,0.0420208,"Missing"
W19-5340,W15-5003,0,0.0306058,"This work ∆ 32.8 49.3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or"
W19-5340,D18-1512,0,0.0370169,"Missing"
W19-5340,C16-1172,0,0.0344343,"Missing"
W19-5340,W16-4602,0,0.027896,"ations from a sentence-level MT system. Our method is light-weight as, similarly to Tiedemann and Scherrer (2017), we do not modify the architecture of the core NMT system. German-English Team BLEU MSRA 42.8 Facebook FAIR 40.8 NEU 40.5 UCAM 39.7 RWTH 39.6 MLLP-UPV 39.3 DFKI 38.8 4 more... Table 7: English-German and German-English primary submissions to the WMT19 shared task. Year 2017 2018 2019 Best in competition 28.3 48.3 44.9 This work ∆ 32.8 49.3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), altho"
W19-5340,P18-2051,1,0.899395,"Missing"
W19-5340,W16-2324,1,0.84347,"3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way,"
W19-5340,P19-1022,1,0.888492,"Missing"
W19-5340,D17-2005,1,0.912788,"Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Koehn et al., 2018b; JunczysDowmunt, 2018b), in our experiments this year we found that a very simple filtering approach based on a small number of crude heuristics can perform as well as dual conditional cross-entropy filtering (JunczysDowmunt, 2018a,b). • Elastic weight consolidation (Kirkpatrick et al., 2017, EWC) is a domain adaptation technique that aims to avoid degradation in performance on the original"
W19-5340,W18-6426,0,0.0220852,"37.3 30.3 test18 43.8 44.3 43.2 43.1 44.8 44.7 44.4 45.2 45.2 Table 3: Using different corpora for back-translation. We back-translated with a ‘base’ model for news-2017 and the big single Transformer model of Stahlberg et al. (2018b) for news-2016 and news-2018. Fine-tuning 1 2 3 4 5 6 No No Cont’d train. Cont’d train. EWC EWC Checkpoint averaging X X X BLEU (test18) En-De De-En 46.7 46.5 46.6 46.4 47.1 46.6 47.3 46.8 47.1 46.4 47.8 46.8 until it converges on a training corpus A, and then continues training on a usually much smaller corpus B which is close to the target domain. Similarly to Schamper et al. (2018); Koehn et al. (2018a), we fine-tune our models on former WMT test sets (2008-2016) to adapt them to the target domain of high-quality news translations. Due to the very small size of corpus B, much care has to be taken to avoid over-fitting. We experimented with different techniques that keep the model parameters in the fine-tuning phase close to the original ones. First, we fine-tuned our models for about 1K-2K iterations (depending on the performance on the news-test2017 dev set) and dumped checkpoints every 500 steps. Averaging all fine-tuning checkpoints together with the last unadapted c"
W19-5340,P16-2049,1,0.835571,"3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way,"
W19-5340,W18-1821,1,0.879725,"context. We report large perplexity reductions compared to sentence-level LMs under the new architecture. Our document-level LM yields small BLEU gains on top of strong NMT ensembles, and we hope to benefit even more from it in document-level human evaluation. Introduction Both fine-tuning and language modelling are techniques widely used for NMT. Fine-tuning is often used to adapt a model to a new domain (Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Ko"
W19-5340,W16-2323,0,0.363162,"ra-sentential and inter-sentential attention masks for an English example from news-test2017. Document-level context helps to predict the next word (‘vinyl’). We stop when the translation score difference to the first-best translation exceeds a threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurati"
W19-5340,P16-1009,0,0.547142,"ra-sentential and inter-sentential attention masks for an English example from news-test2017. Document-level context helps to predict the next word (‘vinyl’). We stop when the translation score difference to the first-best translation exceeds a threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurati"
W19-5340,N19-1209,0,0.0173239,"trophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general domain when used for fine-tuning on a related domain. Language modelling We introduced our new Intra-Inter Transformer architecture for document-level language modelling in Sec. 2. Tab. 5 shows that our architecture achieves much better perplexity than both a sentence-level language model and a documentlevel vanilla Transformer model. Tab. 6 summarizes our translation results with various kinds of language mo"
W19-5340,P16-1162,0,0.694172,"ra-sentential and inter-sentential attention masks for an English example from news-test2017. Document-level context helps to predict the next word (‘vinyl’). We stop when the translation score difference to the first-best translation exceeds a threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurati"
W19-5340,W17-4811,0,0.0278791,"erarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Scherrer, 2017; Maruf et al., 2019). Several studies on document-level NMT indicate that automatic and human sentence-level evaluation metrics often do not correlate"
W19-5340,W18-6321,1,0.908682,"context. We report large perplexity reductions compared to sentence-level LMs under the new architecture. Our document-level LM yields small BLEU gains on top of strong NMT ensembles, and we hope to benefit even more from it in document-level human evaluation. Introduction Both fine-tuning and language modelling are techniques widely used for NMT. Fine-tuning is often used to adapt a model to a new domain (Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Ko"
W19-5340,Q18-1029,0,0.0205767,"e-conditioned n-gram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply conca"
W19-5340,W18-6427,1,0.864639,"Missing"
W19-5340,W18-1819,0,0.0235889,"threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurations (Tab. 1). Preliminary experiments are carried out with the ‘Base’ configuration while we use the ‘Big’ models for our final system. We use news-test2017 as development set to tune model weights and select checkpoints and news-test2018 as tes"
W19-5340,E17-2058,1,0.902934,"Missing"
W19-5340,P18-1117,0,0.019973,", but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Scherrer, 2017; Maruf et al., 2019). Several studies on document-level NMT indicate that automatic and human sentence-level evaluation metrics often do not correlate well with improvements in discourse level phenomena (Bawden et al., 2018; L¨aubli et al., 2018; M¨uller et al., 20"
W19-5340,D17-1301,0,0.162518,"of) Big models with backtranslation (row 2 vs. 3). Extracting n-gram probabilities from traditional PBSMT lattices as described by Stahlberg et al. (2017a) and using them as source-conditioned n-gram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Faceb"
W19-5340,D18-1049,0,0.0368862,"Missing"
W19-5340,I17-1016,0,0.0168159,"ntrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations from SMT and jointly trained NMT together with a"
W19-5340,P17-2060,0,0.0229361,"s (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations from SMT and jointly trained NMT together with a gating function which assigns the weight between SMT and NMT scores dynamically. The AMU-UEDIN submission to WMT16 let SMT take the lead and used NMT as a feature in phrase-based MT (Junczys-Dowmunt et al., Conclusion Our WMT19 submission focused on regularized fine-tuning and language modelling. With our novel Intra-Inter Transformer architecture for document-level LMs we achieved significant reductions in perplexity and minor impr"
W19-5340,W16-2310,0,\N,Missing
W19-5340,W18-6453,0,\N,Missing
W19-5421,kobus-etal-2017-domain,0,0.0320505,"itional ensemble. At step i, where hi = y1:i−1 is decoding history: p(yi |hi , x) = K X pk (yi |hi , x) T X Figure 1: Adaptively adjusting model weights during decoding with Bayesian Interpolation 1.3 p(t|hi , x)λk,t Transfer learning has been applied to NMT in many forms. Luong and Manning (2015) use transfer learning to adapt a general model to indomain data. Zoph et al. (2016) use multilingual transfer learning to improve NMT for lowresource languages. Chu et al. (2017) introduce mixed fine-tuning, which carries out transfer learning to a new domain combined with some original domain data. Kobus et al. (2017) train a single model on multiple domains using domain tags. Khan et al. (2018) sequentially adapt across multiple biomedical domains to obtain one singledomain model. At inference time, Freitag and Al-Onaizan (2016) use uniform ensembles of general and finetuned models. Our Bayesian Interpolation experiments extend previous work by Allauzen and Riley (2011) on Bayesian Interpolation for language model combination. t=1 k=1 (1) This is an adaptively weighted ensemble where, for each source sentence x and output hypothesis y, we re-estimate p(t|hi , x) at each step: p(t|hi , x) = PT p(hi |t, x)p"
W19-5421,P17-2061,0,0.0357315,"asks. We assume throughout that p(t) = T1 , i.e. that tasks are equally likely absent any other information. Weights λk,t define a task-conditional ensemble. At step i, where hi = y1:i−1 is decoding history: p(yi |hi , x) = K X pk (yi |hi , x) T X Figure 1: Adaptively adjusting model weights during decoding with Bayesian Interpolation 1.3 p(t|hi , x)λk,t Transfer learning has been applied to NMT in many forms. Luong and Manning (2015) use transfer learning to adapt a general model to indomain data. Zoph et al. (2016) use multilingual transfer learning to improve NMT for lowresource languages. Chu et al. (2017) introduce mixed fine-tuning, which carries out transfer learning to a new domain combined with some original domain data. Kobus et al. (2017) train a single model on multiple domains using domain tags. Khan et al. (2018) sequentially adapt across multiple biomedical domains to obtain one singledomain model. At inference time, Freitag and Al-Onaizan (2016) use uniform ensembles of general and finetuned models. Our Bayesian Interpolation experiments extend previous work by Allauzen and Riley (2011) on Bayesian Interpolation for language model combination. t=1 k=1 (1) This is an adaptively weigh"
W19-5421,2015.iwslt-evaluation.11,0,0.0653095,"), which also contains a more in-depth derivation and discusses possible hyperparameter configurations. We consider models pk (y|x) trained on K domains, used for T = K domain decoding tasks. We assume throughout that p(t) = T1 , i.e. that tasks are equally likely absent any other information. Weights λk,t define a task-conditional ensemble. At step i, where hi = y1:i−1 is decoding history: p(yi |hi , x) = K X pk (yi |hi , x) T X Figure 1: Adaptively adjusting model weights during decoding with Bayesian Interpolation 1.3 p(t|hi , x)λk,t Transfer learning has been applied to NMT in many forms. Luong and Manning (2015) use transfer learning to adapt a general model to indomain data. Zoph et al. (2016) use multilingual transfer learning to improve NMT for lowresource languages. Chu et al. (2017) introduce mixed fine-tuning, which carries out transfer learning to a new domain combined with some original domain data. Kobus et al. (2017) train a single model on multiple domains using domain tags. Khan et al. (2018) sequentially adapt across multiple biomedical domains to obtain one singledomain model. At inference time, Freitag and Al-Onaizan (2016) use uniform ensembles of general and finetuned models. Our Bay"
W19-5421,L16-1470,0,0.247227,"Missing"
W19-5421,P19-1022,1,0.881224,"Missing"
W19-5421,P16-1162,0,0.128458,"sing SGNMT (Stahlberg et al., 2017). For BI we use 2-gram KENLM models (Heafield, 2011) trained on the source training data for each domain. For validation results we report cased BLEU scores with SacreBLEU (Post, 2018)7 ; test results use case-insensitive BLEU. we additionally reuse strong general domain models trained on the WMT19 news data, including filtered Paracrawl. Details of data preparation and filtering for these models are discussed in Stahlberg et al. (2019). For each language pair we use the same training data in both directions, and use a 32K-merge source-target BPE vocabulary (Sennrich et al., 2016) trained on the ‘base’ domain training data (news for en-de, Scielo health for es-en) For the biomedical data, we preprocess the data using Moses tokenization, punctuation normalization and truecasing. We then use a series of simple heuristics to filter the parallel datasets: • Detected language filtering using the Python langdetect package6 . In addition to mislabelled sentences, this step removes many sentences which are very short or have a high proportion of punctuation or HTML tags. 2.3 Our first experiments involve iterative transfer learning in es2en and en2es to obtain models on three"
W19-5421,D17-2005,1,0.877034,"Cochrane5 1.5K 467 Table 1: Biomedical training and validation data used in the evaluation task. For both language pairs identical data was used in both directions. 2K due to memory constraints. We delay gradient updates by a factor of 8, letting us effectively use a 16K batch size (Saunders et al., 2018). We train each domain model until it fails to improve on the domain validation set in 3 consecutive checkpoints, and perform checkpoint averaging over the final 10 checkpoints to obtain the final model (Junczys-Dowmunt et al., 2016). At inference time we decode with beam size 4 using SGNMT (Stahlberg et al., 2017). For BI we use 2-gram KENLM models (Heafield, 2011) trained on the source training data for each domain. For validation results we report cased BLEU scores with SacreBLEU (Post, 2018)7 ; test results use case-insensitive BLEU. we additionally reuse strong general domain models trained on the WMT19 news data, including filtered Paracrawl. Details of data preparation and filtering for these models are discussed in Stahlberg et al. (2019). For each language pair we use the same training data in both directions, and use a 32K-merge source-target BPE vocabulary (Sennrich et al., 2016) trained on t"
W19-5421,D16-1163,0,0.0185021,"nfigurations. We consider models pk (y|x) trained on K domains, used for T = K domain decoding tasks. We assume throughout that p(t) = T1 , i.e. that tasks are equally likely absent any other information. Weights λk,t define a task-conditional ensemble. At step i, where hi = y1:i−1 is decoding history: p(yi |hi , x) = K X pk (yi |hi , x) T X Figure 1: Adaptively adjusting model weights during decoding with Bayesian Interpolation 1.3 p(t|hi , x)λk,t Transfer learning has been applied to NMT in many forms. Luong and Manning (2015) use transfer learning to adapt a general model to indomain data. Zoph et al. (2016) use multilingual transfer learning to improve NMT for lowresource languages. Chu et al. (2017) introduce mixed fine-tuning, which carries out transfer learning to a new domain combined with some original domain data. Kobus et al. (2017) train a single model on multiple domains using domain tags. Khan et al. (2018) sequentially adapt across multiple biomedical domains to obtain one singledomain model. At inference time, Freitag and Al-Onaizan (2016) use uniform ensembles of general and finetuned models. Our Bayesian Interpolation experiments extend previous work by Allauzen and Riley (2011) on"
W19-5421,W16-2316,0,\N,Missing
W19-5421,W17-4719,0,\N,Missing
W19-5421,W18-6447,0,\N,Missing
W19-5421,W18-1819,0,\N,Missing
W19-5421,W11-2123,0,\N,Missing
W19-5941,C18-3006,0,0.0585947,"Missing"
W19-5941,N15-1020,0,0.0354727,"ecommendation needs. Second, we release a dialog corpus that allows natural language understanding systems to assess how well they interpret user utterances in a conversational context, and promote their more closely mirroring natural dialogue. Finally, we present a brief analysis of user preferences in the movie domain. 2 2.1 Related work Dialog Systems Dialog systems are generally classified as goaldriven or non-goal-driven (Chen et al., 2017). The latter, commonly chatbots, mimic human responses in open domain dialogues, often powered by neural networks trained end-to-end on large corpora (Sordoni et al., 2015; Serban et al., 2016). Goal-driven (a.k.a. task-oriented) systems aim to assist users with specific tasks (e.g., select products). The architecture typically consists of natural language understanding, state tracking, dialogue policy, and language generation (Chen et al., 2018), each often implemented and optimized individually (Young et al., 2013). There is a growing interest in end-to-end trainable task-oriented systems (Bordes and Weston, 2016), yet most are restricted to narrow domains (Serban et al., 2018). Commercial systems, like Google Assistant and Apple’s SIRI, combine chat and task"
W19-5941,H90-1021,0,0.338717,"Missing"
