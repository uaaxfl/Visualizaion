2020.emnlp-demos.26,bethard-etal-2014-cleartk,0,0.0259715,"§2.1.2), an NER processor (Gardner et al., 2018) to find actors and movies from the retrieved reviews, a sentiment processor (Hutto and Gilbert, 2014) for sentence polarity, and an English-German translation processor. A ChatBot Workflow The case study considers the scenario where we have a corpus of movie reviews in English to answer complex queries (e.g., “movies with a positive sentiment starring by a certain actor”) by a 4 Related Work The framework shares some characteristics with UIMA (G¨otz and Suhre, 2004) backed systems, such as DKPro (Eckart de Castilho and Gurevych, 2014), ClearTK (Bethard et al., 2014) and cTakes (Khalifa and Meystre, 2015). There are NLP toolboxes like NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e"
2020.emnlp-demos.26,W16-4011,0,0.0416132,"Missing"
2020.emnlp-demos.26,N13-3004,0,0.0201171,"ike NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e Knowtator (Ogren, 2006), BRAT (Stenetorp et al., 2012), Anafora (Chen and Styler, 2013), GATE (Cunningham et al., 2013), WebAnno (Castilho, 2016), and YEDDA (Yang et al., 2018), our system provides a more flexi201 Figure 4: A system for diagnosis analysis and retrieval from clinical notes. The data-centric approach makes it easy to assemble a variety of components and UI elements. Example text was obtained from UNC School of Medicine. ble experience with customizable plug-ins, extendable data types, and full-fledged NLP support. The Prodigy tool by spaCy is not open-source and supports only pre-defined annotation tasks like NER. 5 Conclusions and Future Work We present a data-ce"
2020.emnlp-demos.26,clarke-etal-2012-nlp,0,0.0133224,"ovie reviews in English to answer complex queries (e.g., “movies with a positive sentiment starring by a certain actor”) by a 4 Related Work The framework shares some characteristics with UIMA (G¨otz and Suhre, 2004) backed systems, such as DKPro (Eckart de Castilho and Gurevych, 2014), ClearTK (Bethard et al., 2014) and cTakes (Khalifa and Meystre, 2015). There are NLP toolboxes like NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e Knowtator (Ogren, 2006), BRAT (Stenetorp et al., 2012), Anafora (Chen and Styler, 2013), GATE (Cunningham et al., 2013), WebAnno (Castilho, 2016), and YEDDA (Yang et al., 2018), our system provides a more flexi201 Figure 4: A system for diagnosis analysis and retrieval from clinical notes. The data-c"
2020.emnlp-demos.26,2020.acl-main.192,0,0.0302599,"hybrid modeling using both neural and symbolic features. Take retrieval for example, the framework offers retrieval processors (§2.2) that retrieve a coarse-grained candidate set with symbolic features (e.g., TF.IDF) first, and then refine the results with more expensive embeddingbased re-ranking (Nogueira and Cho, 2019). Likewise, fast embedding based search is facilitated with the Faiss library (Johnson et al., 2017). Shared modeling approaches. The uniform input/output representation for NLP tasks makes it easy to share the modeling approaches across diverse tasks. For example, similar to Jiang et al. (2020), all tasks involving the Span and Link data types as outputs (e.g., dependency parsing, relation extraction, coreference resolution) can potentially use the exact same neural network architecture for modeling. Further with the standardized APIs of our framework, users can spawn models for all such tasks using the same code with minimal edits. Top right of Figure 2 shows an example where the same relation extractor is implemented with dependency parser for a new task, and the only difference lies in accessing different data features. 2.2 Processors Universal data representation enables a unifo"
2020.emnlp-demos.26,D17-1018,0,0.0183671,"onents. 3 3.1 Case Studies A Clinical Information Workflow We demonstrate an information system for clinical diagnosis analysis, retrieval, and user interaction. Figure 4 shows an overview of the system. To build the workflow, we first define domain-specific data types, such as Clinical Entity Mention, via JSON config files as shown in Figure 2. We then develop processors for text processing: (1) we create an LSTM-based clinical NER processor (Boag et al., 2015), a Span-Relation model based relation extraction processor (He et al., 2018), and a coreference processor with the End-to-End model (Lee et al., 2017) to extract key information; (2) we build a report generation processor following Li et al. (2019) with extracted mentions and relations; (3) we build a simple keyword based dialogue system for user to interact using natural languages. The whole workflow is implemented with minimal engineering effort. For example, the workflow is assembled with just 20 lines of code; and the IE processors are implemented with around 50 lines of code by reusing libraries and models. 3.2 German user. The iterative workflow consists of a review retrieval processor based on the hybrid symbolic-neural feature model"
2020.emnlp-demos.26,P14-5010,0,0.295616,"tion techniques are used to produce summaries from diverse sources. To develop domain-specific NLP systems fast, it is highly desirable to have a unified open-source framework that supports: (1) seamless integration and interoperation across NLP functions ranging from text analysis to retrieval to generation; (2) rich user interaction for data visualization and annotation; (3) extensible plug-ins for customized components; and (4) highly reusable components. A wealth of NLP toolkits exist (§4), such as spaCy (Honnibal and Montani, 2017), DKPro (Eckart de Castilho and Gurevych, 2014), CoreNLP (Manning et al., 2014), for pipelining multiple NLP functions; BRAT (Stenetorp et al., 2012) and YEDDA (Yang et al., 2018) for annotating certain types of data. None of them have addressed all the desiderata uniformly. Combining them for a complete workflow requires non-trivial effort and expertise (e.g., ad-hoc gluing code), posing challenges for maintenance and upgrading. We introduce a new unified framework to support complex NLP workflows that involve text data ingestion, analysis, retrieval, generation, visualization, and annotation. The framework provides an infrastructure to simply plug in arbitrary NLP func"
2020.emnlp-demos.26,N06-4006,0,0.093763,"(Khalifa and Meystre, 2015). There are NLP toolboxes like NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e Knowtator (Ogren, 2006), BRAT (Stenetorp et al., 2012), Anafora (Chen and Styler, 2013), GATE (Cunningham et al., 2013), WebAnno (Castilho, 2016), and YEDDA (Yang et al., 2018), our system provides a more flexi201 Figure 4: A system for diagnosis analysis and retrieval from clinical notes. The data-centric approach makes it easy to assemble a variety of components and UI elements. Example text was obtained from UNC School of Medicine. ble experience with customizable plug-ins, extendable data types, and full-fledged NLP support. The Prodigy tool by spaCy is not open-source and supports only pre-defined annotation ta"
2020.emnlp-demos.26,E12-2021,0,0.310908,"To develop domain-specific NLP systems fast, it is highly desirable to have a unified open-source framework that supports: (1) seamless integration and interoperation across NLP functions ranging from text analysis to retrieval to generation; (2) rich user interaction for data visualization and annotation; (3) extensible plug-ins for customized components; and (4) highly reusable components. A wealth of NLP toolkits exist (§4), such as spaCy (Honnibal and Montani, 2017), DKPro (Eckart de Castilho and Gurevych, 2014), CoreNLP (Manning et al., 2014), for pipelining multiple NLP functions; BRAT (Stenetorp et al., 2012) and YEDDA (Yang et al., 2018) for annotating certain types of data. None of them have addressed all the desiderata uniformly. Combining them for a complete workflow requires non-trivial effort and expertise (e.g., ad-hoc gluing code), posing challenges for maintenance and upgrading. We introduce a new unified framework to support complex NLP workflows that involve text data ingestion, analysis, retrieval, generation, visualization, and annotation. The framework provides an infrastructure to simply plug in arbitrary NLP functions and offers pre-built and reusable components to build desired wo"
2020.emnlp-demos.26,P18-4006,1,0.917441,"ems fast, it is highly desirable to have a unified open-source framework that supports: (1) seamless integration and interoperation across NLP functions ranging from text analysis to retrieval to generation; (2) rich user interaction for data visualization and annotation; (3) extensible plug-ins for customized components; and (4) highly reusable components. A wealth of NLP toolkits exist (§4), such as spaCy (Honnibal and Montani, 2017), DKPro (Eckart de Castilho and Gurevych, 2014), CoreNLP (Manning et al., 2014), for pipelining multiple NLP functions; BRAT (Stenetorp et al., 2012) and YEDDA (Yang et al., 2018) for annotating certain types of data. None of them have addressed all the desiderata uniformly. Combining them for a complete workflow requires non-trivial effort and expertise (e.g., ad-hoc gluing code), posing challenges for maintenance and upgrading. We introduce a new unified framework to support complex NLP workflows that involve text data ingestion, analysis, retrieval, generation, visualization, and annotation. The framework provides an infrastructure to simply plug in arbitrary NLP functions and offers pre-built and reusable components to build desired workflows. Importantly, the fram"
2020.emnlp-main.510,P17-1099,0,0.0241247,"g on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.1 1 Introduction Remarkable progresses have been made in generating generic summaries of documents (Nallapati et al., 2016; See et al., 2017; Narayan et al., 2018), partially due to the large amount of supervision data available. In practice, a document, such as a news article or a medical report, can span multiple topics or aspects. To meet more specific information need in applications such as personalized intelligent assistants, it is often useful to summarize a document with regard to a given aspect, i.e., aspect-based summarization. Recent research has explored the problem of aspect-based abstractive summarization (Krishna and Srinivasan, 2018; Frermann and Klementiev, 2019). A key challenge of the task is the lack of direct"
2020.emnlp-main.510,P16-1009,0,0.0368713,"rcu, 2006; Liu et al., 2012; Xie et al., 2020) which differs from abstract aspects. Incorporating knowledge through weak supervision has primarily been studied in classification or extraction problems (Hu et al., 2016; Peng et al., 2016; Ratner et al., 2017). For example, (Hu et al., 2016) creates soft labels from a logical-rule enhanced teacher model to train neural classifiers. This work explores weak supervisions in the generation setting. Automatic creation of data supervisions also links our work to text data augmentation in either heuristic-based (Wei and Zou, 2019) or automated manner (Sennrich et al., 2016; Hu et al., 2019b). This work embeds rich structured knowledge in the data synthesis process. 3 Approach Given a document and an aspect which can be a word or a phrase, the task aims to generate a summary that concisely describes information in the document that is relevant to the aspect. We present our approach that enables a neural summarization model to summarize on any aspects. The aspect can be any words relevant to (but not necessarily occurring in) the document. Our approach incorporates rich external knowledge sources, including ConceptNet for enriching weak supervisions in training ("
2020.emnlp-main.510,N16-1007,0,0.0271671,"ments that should be included in the summary. Recent work (Frermann and Klementiev, 2019; Krishna and Srinivasan, 2018) on the problem synthesized training data that use news categories as the aspects and thus have a small pre-defined set of aspects available. We aim to enable summarization on any aspects, and develop new weak supervisions by integrating rich external knowledge. Aspect-based summarization has also been explored in the customer reviews domain (Hu and Liu, 2004), where product aspects, customer sentiment, and sometimes textual summaries are extracted (Popescu and Etzioni, 2007; Wang and Ling, 2016; Angelidis and Lapata, 2018). Query-based summarization produces a summary in response to a natural language query/question (Daumé III and Marcu, 2006; Liu et al., 2012; Xie et al., 2020) which differs from abstract aspects. Incorporating knowledge through weak supervision has primarily been studied in classification or extraction problems (Hu et al., 2016; Peng et al., 2016; Ratner et al., 2017). For example, (Hu et al., 2016) creates soft labels from a logical-rule enhanced teacher model to train neural classifiers. This work explores weak supervisions in the generation setting. Automatic c"
2020.emnlp-main.510,D19-1670,0,0.0117019,"anguage query/question (Daumé III and Marcu, 2006; Liu et al., 2012; Xie et al., 2020) which differs from abstract aspects. Incorporating knowledge through weak supervision has primarily been studied in classification or extraction problems (Hu et al., 2016; Peng et al., 2016; Ratner et al., 2017). For example, (Hu et al., 2016) creates soft labels from a logical-rule enhanced teacher model to train neural classifiers. This work explores weak supervisions in the generation setting. Automatic creation of data supervisions also links our work to text data augmentation in either heuristic-based (Wei and Zou, 2019) or automated manner (Sennrich et al., 2016; Hu et al., 2019b). This work embeds rich structured knowledge in the data synthesis process. 3 Approach Given a document and an aspect which can be a word or a phrase, the task aims to generate a summary that concisely describes information in the document that is relevant to the aspect. We present our approach that enables a neural summarization model to summarize on any aspects. The aspect can be any words relevant to (but not necessarily occurring in) the document. Our approach incorporates rich external knowledge sources, including ConceptNet fo"
2020.findings-emnlp.144,D10-1049,0,0.042561,"., 2018; Deriu and Cieliebak, 2018). Yet fine-grained style manipulation is not permitted. This contrasts with the traditional text generation systems which separate content planning and surface realization (Reiter and Dale, 1997), and usually determine the realization with explicit templates (Kukich, 1983; McRoy et al., 2000) or based on syntactic grammars (Robin and McKeown, 1996; Power et al., 2003). Controlling writing style with “hard” templates could suffer from unscalable template creation and lack of generation flexibility. Though previous work (Wiseman et al., 2018; Dou et al., 2018; Angeli et al., 2010) has enabled automatic template extraction, the templates usually act as hard constraints and could harm the content fidelity of generations when the template does not exactly match the content in a record. In this paper, we study a new way of stylistic control in data-to-text generation by using any existing sentences as “soft” templates. That is, we learn to imitate the writing style of a given exemplar sentence. The goal is two-fold: to generate text that not only faithfully describes all content in the record, but also inherits as many of the exemplar’s stylistic characteristics as possibl"
2020.findings-emnlp.144,P18-1015,0,0.0192633,"both content and style in the generation. We conduct empirical studies on corpora from two domains, including restaurant recommendation (Duˇsek et al., 2019) and NBA reports (Wiseman et al., 2017). Experiments show our models strongly improves over a diverse set of comparison methods in terms of both automatic and human evaluations. In particular, given exemplar sentences that match data records to varying degrees, our approach retains a good content-style balance. 2 This highlights the difference from the recent retrievaland-generation work (e.g., Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Peng et al., 2019) which focuses only on content fidelity and thus is a supervised learning problem given the record-description pairs. 2 Related Work Data-to-Text Generation Many efforts have been made to improve the fidelity of generated text to the record content, through sophisticated neural architectures (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019; Iso et al., 2019), hybrid retrieval and generation (Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Pandey et al., 2018; Peng et al., 2019), and others. These approaches do not have the additional goal"
2020.findings-emnlp.144,P19-1599,0,0.0152938,"r research (Subramanian et al., 2019; Logeswaran et al., 2018) has controlled multiple categorical attributes which are largely independent or loosely correlated to each other, a data record in our task, in comparison, can contain a varying number of fields, have many possible values, and are structurally coupled. Our empirical studies (sec 5) show the recent models designed for style transfer fail to perform well on the problem under study. We also note recent work of syntacticallycontrolled paraphrase generation based on either constituency parse (Iyyer et al., 2018) or reference sentences (Chen et al., 2019). The problem nature of data-to-text generation in this work leads to a solution with very different architectures and learning approaches. Controlled Generation without Parallel Data Controlling different aspects (e.g., content, style, discourse structures) in text generation requires grasping the intrinsic mapping between the aspects and the surface text. The lack of parallel data often poses challenges in learning the mapping, making it necessary to incorporate other forms of experiences (supervisions) (Hu and Xing, 2020). For example, the style transfer work (Hu et al., 2017; Shen et al.,"
2020.findings-emnlp.144,W18-6503,0,0.0237881,"I 1 natural language text is generated to describe a given data record such as a box score of a sports player or an infobox table of a restaurant. Though current data-to-text neural approaches with encoder-decoder models could produce fluent text with high fidelity to content (“what to say”), they largely lack control over the writing style, such as sentence structures and word choices (“how to say”). Many efforts have been made to promote the overall diversity in data-to-text generation through, e.g., latent variables (Ye et al., 2020) or customized model architectures (Jagfeld et al., 2018; Deriu and Cieliebak, 2018). Yet fine-grained style manipulation is not permitted. This contrasts with the traditional text generation systems which separate content planning and surface realization (Reiter and Dale, 1997), and usually determine the realization with explicit templates (Kukich, 1983; McRoy et al., 2000) or based on syntactic grammars (Robin and McKeown, 1996; Power et al., 2003). Controlling writing style with “hard” templates could suffer from unscalable template creation and lack of generation flexibility. Though previous work (Wiseman et al., 2018; Dou et al., 2018; Angeli et al., 2010) has enabled au"
2020.findings-emnlp.144,D18-2003,0,0.0181178,"res (Jagfeld et al., 2018; Deriu and Cieliebak, 2018). Yet fine-grained style manipulation is not permitted. This contrasts with the traditional text generation systems which separate content planning and surface realization (Reiter and Dale, 1997), and usually determine the realization with explicit templates (Kukich, 1983; McRoy et al., 2000) or based on syntactic grammars (Robin and McKeown, 1996; Power et al., 2003). Controlling writing style with “hard” templates could suffer from unscalable template creation and lack of generation flexibility. Though previous work (Wiseman et al., 2018; Dou et al., 2018; Angeli et al., 2010) has enabled automatic template extraction, the templates usually act as hard constraints and could harm the content fidelity of generations when the template does not exactly match the content in a record. In this paper, we study a new way of stylistic control in data-to-text generation by using any existing sentences as “soft” templates. That is, we learn to imitate the writing style of a given exemplar sentence. The goal is two-fold: to generate text that not only faithfully describes all content in the record, but also inherits as many of the exemplar’s stylistic char"
2020.findings-emnlp.144,W18-6505,0,0.0173982,"ven exemplar sentences that match data records to varying degrees, our approach retains a good content-style balance. 2 This highlights the difference from the recent retrievaland-generation work (e.g., Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Peng et al., 2019) which focuses only on content fidelity and thus is a supervised learning problem given the record-description pairs. 2 Related Work Data-to-Text Generation Many efforts have been made to improve the fidelity of generated text to the record content, through sophisticated neural architectures (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019; Iso et al., 2019), hybrid retrieval and generation (Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Pandey et al., 2018; Peng et al., 2019), and others. These approaches do not have the additional goal of style control as ours, and usually perform supervised learning based on record-description pairs. Traditional data-to-text generation systems implement a pipeline architecture consisting of separate components, including content planning, sentence planning, and surface realization (e.g., Reiter and Dale, 1997; Kukich, 1983; McRoy et al., 2000; Kondada"
2020.findings-emnlp.144,P16-1154,0,0.0312465,"at it as the reference to reconstruct ye conditioning on (xe , y ˆ). Auxiliary sentence yx is used in an extra auto-encoding loss. • Adversarial Style Transfer (AdvST) (Logeswaran et al., 2018). As another style transfer approach for multiple attributes, the model incorporates back-translation with adversarial training to disentangle content and style representations. Model Configurations We studied both LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) architectures. For LSTM, we use a single layer with the Luong attention (Luong et al., 2015) and copy mechanism (Gu et al., 2016). For Transformer, use the recent copy-augmented variant following (Su et al., 2019) with 3 blocks. During training, we first set (λ = 0, η = 0) to pre-train the model so that it captures the full characteristics of the exemplar sentence. We then switch to (λ = 0.2, η = 1.0) for full training. Adam optimization (Kingma and Ba, 2014) is used with an initial learning rate of 0.001. At inference time, we use beam search with the width 5 and the maximum decoding length 50. 5.3 Automatic Evaluation Metrics Automatic evaluation of the task is an open and challenging problem. We use several quantitat"
2020.findings-emnlp.144,N18-1170,0,0.029039,"particular, though the recent style transfer research (Subramanian et al., 2019; Logeswaran et al., 2018) has controlled multiple categorical attributes which are largely independent or loosely correlated to each other, a data record in our task, in comparison, can contain a varying number of fields, have many possible values, and are structurally coupled. Our empirical studies (sec 5) show the recent models designed for style transfer fail to perform well on the problem under study. We also note recent work of syntacticallycontrolled paraphrase generation based on either constituency parse (Iyyer et al., 2018) or reference sentences (Chen et al., 2019). The problem nature of data-to-text generation in this work leads to a solution with very different architectures and learning approaches. Controlled Generation without Parallel Data Controlling different aspects (e.g., content, style, discourse structures) in text generation requires grasping the intrinsic mapping between the aspects and the surface text. The lack of parallel data often poses challenges in learning the mapping, making it necessary to incorporate other forms of experiences (supervisions) (Hu and Xing, 2020). For example, the style tr"
2020.findings-emnlp.144,W18-6529,0,0.0214795,"thub.com/ha-lins/DTG-SI 1 natural language text is generated to describe a given data record such as a box score of a sports player or an infobox table of a restaurant. Though current data-to-text neural approaches with encoder-decoder models could produce fluent text with high fidelity to content (“what to say”), they largely lack control over the writing style, such as sentence structures and word choices (“how to say”). Many efforts have been made to promote the overall diversity in data-to-text generation through, e.g., latent variables (Ye et al., 2020) or customized model architectures (Jagfeld et al., 2018; Deriu and Cieliebak, 2018). Yet fine-grained style manipulation is not permitted. This contrasts with the traditional text generation systems which separate content planning and surface realization (Reiter and Dale, 1997), and usually determine the realization with explicit templates (Kukich, 1983; McRoy et al., 2000) or based on syntactic grammars (Robin and McKeown, 1996; Power et al., 2003). Controlling writing style with “hard” templates could suffer from unscalable template creation and lack of generation flexibility. Though previous work (Wiseman et al., 2018; Dou et al., 2018; Angeli"
2020.findings-emnlp.144,P13-1138,0,0.0186777,"., 2018; Puduppully et al., 2019; Iso et al., 2019), hybrid retrieval and generation (Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Pandey et al., 2018; Peng et al., 2019), and others. These approaches do not have the additional goal of style control as ours, and usually perform supervised learning based on record-description pairs. Traditional data-to-text generation systems implement a pipeline architecture consisting of separate components, including content planning, sentence planning, and surface realization (e.g., Reiter and Dale, 1997; Kukich, 1983; McRoy et al., 2000; Kondadadi et al., 2013). Recent work (Wiseman et al., 2018) integrates the template use in a more end-to-end neural model. Rather than treating templates as hard constraints as in the previous work, we study the new setting of using existing sentences as exemplars, allowing the model to adaptively imitate the style while ensuring content fidelity. Text Style Transfer There has been growing interest in text style transfer (Hu et al., 2017; Shen et al., 2017; Yang et al., 2018; Subramanian et al., 2019, etc) which assumes an existing sentence of certain content, and modifies single or multiple textual attributes (e.g."
2020.findings-emnlp.144,P83-1022,0,0.726294,"say”), they largely lack control over the writing style, such as sentence structures and word choices (“how to say”). Many efforts have been made to promote the overall diversity in data-to-text generation through, e.g., latent variables (Ye et al., 2020) or customized model architectures (Jagfeld et al., 2018; Deriu and Cieliebak, 2018). Yet fine-grained style manipulation is not permitted. This contrasts with the traditional text generation systems which separate content planning and surface realization (Reiter and Dale, 1997), and usually determine the realization with explicit templates (Kukich, 1983; McRoy et al., 2000) or based on syntactic grammars (Robin and McKeown, 1996; Power et al., 2003). Controlling writing style with “hard” templates could suffer from unscalable template creation and lack of generation flexibility. Though previous work (Wiseman et al., 2018; Dou et al., 2018; Angeli et al., 2010) has enabled automatic template extraction, the templates usually act as hard constraints and could harm the content fidelity of generations when the template does not exactly match the content in a record. In this paper, we study a new way of stylistic control in data-to-text generatio"
2020.findings-emnlp.144,D15-1166,0,0.0173865,"ˆ conditioning on (x, ye ), and then treat it as the reference to reconstruct ye conditioning on (xe , y ˆ). Auxiliary sentence yx is used in an extra auto-encoding loss. • Adversarial Style Transfer (AdvST) (Logeswaran et al., 2018). As another style transfer approach for multiple attributes, the model incorporates back-translation with adversarial training to disentangle content and style representations. Model Configurations We studied both LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) architectures. For LSTM, we use a single layer with the Luong attention (Luong et al., 2015) and copy mechanism (Gu et al., 2016). For Transformer, use the recent copy-augmented variant following (Su et al., 2019) with 3 blocks. During training, we first set (λ = 0, η = 0) to pre-train the model so that it captures the full characteristics of the exemplar sentence. We then switch to (λ = 0.2, η = 1.0) for full training. Adam optimization (Kingma and Ba, 2014) is used with an initial learning rate of 0.001. At inference time, we use beam search with the width 5 and the maximum decoding length 50. 5.3 Automatic Evaluation Metrics Automatic evaluation of the task is an open and challeng"
2020.findings-emnlp.144,W00-1437,0,0.463742,"Missing"
2020.findings-emnlp.144,P18-1123,0,0.021262,"ent retrievaland-generation work (e.g., Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Peng et al., 2019) which focuses only on content fidelity and thus is a supervised learning problem given the record-description pairs. 2 Related Work Data-to-Text Generation Many efforts have been made to improve the fidelity of generated text to the record content, through sophisticated neural architectures (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019; Iso et al., 2019), hybrid retrieval and generation (Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Pandey et al., 2018; Peng et al., 2019), and others. These approaches do not have the additional goal of style control as ours, and usually perform supervised learning based on record-description pairs. Traditional data-to-text generation systems implement a pipeline architecture consisting of separate components, including content planning, sentence planning, and surface realization (e.g., Reiter and Dale, 1997; Kukich, 1983; McRoy et al., 2000; Kondadadi et al., 2013). Recent work (Wiseman et al., 2018) integrates the template use in a more end-to-end neural model. Rather than treating templates as hard constr"
2020.findings-emnlp.144,N19-1263,0,0.0132146,"tyle in the generation. We conduct empirical studies on corpora from two domains, including restaurant recommendation (Duˇsek et al., 2019) and NBA reports (Wiseman et al., 2017). Experiments show our models strongly improves over a diverse set of comparison methods in terms of both automatic and human evaluations. In particular, given exemplar sentences that match data records to varying degrees, our approach retains a good content-style balance. 2 This highlights the difference from the recent retrievaland-generation work (e.g., Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Peng et al., 2019) which focuses only on content fidelity and thus is a supervised learning problem given the record-description pairs. 2 Related Work Data-to-Text Generation Many efforts have been made to improve the fidelity of generated text to the record content, through sophisticated neural architectures (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019; Iso et al., 2019), hybrid retrieval and generation (Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Pandey et al., 2018; Peng et al., 2019), and others. These approaches do not have the additional goal of style control as"
2020.findings-emnlp.144,P19-1195,0,0.0349339,"Missing"
2020.findings-emnlp.144,P19-1003,0,0.0117719,"nce yx is used in an extra auto-encoding loss. • Adversarial Style Transfer (AdvST) (Logeswaran et al., 2018). As another style transfer approach for multiple attributes, the model incorporates back-translation with adversarial training to disentangle content and style representations. Model Configurations We studied both LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) architectures. For LSTM, we use a single layer with the Luong attention (Luong et al., 2015) and copy mechanism (Gu et al., 2016). For Transformer, use the recent copy-augmented variant following (Su et al., 2019) with 3 blocks. During training, we first set (λ = 0, η = 0) to pre-train the model so that it captures the full characteristics of the exemplar sentence. We then switch to (λ = 0.2, η = 1.0) for full training. Adam optimization (Kingma and Ba, 2014) is used with an initial learning rate of 0.001. At inference time, we use beam search with the width 5 and the maximum decoding length 50. 5.3 Automatic Evaluation Metrics Automatic evaluation of the task is an open and challenging problem. We use several quantitative metrics for the two goals of the task, namely content fidelity and style embodim"
2020.findings-emnlp.144,2020.emnlp-main.510,1,0.721335,"iscourse structures) in text generation requires grasping the intrinsic mapping between the aspects and the surface text. The lack of parallel data often poses challenges in learning the mapping, making it necessary to incorporate other forms of experiences (supervisions) (Hu and Xing, 2020). For example, the style transfer work (Hu et al., 2017; Shen et al., 2017; Yang et al., 2018) used auxiliary models such as attribute classifiers and language models for supervision signals. Tang et al. (2019) learned guided conversation flow using standard conversation data combined with logical control. Tan et al. (2020) created weak supervision labels from knowledge bases for aspect-based summarization. This work devises competing training objectives based on common record-description pairs. Joint optimization of the competing objectives drives the model to learn desired behaviors. 3 The Task: Data-to-Text Generation with Style Imitation For clarity, we first formally describe the problem of data-to-text generation with style imitation. We also establish the key notations used in the paper. Consider a data record x which consists of a set of fields and their values (e.g., field “Food” and its value “Italian”"
2020.findings-emnlp.144,W18-5713,0,0.0116018,"lanced embodiment of both content and style in the generation. We conduct empirical studies on corpora from two domains, including restaurant recommendation (Duˇsek et al., 2019) and NBA reports (Wiseman et al., 2017). Experiments show our models strongly improves over a diverse set of comparison methods in terms of both automatic and human evaluations. In particular, given exemplar sentences that match data records to varying degrees, our approach retains a good content-style balance. 2 This highlights the difference from the recent retrievaland-generation work (e.g., Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Peng et al., 2019) which focuses only on content fidelity and thus is a supervised learning problem given the record-description pairs. 2 Related Work Data-to-Text Generation Many efforts have been made to improve the fidelity of generated text to the record content, through sophisticated neural architectures (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019; Iso et al., 2019), hybrid retrieval and generation (Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Pandey et al., 2018; Peng et al., 2019), and others. These approaches do not have t"
2020.findings-emnlp.144,D17-1239,0,0.0873282,"only have access to abundant record-description pairs2 . The proposed approach learns with rich weak supervisions derived from the record-description pairs. Architecture-wise, we develop a hybrid attentioncopy mechanism that offers differentiated treatments of the content and style sources. Further, based on the structural nature of data records, we devise a new content coverage constraint for the balanced embodiment of both content and style in the generation. We conduct empirical studies on corpora from two domains, including restaurant recommendation (Duˇsek et al., 2019) and NBA reports (Wiseman et al., 2017). Experiments show our models strongly improves over a diverse set of comparison methods in terms of both automatic and human evaluations. In particular, given exemplar sentences that match data records to varying degrees, our approach retains a good content-style balance. 2 This highlights the difference from the recent retrievaland-generation work (e.g., Hashimoto et al., 2018; Weston et al., 2018; Cao et al., 2018; Peng et al., 2019) which focuses only on content fidelity and thus is a supervised learning problem given the record-description pairs. 2 Related Work Data-to-Text Generation Man"
2020.findings-emnlp.144,D18-1356,0,0.0786416,"mized model architectures (Jagfeld et al., 2018; Deriu and Cieliebak, 2018). Yet fine-grained style manipulation is not permitted. This contrasts with the traditional text generation systems which separate content planning and surface realization (Reiter and Dale, 1997), and usually determine the realization with explicit templates (Kukich, 1983; McRoy et al., 2000) or based on syntactic grammars (Robin and McKeown, 1996; Power et al., 2003). Controlling writing style with “hard” templates could suffer from unscalable template creation and lack of generation flexibility. Though previous work (Wiseman et al., 2018; Dou et al., 2018; Angeli et al., 2010) has enabled automatic template extraction, the templates usually act as hard constraints and could harm the content fidelity of generations when the template does not exactly match the content in a record. In this paper, we study a new way of stylistic control in data-to-text generation by using any existing sentences as “soft” templates. That is, we learn to imitate the writing style of a given exemplar sentence. The goal is two-fold: to generate text that not only faithfully describes all content in the record, but also inherits as many of the exempla"
2021.acl-short.112,2020.acl-main.740,0,0.04411,"Missing"
2021.acl-short.90,W03-0501,0,0.394269,"Missing"
2021.acl-short.90,P03-1054,0,0.0423435,"Missing"
2021.emnlp-main.136,D19-1403,0,0.0257264,"I. replacing the aggrega4.2 Experimental Settings tor AGGkf with a simple feature concatenator; II. For supervised adaptation, we compare KGML removing extra edges in KG, which are introduced on five recent meta-learning algorithms, includ- by k-nearest neighbor graph. The performance of ing MAML (Finn et al., 2017), ProtoNet (Snell each ablation model and the KGML of Amazon et al., 2017), Matching Network (Vinyals et al., and Huffpost are reported in Table 2. We observe 2016) (MatchingNet), REGRAB (Qu et al., 2020), that (1) KGML outperforms model I, demonstratInduction Network (InductNet) (Geng et al., 2019). ing the effectiveness of the designed aggregator; We conduct the experiments under 1-shot and 5- (2) Comparing between KGML with model II, the shot settings and report the results of KGML with results show that KNN boosts performance. One gradient-based meta-learning (KGML-MAML) potential reason is that KNN densifies the whole 1817 Table 1: Performance for supervised and unsupervised adaptation methods. We report the averaged accuracy over 600 tasks (supervised adaptation)/all meta-testing users (unsupervised adaptation). Supervised Adaptation Amazon Review Huffpost 1-shot 5-shot 1-shot 5-sh"
2021.emnlp-main.136,D18-1398,0,0.0280061,"sks (e.g., Fig. 1). These representations are computed using a graph neural network (GNN) which is meta-trained endto-end jointly with the text classification model. Our approach is compatible with both supervised and unsupervised adaptation of predictive models. Learning-to-learn (or meta-learning) (Bengio et al., 1990; Schmidhuber, 1992; Hochreiter et al., 2001; Vinyals et al., 2016; Finn et al., 2017) has recently emerged as a successful technique for training models on large collections of low-resource tasks. In the natural language domain, it has been used to improve machine translation (Gu et al., 2018), semantic parsing (Sun et al., 2020), text classification (Bao et al., 2019; Geng et al., 2020, 2019; Li et al., 2020), sequence labelling (Li et al., 2021), text Related work. In modern meta-learning, there generation (Guo et al., 2020), knowledge graph are two broad categories of methods: (i) gradientreasoning (Wang et al., 2019), among many other based (Finn et al., 2017; Nichol and Schulman, applications in low-resource settings. 2018; Li et al., 2017; Zhang et al., 2020; ZintMeta-learning has been shown to dominate self- graf et al., 2019; Lee and Choi, 2018; Yao et al., supervised pretr"
2021.emnlp-main.136,D19-1018,0,0.0451139,"Missing"
2021.emnlp-main.599,2020.acl-main.703,0,0.0238792,"luate metrics using human the single aggregated alignment score such as annotations from two commonly-used sources: (1) mean (align(a → b)) (or sum). This is because SummEval (Fabbri et al., 2021) on the CNN/DM 7585 summarization dataset (Hermann et al., 2015; Nallapati et al., 2016). The annotation dataset contains 1,600 examples from 16 summarization systems; (2) QAGS (Wang et al., 2020) (which names the aspect “correctness”) on the XSUM dataset (Narayan et al., 2018), another summarization task with strong abstractive property. The dataset contains 235 outputs from a fine-tuned BART model (Lewis et al., 2020). The QAGS dataset also contains another 239 outputs for CNN/DM, for which we report results in Table D.4 in the appendix. For relevance, we test our metric on the respective annotations from SummEval on CNN/DM. Baselines and Setup For baselines, we include commonly-used metrics reported in previous papers, ranging from reference-based metric, such as ROUGE, BLEU and BERTScore (Zhang et al., 2020a), to reference-free ones, such as SummaQA (Scialom et al., 2019) based on QA and FactCC (Kry´sci´nski et al., 2019) based on sentence classification. For our metrics, we use RoBERTa-large for the emb"
2021.emnlp-main.599,W04-1013,0,0.264465,"valuation is often prohibitively expensive and slow, while accurate automatic evaluation is challenging given the complexity of text modeling and the diverse aspects to be measured for different NLG tasks. Previous work has developed a large variety of 1 Introduction automatic metrics. A popular general strategy is to Natural language generation (NLG) refers to the measure the similarity of generated text against broad set of tasks that produce fluent text from human-written references, such as the classical input data and other contextual information. The BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), diverse tasks serve for vastly different uses in prac- and more recent variants based on neural modtice. For example, summarization compresses a els (e.g., Zhang et al., 2020a; Sellam et al., 2020). source article into a short paragraph containing the However, an NLG task typically involves multiple most important information; translation transduces desirable properties (e.g., consistency, conciseness, content expressed in one language into another; richness) that may have different priorities and need and a chatbot creates novel responses to drive the trade-off depending on the application"
2021.emnlp-main.599,2020.acl-main.448,0,0.106158,"ootball fan Transduction (e.g., style transfer) If you’d be so kind, could you pass the salt, please? Gimme your salt right this minute! Creation (e.g., dialog) I bought my house when I turned 19. That is young! You must be rich. Sadly I still rent my home and have to pay monthly. Figure 1: Illustration of three categories of NLG tasks in terms of information change. Task input is in blue box and output in orange box. Text in red in the dialog output box represents newly created information. for NLG tasks. However, evaluation of NLG has long been considered difficult (Kryscinski et al., 2019; Mathur et al., 2020): human evaluation is often prohibitively expensive and slow, while accurate automatic evaluation is challenging given the complexity of text modeling and the diverse aspects to be measured for different NLG tasks. Previous work has developed a large variety of 1 Introduction automatic metrics. A popular general strategy is to Natural language generation (NLG) refers to the measure the similarity of generated text against broad set of tasks that produce fluent text from human-written references, such as the classical input data and other contextual information. The BLEU (Papineni et al., 2002)"
2021.emnlp-main.599,N19-1170,0,0.0216726,"nerates output that adds on top a to b: it does not measure how b aligns to a. We of input (e.g., dialog history) new information (e.g., next show how the alignment scores can be used from external knowledge). Information alignment to define intuitive metrics for various tasks. Bebetween the output, input, and external sources is sides, the fine-grained alignment scores also offer thus essential for evaluating how well the created a certain level of interpretability for the resulting content engages with the context (Venkatesh et al., metrics, as illustrated by the example in Table C.1. 2018; See et al., 2019) and how meaningful the con3.2 Evaluation of “Compression” Tasks tent is by grounding to the external sources (Dinan et al., 2019a; Smith et al., 2020). We discuss compression evaluation in the context From the above perspective, information align- of text summarization, an extensively studied task ment arises as a common central component that for evaluation in previous work. The task aims to connects evaluations across the tasks. A single ac- extract the most important information from doccurate alignment prediction model would enable ument x and express it in summary y. As above, us to reli"
2021.emnlp-main.599,2020.acl-main.704,0,0.271089,"erent NLG tasks. Previous work has developed a large variety of 1 Introduction automatic metrics. A popular general strategy is to Natural language generation (NLG) refers to the measure the similarity of generated text against broad set of tasks that produce fluent text from human-written references, such as the classical input data and other contextual information. The BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), diverse tasks serve for vastly different uses in prac- and more recent variants based on neural modtice. For example, summarization compresses a els (e.g., Zhang et al., 2020a; Sellam et al., 2020). source article into a short paragraph containing the However, an NLG task typically involves multiple most important information; translation transduces desirable properties (e.g., consistency, conciseness, content expressed in one language into another; richness) that may have different priorities and need and a chatbot creates novel responses to drive the trade-off depending on the application scenarios conversation. Recent years have seen remarkably (Hashimoto et al., 2019; Mir et al., 2019; Mehri and fast progress in improving and making new models Eskenazi, 2020b; Gehrmann et al., 2021)"
2021.emnlp-main.599,2020.acl-main.183,0,0.050123,"w the alignment scores can be used from external knowledge). Information alignment to define intuitive metrics for various tasks. Bebetween the output, input, and external sources is sides, the fine-grained alignment scores also offer thus essential for evaluating how well the created a certain level of interpretability for the resulting content engages with the context (Venkatesh et al., metrics, as illustrated by the example in Table C.1. 2018; See et al., 2019) and how meaningful the con3.2 Evaluation of “Compression” Tasks tent is by grounding to the external sources (Dinan et al., 2019a; Smith et al., 2020). We discuss compression evaluation in the context From the above perspective, information align- of text summarization, an extensively studied task ment arises as a common central component that for evaluation in previous work. The task aims to connects evaluations across the tasks. A single ac- extract the most important information from doccurate alignment prediction model would enable ument x and express it in summary y. As above, us to reliably evaluate many relevant aspects in consistency and relevance have been widely idenvarious applications. tified as key aspects to characterize the c"
2021.emnlp-main.599,2020.acl-main.450,0,0.188582,", 2019; Mehri and fast progress in improving and making new models Eskenazi, 2020b; Gehrmann et al., 2021). Thus a 1 single score without multi-aspect interpretability is Code available at https://github.com/ tanyuqian/ctc-gen-eval often inadequate to characterize generation quality. 7580 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7580–7605 c November 7–11, 2021. 2021 Association for Computational Linguistics A growing number of recent works have proposed aspect-based metrics for popular tasks such as summarization (Kry´sci´nski et al., 2019; Wang et al., 2020) and dialog (Mehri and Eskenazi, 2020b; Nie et al., 2020). Those metrics are typically each designed for individual tasks and aspects, based on specific intuitions. The lack of a common theoretical ground makes it difficult to share the evaluation strengths across the diverse NLG problems, and fails to offer guidance to metric design for emerging tasks and aspects. In this paper, we propose a more unifying perspective of NLG evaluation through the lens of information change, which offers a general framework to measure many key aspects of NLG tasks. In particular, based on the practical use of"
2021.emnlp-main.599,P18-1205,0,0.023644,"e diverse metrics studied in (Mehri and Eskenazi, 2020b) and FED (Mehri and Eskenazi, 2020a), a set of latest unsupervised dialogue metrics based on the DialoGPT model (Zhang et al., 2020b). We 4.3 Experiments for “Creation” Metrics use FED-Interesting from the original paper deDatasets For the engagingness aspect, we use the signed for engagingness and FED-Informative delatest human annotation data collected by (Mehri signed for groundedness, respectively. We also add and Eskenazi, 2020b) (which names the aspect “in- a particularly simple baseline—response length, teresting”) on PersonaChat (Zhang et al., 2018) which as we show performs surprisingly well. For 7587 XSum Consistency 0.5 Topical-Chat Groundedness 0.36 Pearson 0.45 0.4 Pearson 0.35 Spearman + Human Labels 0.35 0.33 0.3 0.32 0.25 Spearman 0.34 0.31 0.58 0.63 ... 0.78 Alignment Estimation Accuracy 0.81 0.83 0.85 gates token-level predictions with more structure. We provide more empirical studies in Appendix §F. In particular, we found that besides the two core aspects, our alignment based method also achieves stronger human correlations than existing metrics on other dialog aspects, such as the understandability and naturalness of respons"
2021.emnlp-main.599,P19-1539,0,0.0554017,"tions of information alignment. Note that the two-way alignments differ from the “consistency” and “relevance” metrics in compression where we have only required output y to align with input x. Our experiments show that it is crucial to account for alignments in both directions for transduction (§4.2). 3.4 Evaluation of “Creation” Tasks We formulate aspects of creation tasks using the example of knowledge-grounded dialog generation. In this task, an agent generates text y as a response to conversation history x while exhibiting information from knowledge context c, e.g., an external document (Qin et al., 2019; Guo et al., 2018) or a set of facts (Dinan et al., 2019b; Zhang et al., R ELEVANCE(y, x, r) = (3) 2018). For the agent, sustaining an engaging conmean (align(r → y)) × mean (align(y → x)) , versation is considered an essential skill (Venkatesh which is the product of both components. Tradi- et al., 2018; Guo et al., 2018; Mehri and Eskenazi, tional reference-based metrics consider only the 2020b). Besides, the generated response must be reference text (rather than the input). For example, grounded in the knowledge context by referring to ROUGE (Lin, 2004) can be seen as measuring the its inf"
2021.emnlp-main.599,D19-1320,0,0.127914,"scinski et al., 2019), which dictates that the summary y should only contain information from x (instead of other sources or hallucinations). The aspect is also referred to as “factual correctness” or “faithfulness” in previous work2 . For y to be fully consistent, all tokens in y should align with x. Therefore, we can straightforwardly devise the consistency metric based on the information alignment defined above: C ONSISTENCY(y, x) = mean (align(y → x)) , (2) which is the average alignment scores of tokens in y w.r.t. x. Our metric offers a simpler solution than the recent QA-based metrics (Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020) that compare the answers extracted from y and x by a Question-Answering system, and is more interpretable than the black-box consistency classification models (Falke et al., 2019; Kry´sci´nski et al., 2019; Maynez et al., 2020). We also achieve stronger empirical performance (§4.1). Relevance As one of the most heavily studied aspects of summarization, relevance concerns how well the summary y retains important information in x (Nenkova and Passonneau, 2004; Zopf et al., 2016). As in previous work, the “importance” of information can be determined by h"
2021.emnlp-main.599,D19-1053,0,0.153296,"elp relations, which is likely due to the higher abstracsentiment transfer data (Shen et al., 2017)3 . The tiveness of XSUM summaries that renders embeddataset contains 8,784 outputs from 12 systems. ding matching inadequate. The sentence-classifier based FactCC metric (Kry´sci´nski et al., 2019), Baselines and Setup We compare with the comwhich is trained to distinguish paraphrases from mon metrics used previously (Mir et al., 2019), and artificially perturbed sentences, also achieves a further include BERTScore (Zhang et al., 2020a), decent correlation on XSUM. However, it seems MoverScore (Zhao et al., 2019) and BLEURT (Selunable to effectively model the summaries on lam et al., 2020), the latest neural text similarity CNN/DM that tend to be longer and richer in infor- metrics. We use BLEURT out-of-the-box as we mation, and thus produces a lower correlation. do not assume access to human scores for fine3 Figure 4 shows the results for relevance on It is arguable whether “sentiment” is part of style (Krishna CNN/DM. Our metrics strongly outperform all et al., 2020). Here we just use the most common dataset. 7586 Engagingness (TopicalChat) Engagingness (PersonaChat) 0.70 0.60 Baselines 0.50 E Ours"
2021.emnlp-main.599,K16-1009,0,0.355664,"new framework that offers a common foundation for characterizing diverse NLG tasks and leads to a set of interpretable metrics for evaluating their key aspects. As discussed in §1, NLG tasks can be categorized as performing compression, transduction, or creation based on changes in conveyed information from input to output. For a compression task (e.g., summarization), the goal is to concisely describe the most important information in the input (e.g., a document). That is, the output should only contain content from the input, namely “consistency” (Cao et al., 2018; Kryscinski et al., 2019; Zopf et al., 2016; Peyrard, 2019), and the included content must be salient, namely “relevance” (Nenkova and Passonneau, 2004; Zopf et al., 2016). Intuitively, with an “information alignment” measure that as- Definition 3.1 (Information Alignment). Let a be a piece of text of length N ; b be arbitrary data. The sesses how the information in a generated output information alignment from text a to b is a vector overlaps with that in the input (and in references that offer clues for salience), we can readily evalu- of alignment scores: align(a → b) = hα1 , α2 , . . . , αN i, (1) ate the two key aspects. The same"
2021.findings-acl.46,N19-1245,0,0.013423,"d on limited handcraft rules and were only validated on small-scale datasets, making it hard to generalize to more complex and real-world cases. Besides, the solving process is sophisticated, which means it is difficult for a human to understand and examine its reliability. To refresh the research on geometric problem solving and promote further study on multimodal numerical reasoning, we propose a large-scale realworld geometric question answering dataset called GeoQA, which contains 5,010 multiple-choice geometric problems collected from real math exams in Chinese middle school. Inspired by Amini et al. (2019), we additionally introduce a new domainspecific language to model precise operation programs corresponding to the geometry problem. These executable programs represent the numerical reasoning steps of geometry problems. Compared with the existing dataset GeoS and GeoS++ (Seo et al., 2015a; Sachan et al., 2017), our GeoQA is larger, more diverse, provides additional program annotation, thus serves as a promising benchmark to improve both generalization and interpretability of the multimodal numerical reasoning approaches. Moreover, we propose the first deep learningbased approach for geometry"
2021.findings-acl.46,P16-1084,0,0.431874,"1: Illustration of a typical geometry problem with the annotated programs in our GeoQA dataset. Introduction In recent years, developing machine learning systems to solve math word problems (MWPs) automatically has attracted increasing attention due to its high academic value and the great application potential in smart education (Bajaj and Sharma, ∗ † Equal contribution. Corresponding author. D. 8 2018; Lin et al., 2018). Most of the existing methods focus on solving arithmetic and algebraic problems, including traditional machine learning approaches (Kushman et al., 2014; Zhou et al., 2015; Huang et al., 2016) and network-based models (Wang et al., 2017, 2018; Xie and Sun, 2019), while solving geometric problems has been rarely investigated (Seo et al., 2014, 2015a; Sachan et al., 2017). As a classic math problem, geometry dominates a large portion of secondary education. Due to its challenges and data characteristics, geometry problem can also serve as a multimodal numerical reasoning benchmark requiring joint reasoning over diagram and text. As shown in Figure 1, a typical geometric question mainly consists of textual descriptions and geometric diagrams. Compared with math word problems, which on"
2021.findings-acl.46,P14-1026,0,0.151975,"Step3: Double(V1) = 2ൈ4 = 8 (V2) Figure 1: Illustration of a typical geometry problem with the annotated programs in our GeoQA dataset. Introduction In recent years, developing machine learning systems to solve math word problems (MWPs) automatically has attracted increasing attention due to its high academic value and the great application potential in smart education (Bajaj and Sharma, ∗ † Equal contribution. Corresponding author. D. 8 2018; Lin et al., 2018). Most of the existing methods focus on solving arithmetic and algebraic problems, including traditional machine learning approaches (Kushman et al., 2014; Zhou et al., 2015; Huang et al., 2016) and network-based models (Wang et al., 2017, 2018; Xie and Sun, 2019), while solving geometric problems has been rarely investigated (Seo et al., 2014, 2015a; Sachan et al., 2017). As a classic math problem, geometry dominates a large portion of secondary education. Due to its challenges and data characteristics, geometry problem can also serve as a multimodal numerical reasoning benchmark requiring joint reasoning over diagram and text. As shown in Figure 1, a typical geometric question mainly consists of textual descriptions and geometric diagrams. Co"
2021.findings-acl.46,2020.acl-main.238,0,0.0365672,"oning steps via the program sequence in favor of the model diagnosis. We further design three highly-relevant pretext tasks to enhance text-diagram semantic representation, including diagram jigsaw location prediction, diagram geometric element prediction, and knowledge point prediction. Extensive experiments are conducted on GeoQA benchmark, and the quantitative comparisons show the superiority of the proposed NGS and auxiliary tasks over other multimodal baselines. In summary, our contributions are three-fold: Though some previous methods (Seo et al., 2014, 2015a; Sachan et al., 2017, 2020; Sachan, 2020) attempt to resolve the mentioned issues, the performance of their geometric problem solving systems is far away from satisfactory. They highly depended on limited handcraft rules and were only validated on small-scale datasets, making it hard to generalize to more complex and real-world cases. Besides, the solving process is sophisticated, which means it is difficult for a human to understand and examine its reliability. To refresh the research on geometric problem solving and promote further study on multimodal numerical reasoning, we propose a large-scale realworld geometric question answer"
2021.findings-acl.46,D17-1081,1,0.886392,"Missing"
2021.findings-acl.46,S17-1029,1,0.855572,"lernter et al., 1960; WenTsun, 1986; Chou et al., 1996; Ye et al., 2008). For example, Seo et al. (2014, 2015a) built the first automated system, GeoS, to solve SAT style geometry problems. GeoS used NLP and computer vision techniques (e.g., OCR) to parse a geometry problem’s text and diagram jointly as logic forms. However, this system highly depended on the manually designed logic forms and was only examined in a small dataset with 185 problems. Besides, the limited logic forms are hard to cover various geometry problems, leading to low generalization. To improve GeoS, Sachan et al. (2017); Sachan and Xing (2017) replaced these handcraft constraints with geometry axiomatic knowledge in the form of horn-clause rules, but their dataset and code are not released. To boost the generalization and interpretability of existing works, we propose a largescale GeoQA benchmark, which is 25 times larger 514 than the only public dataset Seo et al. (2015a), and provides program annotation. Multimodal Reasoning Visual question answering is a representative multimodal task that requires the model to have reasoning ability (Goyal et al., 2017; Yu et al., 2019). Johnson et al. (2017) built a new diagnostic VQA dataset"
2021.findings-acl.46,2020.acl-main.92,0,0.026422,"t the correct answer ci ∈ c. To collect as much useful information as possible, we also provide the natural language-based problem solving explanations e, problem type t, the related knowledge points k, and our annotated programs p for each problem. Therefore, a geometry problem can be represented as (t, d, c, i, e, t, k, p). Fig. 1 shows an example of geometric problems. Moreover, there are three problem types in our GeoQA, i.e., angle calculation, length calculation, and others which contain various types of problems such as area calculation. We adopt the corpus diversity metric proposed by Miao et al. (2020) to evaluate the diversity of GeoQA. The result is 0.47, which is relatively high compared with other math problem datasets, indicating that our dataset is diverse. The source data already contains manually tagged knowledge points in each problem, we design rule-based regular expressions to normalize the original knowledge points to 50 categories. We split our GeoQA into three subsets – train set, valid set, and test set, in a ratio of 7.0: 1.5: 1.5. The data statistics of our GeoQA are shown in Table 1. 3.2 Program Representation The neural network has proved to be a powerful tool to address"
2021.findings-acl.46,D15-1171,0,0.190561,"search on geometric problem solving and promote further study on multimodal numerical reasoning, we propose a large-scale realworld geometric question answering dataset called GeoQA, which contains 5,010 multiple-choice geometric problems collected from real math exams in Chinese middle school. Inspired by Amini et al. (2019), we additionally introduce a new domainspecific language to model precise operation programs corresponding to the geometry problem. These executable programs represent the numerical reasoning steps of geometry problems. Compared with the existing dataset GeoS and GeoS++ (Seo et al., 2015a; Sachan et al., 2017), our GeoQA is larger, more diverse, provides additional program annotation, thus serves as a promising benchmark to improve both generalization and interpretability of the multimodal numerical reasoning approaches. Moreover, we propose the first deep learningbased approach for geometry problem solving, named as Neural Geometric Solver (NGS). It applies a co-attention mechanism to fuse the representation of text and diagram, and predicts the explainable programs based on the cross-modal representation. These sequential programs can be executed to obtain a final answer. B"
2021.findings-acl.46,D18-1132,0,0.427396,"Missing"
2021.findings-acl.46,D17-1088,0,0.345038,"Missing"
2021.findings-acl.46,D15-1096,0,0.507999,"Missing"
2021.naacl-main.341,W19-2311,0,0.0261107,"with RTX6000 GPUs. It took around 4 hours for model ﬁne-tuning and generation with a single GPU. 4.2 Automatic Evaluation 4.2.1 Evaluation Metrics To evaluate the generation quality for the domainspeciﬁc open-ended generation as studied here, we primarily measure the “closeness” between two sets of text, one generated by the model and the other the real text from the target domain. We evaluate with a broad array of automatic metrics, including lexical-based quality metrics and semanticbased quality metrics. We also evaluate the generation diversity. MS-Jaccard (MSJ) is a lexical-based metric (Montahaei et al., 2019), where MSJ-n measures the similarity of n-grams frequencies between two sets of text with Jaccard index. TF-IDF Distance (TID) is deﬁned as the disComparison methods. We compare with a wide range of baselines, categorized into two groups: (1) tance between the average TF-IDF features of two text sets. We use it as an additional lexical-based The large pretrained LMs including BART (Lewis quality measure. et al., 2020) and GPT-2 in both small and large sizes (Radford et al., 2019). The LMs generate text Fréchet BERT Distance (FBD) is a semanticin a standard left-to-right manner; (2) Progressiv"
2021.naacl-main.341,N19-1236,0,0.0217848,"sults show that ProGen achieves strongly improved performance by decomposing the generation into more progressive stages. Our method produces diverse text passages of higher quality and coherence than a broad set of models, including ﬁne-tuned GPT-2, BART, and other various planning-then-generation strategies. 2 Related Work Content planning in generation. The idea of separate content planning and surface realization has been studied in early text generation systems (Reiter and Dale, 1997). Recent neural approaches have also adopted similar planning-thengeneration strategies for data-to-text (Moryossef et al., 2019; Puduppully et al., 2019), storytelling (Fan et al., 2019; Yao et al., 2019; Xu et al., 2020), machine translation (Ford et al., 2018), and others (Hua and Wang, 2019; Yao et al., 2017). The approach enjoys several core advantages: These models often involve customized architec(1) Although the progressive approach implements tures incompatible with the existing large LMs. a conceptually non-monotonic generation process, Scaling those models for long text generation thus generation at each stage can still be performed in can require expensive training, which restricts sysa left-to-right manner"
2021.naacl-main.341,2020.emnlp-main.58,0,0.0820602,"Missing"
2021.naacl-main.341,2020.emnlp-main.349,0,0.015148,"ne them into subsequent ck , each of which contains ﬁner-grained information than that of the preceding stage. At the ﬁnal stage, we reﬁne cK into the full passage by adding the least informative words (e.g., stop words). The generation process corresponds to a decomposition of the conditional probability as: Long text generation. Previous work has made attempts to generate text of up to two or three hundred tokens. Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., P (y, {ck }|x) = P (c1 |x) Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (1) ΠK k=2 P (ck |ck−1 , x) P (y|cK , x) . (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. As the above intuition, ck at early stages as the high-level content plans should contain informa(2019) showed long text samples qualitatively. Our tive or important words, to serve as skeletons for"
2021.naacl-main.341,K19-1079,0,0.124806,"arge-scale LMs. Our proposed ProGen instead generates more coherent samples close to human text. Recent large-scale pretrained language models (LMs), such as GPT-2 (Radford et al., 2019) and BART (Lewis et al., 2020), emerged as an impressive open-ended text generator capable of producing surprisingly ﬂuent text. The massive LMs are typically pretrained on large corpora of generic text once, and then ﬁne-tuned with small domainspeciﬁc data. The latest work has mostly focused on the regime of relatively short text with low hundreds of tokens. For example, Holtzman et al. 1 Introduction (2020); See et al. (2019); Hua and Wang (2020) Generating coherent long text (e.g., 1000s of to- studied GPT-2 and BART generations with a maxkens) is useful in myriad applications of creating re- imum length ranging from 150 to 350 tokens. In ports, essays, and other long-form content. Yet the this work, we study the problem of generating coproblem is particularly challenging as it demands herent, much longer passages of text (e.g., 1000 models to capture global context, plan content, and tokens). GPT-3 (Brown et al., 2020) was reported produce local words in a consistent manner. Prior to produce long essays, yet the"
2021.naacl-main.341,P19-1200,0,0.0213372,"ting of the most informative tokens such as key entities. Then, based on the plan, we gradually reﬁne them into subsequent ck , each of which contains ﬁner-grained information than that of the preceding stage. At the ﬁnal stage, we reﬁne cK into the full passage by adding the least informative words (e.g., stop words). The generation process corresponds to a decomposition of the conditional probability as: Long text generation. Previous work has made attempts to generate text of up to two or three hundred tokens. Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., P (y, {ck }|x) = P (c1 |x) Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (1) ΠK k=2 P (ck |ck−1 , x) P (y|cK , x) . (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. As the above intuition, ck at early stages as the high-level content plans should contain inform"
2021.naacl-main.341,2020.emnlp-main.420,0,0.02236,"rization models or SRL models) which are not always available (e.g., in certain domains or for low-resource languages). In contrast, we propose a simple way for designing the intermediate stages based on word informativeness, which can ﬂexibly increase the number of stages for improved results, and easily create training data for all stages without additional models. Non-monotonic generation and reﬁnement. Another relevant line of research is non-monotonic generation (Welleck et al., 2019; Gu et al., 2019; Stern et al., 2019; Chan et al., 2019; Zhang et al., 2020), inﬁlling (Zhu et al., 2019; Shen et al., 2020; Qin et al., 2020), or reﬁnement (Lee et al., 2018; Novak et al., 2016; Mansimov et al., 2019; Kasai et al., 2020) that differs from the restricted left-toright generation in conventional LMs. Again, those approaches largely depend on specialized architectures and inference, making them difﬁcult to be integrated with the powerful pretrained LMs. The prior studies have focused on generating short text. Our proposed coarse-to-ﬁne progressive generation conceptually presents a non-monotonic process built upon the pretrained monotonic LMs, which permits fast adaptation to any target domain and ge"
2021.naacl-main.341,W19-3620,0,0.0269523,"hown in our empirical study. Besides, creating training data for planning requires additional resources (e.g., pretrained summarization models or SRL models) which are not always available (e.g., in certain domains or for low-resource languages). In contrast, we propose a simple way for designing the intermediate stages based on word informativeness, which can ﬂexibly increase the number of stages for improved results, and easily create training data for all stages without additional models. Non-monotonic generation and reﬁnement. Another relevant line of research is non-monotonic generation (Welleck et al., 2019; Gu et al., 2019; Stern et al., 2019; Chan et al., 2019; Zhang et al., 2020), inﬁlling (Zhu et al., 2019; Shen et al., 2020; Qin et al., 2020), or reﬁnement (Lee et al., 2018; Novak et al., 2016; Mansimov et al., 2019; Kasai et al., 2020) that differs from the restricted left-toright generation in conventional LMs. Again, those approaches largely depend on specialized architectures and inference, making them difﬁcult to be integrated with the powerful pretrained LMs. The prior studies have focused on generating short text. Our proposed coarse-to-ﬁne progressive generation conceptually present"
2021.naacl-main.341,2020.emnlp-main.226,0,0.033005,"e progressive stages. Our method produces diverse text passages of higher quality and coherence than a broad set of models, including ﬁne-tuned GPT-2, BART, and other various planning-then-generation strategies. 2 Related Work Content planning in generation. The idea of separate content planning and surface realization has been studied in early text generation systems (Reiter and Dale, 1997). Recent neural approaches have also adopted similar planning-thengeneration strategies for data-to-text (Moryossef et al., 2019; Puduppully et al., 2019), storytelling (Fan et al., 2019; Yao et al., 2019; Xu et al., 2020), machine translation (Ford et al., 2018), and others (Hua and Wang, 2019; Yao et al., 2017). The approach enjoys several core advantages: These models often involve customized architec(1) Although the progressive approach implements tures incompatible with the existing large LMs. a conceptually non-monotonic generation process, Scaling those models for long text generation thus generation at each stage can still be performed in can require expensive training, which restricts sysa left-to-right manner and thus is directly compati- tematic studies. On the other hand, it is possible to ble with"
2021.naacl-main.341,D17-1233,0,0.0269949,"ence than a broad set of models, including ﬁne-tuned GPT-2, BART, and other various planning-then-generation strategies. 2 Related Work Content planning in generation. The idea of separate content planning and surface realization has been studied in early text generation systems (Reiter and Dale, 1997). Recent neural approaches have also adopted similar planning-thengeneration strategies for data-to-text (Moryossef et al., 2019; Puduppully et al., 2019), storytelling (Fan et al., 2019; Yao et al., 2019; Xu et al., 2020), machine translation (Ford et al., 2018), and others (Hua and Wang, 2019; Yao et al., 2017). The approach enjoys several core advantages: These models often involve customized architec(1) Although the progressive approach implements tures incompatible with the existing large LMs. a conceptually non-monotonic generation process, Scaling those models for long text generation thus generation at each stage can still be performed in can require expensive training, which restricts sysa left-to-right manner and thus is directly compati- tematic studies. On the other hand, it is possible to ble with the powerful pretrained monotonic LMs. adopt some of the content planning strategies (e.g.,"
2021.naacl-main.341,2020.emnlp-main.698,0,0.02198,"ires additional resources (e.g., pretrained summarization models or SRL models) which are not always available (e.g., in certain domains or for low-resource languages). In contrast, we propose a simple way for designing the intermediate stages based on word informativeness, which can ﬂexibly increase the number of stages for improved results, and easily create training data for all stages without additional models. Non-monotonic generation and reﬁnement. Another relevant line of research is non-monotonic generation (Welleck et al., 2019; Gu et al., 2019; Stern et al., 2019; Chan et al., 2019; Zhang et al., 2020), inﬁlling (Zhu et al., 2019; Shen et al., 2020; Qin et al., 2020), or reﬁnement (Lee et al., 2018; Novak et al., 2016; Mansimov et al., 2019; Kasai et al., 2020) that differs from the restricted left-toright generation in conventional LMs. Again, those approaches largely depend on specialized architectures and inference, making them difﬁcult to be integrated with the powerful pretrained LMs. The prior studies have focused on generating short text. Our proposed coarse-to-ﬁne progressive generation conceptually presents a non-monotonic process built upon the pretrained monotonic LMs, which perm"
D08-1017,P99-1070,0,0.0394765,"Missing"
D08-1017,W06-2920,0,0.089328,"l the arcs using a log-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; 162 McDonald et al., 2005b; McDonald and Pereira, 2006). In stacking experiments, the arc labels from the level 0 parser are also used as a feature.4 In the following subsections, we refer to our modification of the MSTParser as MST 1O (the arcfactored version) and MST 2O (the second-order arc-pair-factored version). All our experiments use the non-projective version of this parser. We refer to the MaltParser as Malt. We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi, 2006).5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.6 Statistical significance is measured using Dan Bikel’s randomized parsing evaluation comparator with 10,000 iterations.7 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2. The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4. In all our experiments, the number"
D08-1017,P04-1054,0,0.0321457,"Missing"
D08-1017,P05-1067,0,0.0603954,"Missing"
D08-1017,P99-1059,0,0.362595,"art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of a"
D08-1017,C96-1058,0,0.903966,"of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pr"
D08-1017,P06-2041,0,0.494122,"tures for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution"
D08-1017,D07-1013,0,0.0628025,"of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall"
D08-1017,E06-1011,0,0.769242,"and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics le"
D08-1017,W07-2216,0,0.226249,"nts to solving arg maxy∈Y(x) w> f (x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2 ) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more pred"
D08-1017,P05-1012,0,0.565159,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,H05-1066,0,0.389842,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,W06-2932,0,0.0406989,"odifications noted in footnote 4. In all our experiments, the number of ˜ is L = 2. partitions used to create D 5.2 Experiment: MST 2O + MST 2O Our first experiment stacks the highly accurate MST 2O parser with itself. At level 0, the parser uses only the standard features (§5.1), and at level 1, these are augmented by various subsets of features of x along with the output of the level 0 parser, g(x) (Table 2). The results are shown in Table 3. While we see improvements over the single-parser baseline 4 We made other modifications to MSTParser, implementing many of the successes described by (McDonald et al., 2006). Our version of the code is publicly available at http: //www.ark.cs.cmu.edu/MSTParserStacked. The modifications included an approximation to lemmas for datasets without lemmas (three-character prefixes), and replacing morphology/word and morphology/lemma features with morphology/POS features. 5 The CoNLL-X shared task actually involves thirteen languages; our experiments do not include Czech (the largest dataset), due to time constraints. Therefore, the average results plotted in the last rows of Tables 3, 4, and 5 are not directly comparable with previously published averages over thirteen"
D08-1017,P08-1108,0,0.189783,"ience, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greedi"
D08-1017,W04-2407,0,0.0834669,"Missing"
D08-1017,W06-2933,0,0.0492279,"Missing"
D08-1017,W06-1616,0,0.178317,"reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previous"
D08-1017,W05-1513,0,0.192799,"y parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable inde"
D08-1017,D08-1016,0,0.103348,"non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorithm – nonenon-projective of which have implementations. previously been known to have exact non-projective Dependency syntax is a lightweight syntactic repWe then switch to models that account for where implementa"
D08-1017,W07-2218,0,0.0295174,"on scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorith"
D08-1017,D07-1003,1,0.392434,"Missing"
D08-1017,W03-3023,0,0.499777,"Missing"
D10-1004,W06-2920,0,0.0814866,", using only features in F. Since the other features have not been used before, they have a zero weight, hence can be ignored. When β = ∞, the variational problem in Eq. 24 consists of a MAP computation and the soluˆ t ∈ Y(xt ). Only the tion corresponds to one output y ˆ t but not in yt , or vice-versa, parts that are active in y will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set F is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12 We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. 42 which handles any loss function Lβ,γ .13 Whe"
D10-1004,P08-1109,0,0.0540744,"light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both op"
D10-1004,N10-1112,1,0.181926,"constant, and L(θ; x, y) is a nonnegative convex loss. Examples include the logistic loss used in CRFs (− log Prθ (y|x)) and the hinge loss of structured SVMs (maxy0 ∈Y(x) θ &gt; (φ(x, y0 )− • O(n2 ) XOR - WITH - OUTPUT factors to impose the φ(x, y)) + `(y0 , y) for some cost function `). These constraint that words don’t consume other words’ are both special cases of the family defined in Fig. 4, commodities; i.e., if h 6= k and k 6= 0, then there which also includes the structured perceptron’s loss is a path from h to k iff exactly one outgoing arc (β → ∞, γ = 0) and the softmax-margin loss of Gimpel and Smith (2010; β = γ = 1). in {hh, mi}m=1,...,n carries flow to k: Pn Alg. 1 is closely related to stochastic or online k , h, k ∈ {0, . . . , n}, k ∈ / {0, h}. phk = m=1 fhh,mi (19) gradient descent methods, but with the key advantage of not needing a learning rate hyperparameter. L(G0x ) is thus defined by the constraints in Eq. 12 We sketch the derivation of Alg. 1; full details can and 15–19. The approximate MAP problem, that be found in Martins et al. (2010a). On the tth round, replaces M(G0x ) by L(G0x ) in Eq. 10, thus becomes: one example hxt , yt i is considered. We seek to solve maxz,f ,p θ &gt; F(x"
D10-1004,P08-1067,0,0.058961,"aph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a fact"
D10-1004,P98-1106,0,0.0963536,"qs. 12 and 15–19 are satisfied. This is exactly the LP relaxation considered by Martins et al. (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features.7 They also considered a configuration with non-projectivity features—which fire if an arc is non-projective.8 That configuration can also be obtained here if variables {nhh,mi } are 7 To be precise, the constraints of Martins et al. (2009) are recovered after eliminating the path variables, via Eqs. 18–19. 8 An arc hh, mi is non-projective if there is some word in its span not descending from h (Kahane et al., 1998). 40 2 minθ,ξ λm 2 kθ − θ t k + ξ s.t. L(θ; xt , yt ) ≤ ξ, ξ ≥ 0, 9 (23) Given what was just exposed, it seems appealing to try max-product loopy BP on the factor graph of Fig. 1, or sumproduct loopy BP on the one in Fig. 3. Both attempts present serious challenges: the former requires computing messages sent by the tree factor, which requires O(n2 ) calls to the Chu-LiuEdmonds algorithm and hence O(n5 ) time. No obvious strategy seems to exist for simultaneous computation of all messages, unlike in the sum-product case. The latter is even more challenging, as standard sum-product loopy BP has"
D10-1004,P10-1001,0,0.382192,"propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both optimize objective functions over local approximations of the marginal p"
D10-1004,D07-1015,0,0.0201707,"anning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3 ) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3 ) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3 ) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxy∈Y(x) Prθ (y|x), and computing the marginals Prθ (Yi = yi |x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” There is a faster but more involved O(n2 ) algorithm due to Tarjan (1977). 2 v1 , . . . , vn ∈ SC where SC ⊆ {0, 1}n . otherwise, P Q vi • Message-induced distribution: ω , hmj→C ij=1,...,n • Partition function: ZC (ω) , hv1 ,...,vn"
D10-1004,D10-1125,0,0.168528,"ences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR - WITH OUTPUT ) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP parser. Independently of our work, Koo et al. (2010) recently proposed an efficient dual decomposition method to solve an LP problem similar (but not equal) to the one in Eq. 20,15 with excellent parsing performance. Their parser is also an instance of a turbo parser since it relies on a local approximation of a marginal polytope. While one can also use dual decomposition to address our MAP problem, the fact that our model does not decompose as nicely as the one in Koo et al. (2010) would likely result in slower convergence. 8 Conclusion We presented a unified view of two recent approximate dependency parsers, by stating their underlying factor"
D10-1004,D08-1017,1,0.731315,"Missing"
D10-1004,P09-1039,1,0.118924,"rtins∗† Noah A. Smith∗ Eric P. Xing∗ ∗ School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {afm,nasmith,epxing}@cs.cmu.edu M´ario A. T. Figueiredo† † Instituto de Telecomunicac¸o˜ es Instituto Superior T´ecnico Lisboa, Portugal mtf@lx.it.pt Pedro M. Q. Aguiar‡ ‡ Instituto de Sistemas e Rob´otica Instituto Superior T´ecnico Lisboa, Portugal aguiar@isr.ist.utl.pt Abstract We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions c"
D10-1004,D10-1004,1,0.135472,"1: Factor graph corresponding to the dependency parsing model of Smith and Eisner (2008) with sibling and grandparent features. Circles denote variable nodes, and squares denote factor nodes. Note the loops created by the inclusion of pairwise factors (GRAND and SIB). In Table 1 we present closed-form expressions for the factor-to-variable message ratios mC→i , MC→i (1)/MC→i (0) in terms of their variable-tofactor counterparts mi→C , Mi→C (1)/Mi→C (0); these ratios are all that is necessary when the variables are binary. Detailed derivations are presented in an extended version of this paper (Martins et al., 2010b). 3 Variational Representations Let Px , {Prθ (.|x) |θ ∈ Rd } be the family of all distributions of the form in Eq. 2. We next present an alternative parametrization for the distributions in Px in terms of factor marginals. We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next. Parts and Output Indicators. A part is a pair hC, yC i, where C is a soft factor and yC a partial output assignment. We let R = {hC, yC i |C ∈ Q Csoft , yC ∈ i∈C Yi } be th"
D10-1004,H05-1066,0,0.804287,"gs to the dependency tree. There is a hard factor TREE connected to all variables, that constrains the overall arc configurations to form a spanning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3 ) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3 ) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3 ) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxy∈Y(x) Prθ (y|x), and computing the marginals Prθ (Yi = yi |x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” There is a faster but more involved O(n2 ) algorithm due to Tarjan (1977). 2 v1 , . . . , vn ∈"
D10-1004,W06-2932,0,0.140018,"ptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to infere"
D10-1004,N03-1028,0,0.0351156,"challenges. Often only “supported” features—those observed in the training data—are included, and even those are commonly eliminated when their frequencies fall below a threshold. Important information may be lost as a result of these expedient choices. S Formally, the supported feature set is Fsupp , m i=1 supp φ(xi , yi ), where supp u , {j |uj 6= 0} denotes the support of vector u. Fsupp is a subset of the complete feature set, comprised of those features output, S occur in some candidate Sm that 0 Fcomp , i=1 y0 ∈Y(xi ) supp φ(xi , yi ). Features i in Fcomp Fsupp are called unsupported. Sha and Pereira (2003) have shown that training a CRF-based shallow parser with the complete feature set may improve performance (over the supported one), at the cost of 4.6 times more features. Dependency parsing has a much higher ratio (around 20 for bilexical word-word features, as estimated in the Penn Treebank), due to the quadratic or faster growth of the number of parts, of which only a few are active in a legal output. We propose a simple strategy for handling Fcomp efficiently, which can be applied for those losses in Fig. 4 where β = ∞. (e.g., the structured SVM and perceptron). Our procedure is the follo"
D10-1004,D08-1016,0,0.063561,"rsing by Approximate Variational Inference Andr´e F. T. Martins∗† Noah A. Smith∗ Eric P. Xing∗ ∗ School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {afm,nasmith,epxing}@cs.cmu.edu M´ario A. T. Figueiredo† † Instituto de Telecomunicac¸o˜ es Instituto Superior T´ecnico Lisboa, Portugal mtf@lx.it.pt Pedro M. Q. Aguiar‡ ‡ Instituto de Sistemas e Rob´otica Instituto Superior T´ecnico Lisboa, Portugal aguiar@isr.ist.utl.pt Abstract We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminati"
D10-1004,D07-1014,1,0.820051,"Missing"
D10-1004,W08-2121,0,0.0715542,"Missing"
D10-1004,W03-3023,0,0.164051,"y the tion corresponds to one output y ˆ t but not in yt , or vice-versa, parts that are active in y will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set F is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12 We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. 42 which handles any loss function Lβ,γ .13 When β &lt; ∞, Turbo Parser #1 and the loopy BP algorithm of Smith and Eisner (2008) is used; otherwise, Turbo Parser #2 is used and the LP relaxation is solved with CPLEX. In both cases, we employed the same pruning strategy as Martins et"
D10-1004,C98-1102,0,\N,Missing
D10-1111,J04-3002,0,0.146612,"ed work, and then present our model in Section 3. Then in Section 4, we detail a collapsed Gibbs sampling algorithm for posterior inference. Sections 5 and 6 give details about the dataset used in the evaluation and illustrate the capabilities of our model using both qualitative and quantitative measures. Section 7 describes and evaluates the efficacy of a semi-supervised extension, and finally in Section 8 we conclude and list several directions for future research. 2 Related Work Ideological text is inherently subjective, thus our work is related to the growing area of subjectivity analysis(Wiebe et al., 2004; Riloff et al., 2003). The goal of this area of research is to learn to discriminate between subjective and objective text. In contrast,in modeling ideology, we aim toward contrasting two or more ideological perspectives each of which is subjective in nature. Further more, subjective text can be classified into sentiments which gave rise to a surge of work in automatic opinion mining (Wiebe et al., 2004; Yu and Hatzivassiloglou, 2003; Pang et al., 2002; Turney and Littman, 2003; Popescu and Etzioni, 2005) as well as sentiment 1 In this paper, we use the words ideology, view, perspective inter"
D10-1111,P08-1031,0,0.0205537,"Missing"
D10-1111,W03-0404,0,0.0721291,"Missing"
D10-1111,W03-1017,0,\N,Missing
D10-1111,H05-2017,0,\N,Missing
D10-1111,H05-1043,0,\N,Missing
D10-1111,P08-1036,0,\N,Missing
D10-1111,W02-1011,0,\N,Missing
D10-1111,N09-1054,0,\N,Missing
D10-1111,D09-1146,0,\N,Missing
D10-1124,N10-1038,1,0.204566,"tions. Using term frequency features xd for each author, we predict locations with wordgeography weights a ∈ R2W : lat T lon f (xd ; a) = (xT d a , xd a ) 1282 Weights are trained to minimize the sum of squared Euclidean distances, subject to L1 regularization: X lat lat 2 T lon (xT − ydlon )2 d a − yd ) + (xd a d + λlat ||alat ||1 + λlon ||alon ||1 The minimization problem decouples into two separate latitude and longitude models, which we fit using the glmnet elastic net regularized regression package (Friedman et al., 2010), which obtained good results on other text-based prediction tasks (Joshi et al., 2010). Regularization parameters were tuned on the development set. The L1 penalty outperformed L2 and mixtures of L1 and L2 . Note that for both word-level linear regression here, and the topic-level linear regression in SLDA, the choice of squared Euclidean distance dovetails with our use of spatial Gaussian likelihoods in the geographic topic models, since optimizing a is equivalent to maximum likelihood estimation under the assumption that locations are drawn from equivariant circular Gaussians centered around each f (xd ; a) linear prediction. We experimented with decorrelating the location di"
D14-1158,N09-1053,0,0.0288986,"r methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax ) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work. Outline: We first review existing n-gram smoothing methods (§2) and then present the intuition behind the key components of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We"
D14-1158,D08-1024,0,0.0259081,"low rank trigram models. For SmallRussian the ranges were {1e−5, 5e−5, 1e−4} for both the low rank bigram and the low rank trigram models. For statistical validity, 10 test sets of size equal to the original test set were generated by randomly sampling sentences with replacement from the original test set. Our method outperforms “intMKN” with gains similar to that on the smaller datasets. As shown in Table 3, our method obtains fast training times even for large datasets. 6 Machine Translation Task Table 4 presents results for the MT task, translating from English to Russian7 . We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6 As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7 the best score at WMT 2013 was 19.9 (Bojar et al., 2013) 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weigh"
D14-1158,J07-2003,0,0.0338502,"ineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus avai"
D14-1158,P11-2031,1,0.778325,"he MT task, translating from English to Russian7 . We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6 As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7 the best score at WMT 2013 was 19.9 (Bojar et al., 2013) 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connect"
D14-1158,P10-4002,1,0.832454,"am machine translation (MT) task. We have made the code for our approach publicly available 3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3 . • Small-Engli"
D14-1158,W11-2123,0,0.0366918,"3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011). 5.1 Datasets For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with “&lt;unk&gt;” tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3 . • Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test -"
D14-1158,J10-4005,0,0.0183816,"f ngram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task. 1 Introduction Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010). The predominant approach to language modeling is the n-gram model, wherein the probability of a word sequence P (w1 , . . . , w` ) is decomposed using the chain rule, and then a Markov assumption is made: P (w1 , . . . , w` ) ≈ Q` i−1 P (w i |wi−n+1 ). While this assumption subi=1 stantially reduces the modeling complexity, parameter estimation remains a major challenge. Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have"
D14-1158,D13-1024,0,0.0301227,"at leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax ) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work. Outline: We first review existing n-gram smoothing methods (§2) and then present the intuition behind the key components of our technique: rank (§3.1) and power (§3.2). We then show how these can be interpolated into an ensemble (§4). In the experimental evaluation on English and Russian corpora (§5), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We also include a comparison"
D14-1158,P06-1124,0,0.0530922,"mes to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive"
D14-1158,D13-1140,0,0.0649532,"d retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The 6 As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7 the best score at WMT 2013 was 19.9 (Bojar et al., 2013) 1495 procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score. 7 Related Work Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009)"
D14-1158,P02-1040,0,0.0898218,"of O(n), it is much faster than other recent methods for language modeling such as neural networks and conditional exponential family models where exact computation of the normalizing constant costs O(V ). 5 Experiments To evaluate PLRE, we compared its performance on English and Russian corpora with several vari2 for derivation see proof of Lemma 4 in the supplementary material 1493 ants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3 . To build the hard class-based LMs, we utilized mkcls4 , a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd -order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models. For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) syste"
D14-1158,P13-1005,0,0.0266141,"utation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which√can reduce the cost of exact normalization to O( V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation. There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts. 8 Conclusion We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as well as low rank matrices and tensors, our method captures both the fine-grained and coarse structures in word sequences. Our discounting strategy preserves the marginal constraint and thus generalizes Kneser Ney, and under slight changes can also e"
D14-1158,D11-1104,0,0.0163873,"rarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009). The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is t"
D14-1158,W97-0309,0,0.378024,"milar to how some smoothing methods modify their lower order distributions. Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V ≈ 1 × 106 ) leading to fast training times. This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(κmax ) where κmax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney’s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related wo"
D14-1158,W13-2201,0,\N,Missing
D16-1173,D15-1263,0,0.0143858,"sis Sentence level sentiment classification is to identify the sentiment polarity (e.g., positive or negative) of a sentence (Pang and Lee, 2008). Recently, a number of neural models have been developed and achieved new levels of performance (Kim, 2014; Socher et al., 2013; Lei et al., 2015). Despite the impressive success, most of the existing neural network approaches require large amount of labeled data while encoding very limited linguistic knowledge, making them inefficient to handle sophisticated linguistic phenomena, such as contrastive transitions and negations (Choi and Cardie, 2008; Bhatia et al., 2015). Hu et al. (2016) combines a neural network with a logic rule that captures contrastive sense by observing the word “but” in a sentence. However, such simple deterministic rules suffer from limited generality and robustness. This paper develops a new sentiment neural model that combines a large diverse set of linguistic knowledge through our enhanced framework. Our method efficiently captures complex linguistic patterns from limited data, and yields highly interpretable predictions. 3 Mutual Distillation This section introduces the proposed framework that enables joint learning of knowledge c"
D16-1173,D08-1083,0,0.0437509,"scope. Sentiment Analysis Sentence level sentiment classification is to identify the sentiment polarity (e.g., positive or negative) of a sentence (Pang and Lee, 2008). Recently, a number of neural models have been developed and achieved new levels of performance (Kim, 2014; Socher et al., 2013; Lei et al., 2015). Despite the impressive success, most of the existing neural network approaches require large amount of labeled data while encoding very limited linguistic knowledge, making them inefficient to handle sophisticated linguistic phenomena, such as contrastive transitions and negations (Choi and Cardie, 2008; Bhatia et al., 2015). Hu et al. (2016) combines a neural network with a logic rule that captures contrastive sense by observing the word “but” in a sentence. However, such simple deterministic rules suffer from limited generality and robustness. This paper develops a new sentiment neural model that combines a large diverse set of linguistic knowledge through our enhanced framework. Our method efficiently captures complex linguistic patterns from limited data, and yields highly interpretable predictions. 3 Mutual Distillation This section introduces the proposed framework that enables joint l"
D16-1173,P84-1044,0,0.736739,"Missing"
D16-1173,P16-1228,1,0.358319,"lication domains (Krizhevsky et al., 2012; Hinton et al., 2012; Bahdanau et al., 2014). However, the powerful end-to-end learning comes with limitations, including the requirement on massive amount of labeled data, uninterpretability of prediction results, and difficulty of incorporating human intentions and domain knowledge. To alleviate these drawbacks, recent work has focused on training DNNs with extra domain-specific features (Collobert et al., 2011), combining oracle similarity constraints (Karaletsos et al., 2016), modeling output correlations (Deng et al., 2014), and others. Recently, Hu et al. (2016) proposed a In this paper, we introduce a generalized framework which enables a learning procedure for knowledge representations and their weights jointly with the regulated DNN models. This greatly extends the applicability to massive structures in diverse forms, such as structured models and soft logic rules, facilitating practitioners to incorporate rich domain expertise and fuzzy constraints. Specifically, we propose a mutual distillation method that iteratively transfers information between DNN and structured knowledge, resulting in effective integration of the representation learning cap"
D16-1173,D14-1181,0,0.0206198,"2014). 1671 Though attempts have been made to learn the constraint weights from additional supervisions (Mei et al., 2014) or for tractability purposes (Steinhardt and Liang, 2015), learning and optimizing knowledge expressions jointly with the regulated models from data is still unsolved, and critically restricting the application scope. Sentiment Analysis Sentence level sentiment classification is to identify the sentiment polarity (e.g., positive or negative) of a sentence (Pang and Lee, 2008). Recently, a number of neural models have been developed and achieved new levels of performance (Kim, 2014; Socher et al., 2013; Lei et al., 2015). Despite the impressive success, most of the existing neural network approaches require large amount of labeled data while encoding very limited linguistic knowledge, making them inefficient to handle sophisticated linguistic phenomena, such as contrastive transitions and negations (Choi and Cardie, 2008; Bhatia et al., 2015). Hu et al. (2016) combines a neural network with a logic rule that captures contrastive sense by observing the word “but” in a sentence. However, such simple deterministic rules suffer from limited generality and robustness. This p"
D16-1173,D16-1076,0,0.025142,"Missing"
D16-1173,D15-1180,0,0.0111934,"e been made to learn the constraint weights from additional supervisions (Mei et al., 2014) or for tractability purposes (Steinhardt and Liang, 2015), learning and optimizing knowledge expressions jointly with the regulated models from data is still unsolved, and critically restricting the application scope. Sentiment Analysis Sentence level sentiment classification is to identify the sentiment polarity (e.g., positive or negative) of a sentence (Pang and Lee, 2008). Recently, a number of neural models have been developed and achieved new levels of performance (Kim, 2014; Socher et al., 2013; Lei et al., 2015). Despite the impressive success, most of the existing neural network approaches require large amount of labeled data while encoding very limited linguistic knowledge, making them inefficient to handle sophisticated linguistic phenomena, such as contrastive transitions and negations (Choi and Cardie, 2008; Bhatia et al., 2015). Hu et al. (2016) combines a neural network with a logic rule that captures contrastive sense by observing the word “but” in a sentence. However, such simple deterministic rules suffer from limited generality and robustness. This paper develops a new sentiment neural mod"
D16-1173,D15-1278,0,0.00523205,"anced generalization on limited data size, and improved interpretability of predictions. Our work enjoys general versatility on diverse types of structured knowledge and neural architectures. The principled knowledge and weight learning approach can also be applied to the posterior constraint frameworks (Ganchev et al., 2010; Liang et al., 2009) for regulating other statistical models. 2 Related Work Deep Networks with Structured Knowledge Combining the powerful deep neural models with structured knowledge has been of increasing interest to enhance generalization and improve interpretability (Li et al., 2015; Deng et al., 2014; Johnson et al., 2016). Recently, Hu et al. (2016) proposed to transfer logical knowledge information into neural networks with diverse architectures (e.g., convolutional networks and recurrent networks). They developed an iterative distillation framework that trains the neural network to emulate the predictions of a “teacher” model which is iteratively constructed by imposing posterior constraints on the network. The framework has shown to be effective in regulating different neural models. However, the method has required fixed constraints and manually specified weights,"
D16-1173,D13-1170,0,0.00407876,"1 Though attempts have been made to learn the constraint weights from additional supervisions (Mei et al., 2014) or for tractability purposes (Steinhardt and Liang, 2015), learning and optimizing knowledge expressions jointly with the regulated models from data is still unsolved, and critically restricting the application scope. Sentiment Analysis Sentence level sentiment classification is to identify the sentiment polarity (e.g., positive or negative) of a sentence (Pang and Lee, 2008). Recently, a number of neural models have been developed and achieved new levels of performance (Kim, 2014; Socher et al., 2013; Lei et al., 2015). Despite the impressive success, most of the existing neural network approaches require large amount of labeled data while encoding very limited linguistic knowledge, making them inefficient to handle sophisticated linguistic phenomena, such as contrastive transitions and negations (Choi and Cardie, 2008; Bhatia et al., 2015). Hu et al. (2016) combines a neural network with a logic rule that captures contrastive sense by observing the word “but” in a sentence. However, such simple deterministic rules suffer from limited generality and robustness. This paper develops a new s"
D16-1173,P15-1150,0,0.0128949,"Missing"
D16-1173,P14-1031,0,0.0170978,"Missing"
D16-1173,K15-1021,0,0.0126411,"Missing"
D17-1081,W16-1303,0,0.026239,"l., 2005; Chang et al., 2006; Banko et al., 2007; Etzioni et al., 2008; Mitchell et al., 2015) and scientific publication data (Shah et al., 2003; Peng and McCallum, 2006; Saleem and Latif, 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al., 2003; Canisius and Sporleder, 2007; Yang et al., 2015; Wang et al., 2015; Liang et al., 2015; Wu et al., 2015; Liu et al., 2016b; Wang et al., 2016) or binary relational tuples (Balasubramanian et al., 2002; Clark et al., 2012; Dalvi et al., 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from web documents and scientific publications to educational material usually leads to poor results as 7 Conclusion We presented an approach to harvest structured axiomatic knowledge from math textbooks. Our approach uses rich features based on context and typography, the redundancy of axiomatic knowledge and shared ordering constraints across multiple textb"
D17-1081,D13-1160,0,0.0195907,"ous use of typographical information, the redundancy of information and ordering constraints to improve the harvesting and parsing of axioms. This has not been attempted in previous work. Language to Programs: After harvesting axioms from textbooks, we also present an approach to parse the axiom mentions to horn clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney, 1993, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang et al., 2011; Berant et al., 2013; Yaghmazadeh et al., 2017, inter alia), commands to robots (Shimizu and Haas, 2009; Matuszek et al., 2010; Chen and Mooney, 2011, inter alia), or even general purpose programs (Lei et al., 2013; Ling et al., 2016; Yin and Neubig, 2017; Ling et al., 2017). More specifically, Liu et al. (2016a) and Quirk et al. (2015) learn “If-Then” and “IfThis-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision which would be expensive to obtain. We mitigated this issue by using redundant"
D17-1081,N10-1031,0,0.0175959,"ring geometric entities, we include geometric entities derived from the associated diagrams when available. Real valued feature that computes the length of longest common sub-sequence of words between two axiom mentions normalized by the total number of words in the two mentions. Real valued feature that computes the absolute difference in the number of sentences in the two mentions. We use an off-the-shelf monolingual word aligner – JACANA (Yao et al., 2013) pretrained on PPDB – and compute alignment score between axiom mentions as the feature. We use two common MT evaluation metrics METEOR (Denkowski and Lavie, 2010) and MAXSIM (Chan and Ng, 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. We also use Rouge-S (Lin, 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. Indicator feature that matches templates of equations detected in the axiom mentions. Proportion of common unigrams in the image captions of the diagrams associated with the axiom mentions. If both mention"
D17-1081,H05-1071,0,0.0103325,"ways be available for such problems, and (b) it lacked the deductive geometric reasoning used by students to solve these problems. Our axiomatic solver mitigates these issues by performing deductive reasoning using axiomatic knowledge extracted from textbooks. Information Extraction from Textbooks: Our model builds upon ideas from Information extraction (IE), which is the task of automatically extracting structured information from unstructured and/or semi-structured documents. While there has been a lot of work in IE on domains such as web documents (Chang et al., 2003; Etzioni et al., 2004; Cafarella et al., 2005; Chang et al., 2006; Banko et al., 2007; Etzioni et al., 2008; Mitchell et al., 2015) and scientific publication data (Shah et al., 2003; Peng and McCallum, 2006; Saleem and Latif, 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al., 2003; Canisius and Sporleder, 2007; Yang et al., 2015; Wang et al., 2015; Liang et al., 2015; Wu et al., 2015; Liu et al., 2016b; Wang et al., 2016) or binary relational tuples (Balasubramanian et al., 2002; Clark et al., 2012; Dalvi et"
D17-1081,D07-1087,0,0.0105666,"ally extracting structured information from unstructured and/or semi-structured documents. While there has been a lot of work in IE on domains such as web documents (Chang et al., 2003; Etzioni et al., 2004; Cafarella et al., 2005; Chang et al., 2006; Banko et al., 2007; Etzioni et al., 2008; Mitchell et al., 2015) and scientific publication data (Shah et al., 2003; Peng and McCallum, 2006; Saleem and Latif, 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al., 2003; Canisius and Sporleder, 2007; Yang et al., 2015; Wang et al., 2015; Liang et al., 2015; Wu et al., 2015; Liu et al., 2016b; Wang et al., 2016) or binary relational tuples (Balasubramanian et al., 2002; Clark et al., 2012; Dalvi et al., 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from web documents and scientific publications to educational material usually leads to poor results as 7 Conclusion We presented an approach to har"
D17-1081,P08-1007,0,0.0357699,"metric entities derived from the associated diagrams when available. Real valued feature that computes the length of longest common sub-sequence of words between two axiom mentions normalized by the total number of words in the two mentions. Real valued feature that computes the absolute difference in the number of sentences in the two mentions. We use an off-the-shelf monolingual word aligner – JACANA (Yao et al., 2013) pretrained on PPDB – and compute alignment score between axiom mentions as the feature. We use two common MT evaluation metrics METEOR (Denkowski and Lavie, 2010) and MAXSIM (Chan and Ng, 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. We also use Rouge-S (Lin, 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. Indicator feature that matches templates of equations detected in the axiom mentions. Proportion of common unigrams in the image captions of the diagrams associated with the axiom mentions. If both mentions do not have associated diagra"
D17-1081,P14-1048,0,0.0131776,"ulled to top 100 by frequency. We use these 100 discourse markers as features. We repeat the same procedure by using part-of-speech (POS) instead of words and use them as features. Punctuation at the segment border is an excellent cue. We include indicator features whether there is a punctuation at the segment border. Indicator that the two text spans are part of the same (a) sentence, (b) paragraph. Indicator that the two spans are in the same node in the XML hierarchy. Conjoined with the indicator feature that the two spans are part of the same paragraph. We use an off-the-shelf RST parser (Feng and Hirst, 2014) and include an indicator feature that the segmentation matches the parse segmentation. We also include the RST label as a feature. The distribution of the two text spans is typically dependent on their lengths. We use the ratio of the length of the two spans as an additional feature. Soricut and Marcu (2003) (section 3.1) presented a statistical model for deciding elementary discourse unit boundaries. We use the probability given by this model retrained on our training set as feature. This feature uses both lexical and syntactic information. Head node is the word with the highest occurrence a"
D17-1081,fujita-etal-2014-overview,0,0.0138918,"ns such as the one shown in Figure 2. GEOS derives a logical expression that represents the meaning of the Introduction Recently, researchers have proposed standardized tests as “drivers for progress in AI” (Clark and Etzioni, 2016). There is a growing body of work in solving standardized tests such as reading comprehensions (Richardson et al., 2013; Sachan et al., 2015, inter alia), science question answering (Schoenick et al., 2016; Sachan et al., 2016, inter alia), algebra word problems (Kushman et al., 2014, inter alia), geometry problems (Seo et al., 2015), pre-university entrance exams (Fujita et al., 2014), etc. A major challenge in building these solvers is the lack of subject knowledge. For example, geometry tests require knowledge of geometry axioms and pre-university exams require knowledge of laws of physics, chemistry, etc. In this paper, we present an automatic approach that can (a) harvest such subject knowledge from textbooks, and (b) parse the extracted knowledge to structured programs that the solvers can use. Unlike information extraction systems trained on domains such as web documents (Chang et al., 1 The same axiom can be potentially mentioned in a number of textbooks in differen"
D17-1081,P14-1026,0,0.110283,"l. (2015) recently presented GEOS, an automated end-to-end system that solves SAT style geometry questions such as the one shown in Figure 2. GEOS derives a logical expression that represents the meaning of the Introduction Recently, researchers have proposed standardized tests as “drivers for progress in AI” (Clark and Etzioni, 2016). There is a growing body of work in solving standardized tests such as reading comprehensions (Richardson et al., 2013; Sachan et al., 2015, inter alia), science question answering (Schoenick et al., 2016; Sachan et al., 2016, inter alia), algebra word problems (Kushman et al., 2014, inter alia), geometry problems (Seo et al., 2015), pre-university entrance exams (Fujita et al., 2014), etc. A major challenge in building these solvers is the lack of subject knowledge. For example, geometry tests require knowledge of geometry axioms and pre-university exams require knowledge of laws of physics, chemistry, etc. In this paper, we present an automatic approach that can (a) harvest such subject knowledge from textbooks, and (b) parse the extracted knowledge to structured programs that the solvers can use. Unlike information extraction systems trained on domains such as web doc"
D17-1081,W12-3014,0,0.0154671,"2004; Cafarella et al., 2005; Chang et al., 2006; Banko et al., 2007; Etzioni et al., 2008; Mitchell et al., 2015) and scientific publication data (Shah et al., 2003; Peng and McCallum, 2006; Saleem and Latif, 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al., 2003; Canisius and Sporleder, 2007; Yang et al., 2015; Wang et al., 2015; Liang et al., 2015; Wu et al., 2015; Liu et al., 2016b; Wang et al., 2016) or binary relational tuples (Balasubramanian et al., 2002; Clark et al., 2012; Dalvi et al., 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from web documents and scientific publications to educational material usually leads to poor results as 7 Conclusion We presented an approach to harvest structured axiomatic knowledge from math textbooks. Our approach uses rich features based on context and typography, the redundancy of axiomatic knowledge and shared ordering constraints"
D17-1081,D15-1193,0,0.0417956,"Missing"
D17-1081,P11-1060,0,0.0256625,"ues by making judicious use of typographical information, the redundancy of information and ordering constraints to improve the harvesting and parsing of axioms. This has not been attempted in previous work. Language to Programs: After harvesting axioms from textbooks, we also present an approach to parse the axiom mentions to horn clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney, 1993, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang et al., 2011; Berant et al., 2013; Yaghmazadeh et al., 2017, inter alia), commands to robots (Shimizu and Haas, 2009; Matuszek et al., 2010; Chen and Mooney, 2011, inter alia), or even general purpose programs (Lei et al., 2013; Ling et al., 2016; Yin and Neubig, 2017; Ling et al., 2017). More specifically, Liu et al. (2016a) and Quirk et al. (2015) learn “If-Then” and “IfThis-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision which would be expensive to obtain. We mitigated this issu"
D17-1081,P15-1085,0,0.0167427,"se rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney, 1993, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang et al., 2011; Berant et al., 2013; Yaghmazadeh et al., 2017, inter alia), commands to robots (Shimizu and Haas, 2009; Matuszek et al., 2010; Chen and Mooney, 2011, inter alia), or even general purpose programs (Lei et al., 2013; Ling et al., 2016; Yin and Neubig, 2017; Ling et al., 2017). More specifically, Liu et al. (2016a) and Quirk et al. (2015) learn “If-Then” and “IfThis-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. Solving Geometry Problems: While the problem of using computers to solve geometry questions is old (Feigenbaum and Feldman, 1963; Schattschneider and King, 19"
D17-1081,W04-1013,0,0.0050646,"computes the absolute difference in the number of sentences in the two mentions. We use an off-the-shelf monolingual word aligner – JACANA (Yao et al., 2013) pretrained on PPDB – and compute alignment score between axiom mentions as the feature. We use two common MT evaluation metrics METEOR (Denkowski and Lavie, 2010) and MAXSIM (Chan and Ng, 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. We also use Rouge-S (Lin, 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. Indicator feature that matches templates of equations detected in the axiom mentions. Proportion of common unigrams in the image captions of the diagrams associated with the axiom mentions. If both mentions do not have associated diagrams, this feature doesn’t fire. Indicator matching the current (and parent) node of axiom mentions in respective XML hierarchies. Table 2: Feature set for our axiom alignment model. The features are based on content, structure and typography. bag of conclusio"
D17-1081,P16-1057,0,0.0531274,"Missing"
D17-1081,D13-1020,0,0.0196147,"est axiomatic knowledge of geometry from math textbooks, and use this knowledge to improve the state-of-the-art system for solving SAT style geometry problems. Seo et al. (2015) recently presented GEOS, an automated end-to-end system that solves SAT style geometry questions such as the one shown in Figure 2. GEOS derives a logical expression that represents the meaning of the Introduction Recently, researchers have proposed standardized tests as “drivers for progress in AI” (Clark and Etzioni, 2016). There is a growing body of work in solving standardized tests such as reading comprehensions (Richardson et al., 2013; Sachan et al., 2015, inter alia), science question answering (Schoenick et al., 2016; Sachan et al., 2016, inter alia), algebra word problems (Kushman et al., 2014, inter alia), geometry problems (Seo et al., 2015), pre-university entrance exams (Fujita et al., 2014), etc. A major challenge in building these solvers is the lack of subject knowledge. For example, geometry tests require knowledge of geometry axioms and pre-university exams require knowledge of laws of physics, chemistry, etc. In this paper, we present an automatic approach that can (a) harvest such subject knowledge from textb"
D17-1081,P17-1015,0,0.0323308,"o present an approach to parse the axiom mentions to horn clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney, 1993, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang et al., 2011; Berant et al., 2013; Yaghmazadeh et al., 2017, inter alia), commands to robots (Shimizu and Haas, 2009; Matuszek et al., 2010; Chen and Mooney, 2011, inter alia), or even general purpose programs (Lei et al., 2013; Ling et al., 2016; Yin and Neubig, 2017; Ling et al., 2017). More specifically, Liu et al. (2016a) and Quirk et al. (2015) learn “If-Then” and “IfThis-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. Solving Geometry Problems: While the problem of using computers to solve geometry questions is"
D17-1081,P15-1024,1,0.858505,"f geometry from math textbooks, and use this knowledge to improve the state-of-the-art system for solving SAT style geometry problems. Seo et al. (2015) recently presented GEOS, an automated end-to-end system that solves SAT style geometry questions such as the one shown in Figure 2. GEOS derives a logical expression that represents the meaning of the Introduction Recently, researchers have proposed standardized tests as “drivers for progress in AI” (Clark and Etzioni, 2016). There is a growing body of work in solving standardized tests such as reading comprehensions (Richardson et al., 2013; Sachan et al., 2015, inter alia), science question answering (Schoenick et al., 2016; Sachan et al., 2016, inter alia), algebra word problems (Kushman et al., 2014, inter alia), geometry problems (Seo et al., 2015), pre-university entrance exams (Fujita et al., 2014), etc. A major challenge in building these solvers is the lack of subject knowledge. For example, geometry tests require knowledge of geometry axioms and pre-university exams require knowledge of laws of physics, chemistry, etc. In this paper, we present an automatic approach that can (a) harvest such subject knowledge from textbooks, and (b) parse t"
D17-1081,P16-2076,1,0.883091,"Missing"
D17-1081,P16-1043,1,0.870689,"Missing"
D17-1081,P14-5010,0,0.00438036,"xioms, alignments and parses for grade 6, 7 and 8 textbooks by the four publishers/authors. We use grade 6, 7 and 8 textbook annotations for development, training, and testing, respectively. All the hyper-parameters in all the models are tuned on the development set using grid search. GEOS used 13 types of entities and 94 functions and predicates. We add some more entities, functions and predicates to cover other more complex concepts in geometry not covered in GEOS. Thus, we obtain a final set of 19 entity types and 115 functions and predicates for our parsing model. We use Stanford CoreNLP (Manning et al., 2014) for feature generation. We use two datasets for evaluating our system: (a) practice and official SAT style geometry questions used in GEOS, and (b) an additional dataset of geometry questions collected from the aforementioned textbooks. This dataset consists of a total of 1406 SAT style questions across grades 6-10, and is approximately 7.5 times the size of the dataset used in GEOS. We split the dataset into training (350 questions), 4.2.2 Multi-source Axiomatic Parser Now, we describe a multi-source parser that utilizes the redundancy of axiom extractions from various sources (textbooks). G"
D17-1081,P13-2123,0,0.0277211,"Missing"
D17-1081,D15-1171,0,0.254043,"e this challenge by (a) leveraging the redundancy and shared ordering of axiom mentions across multiple textbooks1 , and (b) utilizing rich contextual and typographical features2 from textbooks to effectively extract and parse axioms. Finally, we also provide an approach to parse the extracted axiom mentions from various textbooks and reconcile them to achieve the best program for each axiom. As a case study, we use our approach to harvest axiomatic knowledge of geometry from math textbooks, and use this knowledge to improve the state-of-the-art system for solving SAT style geometry problems. Seo et al. (2015) recently presented GEOS, an automated end-to-end system that solves SAT style geometry questions such as the one shown in Figure 2. GEOS derives a logical expression that represents the meaning of the Introduction Recently, researchers have proposed standardized tests as “drivers for progress in AI” (Clark and Etzioni, 2016). There is a growing body of work in solving standardized tests such as reading comprehensions (Richardson et al., 2013; Sachan et al., 2015, inter alia), science question answering (Schoenick et al., 2016; Sachan et al., 2016, inter alia), algebra word problems (Kushman e"
D17-1081,P17-1041,0,0.0146222,"from textbooks, we also present an approach to parse the axiom mentions to horn clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney, 1993, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang et al., 2011; Berant et al., 2013; Yaghmazadeh et al., 2017, inter alia), commands to robots (Shimizu and Haas, 2009; Matuszek et al., 2010; Chen and Mooney, 2011, inter alia), or even general purpose programs (Lei et al., 2013; Ling et al., 2016; Yin and Neubig, 2017; Ling et al., 2017). More specifically, Liu et al. (2016a) and Quirk et al. (2015) learn “If-Then” and “IfThis-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. Solving Geometry Problems: While the problem of using computers to solve ge"
D17-1081,N03-1030,0,0.0120492,"ment border. Indicator that the two text spans are part of the same (a) sentence, (b) paragraph. Indicator that the two spans are in the same node in the XML hierarchy. Conjoined with the indicator feature that the two spans are part of the same paragraph. We use an off-the-shelf RST parser (Feng and Hirst, 2014) and include an indicator feature that the segmentation matches the parse segmentation. We also include the RST label as a feature. The distribution of the two text spans is typically dependent on their lengths. We use the ratio of the length of the two spans as an additional feature. Soricut and Marcu (2003) (section 3.1) presented a statistical model for deciding elementary discourse unit boundaries. We use the probability given by this model retrained on our training set as feature. This feature uses both lexical and syntactic information. Head node is the word with the highest occurrence as a lexical head in the lexicalized tree among all the words in the text span. The attachment node is the parent of the head node. We have features for the head words of the left and right spans, the common ancestor (if any), the attachment node and the conjunction of the two head node words. We repeat these"
J19-4002,H05-1071,0,0.0754747,"Missing"
J19-4002,D07-1087,0,0.00946961,"cally extracting structured information from unstructured and/or semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is l"
J19-4002,W04-2504,0,0.561617,"; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes,"
J19-4002,P08-1007,0,0.0604381,"Missing"
J19-4002,W12-3014,0,0.0254481,"; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering const"
J19-4002,J87-1002,0,0.504203,"try problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 201"
J19-4002,C14-1206,0,0.049657,"Missing"
J19-4002,W16-1303,0,0.0164499,"04; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering constraints to improve th"
J19-4002,N10-1031,0,0.0263931,"Missing"
J19-4002,P09-1075,0,0.0225883,"component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Ch"
J19-4002,P12-1007,0,0.0226245,"c devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et"
J19-4002,P14-1048,0,0.380757,"to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse conside"
J19-4002,D14-1168,0,0.0268694,"ann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into"
J19-4002,W12-1622,0,0.0534271,"Missing"
J19-4002,J86-3001,0,0.739399,"isting solver for geometry problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and"
J19-4002,P14-1092,0,0.0603686,"Missing"
J19-4002,P14-1002,0,0.0243145,"nicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without m"
J19-4002,P13-1127,0,0.012602,"Language to Programs: After harvesting axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-s"
J19-4002,P14-1003,0,0.0224181,"The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting."
J19-4002,D15-1193,0,0.0155235,"semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our appr"
J19-4002,P11-1060,0,0.017064,"Missing"
J19-4002,W04-1013,0,0.00883788,"iscourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure Indicator matching the current (and parent) node of axiom mentions in respective JSON hierarchies; i.e., are both nodes mentioned as axioms, diagrams or bounding boxes? Equation Template Indicator feature that matches templates of equations detected in the axiom mentions. The template matcher is designed such that it identifies various rewritings of the same axiom equation, e.g., PA × PB = PT2 and PA × PB = PC2 could refer to the same axiom with point T in one axiom mention"
J19-4002,P16-1057,0,0.0602349,"Missing"
J19-4002,P17-1015,0,0.0126677,"ooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text have been rare beca"
J19-4002,W10-4327,0,0.0608886,"Missing"
J19-4002,P14-5010,0,0.00542909,"Missing"
J19-4002,J96-3006,0,0.282068,"ore explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 20"
J19-4002,P15-1121,0,0.0128899,"nd Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes, indentations, or quotations. It may refer to asso"
J19-4002,P15-1085,0,0.0213097,"Missing"
J19-4002,P15-1024,1,0.90458,"Missing"
J19-4002,D15-1171,0,0.193157,"es in a multimedia document (Hovy 1998) for the various stages of information extraction. Our experiments show the usefulness of all the various typographical features over and above the various lexical semantic and discourse level features considered for the task. We use our model to extract and parse axiomatic knowledge from a novel data set of 20 publicly available math textbooks. We use this structured axiomatic knowledge to build a new axiomatic solver that performs logical inference to solve geometry problems. Our axiomatic solver outperforms GEOS on all existing test sets introduced in Seo et al. (2015) as well as a new test set of geometry questions collected from these textbooks. We also performed user studies on a number of school students studying geometry who found that our axiomatic solver is more interpretable and useful compared with GEOS. 2. Background and Related Work Discourse Analysis: Discourse analysis is the analysis of semantics conveyed by a coherent sequence of sentences, propositions, or speech. Discourse analysis is taken up in a variety of disciplines in the humanities and social sciences and a number of discourse theories have been proposed (Mann and Thompson 1988; Kamp"
J19-4002,N03-1030,0,0.361741,"Missing"
J19-4002,N09-1064,0,0.0330417,"Missing"
J19-4002,K15-2002,0,0.0155504,"icking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is o"
J19-4002,P13-2123,0,0.0111238,"es (constants, predicates, and functions) across the two axioms. When comparing geometric entities, we include geometric entities derived from the associated diagrams when available. Longest Common Subsequence Real valued feature that computes the length of longest common subsequence of words between two axiom mentions normalized by the total number of words in the two mentions. Number of discourse elements Real valued feature that computes the absolute difference in the number of discourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure"
J19-4002,P17-1041,0,0.0605138,"ing axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text"
N15-1074,D07-1109,0,0.0248264,"ver the topic-word multinomials such that similar words share similar topic distributions. Newman et al. (2011) proposed a quadratic regularizer and a convolved Dirichlet regularizer over topic-word multinomials to incorporate the correlation between words. All of these methods directly incorporate the word correlation knowledge into the topic-word distributions in a hard and topic-independent way, which ignore the fact that whether two words are correlated depends on which topic they appear in. There are several works utilizing knowledge with more complex structure to improve topic modeling. Boyd-Graber et al. (2007) incorporate the synset structure in WordNet (Miller, 1995b) into LDA for word sense disambiguation, where each topic is a random process defined over the synsets. Hu et al. (2011) proposed interactive topic modeling, which allows users to iteratively refine the discovered topics by adding constraints such as certain set of words must appear together in the same topic. Andrzejewski et al. (2011) proposed a general framework which uses first order logic to encode various domain knowledge regarding documents, topics and side information into LDA. The vast generality and expressivity of this mode"
N15-1074,P84-1044,0,0.759893,"Missing"
N15-1074,P11-1026,0,0.258424,"opic-word multinomials to incorporate the correlation between words. All of these methods directly incorporate the word correlation knowledge into the topic-word distributions in a hard and topic-independent way, which ignore the fact that whether two words are correlated depends on which topic they appear in. There are several works utilizing knowledge with more complex structure to improve topic modeling. Boyd-Graber et al. (2007) incorporate the synset structure in WordNet (Miller, 1995b) into LDA for word sense disambiguation, where each topic is a random process defined over the synsets. Hu et al. (2011) proposed interactive topic modeling, which allows users to iteratively refine the discovered topics by adding constraints such as certain set of words must appear together in the same topic. Andrzejewski et al. (2011) proposed a general framework which uses first order logic to encode various domain knowledge regarding documents, topics and side information into LDA. The vast generality and expressivity of this model makes its inference to be very hard. Chen et al. (2013) proposed a topic model to model multi-domain knowledge, where each document is an admixture of latent topics and each topi"
N15-1074,E12-1021,0,0.012615,"Missing"
N15-1074,D14-1162,0,0.104556,". Table 1: Dataset Statistics 4.1 Experiment Setup • Dataset: We use two datasets in the experiments: 20-Newsgroups2 and NIPS3 . Their statistics are summarized in Table 1. • External Knowledge: We extract word correlation knowledge from Web Eigenwords4 , where each word has a real-valued vector capturing the semantic meaning of this word based on distributional similarity. Two words are regarded as correlated if their representation vectors are similar enough. It is worth mentioning that, other sources of external word correlation knowledge, such as Word2Vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014), can be readily incorporated into MRF-LDA. • Baselines: We compare our model with three baseline methods: LDA (Blei et al., 2003), DFLDA (Andrzejewski et al., 2009) and QuadLDA (Newman et al., 2011). LDA is the most widely used topic model, but it is unable to incorporate external knowledge. DF-LDA and Quad-LDA are two models designed to incorporate word correlation to improve topic modeling. DF-LDA puts a Dirichlet Forest prior over the topic-word multinomials to encode the Must-Links and Cannot-Links between words. Quad-LDA regularizes the topic-word distributions with a structured prior to"
N18-1058,N10-1086,0,0.173987,"erates questions by memorizing them from the training set and uses edit distance (Levenshtein, 1966) to calculate distance between a question and the input sentence. The second baseline is a MT system – MOSES (Koehn et al., 2007) which models question generation as a translation task where raw sentences are treated as source texts and questions are treated as target texts. The third baseline, DirectIn, uses the longest sub-sentence of the input sentence (using a set of simple sentence splitters) as the question. The fourth baseline, H&S is a rule-based overgenerate-and-rank system proposed by Heilman and Smith (2010). The Question Selection Oracle: The first question we wish to answer is: Is careful question selection even necessary? To answer this, we plot MAP scores for our best QA model (QA+QG, Ensemble+E&E) when we do not have a curriculum learning based oracle (i.e. an oracle which picks questions to be added to the dataset randomly) in Figure 1 as a function of epochs. We observe that 2 https://aclweb.org/aclwiki/Question_ Answering_(State_of_the_art) 634 0.55 CNN (Yang et al., 2015) APCNN (Santos et al., 2016) NASM (Miao et al., 2016) ABCNN (Yin et al., 2015) KVMN (Miller et al., 2016) Wang et al."
N18-1058,P16-1223,0,0.0389388,"Missing"
N18-1058,P17-1171,0,0.0198554,"he focus on QG is much more recent. Recently, there has been a renewed interest in reading comprehensions (also known as machine comprehension – a nomenclature popularized by Richardson et al. (2013)). Various approaches (Sachan et al., 2015; Wang et al., 2015; Sachan and Xing, 2016b; Sachan et al., 2016; Narasimhan and Barzilay, 2015) have been proposed for solving this task. After the release of large benchmarks such as SQUAD, MS MARCO and WikiQA, there has been a surge in interest on using neural network or deep-learning models for QA (Yin et al., 2015; Seo et al., 2016; Shen et al., 2016; Chen et al., 2017; Liu et al., 2017; Hu et al., 2017). In our work, we deal with the answer sentence selection task and adapt the Attentive Reader framework proposed in Hermann 8 Conclusion We described self-training algorithms for jointly learning to answer and ask questions while leveraging unlabeled data. We experimented with neural models for question answering and question generation and various careful strategies for question filtering based on curriculum learning and diversity promotion. This led to improved performance for both question answering and question generation on multiple datasets and new sta"
N18-1058,W14-3348,0,0.0251946,"Missing"
N18-1058,P17-1123,0,0.167392,"our approach on four benchmark datasets: SQUAD, MS MARCO, WikiQA and TrecQA, and show significant improvements over a number of established baselines on both question answering and question generation tasks. We also achieved new state-of-the-art results on two competitive answer sentence selection tasks: WikiQA and TrecQA. 1 Introduction Question Answering (QA) is a well-studied problem in NLP which focuses on answering questions using some structured or unstructured sources of knowledge. Alongside question answering, there has also been some work on generating questions (QG) (Heilman, 2011; Du et al., 2017; Tang et al., 2017) which focuses on generating questions based on given sources of knowledge. QA and QG are closely related1 tasks. However, NLP literature views the two as entirely separate tasks. In this paper, we explore this relationship between the two tasks by jointly learning to generate as well as answer questions. An improved ability to generate as well as answer questions will help us build curious machines that can interact with humans in a better manner. Joint modeling of 1 We can think of QA and QG as inverse of each other. 629 Proceedings of NAACL-HLT 2018, pages 629–640 c New"
N18-1058,D17-1090,0,0.102605,"oposed in our paper, we show the effect of the question selection oracle on the final 636 et al. (2015); Chen et al. (2016) as our base model. While, all these models were trained on question answer pairs, we propose a self-training solution to additionally leverage unsupervised text. Similarly, there have been works on QG. Traditionally, rule based approaches with postprocessing (Woo et al., 2016; Heilman and Smith, 2009, 2010) were the norm in QG. However, recent papers build on neural network approaches such as seq2seq (Du et al., 2017; Tang et al., 2017; Zhou et al., 2017), CNNs and RNNs (Duan et al., 2017) for QG. We also choose the seq2seq paradigm in our work. However, we leverage unsupervised text in contrast to these models. Finally, some very recent works have concurrently recognized the relationship between QA and QG and have proposed joint training (Tang et al., 2017; Wang et al., 2017) for the two. Our work differs from these as we additionally propose self-training to leverage unlabeled data to improve the two models. Self-training has seldom been used in NLP. Most prominently, they have been used for word sense disambiguation (Yarowsky, 1995), noun learning (Riloff et al., 2003) and r"
N18-1058,P07-2045,0,0.00460269,"e a neural network approach to model semantic relatedness of sentence pairs. For the WikiQA and TrecQA dataset, we report results of various existing stateof-the-art approaches on the two datasets2 . For QG, we compare our model against the following four baselines used in previous work (Du et al., 2017). The first baseline is a simple IR baselines taken from Rush et al. (2015) which generates questions by memorizing them from the training set and uses edit distance (Levenshtein, 1966) to calculate distance between a question and the input sentence. The second baseline is a MT system – MOSES (Koehn et al., 2007) which models question generation as a translation task where raw sentences are treated as source texts and questions are treated as target texts. The third baseline, DirectIn, uses the longest sub-sentence of the input sentence (using a set of simple sentence splitters) as the question. The fourth baseline, H&S is a rule-based overgenerate-and-rank system proposed by Heilman and Smith (2010). The Question Selection Oracle: The first question we wish to answer is: Is careful question selection even necessary? To answer this, we plot MAP scores for our best QA model (QA+QG, Ensemble+E&E) when w"
N18-1058,D15-1181,0,0.0533097,"Missing"
N18-1058,P17-1014,0,0.101653,"ing, such a ranking of easy and hard questions is difficult to obtain. A human judgement of ‘easiness’ of a question might not correlate with what is easy for our algorithms in its feature and hypothesis space. We explore various heuristics that define a measure of easiness and learn the ordering by selecting questions using this measure. updated on a subsample of the newly generated datapoints and original unlabelled data. Self-training has been seldom used in NLP. Most prominently, it has been used for WSD (Yarowsky, 1995), noun learning (Riloff et al., 2003) and AMR parsing and generation (Konstas et al., 2017). However, it has not been explored in this way for QA and QG. 5.1 The Question Selection Oracle A key challenge in self-training is selecting which unlabeled data sample to label (iwhich generated questions to add to the training set). The selftraining process may erroneously generate some bad or incorrect questions which can sidetrack the learning process. Thus, we implement a question selection oracle which determines which questions to add among the potentially very large set of questions generated by the QG model in each iteration. Traditional wisdom in self-training (Yarowsky, 1995; Rilo"
N18-1058,N16-1108,0,0.0477264,"Missing"
N18-1058,W04-1013,0,0.0321416,"Missing"
N18-1058,D13-1020,0,0.0408476,"tapering effect on the performance results, so it is clear that the performance will be capped by some upper-bound and we will need better ways of modeling language and meaning to make progress. 7 Related Work Our work proposes an approach for joint modeling QA and QG. While QA has recieved a lot of attention from the research community with large scale community evaluations such as NTCIR, TREC, CLEF spurring progress, the focus on QG is much more recent. Recently, there has been a renewed interest in reading comprehensions (also known as machine comprehension – a nomenclature popularized by Richardson et al. (2013)). Various approaches (Sachan et al., 2015; Wang et al., 2015; Sachan and Xing, 2016b; Sachan et al., 2016; Narasimhan and Barzilay, 2015) have been proposed for solving this task. After the release of large benchmarks such as SQUAD, MS MARCO and WikiQA, there has been a surge in interest on using neural network or deep-learning models for QA (Yin et al., 2015; Seo et al., 2016; Shen et al., 2016; Chen et al., 2017; Liu et al., 2017; Hu et al., 2017). In our work, we deal with the answer sentence selection task and adapt the Attentive Reader framework proposed in Hermann 8 Conclusion We descri"
N18-1058,D17-1085,0,0.01444,"uch more recent. Recently, there has been a renewed interest in reading comprehensions (also known as machine comprehension – a nomenclature popularized by Richardson et al. (2013)). Various approaches (Sachan et al., 2015; Wang et al., 2015; Sachan and Xing, 2016b; Sachan et al., 2016; Narasimhan and Barzilay, 2015) have been proposed for solving this task. After the release of large benchmarks such as SQUAD, MS MARCO and WikiQA, there has been a surge in interest on using neural network or deep-learning models for QA (Yin et al., 2015; Seo et al., 2016; Shen et al., 2016; Chen et al., 2017; Liu et al., 2017; Hu et al., 2017). In our work, we deal with the answer sentence selection task and adapt the Attentive Reader framework proposed in Hermann 8 Conclusion We described self-training algorithms for jointly learning to answer and ask questions while leveraging unlabeled data. We experimented with neural models for question answering and question generation and various careful strategies for question filtering based on curriculum learning and diversity promotion. This led to improved performance for both question answering and question generation on multiple datasets and new state-ofthe-art resul"
N18-1058,W03-0404,0,0.342995,"Missing"
N18-1058,D16-1147,0,0.0497306,"Missing"
N18-1058,P15-1121,0,0.0261575,"better ways of modeling language and meaning to make progress. 7 Related Work Our work proposes an approach for joint modeling QA and QG. While QA has recieved a lot of attention from the research community with large scale community evaluations such as NTCIR, TREC, CLEF spurring progress, the focus on QG is much more recent. Recently, there has been a renewed interest in reading comprehensions (also known as machine comprehension – a nomenclature popularized by Richardson et al. (2013)). Various approaches (Sachan et al., 2015; Wang et al., 2015; Sachan and Xing, 2016b; Sachan et al., 2016; Narasimhan and Barzilay, 2015) have been proposed for solving this task. After the release of large benchmarks such as SQUAD, MS MARCO and WikiQA, there has been a surge in interest on using neural network or deep-learning models for QA (Yin et al., 2015; Seo et al., 2016; Shen et al., 2016; Chen et al., 2017; Liu et al., 2017; Hu et al., 2017). In our work, we deal with the answer sentence selection task and adapt the Attentive Reader framework proposed in Hermann 8 Conclusion We described self-training algorithms for jointly learning to answer and ask questions while leveraging unlabeled data. We experimented with neural"
N18-1058,D15-1044,0,0.0439282,"P for our best self-trained QA model (with 10,000 Wikipedia paragraphs) without any curriculum learning (i.e. candidate questions are added randomly) vs epochs. baselines are CDSSM (Shen et al., 2014) and ABCNN (Yin et al., 2015) which use a neural network approach to model semantic relatedness of sentence pairs. For the WikiQA and TrecQA dataset, we report results of various existing stateof-the-art approaches on the two datasets2 . For QG, we compare our model against the following four baselines used in previous work (Du et al., 2017). The first baseline is a simple IR baselines taken from Rush et al. (2015) which generates questions by memorizing them from the training set and uses edit distance (Levenshtein, 1966) to calculate distance between a question and the input sentence. The second baseline is a MT system – MOSES (Koehn et al., 2007) which models question generation as a translation task where raw sentences are treated as source texts and questions are treated as target texts. The third baseline, DirectIn, uses the longest sub-sentence of the input sentence (using a set of simple sentence splitters) as the question. The fourth baseline, H&S is a rule-based overgenerate-and-rank system pr"
N18-1058,P15-1024,1,0.843476,"it is clear that the performance will be capped by some upper-bound and we will need better ways of modeling language and meaning to make progress. 7 Related Work Our work proposes an approach for joint modeling QA and QG. While QA has recieved a lot of attention from the research community with large scale community evaluations such as NTCIR, TREC, CLEF spurring progress, the focus on QG is much more recent. Recently, there has been a renewed interest in reading comprehensions (also known as machine comprehension – a nomenclature popularized by Richardson et al. (2013)). Various approaches (Sachan et al., 2015; Wang et al., 2015; Sachan and Xing, 2016b; Sachan et al., 2016; Narasimhan and Barzilay, 2015) have been proposed for solving this task. After the release of large benchmarks such as SQUAD, MS MARCO and WikiQA, there has been a surge in interest on using neural network or deep-learning models for QA (Yin et al., 2015; Seo et al., 2016; Shen et al., 2016; Chen et al., 2017; Liu et al., 2017; Hu et al., 2017). In our work, we deal with the answer sentence selection task and adapt the Attentive Reader framework proposed in Hermann 8 Conclusion We described self-training algorithms for jointly l"
N18-1058,P16-2076,1,0.851392,"ound and we will need better ways of modeling language and meaning to make progress. 7 Related Work Our work proposes an approach for joint modeling QA and QG. While QA has recieved a lot of attention from the research community with large scale community evaluations such as NTCIR, TREC, CLEF spurring progress, the focus on QG is much more recent. Recently, there has been a renewed interest in reading comprehensions (also known as machine comprehension – a nomenclature popularized by Richardson et al. (2013)). Various approaches (Sachan et al., 2015; Wang et al., 2015; Sachan and Xing, 2016b; Sachan et al., 2016; Narasimhan and Barzilay, 2015) have been proposed for solving this task. After the release of large benchmarks such as SQUAD, MS MARCO and WikiQA, there has been a surge in interest on using neural network or deep-learning models for QA (Yin et al., 2015; Seo et al., 2016; Shen et al., 2016; Chen et al., 2017; Liu et al., 2017; Hu et al., 2017). In our work, we deal with the answer sentence selection task and adapt the Attentive Reader framework proposed in Hermann 8 Conclusion We described self-training algorithms for jointly learning to answer and ask questions while leveraging unlabeled d"
N18-1058,P16-1043,1,0.947442,"and introduces questions which have the the highest minimum confidence score. a∈A Here, LQA/QG is LQA , LQG or min (LQA , LQG ) depending on which oracle we are using. p(a∗ = a) can be estimated by computing the scores of each of the answer candidates for q and normalizing them. E[LQA/QG ] can be estimated by retraining the model(s) after adding this question. 2) Change in Objective (CiO): Choose question q that causes the smallest increase in LQA/QG . If Our question selection heurisitcs are based on the ideas of curriculum learning and diversity: 1. Curriculum learning (Bengio et al., 2009; Sachan and Xing, 2016a) requires ordering questions on the easiness scale, so that easy questions can be introduced to the learning algorithm first and harder questions can be 632 questions first, our learning algorithm is usually not exposed to a diverse set of questions. This is particularly a problem for deep-learning approaches that learn representations during the process of learning. Hence, when a harder question arrives, it can be difficult for the learner to adjust to the new question as the current representation may not be appropriate for the new level of question difficulty. We tackle this by introducin"
N18-1058,P02-1040,0,0.100603,"Missing"
N18-1058,P16-2079,1,0.915816,"and introduces questions which have the the highest minimum confidence score. a∈A Here, LQA/QG is LQA , LQG or min (LQA , LQG ) depending on which oracle we are using. p(a∗ = a) can be estimated by computing the scores of each of the answer candidates for q and normalizing them. E[LQA/QG ] can be estimated by retraining the model(s) after adding this question. 2) Change in Objective (CiO): Choose question q that causes the smallest increase in LQA/QG . If Our question selection heurisitcs are based on the ideas of curriculum learning and diversity: 1. Curriculum learning (Bengio et al., 2009; Sachan and Xing, 2016a) requires ordering questions on the easiness scale, so that easy questions can be introduced to the learning algorithm first and harder questions can be 632 questions first, our learning algorithm is usually not exposed to a diverse set of questions. This is particularly a problem for deep-learning approaches that learn representations during the process of learning. Hence, when a harder question arrives, it can be difficult for the learner to adjust to the new question as the current representation may not be appropriate for the new level of question difficulty. We tackle this by introducin"
N18-1058,D14-1162,0,0.0809137,"gradually introducing harder questions is intuitive as it allows the learner to improve gradually. Yet, it has one key deficiency. With curriculum learning, by focusing on easy 6 Experiments Implementation Details: We perform the same preprocessing on all the text. We lower-case all the text. We use NLTK for word tokenization. For training our neural networks, we only keep the most frequent 50k words (including entity and placeholder markers), and map all other words to a special <UNK> token. We choose word embedding size d = 100, and use the 100-dimensional pretrained GloVe word embeddings (Pennington et al., 2014) for initialization. We set k, m, k1 and k2 (hyperparameters for self-training) by grid search on a held-out development set. Datasets: We report our results on four datasets: SQUAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007). SQUAD is a cloze-style reading comprehension dataset with questions posed by crowd workers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. MS MARCO contains questions which are real anonymized queries issued through Bing o"
N18-1058,P95-1026,0,0.871917,"chanism as our sentence encoder. We use two LSTM’s: one that makes a forward pass in the sequence and another that makes a backward pass as in the QA model described earlier. We use dropout regularization for LSTMs as in Zaremba et al. (2014) in our implementation. The final context dependent to(e) ken representation ht is the concatenation of the forward and backward pass token representations: (e) (e) (e) h = [~h ; h~ ]. To obtain the final context det t In our self-training framework, we are given unlabeled text in addition to the labeled passages, question and answer pairs. Self-training (Yarowsky, 1995; Riloff et al., 2003), also known as self-teaching, is one of the earliest techniques for using unlabeled data along with labeled data to improve learning. During self-training, the learner keeps on labeling unlabeled examples and retraining itself on an enlarged labeled training set. We extend self-training to jointly learn two models (namely, QA and QG) iteratively. The QA and QG models are first trained on the labeled corpus. Then, the QG model is used to create more questions from the unlabeled text corpus and the QA model is used to answer these newly created questions. These new questio"
N18-1058,P16-1122,0,0.0556907,"Missing"
N18-1058,P15-2115,0,0.0127751,"performance will be capped by some upper-bound and we will need better ways of modeling language and meaning to make progress. 7 Related Work Our work proposes an approach for joint modeling QA and QG. While QA has recieved a lot of attention from the research community with large scale community evaluations such as NTCIR, TREC, CLEF spurring progress, the focus on QG is much more recent. Recently, there has been a renewed interest in reading comprehensions (also known as machine comprehension – a nomenclature popularized by Richardson et al. (2013)). Various approaches (Sachan et al., 2015; Wang et al., 2015; Sachan and Xing, 2016b; Sachan et al., 2016; Narasimhan and Barzilay, 2015) have been proposed for solving this task. After the release of large benchmarks such as SQUAD, MS MARCO and WikiQA, there has been a surge in interest on using neural network or deep-learning models for QA (Yin et al., 2015; Seo et al., 2016; Shen et al., 2016; Chen et al., 2017; Liu et al., 2017; Hu et al., 2017). In our work, we deal with the answer sentence selection task and adapt the Attentive Reader framework proposed in Hermann 8 Conclusion We described self-training algorithms for jointly learning to answer a"
N18-1058,D07-1003,0,0.0142373,"LTK for word tokenization. For training our neural networks, we only keep the most frequent 50k words (including entity and placeholder markers), and map all other words to a special <UNK> token. We choose word embedding size d = 100, and use the 100-dimensional pretrained GloVe word embeddings (Pennington et al., 2014) for initialization. We set k, m, k1 and k2 (hyperparameters for self-training) by grid search on a held-out development set. Datasets: We report our results on four datasets: SQUAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007). SQUAD is a cloze-style reading comprehension dataset with questions posed by crowd workers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. MS MARCO contains questions which are real anonymized queries issued through Bing or Cortana and the documents are related web pages which may or help answer the question. WikiQA is also a datset of queries taken from Bing query logs. Based on user clicks, each query is associated with a Wikipedia page. The summary paragraph of the page is taken as candidate answer sentences, w"
N18-1058,C16-1127,0,0.0327183,"Missing"
N18-1058,W16-6632,0,0.0189303,"p < 0.001). Evaluating the Question Selection Oracle: As discussed earlier, the choice of which subset of questions to add to our labeled dataset while selftraining is important. To evaluate the various heuristics proposed in our paper, we show the effect of the question selection oracle on the final 636 et al. (2015); Chen et al. (2016) as our base model. While, all these models were trained on question answer pairs, we propose a self-training solution to additionally leverage unsupervised text. Similarly, there have been works on QG. Traditionally, rule based approaches with postprocessing (Woo et al., 2016; Heilman and Smith, 2009, 2010) were the norm in QG. However, recent papers build on neural network approaches such as seq2seq (Du et al., 2017; Tang et al., 2017; Zhou et al., 2017), CNNs and RNNs (Duan et al., 2017) for QG. We also choose the seq2seq paradigm in our work. However, we leverage unsupervised text in contrast to these models. Finally, some very recent works have concurrently recognized the relationship between QA and QG and have proposed joint training (Tang et al., 2017; Wang et al., 2017) for the two. Our work differs from these as we additionally propose self-training to lev"
N18-1058,D15-1237,0,0.204842,"wer-case all the text. We use NLTK for word tokenization. For training our neural networks, we only keep the most frequent 50k words (including entity and placeholder markers), and map all other words to a special <UNK> token. We choose word embedding size d = 100, and use the 100-dimensional pretrained GloVe word embeddings (Pennington et al., 2014) for initialization. We set k, m, k1 and k2 (hyperparameters for self-training) by grid search on a held-out development set. Datasets: We report our results on four datasets: SQUAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007). SQUAD is a cloze-style reading comprehension dataset with questions posed by crowd workers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. MS MARCO contains questions which are real anonymized queries issued through Bing or Cortana and the documents are related web pages which may or help answer the question. WikiQA is also a datset of queries taken from Bing query logs. Based on user clicks, each query is associated with a Wikipedia page. The summary paragraph of the page is taken a"
P06-2124,C92-3168,0,0.0606427,"stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of documentpairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared"
P06-2124,E99-1010,0,0.0187804,"translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each documentpair corresponds to an obj"
P06-2124,P02-1040,0,0.0936936,"p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)]. (11) The close-form iterative updating formula B is: Bf,e,k ∝ Nd Jdn Idn M X X XX d δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 5 For α, close-form update is not available, and we resort to gradient accent as in (Sj¨olander et al., 1996) with re-starts to ensure each updated αk >0. We evaluate BiTAM models on the word alignment accuracy and the translation quality. For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu (Papineni et al., 2002) and its variation of NIST scores are reported. 4.2 Data Sparseness and Smoothing The translation lexicons Bf,e,k have a potential size of V 2 K, assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributio"
P06-2124,C96-2141,0,0.43983,"rence and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. 1 Introduction Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpuslevel word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair cooccurrence sta"
P06-2124,J93-2003,0,0.0309061,"Missing"
P06-2124,I05-2021,0,0.01344,"nt topics underlying the context of documentpairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixtur"
P06-2124,P01-1067,0,0.0146613,"topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. 1 Introduction Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpuslevel word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair cooccurrence statistics. For example, the word shot 969 Proceeding"
P06-2124,W02-1039,0,0.00907462,"This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement. The boosted alignments are denoted as BUDA and BBDA in Table. 5, corresponding to the uni-direction and bi-direction alignments, respectively. We see an improvement in alignment quality. 5.6 Evaluating Translations To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities. Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones. We use TIDES Eval’02 CE test set as development data to tune the decoder parameters; the Eval’03 data (919 sentences) is the unseen data. A trigram language model is built using 180 million English words. Across all the reported comparative settings, the key difference is the bilingual ngram-identity of the phrase-pair, which is collected directly from the underlying word alignment. Shown in Table 4 are results for the smalldata track; the large-data track results are in Table 5. For the small-data track, the baseline Bleu scores for IBM-1, HMM and IBM-4 are 15.70, 17.70"
P06-2124,W05-0804,1,0.863228,"n models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each documentpair corresponds to an object; depending on a"
P06-2124,N04-1021,0,\N,Missing
P09-1039,W07-2216,0,0.220657,"that th shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulat"
P09-1039,H05-1066,0,0.685035,"Missing"
P09-1039,P08-1108,0,0.628299,". featureHowrepning, 2002; McDonald and Pereira, resentations over the inputparsing (McDonald et al., 2.2ever, Arc Factorization and Locality in the data-driven setting this2005a). can be The goal of this is to further our current partially bywork incorporating feature repThere has adverted been much recent workrich on dependency understanding of the computational nature of nonresentations over the input (McDonald et al., 2005a). parsing using graph-based, transition-based, and projective forfurther both learning and The goalparsing of thisalgorithms work is to our current hybrid methods; see Nivre and McDonald (2008) inference within setting. We start by understanding ofthe thedata-driven computational nature of nonforprojective an overview. Typical graph-based methods investigating and extending the edge-factored model parsing algorithms for both learning and consider linear classifiers of the form of McDonald et the al. data-driven (2005b). Insetting. particular, we apinference within We start by peal to the Matrix Tree Theorem for multi-digraphs investigating and extending the edge-factored model hw (x) = argmaxy∈Y w&gt; f (x,fory),calculat-(1) to McDonald design polynomial-time algorithms of et al. (2005"
P09-1039,C04-1197,0,0.0420515,"), model word valency, and can learn to favor nearly-projective parses. Introduction We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a p"
P09-1039,W06-1616,0,0.30569,"nearly-projective parses. Introduction We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This"
P09-1039,D08-1016,0,0.497506,"programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. Their formulation includes an exponential number of constraints—one for each possible cycle. Since it is intractable to throw in all constraints"
P09-1039,W08-2121,0,0.0794721,"Missing"
P09-1039,D07-1003,1,0.197007,"NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the dire"
P09-1039,D08-1059,0,0.19634,"Missing"
P09-1039,W06-2920,0,0.447326,"Consider 3 (or 30 ) from §3.1. φka = −1, k ∈ V  {0} (18) a∈δ + (0) φka − (23) From the definition of projective arcs in §2.1, we np have that za = 1 if and only if the arc is active (za = 1) and there is some vertex k in the span of a = hi, ji such that ψik = 0. We are led to the following O(|A |· |V |) constraints for hi, ji ∈ A: • Any node consumes its own commodity and no other: X k ∈ V  {0}. zanp , I(a ∈ y and a is nonprojective). i∈V • The root sends one unit of commodity to each node: φka − j, k ∈ V  {0} np For most languages, dependency parse trees tend to be nearly projective (cf. Buchholz and Marsi, 2006). We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data. The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3 ) variables and constraints. In this model, every node k 6= 0 defines a commodity: one unit of commodity k originates at the root node and must be delivered to node k; the variable φkij denotes the flow of commodity k in arc hi, ji. We first replace (4–9) by (18–22"
P09-1039,W08-2102,0,0.0499262,"ed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. For scalability (and noting that some of the models require O(|V |· |A|) constraints and variables, which, when A = V 2 , grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser. The ranker is a local model trained using a max-margin criterion; it is arc-factored and not subject to any structural constraints, so it is very fast. The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al. (2005) by solving a sequence of loss-augmented inference problems.12 The number of iterations was set to 10. The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8 We used the provided t"
P09-1039,D07-1101,0,0.261932,"the first child of a given word. The ability to handle such “ordered” features is intimately associated with Eisner’s dynamic programming parsing algorithm and with the Markovian assumptions made explicitly by his generative model. We next show how similar features zijk V but this would yield a constraint matrix with O(n4 ) non-zero elements. Instead, we define auxiliary variables βjk and γij : sibl zijk ≥ zij + zik − 1 (14) for all triples hi, j, ki ∈ Rsibl , and zijk if hi, ji and hi, ki are consecutive siblings,   0 otherwise, ( first child zij As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki |hi, ji ∈ A, hj, ki ∈ A}. To include such features in our formulation, we need to add extra variables zsibl , hzr ir∈Rsibl and zgrand , hzr ir∈Rgrand that indicate the presence of sibling and grandparent arcs. Observe that these indicator variables are conjunctions of arc indicator varisibl = z ∧ z and z grand = z ∧ z . ables, i.e., zijk ij ij ik jk ijk Hence,"
P09-1039,P04-1054,0,0.018088,"ather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say t"
P09-1039,N07-1030,0,0.0284769,"e also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic"
P09-1039,P05-1067,0,0.026854,"tions focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A ="
P09-1039,P99-1059,0,0.0534909,"grammar b of Wang and Har work on empiric note include the w ing systems, no showing that the of Wang and Ha note include the showing that th shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their per"
P09-1039,C96-1058,0,0.889999,"ss; the typical loss functhat they can be used in many important learning tions over all possible dependency graphs for a given problems including min-risk decodtionandis inference the Hamming loss, `(y 0 ; y) , |{hi, ji ∈ sentence. To motivate these algorithms, we show ing, training globally normalized log-linear mody 0 that : hi,they ji ∈ /cany}|. Tractability is usually ensured be used in many important learning els, syntactic language modeling, and unsupervised byand strong factorization like the one inference problemsassumptions, including min-risk decodunderlying the arc-factored model (Eisner, 1996; ing, training globally normalized log-linear modMcDonald et al., 2005),modeling, which forbids any feature els, syntactic language and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f (x, y) as: 1 The general case where A ⊆ V 2 is also of interest; it arises whenever a constraint or a lexicon forbids some arcs from appearing in dependency tree. It may also arise as a consequence of a first-stage pruning step where some candidate arcs are eliminated; this will be further discussed in §4. 2 Or “directed spanning tree with designated root r.” 3"
P09-1039,P98-1106,0,0.0175876,"0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the directed graph D if hV, Bi is a (directed) tree rooted at r. We define the set of legal dependency parse trees of x (denoted Y(x)) as the set of 0-arborescences of D, i.e., we admit each arborescence as a potential dependency tree. Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i. We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) &lt; k &lt; max(i, j), there is a directed path in y from i to k). A dependency tree is called projective if it only contains projective arcs. Fig. 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x). This is the binary vector z , hza ia∈A with each component defined as za = I(a ∈ y) (here, I(.) denotes the indicator function). Considering simultaneously all incidence vectors of legal depend"
P09-1039,N06-1015,0,0.00916773,"of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-pr"
P09-1039,D08-1017,1,0.875982,": • Disabled arcs do not carry any flow: φka ≤ za , a ∈ A, k ∈ V (20)  0 0   i ≤ π(k) ≤ j , if i0 &lt; k &lt; j 0 , π(k) &lt; i0 ∨ π(k) &gt; j 0 , if k &lt; i0 or k &gt; j 0   or k = i. • There are exactly n enabled arcs: P a∈A za =n (21) 347 Then, Y(x) will be redefined as the set of projective dependency parse trees. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages. With the exception of Portuguese, the best results are achieved with the full set of features. We can also observ"
P09-1039,E06-1011,0,0.807214,"n solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. T"
P09-1039,C98-1102,0,\N,Missing
P11-1137,W10-1757,0,0.0335859,"Missing"
P11-1137,D10-1124,1,0.267148,"Missing"
P11-1137,N04-1039,0,0.241069,"Missing"
P11-1137,W03-1018,0,0.0271586,"Missing"
P11-1137,W04-3223,0,0.0219742,"Missing"
P14-1100,D13-1059,0,0.0147988,"s to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap. More specifically, we approach unsupervised constituent parsing from the perspective of structure learning as opposed to parameter learning. We associate each sentence with an undirected latent tree graphical model, which is a tree consisting of both observed variables (corresponding to the words in the sentence) and an additional set of latent variables that a"
P14-1100,J92-4003,0,0.134004,"9 CCM-UB 62.9 23.7 19.1 16.6 15.2 13.8 Table 1: Comparison of different CCM variants on English (training). U stands for universal POS tagset, OB stands for conjoining original POS tags with Brown clusters and UB stands for conjoining universal POS tags with Brown clusters. The best setting is just the vanilla setting, CCM. • Otherwise find the first non-participle verb (say at index j) and return ([0, j − 1], [j, `(x)]). • If no verb exists, return ([0, 1], [1, `(x)]). Word embeddings As mentioned earlier, each wi can be an arbitrary feature vector. For all languages we use Brown clustering (Brown et al., 1992) to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings Turian et al. (2010) of length 50. We also explored two types of CCA embeddings: OSCCA and TSCCA, given in Dhillon et al. (2012). The OSCCA embeddings behaved better, so we only report its results. Choice of kernel For our experiments, we use the kernel Kγ (j, k, j 0 , k 0 |x, x0 )   κ(j, k, j 0 ,"
P14-1100,N09-1009,1,0.888628,"ly annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These"
P14-1100,J12-3003,1,0.833961,"k et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or ma"
P14-1100,P12-1024,1,0.811325,"atrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If wi and zi were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m. Assumption 1 allows for the wi to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assumptions are made with spectral parameter learning methods e.g. Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012). Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p > m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): dspectral (i, j) = − log Λm (Σx (i, j)) + 12 log Λm (Σx (i, i)) + 12 log Λm (Σx (j, j)) (5) where Λm (A) denotes the product of the top m singular values of A and Σx (i, j) := E[vi vj> |x], i.e. the uncentered cross-covariance matrix. We can then show t"
P14-1100,P99-1059,0,0.414016,"g et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community (Zhou et al., 2010; Kolar et al., 2010b; Kolar et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of"
P14-1100,P10-2036,0,0.0410088,"Missing"
P14-1100,N12-1069,0,0.0495455,"likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to prov"
P14-1100,P12-2004,0,0.129699,"unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other"
P14-1100,P13-1044,0,0.089759,"ning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoreti"
P14-1100,N09-1012,0,0.0441815,"red for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while emp"
P14-1100,P02-1017,0,0.54601,"oach is grammarless – we directly learn the bracketing structure of a given sentence without using a grammar model. The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. Although finding the “minimal” latent tree is NP-hard in general, for the case of projective trees we find that it can be found using bilexical parsing algorithms. Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. 1 Introduction Solutions to the problem of grammar induction have been long sought after since the early days of computational linguistics and are interesting both from cognitive and engineering perspectives. Cognitively, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees. This means the unsupervised setting is a better model for studying language acquisition. From the engineering perspective, training data for unsupervised parsing exists in abundance (i.e. sentences and part-of-s"
P14-1100,J93-2004,0,0.0460967,"d from Sparse Data We now address the data sparsity problem, in particular that D(x) can be very small, and therefore estimating d for each POS sequence separately can be problematic.3 In order to estimate d from data, we need to estimate the covariance matrices Σx (i, j) (for i, j ∈ {1, . . . , `(x)}) from Eq. 5. To give some motivation to our solution, consider estimating the covariance matrix Σx (1, 2) for the tag sequence x = (DT1 , NN2 , VBD3 , DT4 , NN5 ). D(x) may be insufficient for an accurate empirical es3 This data sparsity problem is quite severe – for example, the Penn treebank (Marcus et al., 1993) has a total number of 43,498 sentences, with 42,246 unique POS tag sequences, averaging |D(x) |to be 1.04. 1067 timate. However, consider another sequence x0 = (RB1 , DT2 , NN3 , VBD4 , DT5 , ADJ6 , NN7 ). Although x and x0 are not identical, it is likely that Σx0 (2, 3) is similar to Σx (1, 2) because the determiner and the noun appear in similar syntactic context. Σx0 (5, 7) also may be somewhat similar, but Σx0 (2, 7) should not be very similar to Σx (1, 2) because the noun and the determiner appear in a different syntactic context. The observation that the covariance matrices depend on lo"
P14-1100,P07-1049,0,0.152566,"et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of the correct underlying latent tree and characterize the associated sample complexity under our technique. Empirically we evaluate our method on data in English, German and Chinese. Our algorithm performs favorably to Klein and Manning’s (2002) constituent-context model (CCM), without the need for careful initialization. In addition, we also analyze CCM’s sensitivity to initialization, and compare our results to Seginer’s algorithm (Seginer, 2007). 2 Learning Setting and Model In this section, we detail the learning setting and a conditional tree model we learn the structure for. 2.1 Learning Setting Let w = (w1 , ..., w` ) be a vector of words corresponding to a sentence of length `. Each wi is represented by a vector in Rp for p ∈ N. The vector is an embedding of the word in some space, choVBD DT NN VBD DT NN Figure 2: Candidate constituent parses for x = (VBD, DT, NN) (left-correct, right -incorrect) sen from a fixed dictionary that maps word types to Rp . In addition, let x = (x1 , ..., x` ) be the associated vector of part-of-spee"
P14-1100,P05-1044,0,0.0210609,"per than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovs"
P14-1100,N10-1116,0,0.0306316,"ining. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, gener"
P14-1100,W10-2902,0,0.0514068,"ining. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, gener"
P14-1100,D13-1204,0,0.110681,"er, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap. More specifically, we approach unsupervised constituent parsing from the perspective of structure lear"
P14-1100,P10-1040,0,0.00857564,"g, CCM. • Otherwise find the first non-participle verb (say at index j) and return ([0, j − 1], [j, `(x)]). • If no verb exists, return ([0, 1], [1, `(x)]). Word embeddings As mentioned earlier, each wi can be an arbitrary feature vector. For all languages we use Brown clustering (Brown et al., 1992) to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings Turian et al. (2010) of length 50. We also explored two types of CCA embeddings: OSCCA and TSCCA, given in Dhillon et al. (2012). The OSCCA embeddings behaved better, so we only report its results. Choice of kernel For our experiments, we use the kernel Kγ (j, k, j 0 , k 0 |x, x0 )   κ(j, k, j 0 , k 0 |x, x0 ) = max 0, 1 − γ where γ denotes the user-specified bandwidth, |j − k |− |j 0 − k 0 | and κ(j, k, j 0 , k 0 |x, x0 ) = if |j − k |+ |j 0 − k 0 | x(j) = x(j 0 ) and x(k 0 ) = x(k), and sign(j − k) = sign(j 0 − k 0 ) (and ∞ otherwise). The kernel is non-zero if and only if the tags at position j and k in x ar"
P14-1100,petrov-etal-2012-universal,0,\N,Missing
P15-1024,D13-1160,0,0.200831,"Missing"
P15-1024,P06-1009,0,0.0240916,"Missing"
P15-1024,W11-2504,0,0.0416848,"Missing"
P15-1024,P14-1048,0,0.0725688,"Missing"
P15-1024,D14-1070,0,0.0602408,"Missing"
P15-1024,P14-1092,0,0.0832203,"Missing"
P15-1024,C02-1150,0,0.0345414,"Missing"
P15-1024,D08-1084,0,0.0152555,"Missing"
P15-1024,D13-1020,1,0.678139,"Missing"
P15-1024,D13-1144,0,0.107596,"Missing"
P15-1024,P12-3013,0,0.0444318,"Missing"
P15-1024,Q14-1018,0,0.00854491,"Missing"
P15-1024,P14-1090,0,0.0840179,"Missing"
P15-1024,P13-2123,0,0.111101,"Missing"
P15-1024,D13-1056,0,0.0205751,"Missing"
P15-1024,P13-1171,0,0.0257642,"Missing"
P15-1125,P14-1023,0,0.062094,"Missing"
P15-1125,D14-1110,0,0.111195,"i=1 in a document, let A = {emi }i=1 be a configuration of its entity assignments. The score of A is formulated as probability P (A|M) ∝ YM i=1 P (emi |mi ) XM j=1 j6=i 1  , d emi , emj +  where for each entity assignment we define its global relatedness to other entity assignments as the sum of the reciprocal distances ( = 0.01 is a constant used to avoid divide-by-zero). Direct enumeration of all potential configurations is computationally prohibitive, we therefore use simulated annealing to search for an optimal solution. 3.2 Entity Search Entity search has attracted a growing interest (Chen et al., 2014b; Balog et al., 2011). Unlike conventional web search that finds unorganized web pages, entity search retrieves knowledge directly by generating a list of relevant entities in response to a search request. The input of the entity search task is a natural language question Q along with one or more desired entity categories C. For example, a query can be Q =“films directed by Akira Kurosawa” and C ={Japanese films}. Previous methods typically score candidate entities by measuring both the similarity between entity content and the query question Q (text matching), and the similarity between cate"
P15-1125,W14-1609,0,0.013967,"el associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014)"
P15-1125,P14-1146,0,0.00571632,"vant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representa"
P15-1125,W13-3212,0,0.0212571,"tegory distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An"
P15-1125,D12-1010,0,0.0219479,"Missing"
P15-1125,D14-1167,0,0.175617,"ssos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An entity hierarchy groups entities into categories which are further organized to form a taxonomy. It provides rich structured knowledge on entity relatedness (Resnik, 1995). Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features (Kaptein and Kamps, 20"
P15-1125,D14-1032,0,0.0416497,"the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure amon"
P15-1125,P14-2050,0,0.00834056,"o capture semantic similarity of the internal categories. This can be helpful in taxonomy refinement and relation discovery. 5 Related Work Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model (Mikolov et al., 2013a) is one of the most popular methods to learn word representations. The model aims to find a representation for each word that is useful for predicting its context words. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works a"
P15-1125,P14-1011,0,0.0169303,"ths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchi"
P16-1043,fujita-etal-2014-overview,0,0.0701904,"Missing"
P16-1043,J00-4006,0,0.0772935,"Missing"
P16-1043,P14-1026,0,0.0823559,"Missing"
P16-1043,D14-1070,0,0.0830061,"Missing"
P16-1043,D13-1020,0,0.28804,"Missing"
P16-1043,P15-1024,1,0.91524,"Missing"
P16-1043,P16-2076,1,0.887127,"Missing"
P16-1043,P13-1171,0,0.0575854,"Missing"
P16-1169,P14-1098,0,0.0431053,"Missing"
P16-1169,I13-1095,0,0.0245992,"of our system. (a) Input: a collection of label items, represented by text and images; (b) Output: we build a taxonomy from scratch by extracting features based on distributed representations of text and images. Introduction Human knowledge is naturally organized as semantic hierarchies. For example, in WordNet (Miller, 1995), specific concepts are categorized and assigned to more general ones, leading to a semantic hierarchical structure (a.k.a taxonomy). A variety of NLP tasks, such as question answering (Harabagiu et al., 2003), document clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 20"
P16-1169,S13-1005,0,0.0372593,"ion of category items (synsets), with associated images and a label hierarchy (sampled from WordNet) over them. The original ImageNet taxonomy is preprocessed, resulting in a tree structure with 28231 nodes. Word embedding training. We train word embedding for synsets by replacing each word/phrase in a synset with a unique token and then using Google’s word2vec tool (Mikolov et al., 2013). We combine three public available corpora together, including the latest Wikipedia dump (Wikipedia, 2014), the One Billion Word Language Modeling Benchmark (Chelba et al., 2013) and the UMBC webbase corpus (Han et al., 2013), resulting in a corpus with total 6 billion tokens. The dimension of the embedding is set to 200. Image processing. we employ the ILSVRC12 pre-trained convolutional neural networks (Simonyan and Zisserman, 2014) to embed each image into the vector space. Then, for each category xn with images, we estimate a multivariate Gaussian parameterized by Nxn = (µxn , Σxn ), and constrain Σxn to be diagonal to prevent overfitting. For categories with very few images, we only estimate a mean vector µxn . For nodes that do not have images, we ignore the visual feature. Training configuration. The feature"
P16-1169,D14-1005,0,0.0260877,". Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies. We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g. ImageNet (Deng et al.,"
P16-1169,P15-2020,0,0.0188338,"nstrate the effectiveness of integrating visual features with language features for taxonomy induction. We also provide qualitative analysis on our features, the learned model, and the taxonomies induced to provide further insights (Section 5.3). 2 Related Work Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015). The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014), syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them, our model is discriminatively trained w"
P16-1169,D10-1108,0,0.40272,"et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and"
P16-1169,P14-1068,0,0.0263621,"sults have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies. We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g."
P16-1169,P06-1101,0,0.272427,"t clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limite"
P16-1169,D14-1088,0,0.0124141,"approaches. Extensive comparisons demonstrate the effectiveness of integrating visual features with language features for taxonomy induction. We also provide qualitative analysis on our features, the learned model, and the taxonomies induced to provide further insights (Section 5.3). 2 Related Work Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015). The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014), syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them,"
P16-1169,D15-1117,0,0.0449436,"ceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and usability for knowledge transfer. Hence, we propose to combine both visual and"
P16-1169,P09-1031,0,0.430576,"et al., 2003), document clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies. Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hie"
P16-1228,N16-1030,0,0.0602172,"-trained word vectors to capture character- and word-level information, respectively. These features are then fed into a bi-directional RNN with LSTM units for sequence tagging. Compared to (Chiu and Nichols, 2015) we omit the character type and capitalization features, as well as the additive transition matrix in the output layer. Figure 3 shows the network architecture. Logic Rules The base network largely makes independent tagging decisions at each position, ignoring the constraints on successive labels for a valid tag sequence (e.g., I-ORG cannot follow B-PER). In contrast to recent work (Lample et al., 2016) which adds a conditional random field (CRF) to capture bi-gram dependencies between outputs, we instead apply logic rules which does not introduce extra parameters to learn. An example rule is: equal(yi−1 , I-ORG) ⇒ ¬ equal(yi , B-PER) 1 NYC Char+Word Representation (6) Replacing ∧ with & in Eq.(5) leads to a probably more intuitive rule which takes the value σθ (B)+ when y = +, and 1 − σθ (B)+ otherwise. Figure 3: The architecture of the bidirectional LSTM recurrent network for NER. The CNN for extracting character representation is omitted. The confidence levels are set to ∞ to prevent any"
P16-1228,D15-1104,0,0.00591606,"based methods (Rows 4-7), including the BLSTM-CRF model (Lample et al., 2016) which applies a conditional random field (CRF) on top of a BLSTM in order to capture the transition patterns and encourage valid sequences. In contrast, our method implements the desired constraints in a more straightforward way by using the declarative logic rule language, and at the same time does not introduce extra model parameters to learn. Further integration of the list rule (Row 3) provides a second boost in performance, achieving an F1 score very close to the best-performing systems including Joint-NER-EL (Luo et al., 2015) (Row 8), a probabilistic graphical model optimizing NER and entity linking jointly with massive external resources, and BLSTM-CRF (Ma and Hovy, 2016), a combination of BLSTM and CRF with more parameters than our rule-enhanced neural networks. From the table we see that the accuracy gap between the joint teacher model q and the distilled student p is relatively larger than in the sentiment classification task (Table 1). This is because in the Discussion and Future Work We have developed a framework which combines deep neural networks with first-order logic rules to allow integrating human know"
P16-1228,P16-1101,1,0.0185807,"der to capture the transition patterns and encourage valid sequences. In contrast, our method implements the desired constraints in a more straightforward way by using the declarative logic rule language, and at the same time does not introduce extra model parameters to learn. Further integration of the list rule (Row 3) provides a second boost in performance, achieving an F1 score very close to the best-performing systems including Joint-NER-EL (Luo et al., 2015) (Row 8), a probabilistic graphical model optimizing NER and entity linking jointly with massive external resources, and BLSTM-CRF (Ma and Hovy, 2016), a combination of BLSTM and CRF with more parameters than our rule-enhanced neural networks. From the table we see that the accuracy gap between the joint teacher model q and the distilled student p is relatively larger than in the sentiment classification task (Table 1). This is because in the Discussion and Future Work We have developed a framework which combines deep neural networks with first-order logic rules to allow integrating human knowledge and intentions into the neural models. In particular, we proposed an iterative distillation procedure that transfers the structured information"
P16-1228,P05-1015,0,0.154071,"Missing"
P16-1228,D14-1162,0,0.117795,"Missing"
P16-1228,D14-1181,0,0.00636895,"NN architecture for sentence-level sentiment analysis. The sentence representation vector is followed by a fully-connected layer with softmax output activation, to output sentiment predictions. 4.1 Sentiment Classification Sentence-level sentiment analysis is to identify the sentiment (e.g., positive or negative) underlying an individual sentence. The task is crucial for many opinion mining applications. One challenging point of the task is to capture the contrastive sense (e.g., by conjunction “but”) within a sentence. Base Network We use the single-channel convolutional network proposed in (Kim, 2014). The simple model has achieved compelling performance on various sentiment classification benchmarks. The network contains a convolutional layer on top of word vectors of a given sentence, followed by a max-over-time pooling layer and then a fullyconnected layer with softmax output activation. A convolution operation is to apply a filter to word windows. Multiple filters with varying window sizes are used to obtain multiple features. Figure 2 shows the network architecture. Logic Rules One difficulty for the plain neural network is to identify contrastive sense in order to capture the dominan"
P16-1228,D13-1170,0,0.00539502,"Missing"
P16-1228,W03-0419,0,0.0329258,"Missing"
P16-1228,P14-1031,0,0.019169,"Missing"
P16-1228,K15-1021,0,0.032223,"Missing"
P16-1228,N16-1178,0,0.0164839,"Missing"
P16-1228,Q16-1026,0,\N,Missing
P16-2076,D13-1160,0,0.1058,"Missing"
P16-2076,W11-0818,0,0.0224838,"Missing"
P16-2076,P06-1009,0,0.102508,"Missing"
P16-2076,D08-1084,0,0.0806499,"Missing"
P16-2076,P14-1048,0,0.0757453,"Missing"
P16-2076,D14-1070,0,0.127152,"Missing"
P16-2076,P14-1092,0,0.0936761,"Missing"
P16-2076,P08-1028,0,0.0763919,"Missing"
P16-2076,D13-1020,0,0.231235,"Missing"
P16-2076,P15-1024,1,0.262277,"Missing"
P16-2076,D13-1144,0,0.0706105,"Missing"
P16-2076,P12-3013,0,0.0471344,"Missing"
P16-2076,Q14-1018,0,0.0457743,"Missing"
P16-2076,P14-1090,0,0.0604213,"Missing"
P16-2076,P13-2123,0,0.121834,"Missing"
P16-2076,P13-1171,0,0.110514,"Missing"
P16-2079,W13-2322,0,0.110281,"Missing"
P16-2079,J10-4006,0,0.0938233,"Missing"
P16-2079,W11-2504,0,0.0202029,"Missing"
P16-2079,D08-1094,0,0.114465,"Missing"
P16-2079,P14-1048,0,0.0219016,"Missing"
P16-2079,P14-1134,0,0.211072,"Missing"
P16-2079,P13-2083,1,0.907161,"Missing"
P16-2079,P14-1092,0,0.0715063,"Missing"
P16-2079,N13-1090,0,0.0888254,"Missing"
P16-2079,P08-1028,0,0.137632,"Missing"
P16-2079,P15-1121,0,0.0943029,"Missing"
P16-2079,D13-1020,0,0.210849,"Missing"
P16-2079,P15-1024,1,0.871984,"Missing"
P16-2079,D15-1197,0,0.072737,"Missing"
P16-2079,D13-1144,0,0.0668489,"Missing"
P16-2079,P15-2115,0,0.0948417,"Missing"
P16-2079,W16-0103,0,0.0199121,"Missing"
P17-1093,P13-2013,0,0.0596199,"ipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an implicit network, serving as an adaptive regularization mechanism for en1007 hanced robustness and generalization. Our framework is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfel"
P17-1093,D15-1262,0,0.320744,"g process. This paper aims to advance implicit parsing by making use of annotated implicit connectives available in training data. Few recent work has explored such combination. Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually suffers from error propagation, and the method itself has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016). Our work is orthogonal and complementary to this line. In this paper, we propose a novel neural method that incorporates implicit connectives in a principled adversarial framework. We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning behavior by extracting simi"
P17-1093,D16-1020,0,0.147016,"Missing"
P17-1093,P17-2096,1,0.756078,"y feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic samples through competition between a generator and a real/fake discriminator. Professor forcing (Lamb et al., 2016) applies a similar idea to improve long-term generation of a recurrent neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017; Liang et al., 2017) extend the framework for controllable image/text generation. Li et"
P17-1093,K15-2005,1,0.912243,"Missing"
P17-1093,P16-1163,0,0.603872,"Missing"
P17-1093,D14-1168,0,0.0670168,"riminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark. 1 [Arg1]: Never mind. [Arg2]: You already know the answer. [Implicit connective]: Because [Discourse relation]: Cause Introduction Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics. Identification of discourse relations can benefit a variety of downstream applications including question answering (Liakata et al., 2013), machine translation (Li et al., 2014), text summarization (Gerani et al., 2014), opinion spam detection (Chen and Zhao, 2015), and so forth. ∗ Corresponding authors. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA0"
P17-1093,P16-1228,1,0.784768,"extract highly discriminative features by mimicking a connective-augmented network. Our method achieved state-of-the-art performance for implicit discourse relation classification. Besides implicit connective examples, our model can naturally exploit enormous explicit connective data to further improve discourse parsing. The proposed adversarial feature imitation scheme is also generally applicable to other context to incorporate indicative side information available at training time for enhanced inference. Our framework shares a similar spirit of the iterative knowledge distillation method (Hu et al., 2016a,b) which train a “student” network to mimic the classification behavior of a knowledgeinformed “teacher” network. Our approach encourages imitation on the feature level instead of the final prediction level. This allows our approach to apply to regression tasks, and more interestingly, the context in which the student and teacher networks have different prediction outputs, e.g., performing different tasks, while transferring knowledge between each other can be beneficial. Besides, our adversarial mechanism provides an adaptive metric to measure and drive the imitation procedure. 1014 Referen"
P17-1093,D16-1173,1,0.838978,"extract highly discriminative features by mimicking a connective-augmented network. Our method achieved state-of-the-art performance for implicit discourse relation classification. Besides implicit connective examples, our model can naturally exploit enormous explicit connective data to further improve discourse parsing. The proposed adversarial feature imitation scheme is also generally applicable to other context to incorporate indicative side information available at training time for enhanced inference. Our framework shares a similar spirit of the iterative knowledge distillation method (Hu et al., 2016a,b) which train a “student” network to mimic the classification behavior of a knowledgeinformed “teacher” network. Our approach encourages imitation on the feature level instead of the final prediction level. This allows our approach to apply to regression tasks, and more interestingly, the context in which the student and teacher networks have different prediction outputs, e.g., performing different tasks, while transferring knowledge between each other can be beneficial. Besides, our adversarial mechanism provides an adaptive metric to measure and drive the imitation procedure. 1014 Referen"
P17-1093,Q15-1024,0,0.422786,"and future work. 2 2.1 Related Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage connective information. Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an impli"
P17-1093,N16-1037,0,0.119389,"lated Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage connective information. Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an implicit network, servi"
P17-1093,D15-1264,0,0.126217,"ims to advance implicit parsing by making use of annotated implicit connectives available in training data. Few recent work has explored such combination. Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually suffers from error propagation, and the method itself has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016). Our work is orthogonal and complementary to this line. In this paper, we propose a novel neural method that incorporates implicit connectives in a principled adversarial framework. We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning behavior by extracting similarly salient fea"
P17-1093,P13-1047,0,0.04657,"on accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an implicit network, serving as an adaptive regularization mechanism for en1007 hanced robustness and generalization. Our framework is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et a"
P17-1093,P14-2047,0,0.0324499,"licit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark. 1 [Arg1]: Never mind. [Arg2]: You already know the answer. [Implicit connective]: Because [Discourse relation]: Cause Introduction Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics. Identification of discourse relations can benefit a variety of downstream applications including question answering (Liakata et al., 2013), machine translation (Li et al., 2014), text summarization (Gerani et al., 2014), opinion spam detection (Chen and Zhao, 2015), and so forth. ∗ Corresponding authors. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Socie"
P17-1093,K16-2008,1,0.907018,"Missing"
P17-1093,D13-1070,0,0.0212508,"n scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark. 1 [Arg1]: Never mind. [Arg2]: You already know the answer. [Implicit connective]: Because [Discourse relation]: Cause Introduction Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics. Identification of discourse relations can benefit a variety of downstream applications including question answering (Liakata et al., 2013), machine translation (Li et al., 2014), text summarization (Gerani et al., 2014), opinion spam detection (Chen and Zhao, 2015), and so forth. ∗ Corresponding authors. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCR"
P17-1093,D09-1036,0,0.784133,"Missing"
P17-1093,D16-1130,0,0.531433,"ous methods. Section 5 discusses extensions and future work. 2 2.1 Related Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage connective information. Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discrim"
P17-1093,P09-1077,0,0.834033,"201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA041). Various attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c). Despite impressive performance, the absence of strong explicit connective cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connectives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining"
P17-1093,prasad-etal-2008-penn,0,0.131206,"eatly improves over standalone neural models and previous bestperforming approaches. We also demonstrate that our implicit recognition network successfully imitates and extracts crucial hidden representations. We begin by briefly reviewing related work in section 2. Section 3 presents the proposed adversarial model. Section 4 shows substantially improved experimental results over previous methods. Section 5 discusses extensions and future work. 2 2.1 Related Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage"
P17-1093,C16-1180,1,0.833631,"Missing"
P17-1093,K16-2010,1,0.883387,"43 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA041). Various attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c). Despite impressive performance, the absence of strong explicit connective cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connectives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining both the implicit connective and contextual semantics. 1006 Proceedings of the 55"
P17-1093,D16-1246,1,0.897429,"43 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA041). Various attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c). Despite impressive performance, the absence of strong explicit connective cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connectives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining both the implicit connective and contextual semantics. 1006 Proceedings of the 55"
P17-1093,N15-1081,0,0.19495,"tion to guide the reasoning process. This paper aims to advance implicit parsing by making use of annotated implicit connectives available in training data. Few recent work has explored such combination. Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually suffers from error propagation, and the method itself has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016). Our work is orthogonal and complementary to this line. In this paper, we propose a novel neural method that incorporates implicit connectives in a principled adversarial framework. We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning beha"
P17-1093,N16-1064,1,0.80589,"ework is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic samples through competition between a generator and a real/fake discriminator. Professor forcing (Lamb et al., 2016) applies a similar idea to improve long-term generation of a recurrent neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017; Liang et al., 2017) extend the framework for co"
P17-1093,D16-1037,0,0.030373,"o-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic samples through competition between a generator and a real/fake discriminator. Professor forcing (Lamb et al., 2016) applies a similar idea to improve long-term generation of a recurrent neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017; Liang et al., 2017) extend the framework for controllable image/tex"
P17-1093,K15-2001,0,0.0848951,"prior work of implicit discourse relation classification, we evaluate on two popular experimental settings: 1) multi-class classification for 2nd-level types (Lin et al., 2009; Ji and Eisenstein, 2015), and 2) oneversus-others binary classifications for 1st-level classes (Pitler et al., 2009). We describe the detailed configurations in the following respective sections. We will focus our analysis on the multiclass classification setting, which is most realistic in practice and serves as a building block for a complete discourse parser such as that for the shared tasks of CoNLL-2015 and 2016 (Xue et al., 2015, 2016). Model Training Here we provide the detailed architecture configurations of each component we used in the experiments. Discriminator The discriminator is a binary classifier to identify the correct source of an input feature vector. To make it a strong rival to the feature imitating network (i-CNN), we model the discriminator as a multi-layer perceptron (MLP) enhanced with gated mechanism for efficient information flow (Srivastava et al., 2015; Qin et al., 2016c), as shown in Figure 3. 4 Experiment Setup • Throughout the experiments i-CNN and aCNN contain 3 sets of convolutional filter"
P17-1093,P16-1131,1,0.805493,"Missing"
P17-1093,K16-2001,0,0.225587,"Missing"
P17-1093,C10-2172,0,0.715684,"tion network is then trained to correctly classify relations and simultaneously to fool the discriminator, resulting in an adversarial framework. The adversarial mechanism has been an emerging method in different context, especially for image generation (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c). Our adversarial framework is unique to address neural feature emulation between two models. Besides, to the best of our knowledge, this is the first adversarial approach in the context of discourse parsing. Compared to previous connective exploiting work (Zhou et al., 2010; Xu et al., 2012), our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation. Our method is evaluated on the PDTB 2.0 benchmark in a variety of experimental settings. The proposed adversarial model greatly improves over standalone neural models and previous bestperforming approaches. We also demonstrate that our implicit recognition network successfully imitates and extracts crucial hidden representations. We begin by briefly reviewing related work in section 2. Section 3 presents the proposed adversarial model"
P17-1129,D14-1058,0,0.023364,"eport written for? the king of France 2. What is the newer, more widely accepted theory behind the spread of the plague? bad air 3. What is the bad air theory officially known as? Miasma theory Figure 1: An example of the SQuAD QA task Introduction Reading comprehension (RC) aims to answer questions by understanding texts, which is a challenge task in natural language processing. Various RC tasks and datasets have been developed, including Machine Comprehension Test (Richardson et al., 2013) for multiple-choice question answering (QA) (Sachan et al., 2015; Wang and McAllester, 2015), Algebra (Hosseini et al., 2014) and Science (Clark and Etzioni, 2016) for passing standardized tests (Clark et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and Children’s Book Test (Hill et al., 2015) for cloze-style QA (Chen et al., 2016; Shen et al., 2016), WikiQA (Yang et al., 2015), Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and Microsoft Machine Reading Comprehension (Nguyen et al., 2016) for open domain QA. In this paper, we are specifically interested in solving the SQuAD QA task (Figure 1 shows an example), in light of its following features: (1) large scale: 107,785 questions, 23,215"
P17-1129,P16-1086,0,0.0421428,"l., 2016), LSTM (Seo et al., 2016), pointer network (Vinyals et al., 2015; Wang and Jiang, 2016), dynamic pointer decoder (Xiong et al., 2016). The representation learning in previous approaches is conducted over individual words using the following encoders: LSTM in (Wang et al., 2016; Xiong et al., 2016); bi-directional gated recurrent unit (Chung et al., 2014) in (Yu et al., 2016); match-LSTM in (Wang and Jiang, 2016); bi-directional LSTM in (Seo et al., 2016). In previous approaches, the attention (Bahdanau et al., 2014; Xu et al., 2015) mechanism is mostly word-based and flat-structured (Kadlec et al., 2016; Sordoni et al., 2016; Wang and Jiang, 1412 2016; Wang et al., 2016; Yu et al., 2016): the attention scores are computed between individual words, are normalized globally and are used to summarize word-level encodings in a flat manner. Cui et al. (2016); Xiong et al. (2016) explored a coattention mechanism to learn question-topassage and passage-to-question summaries. Seo et al. (2016) proposed to directly use the attention weights as augmented features instead of applying them for early summarization. 5 Conclusions and Future Work To solve the SQuAD question answering problem, we design a co"
P17-1129,P16-1223,0,0.0464693,"Missing"
P17-1129,P14-5010,0,0.00638931,"Missing"
P17-1129,D14-1162,0,0.0797877,"↑ . For an internal node, the inputs are the hidden states and memory cells of its children and the transition equations are defined as: P (i,l) (l) (i) i↑ = σ( L l=1 W↑ h↑ + b↑ ) (l) (f,l) (l) (f,l) ∀l, f↑ = σ(W↑ h↑ + b↑ ) P (o,l) (l) (o) h↑ + b↑ ) o↑ = σ( L l=1 W↑ P (u,l) (l) (u) h↑ + b↑ ) u↑ = tanh( L l=1 W↑ P (l) (l) c↑ = i↑ u↑ + L l=1 f↑ c↑ h↑ = o↑ tanh(c↑ ) (1) where the weight parameters W and bias parame(i,l) ters b with superscript l such as W↑ are specific to the l-th child. For a leaf node which represents a single word, it has no forget gate and the input is the wording embedding (Pennington et al., 2014) of this word. In the top-down direction, the gates, memory cell and hidden state are defined in a similar fashion as the bottom-up direction (Eq.(1)). For an internal node except the root, the inputs are the hid(p) (p) den state h↓ and memory cell c↓ of its parents. (p) (p) For a leaf node, in addition to h↓ and c↓ , the inputs also contain the word embedding. For the (r) root node, the top-down hidden state h↓ is set to (r) (r) its bottom-up hidden state h↑ . h↑ captures the semantics of all constituents, which is then replicated as hr↓ and propagated downwards to each individual constituent"
P17-1129,D16-1264,0,0.346427,"h is a challenge task in natural language processing. Various RC tasks and datasets have been developed, including Machine Comprehension Test (Richardson et al., 2013) for multiple-choice question answering (QA) (Sachan et al., 2015; Wang and McAllester, 2015), Algebra (Hosseini et al., 2014) and Science (Clark and Etzioni, 2016) for passing standardized tests (Clark et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and Children’s Book Test (Hill et al., 2015) for cloze-style QA (Chen et al., 2016; Shen et al., 2016), WikiQA (Yang et al., 2015), Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and Microsoft Machine Reading Comprehension (Nguyen et al., 2016) for open domain QA. In this paper, we are specifically interested in solving the SQuAD QA task (Figure 1 shows an example), in light of its following features: (1) large scale: 107,785 questions, 23,215 paragraphs; (2) nonsynthetic: questions are generated by crowdworkers; (3) large search space of candidate answers. We study two major problems: (1) how to generate candidate answers? Unlike in multiplechoice QA and cloze-style QA where a small amount of answer choices are given, an answer in SQuAD could be any span in the text,"
P17-1129,D13-1020,0,0.0433602,"e of the art performance and the ablation study corroborates the effectiveness of individual modules. 1 1. Who was the medical report written for? the king of France 2. What is the newer, more widely accepted theory behind the spread of the plague? bad air 3. What is the bad air theory officially known as? Miasma theory Figure 1: An example of the SQuAD QA task Introduction Reading comprehension (RC) aims to answer questions by understanding texts, which is a challenge task in natural language processing. Various RC tasks and datasets have been developed, including Machine Comprehension Test (Richardson et al., 2013) for multiple-choice question answering (QA) (Sachan et al., 2015; Wang and McAllester, 2015), Algebra (Hosseini et al., 2014) and Science (Clark and Etzioni, 2016) for passing standardized tests (Clark et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and Children’s Book Test (Hill et al., 2015) for cloze-style QA (Chen et al., 2016; Shen et al., 2016), WikiQA (Yang et al., 2015), Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and Microsoft Machine Reading Comprehension (Nguyen et al., 2016) for open domain QA. In this paper, we are specifically interested in solving"
P17-1129,P15-1024,1,0.842256,"tiveness of individual modules. 1 1. Who was the medical report written for? the king of France 2. What is the newer, more widely accepted theory behind the spread of the plague? bad air 3. What is the bad air theory officially known as? Miasma theory Figure 1: An example of the SQuAD QA task Introduction Reading comprehension (RC) aims to answer questions by understanding texts, which is a challenge task in natural language processing. Various RC tasks and datasets have been developed, including Machine Comprehension Test (Richardson et al., 2013) for multiple-choice question answering (QA) (Sachan et al., 2015; Wang and McAllester, 2015), Algebra (Hosseini et al., 2014) and Science (Clark and Etzioni, 2016) for passing standardized tests (Clark et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and Children’s Book Test (Hill et al., 2015) for cloze-style QA (Chen et al., 2016; Shen et al., 2016), WikiQA (Yang et al., 2015), Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and Microsoft Machine Reading Comprehension (Nguyen et al., 2016) for open domain QA. In this paper, we are specifically interested in solving the SQuAD QA task (Figure 1 shows an example), in light of its fo"
P17-1129,P15-1150,0,0.408478,"s the search space. Different from (Rajpurkar et al., 2016) who only consider exact constituents, we adopt a constituent expansion mechanism which greatly improves the coverage of correct answers. For the representation learning of candidate answers which are sequences of constituents, we first encode individual constituents using a chainof-trees LSTM (CT-LSTM) and tree-guided attention mechanism, then feed these encodings into a chain LSTM (Hochreiter and Schmidhuber, 1997) to generate representations for the constituent sequences. The CT-LSTM seamlessly integrates intra-sentence tree LSTMs (Tai et al., 2015) which capture the local syntactic properties of constituents and an inter-sentence chain LSTM which glues together the sequence of tree LSTMs such that the semantics of each sentence can be propagated to others. The tree-guided attention leverages the hierarchical relations among constituents to learn question-aware representations. 60 Miasma theory Candidate Answers Tree-‐Guided Attention Encoder Chain-‐of-‐Trees LSTM for Passage Encoding That the plague was caused by bad air became the most widely accepted theory. Today, this is known as the Miasma theory. Tree LSTM for Question Encoding"
P17-1129,P15-2115,0,0.020207,"l modules. 1 1. Who was the medical report written for? the king of France 2. What is the newer, more widely accepted theory behind the spread of the plague? bad air 3. What is the bad air theory officially known as? Miasma theory Figure 1: An example of the SQuAD QA task Introduction Reading comprehension (RC) aims to answer questions by understanding texts, which is a challenge task in natural language processing. Various RC tasks and datasets have been developed, including Machine Comprehension Test (Richardson et al., 2013) for multiple-choice question answering (QA) (Sachan et al., 2015; Wang and McAllester, 2015), Algebra (Hosseini et al., 2014) and Science (Clark and Etzioni, 2016) for passing standardized tests (Clark et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and Children’s Book Test (Hill et al., 2015) for cloze-style QA (Chen et al., 2016; Shen et al., 2016), WikiQA (Yang et al., 2015), Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and Microsoft Machine Reading Comprehension (Nguyen et al., 2016) for open domain QA. In this paper, we are specifically interested in solving the SQuAD QA task (Figure 1 shows an example), in light of its following features: (1) large"
P18-1098,W07-1013,0,0.877913,"ge summaries, using a combination of three classifiers: k-nearest neighbors, relevance feedback, and Bayesian independence classifiers. This method assigns a single code to each patient visit. However, in clinical practice, each patient is usually assigned with multiple codes. Franz et al. (2000) investigated the automated coding of German-language free-text diagnosis phrases. This approach performs one-to-one mapping between diagnosis descriptions and ICD codes. This is not in accordance with the coding practice where one-to-many and many-to-one mappings widely exist (O’malley et al., 2005). Pestian et al. (2007) studied the assignment of ICD-9 codes to radiology reports. Kavuluru et al. (2013) proposed an unsupervised ensemble approach to automatically perform ICD-9 coding based on textual narratives in electronic health records (EHRs) Kavuluru et al. (2015) developed multi-label classification, feature selection, and learning to rank approaches for ICD-9 code assignment of in-patient visits based on EHRs. Koopman et al. (2015) explored the automatic ICD-10 classification of cancers from free-text death certificates. These methods did not consider the hierarchical relationship or importance order amo"
P18-1098,P15-1150,0,0.192851,"orts. Kavuluru et al. (2013) proposed an unsupervised ensemble approach to automatically perform ICD-9 coding based on textual narratives in electronic health records (EHRs) Kavuluru et al. (2015) developed multi-label classification, feature selection, and learning to rank approaches for ICD-9 code assignment of in-patient visits based on EHRs. Koopman et al. (2015) explored the automatic ICD-10 classification of cancers from free-text death certificates. These methods did not consider the hierarchical relationship or importance order among codes. The tree LSTM network was first proposed by (Tai et al., 2015) to model the constituent or dependency parse trees of sentences. Teng and Zhang (2016) extended the unidirectional tree LSTM to a bidirectional one. Xie and Xing (2017) proposed a sequence-of-trees LSTM network to model a passage. In this network, a sequential LSTM is used to compose a sequence of tree LSTMs. The tree LSTMs are built on the constituent parse trees of individual sentences and the sequential LSTM is built on the sequence of sentences. Our proposed tree-of-sequences LSTM network differs from the previous works in twofold. First, it is applied to a code tree to capture the hierar"
P18-1098,P17-1129,1,0.92801,"(EHRs) Kavuluru et al. (2015) developed multi-label classification, feature selection, and learning to rank approaches for ICD-9 code assignment of in-patient visits based on EHRs. Koopman et al. (2015) explored the automatic ICD-10 classification of cancers from free-text death certificates. These methods did not consider the hierarchical relationship or importance order among codes. The tree LSTM network was first proposed by (Tai et al., 2015) to model the constituent or dependency parse trees of sentences. Teng and Zhang (2016) extended the unidirectional tree LSTM to a bidirectional one. Xie and Xing (2017) proposed a sequence-of-trees LSTM network to model a passage. In this network, a sequential LSTM is used to compose a sequence of tree LSTMs. The tree LSTMs are built on the constituent parse trees of individual sentences and the sequential LSTM is built on the sequence of sentences. Our proposed tree-of-sequences LSTM network differs from the previous works in twofold. First, it is applied to a code tree to capture the hierarchical relationship among codes. Second, it uses a tree LSTM to compose a hierarchy 1067 Diagnosis Descriptions 1. Prematurity at 35 4/7 weeks gestation 2. Twin number t"
P18-1098,N16-1174,0,0.0443806,"ng importance. of sequential LSTMs. Adversarial learning (Goodfellow et al., 2014) has been widely applied to image generation (Goodfellow et al., 2014), domain adaption (Ganin and Lempitsky, 2015), feature learning (Donahue et al., 2016), text generation (Yu et al., 2017), to name a few. In this paper, we use adversarial learning for mitigating the discrepancy among the writing styles of a pair of sentences. The attention mechanism was widely used in machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), reading comprehension (Seo et al., 2016), text classification (Yang et al., 2016), etc. In this work, we compute attention between sentences to perform many-to-one and one-to-many mappings. 3 Dataset and Preprocessing We performed the study on the publicly available MIMIC-III dataset (Johnson et al., 2016), which contains de-identified electronic health records (EHRs) of 58,976 patient visits in the Beth Israel Deaconess Medical Center from 2001 to 2012. Each EHR has a clinical note called discharge summary, which contains multiple sections of information, such as ‘discharge diagnosis’, ‘past medical history’, etc. From the ‘discharge diagnosis’ and ‘final diagnosis’ secti"
P18-1240,W14-3348,0,0.00924385,"uipped with a hierarchical LSTM decoder. To further show the effectiveness of the proposed coattention mechanism, we also implemented two ablated versions of our model: Ours-Semanticonly and Ours-Visual-only, which takes solely the semantic attention or visual attention context vector to produce topic vectors. 4.4 Quantitative Results We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image captioning evaluation tool 4 which provides evaluation on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedantam et al., 2015). For paragraph generation, as shown in the upper part of Table 1, it is clear that models with a single LSTM decoder perform much worse than those with a hierarchical LSTM decoder. Note that the only difference between Ours-no-Attention and CNN-RNN (Vinyals et al., 2015) is that Oursno-Attention adopts a hierarchical LSTM decoder while CNN-RNN (Vinyals et al., 2015) adopts a single-layer LSTM. The comparison between these two models directly demonstrates the effectiveness of the hierarchical LSTM. This result is not surprising since it is"
P18-1240,W04-1013,0,0.182156,"oder. To further show the effectiveness of the proposed coattention mechanism, we also implemented two ablated versions of our model: Ours-Semanticonly and Ours-Visual-only, which takes solely the semantic attention or visual attention context vector to produce topic vectors. 4.4 Quantitative Results We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image captioning evaluation tool 4 which provides evaluation on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedantam et al., 2015). For paragraph generation, as shown in the upper part of Table 1, it is clear that models with a single LSTM decoder perform much worse than those with a hierarchical LSTM decoder. Note that the only difference between Ours-no-Attention and CNN-RNN (Vinyals et al., 2015) is that Oursno-Attention adopts a hierarchical LSTM decoder while CNN-RNN (Vinyals et al., 2015) adopts a single-layer LSTM. The comparison between these two models directly demonstrates the effectiveness of the hierarchical LSTM. This result is not surprising since it is well-known that a s"
P18-1240,D15-1280,0,0.0273604,"ion, as shown in the upper part of Table 1, it is clear that models with a single LSTM decoder perform much worse than those with a hierarchical LSTM decoder. Note that the only difference between Ours-no-Attention and CNN-RNN (Vinyals et al., 2015) is that Oursno-Attention adopts a hierarchical LSTM decoder while CNN-RNN (Vinyals et al., 2015) adopts a single-layer LSTM. The comparison between these two models directly demonstrates the effectiveness of the hierarchical LSTM. This result is not surprising since it is well-known that a single-layer LSTM cannot effectively model long sequences (Liu et al., 2015; Martin and Cundy, 2018). Additionally, employing semantic attention alone (Ours-Semantic-only) or visual attention alone (Ours-Visual-only) to generate topic vectors does not seem to help caption generation a lot. The potential reason might be that visual at2582 4 https://github.com/tylin/coco-caption Figure 3: Illustration of paragraph generated by Ours-CoAttention, Ours-no-Attention, and Soft Attention models. The underlined sentences are the descriptions of detected abnormalities. The second image is a lateral x-ray image. Top two images are positive results, the third one is a partial fa"
P18-1240,P02-1040,0,0.10972,"NN-RNN (Vinyals et al., 2015) equipped with a hierarchical LSTM decoder. To further show the effectiveness of the proposed coattention mechanism, we also implemented two ablated versions of our model: Ours-Semanticonly and Ours-Visual-only, which takes solely the semantic attention or visual attention context vector to produce topic vectors. 4.4 Quantitative Results We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image captioning evaluation tool 4 which provides evaluation on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedantam et al., 2015). For paragraph generation, as shown in the upper part of Table 1, it is clear that models with a single LSTM decoder perform much worse than those with a hierarchical LSTM decoder. Note that the only difference between Ours-no-Attention and CNN-RNN (Vinyals et al., 2015) is that Oursno-Attention adopts a hierarchical LSTM decoder while CNN-RNN (Vinyals et al., 2015) adopts a single-layer LSTM. The comparison between these two models directly demonstrates the effectiveness of the hierarchical LSTM. This r"
P19-1565,J90-1003,0,0.136582,"module (middle panel, section 4.3) for producing the next response. ticularly suitable for the learning in section 5. Architecturally, we study three different approaches as representative paradigms for predicting the next-turn keyword distribution, including pairwise keyword linear transition, neural-based prediction, and kernel-based method. Pairwise PMI-based Transition The most straightforward way for keyword transition is to construct a keyword pairwise matrix that characterizes the association between keywords in the observed conversation data. We use pointwise mutual information (PMI) (Church and Hanks, 1990) as the measure, which, given two keywords wi and wj , computes likeliness of wj → wi with PMI(wi , wj ) = log p(wi |wj )/p(wi ), (1) where p(wi |wj ) is the ratio of transitioning to wi in the next turn given wj in the current turn, and p(wi ) is the ratio of wi occurrence. Both quantities can be directly counted from the conversation data beforehand. At test time, we first use a keyword extractor (section 5) to extract keywords of the current utterance. Assuming all these keywords are independent, for each candidate next keyword, we sum up their PMI scores w.r.t the candidate. The resulting"
P19-1565,P17-1045,0,0.0249,"kernelbased method. We conduct quantitative and human evaluations to measure the performance of sub-modules and the whole system. Our agent is able to generate meaningful and effective conversations with a decent success rate of reaching the targets, improving over other approaches in different respects. We show target-guided open-domain conversation is a promising and potentially important direction for future research. domain) systems. For task-oriented dialogue systems, the system is designed to accomplish specific goals, e.g., providing bus schedule (Raux et al., 2005; Young et al., 2007; Dhingra et al., 2017). Besides information giving, other tasks have been extensively studied, such as negotiations (DeVault et al., 2015; Lewis et al., 2017; He et al., 2018; Cao et al., 2018), symmetric collaborations (He et al., 2017), etc. 2 Due to the lack of full supervision data, the solution proposed in this work divides the task into two competitive sub-objectives, each of which can be conquered with either direct supervision or simple rules. Such a divide-and-conquer approach Related Work The past end-to-end dialogue research can be broadly divided into two categories: task-oriented dialogue systems and c"
P19-1565,N18-5020,0,0.0882194,"he agent is given a target subject e-books which is unknown to the human. The goal is to guide the conversation naturally to the target. Utterance keywords are highlighted in red (agent) and blue (human) and in italic. Creating intelligent agent that can carry out opendomain conversation with human is a long-lasting challenge. Impressive progress has been made, advancing from early rule-based systems, e.g., Eliza (Weizenbaum et al., 1966), to recent end-toend neural conversation models that are trained on massive data (Shang et al., 2015; Li et al., 2015) and make use of background knowledge (Fang et al., 2018; Qin et al., 2019; Liu et al., 2018). However, current open-domain systems still struggle to conduct engaging conversations (Ram ∗ corresponding authors Data and code are publicly available https://github.com/squareRoot3/ Target-Guided-Conversation book Really? You are smart. Introduction 1 work at et al., 2018), and often generate inconsistent or uncontrolled results. Further, many practical opendomain dialogue applications do have specific goals to achieve even though the conversations are open-ended, e.g., accomplishing nursing goals in therapeutic conversation, inspiring ideas in educatio"
P19-1565,P17-1162,0,0.0132548,"rate of reaching the targets, improving over other approaches in different respects. We show target-guided open-domain conversation is a promising and potentially important direction for future research. domain) systems. For task-oriented dialogue systems, the system is designed to accomplish specific goals, e.g., providing bus schedule (Raux et al., 2005; Young et al., 2007; Dhingra et al., 2017). Besides information giving, other tasks have been extensively studied, such as negotiations (DeVault et al., 2015; Lewis et al., 2017; He et al., 2018; Cao et al., 2018), symmetric collaborations (He et al., 2017), etc. 2 Due to the lack of full supervision data, the solution proposed in this work divides the task into two competitive sub-objectives, each of which can be conquered with either direct supervision or simple rules. Such a divide-and-conquer approach Related Work The past end-to-end dialogue research can be broadly divided into two categories: task-oriented dialogue systems and chat-oriented (a.k.a openOn the other hand, chat-oriented dialogue systems have been created to model open-domain conversations without specific goals. Prior work has been focusing on developing novel neural architec"
P19-1565,D16-1230,0,0.0486424,"t, and focus on measuring how accurate the systems can predict the next keyword and retrieve the correct response on the test set of the conversation data. The evaluation largely follows the protocol of previous chit-chat systems (e.g., Wu et al., 2016), and validates the effect of the keyword-augmented conversation production. Evaluation metrics For the keyword prediction task, we measure three metrics: (1) Rw @K: keywords recall at position K (= 1, 3, 5) in all (over 2600) possible keywords, (2) P @1: precision at the first position, and (3) Cor.: the word embedding based correlation score (Liu et al., 2016). For the response selection task, we randomly sample 19 negative responses for each test case, and calculate R20 @K, i.e., recall at position K in the 20 candidate (positive and negative) responses, as well as MRR, the mean reciprocal rank. Results Table 3 shows the evaluation results. Our system with Kernel transition module outperforms all other systems in terms of all metrics on both two tasks, expect for R20 @3 where the system with PMI transition performs best. The Kernel approach can predict the next keywords more precisely. In the task of response selection, our systems that are augmen"
P19-1565,P18-1138,0,0.0197952,"books which is unknown to the human. The goal is to guide the conversation naturally to the target. Utterance keywords are highlighted in red (agent) and blue (human) and in italic. Creating intelligent agent that can carry out opendomain conversation with human is a long-lasting challenge. Impressive progress has been made, advancing from early rule-based systems, e.g., Eliza (Weizenbaum et al., 1966), to recent end-toend neural conversation models that are trained on massive data (Shang et al., 2015; Li et al., 2015) and make use of background knowledge (Fang et al., 2018; Qin et al., 2019; Liu et al., 2018). However, current open-domain systems still struggle to conduct engaging conversations (Ram ∗ corresponding authors Data and code are publicly available https://github.com/squareRoot3/ Target-Guided-Conversation book Really? You are smart. Introduction 1 work at et al., 2018), and often generate inconsistent or uncontrolled results. Further, many practical opendomain dialogue applications do have specific goals to achieve even though the conversations are open-ended, e.g., accomplishing nursing goals in therapeutic conversation, inspiring ideas in education, making recommendation and persuasi"
P19-1565,D18-1256,0,0.131208,"ishing nursing goals in therapeutic conversation, inspiring ideas in education, making recommendation and persuasion, and so forth. Thus, there is a strong demand to enable the integration of goals and strategy into opendomain dialogue systems, and it imposes challenges to both: first, how to define the goal for an open-domain chat system; and second, how to encode dialogue strategy into the response production process. It is also crucial to attain a general method that is not tailored towards specialized goals that require domain-specific handcrafting and annotations (Yarats and Lewis, 2018; He et al., 2018; Li et al., 2018). 5624 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5624–5634 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics This paper makes a step towards open-domain dialogue agents with conversational goals. In particular, we want the system to chat naturally with humans on open domain topics and proactively guide the conversation to a designated target subject. For example, in Figure 1, given a target e-books and an arbitrary starting topic such as tired, the agent drives the conversation in a"
P19-1565,D14-1162,0,0.0894678,"t one keyword from the keyword set. Such a pipeline approach achieves a strong baseline performance, as shown in the following. Ours As in section 4.1, our full system has several variants in the turn-level keyword transition module, including the PMI, Neural, and Kernel methods. For comparison, we also use a Random method which randomly picks a keyword for next response. Training Details We use the same configuration for the common parts of all agents. We apply a single-layer GRU (Chung et al., 2014) in all encoders. Both the word embedding and hidden dimensions are set to 200. We use GloVe (Pennington et al., 2014) to initialize word embeddings. We apply Adam optimization (Kingma and Ba, 2014) with an initial learning rate of 0.001 and annealing to 0.0001 in 10 epochs. Systems are implemented with a text generation toolkit Texar (Hu et al., 2019). 6.2 Turn-level Evaluation We first evaluate the performance of each conversation turn, in terms of both turn-level keyword prediction and response selection. That is, we disable the discourse-level target constraint, and focus on measuring how accurate the systems can predict the next keyword and retrieve the correct response on the test set of the conversatio"
P19-1565,P19-3027,1,0.875144,"Missing"
P19-1565,P19-1539,0,0.0419751,"target subject e-books which is unknown to the human. The goal is to guide the conversation naturally to the target. Utterance keywords are highlighted in red (agent) and blue (human) and in italic. Creating intelligent agent that can carry out opendomain conversation with human is a long-lasting challenge. Impressive progress has been made, advancing from early rule-based systems, e.g., Eliza (Weizenbaum et al., 1966), to recent end-toend neural conversation models that are trained on massive data (Shang et al., 2015; Li et al., 2015) and make use of background knowledge (Fang et al., 2018; Qin et al., 2019; Liu et al., 2018). However, current open-domain systems still struggle to conduct engaging conversations (Ram ∗ corresponding authors Data and code are publicly available https://github.com/squareRoot3/ Target-Guided-Conversation book Really? You are smart. Introduction 1 work at et al., 2018), and often generate inconsistent or uncontrolled results. Further, many practical opendomain dialogue applications do have specific goals to achieve even though the conversations are open-ended, e.g., accomplishing nursing goals in therapeutic conversation, inspiring ideas in education, making recommen"
P19-1565,W18-5028,1,0.859263,"Missing"
P19-1565,D17-1259,0,0.0180563,"t is able to generate meaningful and effective conversations with a decent success rate of reaching the targets, improving over other approaches in different respects. We show target-guided open-domain conversation is a promising and potentially important direction for future research. domain) systems. For task-oriented dialogue systems, the system is designed to accomplish specific goals, e.g., providing bus schedule (Raux et al., 2005; Young et al., 2007; Dhingra et al., 2017). Besides information giving, other tasks have been extensively studied, such as negotiations (DeVault et al., 2015; Lewis et al., 2017; He et al., 2018; Cao et al., 2018), symmetric collaborations (He et al., 2017), etc. 2 Due to the lack of full supervision data, the solution proposed in this work divides the task into two competitive sub-objectives, each of which can be conquered with either direct supervision or simple rules. Such a divide-and-conquer approach Related Work The past end-to-end dialogue research can be broadly divided into two categories: task-oriented dialogue systems and chat-oriented (a.k.a openOn the other hand, chat-oriented dialogue systems have been created to model open-domain conversations without"
P19-1565,P15-1152,0,0.142678,"Missing"
P19-1565,P16-1094,0,0.0151385,"chat-oriented (a.k.a openOn the other hand, chat-oriented dialogue systems have been created to model open-domain conversations without specific goals. Prior work has been focusing on developing novel neural architectures that improve next utterance generation or retrieval task performance by training on large open-domain chit-chat dataset (Sordoni et al., 2015; Serban et al., 2016; Zhou et al., 2016; Wu et al., 2018). However, despite the steady improvement over model architectures, the current systems can still suffer from a range of limitations, e.g., dull responses, inconsistent persona (Li et al., 2016a), etc. The commercial chatbot XiaoIce (Zhou et al., 2018) and the first Amazon Alexa challenge winner (Fang et al., 2018) have stressed to improve engagement with users. Also, to encourage discourse-level strategy, prior work has developed different system action representations that enable the model to reason at the dialogue level. One line of work has utilized latent variable models (Zhao et al., 2017; Yarats and Lewis, 2018; Zhao et al., 2019) to infer a latent representation of system responses, which separates the natural language generation process from decision-making. Another approac"
P19-1565,D16-1127,0,0.0215524,"chat-oriented (a.k.a openOn the other hand, chat-oriented dialogue systems have been created to model open-domain conversations without specific goals. Prior work has been focusing on developing novel neural architectures that improve next utterance generation or retrieval task performance by training on large open-domain chit-chat dataset (Sordoni et al., 2015; Serban et al., 2016; Zhou et al., 2016; Wu et al., 2018). However, despite the steady improvement over model architectures, the current systems can still suffer from a range of limitations, e.g., dull responses, inconsistent persona (Li et al., 2016a), etc. The commercial chatbot XiaoIce (Zhou et al., 2018) and the first Amazon Alexa challenge winner (Fang et al., 2018) have stressed to improve engagement with users. Also, to encourage discourse-level strategy, prior work has developed different system action representations that enable the model to reason at the dialogue level. One line of work has utilized latent variable models (Zhao et al., 2017; Yarats and Lewis, 2018; Zhao et al., 2019) to infer a latent representation of system responses, which separates the natural language generation process from decision-making. Another approac"
P19-1565,N15-1020,0,0.0861176,"Missing"
P19-1565,P17-1062,0,0.0180165,"Alexa challenge winner (Fang et al., 2018) have stressed to improve engagement with users. Also, to encourage discourse-level strategy, prior work has developed different system action representations that enable the model to reason at the dialogue level. One line of work has utilized latent variable models (Zhao et al., 2017; Yarats and Lewis, 2018; Zhao et al., 2019) to infer a latent representation of system responses, which separates the natural language generation process from decision-making. Another approach has created hybrid systems to incorporate hand-crafted coarse-grained actions (Williams et al., 2017; He et al., 2018) as a part of the neural dialogue systems. These systems have typically focused on specific domains such as price negotiation and movie recommendation. Building upon the prior work, this paper creates novelty in terms of both defining goals for open-domain chatting and creating system actions representations. Our structured solution use predicted keywords as a non-parametric representation of the intended content for the next system response. 5625 represents a general means of addressing complex task objectives with no end-to-end supervision available. A similar approach has"
P19-1565,P18-1103,0,0.0134554,"supervision or simple rules. Such a divide-and-conquer approach Related Work The past end-to-end dialogue research can be broadly divided into two categories: task-oriented dialogue systems and chat-oriented (a.k.a openOn the other hand, chat-oriented dialogue systems have been created to model open-domain conversations without specific goals. Prior work has been focusing on developing novel neural architectures that improve next utterance generation or retrieval task performance by training on large open-domain chit-chat dataset (Sordoni et al., 2015; Serban et al., 2016; Zhou et al., 2016; Wu et al., 2018). However, despite the steady improvement over model architectures, the current systems can still suffer from a range of limitations, e.g., dull responses, inconsistent persona (Li et al., 2016a), etc. The commercial chatbot XiaoIce (Zhou et al., 2018) and the first Amazon Alexa challenge winner (Fang et al., 2018) have stressed to improve engagement with users. Also, to encourage discourse-level strategy, prior work has developed different system action representations that enable the model to reason at the dialogue level. One line of work has utilized latent variable models (Zhao et al., 201"
P19-1565,P18-1205,0,0.103397,". We decouple the whole system into separate modules and address the challenges at different granularity. Specifically, we explicitly model and control the intended content of each system response by introducing coarse-grained utterance keywords. We then impose a discourse-level rule that encourages the keywords to approach the end target during the course of the conversation; and we attain smooth conversation transition at each dialogue turn through turn-level supervised learning. To this end, we further derive a keyword-augmented conversation dataset from an existing daily-life chat corpus (Zhang et al., 2018) and use it for learning keyword transitions and utterance production. We study different keyword transition approaches, including pairwise PMI-based transition, neural-based prediction, and a hybrid kernelbased method. We conduct quantitative and human evaluations to measure the performance of sub-modules and the whole system. Our agent is able to generate meaningful and effective conversations with a decent success rate of reaching the targets, improving over other approaches in different respects. We show target-guided open-domain conversation is a promising and potentially important direct"
P19-1565,N19-1123,1,0.80909,"dy improvement over model architectures, the current systems can still suffer from a range of limitations, e.g., dull responses, inconsistent persona (Li et al., 2016a), etc. The commercial chatbot XiaoIce (Zhou et al., 2018) and the first Amazon Alexa challenge winner (Fang et al., 2018) have stressed to improve engagement with users. Also, to encourage discourse-level strategy, prior work has developed different system action representations that enable the model to reason at the dialogue level. One line of work has utilized latent variable models (Zhao et al., 2017; Yarats and Lewis, 2018; Zhao et al., 2019) to infer a latent representation of system responses, which separates the natural language generation process from decision-making. Another approach has created hybrid systems to incorporate hand-crafted coarse-grained actions (Williams et al., 2017; He et al., 2018) as a part of the neural dialogue systems. These systems have typically focused on specific domains such as price negotiation and movie recommendation. Building upon the prior work, this paper creates novelty in terms of both defining goals for open-domain chatting and creating system actions representations. Our structured soluti"
P19-1565,P17-1061,1,0.841958,"Wu et al., 2018). However, despite the steady improvement over model architectures, the current systems can still suffer from a range of limitations, e.g., dull responses, inconsistent persona (Li et al., 2016a), etc. The commercial chatbot XiaoIce (Zhou et al., 2018) and the first Amazon Alexa challenge winner (Fang et al., 2018) have stressed to improve engagement with users. Also, to encourage discourse-level strategy, prior work has developed different system action representations that enable the model to reason at the dialogue level. One line of work has utilized latent variable models (Zhao et al., 2017; Yarats and Lewis, 2018; Zhao et al., 2019) to infer a latent representation of system responses, which separates the natural language generation process from decision-making. Another approach has created hybrid systems to incorporate hand-crafted coarse-grained actions (Williams et al., 2017; He et al., 2018) as a part of the neural dialogue systems. These systems have typically focused on specific domains such as price negotiation and movie recommendation. Building upon the prior work, this paper creates novelty in terms of both defining goals for open-domain chatting and creating system ac"
P19-1565,D16-1036,0,0.0202488,"with either direct supervision or simple rules. Such a divide-and-conquer approach Related Work The past end-to-end dialogue research can be broadly divided into two categories: task-oriented dialogue systems and chat-oriented (a.k.a openOn the other hand, chat-oriented dialogue systems have been created to model open-domain conversations without specific goals. Prior work has been focusing on developing novel neural architectures that improve next utterance generation or retrieval task performance by training on large open-domain chit-chat dataset (Sordoni et al., 2015; Serban et al., 2016; Zhou et al., 2016; Wu et al., 2018). However, despite the steady improvement over model architectures, the current systems can still suffer from a range of limitations, e.g., dull responses, inconsistent persona (Li et al., 2016a), etc. The commercial chatbot XiaoIce (Zhou et al., 2018) and the first Amazon Alexa challenge winner (Fang et al., 2018) have stressed to improve engagement with users. Also, to encourage discourse-level strategy, prior work has developed different system action representations that enable the model to reason at the dialogue level. One line of work has utilized latent variable models"
P19-1657,P18-1240,1,0.845913,"lar appearance of the cardiac silhouette. Considerations would include pericardial effusion or dilated cardiomyopathy. Figure 1: An example of chest X-ray image along with its report. In the report, the Findings section records detailed descriptions for normal and abnormal findings; the Impression section provides a diagnostic conclusion. The underlined sentence is an abnormal finding. Introduction Chest X-Ray (CXR) image report generation aims to automatically generate detailed findings and diagnoses for given images, which has attracted growing attention in recent years (Wang et al., 2018a; Jing et al., 2018; Li et al., 2018). This technique can greatly reduce the workload of radiologists for interpreting CXR images and writing corresponding reports. In spite of the progress made in this area, it is still challenging for computers to accurately write reports. Besides the difficulties in detecting lesions from images, the complex structure of textual reports can prevent the success of automatic report generation. As shown in Figure 1, the report for a CXR image usually comprises two major sections: Findings and Impression. Findings section records detailed descriptions about normal and abnormal fi"
P19-1657,P02-1040,0,0.119794,"tem (CMAS), which consists of three agents: Planner (PL), Abnormality Writer (AW) and Normality Writer (NW). Given an image, the system will run several loops until PL decides to stop the process. Within each loop, the agents cooperate with each other in the following fashion: 1) PL examines an area of the input image (or a sentence of Findings), and decides whether the examined area contains lesions. 2) Either AW or NW will generate a sentence for the area based on the order given by PL. To train the system, REINFORCE algorithm (Williams, 1992) is applied to optimize the reward (e.g. BLEU-4 (Papineni et al., 2002)). To the best of our knowledge, our work is the first effort to investigate the structure of CXR reports. The major contributions of our work are summarized as follows. First, we propose a twostage framework by exploiting the structure of the reports. Second, We propose a novel Cooperative Multi-Agent System (CMAS) for modeling the sentence generation process of each section. Third, we perform extensive quantitative experiments to evaluate the overall quality of the generated reports, as well as the model’s ability for detecting medical abnormality terms. Finally, we perform substantial quali"
P19-1657,P17-1117,0,0.0124802,"t al., 2016), were proposed for improving the performances. Some other efforts have been made for building variants of the hierarchical Long-Short-Term-Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to generate paragraphs (Krause et al., 2017; Yu et al., 2016; Liang et al., 2017). Recently, deep reinforcement learning has attracted growing attention in the field of visual captioning (Ren et al., 2017; Rennie et al., 2017; Liu et al., 2017; Wang et al., 2018b). Additionally, other tasks related to visual captioning, (e.g., dense captioning (Johnson et al., 2016), multi-task learning (Pasunuru and Bansal, 2017)) also attracted a lot of research attention. Chest X-ray Image Report Generation Shin et al. (2016) first proposed a variant of CNN-RNN framework to predict tags (location and severity) of chest X-ray images. Wang et al. (2018a) proposed a joint framework for generating reference reports and performing disease classification at the same time. However, this method was based on a single-sentence generation model (Xu et al., 2015), and obtained low BLEU scores. Jing et al. (2018) proposed a hierarchical language model equipped with co-attention to better model the paragraphs, but it tended to pr"
P19-1657,W04-1013,0,\N,Missing
P19-3027,J90-2002,0,0.853908,"a high concept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io.1 1 Introduction Text generation spans a broad set of natural language processing tasks that aim to generate natural language from input data or machine representations. Such tasks include machine translation (Brown et al., 1990; Bahdanau et al., 2014), dialog systems (Williams and Young, 2007; Serban et al., 2016; Tang et al., 2019), text summarization (Hovy and Lin, 1998), text paraphrasing and manipulation (Madnani and Dorr, 2010; Hu et al., 2017; Lin et al., 2019), and more. Recent years have seen rapid progress of this active area, in part due to the integration of modern deep learning approaches in many of the tasks. On the other hand, considerable research efforts are still needed 1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794 159 Proceedings of the 57th Annual Meetin"
P19-3027,N19-1423,0,0.0249524,"e to other parts of the model. Using customized modules with the library APIs is even more flexible, since the APIs are designed to be fully compatible with native TensorFlow/PyTorch programming interfaces. 4 Plug-in and Swap-out Modules Case Study: Transformer on Different Tasks We present a case study to show that Texar can greatly reduce implementation efforts and enable technique sharing among different tasks. Transformer, as first introduced in (Vaswani et al., 2017), has greatly improved the machine translation results and created other successful models such as BERT for text embedding (Devlin et al., 2019) and GPT-2 for language modeling (Radford et al., 2018). Texar supports easy construction of It is convenient to change from one modeling paradigm to another by simply plugging in/swapping out a single or few modules, or even merely changing a configuration parameter. For example, given the base code of an encoderdecoder model in Figure 3 (right panel), Figure 4 illustrates how one can switch between different learning paradigms by changing only Lines.14–19 of the original code (maximum-likelihood learn162 (a) Adversarial learning "" 0/1 Discriminator ""! 1 ""! 2 discriminator = Conv1DClassifier("
P19-3027,P16-1154,0,0.105926,"Missing"
P19-3027,X98-1026,0,0.299552,"o fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io.1 1 Introduction Text generation spans a broad set of natural language processing tasks that aim to generate natural language from input data or machine representations. Such tasks include machine translation (Brown et al., 1990; Bahdanau et al., 2014), dialog systems (Williams and Young, 2007; Serban et al., 2016; Tang et al., 2019), text summarization (Hovy and Lin, 1998), text paraphrasing and manipulation (Madnani and Dorr, 2010; Hu et al., 2017; Lin et al., 2019), and more. Recent years have seen rapid progress of this active area, in part due to the integration of modern deep learning approaches in many of the tasks. On the other hand, considerable research efforts are still needed 1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794 159 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 159–164 c Florence, Italy, July 28 - August 2, 2019. 2019 Associati"
P19-3027,E17-3017,0,0.0765728,"Missing"
P19-3027,P18-4020,0,0.0235839,"Missing"
P19-3027,P17-4012,0,0.221271,".g., Figure 4). Modularity makes Texar particularly suitable for fast prototyping and experimentation. Extensibility. The toolkit provides interfaces ranging from simple configuration files to full library APIs. Users of different needs and expertise are free to choose different interfaces for appropriate programmability and internal accessibility. The library APIs are fully compatible with the na2 Related Work There exist several toolkits that focus on one or a few specific tasks. For neural machine translation and alike, there are Tensor2Tensor (Vaswani et al., 2018) on TensorFlow, OpenNMT (Klein et al., 2017) on PyTorch, Nematus (Sennrich et al., 2017) on Theano, MarianNMT (JunczysDowmunt et al., 2018) on C++, etc. ParlAI (Miller et al., 2017) is a specialized platform for dialogue. Differing from the task-focusing tools, Texar aims to cover as many text generation tasks as possible. The goal of versatility poses unique design challenges. On the other end of the spectrum, there are libraries for more general NLP or ML applications: AllenNLP (allennlp.org), GluonNLP (gluon-nlp.mxnet.io) and others are designed for the broad NLP tasks in general, while Keras (keras.io) is for high conceptual-level p"
P19-3027,W18-1819,0,0.0255903,"learning involves only minimal code changes (e.g., Figure 4). Modularity makes Texar particularly suitable for fast prototyping and experimentation. Extensibility. The toolkit provides interfaces ranging from simple configuration files to full library APIs. Users of different needs and expertise are free to choose different interfaces for appropriate programmability and internal accessibility. The library APIs are fully compatible with the na2 Related Work There exist several toolkits that focus on one or a few specific tasks. For neural machine translation and alike, there are Tensor2Tensor (Vaswani et al., 2018) on TensorFlow, OpenNMT (Klein et al., 2017) on PyTorch, Nematus (Sennrich et al., 2017) on Theano, MarianNMT (JunczysDowmunt et al., 2018) on C++, etc. ParlAI (Miller et al., 2017) is a specialized platform for dialogue. Differing from the task-focusing tools, Texar aims to cover as many text generation tasks as possible. The goal of versatility poses unique design challenges. On the other end of the spectrum, there are libraries for more general NLP or ML applications: AllenNLP (allennlp.org), GluonNLP (gluon-nlp.mxnet.io) and others are designed for the broad NLP tasks in general, while Ker"
P19-3027,D15-1166,0,0.0440152,"rder to improve techniques and enable realworld applications. A few remarkable open-source toolkits have been developed (section 2) which largely focus on one or a few specific tasks or algorithms. Emerging new applications and approaches instead are often developed by individual teams in a more adhoc manner, which can easily result in hard-tomaintain custom code and duplicated efforts. The variety of text generation tasks indeed have many common properties and share a set of key underlying techniques, such as neural encoderdecoders (Sutskever et al., 2014), attentions (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017), memory networks (Sukhbaatar et al., 2015), adversarial methods (Goodfellow et al., 2014; Lamb et al., 2016), reinforcement learning (Ranzato et al., 2015; Tan et al., 2018), structured supervision (Hu et al., 2018; Yang et al., 2018), as well as optimization techniques, data pre-processing and result post-processing, evaluations, etc. These techniques are often combined together in various ways to tackle different problems. Figure 1 summarizes examples of various model architectures. It is therefore highly desirable to have an opensource platform that unifies the devel"
P19-3027,J10-3003,0,0.0195804,"lkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io.1 1 Introduction Text generation spans a broad set of natural language processing tasks that aim to generate natural language from input data or machine representations. Such tasks include machine translation (Brown et al., 1990; Bahdanau et al., 2014), dialog systems (Williams and Young, 2007; Serban et al., 2016; Tang et al., 2019), text summarization (Hovy and Lin, 1998), text paraphrasing and manipulation (Madnani and Dorr, 2010; Hu et al., 2017; Lin et al., 2019), and more. Recent years have seen rapid progress of this active area, in part due to the integration of modern deep learning approaches in many of the tasks. On the other hand, considerable research efforts are still needed 1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794 159 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 159–164 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Prior A ? E ? ? D E E1 D1 ?"
P19-3027,D17-2014,0,0.051363,"es interfaces ranging from simple configuration files to full library APIs. Users of different needs and expertise are free to choose different interfaces for appropriate programmability and internal accessibility. The library APIs are fully compatible with the na2 Related Work There exist several toolkits that focus on one or a few specific tasks. For neural machine translation and alike, there are Tensor2Tensor (Vaswani et al., 2018) on TensorFlow, OpenNMT (Klein et al., 2017) on PyTorch, Nematus (Sennrich et al., 2017) on Theano, MarianNMT (JunczysDowmunt et al., 2018) on C++, etc. ParlAI (Miller et al., 2017) is a specialized platform for dialogue. Differing from the task-focusing tools, Texar aims to cover as many text generation tasks as possible. The goal of versatility poses unique design challenges. On the other end of the spectrum, there are libraries for more general NLP or ML applications: AllenNLP (allennlp.org), GluonNLP (gluon-nlp.mxnet.io) and others are designed for the broad NLP tasks in general, while Keras (keras.io) is for high conceptual-level programming without specific task focuses. In comparison, Texar has a proper focus on the text generation sub-area, and provide a comprehe"
P19-3027,P17-1061,1,0.898451,"Missing"
Q14-1015,D10-1124,1,0.880985,"Missing"
Q14-1015,N09-1058,0,0.0314007,"adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. c Submitted 10/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 2003). To our knowledge,"
Q14-1015,D09-1079,0,0.0148112,"nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. c Submitted 10/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. line variational algorithms (Sato, 20"
Q14-1015,N10-1062,0,0.0210279,"he data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. c Submitted 10/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 200"
S17-1029,D15-1086,0,0.0218667,"logy (Meltzoff and Moore, 1977; Meltzoff, 1995). Learning from demonstration is a popular way of learning policies from example state to action mappings in robotics applications. Imitation learning (Schaal, 1999; Abbeel and Ng, 2004; Ross et al., 2011) is a popular instance of learning from demonstration where the algorithm observes a human expert perform a series of actions to accomplish the task and learns a policy that “imitates” the expert with the purpose of generalizing to unseen data. Imitation learning is increasingly being used in NLP (Vlachos and Clark, 2014; Berant and Liang, 2015; Augenstein et al., 2015; Beck et al., 2016; Goodman et al., 2016a,b). However, all these models focus on learning respective NLP models from the final supervision e.g. semantic parses or denotations. However, we provide a technique to learn from demonstrations by learning a joint semantic parsing and deduction model. Another related line of work is Hixon et al. (2015) who acquire knowledge in the form of knowledge graphs for question answering from natural language dialogs and (Goldwasser and Roth, 2014) who propose a technique called learning from natural instructions. Learning from natural instructions allows huma"
S17-1029,fujita-etal-2014-overview,0,0.0308129,"Solving Geometry Problems: Standardized tests have been recently proposed as ‘drivers for progress in AI’ (Clark and Etzioni, 2016). These tests are easily accessible, and measurable, and hence have attracted several NLP researchers. There is a growing body of work on solving standardized tests such as reading comprehensions (Richardson et al., 2013, inter alia), science question answering (Clark, 2015; Schoenick et al., 2016, inter alia), algebra word problems (Kushman et al., 2014; Roy and Roth, 2015, inter alia), geometry problems (Seo et al., 2014, 2015) and pre-university entrance exams (Fujita et al., 2014; Arai and Matsuzaki, 2014). While the problem of using computers to solve geometry questions is old (Feigenbaum and Feldman, 1963; Schattschneider and King, 1997; Davis, 2006), NLP and vision techniques were first used to solve geometry problems in Seo et al. (2015). While Seo et al. (2014) only aligned geometric shapes with their textual mentions, Seo User Study on Interpretability A key benefit of our axiomatic solver is that it provides an easy-to-understand student-friendly demonstrative solution to geometry problems. This is important because students typically learn geometry by rigorous"
S17-1029,W16-2381,0,0.029102,"1977; Meltzoff, 1995). Learning from demonstration is a popular way of learning policies from example state to action mappings in robotics applications. Imitation learning (Schaal, 1999; Abbeel and Ng, 2004; Ross et al., 2011) is a popular instance of learning from demonstration where the algorithm observes a human expert perform a series of actions to accomplish the task and learns a policy that “imitates” the expert with the purpose of generalizing to unseen data. Imitation learning is increasingly being used in NLP (Vlachos and Clark, 2014; Berant and Liang, 2015; Augenstein et al., 2015; Beck et al., 2016; Goodman et al., 2016a,b). However, all these models focus on learning respective NLP models from the final supervision e.g. semantic parses or denotations. However, we provide a technique to learn from demonstrations by learning a joint semantic parsing and deduction model. Another related line of work is Hixon et al. (2015) who acquire knowledge in the form of knowledge graphs for question answering from natural language dialogs and (Goldwasser and Roth, 2014) who propose a technique called learning from natural instructions. Learning from natural instructions allows human teachers to inter"
S17-1029,P16-1001,0,0.013322,"95). Learning from demonstration is a popular way of learning policies from example state to action mappings in robotics applications. Imitation learning (Schaal, 1999; Abbeel and Ng, 2004; Ross et al., 2011) is a popular instance of learning from demonstration where the algorithm observes a human expert perform a series of actions to accomplish the task and learns a policy that “imitates” the expert with the purpose of generalizing to unseen data. Imitation learning is increasingly being used in NLP (Vlachos and Clark, 2014; Berant and Liang, 2015; Augenstein et al., 2015; Beck et al., 2016; Goodman et al., 2016a,b). However, all these models focus on learning respective NLP models from the final supervision e.g. semantic parses or denotations. However, we provide a technique to learn from demonstrations by learning a joint semantic parsing and deduction model. Another related line of work is Hixon et al. (2015) who acquire knowledge in the form of knowledge graphs for question answering from natural language dialogs and (Goldwasser and Roth, 2014) who propose a technique called learning from natural instructions. Learning from natural instructions allows human teachers to interact with an automated"
S17-1029,D13-1160,0,0.121509,"Missing"
S17-1029,S16-1180,0,0.0353193,"95). Learning from demonstration is a popular way of learning policies from example state to action mappings in robotics applications. Imitation learning (Schaal, 1999; Abbeel and Ng, 2004; Ross et al., 2011) is a popular instance of learning from demonstration where the algorithm observes a human expert perform a series of actions to accomplish the task and learns a policy that “imitates” the expert with the purpose of generalizing to unseen data. Imitation learning is increasingly being used in NLP (Vlachos and Clark, 2014; Berant and Liang, 2015; Augenstein et al., 2015; Beck et al., 2016; Goodman et al., 2016a,b). However, all these models focus on learning respective NLP models from the final supervision e.g. semantic parses or denotations. However, we provide a technique to learn from demonstrations by learning a joint semantic parsing and deduction model. Another related line of work is Hixon et al. (2015) who acquire knowledge in the form of knowledge graphs for question answering from natural language dialogs and (Goldwasser and Roth, 2014) who propose a technique called learning from natural instructions. Learning from natural instructions allows human teachers to interact with an automated"
S17-1029,Q15-1039,0,0.0128768,"in developmental psychology (Meltzoff and Moore, 1977; Meltzoff, 1995). Learning from demonstration is a popular way of learning policies from example state to action mappings in robotics applications. Imitation learning (Schaal, 1999; Abbeel and Ng, 2004; Ross et al., 2011) is a popular instance of learning from demonstration where the algorithm observes a human expert perform a series of actions to accomplish the task and learns a policy that “imitates” the expert with the purpose of generalizing to unseen data. Imitation learning is increasingly being used in NLP (Vlachos and Clark, 2014; Berant and Liang, 2015; Augenstein et al., 2015; Beck et al., 2016; Goodman et al., 2016a,b). However, all these models focus on learning respective NLP models from the final supervision e.g. semantic parses or denotations. However, we provide a technique to learn from demonstrations by learning a joint semantic parsing and deduction model. Another related line of work is Hixon et al. (2015) who acquire knowledge in the form of knowledge graphs for question answering from natural language dialogs and (Goldwasser and Roth, 2014) who propose a technique called learning from natural instructions. Learning from natural"
S17-1029,N15-1086,0,0.0325116,"erform a series of actions to accomplish the task and learns a policy that “imitates” the expert with the purpose of generalizing to unseen data. Imitation learning is increasingly being used in NLP (Vlachos and Clark, 2014; Berant and Liang, 2015; Augenstein et al., 2015; Beck et al., 2016; Goodman et al., 2016a,b). However, all these models focus on learning respective NLP models from the final supervision e.g. semantic parses or denotations. However, we provide a technique to learn from demonstrations by learning a joint semantic parsing and deduction model. Another related line of work is Hixon et al. (2015) who acquire knowledge in the form of knowledge graphs for question answering from natural language dialogs and (Goldwasser and Roth, 2014) who propose a technique called learning from natural instructions. Learning from natural instructions allows human teachers to interact with an automated learner using natural instructions, allowing the teacher to communicate the domain expertise to the learner via natural language. However, this work was evaluated on a very simple Freecell game with a very small number of concepts (3). On the other hand, our model is evaluated on a real task of solving SA"
S17-1029,C16-1278,0,0.158225,"d φ is the concatenation [φφ 1 φ 2 ]. The complexity of searching for the highest scoring latent parse is exponential. Hence, we use beam search with a fixed beam size (100) for inference. That is, in each step, we only expand the ten most promising candidates so far given by the current score. We first infer p1 to identify a beam of concepts. Then, we infer p2 to identify relations among candidate concepts. We find the optimal parameters θ p using maximum-likelihood estimation with L2 regularization: θ ∗p = arg max θp ∑ (x,p)∈Train 4.2 Our deductive solver, inspired from Krishnamurthy et al. (2016), uses the parsed state and axiom information (when provided) and learns to score the sequence of axiom applications which can lead to the solution of the problem. Our solver uses a log-linear model over the space of possible axiom applications. Given a set of theorems T and optionally demonstration d, we assume T = [t1 ,t2 , . . .tk ] to be a sequence of theorem applications. Each theorem application leads to a change in state. Let s0 be the initial state determined by the logical formula derived from the question text and the diagram. Let s = [s1 , s2 , . . . sk ] be the sequence of program"
S17-1029,D15-1202,0,0.0251586,"n model alone. 6.2 Interpretability GEOS++ O.S. 2.7 3.0 3.0 3.7 2.7 3.6 2.4 3.4 2.8 3.1 2.7 3.4 7 Related Work Solving Geometry Problems: Standardized tests have been recently proposed as ‘drivers for progress in AI’ (Clark and Etzioni, 2016). These tests are easily accessible, and measurable, and hence have attracted several NLP researchers. There is a growing body of work on solving standardized tests such as reading comprehensions (Richardson et al., 2013, inter alia), science question answering (Clark, 2015; Schoenick et al., 2016, inter alia), algebra word problems (Kushman et al., 2014; Roy and Roth, 2015, inter alia), geometry problems (Seo et al., 2014, 2015) and pre-university entrance exams (Fujita et al., 2014; Arai and Matsuzaki, 2014). While the problem of using computers to solve geometry questions is old (Feigenbaum and Feldman, 1963; Schattschneider and King, 1997; Davis, 2006), NLP and vision techniques were first used to solve geometry problems in Seo et al. (2015). While Seo et al. (2014) only aligned geometric shapes with their textual mentions, Seo User Study on Interpretability A key benefit of our axiomatic solver is that it provides an easy-to-understand student-friendly demo"
S17-1029,D16-1016,0,0.303627,"lly the fact that ∠AMO is 90◦ to conclude that ∠MOA is 60◦ . (2) Conclude that 4MOA ∼ 4MOB (using a similar triangle theorem) and then, conclude that ∠MOB = ∠MOA = 60◦ (using the theorem that corresponding angles of similar triangles are equal). (3) Use angle sum rule to conclude that ∠AOB = ∠MOB + ∠MOA = 120◦ . (4) Use the theorem that the angle subtended by an arc of a circle at the centre is double the angle subtended by it at any point on the circle to conclude that ∠ADB = 0.5×∠AOB = 60◦ . etry problems. We also present a technique inspired from recent work in situated question answering (Krishnamurthy et al., 2016) that jointly learns how to interpret the demonstration and use this interpretation to solve geometry problems. We model the interpretation task (the task of recognizing various states in the demonstration) as a semantic parsing task. We model state transitions in the demonstration via a deduction model that treats each application of a theorem of geometry as a state transition. We describe techniques to learn the two models separately as well as jointly from various kinds of supervision: (a) when we only have a set of question-answer pairs as supervision, (b) when we have a set of questions a"
S17-1029,P14-1026,0,0.054531,"mpared to the deduction model alone. 6.2 Interpretability GEOS++ O.S. 2.7 3.0 3.0 3.7 2.7 3.6 2.4 3.4 2.8 3.1 2.7 3.4 7 Related Work Solving Geometry Problems: Standardized tests have been recently proposed as ‘drivers for progress in AI’ (Clark and Etzioni, 2016). These tests are easily accessible, and measurable, and hence have attracted several NLP researchers. There is a growing body of work on solving standardized tests such as reading comprehensions (Richardson et al., 2013, inter alia), science question answering (Clark, 2015; Schoenick et al., 2016, inter alia), algebra word problems (Kushman et al., 2014; Roy and Roth, 2015, inter alia), geometry problems (Seo et al., 2014, 2015) and pre-university entrance exams (Fujita et al., 2014; Arai and Matsuzaki, 2014). While the problem of using computers to solve geometry questions is old (Feigenbaum and Feldman, 1963; Schattschneider and King, 1997; Davis, 2006), NLP and vision techniques were first used to solve geometry problems in Seo et al. (2015). While Seo et al. (2014) only aligned geometric shapes with their textual mentions, Seo User Study on Interpretability A key benefit of our axiomatic solver is that it provides an easy-to-understand s"
S17-1029,P11-1060,0,0.0965876,"Missing"
S17-1029,D15-1171,0,0.329235,"acher and the learner generalizes from these demonstrations in order to execute the task. In this paper, we introduce the novel task of question answering using natural language As a case study, we propose the task of learning to solve SAT geometry problems (such as the one in Figure 1) using demonstrative solutions to these problems (such as the one in Figure 2). Such demonstrations are common in textbooks as they help students learn how to solve geometry problems effectively. We build a new dataset of demonstrative solutions of geometry problems and show that it can be used to improve GEOS (Seo et al., 2015), the state-of-the-art in solving geom251 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 251–261, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics 1. Sum of interior angles of a triangle is 1800 =&gt; OAM + AMO + MOA = 1800 =&gt; MOA = 600 vided to students on educational platforms such as MOOCs to assist in their learning. We present an experimental evaluation of our approach on the two datasets previously introduced in Seo et al. (2015) and a new dataset collected by us from a number of math textbooks in India."
S17-1029,P14-5010,0,0.00284455,"velopment (150 questions) and test (906 questions) with equal proportion of grade 6-10 questions. We also annotated the training and development set questions with ground-truth logical forms. GEOS used 13 types of entities, 94 functions and predicates. We added some more entities, functions and predicates to cover other more complex concepts in geometry not covered in GEOS. Thus, we obtained a final set of 19 entity types and 115 functions and predicates. We use the training set to train our semantic parser with expanded set of entity types, functions and predicates. We used Stanford CoreNLP (Manning et al., 2014) for linguistic pre-processing. We also adapted the GEOS solver to the expanded set of entities, functions and predicates for comparison purposes. We call this system GEOS++. 6.1 P 61 62 63 66 67 Quantitative Results We evaluated our joint model of semantic parsing and deduction with various settings for training: training on question-answer pairs or demonstra257 GEOS O.S. (Parser) O.S. (Joint) P 0.82 0.88 0.89 R 0.63 0.75 0.80 F1 0.71 0.81 0.84 Grade 6 Grade 7 Grade 8 Grade 9 Grade 10 Overall Table 4: Precision, Recall and F1 scores of the parses induced by GEOS and our models when only the p"
S17-1029,Q14-1042,0,0.0129814,"e work on social learning in developmental psychology (Meltzoff and Moore, 1977; Meltzoff, 1995). Learning from demonstration is a popular way of learning policies from example state to action mappings in robotics applications. Imitation learning (Schaal, 1999; Abbeel and Ng, 2004; Ross et al., 2011) is a popular instance of learning from demonstration where the algorithm observes a human expert perform a series of actions to accomplish the task and learns a policy that “imitates” the expert with the purpose of generalizing to unseen data. Imitation learning is increasingly being used in NLP (Vlachos and Clark, 2014; Berant and Liang, 2015; Augenstein et al., 2015; Beck et al., 2016; Goodman et al., 2016a,b). However, all these models focus on learning respective NLP models from the final supervision e.g. semantic parses or denotations. However, we provide a technique to learn from demonstrations by learning a joint semantic parsing and deduction model. Another related line of work is Hixon et al. (2015) who acquire knowledge in the form of knowledge graphs for question answering from natural language dialogs and (Goldwasser and Roth, 2014) who propose a technique called learning from natural instruction"
S17-1029,S12-1060,0,0.0358529,"Missing"
S17-1029,D13-1020,0,0.0216395,"s as well as demonstrations achieves an even better accuracy. Our joint model of parsing and deduction induces more accurate programs as compared to the deduction model alone. 6.2 Interpretability GEOS++ O.S. 2.7 3.0 3.0 3.7 2.7 3.6 2.4 3.4 2.8 3.1 2.7 3.4 7 Related Work Solving Geometry Problems: Standardized tests have been recently proposed as ‘drivers for progress in AI’ (Clark and Etzioni, 2016). These tests are easily accessible, and measurable, and hence have attracted several NLP researchers. There is a growing body of work on solving standardized tests such as reading comprehensions (Richardson et al., 2013, inter alia), science question answering (Clark, 2015; Schoenick et al., 2016, inter alia), algebra word problems (Kushman et al., 2014; Roy and Roth, 2015, inter alia), geometry problems (Seo et al., 2014, 2015) and pre-university entrance exams (Fujita et al., 2014; Arai and Matsuzaki, 2014). While the problem of using computers to solve geometry questions is old (Feigenbaum and Feldman, 1963; Schattschneider and King, 1997; Davis, 2006), NLP and vision techniques were first used to solve geometry problems in Seo et al. (2015). While Seo et al. (2014) only aligned geometric shapes with thei"
W05-0804,J93-2003,0,0.0271161,"Missing"
W05-0804,C00-2163,0,0.0204768,"HMM is defined as follows: P (f1J |eI1 ) = J XY P (fj |eaj )P (aj |aj−1 ), (2) j=1 aJ 1 26 where P (aj |aj−1 ) is the transition probability. This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence. An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with. The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior. The HMM with these refinements is used as our baseline. Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters. 2.2 Extensions to HMM with word clusters Let F denote the cluster mapping fj → F(fj ), which assigns French word fj to its cluster ID Fj = F(fj ). Similarly E maps English word ei to its cluster ID of Ei = E(ei ). In this paper, we assume each word belongs to one cluster only. With bilingual word clusters, we can extend the HMM model in Eqn. 1 in the following two ways: P (f1J |eI1 ) = P aJ 1 QJ j=1 P (fj |eaj )· P (aj |aj−1 , E(eaj−1 ), F(fj−1 )), (3) where E(eaj−"
W05-0804,J03-1002,0,0.00752818,"ch French word fj is an observation, and it is generated by a HMM state defined as [eaj , aj ], where the alignment aj for position j is considered to have a dependency on the previous alignment aj−1 . Thus the first-order HMM is defined as follows: P (f1J |eI1 ) = J XY P (fj |eaj )P (aj |aj−1 ), (2) j=1 aJ 1 26 where P (aj |aj−1 ) is the transition probability. This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence. An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with. The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior. The HMM with these refinements is used as our baseline. Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters. 2.2 Extensions to HMM with word clusters Let F denote the cluster mapping fj → F(fj ), which assigns French word fj to its cluster ID Fj = F(fj ). Similarly E maps English word ei to its cluster ID of Ei = E(ei ). In this paper, we assume each wor"
W05-0804,E99-1010,0,0.493638,"(Ei |Ei−1 )P (ei |Ei ). (7) i=1 We need to fix the number of clusters beforehand, otherwise the optimum is reached when each word 27 = arg max {F } J Y P (Fj |Eaj )P (fj |Fj ). (8) j=1 Overall, this bilingual word clustering algorithm is essentially a two-step approach. In the first step, E is inferred by optimizing the monolingual likelihood of English data, and secondly F is inferred by optimizing the bilingual part without changing E. In this way, the algorithm is easy to implement without much change from the monolingual correspondent. This approach was shown to give the best results in (Och, 1999). We use it as our baseline to compare with. 3.2 Bilingual Word Spectral Clustering Instead of using word alignment to bridge the parallel sentence pair, and optimize the likelihood in two separate steps, we develop an alignment-free algorithm using a variant of spectral clustering algorithm. The goal is to build high cluster-level translation quality suitable for translation modelling, and at the same time maintain high intra-cluster similarity , and low inter-cluster similarity for monolingual clusters. 3.2.1 Notations We define the vocabulary VF as the French vocabulary with a size of |VF |"
W05-0804,W02-1012,0,0.0294654,"Missing"
W05-0804,C96-2141,0,0.484864,"o an English sentence with I words denoted by eI1 = e1 e2 ...eI . The SMT system first proposes multiple English hypotheses in its model space. Among all the hypotheses, the system selects the one with the highest conditional probability according to Bayes’s decision rule: eˆI1 = arg max P (eI1 |f1J ) = arg max P (f1J |eI1 )P (eI1 ), {eI1 } {eI1 } (1) where is called translation model, and P (eI1 ) is called language model. The translation model is the key component, which is the focus in this paper. P (f1J |eI1 ) 2.1 HMM-based Translation Model HMM is one of the effective translation models (Vogel et al., 1996), which is easily scalable to very large training corpus. To model word-to-word translation, we introduce the mapping j → aj , which assigns a French word fj in position j to a English word ei in position i = aj denoted as eaj . Each French word fj is an observation, and it is generated by a HMM state defined as [eaj , aj ], where the alignment aj for position j is considered to have a dependency on the previous alignment aj−1 . Thus the first-order HMM is defined as follows: P (f1J |eI1 ) = J XY P (fj |eaj )P (aj |aj−1 ), (2) j=1 aJ 1 26 where P (aj |aj−1 ) is the transition probability. This"
W05-0804,W05-0825,1,0.848237,"Missing"
W11-2202,S07-1012,0,0.0908542,"Missing"
W11-2202,D08-1031,0,0.0174584,"rence resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider how to exploit such features for the more holistic information extraction setting. 10 9 Conclusion This paper presents a Bayesian nonparametric approach to recover structured records from text. Using only a small set of prototype records, we are able to recover an accurate table that jointly identifies entities and internal name structure. In our view, the main advantage of a Bayesian approach compared to more heuristic alternatives is that it facilitates incorporation of additional information sources when availab"
W11-2202,W98-1118,0,0.0439867,"rk. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches be"
W11-2202,P11-1098,0,0.0389599,"text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not"
W11-2202,N01-1007,0,0.441624,"m of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem tha"
W11-2202,doddington-etal-2004-automatic,0,0.0381729,"ave many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a non-temporal version of our model (as described in Sections 3 and 4), and a temporal version with 5 epochs. For"
W11-2202,S07-1058,0,0.0623979,"that appear in names (such as titles and first names). We are aware of no existing system that performs all three of these tasks jointly. We evaluate on a dataset of political blogs, measuring our system’s ability to discover a set of reference entities (recall) while maintaining a compact number of rows and columns (precision). With as few as five partially-complete prototype examples, our approach gives accurate tables that match well against a manually-annotated reference list. Our method outperforms a baseline singlelink clustering approach inspired by one of the most successful entries (Elmacioglu et al., 2007) in the SEMEVAL “Web People Search” shared task (Artiles et al., 2007). 2 Task Definition In this work, we assume that a bag of M mentions in text have been identified. The mth mention wm is a sequence of contiguous word tokens (its length is denoted Nm ) understood to refer to a real-world entity. The entities (and the mapping of mentions to entities) are not known in advance. While our focus in this paper is names of people, the task is defined in a more generic way. Formally, the task is to construct a table x where rows correspond to entities and columns to functional fields. The number of"
W11-2202,N09-1019,0,0.0814631,"d entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. Resolving Mentions to Entities The problem of resolving mentions to entities has been approach from a var"
W11-2202,P05-1045,0,0.0716754,"the entities which are mentioned in raw text. We annotate a new dataset of blog text for this purpose, and design precision and recall metrics to reward systems that recover as much of the reference set as possible, while avoiding spurious entities and fields. We also perform a qualitative analysis, noting the areas where our method outperforms string matching approaches, and where there is need for further improvement. Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors). The resulting dataset has 19,247 mentions comprising 45,466 word tokens, and 813 unique mention strings. Gold standard We develop a reference set of 100 entities for evaluation. This set was created by sorting the unique name strings in the training set by frequency, and manually merging strings that reference the same entity. We also manually discarded strings from"
W11-2202,P07-1107,0,0.0705131,"ns and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi a"
W11-2202,P10-2054,0,0.24949,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,N10-1061,0,0.177867,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,D08-1068,0,0.0585186,"to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future"
W11-2202,P11-1080,0,0.346535,"for the 100 entities. Most entities only include first and last names, though the most frequent entities have many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a"
W11-2202,W02-2024,0,0.109779,"ng over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametri"
W11-2202,N10-1082,0,\N,Missing
W18-2503,N16-1101,0,0.0616643,"Missing"
W18-2503,D18-1418,0,0.0475006,"Missing"
W18-2503,D17-1151,0,0.0564881,"Missing"
W18-2503,P17-4012,0,0.435972,"algorithms. Such a unified platform enables reuse of common components and functionalities, standardizes design, implementation, and experimentation, fosters reproducible research, and importantly, encourages technique sharing among different text generation tasks, so that an algorithmic advance originally developed for a specific task can quickly be evaluated and potentially generalized to many other tasks. Though a few remarkable open-source toolkits have been developed, they have been largely designed for one or few specific tasks, especially neural machine translation (Britz et al., 2017; Klein et al., 2017; Neubig et al., 2018) and dialog related algorithms (Miller et al., 2017). This paWe introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradi"
W18-2503,E17-1001,0,0.059147,"Missing"
W18-2503,D15-1166,0,0.411036,"approaches in many of the tasks. On the other hand, considerable research efforts are still needed to improve relevant techniques and enable real-world practical applications. The variety of text generation tasks share many common properties and goals, e.g., to generate well-formed, grammatical and readable text, and to realize in the generation the desired information inferred from inputs. To this end, a few key models and algorithms are increasingly widely-used to empower the different applications, such as neural encoder-decoders (Sutskever et al., 2014), attentions (Bahdanau et al., 2014; Luong et al., 2015b), memory networks (Sukhbaatar et al., 2015), adversarial methods (Goodfellow et al., 2014; Hu et al., 2017b; Lamb et al., 2016), reinforcement learning (Ranzato et al., 2015; Bahdanau et al., 2016), as well as some optimization techniques, data preprocessing and result post-processing procedures, evaluations, etc. It is therefore highly desirable to have an opensource platform that unifies the development of the diverse yet closely-related applications, backed with clean and consistent implementations of the core algorithms. Such a unified platform enables reuse of common components and func"
W18-2503,W18-1819,0,0.0403405,"1M) with the LSTM RNN model. The test set perplexity is significantly higher than the LSTM RNNs, which is not unreasonable because LSTM RNN models are well studied for language modeling and a number of optimal modeling and optimization choices are already known. 3.3 4 Related Work Text generation is a broad research area with rapid advancement. Figure 2 summarizes some popular and emerging models used in the diverse contexts of the field. There are some existing toolkits that focus on tasks of neural machine translation and alike, such as Google Seq2seq (Britz et al., 2017) and Tensor2Tensor (Vaswani et al., 2018) on TensorFlow, OpenNMT (Klein et al., 2017) on (Py)Torch, XNMT (Neubig et al., 2018) on DyNet, and Nematus (Sennrich et al., 2017) on Theano, and MarianNMT (Junczys-Dowmunt et al., 2018) on C++. ParlAI (Miller et al., 2017) is a software platform specialized for dialog research. Differing from these task-specific toolkits, Texar aims to cover as many text generation tasks as possible. The goal of versatility poses unique challenges to the design. We combat the challenges through proper pipeline decomposition, ready-to-assemble modules, and user interfaces of varying abstract levels. There are"
W18-2503,J10-3003,0,0.0206367,"ts can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit. 1 Introduction Text generation spans a broad set of natural language processing tasks that aim at generating natural language from input data or machine representations. Such tasks include machine translation (Bahdanau et al., 2014; Brown et al., 1990), dialog systems (Williams and Young, 2007; Serban et al., 2016), text summarization (Hovy and Lin, 1998; See et al., 2017), article writing (Wiseman et al., 2017), text paraphrasing and manipulation (Madnani and Dorr, 2010; Hu et al., 2017a), image captioning (Vinyals et al., 2015b; Karpathy and Fei-Fei, 2015), and more. Recent years have seen rapid progress of this active area in both academia and industry, especially with the adop13 Proceedings of Workshop for NLP Open Source Software, pages 13–22 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics per introduces Texar, a general-purpose text generation toolkit that aims to support most of the popular applications in the field, by providing researchers and practitioners a unified and flexible framework for building their mode"
W18-2503,D17-2014,0,0.236515,"nd functionalities, standardizes design, implementation, and experimentation, fosters reproducible research, and importantly, encourages technique sharing among different text generation tasks, so that an algorithmic advance originally developed for a specific task can quickly be evaluated and potentially generalized to many other tasks. Though a few remarkable open-source toolkits have been developed, they have been largely designed for one or few specific tasks, especially neural machine translation (Britz et al., 2017; Klein et al., 2017; Neubig et al., 2018) and dialog related algorithms (Miller et al., 2017). This paWe introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make Texar particularly suitable for technique sharing a"
W18-2503,W18-1818,0,0.357082,"nified platform enables reuse of common components and functionalities, standardizes design, implementation, and experimentation, fosters reproducible research, and importantly, encourages technique sharing among different text generation tasks, so that an algorithmic advance originally developed for a specific task can quickly be evaluated and potentially generalized to many other tasks. Though a few remarkable open-source toolkits have been developed, they have been largely designed for one or few specific tasks, especially neural machine translation (Britz et al., 2017; Klein et al., 2017; Neubig et al., 2018) and dialog related algorithms (Miller et al., 2017). This paWe introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make"
W18-2503,D17-1239,0,0.0172147,"tensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit. 1 Introduction Text generation spans a broad set of natural language processing tasks that aim at generating natural language from input data or machine representations. Such tasks include machine translation (Bahdanau et al., 2014; Brown et al., 1990), dialog systems (Williams and Young, 2007; Serban et al., 2016), text summarization (Hovy and Lin, 1998; See et al., 2017), article writing (Wiseman et al., 2017), text paraphrasing and manipulation (Madnani and Dorr, 2010; Hu et al., 2017a), image captioning (Vinyals et al., 2015b; Karpathy and Fei-Fei, 2015), and more. Recent years have seen rapid progress of this active area in both academia and industry, especially with the adop13 Proceedings of Workshop for NLP Open Source Software, pages 13–22 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics per introduces Texar, a general-purpose text generation toolkit that aims to support most of the popular applications in the field, by providing researchers and practition"
W18-2503,P17-1099,0,0.0505047,"Missing"
W18-2503,E17-3017,0,0.0583961,"Missing"
W18-2503,P17-1061,1,0.866504,"Missing"
W18-2503,J90-2002,0,\N,Missing
W18-2503,X98-1026,0,\N,Missing
W18-2503,P16-1154,0,\N,Missing
W18-2503,P16-1014,0,\N,Missing
W18-2503,P18-4020,0,\N,Missing
W18-2503,K16-1002,0,\N,Missing
W18-2503,N19-1423,0,\N,Missing
