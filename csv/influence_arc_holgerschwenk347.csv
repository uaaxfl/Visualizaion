2006.iwslt-papers.2,2005.iwslt-1.5,0,0.0293593,"ent speeches (T C -S TAR evaluations). Therefore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator"
2006.iwslt-papers.2,2005.iwslt-1.12,0,0.0293042,"ent speeches (T C -S TAR evaluations). Therefore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator"
2006.iwslt-papers.2,2005.iwslt-1.15,0,0.0276075,"ore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the re"
2006.iwslt-papers.2,W05-0821,0,0.0268435,", it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions of the word representation, better generalization t"
2006.iwslt-papers.2,2003.mtsummit-papers.6,0,0.224734,"on lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A"
2006.iwslt-papers.2,P06-2093,1,0.833984,"epresentation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the ngram probabilities. This is still a n-gram approach, but the LM posterior probabilities are ”interpolated” for any possible context of length n-1 instead of backing-off to shorter contexts. This approach was successfully used in large vocabulary continuous speech recognition [9], and initial experiments have shown that it can be used to improve a word-based statistical machine translation system [10]. Here, the continuous space LM is applied the first time to a state-of-the-art phrase-based SMT system. Translation of four different languages is considered: Mandarin, Japanese, Arabic and Italian to English. These languages exhibit very different characteristics, e.g. with respect to word order, which may affect the role of the target LM, although a reordering model is used in the SMT systems. We also investigate the use of the continuous space LM in a SMT system based on bilingual n-grams. This paper is organized as follows. In the next section we first describe the baseline statistical ma"
2006.iwslt-papers.2,2005.iwslt-1.23,1,0.849141,"ine statistical machine translation systems. Section 3 presents the architecture and training algorithms of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions. 166 2. Baseline systems During the last few years, the use of context in SMT systems has provided great improvements in translation. SMT has evolved from the original word-based approach to phrasebased translation systems. In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. [11]. Two basic issues differentiate the n-gram-based system from the phrase-based: training data are monotonically segmented into bilingual units; and the model considers n-gram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. [12]. Both systems follow a maximum entropy approach, in which a log-linear combination of multiple models is implemented, as an alternative to the source-channel approach: This simplifies the introduction of several additional models explaining the translation process, as the search becomes: e∗ = arg max p(e"
2006.iwslt-papers.2,2005.mtsummit-papers.36,1,0.808081,"few years, the use of context in SMT systems has provided great improvements in translation. SMT has evolved from the original word-based approach to phrasebased translation systems. In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. [11]. Two basic issues differentiate the n-gram-based system from the phrase-based: training data are monotonically segmented into bilingual units; and the model considers n-gram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. [12]. Both systems follow a maximum entropy approach, in which a log-linear combination of multiple models is implemented, as an alternative to the source-channel approach: This simplifies the introduction of several additional models explaining the translation process, as the search becomes: e∗ = arg max p(e|f ) X λi hi (e, f ))} = arg max{exp( e (1) i where f and e are sentences in the source and target language respectively. The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set. Both the n-grambased and the"
2006.iwslt-papers.2,2006.iwslt-evaluation.18,1,0.876569,"e λi weights are typically optimized to maximize a scoring function on a development set. Both the n-grambased and the phrase-based system use a language model on the target language as feature function, i.e. P (e), but they differ in the translation model. In both cases, it is based on bilingual units. A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, each system learns its dictionary of bilingual fragments. Both SMT approaches were evaluated in I WSLT’06 evaluation and they are described in detail in [13, 14]. Therefore, we only give a short summary in the following two sections. • no smaller tuples can be extracted without violating the previous constraints. As a consequence of these constraints, only one segmentation is possible for a given sentence pair. Two important issues regarding this translation model must be considered. First, it often occurs that a large number of single-word translation probabilities are left out of the model. This happens for all words that are always embedded in tuples containing two or more words, then no translation probability for an independent occurrence of thes"
2006.iwslt-papers.2,takezawa-etal-2002-toward,0,0.0402314,"corresponds basically to a table look-up using hashing techniques, while a forward pass through the neural network is necessary for the continuous space LM. Very efficient optimizations are possible, in particular when n-grams with the same context can be grouped together, but a reorganization of the decoder may be necessary. More details on optimizing the neural network LM can be found in [9]. 4. Experimental Evaluation In this work we report results on the Basic Traveling Expression Corpus (B TEC). This corpus consists of typical sentences from phrase books for tourists in several languages [16]. Translation to English from four languages is considered: Mandarin, Japanese, Arabic and Italian. The reference phrase- and n-gram-based SMT systems participated in the open data track of the 2006 I WSLT evaluation [13, 14], i.e. only the supplied subset of the full B TEC corpus was used to train all the statistical models. Details on the data preprocessed as in [13, 14] are summarized in Table 1. We report results on the supplied development corpus of 489 sentences (less than 6k words) using the BLEU score with seven references translations. The scoring is case insensitive and punctuations"
2006.iwslt-papers.2,2005.iwslt-1.6,0,0.0223383,"st advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions"
2006.iwslt-papers.2,2005.iwslt-1.11,0,\N,Missing
2007.iwslt-1.26,2006.iwslt-papers.2,1,0.882609,"4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5 reports on all experiments carried out from Arabic and Chinese into English for IWSLT 2007. Finally"
2007.iwslt-1.26,N04-1033,0,0.0623584,"ls, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. In this way, it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k th tuple of a given bilingual sentence pair segmented in K tuples. 2.2. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an ex"
2007.iwslt-1.26,2005.mtsummit-papers.37,1,0.856996,"following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. where tn refers to the nth word in the partial translation hypothesis T . Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (T ) = exp(number of words in T ). The third and fourth fe"
2007.iwslt-1.26,2006.iwslt-papers.5,1,0.867108,"ChineseEnglish task, a secondary run was performed with a rescoring module, as described in Sections 4 and 5.3.2. 2.5. Feature Weights Optimization To tune the weight of each feature function in the SMT system, we used the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm [12]. SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function in each iteration, regardless of the dimension of the optimization problem. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients [13]. The SPSA procedure is in the general recursive stochastic approximation form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆk (λ λ (5) lation tuples (as no word within a tuple can be linked to a word out of it [9]). Starting from the monotonic graph, each sequence of input POS tags fulfilling a source-side rewrite rule implies the addition of a reordering arc (which encodes the reordering detailed in the target-side of the rule). Figure 2 shows how three rewrite rules applied over an input sentence extend the search graph given the reordering patterns that match the source POS tag sequence 1 . ˆ k ) is the esˆ"
2007.iwslt-1.26,N07-2022,1,0.823419,"o phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3, 4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5"
2007.iwslt-1.26,N06-2013,0,0.076004,"fluency and METEOR is well correlated to adequacy [4], we supposed that adding all references was beneficial to monolingual language models but not to the bilingual language model. Table 2: Chinese→English corpus statistics. 5.2. Data Preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available"
2007.iwslt-1.26,W03-1730,0,0.018621,"ts length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal"
2007.iwslt-1.26,A00-1031,0,0.0277074,"he MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results i"
2007.iwslt-1.26,E99-1010,0,0.0816899,"ion produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50"
2007.iwslt-1.26,J03-1002,0,0.00791937,"y available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50 classes and taking the union of source-target and target-source alignments. Table 3 show results for the new features of this year’s system. We optimized the foll"
2007.jeptalnrecital-poster.25,2003.mtsummit-papers.6,0,0.038714,"ns morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme unit´es du syst`eme de traduction et de n’introduire les cat´egories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’int´egrer les informations syntaxiques dans le mod`ele de traduction lui-mˆeme. De plus, nous proposons de combiner cette approche avec les ` notre connaism´ethodes classiques de r´e´evaluation de listes de n meilleures hypoth`eses. A sance, cette approche n’a pas ´et´e ´evalu´ee sur une large tˆache (elle a ´et´e appliqu´ee par (Hwang et al., 2007) `a la tˆach"
2007.jeptalnrecital-poster.25,W06-2606,0,0.0276563,"re en compte les contraintes syntaxiques ou les d´ependances `a long terme entre les mots. Il apparaˆıt donc n´ecessaire d’utiliser des m´ethodes dans lesquelles les propri´et´es structurelles des langues sont explicitement repr´esent´ees. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme unit´es du syst`eme de traduction et de n’introduire les cat´egories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’int´egrer les informations syntaxiques dans le mod`ele de traduction"
2007.jeptalnrecital-poster.25,W05-0821,0,0.0315029,"s syntaxiques ou les d´ependances `a long terme entre les mots. Il apparaˆıt donc n´ecessaire d’utiliser des m´ethodes dans lesquelles les propri´et´es structurelles des langues sont explicitement repr´esent´ees. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme unit´es du syst`eme de traduction et de n’introduire les cat´egories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’int´egrer les informations syntaxiques dans le mod`ele de traduction lui-mˆeme. De plus, nous propos"
2007.jeptalnrecital-poster.25,2003.mtsummit-tttt.3,0,0.177459,"ique, mod´elisation linguistique dans un espace continu, analyse morpho-syntaxique, d´esambigu¨ısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un th`eme de recherche depuis plusieurs d´ecennies et diff´erentes approches ont ´et´e propos´ees, telles que la traduction par r`egles, la traduction `a base d’exemples ou la traduction statistique. Les travaux r´ecents en traduction statistique confirment que les mod`eles fond´es sur des s´equences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fond´es sur des mots (Brown et al., 1993). En utilisant des s´equences de mots, les syst`emes de traduction parviennent a` pr´eserver certaines contraintes locales sur l’ordre des mots. L’entraˆınement d’un tel mod`ele n´ecessite l’alignement d’un corpus parall`ele. Les r´egularit´es du langage naturel comme celles de la syntaxe, ou, encore a` un niveau sup´erieur, celles de la s´emantique sont ainsi, en principe, implicitement captur´ees par les mod`eles. 253 ´chelotte, H. Bonneau-Maynard, A. Allauzen H. Schwenk, D. De Depuis"
2007.jeptalnrecital-poster.25,N04-1021,0,0.0224246,"ucturation syntaxique pour restituer le sens de l’´enonc´e d’origine. La mod´elisation du langage comme une source markovienne (mod`ele de langage n-gramme), avec comme unit´e le mot ou la s´equence de mots, ne permet pas de prendre en compte les contraintes syntaxiques ou les d´ependances `a long terme entre les mots. Il apparaˆıt donc n´ecessaire d’utiliser des m´ethodes dans lesquelles les propri´et´es structurelles des langues sont explicitement repr´esent´ees. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont d´ej`a ´et´e men´ees. (Och et al., 2004) ont explor´e de nombreuses fonctions caract´eristiques, dont certaines d’ordre syntaxique. La r´e´evaluation des n meilleures hypoth`eses avec des ´etiquettes morpho-syntaxiques a ´egalement ´et´e ´etudi´ee par (Hasan et al., 2006). Dans (Kirchhoff & Yang, 2005), un mod`ele de langage factoris´e quadrigramme utilisant des informations syntaxiques n’a pas montr´e des performances meilleures qu’un mod`ele n-gramme de mots. Les mod`eles de langage fond´es sur la syntaxe ont enfin ´et´e explor´es par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des s´equences de mots comme u"
2007.jeptalnrecital-poster.25,P02-1038,0,0.0392758,"derni`eres ann´ees, les travaux en traduction statistique ont ´etendu avec succ`es l’unit´e qu’´etait le mot a` la s´equence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unit´e se d´efinit alors comme un groupe de mots successifs ˜f de la langue source. Sa ˜ dans la phrase cible. Les s´equences traduction est ´egalement une s´equence de mots e de mots peuvent ˆetre extraites automatiquement `a partir de donn´ees bilingues align´ees au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de d´ecomposer le probl`eme de la mani`ere suivante (Och & Ney, 2002) : e∗ = arg max p(e|f ) = arg max{exp e ! λi hi (e, f )} (1) i o` u chaque fonction hi quantifie l’ad´equation des phrases f et e1 . Les coefficients λi pond`erent l’importance relative de ces fonctions. 2.1 D´ ecodeur Moses Moses2 est un syst`eme de traduction automatique `a base de s´equences de mots a` l’´etat de l’art. Il est distribu´e librement avec les scripts n´ecessaires `a l’entraˆınement d’un syst`eme de traduction complet, ainsi qu’une mise en œuvre efficace d’un algorithme de recherche de type recherche en faisceau pour produire les traductions. Le d´ecodeur Moses peut ´egalement"
2007.jeptalnrecital-poster.25,W99-0604,0,0.119817,", approche statistique, mod´elisation linguistique dans un espace continu, analyse morpho-syntaxique, d´esambigu¨ısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un th`eme de recherche depuis plusieurs d´ecennies et diff´erentes approches ont ´et´e propos´ees, telles que la traduction par r`egles, la traduction `a base d’exemples ou la traduction statistique. Les travaux r´ecents en traduction statistique confirment que les mod`eles fond´es sur des s´equences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fond´es sur des mots (Brown et al., 1993). En utilisant des s´equences de mots, les syst`emes de traduction parviennent a` pr´eserver certaines contraintes locales sur l’ordre des mots. L’entraˆınement d’un tel mod`ele n´ecessite l’alignement d’un corpus parall`ele. Les r´egularit´es du langage naturel comme celles de la syntaxe, ou, encore a` un niveau sup´erieur, celles de la s´emantique sont ainsi, en principe, implicitement captur´ees par les mod`eles. 253 ´chelotte, H. Bonneau-Maynard, A. Allauzen H. S"
2007.jeptalnrecital-poster.25,P02-1040,0,0.0799804,"s large, puisqu’un syst`eme de traduction inclut toujours un mod`ele de langage cible hi (e, f ) = p(e). 2 http://www.statmt.org/moses/ 255 ´chelotte, H. Bonneau-Maynard, A. Allauzen H. Schwenk, D. De deux sens, les probabilit´es de traduction des mots dans les deux sens, une mesure de distorsion, deux p´enalit´es d’insertion de mots et de s´equences de mots, et la probabilit´e calcul´ee par le mod`ele de langage de la langue cible. L’approche couramment employ´ee pour optimiser les poids λi des fonctions caract´eristiques est la maximisation sur un corpus de d´eveloppement de la mesure BLEU (Papineni et al., 2002). Pour cela, l’outil d’optimisation num´erique Condor (Berghen & Bersini, 2005) est int´egr´e `a l’algorithme it´eratif suivant : 1. Partant d’un jeu de poids initial, les listes des n = 1000 meilleures hypoth`eses sont g´en´er´ees avec Moses (une liste par phrase source). 2. Ces listes sont r´e´evalu´ees en utilisant le jeu de poids courant. 3. Les meilleures hypoth`eses sont extraites et ´evalu´ees. ` partir du score BLEU aisni calcul´e, Condor calcule un nouveau jeu de poids 4. A (l’algorithme retourne alors a` l’´etape 2), sauf si un maximum local est d´etect´e ce qui met fin `a l’algorith"
2007.jeptalnrecital-poster.25,2006.iwslt-papers.2,1,0.898341,"Missing"
2007.mtsummit-papers.18,J96-1002,0,0.0353272,"iscussion of future research issues. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f that maximizes the posterior probability: e∗ = argmax Pr(e|f ) e X = argmax Pr(e, A|f ) e ≈ argmax max Pr(e, A|f ) e (1) A A (2) In the above equations, A denotes a correspondence between source and target words and is called an alignment. The Moses decoder makes the so-called maximum approximation as in Equation 2. The Pr(e, A|f ) probability is modeled by a combination of feature functions, according to the maximum entropy framework (Berger et al., 1996): X Pr(e, A|f ) ∝ exp λi fi (e, A|f ) (3) i The translation process involves segmenting the source sentence into source phrases f˜; translating each source phrase into a target phrase e˜, and optionally reordering the target phrases to produce the target sentence e∗ . A phrase is here defined as a group of words that should be translated together (Koehn et al., 2003; Och and Ney, 2003). The segmentation stage is not modeled explicitly by any feature function, which amounts to considering every segmentation equally likely. A phrase table provides several scores that quantitize the relevance of"
2007.mtsummit-papers.18,N03-2002,0,0.0323071,"there is no significant difference after rescoring the n-best lists with the continuous space language model. We conjecture that both approaches correct the same translation problems. The results reported in Table 4 were obtained by rescoring with word language models, even in the last row. We believe that it is necessary to use the enriched representation also in the language models in order to take full advantage of the disambiguation in the translation model. Rescoring with simple POS language models was tried, but without success. We are now working on the use of factored language models (Bilmes and Kirchhoff, 2003) that simultaneously use the word and POS information. Figure 5 shows comparative translation examples from the baseline and the enriched translation systems. In the first example, the baseline system outputs “durante los u´ ltimos sesiones” where the enriched translation system produces “en los u´ ltimos per´ıodos de sesiones”, a better translation that may be attributed to the introduction of the masculine word “per´ıodos”, allowing the system to build a syntactically correct sentence. In the second example, the syntactical error “no puede ser un cierto reconocimiento” produced by the baseli"
2007.mtsummit-papers.18,W07-0409,1,0.882047,"Missing"
2007.mtsummit-papers.18,2005.mtsummit-posters.19,0,0.0305021,"These n-best lists are then rescored with the continuous space language model. 1. The n-best lists are reranked using the current set of weights. The current hypothesis is extracted and scored against the reference translations. 2. The obtained BLEU score is passed to Condor, which either computes a new set of weights (the algorithm then proceeds to step 1) or detects that a local maximum has been reached and the algorithm stops iterating. Each of the two passes uses its own set of eight weights and is tuned separately, a feature shared with other systems, for instance (L¨oo¨ f et al., 2006; Cettolo et al., 2005). The second pass is often taken as an opportunity to compute several feature functions on the n-best list, yet after several experiments we chose not to follow this direction. The described system is thus voluntarily simple, with the hope that it will generalize well to new data. We believe that adding many feature functions, especially some that could just be ad hoc fixes to phenomena from the development data, in conjunction with performing a numerical optimization of the λi that is unaware of the highly discontinuous nature of BLEU (Papineni et al., 2002), bear the risk of heavily over-fit"
2007.mtsummit-papers.18,P07-2045,0,0.0664233,"Missing"
2007.mtsummit-papers.18,N03-1017,0,0.0260625,"Missing"
2007.mtsummit-papers.18,koen-2004-pharaoh,0,0.0334108,"science. From the pioneer works to today’s research, many paradigms have been explored, for instance rulebased, example-based, knowledge-based and statistical approaches to machine translation. Statistical machine translation (SMT) seems today to be the preferred approach of many industrial and academic research laboratories, each of them developing their own set of tools. In 1999 however, a summer workshop at Johns-Hopkins University hosted the creation of the EGYPT toolkit1 , on which the widely used training tool Giza++ (Och and Ney, 2003) is based. Later, the Pharaoh phrase-based decoder (Koehn, 2004) became available and distributed in binary form2 , but as far as we know, Pharaoh was not widely used. More recently, another workshop3 released an open source toolkit, which includes a decoder, Moses (Koehn and al., 2007), and a comprehensive set of softwares and scripts to build a complete SMT system—namely determining word alignments, extracting phrases, performing the translation and tuning system parameters. In this paper, we describe the development of a state-of-theart SMT system based on the Moses suite. Several new features were added, in particular a two-pass decoding strategy using"
2007.mtsummit-papers.18,J03-1002,0,0.0151761,"rst natural language processing applications investigated in computer science. From the pioneer works to today’s research, many paradigms have been explored, for instance rulebased, example-based, knowledge-based and statistical approaches to machine translation. Statistical machine translation (SMT) seems today to be the preferred approach of many industrial and academic research laboratories, each of them developing their own set of tools. In 1999 however, a summer workshop at Johns-Hopkins University hosted the creation of the EGYPT toolkit1 , on which the widely used training tool Giza++ (Och and Ney, 2003) is based. Later, the Pharaoh phrase-based decoder (Koehn, 2004) became available and distributed in binary form2 , but as far as we know, Pharaoh was not widely used. More recently, another workshop3 released an open source toolkit, which includes a decoder, Moses (Koehn and al., 2007), and a comprehensive set of softwares and scripts to build a complete SMT system—namely determining word alignments, extracting phrases, performing the translation and tuning system parameters. In this paper, we describe the development of a state-of-theart SMT system based on the Moses suite. Several new featu"
2007.mtsummit-papers.18,P02-1040,0,0.0814218,"for instance (L¨oo¨ f et al., 2006; Cettolo et al., 2005). The second pass is often taken as an opportunity to compute several feature functions on the n-best list, yet after several experiments we chose not to follow this direction. The described system is thus voluntarily simple, with the hope that it will generalize well to new data. We believe that adding many feature functions, especially some that could just be ad hoc fixes to phenomena from the development data, in conjunction with performing a numerical optimization of the λi that is unaware of the highly discontinuous nature of BLEU (Papineni et al., 2002), bear the risk of heavily over-fitting the development data. Some experimental evidence for this are provided in the results section. Overall, there are roughly 60 million words of texts available to train the target language models. This is a quite limited amount in comparison to tasks like the N IST machine translation evaluations for which several billion words of newspaper texts are available. Therefore, specific techniques must be deployed to make the most of the limited resources. We use MERT, which is distributed along with the Moses decoder, to tune the first pass. The weights were ad"
2007.mtsummit-papers.18,2006.iwslt-papers.2,1,0.614623,"Missing"
2008.iwslt-evaluation.9,W08-0313,1,0.765935,"Arabic words. A continuous space language model was deployed to improve the modeling of the target language. Both approaches achieved significant improvements in the BLEU score. The system achieves a score of 49.4 on the test set of the 2008 IWSLT evaluation. 1. Introduction This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation. We only participated in the Arabic/English BTEC task. The architecture of the system is very similar to a large system built for the NIST Arabic/English task [1] or a system built for the translation between French and English [2]. All three are statistical phrase-based machine translation systems based on the freely available Moses decoder [3], with extensions for rescoring nbest lists with a continuous space language model in a second pass. No system combination is used. The training data of the translation model of the IWSLT system is limited to the provided BTEC corpora. Small improvements could be achieved using additional language model training data, namely LDC’s Gigaword corpus. All the models are case sensitive and include punctuation markers. We compare two different tokenization of the Arabic source text: a"
2008.iwslt-evaluation.9,P07-2045,0,0.0103883,"pproaches achieved significant improvements in the BLEU score. The system achieves a score of 49.4 on the test set of the 2008 IWSLT evaluation. 1. Introduction This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation. We only participated in the Arabic/English BTEC task. The architecture of the system is very similar to a large system built for the NIST Arabic/English task [1] or a system built for the translation between French and English [2]. All three are statistical phrase-based machine translation systems based on the freely available Moses decoder [3], with extensions for rescoring nbest lists with a continuous space language model in a second pass. No system combination is used. The training data of the translation model of the IWSLT system is limited to the provided BTEC corpora. Small improvements could be achieved using additional language model training data, namely LDC’s Gigaword corpus. All the models are case sensitive and include punctuation markers. We compare two different tokenization of the Arabic source text: a full word mode and a morphological decomposition kindly provided by SYSTRAN. The later one achieved improvements in"
2008.iwslt-evaluation.9,2003.mtsummit-tttt.3,0,0.0235751,"mode and a morphological decomposition kindly provided by SYSTRAN. The later one achieved improvements in the BLEU score of several points. This paper is organized as follows. In the next section, the main architecture of the SMT system architecture is presented. In the following section the experimental results are provided and commented. The paper concludes with a discussion of future research issues. 2. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units [4, 5] and a log linear framework in order to introduce several models - 63 - e∗ = arg max p(e|f ) = arg max p(f , e)P (e) e X λi hi (e, f ))} arg max{exp( = e e (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [6]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit [3] an"
2008.iwslt-evaluation.9,J03-1002,0,0.00258594,"mode and a morphological decomposition kindly provided by SYSTRAN. The later one achieved improvements in the BLEU score of several points. This paper is organized as follows. In the next section, the main architecture of the SMT system architecture is presented. In the following section the experimental results are provided and commented. The paper concludes with a discussion of future research issues. 2. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units [4, 5] and a log linear framework in order to introduce several models - 63 - e∗ = arg max p(e|f ) = arg max p(f , e)P (e) e X λi hi (e, f ))} arg max{exp( = e e (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [6]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit [3] an"
2008.iwslt-evaluation.9,P02-1038,0,0.0394729,"ults are provided and commented. The paper concludes with a discussion of future research issues. 2. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units [4, 5] and a log linear framework in order to introduce several models - 63 - e∗ = arg max p(e|f ) = arg max p(f , e)P (e) e X λi hi (e, f ))} arg max{exp( = e e (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [6]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit [3] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target LM is then constructed as detailed in section 3.2. The translation its"
2008.iwslt-evaluation.9,2006.iwslt-papers.2,1,0.829288,"the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the n-gram probabilities. This is still a n-gram approach, but the language model posterior probabilities are “interpolated” for any possible context of length n − 1 instead of backing-off to shorter contexts. This approach was already successfully applied in statistical machine translation systems, ranging from small IWSLT systems [9, 10] to large NIST systems [1]. A standard fully-connected multi-layer perceptron is used. The inputs to the neural network are the indices of the n − 1 previous words in the vocabulary hj = wj−n+1 , . . . , wj−2 , wj−1 and the outputs are the posterior probabilities of all words of the vocabulary: P (wj = i|hj ) ∀i ∈ [1, N ] (2) where N is the size of the vocabulary. The input uses the so-called 1-of-n coding, i.e., the ith word of the vocabulary is - 64 - The value of the output neuron pi corresponds directly to the probability P (wj = i|hj ). Training is performed with the standard backpropagat"
2008.iwslt-evaluation.9,2006.iwslt-evaluation.15,0,0.020417,"those from last year’s evaluation. Once the this year’s Arabic test data was available we build an interpolated language model on the source part of the BTEC corpus and all development corpora. After analyzing the interpolation coefficients, we found evidence that this year’s test data has similar characteristics than Dev4 and Dev5 and to less extent Dev6. Therefore, we decided to add the last two corpora to the training material after optimizing the system and to retrain the full system keeping all settings unmodified. This idea was already successfully proposed in previous IWSLT evaluations [12]. We have envisaged the use of additional sources of bitexts in order to improve the translation model, in particular large amounts of data that are available to build an Arabic/English translations system for the NIST task. However, initial experiments were not very concluding and that data is not used in the final system. We also investigated the possibility to increase the amount of monolingual data. This is detailed in the next section. We have realized that the test data of this year’s evaluation did contain only few punctuation marks. Many sentence had no punctuation at the end. This is"
2008.iwslt-evaluation.9,N06-2013,0,0.0611447,"Missing"
2008.iwslt-evaluation.9,2007.iwslt-1.13,0,0.0373629,"Missing"
2008.iwslt-evaluation.9,W07-0728,0,0.0497444,"Missing"
2008.iwslt-evaluation.9,W07-0732,0,0.0337468,"Missing"
2008.iwslt-evaluation.9,2006.iwslt-evaluation.17,0,\N,Missing
2008.iwslt-evaluation.9,2007.iwslt-1.11,0,\N,Missing
2008.iwslt-papers.6,W07-0728,0,0.019796,"ation of more general texts. A recent evalua- 182 - tion on automatic translation between European languages has for instance shown that statistical systems perform very well on test data drawn from the European Parliament corpus, i.e. texts of the same type that they were trained on, but their performance can be inferior to rule-based systems for general news data [2]. There are several directions of research to improve the genericity of SMT systems, for instance factored translation model [3], the integration of high quality dictionaries [4] or statistical post-editing of rule-based systems [5, 6]. In this work we investigate whether large-scale unsupervised training is useful to develop a generic SMT system. We define unsupervised training as using the system itself to produce additional bilingual data, i.e. without using a human to perform the translations. The SMT system used to translate the texts was of course itself trained on some bitexts, but these bitexts may be limited in size or little related to the translation task. These resources used to build the initial SMT system are usually not considered as supervision in the framework of unsupervised training applied to additional"
2008.iwslt-papers.6,W07-0732,0,0.012023,"ation of more general texts. A recent evalua- 182 - tion on automatic translation between European languages has for instance shown that statistical systems perform very well on test data drawn from the European Parliament corpus, i.e. texts of the same type that they were trained on, but their performance can be inferior to rule-based systems for general news data [2]. There are several directions of research to improve the genericity of SMT systems, for instance factored translation model [3], the integration of high quality dictionaries [4] or statistical post-editing of rule-based systems [5, 6]. In this work we investigate whether large-scale unsupervised training is useful to develop a generic SMT system. We define unsupervised training as using the system itself to produce additional bilingual data, i.e. without using a human to perform the translations. The SMT system used to translate the texts was of course itself trained on some bitexts, but these bitexts may be limited in size or little related to the translation task. These resources used to build the initial SMT system are usually not considered as supervision in the framework of unsupervised training applied to additional"
2008.iwslt-papers.6,2006.iwslt-papers.3,0,0.346205,"ase table translation unsupervised training Figure 1: Principle of lightly-supervised training of an SMT system. Lightly-supervised and unsupervised training has been successfully applied to large vocabulary continuous speech recognition, see for instance [7, 8], but it is not yet widely used in MT. We are only aware of a few pieces of related work. Ueffing et al. used an SMT system to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is used jointly with the generic phrase table [9]. Given the small size of the translated additional data, this technique was presented as domain adaptation rather than unsupervised training of an SMT system. In follow up work, this approach was refined [10], but it was again applied to the test data only. Domain adaptation was also performed simultaneously for the translation, language and reordering model [11]. On the other hand, there is quite some work on the generation of additional bilingual resources to train an SMT system. Munteanu and Marcu proposed an algorithm to automatically detect sentences that are possible translations of eac"
2008.iwslt-papers.6,P07-1004,0,0.421486,"continuous speech recognition, see for instance [7, 8], but it is not yet widely used in MT. We are only aware of a few pieces of related work. Ueffing et al. used an SMT system to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is used jointly with the generic phrase table [9]. Given the small size of the translated additional data, this technique was presented as domain adaptation rather than unsupervised training of an SMT system. In follow up work, this approach was refined [10], but it was again applied to the test data only. Domain adaptation was also performed simultaneously for the translation, language and reordering model [11]. On the other hand, there is quite some work on the generation of additional bilingual resources to train an SMT system. Munteanu and Marcu proposed an algorithm to automatically detect sentences that are possible translations of each other in large collections of Arabic and English newspaper collections [12]. Their approach does not use an SMT system, but a relatively small word-based bilingual dictionary to translate some of the words o"
2008.iwslt-papers.6,P08-2040,0,0.412897,". used an SMT system to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is used jointly with the generic phrase table [9]. Given the small size of the translated additional data, this technique was presented as domain adaptation rather than unsupervised training of an SMT system. In follow up work, this approach was refined [10], but it was again applied to the test data only. Domain adaptation was also performed simultaneously for the translation, language and reordering model [11]. On the other hand, there is quite some work on the generation of additional bilingual resources to train an SMT system. Munteanu and Marcu proposed an algorithm to automatically detect sentences that are possible translations of each other in large collections of Arabic and English newspaper collections [12]. Their approach does not use an SMT system, but a relatively small word-based bilingual dictionary to translate some of the words of the source sentence. These lexical translations are then used as a query to extract candidate translations using information retrieval techniques. Finally,"
2008.iwslt-papers.6,J05-4003,0,0.020464,"was presented as domain adaptation rather than unsupervised training of an SMT system. In follow up work, this approach was refined [10], but it was again applied to the test data only. Domain adaptation was also performed simultaneously for the translation, language and reordering model [11]. On the other hand, there is quite some work on the generation of additional bilingual resources to train an SMT system. Munteanu and Marcu proposed an algorithm to automatically detect sentences that are possible translations of each other in large collections of Arabic and English newspaper collections [12]. Their approach does not use an SMT system, but a relatively small word-based bilingual dictionary to translate some of the words of the source sentence. These lexical translations are then used as a query to extract candidate translations using information retrieval techniques. Finally, a maximum entropy classifier is used to select the most promising candidate translations. In another work, a rule-based system was used to generate additional bitexts to train an SMT system [13]. In this paper we investigate whether it is possible to use an SMT system itself to translate several hundred milli"
2008.iwslt-papers.6,D07-1030,0,0.115497,"ences that are possible translations of each other in large collections of Arabic and English newspaper collections [12]. Their approach does not use an SMT system, but a relatively small word-based bilingual dictionary to translate some of the words of the source sentence. These lexical translations are then used as a query to extract candidate translations using information retrieval techniques. Finally, a maximum entropy classifier is used to select the most promising candidate translations. In another work, a rule-based system was used to generate additional bitexts to train an SMT system [13]. In this paper we investigate whether it is possible to use an SMT system itself to translate several hundred millions of words of monolingual data and to use these automatic translations to improve the SMT system. We concentrate on - 183 - the translation from French into English. Lightly-supervised training is performed on all the texts from the AFP news agency in LDC’s Gigaword collection. This totals several hundreds of millions of words. In previous work it is mentioned that unsupervised training of SMT systems bears the problem that in principle no new translations can be learned. It is"
2008.iwslt-papers.6,2003.mtsummit-tttt.3,0,0.177891,"Baseline system The goal of SMT is to produce a target sentence e from a source sentence f . Among all possible target language sentences the one with the highest probability is chosen: e∗ = arg max Pr(e|f ) (1) = arg max Pr(f |e) Pr(e) (2) e e where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This approach is usually referred to as the noisy source-channel approach in SMT [14]. Bilingual corpora are needed to train the translation model and monolingual texts to train the target language model. It is today common practice to use phrases as translation units [15, 16] instead of the original word-based approach. A phrase is defined as a group of source words f˜ that should be translated together into a group of target words e˜. The translation model in phrase-based systems includes the phrase translation probabilities in both directions, i.e. P (˜ e|f˜) and ˜ P (f |˜ e). The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e∗ = = arg max P r(e|f ) X λi hi (e, f ))} arg max{exp( e (3) i The feature functions hi are the system models and the λi weights are typically optimized to m"
2008.iwslt-papers.6,J03-1002,0,0.00516294,"Baseline system The goal of SMT is to produce a target sentence e from a source sentence f . Among all possible target language sentences the one with the highest probability is chosen: e∗ = arg max Pr(e|f ) (1) = arg max Pr(f |e) Pr(e) (2) e e where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This approach is usually referred to as the noisy source-channel approach in SMT [14]. Bilingual corpora are needed to train the translation model and monolingual texts to train the target language model. It is today common practice to use phrases as translation units [15, 16] instead of the original word-based approach. A phrase is defined as a group of source words f˜ that should be translated together into a group of target words e˜. The translation model in phrase-based systems includes the phrase translation probabilities in both directions, i.e. P (˜ e|f˜) and ˜ P (f |˜ e). The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e∗ = = arg max P r(e|f ) X λi hi (e, f ))} arg max{exp( e (3) i The feature functions hi are the system models and the λi weights are typically optimized to m"
2008.iwslt-papers.6,N07-1061,0,0.0195617,"t for the translation of news material from Mandarin and Arabic into English, using more than 170M words of bitexts that are easily available from the LDC. The possibility to develop a MT system using only aligned bilingual texts is generally mentioned as an advantage of SMT systems. On the other hand, this can also be a handicap for this approach. For some language pairs bilingual corpora just do not exist, e.g. Japanese/Spanish, or the existing corpora are too small to build a good SMT system. There is some research trying to tackle this problem by using an intermediate pivot language, e.g. [1]. It can also happen that the available bitexts do not correspond to the domain for which we want to build a translation system. Many of the available bitexts were produced by multilingual organizations, in particular the European and Canadian Parliament or the United Nations. A particular jargon is often used in these texts, that may not be appropriate for the translation of more general texts. A recent evalua- 182 - tion on automatic translation between European languages has for instance shown that statistical systems perform very well on test data drawn from the European Parliament corpus,"
2008.iwslt-papers.6,P02-1038,0,0.151585,"phrase is defined as a group of source words f˜ that should be translated together into a group of target words e˜. The translation model in phrase-based systems includes the phrase translation probabilities in both directions, i.e. P (˜ e|f˜) and ˜ P (f |˜ e). The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e∗ = = arg max P r(e|f ) X λi hi (e, f ))} arg max{exp( e (3) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [17]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. The system is based on the Moses SMT toolkit [18] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted using the default settings of the Moses SMT toolkit. The 4-gram back-off target LM is trained on the English part of the bitexts and the Gigaword corpus of about"
2008.iwslt-papers.6,W08-0309,0,0.0371549,"he available bitexts were produced by multilingual organizations, in particular the European and Canadian Parliament or the United Nations. A particular jargon is often used in these texts, that may not be appropriate for the translation of more general texts. A recent evalua- 182 - tion on automatic translation between European languages has for instance shown that statistical systems perform very well on test data drawn from the European Parliament corpus, i.e. texts of the same type that they were trained on, but their performance can be inferior to rule-based systems for general news data [2]. There are several directions of research to improve the genericity of SMT systems, for instance factored translation model [3], the integration of high quality dictionaries [4] or statistical post-editing of rule-based systems [5, 6]. In this work we investigate whether large-scale unsupervised training is useful to develop a generic SMT system. We define unsupervised training as using the system itself to produce additional bilingual data, i.e. without using a human to perform the translations. The SMT system used to translate the texts was of course itself trained on some bitexts, but thes"
2008.iwslt-papers.6,P07-2045,0,0.00743347,"opy approach simplifies the introduction of several additional models explaining the translation process : e∗ = = arg max P r(e|f ) X λi hi (e, f ))} arg max{exp( e (3) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [17]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. The system is based on the Moses SMT toolkit [18] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted using the default settings of the Moses SMT toolkit. The 4-gram back-off target LM is trained on the English part of the bitexts and the Gigaword corpus of about 3.2 billion words. The translation model was trained on three parallel corpora: • the Europarl corpus (40.1M words), • the news-commentary corpus (1.6M words), • the Canadian Hansard corpus (72.4M words). All word counts are given after tokenisation for the French part of the bitexts."
2008.iwslt-papers.6,D07-1091,0,0.0138059,"ed Nations. A particular jargon is often used in these texts, that may not be appropriate for the translation of more general texts. A recent evalua- 182 - tion on automatic translation between European languages has for instance shown that statistical systems perform very well on test data drawn from the European Parliament corpus, i.e. texts of the same type that they were trained on, but their performance can be inferior to rule-based systems for general news data [2]. There are several directions of research to improve the genericity of SMT systems, for instance factored translation model [3], the integration of high quality dictionaries [4] or statistical post-editing of rule-based systems [5, 6]. In this work we investigate whether large-scale unsupervised training is useful to develop a generic SMT system. We define unsupervised training as using the system itself to produce additional bilingual data, i.e. without using a human to perform the translations. The SMT system used to translate the texts was of course itself trained on some bitexts, but these bitexts may be limited in size or little related to the translation task. These resources used to build the initial SMT system"
2008.iwslt-papers.6,W08-0313,1,0.907546,"hese texts, that may not be appropriate for the translation of more general texts. A recent evalua- 182 - tion on automatic translation between European languages has for instance shown that statistical systems perform very well on test data drawn from the European Parliament corpus, i.e. texts of the same type that they were trained on, but their performance can be inferior to rule-based systems for general news data [2]. There are several directions of research to improve the genericity of SMT systems, for instance factored translation model [3], the integration of high quality dictionaries [4] or statistical post-editing of rule-based systems [5, 6]. In this work we investigate whether large-scale unsupervised training is useful to develop a generic SMT system. We define unsupervised training as using the system itself to produce additional bilingual data, i.e. without using a human to perform the translations. The SMT system used to translate the texts was of course itself trained on some bitexts, but these bitexts may be limited in size or little related to the translation task. These resources used to build the initial SMT system are usually not considered as supervision in the"
2008.iwslt-papers.6,2007.mtsummit-papers.43,0,0.0210207,"binarize the phrase table and to keep it on the disk. In this representation 46GB of disk space are needed. It is certainly possible to reduce these storage needs by filtering the phrase table in order to suppress unlikely entries, but this was not used in this work. Translation was performed by batches of 200 000 sentences on several machines in parallel. The processing time to translate 275M words amounts to about 1000 hours which corresponds to a translation speed of more than 75 words per second. We anticipate that this could be substantially improved, in particular by using cube-pruning [19] that was recently implemented in the Moses decoder. 100-best lists were generated including the values of the various feature functions and the segmentation information. Figure 2 shows some examples of the automatic translations. We attribute the apparent good quality to the good coverage of our bilingual dictionary and to the quality of the target language model. We plan to make these automatic translations available to the research commuProceedings of IWSLT 2008, Hawaii - U.S.A. 350 large SMT sys, afp2x small SMT sys, afp2x afp9x eparl 23 dev test 250 BLEU score Words of bitext [M] 300 200"
2009.iwslt-evaluation.10,2006.iwslt-evaluation.15,0,0.0321155,"and Dev3 corpora. The target language model was trained on the English side of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation prob"
2009.iwslt-evaluation.10,P07-2045,0,0.00776614,"e of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion"
2009.iwslt-evaluation.10,W07-0732,0,0.0227542,"us 376k words En Btec human translations Dev1−3 Giza++ Phrase extraction SRILM CSLM Phrase table 4g LM 4g CSLM phrase table SMT system 4−gram LM Moses Ar/En Src Moses 1000 bests Trg Src LM rescoring phrase table Trg SYSTRAN decode optimized with MERT λi Moses En’/En Condor BLEU SPE system 2nd pass optimisation Figure 2: Comparison of SMT and SPE systems. Figure 1: Architecture of the SMT system. 3. SPE System In the last years, there is increasing interest in the interaction between rule-based and statistical machine translation. A popular and successful idea is statistical post editing (SPE) [6, 7]. The principle idea is to train an SMT system to correct the outputs of a rule-based translation system. This is shown in figure 2. The operation performed by the rulebased translation system could also be seen as a very good tokenization or preprocessing, that actually performs many of the translation steps. Therefore, the task of the SMT system itself is very simplified. Accordingly, we argue that an SMT and SPE system are only two extreme cases of the interaction between tokenization/preprocessing and translation itself. An interesting question is whether both systems can be combined since"
2009.iwslt-evaluation.10,J07-2003,0,0.131844,"Missing"
2009.iwslt-evaluation.10,W09-0424,0,0.0287796,"Missing"
2009.iwslt-evaluation.10,2008.iwslt-evaluation.6,0,0.0423786,"Missing"
2009.iwslt-evaluation.10,P07-1040,0,0.0322066,"he development set (Dev6) using the provided Z-MERT procedure. The grammar rules extraction tools and Z-MERT are provided in the Joshua toolkit. Figure 3 summarizes the architecture of the Joshua translation system. The decoder is based on the token pass decoding algorithm. The scores used to evaluate the hypotheses are the following: 5. System combination • the system score : this replaces the score of the translation model. Until now, the words given by all systems 1 have the same probability which is . M The system combination approach is based on confusion network decoding as described in [11, 12] and shown in Figure 4. The protocol can be decomposed into three steps : • the language model (LM) probability. The 4-gram LM used for the combination is the same than the one used by each single system. 1. 1-best hypotheses from all M systems are aligned and confusion networks are built. It is obvious that this combination framework is not optimal, but as we can see in the results section, this simple architecture can already achieve improvements when combining only two systems. 2. All confusion networks are connected into a single lattice. 6. Experimental Evaluation 3. A 4-gram language mod"
2009.iwslt-evaluation.10,2006.amta-papers.25,0,0.0173726,"ter optimize our hierarchical systems built with Joshua. Rescoring the n-best lists with the continuous space LM achieved an improvement of 1.2 BLEU on the internal test set for the Arabic/English SMT system, and 0.6 BLEU for the SPE system. Due to time constraints, the continuous space LM was not applied on the hierarchical system. The improvements obtained by the CSLM are generally smaller 5.1. Hypotheses alignment and confusion network generation For each segment, the best hypotheses of M − 1 systems are aligned against the last one used as backbone. The alignment is done with the TER tool [13], without any tuning performed at this step (default edit costs are used). M confusion networks are generated in this way. Then all the confusion networks are connected into a single lattice by adding a first and last node. The probability of the first arcs must reflect how well such system provides a well structured hypothesis (good order). In our experiments, no tuning was done at this step, and we chose equal prior probabilities for all systems. - 67 - Proceedings of IWSLT 2009, Tokyo - Japan Approach: Train bitexts Arabic/English: Btec+Dev123 Btec+Dev1236 LM SMT Moses Dev Test Hierarchical"
2009.iwslt-evaluation.10,W07-0728,0,\N,Missing
2009.iwslt-evaluation.10,2008.iwslt-evaluation.10,0,\N,Missing
2009.mtsummit-posters.17,2008.iwslt-papers.1,0,0.0236257,"oder is able to output the phrase and word alignments. This would speed up the process of creating the adapted SMT system since we skip the timeconsuming word alignment performed by GIZA++. Source AFP APW ASB HYT NHR UMH XIN Arabic 145M 7M 175M 188M 1M 58M French 570M 200M - Table 4: Characteristics of the available monolingual Gigaword corpora (number of words). It could also be that the decoder-induced word alignments are more appropriate than those performed by GIZA++. This was partially investigated in the framework of pivot translation to produce artificially bitexts in another language (Bertoldi et al., 2008). Finally, instead of only using the 1-best translation we could also use the n-best list. LDC’s Arabic and French Gigaword corpora are described in Table 4. There is only one source that does exist in both languages: the AFP collection. It is likely that the Arabic and French texts partially cover the same facts, but they are usually not direct translations.6 In fact, we were informed that journalists at AFP have the possibility to freely change the sentences when they report on a fact based on text already available in another language. Nevertheless, it can be expected that using these texts"
2009.mtsummit-posters.17,W08-0309,0,0.0498661,"Missing"
2009.mtsummit-posters.17,P08-2040,0,0.399797,"e probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language"
2009.mtsummit-posters.17,W07-0722,0,0.0834673,"ual data. One can distinguish two types of translation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for i"
2009.mtsummit-posters.17,W07-0717,0,0.203525,"nd second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in"
2009.mtsummit-posters.17,W08-0509,0,0.0246917,"approached by pivoting through English, but we don’t have comparable BLEU scores for this kind of approach. 3 Baseline system The baseline system is a standard phrase-based SMT system based on the the Moses SMT toolkit (Koehn et al., 2007). It uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. It constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).4 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the CMERT tool. 4 The source is available at http://www.cs.cmu.edu/ ˜qing/ 3.1 Tokenization There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance (Habash and Sadat"
2009.mtsummit-posters.17,N06-2013,0,0.062687,"and Vogel, 2008).4 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the CMERT tool. 4 The source is available at http://www.cs.cmu.edu/ ˜qing/ 3.1 Tokenization There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance (Habash and Sadat, 2006). It is clear that such a decomposition is most helpful when the translation model training data is limited, but this is less obvious for tasks where several hundreds of millions of words of bitexts are available. Most of the published work is based on the freely available tools, like the Buckwalter transliterator and the MADA and TOKAN tools for morphological analysis from Columbia University. In this work, we compare two different tokenization of the Arabic source text: a full word mode and the morphological decomposition provided by the sentence analysis module of SYSTRAN’s rulebased Arabic"
2009.mtsummit-posters.17,hasan-ney-2008-multi,0,0.0458589,"ds Vocab Words Vocab DGA T RAMES 262k 30k 400k 18k News 1.1M 67k 1.3M 41k commentary UN 149M 712k 212M 420k Table 1: Characteristics of the available bitexts The DGA also provided a test set that was created in the same way than the in-domain bitexts. Four high-quality reference translations are available. We randomly split this data into a development set for system tuning and an internal test set. The details of the development and test set are given in Table 2. We are only aware of one other large Arabic/French news translation system, the one that was developed during the T RAMES project (Hasan and Ney, 2008). In that work, results are reported on the same test set, but different bitexts for training were 1 Traduction Automatique par M´ethodes Statistiques Direction g´en´erale de l’armement 3 http://www.project-syndicate.org 2 Dev data: Sentences Words Test data: Sentences Words Arabic French 235 9931 940 (4x) 45038 231 10246 924 (4x) 47066 Table 2: Characteristics of the development and test set. used, namely the T RAMES bitexts, UN data from the period 2001 to April 2007, archives of Amnesty International and articles from Le Monde Diplomatique. The authors report a BLEU score of 41.1 on the who"
2009.mtsummit-posters.17,2005.eamt-1.19,0,0.0862915,"guage modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008). Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006). The idea is to translate the test data, to filter t"
2009.mtsummit-posters.17,W07-0733,0,0.161708,"ation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applie"
2009.mtsummit-posters.17,P07-2045,0,0.00472637,"Missing"
2009.mtsummit-posters.17,2008.iwslt-papers.6,1,0.816939,"obably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing, 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. We could obtain small improvements in the BLEU score in a French/English translation system. Although this technique seems to be close to self enhancing as proposed by (Ueffing, 2006), there is a conceptual difference. We do not use the test data to adapt the translation model, but large amounts of monolingual training data in the source language and we create a complete new model that can be applied to any test data without additional mod"
2009.mtsummit-posters.17,D08-1090,0,0.118412,"This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008). Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with t"
2009.mtsummit-posters.17,2006.iwslt-papers.3,0,0.515306,"mation retrieval techniques, for instance (Hildebrand et al., 2005). Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008). Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be"
2009.mtsummit-posters.17,P07-1004,0,0.353323,"ble that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing, 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. We could obtain small improvements in the BLEU score in a French/English translation system. Although this technique seems to be close to self enhancing as proposed by (Ueffing, 2006), there is a conceptual difference. We do not use the t"
2009.mtsummit-posters.17,C04-1059,0,0.0987882,"slation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005). Recentl"
2009.mtsummit-posters.17,W07-0718,0,\N,Missing
2010.jeptalnrecital-long.1,W07-0722,0,0.0454619,"Missing"
2010.jeptalnrecital-long.1,W07-0717,0,0.0425072,"Missing"
2010.jeptalnrecital-long.1,W08-0509,0,0.0527201,"Missing"
2010.jeptalnrecital-long.1,N06-2013,0,0.0856565,"Missing"
2010.jeptalnrecital-long.1,hasan-ney-2008-multi,0,0.047962,"Missing"
2010.jeptalnrecital-long.1,2005.eamt-1.19,0,0.0478676,"Missing"
2010.jeptalnrecital-long.1,2009.mtsummit-papers.7,0,0.0448217,"Missing"
2010.jeptalnrecital-long.1,P07-2045,0,0.00397342,"Missing"
2010.jeptalnrecital-long.1,W07-0733,0,0.0682843,"Missing"
2010.jeptalnrecital-long.1,2008.iwslt-papers.6,1,0.898675,"Missing"
2010.jeptalnrecital-long.1,D08-1090,0,0.038839,"Missing"
2010.jeptalnrecital-long.1,2006.amta-papers.25,0,0.115158,"Missing"
2010.jeptalnrecital-long.1,2006.iwslt-papers.3,0,0.0563452,"Missing"
2010.jeptalnrecital-long.1,P07-1004,0,0.0553025,"Missing"
2010.jeptalnrecital-long.1,C04-1059,0,0.0450443,"Missing"
2011.iwslt-evaluation.10,2011.iwslt-evaluation.1,0,0.0188061,"f Le Mans, France firstname.lastname@lium.univ-lemans.fr Abstract This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign. We participated in three of the proposed tasks, namely the Automatic Speech Recognition task (ASR), the ASR system combination task (ASR_SC) and the Spoken Language Translation task (SLT), since these tasks are all related to speech translation. We present the approaches and specificities we developed on each task. 1. Introduction This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign [1]. This year, new interesting tasks were proposed compared to last year evaluation campaign. As a matter of fact, the three tasks we participated in are all linked together in the same pipeline: speech recognition, ASR system combination and speech translation. Like the last year campaign, all of the considered tasks were related to the TED talks, requiring speech recognition of English, and speech translation from English to French. 2.1. The LIUM’s TED corpus For our system training, we aimed at using audio and transcripts from the TED talks. In order to get the desired data, we developed a sp"
2011.iwslt-evaluation.10,W11-2158,1,0.806845,"tter performance, but with the help of a second level of filtering to discard the out-of-domain data. For this task, we considered the following corpora among those available: the latest versions of News-Commentary and Europarl, the TED corpus provided by the organizers and a subset of the French–English 109 Gigaword. Like the last year’s evaluation campaign, we didn’t took into account the un200x corpus due to our experiments, showing its inappropriate style regarding the TED in-domain data. The Gigaword corpus was filtered with the same techniques used in our WMT 11 systems, as described in [20]. We call this internal subset ccb2. Table 5 summarizes the characteristics of those different corpora. In order to filter our ccb2 corpus, we tried a filtering approach based on LM perplexities, inspired by previous work described in [21]. We first built a 4-gram LM on the English data from the TED corpus. Using this LM, we computed the perplexity of each sentence from the ccb2 English part and sorted them in an ascending order. We then applied different thresholds on the sorted corpus and the resulting sets were integrated in our training data, in order to study the impact of the selection o"
2011.iwslt-evaluation.10,2010.iwslt-evaluation.14,1,0.868227,"y the organizers after the dev2010 and tst2010 run submissions) in a confusion network and extracting the most likely solution from it. We also changed the repartition of the talks between the original development and test set, increasing the size of the dev set by reducing the size of the test set, in order to make the tuning process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in bo"
2011.iwslt-evaluation.10,1983.tc-1.13,0,0.0562963,"and tst2010 run submissions) in a confusion network and extracting the most likely solution from it. We also changed the repartition of the talks between the original development and test set, increasing the size of the dev set by reducing the size of the test set, in order to make the tuning process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We us"
2011.iwslt-evaluation.10,N03-1017,0,0.00415307,"g process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, usi"
2011.iwslt-evaluation.10,J03-1002,0,0.00381576,"g process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, usi"
2011.iwslt-evaluation.10,P02-1038,0,0.0983405,"ase, it was necessary to recover them for the final output. We used the same technique as in last year’s evaluation campaign, namely recasing using a separate SMT system dedicated to this task [12]. This technique is summarized by figure 1. The main differences with last year are: • less but more appropriate training data using perplexity data selection based on a French in-domain LM; (1) • suppression of the lexical reordering (instead of limiting it); The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [16]. • a better development set for the tuning of the system, coming from a real ASR output. = e i 1 The source is available at http://www.cs.cmu.edu/~qing/ 82 Corpus TED News-Comm. Europarl v6 ccb2 TOTAL LIUM dev2010 LIUM tst2010 #En tokens (millions) Orig. ASR 2.0M 1.8M 2.8M 2.6M 50.6M 46.6M 232.5M 220.0M 287.9M 271.0M N/A 39k N/A 9k #Fr tokens (millions) Orig. ASR 2.2M 2.0M 3.3M 3.1M 56.2M 51.2M 272.6M 258.4M 334.3M 314.7M N/A 39k N/A 9k Table 5: Characteristics of the considered parallel corpora. Orig is the original data while ASR is the processed data. 4.2. Bilingual data selection Data set"
2011.iwslt-evaluation.10,P07-2045,0,0.0108598,"y a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, using the MERT tool. 4.1.2. Language modeling The French language models were trained on all the French parts of the allowed parallel corpora, in addition to the proposed News monolingual corpus. 4-gram back-off"
2011.iwslt-evaluation.10,W08-0509,0,0.0733857,"development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, using the MERT tool. 4.1.2. Language modeling The French language models were trained on all the French parts of the allowed parallel corpora, in addition to the proposed News monolingual corpus. 4-gram back-off LMs were used. The word list contains all the French words of our phrase table filtered on the 150k words from the ASR decoding vocabulary. Separa"
2011.mtsummit-papers.17,W09-0401,0,0.0262197,"(Specia, 2011) or the comparison with human translation (Plitt and Masselot, 2010). Other approaches take into account “user activity data” covering keystrokes (Barrett et al., 2001) or eye movement detection (Doherty et al., 2010). Implicitly, estimating PE effort is the driver for establishing better quality evaluation metrics. For instance HTER (Snover et al., 2006) calculating translation edit rate towards targeted reference translation provides a reproducible metric, well correlated with human judgment on translation quality and close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch et al. (2009) introduced a new task: editing to evaluate translation where the edited translation is not used as a reference nor the reviewer asked to perform the least number of edits, but to make the translation ﬂuent without access to reference translation. The edited translation is then evaluated in a second phase of the evaluation task. However, the result of this task is not conclusive due to the variability between posteditors, and no strong correlation is observed with sentence quality judgment. With METEOR, Lavie and Agarwal (2007) introduced the possibility of evaluating quality based on intuitiv"
2011.mtsummit-papers.17,W07-0732,1,0.921397,"and is therefore naturally suited for HTER evaluation, however in our approach, translation edit rate based on “mechanical edits” count is just an intermediate analysis to expose “logical edits” taking into account part of speech, lemmatization, and constituent structure of the sentences. 1.3 Can we reduce the effort? Beyond analysis, the general problem is how PE effort can be reduced. Multiple approaches can be quoted for that purpose: Guzm´an (2007) describes a set-up where MT output passes through a set of PE rules designed to smooth out translation output for a highly customized system. Dugast et al. (2007) and Simard et al. (2007) describe a set-up where an SMT system is trained on a bilingual corpus constituted with both MT output and human reference, and show how the sys165 tem learn how to “correct the translation output”. Schwenk et al. (2009) reproduce this with a Statistical Post-Editing (SPE) system trained on very large corpus making the initial translation as a mere preprocessing. In both cases, the SMT system beneﬁts from higher similarity between pretranslated text and reference compared to source and reference; however, if the ﬁnal quality is higher, the system does not learn post-e"
2011.mtsummit-papers.17,2005.eamt-1.13,0,0.0661961,"Missing"
2011.mtsummit-papers.17,P07-2045,0,0.00327859,"y four different professional translators, who were French native speakers (Plitt and Masselot, 2010). The post-editors were provided with simple PE guidelines to produce publishable quality at the lowest effort, avoiding changes due to stylistic or personal preferences. The post-editors are presented once sentence at a time, in the same order in which they appear in the original source document, without any further functionality supporting the PE activity (e.g. no terminology lookup). Some of the PE tasks used MT outputs generated with a Moses engine, an SMT system trained on in-domain data (Koehn et al., 2007), others with the SYSTRAN system. Note that the post-editors were not informed which MT system was used. Although our aim was not to compare RBMT versus SMT, it was interesting to note that our approach applies equally on both system outputs. 4.1 Human Baseline A subset of 100 sentences (the baseline) was tagged manually using XML format as shown in ﬁgure 2. The aim is to compare our automatic results to this reference analysis. Table 1 describes the human analysis of 100 sentences: in these sentences each PEA has been classiﬁed according to the previous typology. The left part corresponds to"
2011.mtsummit-papers.17,W07-0734,0,0.0303739,"nd close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch et al. (2009) introduced a new task: editing to evaluate translation where the edited translation is not used as a reference nor the reviewer asked to perform the least number of edits, but to make the translation ﬂuent without access to reference translation. The edited translation is then evaluated in a second phase of the evaluation task. However, the result of this task is not conclusive due to the variability between posteditors, and no strong correlation is observed with sentence quality judgment. With METEOR, Lavie and Agarwal (2007) introduced the possibility of evaluating quality based on intuitive “human assimilation”: matches on lemmatized forms, and synonymy seek to address deﬁciencies of simpler word-based metrics. In our context, post-editors are professional translators with very strict guidelines to perform “light” PE (which is possible on technical documentation for already highly customized translation). This creates natural “human targeted reference” and is therefore naturally suited for HTER evaluation, however in our approach, translation edit rate based on “mechanical edits” count is just an intermediate an"
2011.mtsummit-papers.17,N10-1062,0,0.0495022,"Missing"
2011.mtsummit-papers.17,W09-0423,1,0.841583,", and constituent structure of the sentences. 1.3 Can we reduce the effort? Beyond analysis, the general problem is how PE effort can be reduced. Multiple approaches can be quoted for that purpose: Guzm´an (2007) describes a set-up where MT output passes through a set of PE rules designed to smooth out translation output for a highly customized system. Dugast et al. (2007) and Simard et al. (2007) describe a set-up where an SMT system is trained on a bilingual corpus constituted with both MT output and human reference, and show how the sys165 tem learn how to “correct the translation output”. Schwenk et al. (2009) reproduce this with a Statistical Post-Editing (SPE) system trained on very large corpus making the initial translation as a mere preprocessing. In both cases, the SMT system beneﬁts from higher similarity between pretranslated text and reference compared to source and reference; however, if the ﬁnal quality is higher, the system does not learn post-editing. Through the introduction of PEA, our study shows that a large part of the PE effort can be classiﬁed and automatically learn. The rest of this paper is organized as follows: in section 2, we introduce and deﬁne the notion of PEA. We also"
2011.mtsummit-papers.17,W07-0728,0,0.0223805,"ly suited for HTER evaluation, however in our approach, translation edit rate based on “mechanical edits” count is just an intermediate analysis to expose “logical edits” taking into account part of speech, lemmatization, and constituent structure of the sentences. 1.3 Can we reduce the effort? Beyond analysis, the general problem is how PE effort can be reduced. Multiple approaches can be quoted for that purpose: Guzm´an (2007) describes a set-up where MT output passes through a set of PE rules designed to smooth out translation output for a highly customized system. Dugast et al. (2007) and Simard et al. (2007) describe a set-up where an SMT system is trained on a bilingual corpus constituted with both MT output and human reference, and show how the sys165 tem learn how to “correct the translation output”. Schwenk et al. (2009) reproduce this with a Statistical Post-Editing (SPE) system trained on very large corpus making the initial translation as a mere preprocessing. In both cases, the SMT system beneﬁts from higher similarity between pretranslated text and reference compared to source and reference; however, if the ﬁnal quality is higher, the system does not learn post-editing. Through the intro"
2011.mtsummit-papers.17,2006.amta-papers.25,0,0.52501,"measure post-editing effort? The measure of PE effort is important from a business perspective since it sets up the productivity of post-editors and subsequently the potential for additional cost-saving. The most criteria are the measure of PE time (Specia, 2011) or the comparison with human translation (Plitt and Masselot, 2010). Other approaches take into account “user activity data” covering keystrokes (Barrett et al., 2001) or eye movement detection (Doherty et al., 2010). Implicitly, estimating PE effort is the driver for establishing better quality evaluation metrics. For instance HTER (Snover et al., 2006) calculating translation edit rate towards targeted reference translation provides a reproducible metric, well correlated with human judgment on translation quality and close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch et al. (2009) introduced a new task: editing to evaluate translation where the edited translation is not used as a reference nor the reviewer asked to perform the least number of edits, but to make the translation ﬂuent without access to reference translation. The edited translation is then evaluated in a second phase of the evaluation task. However, the"
2011.mtsummit-papers.17,2011.eamt-1.12,0,0.0968263,"ant factor in reducing post-editing time” (Martinez, 2003). However describing the errors does not provide us with a methodology for ﬁxing them and always leads to system-dependent remediation approaches. In our approach, we are less interested in understanding the errors than deﬁning the correct action to obtain a good translation. 1.2 How can we measure post-editing effort? The measure of PE effort is important from a business perspective since it sets up the productivity of post-editors and subsequently the potential for additional cost-saving. The most criteria are the measure of PE time (Specia, 2011) or the comparison with human translation (Plitt and Masselot, 2010). Other approaches take into account “user activity data” covering keystrokes (Barrett et al., 2001) or eye movement detection (Doherty et al., 2010). Implicitly, estimating PE effort is the driver for establishing better quality evaluation metrics. For instance HTER (Snover et al., 2006) calculating translation edit rate towards targeted reference translation provides a reproducible metric, well correlated with human judgment on translation quality and close by deﬁnition to “translation post-editing”. In WMT09, Callison-Burch"
2011.mtsummit-papers.17,vilar-etal-2006-error,0,0.234162,"Missing"
2012.amta-papers.21,D11-1033,0,0.0501955,"M on the achieved by extending equation 2 as follows: source (or target) side of the bitexts, independently C for each corpus. There is a well known EM proceX ˜ wc Countc (˜ si , tij ) dure to linearly interpolate these individual LMs c=1 to minimize the perplexity on some development P (t˜ij |˜ si ) = C (3) X X data. The resulting corpus coefficients can be diwc Countc (˜ si , t˜ik ) rectly used to weight the parallel corpora. c=1 k Perplexity can also be used to weight each individual sentence. This was used to select a releThe equation 3 is identical as given in (Matvant subset of LM data (Axelrod et al., 2011) or soukas et al., 2009), where wc represents the bitexts (Moore and Lewis, 2010). In our case, we features-mapped to a weight calculated for each build a LM on the source side of the in-domain sentence by neural network. However in our case corpus and use this model to calculate the perplexit represents the direct weight for each corpus. If ity of each sentence in all the other corpora. Since all corpus weights are identical, equation 3 simlower perplexity represents “better” sentences, we plifies to the original formulation in equation 2. set q(si , ti ) to the inverse of the perplexity. It"
2012.amta-papers.21,W11-2138,0,0.0201059,"domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vecto"
2012.amta-papers.21,P08-2040,0,0.0322483,"f the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a g"
2012.amta-papers.21,W07-0722,0,0.0195778,"e genre of the data may be also different, for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation qualit"
2012.amta-papers.21,W07-0717,0,0.225857,"for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a techn"
2012.amta-papers.21,D10-1044,0,0.0928776,"scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vectors were mapped to scalar weights (0, 1) which are then used to estimate probability with weighted counts. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips and Brown (2011) trained the models with a second-order Taylor approximation of weighted translation instances and discount models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions"
2012.amta-papers.21,2010.amta-papers.21,0,0.0305351,"the goodness scores at the important to note that our approach is a generalizasentence level, we will get: tion of data selection approaches: instead of doing a hard decision which data to keep to discard, we ( ) C S X Y keep all the sentences and attach a weight to each s wc Countc (˜ si , t˜ij ) · hγc,s (˜ si , t˜ij ) one (this weight could be zero in an extreme case). c=1 s=1 P (t˜ij |˜ si ) = C ( ) S X X Y It was also observed that parallel sentences s wc Countc (˜ si , t˜ik ) · hγc,s (˜ si , t˜ik ) which are closer to the test set period are more imc=1 s=1 k (4) portant than older ones (Hardt and Elming, 2010; where γs is an additional parameter to weight Levenberg et al., 2010; Shah et al., 2011), in parthe different sentence goodness scores among each ticular when translating texts in the news domain. other. We implemented phrase probability calculaFollowing (Shah et al., 2011), we use an exponention according to equation 4 in the the memscore tial decay function: tool of Moses. q(si , ti ) = e−α·∆t i (5) 2.3 Calculation of the corpus weights and where α is the decay factor and ∆t is the dissentence goodness scores cretized time distance (0 for most recent part, 1 Our theoretical framework and i"
2012.amta-papers.21,C10-1056,0,0.0151975,"ach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips and Brown (2011) trained the models with a second-order Taylor approximation of weighted translation instances and discount models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to boost good phrase translations and bad translations are discarded. Shah et al. (2010) proposed a technique to weight heterogeneous data by weighted resampling of the alignments. In an extended work, the same authors proposed to consider meta-weights for each part of the training data (Shah et al., 2011). The work proposed in this paper is an extension and generalization of several"
2012.amta-papers.21,W07-0733,0,0.0577815,"c text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted"
2012.amta-papers.21,N03-1017,0,0.00993485,"urteen feature functions: the above mentioned four scores for the phrases, a phrase and word penalty, six scores for the lexicalized distortion model, a language model score and a distance based reordering model. The phrase-table itself is created by the following procedure 1. collect parallel training data 2. eventually discard sentence pairs that are too long or which have a large length difference 3. run Giza++ on this data in both directions (source-to-target and target-to-source) 4. use some heuristics to symmetrize the alignments in both directions, e.g. the so-called grow-diagonal-... (Koehn et al., 2003) and extract a list of phrases 5. calculate the lexical probabilities 6. calculate the phrase probabilities P (t˜|˜ s) and ˜ P (˜ s|t). 7. create the phrase table by merging the forward and backward probabilities In our approach we only modify the way how the phrase translations probabilities P (t˜|˜ s) and P (˜ s|t˜) are calculated. The goal is to increase the probability of phrase pairs which we believe to be more important for the considered task, to be more reliable, etc; and consequently, to down weight those which should be used less often. It is important to point out that our phrase ta"
2012.amta-papers.21,P07-2045,0,0.00815239,"odness scores file. Then, phrases are extracted and the goodness score q(si , ti ) is synchronized with the phrases. In the case that one phrase occurs in multiple sentences (this actually happens quite often), we use the arithmetic mean of the goodness scores in our experiments. The maximum or some other interpolation functions could En tokens 2.0 2.8 50.6 232.5 287.9 Fr tokens 2.2 3.3 56.2 272.6 334.3 Table 1: Size of parallel corpora (in millions) to build baseline systems for WMT and IWSLT Tasks. 3 Experimental evaluation We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007). The scoring framework is implemented by extending the memory based scoring tool called memscore (Hardmeier, 2010) available in the Moses toolkit. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The MERT tool (Och, 2003) is used to tune the coefficients of these feature functions. The experiments are performed on two well-known evaluation tasks i.e. the 2011 WMT and IWSLT English/Fren"
2012.amta-papers.21,W11-2132,1,0.860059,"Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the gi"
2012.amta-papers.21,N10-1062,0,0.0172727,"eneralizasentence level, we will get: tion of data selection approaches: instead of doing a hard decision which data to keep to discard, we ( ) C S X Y keep all the sentences and attach a weight to each s wc Countc (˜ si , t˜ij ) · hγc,s (˜ si , t˜ij ) one (this weight could be zero in an extreme case). c=1 s=1 P (t˜ij |˜ si ) = C ( ) S X X Y It was also observed that parallel sentences s wc Countc (˜ si , t˜ik ) · hγc,s (˜ si , t˜ik ) which are closer to the test set period are more imc=1 s=1 k (4) portant than older ones (Hardt and Elming, 2010; where γs is an additional parameter to weight Levenberg et al., 2010; Shah et al., 2011), in parthe different sentence goodness scores among each ticular when translating texts in the news domain. other. We implemented phrase probability calculaFollowing (Shah et al., 2011), we use an exponention according to equation 4 in the the memscore tial decay function: tool of Moses. q(si , ti ) = e−α·∆t i (5) 2.3 Calculation of the corpus weights and where α is the decay factor and ∆t is the dissentence goodness scores cretized time distance (0 for most recent part, 1 Our theoretical framework and implementation is generic and does not depend on the exact calculation"
2012.amta-papers.21,D09-1074,0,0.38334,"hao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vectors were mapped to scalar weights (0, 1) which are then used to estimate probability with weighted counts. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips a"
2012.amta-papers.21,P10-2041,0,0.0432961,"the bitexts, independently C for each corpus. There is a well known EM proceX ˜ wc Countc (˜ si , tij ) dure to linearly interpolate these individual LMs c=1 to minimize the perplexity on some development P (t˜ij |˜ si ) = C (3) X X data. The resulting corpus coefficients can be diwc Countc (˜ si , t˜ik ) rectly used to weight the parallel corpora. c=1 k Perplexity can also be used to weight each individual sentence. This was used to select a releThe equation 3 is identical as given in (Matvant subset of LM data (Axelrod et al., 2011) or soukas et al., 2009), where wc represents the bitexts (Moore and Lewis, 2010). In our case, we features-mapped to a weight calculated for each build a LM on the source side of the in-domain sentence by neural network. However in our case corpus and use this model to calculate the perplexit represents the direct weight for each corpus. If ity of each sentence in all the other corpora. Since all corpus weights are identical, equation 3 simlower perplexity represents “better” sentences, we plifies to the original formulation in equation 2. set q(si , ti ) to the inverse of the perplexity. It is Considering in addition the goodness scores at the important to note that our"
2012.amta-papers.21,P03-1021,0,0.0201975,"in millions) to build baseline systems for WMT and IWSLT Tasks. 3 Experimental evaluation We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007). The scoring framework is implemented by extending the memory based scoring tool called memscore (Hardmeier, 2010) available in the Moses toolkit. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The MERT tool (Och, 2003) is used to tune the coefficients of these feature functions. The experiments are performed on two well-known evaluation tasks i.e. the 2011 WMT and IWSLT English/French evaluations. The corpora and their sizes used to build the systems for both these tasks are given in table 1. 3.1 Experiments on the WMT task For the WMT task we used the official development sets of the 2011 WMT translation tasks, i.e news-test09 as development corpus and newstest10 as test corpus. We built English-French systems by using the time-stamped Europarl and 1 with CONDOR (Berghen and Bersini, 2005) WMT Task Baselin"
2012.amta-papers.21,2011.mtsummit-papers.2,0,0.0141079,"al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vectors were mapped to scalar weights (0, 1) which are then used to estimate probability with weighted counts. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips and Brown (2011) trained the models with a second-order Taylor approximation of weighted translation instances and discount models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation p"
2012.amta-papers.21,2008.iwslt-papers.6,1,0.85596,"sus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted"
2012.amta-papers.21,W10-1759,1,0.927706,"unt models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to boost good phrase translations and bad translations are discarded. Shah et al. (2010) proposed a technique to weight heterogeneous data by weighted resampling of the alignments. In an extended work, the same authors proposed to consider meta-weights for each part of the training data (Shah et al., 2011). The work proposed in this paper is an extension and generalization of several ideas proposed in previous works such as weighted counts with goodness scores. However our proposed framework gives the flexibility to inject the goodness scores in a unified formulation calculated at various levels. It is based on the following principles: • the use of a set of “quality measures” at"
2012.amta-papers.21,I11-1148,1,0.909749,"bability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to boost good phrase translations and bad translations are discarded. Shah et al. (2010) proposed a technique to weight heterogeneous data by weighted resampling of the alignments. In an extended work, the same authors proposed to consider meta-weights for each part of the training data (Shah et al., 2011). The work proposed in this paper is an extension and generalization of several ideas proposed in previous works such as weighted counts with goodness scores. However our proposed framework gives the flexibility to inject the goodness scores in a unified formulation calculated at various levels. It is based on the following principles: • the use of a set of “quality measures” at different levels: weights for each corpus (or data source) and for each individual sentence in the bitexts. • no additional feature functions to express the quality or appropriateness of certain phrase pairs, but we mo"
2012.amta-papers.21,2006.iwslt-papers.3,0,0.0463553,"data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimi"
2012.amta-papers.21,P07-1004,0,0.0253161,", the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discrimina"
2012.amta-papers.21,C04-1059,0,0.23709,"be also different, for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al."
2012.amta-papers.21,W04-3227,0,0.19264,"be also different, for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on translation quality. Matsoukas et al."
2012.iwslt-papers.12,W12-3123,0,0.0744865,"nsequently, they need to be regularly re-trained in order to be updated, which is usually computationally demanding. The goal of incremental adaptation is then twofold: to adapt the system on the ﬂy when new resources are available without re-training the entire system. Post-Editing (PE) the output of SMT systems is widely used, amongst others, by professional translators of localization services which need for example to translate technical data in speciﬁc domains into several languages. However, the use of PE is restricted by some aspects that must be taken into consideration. As resumed by [1], the time spent by the post-editor is a commonly used measure of the PE effort, which should not to be, in case of poor translation quality, more important than translation from scratch. Even if this temporal aspect can be see as the most important, PE effort can be evaluated using automatic metrics based on the edit Jean Senellart† †Systran SA 5, rue Feydeau 75002 Paris, France lastname@systran.fr distance. These metrics commonly use the number of required edits of the MT system output to reach a reference translation. From then, the combination of PE and incremental adaptation can be seen a"
2012.iwslt-papers.12,2011.mtsummit-papers.17,1,0.734204,"ion from scratch. Even if this temporal aspect can be see as the most important, PE effort can be evaluated using automatic metrics based on the edit Jean Senellart† †Systran SA 5, rue Feydeau 75002 Paris, France lastname@systran.fr distance. These metrics commonly use the number of required edits of the MT system output to reach a reference translation. From then, the combination of PE and incremental adaptation can be seen as a way to reduce the task effort by allowing a MT system to gradually learn from its own errors. Especially considering the repetitive nature of the task highlighted by [2]. However, incremental adaptation is still a tricky task: how to adapt the system correctly? Adaptation should not degrade system performance and accuracy. Some approaches are possible and we will try to see the impact of several of them in the second part of this article. First of all, we present a new experimental approach for incremental adaptation of a MT system using PE analysis. Starting from a generic baseline, we have gradually adapted our system by translating an in-domain corpora which was split beforehand. Each part of the corpora was translated using the translation model adapted a"
2012.iwslt-papers.12,2010.amta-papers.21,0,0.722949,"of them in the second part of this article. First of all, we present a new experimental approach for incremental adaptation of a MT system using PE analysis. Starting from a generic baseline, we have gradually adapted our system by translating an in-domain corpora which was split beforehand. Each part of the corpora was translated using the translation model adapted at the previous step, i.e. updated with new extracted phrases. These phrases are the result of a word-to-word alignment combination we present afterward. 1.1. Similar work The most similar approach in the literature is proposed in [3] who present an incremental re-training algorithm to simulate a post-editing situation. It is proposed to extract new phrases from approximate alignments which were obtained by a modiﬁed version of Giza-pp [4]. An initial alignment with one-to-one links between the same sentence positions is created and then iteratively updated as long as improvements are observed. In practice, a greedy search algorithm is used to ﬁnd the locally optimal word alignment. All source positions carrying only one link are tried, and the single link change which produces the highest probability increase according to"
2012.iwslt-papers.12,J03-1002,0,0.0039137,"adapted our system by translating an in-domain corpora which was split beforehand. Each part of the corpora was translated using the translation model adapted at the previous step, i.e. updated with new extracted phrases. These phrases are the result of a word-to-word alignment combination we present afterward. 1.1. Similar work The most similar approach in the literature is proposed in [3] who present an incremental re-training algorithm to simulate a post-editing situation. It is proposed to extract new phrases from approximate alignments which were obtained by a modiﬁed version of Giza-pp [4]. An initial alignment with one-to-one links between the same sentence positions is created and then iteratively updated as long as improvements are observed. In practice, a greedy search algorithm is used to ﬁnd the locally optimal word alignment. All source positions carrying only one link are tried, and the single link change which produces the highest probability increase according to the Giza-pp model 4 is kept. The resulting alignment is improved with two simple post-processing steps. First, each unknown word in source side is aligned with the ﬁrst non-aligned unknown word on the target"
2012.iwslt-papers.12,N10-1062,0,0.0334028,"aligned using edit distance algorithm of TER; 3. Source-reference alignment: the alignment links are deduced from combination of alignments of both step 1 and 2. Phrase pairs are then extracted, scored and added to translation model which is ﬁnally re-trained. ment algorithm which is partially based on the edit-distance algorithm. As argued in [3], “to be practical, incremental retraining must be performed in less than one second”. For comparison, our entire alignment process takes few hundredths of second for 1500 sentences, in comparison to several seconds per sentences as reported in [3]. [5] present stream based incremental adaptation using an on-line version of the EM algorithm. This approach designed for large amounts of incoming data is not really adapted for the post-editing context. Like [3], we propose an incremental adaptation workﬂow that is more oriented to real time processing. As part of our experiments, we have compared our approach with the use of the freely available tool named IncGiza-pp,1 an incremental version of Giza-pp. It is precisely intended to inject new data into an SMT system without having to restart the entire word alignment procedure. To our knowledge,"
2012.iwslt-papers.12,P07-2045,0,0.0108708,"December 6th-7th, 2012 Figure 2: Example of a source-to-reference alignment using using the automatic translation as pivot. The alignment links between the source sentence and the translation are generated by the MT system. Those between the translation and its postedited version (i.e. the reference) are calculated by TER. Finally, the source-to-reference alignment links are deduced by an alignment combination based on both alignment sets computed before. 2.1.1. Translation: source to translation alignment The SMT system used to translate the source sentences is based on the Moses SMT toolkit [6]. Moses can provide the word-to-word alignments between the source sentence and the translation hypothesis. This aligning information represents the ﬁrst part of our alignment combination. This automatic translation is “compared” with the reference translation using an edit distance algorithm. 2.1.2. Analysis: edit distance alignment In this paper, we use the Translation Error Rate (TER) algorithm as proposed in [7]. TER is an extension of the Word Error Rate (WER) which is more suitable for machine translation since it can take into account word reorderings. TER uses the following edit types:"
2012.iwslt-papers.12,2006.amta-papers.25,0,0.0471864,"on both alignment sets computed before. 2.1.1. Translation: source to translation alignment The SMT system used to translate the source sentences is based on the Moses SMT toolkit [6]. Moses can provide the word-to-word alignments between the source sentence and the translation hypothesis. This aligning information represents the ﬁrst part of our alignment combination. This automatic translation is “compared” with the reference translation using an edit distance algorithm. 2.1.2. Analysis: edit distance alignment In this paper, we use the Translation Error Rate (TER) algorithm as proposed in [7]. TER is an extension of the Word Error Rate (WER) which is more suitable for machine translation since it can take into account word reorderings. TER uses the following edit types: insertion, deletion, substitution and shift. The TER is computed between the output of our SMT system and the corresponding reference translation, and the word-to-word alignments are inferred. We only keep the aligned and substituted edit types in order to extract what we consider as the most interesting phrase pairs. Indeed, we argue that what is aligned correspond to what our system knows, while what is substitut"
2012.iwslt-papers.12,W09-0441,0,0.0335301,"ath has shift then foreach shift do updateWordPosition(tgt, shift); end end foreach edit-type of edit-path do if edit-type is ‘align’ or ‘substitution’ then alignment(tgt-word, ref-word) = 1; end end foreach ref-word of ref do foreach tgt-word aligned to ref-word do if isAligned?(src-word, tgt-word) then alignment(src-word, ref-word) = 1; end end end Algorithm 1: Source-to-reference alignment algorithm at word level. Using both source-to-translation alignments and translation-to-reference edit-path, the source-toreference alignments path are build. Our approach can be extended to use TER-Plus [8], an extension of TER using paraphrases, stemming and synonyms in order to obtain better word-to-word alignments. 3. Experimental evaluation 2.1.3. Adaptation: source to reference alignment Considering the SMT translation hypothesis as a “pivot” for aligning both source and its reference sentence, we have designed the word-to-word alignment algorithm shown by Algorithm 1. It combines source-to-translation and translationto-reference alignments, and then deduces the source-toreference alignment path. From this path, the translation model is ﬁnally updated using the standard training phrase extr"
2012.iwslt-papers.12,lambert-etal-2012-automatic,1,0.833456,"rl and News Commentary with 50 million and 3 million words, respectively. They were used to train our SMT baseline systems. The third corpus, named “absINFO”, contains 500 thousand words randomly selected from abstracts of scientiﬁc papers in the domain of Computer Science. Information on the sub-domains is also available (networks, AI, data base, theoretical CS, . . .), but was not used in this study. The corpus if freely available to support research in domain adaptation and was already used by the 2012 JHU summer workshop on this topic. A detailed description of this corpus can be found in [9]. This in-domain corpus was split into three sub-corpora: • absINFO.corr.train is composed of 350k words and is used to simulate the user post-editing or corrective training. • absINFO.dev is a set of 75k words and used for development. • absINFO.test another set of 75k words used as a test corpus to monitor the performance of our adaptation workﬂow. Moreover, in order to better simulate a sequential postediting process, the absINFO.corr.train corpus was split into 10 sub-sets (about 1.5k sentences with 35k words each). This corresponds quite well to the update of an MT system after a post-cor"
2012.iwslt-papers.12,P03-1021,0,\N,Missing
2012.iwslt-papers.6,W10-2407,0,0.186067,"Missing"
2012.iwslt-papers.6,P06-1142,0,0.0732457,"Missing"
2012.iwslt-papers.6,W11-3208,0,0.139735,"Missing"
2012.iwslt-papers.6,D11-1128,0,0.150284,"Missing"
2012.iwslt-papers.6,P12-1049,0,0.111056,"Missing"
2012.iwslt-papers.6,W09-3504,0,0.198118,"Missing"
2012.iwslt-papers.6,P11-1044,0,0.116572,"Missing"
2012.iwslt-papers.6,N03-1033,0,0.0297077,"literation system on TPs and evaluate the system performance which is correlated with the transliteration mining quality.                        3.1. TM algorithm for parallel corpora The algorithm as shown in Figure 1 is designed to compare two aligned words and detect the words which are transliteration of each other, with respect to the observations in section 3.3. We developed the following TM algorithm: (1) First, the parallel corpus is tagged using a part-ofspeech (POS) tagger. We used Stanford POS tagger [11] for English and Mada/Tokan [12] for Arabic POS tagging. (2) Then, we align the tagged bitext using Giza++ [13], using the source/target alignment ﬁle, remove all aligned word pairs with POS tags other than noun (NN) or proper noun (PNN) tags and remove all English words starting with lower-case letters. Words which have most lowest align     Figure 2: Calculating the three levels of similarity scores As shown in Figure 2, we developed a three normalization functions which can be used to normalize the Arabic transliterated word and English word to be more comparable to each other"
2012.iwslt-papers.6,J03-1002,0,0.00569818,"g quality.                        3.1. TM algorithm for parallel corpora The algorithm as shown in Figure 1 is designed to compare two aligned words and detect the words which are transliteration of each other, with respect to the observations in section 3.3. We developed the following TM algorithm: (1) First, the parallel corpus is tagged using a part-ofspeech (POS) tagger. We used Stanford POS tagger [11] for English and Mada/Tokan [12] for Arabic POS tagging. (2) Then, we align the tagged bitext using Giza++ [13], using the source/target alignment ﬁle, remove all aligned word pairs with POS tags other than noun (NN) or proper noun (PNN) tags and remove all English words starting with lower-case letters. Words which have most lowest align     Figure 2: Calculating the three levels of similarity scores As shown in Figure 2, we developed a three normalization functions which can be used to normalize the Arabic transliterated word and English word to be more comparable to each other phonically. These normalized forms are used to 187 The 9th International Workshop on Spoken Language Translati"
2012.iwslt-papers.6,P07-2045,0,0.00388265,"2,3 T LSi = Levenshtein(N ormi (At ), N ormi (E)) |N ormi (E)| (1) In this formula, Levenshtein function is the edit distance between the two words, which is the number of singlecharacter edits required to change the ﬁrst word into the second one. 3.3. Customized English pronunciation similarity comparison for Arabic-English transliteration Our TM algorithm is based on the following pronunciation (and hence transliteration) observations in the English language considering the transliteration task from Arabic language characteristics: The transliteration system is built using the moses toolkit [14]. We train a letter-based SMT system on the list of TPs extracted using our TM algorithm explained in section 3.1. The distortion limit is set to 0 to disable any reordering. The transliteration system should be able to learn the proper letter mapping using the alignment of the letters, and hence be able to generate the possible transliterations of a name written in the source language script using the learned mapping rules into a name written in the target language script. This research focuses on the following points: • Evaluate the performance of TM the algorithm by using the TPs to build a"
2012.iwslt-papers.6,P05-1045,0,0.00376052,"didate in the candidate list produced by a transliteration system. • F-Score= Fuzziness in Top-1. The mean F-score measures how different, on average, the top transliteration candidate is from its closest reference. • MRR=Mean Reciprocal Rank measures traditional MRR for any right answer produced by the system, among the candidates. • M APref tightly measures the precision in the n-best candidates for the i-th source name, for which reference transliterations are available. (using only XIN, AFP and NYT parts) by extracting a list of proper names using the Stanford name entity recognizer (NER) [16]. The second resource (LM2) is the English part of the extracted TPs. The Table 1 below compares the results of using LM1 vs. LM2. These results show that the target part (i.e. LM2) of the extracted TPs gives better ACC score while it has some impact on the mean F-score. We decided to use LM2 in all other experiments that measure other variables. System LM1 LM2 ACC 0.43750 0.44159 Mean F-Score 0.88160 0.87860 MRR 0.54787 0.54862 M APref 0.43750 0.44160 Table 1: LM1 vs. LM2 3.5.4. Three levels similarity scores thresholds selections Several systems were trained to evaluate the best thresholds t"
2012.iwslt-papers.6,W02-0505,0,\N,Missing
2012.iwslt-papers.6,W12-4402,0,\N,Missing
2013.mtsummit-wptp.13,D11-1033,0,0.0228773,"o adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if"
2013.mtsummit-wptp.13,N09-2038,0,0.0242009,"the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in impr"
2013.mtsummit-wptp.13,W12-3155,1,0.851259,"ently homogeneous, the language is sufficiently complex, and there is sufficient multilingual data available to train and tune MT models. The paper is organized as follows. Section 2 lists some of the related works. Section 3 introduces methods used for project adaptation. Section 4 briefly describes the conduct of the field test. Section 5 and Section 6, respectively, introduce the set-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (e"
2013.mtsummit-wptp.13,2011.iwslt-evaluation.18,1,0.827144,"elated Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is pe"
2013.mtsummit-wptp.13,P11-2031,0,0.0146555,"Dntgt , n=0,1) or the concatenation of the source side of both the development and test set (D01src ); we name FGtgt and FGsrc the selected corpus and the models trained on it in the two cases. The table also provides the percentage of data selected, computed with respect to the target side. The optimal splitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Fed"
2013.mtsummit-wptp.13,2012.amta-papers.22,1,0.534373,"MT suggestions for the second half of the document came from a system adapted to the text of the first day by means of one of the adaptation methods tested in our experiments (Section 6). Translators post-edited machine-generated translations for correcting mistakes and making them stylistically appropriate. The document was selected such that the size of its halves corresponds approximately to the daily productivity of professional translators, that is three to five thousand words. A report on the field test including an analysis of the productivity of translators has already been published (Federico et al., 2012). Moreover, we performed a preliminary measure of the performance of MT outputs versus the post-edition of each translator. In both cases, pretty large inter-translator differences were observed. Since the limited number of subjects would have led to scores with large variances, we decided to choose segments en→de up, called backoff, in which the indicator feature is discarded. Again, the backoff method proposed by Niehues and Waibel (2012) differs slightly in the way the scores of the phrase pairs stemming from the background phrase table are computed. Language model: As concerns the LM adapt"
2013.mtsummit-wptp.13,P02-1023,0,0.0361356,"Missing"
2013.mtsummit-wptp.13,W07-0733,0,0.0352986,"t-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performan"
2013.mtsummit-wptp.13,P07-2045,1,0.00690725,"plitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model have been optimized by means of the Margin Infused Relaxed Algorithm (MIRA) process (Hasler et al., 2011) provided within the Moses toolkit. Various models have been built by means of the methods described in Section 3. Here the list o"
2013.mtsummit-wptp.13,D12-1037,0,0.0226802,"and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine transla"
2013.mtsummit-wptp.13,D09-1074,0,0.0233102,"Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram"
2013.mtsummit-wptp.13,P10-2041,0,0.0161319,"of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if the new data is enough relevant to the task at hand, a condition"
2013.mtsummit-wptp.13,W08-0320,0,0.0387515,"Missing"
2013.mtsummit-wptp.13,2012.amta-papers.19,0,0.127366,"rien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et a"
2013.mtsummit-wptp.13,W12-3147,1,0.900227,"Missing"
2013.mtsummit-wptp.13,steinberger-etal-2006-jrc,0,0.0170217,"Missing"
2013.mtsummit-wptp.13,I08-2088,0,0.0276276,"on tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistic"
2013.mtsummit-wptp.13,W07-0717,0,0.0387197,"a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the M"
2013.mtsummit-wptp.13,D10-1044,0,0.0217311,"ifferent approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is"
2014.iwslt-evaluation.14,rousseau-etal-2014-enhancing,1,0.817939,"ing (NLP) systems like the ones we are going to present here can often be enhanced using various methods, which can occur before, during or after the actual system processing. Among these, one of the most efficient pre-processing method is data selection, i.e. the fact to determine which data will be injected into the system we are going to build. For this campaign, many data selection processing was done, both in ASR and SLT tasks. 2.1. Selection for the ASR task 2.1.1. Acoustic models training data selection For our acoustic modeling we used as a primary source the TED-LIUM corpus release 2 [1], removing from it all talks recorded after December 31st, 2010. In order to strengthen this base, we first added data from the Euronews corpora [2] distributed by the campaign organizers and from the 1997 English Broadcast News Speech (HUB4) [3]. Then, from the MediaEval 2014 evaluation campaign Search and Hyperlinking Task data transcripts (BBC recordings from 2008 which were decoded by the LIUM) [4], we applied a threshold on our confidence measures to select the best possible segments for our system within a limit of 50 hours of speech. Table 1 summarizes the characteristics of the data in"
2014.iwslt-evaluation.14,gretter-2014-euronews,0,0.147438,"e actual system processing. Among these, one of the most efficient pre-processing method is data selection, i.e. the fact to determine which data will be injected into the system we are going to build. For this campaign, many data selection processing was done, both in ASR and SLT tasks. 2.1. Selection for the ASR task 2.1.1. Acoustic models training data selection For our acoustic modeling we used as a primary source the TED-LIUM corpus release 2 [1], removing from it all talks recorded after December 31st, 2010. In order to strengthen this base, we first added data from the Euronews corpora [2] distributed by the campaign organizers and from the 1997 English Broadcast News Speech (HUB4) [3]. Then, from the MediaEval 2014 evaluation campaign Search and Hyperlinking Task data transcripts (BBC recordings from 2008 which were decoded by the LIUM) [4], we applied a threshold on our confidence measures to select the best possible segments for our system within a limit of 50 hours of speech. Table 1 summarizes the characteristics of the data included in our ASR system acoustic models. Corpus TED-LIUM Euronews 1997 HUB4 MedialEval 14 Total Duration 130.1h 68.2h 75.0h 50.0h 323.3h Segments 6"
2014.iwslt-evaluation.14,W08-0509,0,0.0139487,"d. 4.1. Architecture of the LIUM SLT system The SMT system is based on the Moses toolkit [11]. The standard 14 feature functions were used (i.e phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, word and phrase penalty and target language model (LM) probability). In addition to these, an Operation Sequence Model (OSM) [12] have been trained and included in the system. 4.1.1. Translation model The translation models have been trained with the standard procedure. First, the bitexts are word aligned in both directions with GIZA++ [13]. Then the phrase pairs are extracted and the lexical and phrase probabilities are computed. The weights have been optimized with MERT using two versions of the development data. For some systems, the provided transcriptions were used, and for others, the outputs of our ASR system was used. This was performed for the sake of comparing the impact of ASR systems improvement (observed during the last few years). 4.1.2. Language modeling Table 3: Interpolation coefficients and perplexities for the bigram, trigram, quadrigram and CSLM language models used in the LIUM ASR system. The language model"
2014.iwslt-evaluation.14,P07-2045,0,0.0036987,"Missing"
2014.iwslt-evaluation.14,P11-1105,0,\N,Missing
2015.iwslt-papers.5,J93-2004,0,\N,Missing
2015.iwslt-papers.5,D11-1033,0,\N,Missing
2015.iwslt-papers.5,P02-1040,0,\N,Missing
2015.iwslt-papers.5,P06-2093,1,\N,Missing
2015.iwslt-papers.5,P10-2041,0,\N,Missing
2015.iwslt-papers.5,P07-2045,0,\N,Missing
2015.iwslt-papers.5,E12-1055,0,\N,Missing
2015.iwslt-papers.5,J03-1002,0,\N,Missing
2015.iwslt-papers.5,C12-2104,1,\N,Missing
2015.iwslt-papers.5,P14-1023,0,\N,Missing
2015.iwslt-papers.5,W11-2123,0,\N,Missing
2020.acl-main.653,P15-1039,0,0.0132327,"quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularly attractive, as it enables fairer comparison"
2020.acl-main.653,P19-1620,0,0.0244799,"rt cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been"
2020.acl-main.653,P19-1309,1,0.82722,"le in every target language, we use contexts containing an N -way parallel sentence. Our approach is similar to WikiMatrix (Schwenk et al., 2019) which extracts parallel sentences for many language pairs in Wikipedia, but we limit the search de es ar zh vi hi 5.4M 1.1M 83.7k 24.1K 9.2k 1340 Table 1: Incremental alignment with English to obtain 7-way aligned sentences. for parallel sentences to documents on the same topic only, and aim for N -way parallel sentences. To detect parallel sentences we use the LASER toolkit,3 which achieves state-of-the-art performance in mining parallel sentences (Artetxe and Schwenk, 2019). LASER uses multilingual sentence embeddings and a distance or margin criterion in the embeddings space to detect parallel sentences. The reader is referred to Artetxe and Schwenk (2018) and Artetxe and Schwenk (2019) for a detailed description. See Appendix A.6 for further details and statistics on the number of parallel sentences mined for all language pairs. We first independently align all languages with English, then intersect these sets of parallel sentences, forming sets of N-way parallel sentences. As shown in Table 1, starting with 5.4M parallel English/German sentences, the number o"
2020.acl-main.653,P17-1171,0,0.0249807,"fied Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure pro"
2020.acl-main.653,D18-1269,1,0.782924,"tly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularly attractive, as it enables fairer comparison across languages, requires fewe"
2020.acl-main.653,D19-1169,0,0.0185285,"l QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) st"
2020.acl-main.653,D19-1600,0,0.0156129,"l QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) st"
2020.acl-main.653,L18-1431,0,0.196684,"uture, and to estab8 https://en.wikipedia.org/wiki/ Wikipedia:Translation#Avoid_machine_ translations 7322 lish standard splits. However, in our experiments, we only make use of the English development data and study strict zero-shot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a hi"
2020.acl-main.653,N19-1423,0,0.592918,"valuate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent su"
2020.acl-main.653,P19-1610,0,0.0145569,"hot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a highly-parallel multilingual QA benchmark in seven languages. We developed several baselines on two cross-lingual understanding tasks on MLQA with state-of-the-art methods, and demonstrate significant room for improvement. We hope t"
2020.acl-main.653,L18-1440,0,0.0375486,"al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate extractive QA on a manuallytranslated set of 327 SQuAD instances in Japanese and French, and develop a phrase-alignment modelling technique, showing improvements over backtranslation. L"
2020.acl-main.653,R19-1053,0,0.0122478,"rove QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate extractive QA on a manuallytranslated set of 327 SQuAD instances in Japanese and French, and develop a phrase-alignment modelling technique, showing improvements over backtranslation. Like us, they build multi-way parallel extractive QA data, but MLQA has many"
2020.acl-main.653,W18-2605,0,0.0222377,"Related Work Monolingual QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset"
2020.acl-main.653,D19-1607,0,0.0123007,"n explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate"
2020.acl-main.653,D17-1215,0,0.0366045,"d study strict zero-shot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a highly-parallel multilingual QA benchmark in seven languages. We developed several baselines on two cross-lingual understanding tasks on MLQA with state-of-the-art methods, and demonstrate significant room for imp"
2020.acl-main.653,D19-1249,0,0.0291755,"nerate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et al. (2018) investigate extractive QA on a manuallytranslated set of 327 SQuAD instances in Japanese and French, and develop a phrase-alignment modelling technique, showing improvements over backtranslation. Like us, they build multi-way parallel extractive QA data, but MLQA has many more instances, covers more languages and does not require manual document translation. Liu et al. (2019a) explore cross-lingual open-domain QA"
2020.acl-main.653,P17-1147,0,0.0314601,"n other topics. Further statistics are given in Appendix A.2. en # Articles # Contexts # Instances de es ar zh vi hi 5530 2806 2762 2627 2673 2682 2255 10894 4509 5215 5085 4989 5246 4524 12738 5029 5753 5852 5641 6006 5425 Table 4: Number of Wikipedia articles with a context in MLQA. 3 Related Work Monolingual QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a"
2020.acl-main.653,C12-1089,0,0.0611895,"(Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly paral"
2020.acl-main.653,Q19-1026,0,0.0773017,"has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA wi"
2020.acl-main.653,P18-2124,0,0.0852957,"Missing"
2020.acl-main.653,D19-1283,0,0.0157402,"s a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al. (2018) release a parallel QA dataset in English and Hindi, Hardalov et al. (2019) investigate QA transfer from English to Bulgarian, Liu et al. (2019b) release a cloze QA dataset in Chinese and English, and Jing et al. (2019) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages. Asai et"
2020.acl-main.653,L18-1437,0,0.0315326,", large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks (Cimiano et al., 2013; Unger et al., 2015), with more recent work from Zimina et al. (2018). Lee et al. (2018) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset. Kumar et al. (2019) study question generation by leveraging English questions to generate better Hindi questions, and Lee and Lee (2019) and Cui et al. (2019a) develop modelling approaches to improve performance on Chinese QA tasks using English resources. Lee et al. (2019) and Hsu et al. (2019) explore modelling approaches for zero-shot transfer and Singh et al. (2019) explore how training with cross-lingual data regularizes QA models. Cross-lingual QA Data Gupta et al"
2020.acl-main.653,D16-1264,0,0.568652,"i, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we"
2020.acl-main.653,D13-1020,0,0.0318853,"man, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are significantly behind training-language performance. 1 Introduction Question answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA (Richardson, 2013; Rajpurkar et al., 2016; Chen et al., 2017; Kwiatkowski et al., 2019). The field has made rapid advances in recent years, even exceeding human performance in some settings (Devlin et al., 2019; Alberti et al., 2019). Despite such popularity, QA datasets in languages other than English remain scarce, even for relatively high-resource languages (Asai et al., 2018), as collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization"
2020.acl-main.653,2021.eacl-main.115,1,0.683099,"Missing"
2020.acl-main.653,P19-1484,1,0.835066,"void_machine_ translations 7322 lish standard splits. However, in our experiments, we only make use of the English development data and study strict zero-shot settings. Other evaluation setups could be envisioned, e.g. by exploiting the target language development sets for hyperparameter optimisation or fine-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC (Cui et al., 2018), or using unsupervised QA techniques to assist transfer (Lewis et al., 2019). Finally, a large body of work suggests QA models are over-reliant on word-matching between question and context (Jia and Liang, 2017; Gan and Ng, 2019). G-XLT represents an interesting testbed, as simple symbolic matching is less straightforward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nuanced and robust than it may initially appear. 7 Conclusion We have introduced MLQA, a highly-parallel multilingual QA benchmark in seven languages. We developed seve"
2020.acl-main.653,L18-1560,1,0.854934,"collecting such datasets at sufficient scale and quality is difficult and costly. There 1 MLQA is publicly available at https://github. com/facebookresearch/mlqa are two reasons why this lack of data prevents internationalization of QA systems. First, we cannot measure progress on multilingual QA without relevant benchmark data. Second, we cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularl"
2020.acl-main.653,P19-1227,0,0.135144,"cannot easily train end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classification (Lewis et al., 2004; Klementiev et al., 2012; Schwenk and Li, 2018), semantic role labelling (Akbik et al., 2015) and NLI (Conneau et al., 2018), we argue that while multilingual QA training data might be useful but not strictly necessary, multilingual evaluation data is a must-have. Recognising this need, several cross-lingual datasets have recently been assembled (Asai et al., 2018; Liu et al., 2019a). However, these generally cover only a small number of languages, combine data from different authors and annotation protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section 3). Highly parallel data is particularly attractive, as it enables fairer comparison across languages, requires fewer source language annotations, and allows for additional evaluation setups at no extra annotation cost. A purpose-built evaluation benchmark dataset covering a range of diverse languages, and following the popular extractive QA paradigm on a practically-useful"
2020.acl-main.653,W17-2623,0,0.0341294,"stics are given in Appendix A.2. en # Articles # Contexts # Instances de es ar zh vi hi 5530 2806 2762 2627 2673 2682 2255 10894 4509 5215 5085 4989 5246 4524 12738 5029 5753 5852 5641 6006 5425 Table 4: Number of Wikipedia articles with a context in MLQA. 3 Related Work Monolingual QA Data There is a great variety of English QA data, popularized by MCTest (Richardson, 2013), CNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2016), and WikiQA (Yang et al., 2015) amongst others. Large span-based datasets such as SQuAD (Rajpurkar et al., 2016, 2018), TriviaQA (Joshi et al., 2017), NewsQA (Trischler et al., 2017), and Natural Questions (Kwiatkowski et al., 2019) have seen extractive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader (He et al., 2018), CMRC (Cui et al., 2019b) and DRCD (Shao et al., 2018). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean (Lim et al., 2019) and Arabic (Mozannar et al., 2019). Cross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QAL"
2020.acl-main.653,D15-1237,0,0.0968098,"Missing"
2021.acl-long.507,W06-2810,0,0.191067,"Missing"
2021.acl-long.507,C18-1116,0,0.0165659,"ning approach that we take. Data used to Mine Many previous methods for data mining focused on Wikipedia. Otero and L´opez (2010) and Patry and Langlais (2011), for instance, aligned entire parallel documents. For example, Adafre and de Rijke (2006) and Mohammadi and GhasemAghaee (2010) used machine translation systems to compare Dutch and Persian Wikipedias to English, to identify aligned sentences. Various other worked used similarities in mentioned entities to align text, such as Gottschalk and Demidova (2017) and Tsai and Roth (2016). Work such as Smith et al. (2010); Tufis et al. (2013); Aghaebrahimian (2018) used Wikipedia to mine parallel sentences, but focused on fewer languages, often high resource. In contrast, our system mines not in Wikipedia but in CommonCrawl, a much larger source of data — and is applied to a much larger quantity of languages. Work has extended mining beyond Wikipedia. For example, ParaCrawl1 has been heavily used (e.g. in WMT), which is based on several noisy multilingual crawls (Koehn et al., 2018, 2019). ElKishky et al. (2019) focused on mining documents in Common Crawl rather than sentences. Our work continues this line of scalable mining on the web, but pushes to la"
2021.acl-long.507,W17-2508,0,0.0156832,"0 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Much previous work has explored the automatic creation of parallel data from monolingual resources. In this section, we detail various approaches and illustrate the differences of our algorithmic approach and the scale of our mining. Mining Methodology At the start, various approaches used alignment on information beyond text itself, such as with document metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´ıkov´a et al., 2020). Previous work such as Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019)"
2021.acl-long.507,W16-2347,0,0.019861,"ion of parallel data from monolingual resources. In this section, we detail various approaches and illustrate the differences of our algorithmic approach and the scale of our mining. Mining Methodology At the start, various approaches used alignment on information beyond text itself, such as with document metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´ıkov´a et al., 2020). Previous work such as Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019) used bilingual embeddings, which is not scalable for mining many different languages. Compared to work such as Schwenk (2018), we dras"
2021.acl-long.507,2020.emnlp-main.480,0,0.031241,"d to work such as Schwenk (2018), we drastically increase the scale of our mining and produce two orders of magnitude more data — this is possible by the increased efficiency and scalability of our improved mining methods. A few mining approaches were applied to large quantities of language pairs. For example, the ParaCrawl project1 mined data for all European languages. Bitextor (Espl`a-Gomis and Forcada, 2010) was applied to many languages, but took an approach that required identifying parallel documents first and then extracting aligned sentences. This is similar to the ccAligned project (El-Kishky et al., 2020). In contrast to these, we mine much larger quantities of parallel data due to the global margin-based mining approach that we take. Data used to Mine Many previous methods for data mining focused on Wikipedia. Otero and L´opez (2010) and Patry and Langlais (2011), for instance, aligned entire parallel documents. For example, Adafre and de Rijke (2006) and Mohammadi and GhasemAghaee (2010) used machine translation systems to compare Dutch and Persian Wikipedias to English, to identify aligned sentences. Various other worked used similarities in mentioned entities to align text, such as Gottsch"
2021.acl-long.507,P16-1189,0,0.0162903,"guage Processing, pages 6490–6500 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Much previous work has explored the automatic creation of parallel data from monolingual resources. In this section, we detail various approaches and illustrate the differences of our algorithmic approach and the scale of our mining. Mining Methodology At the start, various approaches used alignment on information beyond text itself, such as with document metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´ıkov´a et al., 2020). Previous work such as Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (201"
2021.acl-long.507,L18-1550,1,0.895347,"Missing"
2021.acl-long.507,W18-6317,0,0.0693151,"Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´ıkov´a et al., 2020). Previous work such as Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019) used bilingual embeddings, which is not scalable for mining many different languages. Compared to work such as Schwenk (2018), we drastically increase the scale of our mining and produce two orders of magnitude more data — this is possible by the increased efficiency and scalability of our improved mining methods. A few mining approaches were applied to large quantities of language pairs. For example, the ParaCrawl project1 mined data for all European languages. Bitextor (Espl`a-Gomis and Forcada, 2010) was applied to many languages, but took an approach that required iden"
2021.acl-long.507,2005.mtsummit-papers.11,0,0.207815,"g data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available. 1 Introduction Parallel data, i.e. sentences in two languages which are mutual translations, are a crucial resource for many multilingual natural language processing tasks. Traditionally, high quality parallel texts are obtained from the publications of international organizations like the the United Nations (Ziemski et al., 2016) or the European Parliament (Koehn, 2005). These are professional human translations, but they are in a more formal language and tend to be limited to political topics. Another direction • development of a new highly efficient and parallelized processing pipeline to confront the substantial computational challenge; • unprecedented size: 10.8 billion mined parallel sentences in 90 different languages; • all these resources are freely available; • we demonstrate the quality of our mined data on a variety of machine translation benchmarks, such as TED, WMT, and WAT, achieving highly competitive results. 6490 Proceedings of the 59th Annu"
2021.acl-long.507,W19-5404,0,0.031178,"Missing"
2021.acl-long.507,W18-6453,0,0.0169673,"d similarities in mentioned entities to align text, such as Gottschalk and Demidova (2017) and Tsai and Roth (2016). Work such as Smith et al. (2010); Tufis et al. (2013); Aghaebrahimian (2018) used Wikipedia to mine parallel sentences, but focused on fewer languages, often high resource. In contrast, our system mines not in Wikipedia but in CommonCrawl, a much larger source of data — and is applied to a much larger quantity of languages. Work has extended mining beyond Wikipedia. For example, ParaCrawl1 has been heavily used (e.g. in WMT), which is based on several noisy multilingual crawls (Koehn et al., 2018, 2019). ElKishky et al. (2019) focused on mining documents in Common Crawl rather than sentences. Our work continues this line of scalable mining on the web, but pushes to large-scale mining to produce billions of aligned sentences. 3 Distance-based mining approach We leverage massively multilingual sentence embeddings and a margin-based criterion to mine parallel sentences. The core idea is to learn a multilingual sentence embedding, or an embedding space in which semantically similar sentences are close, independent of the language they are written in. This means that distance in the embedd"
2021.acl-long.507,D18-2012,0,0.0150163,".7 26.1 17.9 18.4 18.3 26.4 22.1 20.1 24.7 25.6 13.1 16.9 20.7 23.6 19.6 19.9 25.5 20.5 26.8 20.8 20.8 18.8 zh 16.7 18.0 17.5 17.9 19.0 18.7 24.7 20.5 17.5 15.2 13.6 17.1 14.7 17.3 18.2 19.3 13.9 16.6 16.0 17.3 15.6 17.2 17.3 18.0 16.7 15.7 16.2 - Table 2: BLEU scores on the TED test set. NMT systems were trained on bitexts mined in CCMatrix only. involving Chinese, Japanese and Korean as it creates artifacts. We consider 29 different languages, resulting in 778 NMT systems to train. We apply the same preprocessing and training procedure for all language pairs. We train a SentencePiece Model (Kudo and Richardson, 2018) with a vocabulary of size 50k. The bitext were not filtered to remove sentences which may appear in the TED dev or test sets. Also, we did not try to optimize the architecture of the NMT models to to size of the bitexts for each language pair. Instead, for all the pairs, we use the same architecture, a Transformer model with six layers for both the encoder and decoder. We use a dimension of 512 and 4096 for the feed-forward. We train each model for 50 epochs with an initial learning rate of 0.001. We keep the model with the best BLEU on the TED validation set. In Table 2, we report tokenized"
2021.acl-long.507,L16-1147,0,0.0619198,"Missing"
2021.acl-long.507,I11-1062,0,0.0146938,"s the wrong language label. As noise in this stage will affect our mining process, we perform strict filtering using two LID systems on each sentence, fastText (Grave et al., 4 https://pypi.org/project/ sentence-splitter/ 6492 text1FR embed1FR … … textNFR embedNFR D1 Idx1 Idx1 … DN … IdxN indexDE D1 IdxM margin based mining DM embed1DE text1DE … … embedMDE textMDE indexFR Figure 2: Parallelized processing flow to mine parallel sentences. Left: forward distances; Right: backward distances. Middle: both distances are combined according to Equation 3.1 and the extracted bitext. 2018) and LangID (Lui and Baldwin, 2011), and discard the data if the two disagree or have low confidence. This processing yields a corpus of Ni unique sentences for each language Li . These texts are the basis for index creation and mining (see column size in Table 1). Index creation. We follow Schwenk et al. (2019) and use the highly optimized FAISS library (Johnson et al., 2017)5 to create compact indices of the sentence embedding. LASER’s sentence representations are 1024-dimensional, which means that the embeddings of all sentences would require 71 · 109 × 1024 × 4 ≈ 290 TB to store. To practically handle this scale, we use an"
2021.acl-long.507,J05-4003,0,0.205106,"evious work has explored the automatic creation of parallel data from monolingual resources. In this section, we detail various approaches and illustrate the differences of our algorithmic approach and the scale of our mining. Mining Methodology At the start, various approaches used alignment on information beyond text itself, such as with document metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´ıkov´a et al., 2020). Previous work such as Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019) used bilingual embeddings, which is not scalable for mining many different languages. Compared"
2021.acl-long.507,W19-5333,1,0.841641,"rently have no available bitext training data. In particular CCMatrix bitexts have been used to train a massively multilingual NMT systems for 100×100 languages (Fan et al., 2020). 5.2 WMT’19 Evaluation Next, we focus on arguably the most competitive translation benchmark, the WMT news translation task, to compare our mined data to the best existing systems. We only consider the high resource directions, as they constitute the largest challenge — existing systems perform strongly, and previous work incorporating mined data from Paracrawl (Ott et al., 2018) only found marginal gains. We follow Ng et al. (2019) and trained systems on en-de, en-ru, en-zh, and de-fr. We used the Transformer Big architecture with FFN size 8192, 6496 System Single systems de-en en-de en-ru ru-en zh-en en-zh de-fr fr-de NT’18 WMT bitext NT’18 CCMatrix 46.2 49.9 45.9 50.3 33.5 35.7 33.4 36.9 25.8 30.2 39.2 40.8 - - NT’19 WMT bitext NT’19 CCMatrix 41.0 43.3 40.4 44.5 31.4 35.5 38.1 41.8 34.8 35.6 37.9 33.5 NT’20 WMT bitext NT’20 CCMatrix 40.3 39.2 31.9 35.1 24.0 25.5 35.5 37.1 35.0 38.8 33.8 33.8 42.8 44.9 36.3 40.2 39.9 44.6 37.3 35.0 Ensembles + BT NT’19 best + Reranking Table 3: BLEU scores on the Newstest’18, Newstest’"
2021.acl-long.507,W11-1212,0,0.0291918,"large quantities of language pairs. For example, the ParaCrawl project1 mined data for all European languages. Bitextor (Espl`a-Gomis and Forcada, 2010) was applied to many languages, but took an approach that required identifying parallel documents first and then extracting aligned sentences. This is similar to the ccAligned project (El-Kishky et al., 2020). In contrast to these, we mine much larger quantities of parallel data due to the global margin-based mining approach that we take. Data used to Mine Many previous methods for data mining focused on Wikipedia. Otero and L´opez (2010) and Patry and Langlais (2011), for instance, aligned entire parallel documents. For example, Adafre and de Rijke (2006) and Mohammadi and GhasemAghaee (2010) used machine translation systems to compare Dutch and Persian Wikipedias to English, to identify aligned sentences. Various other worked used similarities in mentioned entities to align text, such as Gottschalk and Demidova (2017) and Tsai and Roth (2016). Work such as Smith et al. (2010); Tufis et al. (2013); Aghaebrahimian (2018) used Wikipedia to mine parallel sentences, but focused on fewer languages, often high resource. In contrast, our system mines not in Wiki"
2021.acl-long.507,N18-2084,0,0.0134138,"in as the test sets. Nevertheless, we show on the many to many TED corpus that our mined data produces high quality translation systems, even through distant language pairs not aligned through English and low resource languages. Finally, we demonstrate that models trained on CCMatrix can surpass state of the art systems in WMT’19 and WAT’20. 5.1 TED Evaluation We examine the quality of our mined bitext across a diverse set of languages, focusing on performance of bitext pairs not aligned through English. Following Gottschalk and Demidova (2017), we evaluate on the test sets of the TED corpus (Qi et al., 2018), which contains parallel TED talk transcripts in 58 languages. This corpus is tokenized, so we detokenize using Moses, with the exception of pairs 6494 6495 2157 786 263 778 9 Catalan Spanish French Galician Italian Portuguese Romanian Belarusian Bulgarian Czech Croatian Polish Russian Slovak Slovenian Serbian Ukrainian Greek Estonian Finnish Hungarian Lithuanian Latvian Basque Albanian Turkish Arabic Hebrew Farsi ca es fr gl it pt ro be bg cs hr pl ru sk sl sr uk el et fi hu lt lv eu sq tr ar he fa sw Swahili de 14.5 9.8 12.8 18.6 0.3 1.3 3.4 8.7 - - en is 40.9 19.4 50.1 71.4 nl 2.3 16.6 68."
2021.acl-long.507,P99-1068,0,0.474375,"l Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6490–6500 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Much previous work has explored the automatic creation of parallel data from monolingual resources. In this section, we detail various approaches and illustrate the differences of our algorithmic approach and the scale of our mining. Mining Methodology At the start, various approaches used alignment on information beyond text itself, such as with document metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and"
2021.acl-long.507,J03-3002,0,0.39764,"he Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6490–6500 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Much previous work has explored the automatic creation of parallel data from monolingual resources. In this section, we detail various approaches and illustrate the differences of our algorithmic approach and the scale of our mining. Mining Methodology At the start, various approaches used alignment on information beyond text itself, such as with document metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´"
2021.acl-long.507,P18-2037,1,0.914759,"ment metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´ıkov´a et al., 2020). Previous work such as Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019) used bilingual embeddings, which is not scalable for mining many different languages. Compared to work such as Schwenk (2018), we drastically increase the scale of our mining and produce two orders of magnitude more data — this is possible by the increased efficiency and scalability of our improved mining methods. A few mining approaches were applied to large quantities of language pairs. For example, the ParaCrawl project1"
2021.acl-long.507,P16-1162,0,0.024611,"Missing"
2021.acl-long.507,tiedemann-2012-parallel,0,0.067915,"Missing"
2021.acl-long.507,N16-1072,0,0.017136,"ine much larger quantities of parallel data due to the global margin-based mining approach that we take. Data used to Mine Many previous methods for data mining focused on Wikipedia. Otero and L´opez (2010) and Patry and Langlais (2011), for instance, aligned entire parallel documents. For example, Adafre and de Rijke (2006) and Mohammadi and GhasemAghaee (2010) used machine translation systems to compare Dutch and Persian Wikipedias to English, to identify aligned sentences. Various other worked used similarities in mentioned entities to align text, such as Gottschalk and Demidova (2017) and Tsai and Roth (2016). Work such as Smith et al. (2010); Tufis et al. (2013); Aghaebrahimian (2018) used Wikipedia to mine parallel sentences, but focused on fewer languages, often high resource. In contrast, our system mines not in Wikipedia but in CommonCrawl, a much larger source of data — and is applied to a much larger quantity of languages. Work has extended mining beyond Wikipedia. For example, ParaCrawl1 has been heavily used (e.g. in WMT), which is based on several noisy multilingual crawls (Koehn et al., 2018, 2019). ElKishky et al. (2019) focused on mining documents in Common Crawl rather than sentences"
2021.acl-long.507,R13-1091,0,0.0502844,"Missing"
2021.acl-long.507,P03-1010,0,0.204204,"tics 2 Related work Much previous work has explored the automatic creation of parallel data from monolingual resources. In this section, we detail various approaches and illustrate the differences of our algorithmic approach and the scale of our mining. Mining Methodology At the start, various approaches used alignment on information beyond text itself, such as with document metadata (Resnik, 1999; Resnik and Smith, 2003). Later, work aligned based on text with techniques such as Jaccard similarity (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018), crosslingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), language models (Buck and Koehn, 2016), translation (AbdulRauf and Schwenk, 2009; Bouamor and Sajjad, 2018), or bag-of-words (Buck and Koehn, 2016). In contrast, we use massively multilingual sentence embeddings trained on almost 100 languages, and then conduct margin-based mining in the multilingual embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a,b; Kvapil´ıkov´a et al., 2020). Previous work such as Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019) used bilingual embeddings, which is not scalable for mining many di"
2021.acl-long.507,2020.lrec-1.494,1,0.827688,"Missing"
2021.acl-long.507,L16-1561,0,0.0309883,"est WMT’19 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available. 1 Introduction Parallel data, i.e. sentences in two languages which are mutual translations, are a crucial resource for many multilingual natural language processing tasks. Traditionally, high quality parallel texts are obtained from the publications of international organizations like the the United Nations (Ziemski et al., 2016) or the European Parliament (Koehn, 2005). These are professional human translations, but they are in a more formal language and tend to be limited to political topics. Another direction • development of a new highly efficient and parallelized processing pipeline to confront the substantial computational challenge; • unprecedented size: 10.8 billion mined parallel sentences in 90 different languages; • all these resources are freely available; • we demonstrate the quality of our mined data on a variety of machine translation benchmarks, such as TED, WMT, and WAT, achieving highly competitive r"
2021.acl-long.507,N10-1063,0,0.0448695,"lel data due to the global margin-based mining approach that we take. Data used to Mine Many previous methods for data mining focused on Wikipedia. Otero and L´opez (2010) and Patry and Langlais (2011), for instance, aligned entire parallel documents. For example, Adafre and de Rijke (2006) and Mohammadi and GhasemAghaee (2010) used machine translation systems to compare Dutch and Persian Wikipedias to English, to identify aligned sentences. Various other worked used similarities in mentioned entities to align text, such as Gottschalk and Demidova (2017) and Tsai and Roth (2016). Work such as Smith et al. (2010); Tufis et al. (2013); Aghaebrahimian (2018) used Wikipedia to mine parallel sentences, but focused on fewer languages, often high resource. In contrast, our system mines not in Wikipedia but in CommonCrawl, a much larger source of data — and is applied to a much larger quantity of languages. Work has extended mining beyond Wikipedia. For example, ParaCrawl1 has been heavily used (e.g. in WMT), which is based on several noisy multilingual crawls (Koehn et al., 2018, 2019). ElKishky et al. (2019) focused on mining documents in Common Crawl rather than sentences. Our work continues this line of"
2021.eacl-main.115,E09-1003,1,0.702368,"1. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 201"
2021.eacl-main.115,W06-2810,0,0.238082,"Missing"
2021.eacl-main.115,C18-1116,0,0.0169201,"of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by training cross-lingual embeddings on twelve languages. Gottschalk and Demidova (2017) searched for parallel text passages in Wikipedia by comparing their named entities and time expressions. Finally, Aghaebrahimian (2018) propose an approach based on bilingual BiLSTM sentence encoders to mine German, French and Persian parallel texts with English. Parallel data consisting of aligned Wikipedia titles have been extracted for twenty-three languages.5 We are not aware of other attempts to systematically mine for parallel sentences in the textual content of Wikipedia for a large number of languages. 4 http://www.statmt.org/cc-aligned/ https://linguatools.org/tools/ corpora/wikipedia-parallel-titles-corpora/ 1352 5 3 Distance-based mining approach 3.2 The underlying idea of the mining approach used in this work is t"
2021.eacl-main.115,W17-2508,0,0.0153074,"ouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research described in Espa˜na-Bon"
2021.eacl-main.115,W16-2347,0,0.021176,"arable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high"
2021.eacl-main.115,W19-5435,1,0.903702,"Missing"
2021.eacl-main.115,2020.emnlp-main.480,1,0.867371,"n Bouamor and Sajjad (2018) or Gr´egoire and Langlais (2017). However, in those works, mining is not solely based on multilingual sentence embeddings, but they are part of a larger system. To the best of our knowledge, this work is the first one that applies the same mining approach 3 http://www.paracrawl.eu/ to all combinations of many different languages, written in more than twenty different scripts. In follow up work, the same underlying mining approach was applied to a huge collection of Common Crawl texts (Schwenk et al., 2019). Hierarchical mining in Common Crawl texts was performed by El-Kishky et al. (2020).4 Wikipedia is arguably the largest comparable corpus. One of the first attempts to exploit this resource was performed by Adafre and de Rijke (2006). An MT system was used to translate Dutch sentences into English and to compare them with the English texts, yielding several hundreds of Dutch/English bitexts. Later, a similar technique was applied to Persian/English (Mohammadi and GhasemAghaee, 2010). Structural information in Wikipedia such as the topic categories of documents was used in the alignment of multilingual corpora (Otero and L´opez, 2010). In another work, the mining approach of"
2021.eacl-main.115,P16-1189,0,0.0161657,"(Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research d"
2021.eacl-main.115,W17-2509,0,0.0426724,"Missing"
2021.eacl-main.115,W18-6317,0,0.289421,"wls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research described in Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019). However, in all these works, only bilingual sentence representations have been trained. Such an approach does not scale to many languages, in particular when considering all possible language pairs in Wikipedia. Finally, related ideas have been also proposed in Bouamor and Sajjad (2018) or Gr´egoire and Langlais (2017). However, in those works, mining is not solely based on multilingual sentence embeddings, but they are part of a larger system. To the best of our knowledge, this work is the first one that applies the same mining approach 3 http://www.paracrawl.eu/ to all"
2021.eacl-main.115,2005.mtsummit-papers.11,0,0.769279,"ugh English. Introduction Most of the current approaches in Natural Language Processing are data-driven. The size of the resources used for training is often the primary concern, but the quality and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into anoth"
2021.eacl-main.115,W19-5404,1,0.921522,"Missing"
2021.eacl-main.115,W18-6453,0,0.0202635,"icipants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research described in Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019). However, in all these works, only bilingual sentence representations have been trained. Such an approa"
2021.eacl-main.115,D18-2012,0,0.128227,"Missing"
2021.eacl-main.115,L16-1147,0,0.0527975,"ty and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into English. Eventually, the translated articles have been later independently edited and are not parallel anymore. Wikipedia strongly discourages the use of unedited m"
2021.eacl-main.115,J05-4003,0,0.536353,"tational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Ja"
2021.eacl-main.115,N19-4009,0,0.23688,"it provides an N -way parallel test sets for English, French, German and Czech. We favoured the translation between two morphologically rich languages from different families and considered the following language pairs: German/English, German/French, Czech/German and Czech/French. The size of the mined bitexts is in the range of 100k to more than 2M (see Table 2 and Figure 1). We did not try to optimize the architecture of the NMT system to the size of the bitexts and used the same architecture for all systems: the encoder and decoder are 5-layer transformer models as implemented in fairseq (Ott et al., 2019). The goal of this study is not to develop the best performing NMT system for the considered languages pairs, but to compare different mining parameters. The evolution of the BLEU score in function of the margin threshold is given in Figure 1. Decreasing the threshold naturally leads to more mined data – we observe an exponential increase of the data size. The performance of the NMT systems trained on the mined data seems to change as expected: the BLEU score first improves with increasing amounts of available training data, reaches a maximum and than decreases since the additional data gets m"
2021.eacl-main.115,W11-1212,0,0.062814,"ral hundreds of Dutch/English bitexts. Later, a similar technique was applied to Persian/English (Mohammadi and GhasemAghaee, 2010). Structural information in Wikipedia such as the topic categories of documents was used in the alignment of multilingual corpora (Otero and L´opez, 2010). In another work, the mining approach of Munteanu and Marcu (2005) was applied to extract large corpora from Wikipedia in sixteen languages (Smith et al., 2010). Otero et al. (2011) measured the comparability of Wikipedia corpora by the translation equivalents on three languages Portuguese, Spanish, and English. Patry and Langlais (2011) came up with a set of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by training cross-lingual embeddings on twelve languages. Gottschalk and Demidova (2017) searched for parallel text passages in Wikipedia by comparing their named entities and time exp"
2021.eacl-main.115,W18-6319,0,0.0173349,"sed in (Qi et al., 2018). NMT systems were trained on bitexts mined in Wikipedia only (with at least twenty-five thousand parallel sentences). No other resources were used. parameter settings shown in Figure 2 in the appendix. Since the TED development and test sets were already tokenized, we first detokenize them using Moses. We trained NMT systems for all possible language pairs with more than 25k mined sentences. This gives us in total 1886 language pairs in 45 languages. We train L1 → L2 and L2 → L1 with the same mined bitexts L1 /L2 . Scores on the test sets were computed with SacreBLEU (Post, 2018), see Table 4. Some additional results are reported in Table 6 in the annex. 23 NMT systems achieve BLEU scores over 30, the best one being 37.3 for Brazilian Portuguese to English. Several results are worth mentioning, like Farsi/English: 16.7, Hebrew/English: 25.7, Indonesian/English: 24.9 or English/Hindi: 25.7 We also achieve interesting results for translation between various non English language pairs for which it is usually not easy to find parallel data, e.g. Norwegian ↔ Danish ≈33, Norwegian ↔ Swedish ≈25, Indonesian ↔ Vietnamese ≈16 or Japanese / Korean ≈17. Our results on the TED se"
2021.eacl-main.115,P99-1068,0,0.431341,"guages. The paper concludes with a discussion of future research directions. 2 https://en.wikipedia.org/wiki/ Wikipedia:Translation 1351 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical"
2021.eacl-main.115,J03-3002,0,0.346661,"per concludes with a discussion of future research directions. 2 https://en.wikipedia.org/wiki/ Wikipedia:Translation 1351 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilitie"
2021.eacl-main.115,P18-2037,1,0.827644,"roach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 1620 different language pairs, out of which only 34M are aligned with English. This corpus is freely available.1 1 hgong6 @illinois.edu In this work, we build on a recent approach to mine parallel texts based on a distance measure in a joint multilingual sentence embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a), and a freely available encoder for 93 languages. We approach the computational challenge to mine in almost six hundred million sentences by using fast indexing and similarity search algorithms. The paper is organized as follows. In the next section, we first discuss related work. We then summarize the underlying mining approach. Section 4 describes in detail how we applied this approach to extract parallel sentences from Wikipedia in 1620 language pairs. In section 5, we assess the quality of the extracted bitexts by training NMT systems for a subset of language"
2021.eacl-main.115,N10-1063,0,0.0736285,"source was performed by Adafre and de Rijke (2006). An MT system was used to translate Dutch sentences into English and to compare them with the English texts, yielding several hundreds of Dutch/English bitexts. Later, a similar technique was applied to Persian/English (Mohammadi and GhasemAghaee, 2010). Structural information in Wikipedia such as the topic categories of documents was used in the alignment of multilingual corpora (Otero and L´opez, 2010). In another work, the mining approach of Munteanu and Marcu (2005) was applied to extract large corpora from Wikipedia in sixteen languages (Smith et al., 2010). Otero et al. (2011) measured the comparability of Wikipedia corpora by the translation equivalents on three languages Portuguese, Spanish, and English. Patry and Langlais (2011) came up with a set of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by tr"
2021.eacl-main.115,tiedemann-2012-parallel,0,0.144494,"he primary concern, but the quality and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into English. Eventually, the translated articles have been later independently edited and are not parallel anymore. Wikipedia"
2021.eacl-main.115,N16-1072,0,0.0291658,"extract large corpora from Wikipedia in sixteen languages (Smith et al., 2010). Otero et al. (2011) measured the comparability of Wikipedia corpora by the translation equivalents on three languages Portuguese, Spanish, and English. Patry and Langlais (2011) came up with a set of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by training cross-lingual embeddings on twelve languages. Gottschalk and Demidova (2017) searched for parallel text passages in Wikipedia by comparing their named entities and time expressions. Finally, Aghaebrahimian (2018) propose an approach based on bilingual BiLSTM sentence encoders to mine German, French and Persian parallel texts with English. Parallel data consisting of aligned Wikipedia titles have been extracted for twenty-three languages.5 We are not aware of other attempts to systematically mine for parallel sentences in the text"
2021.eacl-main.115,R13-1091,0,0.0522314,"Missing"
2021.eacl-main.115,P03-1010,0,0.248949,"f the Association for Computational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translatio"
2021.eacl-main.115,L16-1561,0,0.0366476,"e current approaches in Natural Language Processing are data-driven. The size of the resources used for training is often the primary concern, but the quality and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into Eng"
2021.eacl-main.115,N18-2084,0,0.438535,"y important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into English. Eventually, the translated articles have been later independently edited and are not parallel anymore. Wikipedia strongly discourages the use of unedited machine translation,2 but the existen"
2021.iwslt-1.14,2020.lrec-1.520,0,0.0396711,"scale dataset for multilingual speech translation, CoVoST contains translations from 11 languages to English. We use its data in 5 language directions 2 . • EuroParl (Iranzo-S´anchez et al., 2020). Collected from debates in European Parliment, EuroParl provides speech-to-text translations in 6 European languages. Its data in 11 language directions 3 is used in model training. it-en 10.3 35.5 12.8 2.3 Mined data We also mined additional speech-to-text data from unlabeled corpora. The audio corpora used in our experiments include Common Voice and Multilingual LibriSpeech (MLS). • Common Voice (Ardila et al., 2020). It is a massive collection of multilingual audios and their transcriptions in 29 languages. • MLS (Pratap et al., 2020). It is a speech corpus collected from audiobooks of LibriVox in 8 languages. The text corpus used for mining is CCNet, which serves as the source of target translations (Wenzek et al., 2020). Collected from snapshots of CommonCrawl dataset, CCNet provides a large-scale and high-quality monolingual datasets. Since the audio corpora provide transcripts for audios, we could align source audios with target translations by finding the alignments between 2 {es, fr, it, pt, ru}-en"
2021.iwslt-1.14,P19-1309,1,0.803343,"et, which serves as the source of target translations (Wenzek et al., 2020). Collected from snapshots of CommonCrawl dataset, CCNet provides a large-scale and high-quality monolingual datasets. Since the audio corpora provide transcripts for audios, we could align source audios with target translations by finding the alignments between 2 {es, fr, it, pt, ru}-en es-{en, fr, it, pt}, fr-{en, es, pt}, it-{en, es}, pt-{en, es}, ru-en 3 fr-es 85.5 20.0 18.7 42.7 pt-es 9.5 4.4 1.3 it-es 20.6 6.6 3.4 source transcripts and target texts. LASER alignment is applied for the crosslingual text alignment (Artetxe and Schwenk, 2019). It generates sentence embeddings with a pre-trained multilingual text encoder (Schwenk and Douze, 2017), and use them to measure the semantic similarity between sentences. Table 1 summarizes the statistics of the data used in our experiments. It reports the total length of audios in TEDx, CoVost and EuroParl datasets. Moreover, we include the statistics of mined speech from Common Voice and MLS. The mined data has an equivalent size to TEDx dataset in training directions. It also provides a good amount of speech data in zero-shot directions including it-en, pt-es and it-es. 2.3 2.2 pt-en 134"
2021.iwslt-1.14,2020.acl-main.747,1,0.805531,"Missing"
2021.iwslt-1.14,D18-2012,0,0.0253432,"taset. Due to time limits, we don’t use extra parallel text data to enhance the performance. 3.3 Speech only Finetuning In the last stage, the model is fine-tuned in the speech-to-text translation task with speech input only. The text encoder is dropped and no text input data is used. 4 4.1 Experiments Experimental Setting Both wav2vec 2.0 model and mBART model are trained with the large configuration. There are 24 transformer layers in the wav2vec 2.0 model, and 12 transformer layers in both mBART encoder and decoder. We build the mBART model with a target vocabulary of 64,000 SentencePiece (Kudo and Richardson, 2018) tokens, which are shared among all 6 evaluation languages7 . For the mBART model with IPA phoneme input, the vocabulary size is 516 which includes phoneme variants with “ ” attached to denote the word leading phoneme. A language id symbol “hLIDi” is used as the initial token to predict the sentence. Speech recognition task is treated as the same as the speech translation task but with the source speech language id symbol. The primary system results submitted are from an ensemble system with three models. All three models are trained with 3-stage optimization discussed in section 3 with differ"
2021.iwslt-1.14,2020.tacl-1.47,1,0.783007,"odules pretrained from self-supervised learning, multitask joint training, and task-specific fine-tuning. The pretrained modules make use of a large amount of unlabeled data, joint training focuses on transferring knowledge from a relatively simple text-to-text task to a speech-to-text task, and the model is finetuned on speech-to-text translation task to boost in-task performance. 3.1 Modality Dependent Pretraining Our model leverages large amounts of unlabelled data from different modalities through two pretrained models: a wav2vec 2.0 (Baevski et al., 2020) and a multilingual BART (mBART) (Liu et al., 2020). wav2vec 2.0 is a simple and powerful framework to learn high quality speech representation from unlabelled audio data. Given the raw input audio samples, the model learns both latent representations and context representations through a contrastive task to distinguish true latent from distractors. Two multilingual wav2vec 2.0 models are explored during our development. One (“XLSR-53”) is trained on 56K-hour speech in 53 languages (Conneau et al., 2020a), and another (“VP-100K”) is trained on 100K-hour speech in 23 languages (Wang et al., 2021). The pretrained wav2vec 2.0 models are used to i"
2021.iwslt-1.14,W17-2619,1,0.830452,"mmonCrawl dataset, CCNet provides a large-scale and high-quality monolingual datasets. Since the audio corpora provide transcripts for audios, we could align source audios with target translations by finding the alignments between 2 {es, fr, it, pt, ru}-en es-{en, fr, it, pt}, fr-{en, es, pt}, it-{en, es}, pt-{en, es}, ru-en 3 fr-es 85.5 20.0 18.7 42.7 pt-es 9.5 4.4 1.3 it-es 20.6 6.6 3.4 source transcripts and target texts. LASER alignment is applied for the crosslingual text alignment (Artetxe and Schwenk, 2019). It generates sentence embeddings with a pre-trained multilingual text encoder (Schwenk and Douze, 2017), and use them to measure the semantic similarity between sentences. Table 1 summarizes the statistics of the data used in our experiments. It reports the total length of audios in TEDx, CoVost and EuroParl datasets. Moreover, we include the statistics of mined speech from Common Voice and MLS. The mined data has an equivalent size to TEDx dataset in training directions. It also provides a good amount of speech data in zero-shot directions including it-en, pt-es and it-es. 2.3 2.2 pt-en 134.2 44.1 14.6 9.6 - Text Data We use additional text data to train mBART model, which later is used to ini"
2021.iwslt-1.14,2021.acl-long.328,1,0.845412,"ons. We ensemble models with XLSR-53 encoder and VP-100K encoder respectively to achieve the best performance. mBART is a sequence-to-sequence generative pretraining scheme, specifically a denoising autoencoder (DAE) to predict the original text from its noisy version such as random span masking and order permutation (Liu et al., 2020). The model is pretrained with monolingual data and finetuned with parallel data as described in subsection 2.3. The encoder and decoder in mBART model are used to initialize the encoder and decoder in the joint trained model of the second stage. Previous study (Tang et al., 2021b) shows that it makes the knowledge transfer from the text-to-text task to speech-to-text task easier by representing the input text as its pronunciation form, i.e., the phoneme sequence. We also investigate representing the input text as its pronunciation forms rather than sentencepiece tokens during our development. We choose International Phonetic Alphabet (IPA) as input text representation since it can be shared across different languages. espeak6 is used to convert the text word into IPA phonemes. 3.2 Multitask Joint Training In the second stage, we choose to optimize the speech-to-text"
2021.iwslt-1.14,tiedemann-2012-parallel,0,0.0340439,"of speech data in zero-shot directions including it-en, pt-es and it-es. 2.3 2.2 pt-en 134.2 44.1 14.6 9.6 - Text Data We use additional text data to train mBART model, which later is used to initialize our speech-to-text model. mBART model is first trained with monolingual text data from five languages4 using selfsupervised training. Then they are finetuned with parallel text data from seven evaluation directions as a multilingual text-to-text translation model. The monolingual text data comes from the CC100 dataset (Conneau et al., 2020b) and the parallel text data are downloaded from OPUS (Tiedemann, 2012). 5 3 Methods Our evaluation system is based on an encoder decoder model with the state-of-the-art Transformer architecture. The submitted model is developed 4 Five languages include en,es,fr,it and pt. The following datasets are used: CommonCrawl, OPUSBooks v1, CAPES v1, DGT v2019, ECB v1, ELRA-W0138 v1, ELRA-W0201 v1, ELRC 2682 v1, EMEA v3, EUbookshop v2, EuroPat v1, Europarl v8, GlobalVoices v2018q4, JRC-Acquis v3.0, JW300 v1b, Multi ParaCrawl v7.1, MultiUN v1, News-Commentary v14, QED v2.0a, SciELO v1, TED2013 v1.1, TED2020 v1, Tanzil v1, Tatoeba v2020-1109, TildeMODEL v2018,UNPC v1.0, and"
2021.iwslt-1.14,2020.lrec-1.517,1,0.784888,"the 18th International Conference on Spoken Language Translation, pages 131–137 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics Table 1: Audio Length in Hours of TEDx, CoVoST, EuroParl and Mined Data TEDx CoVoST EuroParl Common Voice (mined data) MLS (mined data) 2.1 es-en 163.7 113.0 20.7 52.7 23.9 fr-en 119.9 264.1 31.0 39.6 64.7 Public data Besides TEDx dataset provided by the shared task, we also include two other public datasets, CoVoST and EuroParl, which provides parallel audio-text samples in some of the test directions of TEDx. • CoVoST (Wang et al., 2020). As a large scale dataset for multilingual speech translation, CoVoST contains translations from 11 languages to English. We use its data in 5 language directions 2 . • EuroParl (Iranzo-S´anchez et al., 2020). Collected from debates in European Parliment, EuroParl provides speech-to-text translations in 6 European languages. Its data in 11 language directions 3 is used in model training. it-en 10.3 35.5 12.8 2.3 Mined data We also mined additional speech-to-text data from unlabeled corpora. The audio corpora used in our experiments include Common Voice and Multilingual LibriSpeech (MLS). • Co"
2021.iwslt-1.14,2021.acl-long.80,1,0.837127,"Missing"
2021.iwslt-1.14,2020.lrec-1.494,0,0.0479893,"Missing"
C12-2104,D07-1090,0,0.00934324,"vement, but we have changed only one score of the phrase-table. Exactly the same approach can be used to estimate the inverse phrase translation probability P(¯s|¯t). 1076 3.3 Integration into the decoder As far as we know, there is only one attempt to integrate the CSLM directly into the translation process (Zamora-Martínez et al., 2010). This is in fact tricky since many LM probabilities are requested and it is not straight forward to delay a bunch of requests so that we can use the CSLM more efficiently. A possible implementation could be based on the work on distributed LMs, for instance (Brants et al., 2007). Previous works on continuous space translation models in an bilingual tuple system only used rescoring (Schwenk et al., 2007; Le et al., 2012). On the other hand, it seems to be easier to integrate the approach proposed in this paper directly into the decoder. When translating a sentence, Moses first enumerates all the possible segmentations of the source sentence, given the known phrases in the phrase-table. Once all those probabilities are obtained, the translation model is not queried any more. This process largely simplifies the use of continuous space methods. Two options come to mind:"
C12-2104,W06-1607,0,0.00910996,"to a sentence in the target language t. Then, the fundamental equation of SMT is: t∗ = arg max P(t|s) = arg max P(s|t)P(t)/P(s) = arg max P(s|t)P(t) t t t (1) The translation model P(s|t) is estimated from bitexts and the language model P(t) from monolingual data. A popular approach are phrase-based models which translate short sequences of words together (Koehn et al., 2003; Och and Ney, 2003). The translation probabilities of these phrase pairs are usually estimated by simple relative frequency. We are only aware of few works to perform more sophisticated smoothing techniques, for instance (Foster et al., 2006). The log-linear approach is commonly used to consider more feature functions (Och, 2003). In the Moses system, four feature functions are usually used for the translation model: the forward and backward phrase translation probabilities and lexical probabilities in both directions. These four feature functions together could be seen as a particular smoothing technique of the translation model. In other works, hundreds or thousands of features are used. The dominant approach in language modeling are so-called back-off n-gram models. An alternative approach was proposed by (Bengio and Ducharme,"
C12-2104,N03-1017,0,0.0603446,"ING 2012, Mumbai, December 2012. 1071 1 Introduction In the statistical approach to machine translation (SMT), all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is: t∗ = arg max P(t|s) = arg max P(s|t)P(t)/P(s) = arg max P(s|t)P(t) t t t (1) The translation model P(s|t) is estimated from bitexts and the language model P(t) from monolingual data. A popular approach are phrase-based models which translate short sequences of words together (Koehn et al., 2003; Och and Ney, 2003). The translation probabilities of these phrase pairs are usually estimated by simple relative frequency. We are only aware of few works to perform more sophisticated smoothing techniques, for instance (Foster et al., 2006). The log-linear approach is commonly used to consider more feature functions (Och, 2003). In the Moses system, four feature functions are usually used for the translation model: the forward and backward phrase translation probabilities and lexical probabilities in both directions. These four feature functions together could be seen as a particular smooth"
C12-2104,D10-1076,0,0.0113528,"undreds or thousands of features are used. The dominant approach in language modeling are so-called back-off n-gram models. An alternative approach was proposed by (Bengio and Ducharme, 2001; Bengio et al., 2003). The basic idea is to project the words into a continuous space and to perform the probability estimation in that space. The projection as well as the estimation can be jointly performed by a multi-layer neural network. The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Martínez et al., 2010; Schwenk et al., 2012). Given this success for language modeling, there were also two attempts to apply the same ideas to the translation model. Both were developed for tuple-based translation systems, e.g. based on bilingual units. This allows to to see the translations model like a standard n-gram LM task and it is straight forward to apply the CSLM (Schwenk et al., 2007). In the second work, this idea was improved by considering different factorization of the joint probability, in particular word-based ones: the principal idea is to predict the probability of"
C12-2104,N12-1005,0,0.709049,"for language modeling, there were also two attempts to apply the same ideas to the translation model. Both were developed for tuple-based translation systems, e.g. based on bilingual units. This allows to to see the translations model like a standard n-gram LM task and it is straight forward to apply the CSLM (Schwenk et al., 2007). In the second work, this idea was improved by considering different factorization of the joint probability, in particular word-based ones: the principal idea is to predict the probability of a target word given the context of the previous source and target words (Le et al., 2012). The authors report good improvements in the BLEU scores for several tasks, but the approach seems to be complicated, in particular with respect to the training of the model or direct integration into the decoder. In this work we propose a generic architecture which can be used in the standard pipeline to build a phrase-based SMT system. The continuous space translation model (CSTM) is trained on exactly the same data, i.e. the so-called extract files and no additional word alignments, segmentation, etc is necessary. In machine learning, we are generally not interested in memorizing perfectly"
C12-2104,P03-1021,0,0.0728361,"s) = arg max P(s|t)P(t)/P(s) = arg max P(s|t)P(t) t t t (1) The translation model P(s|t) is estimated from bitexts and the language model P(t) from monolingual data. A popular approach are phrase-based models which translate short sequences of words together (Koehn et al., 2003; Och and Ney, 2003). The translation probabilities of these phrase pairs are usually estimated by simple relative frequency. We are only aware of few works to perform more sophisticated smoothing techniques, for instance (Foster et al., 2006). The log-linear approach is commonly used to consider more feature functions (Och, 2003). In the Moses system, four feature functions are usually used for the translation model: the forward and backward phrase translation probabilities and lexical probabilities in both directions. These four feature functions together could be seen as a particular smoothing technique of the translation model. In other works, hundreds or thousands of features are used. The dominant approach in language modeling are so-called back-off n-gram models. An alternative approach was proposed by (Bengio and Ducharme, 2001; Bengio et al., 2003). The basic idea is to project the words into a continuous spac"
C12-2104,J03-1002,0,0.0111158,"cember 2012. 1071 1 Introduction In the statistical approach to machine translation (SMT), all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is: t∗ = arg max P(t|s) = arg max P(s|t)P(t)/P(s) = arg max P(s|t)P(t) t t t (1) The translation model P(s|t) is estimated from bitexts and the language model P(t) from monolingual data. A popular approach are phrase-based models which translate short sequences of words together (Koehn et al., 2003; Och and Ney, 2003). The translation probabilities of these phrase pairs are usually estimated by simple relative frequency. We are only aware of few works to perform more sophisticated smoothing techniques, for instance (Foster et al., 2006). The log-linear approach is commonly used to consider more feature functions (Och, 2003). In the Moses system, four feature functions are usually used for the translation model: the forward and backward phrase translation probabilities and lexical probabilities in both directions. These four feature functions together could be seen as a particular smoothing technique of the"
C12-2104,2011.iwslt-evaluation.10,1,0.447421,"Missing"
C12-2104,D07-1045,1,0.945261,"y a multi-layer neural network. The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Martínez et al., 2010; Schwenk et al., 2012). Given this success for language modeling, there were also two attempts to apply the same ideas to the translation model. Both were developed for tuple-based translation systems, e.g. based on bilingual units. This allows to to see the translations model like a standard n-gram LM task and it is straight forward to apply the CSLM (Schwenk et al., 2007). In the second work, this idea was improved by considering different factorization of the joint probability, in particular word-based ones: the principal idea is to predict the probability of a target word given the context of the previous source and target words (Le et al., 2012). The authors report good improvements in the BLEU scores for several tasks, but the approach seems to be complicated, in particular with respect to the training of the model or direct integration into the decoder. In this work we propose a generic architecture which can be used in the standard pipeline to build a ph"
C12-2104,P06-2093,1,0.802479,"del. In other works, hundreds or thousands of features are used. The dominant approach in language modeling are so-called back-off n-gram models. An alternative approach was proposed by (Bengio and Ducharme, 2001; Bengio et al., 2003). The basic idea is to project the words into a continuous space and to perform the probability estimation in that space. The projection as well as the estimation can be jointly performed by a multi-layer neural network. The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Martínez et al., 2010; Schwenk et al., 2012). Given this success for language modeling, there were also two attempts to apply the same ideas to the translation model. Both were developed for tuple-based translation systems, e.g. based on bilingual units. This allows to to see the translations model like a standard n-gram LM task and it is straight forward to apply the CSLM (Schwenk et al., 2007). In the second work, this idea was improved by considering different factorization of the joint probability, in particular word-based ones: the principal idea is to predict th"
C12-2104,W12-2702,1,0.903838,"dominant approach in language modeling are so-called back-off n-gram models. An alternative approach was proposed by (Bengio and Ducharme, 2001; Bengio et al., 2003). The basic idea is to project the words into a continuous space and to perform the probability estimation in that space. The projection as well as the estimation can be jointly performed by a multi-layer neural network. The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Martínez et al., 2010; Schwenk et al., 2012). Given this success for language modeling, there were also two attempts to apply the same ideas to the translation model. Both were developed for tuple-based translation systems, e.g. based on bilingual units. This allows to to see the translations model like a standard n-gram LM task and it is straight forward to apply the CSLM (Schwenk et al., 2007). In the second work, this idea was improved by considering different factorization of the joint probability, in particular word-based ones: the principal idea is to predict the probability of a target word given the context of the previous sourc"
C12-2104,2010.iwslt-evaluation.4,1,0.867419,"nds of features are used. The dominant approach in language modeling are so-called back-off n-gram models. An alternative approach was proposed by (Bengio and Ducharme, 2001; Bengio et al., 2003). The basic idea is to project the words into a continuous space and to perform the probability estimation in that space. The projection as well as the estimation can be jointly performed by a multi-layer neural network. The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Martínez et al., 2010; Schwenk et al., 2012). Given this success for language modeling, there were also two attempts to apply the same ideas to the translation model. Both were developed for tuple-based translation systems, e.g. based on bilingual units. This allows to to see the translations model like a standard n-gram LM task and it is straight forward to apply the CSLM (Schwenk et al., 2007). In the second work, this idea was improved by considering different factorization of the joint probability, in particular word-based ones: the principal idea is to predict the probability of a target word given the contex"
C14-2028,2013.mtsummit-papers.5,1,0.880457,"Missing"
C14-2028,2013.mtsummit-wptp.13,1,0.900099,"Missing"
C14-2028,2012.amta-papers.22,1,0.865102,"Missing"
C14-2028,2013.mtsummit-wptp.10,0,0.135035,"Missing"
C14-2028,W13-2231,1,0.678754,"Missing"
C14-2028,P14-1067,1,0.806908,"Missing"
C14-2028,2013.mtsummit-wptp.7,1,\N,Missing
D07-1045,N06-2001,0,0.0137965,"or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing is less important when large amounts of data are available. We plan to investigate whether this also holds for smoothing of the probabilities in phrase- or tuplebased statistical machine translation systems. 6 Acknowledgments This work has been partially funded by the European Union under the integrated project T C -S TAR (IST2002-FP6-506738), b"
D07-1045,N03-2002,0,0.0217703,"of probabilities in N-gram- or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing is less important when large amounts of data are available. We plan to investigate whether this also holds for smoothing of the probabilities in phrase- or tuplebased statistical machine translation systems. 6 Acknowledgments This work has been partially funded by the European Union under the integrated project"
D07-1045,W06-1607,0,0.0566714,"or instance found in (Chen and Goodman, 1999). Language models and phrase tables have in common that the probabilities of rare events may be overestimated. However, in language modeling probability mass must be redistributed in order to account for the unseen n-grams. Generalization to unseen events is less important in phrase-based SMT systems since the system searches only for the best segmentation and the best matching phrase pair among the existing ones. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation sy"
D07-1045,2003.mtsummit-tttt.3,0,0.0660512,"ware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrases is introduced. On the other hand, some restrictions on the seg"
D07-1045,P07-2045,0,0.00408354,"ion task (over 40 BLEU percentage). Using the continuous space model for the translation and target language model, an improvement of 2.5 BLEU on the development data and 1.5 BLEU on the test data was observed. Despite these encouraging results, we believe that additional research on improved estimation of probabilities in N-gram- or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing i"
D07-1045,P02-1038,0,0.0206542,") is to produce a target sentence e from a source sentence f . Among all possible target language sentences the one with the highest probability is chosen: e∗ = arg max Pr(e|f ) = arg max Pr(f |e) Pr(e) e X = arg max{exp( e where Pr(f |e) is the translation model and Pr(e) is the target language model. This approach is usually referred to as the noisy source-channel approach in statistical machine translation (Brown et al., 1993). λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase translation probabilities P (˜ e|˜f ) and P (˜f |˜ e) are usually obtained using relative frequency estimates. Statistical learning theory, however, tells us that relative frequency estimates have several drawbacks, in particular high variance and low bias. Phrase tables may contain several millions of entries, most of which appear only once or twice, which means that we are confronted with a data sparseness problem. Surprisingly, there seems to be little work addressing the issue of smoothing of the phrase table probabilities. On the other hand, smoothing of relative frequency es"
D07-1045,W99-0604,0,0.0167882,"Missing"
D07-1045,N04-1021,0,0.0338653,"est matching phrase pair among the existing ones. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrase"
D07-1045,2006.iwslt-papers.2,1,0.869281,"Missing"
D07-1045,takezawa-etal-2002-toward,0,0.016304,"den layer (200 to 500). Therefore, in previous applications of the continuous space n-gram model, the output was limited to the s most frequent units, s ranging between 2k and 12k (Schwenk, 2007). This is called a short-list. Train (bitexts) Dev Eval Sents 20k 489 500 Words 155.4/166.3k 5.2k 6k 4 Experimental Evaluation In this work we report results on the Basic Traveling Expression Corpus (B TEC) as used in the 2006 evaluations of the international workshop on spoken language translation (I WSLT). This corpus consists of typical sentences from phrase books for tourists in several languages (Takezawa et al., 2002). We report results on the supplied development corpus of 489 sentences and the official test set of the I WSLT’06 evaluation. The main measure is the BLEU score, using seven reference translations. The scoring is case insensitive and punctuations are ignored. Details on the available data are summarized in Table 1. We concentrated first on the translation from Italian to English. All participants in the I WSLT evaluation achieved much better performances for this language pair than for the other considered translation directions. This makes it more difficult to achieve additional improvements"
D07-1045,N04-1033,0,0.0469216,"t performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrases is introduced. On the other hand, some restrictions on the segmentation of the sour"
D07-1045,2005.mtsummit-papers.36,1,\N,Missing
D14-1179,D13-1106,0,0.10598,"Missing"
D14-1179,D11-1033,0,0.125416,"Missing"
D14-1179,P14-1129,0,0.106494,"Missing"
D14-1179,2006.iwslt-papers.2,1,0.141042,"Missing"
D14-1179,D13-1176,0,0.236308,"Missing"
D14-1179,C12-2104,1,0.137075,"Missing"
D14-1179,N03-1017,0,0.145005,"Missing"
D14-1179,2005.mtsummit-papers.11,0,0.132804,"Missing"
D14-1179,W02-1018,0,0.124674,"Missing"
D14-1179,P10-2041,0,0.126969,"Missing"
D14-1179,N12-1005,0,0.143701,"Missing"
D14-1179,D13-1140,0,0.122241,"Missing"
D14-1179,D13-1141,0,0.0877611,"Missing"
D17-1070,S14-2010,0,0.0512224,"ion x and the image y to the same embedding space. We use a margin α = 0.2 and 30 contrastive terms. We use the same splits as in (Karpathy and Fei-Fei, 2015), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test. For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and STS14 - Semantic Textual Similarity While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 (Agirre et al., 2014). This dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with 3 https://www.github.com/ facebookresearch/SentEval 674 name SNLI task NLI N 560k SICK-E NLI 10k SICK-R STS 10k STS14 STS 4.5k premise ”Two women are embracing while holding to go packages.” A man is typing on a machine used for stenography ”A man is singing a song and playing the guitar” ”Liquid ammonia leak kills 15 in Shanghai” hypothesis ”Two woman are holding packages.” label entailment The man isn’t operating a"
D17-1070,D15-1075,0,0.924908,"vel understanding task that involves reasoning about the semantic relationships within sentences. Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks. Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes. Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings comMany modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently ou"
D17-1070,N13-1092,0,0.0224448,"Missing"
D17-1070,N16-1162,0,0.72628,"Missing"
D17-1070,marelli-etal-2014-sick,0,0.201032,"arch/InferSent 670 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014). These models obtained significantly lower results compared to the unsupervised Skip-Thought approach. Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015). The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017). To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders. As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data. pared to all existing alternat"
D17-1070,D13-1090,0,0.0314371,"8.7 88.2 68.4/76.8 72.7/80.9 75.1/82.3 76.2/83.1 0.849 0.863 0.885 0.884 83.1 83.1 86.3 86.3 .46/.42 .67/.70 .43/.42 .71/ .55/.54 .68/.65 .70/.67 92.4 - 80.4/85.9 - 0.868 84.5 - - Table 4: Transfer test results for various architectures trained in different ways. Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way. † indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods. regard to the embedding size. 5.2 Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demons"
D17-1070,D16-1046,0,0.0556965,"ds in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014). These models obtained significantly lower results compared to the unsupervised Skip-Thought approach. Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015). The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017). To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders. As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data. pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while bei"
D17-1070,D14-1162,0,0.115818,"erence datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1 . 1 Holger Schwenk Facebook AI Research schwenk@fb.com Introduction Distributed representations of words (or word embeddings) (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown to provide useful features for various tasks in natural language processing and computer vision. While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence. That is, how to capture the 1 https://www.github.com/ facebookresearch/InferSent 670 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine tran"
D17-1070,S14-2055,0,0.0154055,"3.1 86.3 86.3 .46/.42 .67/.70 .43/.42 .71/ .55/.54 .68/.65 .70/.67 92.4 - 80.4/85.9 - 0.868 84.5 - - Table 4: Transfer test results for various architectures trained in different ways. Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way. † indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods. regard to the embedding size. 5.2 Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows. We hyp"
D17-1070,D16-1157,0,0.0885981,"information or semantics of the input data by specializing too much on these biases. Learning models on large unsupervised task makes it harder for the model to specialize. Littwin and Wolf (2016) showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder. They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect. Recent work on generating sentence embeddings range from models that compose word embeddings (Le and Mikolov, 2014; Arora et al., 2017; Wieting et al., 2016b) to more complex neural network architectures. SkipThought vectors (Kiros et al., 2015) propose an objective function that adapts the skip-gram model for words (Mikolov et al., 2013) to the sentence level. By encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks. They further obtained better results using layer-norm regularization of their model in (Ba et al., 2016). Hill et al. (2016) showed that the task on which sentence embeddings are trained significantly impacts their quality."
D17-1070,P15-1150,0,\N,Missing
D17-1070,W14-4012,0,\N,Missing
D17-1070,L18-1269,1,\N,Missing
D18-1269,L18-1548,0,0.03349,"dation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 2012), the OpenSubtitles 2016 and 2018 corpora (Tiedemann, 2012) and LDC2010T21, LDC2010T23 LDC corpora, and obtained a total of 64k parallel sentences. For Swahili, we were 2481 en fr es de el bg ru tr ar vi th z"
D18-1269,P17-1042,0,0.0297003,"in language understanding has been at the word level. Several approaches have been proposed to learn cross-lingual word representations, i.e., word representations where translations are close in the embedding space. Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al., 2017; Conneau et al., 2018). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as continuous bag-of-words (CBOW). Although naïve, this method often provides a strong baseline. More sophisticated approaches—such as the unsupervised SkipThought model of Kiros et al. (2015) that extends the skip-gram mo"
D18-1269,D18-1549,1,0.811153,"ings. 4.2.1 Aligning Word Embeddings Multilingual word embeddings are an efficient way to transfer knowledge from one language to another. For instance, Zhang et al. (2016) show that cross-lingual embeddings can be used to extend an English part-of-speech tagger to the cross-lingual setting, and Xiao and Guo (2014) achieve similar results in dependency parsing. Cross-lingual embeddings also provide an efficient mechanism to bootstrap neural machine translation (NMT) systems for low-resource language pairs, which is critical in the case of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018). In that case, the use cross-lingual embeddings directly helps the alignment of sentence-level encoders. Cross-lingual embeddings can be generated efficiently using a very small amount of supervision. By using a small parallel dictionary with n = 5000 word pairs, it is possible to learn a linear mapping to minimize W ? = argmin kW X − Y kF = U V T , W ∈Od (R) where d is the dimension of the embeddings, and X and Y are two matrices of shape (d, n) that correspond to the aligned word embeddings that appear in the parallel dictionary, Od (R) is the group of orthogonal matrices of dimension d, an"
D18-1269,D15-1075,1,0.919597,"ross-lingual document classification (Klementiev et al., 2012; Schwenk and Li, 2018), there are very few, if any, XLU benchmarks for more difficult language understanding tasks like natural language inference. Large-scale natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examp"
D18-1269,D18-2029,0,0.0977534,"ding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examples in NLI three-way classification format in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu, making a total of 112,500 annotated pairs. These languages span several language families, and with the inclusion of Swahili and Urdu, include two lower-resource languages as well. Because of its"
D18-1269,L18-1269,1,0.779888,"ng data (X - BILSTM). The former evaluates transfer learning while the latter evaluates NLIspecific encoders trained on in-domain data. Both approaches use the same alignment loss for aligning sentence embedding spaces from multiple languages which is present below. We consider two ways of extracting feature vectors from the BiLSTM: either using the initial and final hidden states (Sutskever et al., 2014), or using the element-wise max over all states (Collobert and Weston, 2008). The first approach is commonly used as a strong baseline for monolingual sentence embeddings (Arora et al., 2017; Conneau and Kiela, 2018; Gouews et al., 2014). Concretely, we consider the English fastText word embedding space as being fixed, and fine-tune embeddings in other languages so that the average of the word vectors in a sentence is close to the average of the word vectors in its English translation. The second approach consists in learning an English sentence encoder on the MultiNLI training data along with an encoder on the target language, with the objective that the representations of two translations are nearby in the embedding space. In both approaches, an English encoder is fixed, and we train target language en"
D18-1269,D17-1070,1,0.920978,"ractical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examples in NLI three-way classification format in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu, making a total of 112,500 annotated pairs. These languages span several language families, and with the inclusion of Swahili and Urdu, include two lowe"
D18-1269,E14-1049,0,0.060128,"ur baselines in Section 4, and finally present and discuss results in Section 5. 2 Related Work Multilingual Word Embeddings Much of the work on multilinguality in language understanding has been at the word level. Several approaches have been proposed to learn cross-lingual word representations, i.e., word representations where translations are close in the embedding space. Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al., 2017; Conneau et al., 2018). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as continuous bag-of-words (CBOW). Although naïve, this method"
D18-1269,P11-1134,0,0.0297861,"e are many ways to aggregate sentence embeddings, the comparison between different sentence embeddings is difficult. Moreover, the distribution of classes in the Reuters corpus is highly unbalanced, and the dataset does not provide a development set in the target language, further complicating experimental comparisons. In addition to the Reuters corpus, Cer et al. (2017) propose sentence-level multilingual training and evaluation datasets for semantic textual similarity in four languages. There have also been efforts to build multilingual RTE datasets, either through translating English data (Mehdad et al., 2011), or annotating sentences from a parallel corpora (Negri et al., 2011). More recently, Agic and Schluter (2017) provide a corpus, that is very complementary to our work, of human translations for 1332 pairs of the SNLI data into Arabic, French, Russian, and Spanish. Among all these benchmarks, XNLI is the first large-scale corpus for evaluating sentence-level representations on that many languages. In practice, cross-lingual sentence understanding goes beyond translation. For instance, Mohammad et al. (2016) analyze the differences in human sentiment annotations of Arabic sentences and their E"
D18-1269,L18-1550,0,0.0260448,"r mapping to minimize W ? = argmin kW X − Y kF = U V T , W ∈Od (R) where d is the dimension of the embeddings, and X and Y are two matrices of shape (d, n) that correspond to the aligned word embeddings that appear in the parallel dictionary, Od (R) is the group of orthogonal matrices of dimension d, and U and V are obtained from the singular value decomposition (SVD) of Y X T : U ΣV T = SVD(Y X T ). Xing et al. (2015) show that enforcing the orthogonality constraint on the linear mapping leads to better results on the word translation task. In this paper, we use common-crawl word embeddings (Grave et al., 2018) aligned with the MUSE library of Conneau et al. (2018). 4.2.2 Universal Multilingual Sentence Embeddings Most of the successful recent approaches for learning universal sentence representations have relied on English (Kiros et al., 2015; Arora et al., 2017; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). While notable recent approaches have considered building a shared sentence encoder for multiple languages using publicly available parallel corpora (Johnson et al., 2016; Schwenk et al., 2017; España-Bonet et al., 2017), the lack of a large-scale, sentence-level semantic ev"
D18-1269,N18-2017,1,0.86915,"Missing"
D18-1269,P14-1006,0,0.0217936,"the model are used as contextualized word vectors (Peters et al., 2018), or when the full model is finetuned on transfer tasks (Radford et al.; Howard and Ruder, 2018). Multilingual Sentence Representations There has been some effort on developing multilingual sentence embeddings. For example, Chandar et al. (2013) train bilingual autoencoders with the objective of minimizing reconstruction error between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a compositional vector model involving unigrams and bigrams to learn document level representations. Pham et al. (2015) directly train embedding representations for sentences with no attempt at compositionality. Zhou et al. (2016) learn bilingual document representations by minimizing the Euclidean distance between document representations and their translations. Cross-lingual Evaluation Benchmarks The lack of evaluation benchmark has hindered the development of such multilingual representations. Most previous approaches use the Reuters crosslingual document classification corpus Klem"
D18-1269,C12-1089,0,0.222208,"e) and the resulting system can perform the task only in the training language. In practice, however, systems used in major international products need to handle inputs in many languages. In these settings, it is nearly impossible to annotate data in all languages that a system might encounter during operation. A scalable way to build multilingual systems is through cross-lingual language understanding (XLU), in which a system is trained primarily on data in one language and evaluated on data in others. While XLU shows promising results for tasks such as cross-lingual document classification (Klementiev et al., 2012; Schwenk and Li, 2018), there are very few, if any, XLU benchmarks for more difficult language understanding tasks like natural language inference. Large-scale natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a mil"
D18-1269,P14-2037,0,0.0157812,"ction 3. We describe our baselines in Section 4, and finally present and discuss results in Section 5. 2 Related Work Multilingual Word Embeddings Much of the work on multilinguality in language understanding has been at the word level. Several approaches have been proposed to learn cross-lingual word representations, i.e., word representations where translations are close in the embedding space. Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al., 2017; Conneau et al., 2018). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as continuous bag-of-words (CBOW). Alth"
D18-1269,2005.mtsummit-papers.11,0,0.0515074,"a rate of 0.1. For X-BiLSTMs, we perform model selection on the XNLI validation set in each target language. For X-CBOW, we keep a validation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 2012), the OpenSubtitles 2016 and 2018 corpora (Tiedemann, 2012) and LD"
D18-1269,D11-1062,0,0.0230347,"n different sentence embeddings is difficult. Moreover, the distribution of classes in the Reuters corpus is highly unbalanced, and the dataset does not provide a development set in the target language, further complicating experimental comparisons. In addition to the Reuters corpus, Cer et al. (2017) propose sentence-level multilingual training and evaluation datasets for semantic textual similarity in four languages. There have also been efforts to build multilingual RTE datasets, either through translating English data (Mehdad et al., 2011), or annotating sentences from a parallel corpora (Negri et al., 2011). More recently, Agic and Schluter (2017) provide a corpus, that is very complementary to our work, of human translations for 1332 pairs of the SNLI data into Arabic, French, Russian, and Spanish. Among all these benchmarks, XNLI is the first large-scale corpus for evaluating sentence-level representations on that many languages. In practice, cross-lingual sentence understanding goes beyond translation. For instance, Mohammad et al. (2016) analyze the differences in human sentiment annotations of Arabic sentences and their English translations, and conclude that most of them come from cultural"
D18-1269,N18-1202,0,0.465016,"le natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examples in NLI three-way classification format in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu, making a total of 112,500 annotated pairs."
D18-1269,W15-1512,0,0.0419802,"Missing"
D18-1269,S18-2023,0,0.0414974,"Missing"
D18-1269,L18-1560,1,0.795078,"em can perform the task only in the training language. In practice, however, systems used in major international products need to handle inputs in many languages. In these settings, it is nearly impossible to annotate data in all languages that a system might encounter during operation. A scalable way to build multilingual systems is through cross-lingual language understanding (XLU), in which a system is trained primarily on data in one language and evaluated on data in others. While XLU shows promising results for tasks such as cross-lingual document classification (Klementiev et al., 2012; Schwenk and Li, 2018), there are very few, if any, XLU benchmarks for more difficult language understanding tasks like natural language inference. Large-scale natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and thes"
D18-1269,W17-2619,1,0.925242,"nneau et al., 2017; Subramanian et al., 2018), some recent developments have shown that pretrained language models can also transfer very well, either when the hidden states of the model are used as contextualized word vectors (Peters et al., 2018), or when the full model is finetuned on transfer tasks (Radford et al.; Howard and Ruder, 2018). Multilingual Sentence Representations There has been some effort on developing multilingual sentence embeddings. For example, Chandar et al. (2013) train bilingual autoencoders with the objective of minimizing reconstruction error between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a compositional vector model involving unigrams and bigrams to learn document level representations. Pham et al. (2015) directly train embedding representations for sentences with no attempt at compositionality. Zhou et al. (2016) learn bilingual document representations by minimizing the Euclidean distance between document representations and their translations. Cross-lingual Evaluation Benchmarks The lac"
D18-1269,D16-1217,0,0.0166198,"Schluter (2017) provide a corpus, that is very complementary to our work, of human translations for 1332 pairs of the SNLI data into Arabic, French, Russian, and Spanish. Among all these benchmarks, XNLI is the first large-scale corpus for evaluating sentence-level representations on that many languages. In practice, cross-lingual sentence understanding goes beyond translation. For instance, Mohammad et al. (2016) analyze the differences in human sentiment annotations of Arabic sentences and their English translations, and conclude that most of them come from cultural differences. Similarly, Smith et al. (2016) show that most of the degradation in performance when applying a classification model trained in English to Spanish data translated to English is due to cultural differences. One of the limitations of the XNLI corpus is that it does not capture these differences, since it was obtained by translation. We see the XNLI evaluation as a necessary step for multilingual NLP before tackling the even more complex problem of domain-adaptation that occurs when handling this the change in style from one language to another. 3 The XNLI Corpus Because the test portion of the Multi-Genre NLI data is private"
D18-1269,L16-1561,0,0.025266,"er of 128 hidden units, regularized with dropout (Srivastava et al., 2014) at a rate of 0.1. For X-BiLSTMs, we perform model selection on the XNLI validation set in each target language. For X-CBOW, we keep a validation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 20"
D18-1269,tiedemann-2012-parallel,0,0.0411264,"set in each target language. For X-CBOW, we keep a validation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 2012), the OpenSubtitles 2016 and 2018 corpora (Tiedemann, 2012) and LDC2010T21, LDC2010T23 LDC corpora, and obtained a total of 64k parallel sentences. F"
D18-1269,L18-1239,0,0.0441002,"Missing"
D18-1269,W18-5446,1,0.847931,"Missing"
D18-1269,W14-1613,0,0.0296834,"ut a vector of fixed size as a sentence representation. While previous work shows that performance on the NLI task can be improved by using cross-sentence attention between the premise and 2479 hypothesis (Rocktäschel et al., 2016; Gong et al., 2018), we focus on methods with fixed-size sentence embeddings. 4.2.1 Aligning Word Embeddings Multilingual word embeddings are an efficient way to transfer knowledge from one language to another. For instance, Zhang et al. (2016) show that cross-lingual embeddings can be used to extend an English part-of-speech tagger to the cross-lingual setting, and Xiao and Guo (2014) achieve similar results in dependency parsing. Cross-lingual embeddings also provide an efficient mechanism to bootstrap neural machine translation (NMT) systems for low-resource language pairs, which is critical in the case of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018). In that case, the use cross-lingual embeddings directly helps the alignment of sentence-level encoders. Cross-lingual embeddings can be generated efficiently using a very small amount of supervision. By using a small parallel dictionary with n = 5000 word pairs, it is possible to learn a line"
D18-1269,N15-1104,0,0.0429287,"ders. Cross-lingual embeddings can be generated efficiently using a very small amount of supervision. By using a small parallel dictionary with n = 5000 word pairs, it is possible to learn a linear mapping to minimize W ? = argmin kW X − Y kF = U V T , W ∈Od (R) where d is the dimension of the embeddings, and X and Y are two matrices of shape (d, n) that correspond to the aligned word embeddings that appear in the parallel dictionary, Od (R) is the group of orthogonal matrices of dimension d, and U and V are obtained from the singular value decomposition (SVD) of Y X T : U ΣV T = SVD(Y X T ). Xing et al. (2015) show that enforcing the orthogonality constraint on the linear mapping leads to better results on the word translation task. In this paper, we use common-crawl word embeddings (Grave et al., 2018) aligned with the MUSE library of Conneau et al. (2018). 4.2.2 Universal Multilingual Sentence Embeddings Most of the successful recent approaches for learning universal sentence representations have relied on English (Kiros et al., 2015; Arora et al., 2017; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). While notable recent approaches have considered building a shared sentence en"
D18-1269,P16-1133,0,0.0359767,"multilingual sentence embeddings. For example, Chandar et al. (2013) train bilingual autoencoders with the objective of minimizing reconstruction error between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a compositional vector model involving unigrams and bigrams to learn document level representations. Pham et al. (2015) directly train embedding representations for sentences with no attempt at compositionality. Zhou et al. (2016) learn bilingual document representations by minimizing the Euclidean distance between document representations and their translations. Cross-lingual Evaluation Benchmarks The lack of evaluation benchmark has hindered the development of such multilingual representations. Most previous approaches use the Reuters crosslingual document classification corpus Klementiev et al. (2012) for evaluation. However, the classification in this corpus is done at document level, and, as there are many ways to aggregate sentence embeddings, the comparison between different sentence embeddings is difficult. Mor"
E09-1003,W08-0309,0,0.0192399,"plural form of adjectives and nouns are counted as multiple entries. The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development 18 French Gigaword English translations used as queries per day articles parallel sentences with extra words at ends candidate sentence pairs parallel sentences SMT FR EN length comparison tail removal + number / table removing 174M words WER/TER 133M words 26.8M words 24.3M words +−5 day articles from English Gigaword Figure 3: Architecture of the parallel sentence extraction system. SMT (Callison-Burch et al., 2008), called newstest2008 in the following. The size of this corpus amounts to 2051 lines and about 44 thousand words. This data was randomly split into two parts for development and testing. Note that only one reference translation is available. We also noticed several spelling errors in the French source texts, mainly missing accents. These were mostly automatically corrected using the Linux spell checker. This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al., 2008). The system tuned on this development data is"
E09-1003,W04-3208,0,0.3703,"ce, the SMT translation used as query and the potential parallel sentence as determined by information retrieval. Bold parts are the extra tails at the end of the sentences which we automatically removed. include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. They identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on a sentence pair similarity score and use DP to find the least-cost alignment over the document pair. Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. They work on “very non-parallel corpora”. They then generate all possible sentence pairs and select the best ones based on a threshold on cosine similarity scores. Using the extracted sentences they learn a dictionary and iterate over with more sentence pairs. Recent work by Munteanu and Marcu (2005) uses a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using information retrieval (IR) t"
E09-1003,J93-1004,0,0.141176,"slate French to English using an SMT system as described above. These translated texts are then used to perform information retrieval from the English corpus, followed by simple metrics like WER and TER to filter out good sentence pairs and eventually generate a parallel corpus. We show that a parallel corpus obtained using this technique helps considerably to improve an SMT system. We shall also be trying to answer the following question over the course of this study: do we need 3 LDC corpora LDC2007T07 (English) and LDC2006T17 (French). 19 and pass the sentence pair through further filters. Gale and Church (1993) based their align program on the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. We also use the same logic in our initial selection of the sentence pairs. A sentence pair is selected for further processing if the length ratio is not more than 1.6. A relaxed factor of 1.6 was chosen keeping in consideration the fact that French sentences are longer than their respective English translations. Finally, we discarded all sentences that contain a large fraction of"
E09-1003,2003.mtsummit-tttt.3,0,0.310757,"ords) were added. In the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on (1) (2) e∗ = arg max P r(e|f ) X 4−gram LM SMT baseline system Fr where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This approach is usually referred to as the noisy sourcechannel approach in SMT (Brown et al., 1993). Bilingual corpora are needed to train the translation model and monolingual texts to train the target language model. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) instead of the original word-based approach. A phrase is defined as a group of source words f˜ that should be translated together into a group of target words e˜. The translation model in phrase-based systems includes the phrase translation probabilities in both directions, i.e. P (˜ e|f˜) and P (f˜|˜ e). The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : = arg max{exp( 3.3G words phrase table The goal of SMT is to produce a target sentence e from a source sentence f . Among all possible targe"
E09-1003,P07-2045,0,0.0100841,"Missing"
E09-1003,J05-4003,0,0.923332,"ify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on a sentence pair similarity score and use DP to find the least-cost alignment over the document pair. Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. They work on “very non-parallel corpora”. They then generate all possible sentence pairs and select the best ones based on a threshold on cosine similarity scores. Using the extracted sentences they learn a dictionary and iterate over with more sentence pairs. Recent work by Munteanu and Marcu (2005) uses a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using information retrieval (IR) techniques. Candidate sentences are determined based on word overlap and the decision whether a sentence pair is parallel or not is performed by a maximum entropy classifier trained on parallel sentences. Bootstrapping is used and the size of the learned bilingual dictionary is increased over iterations to get better results. Our technique is similar to that of (Munteanu and Marcu, 2005) but we byp"
E09-1003,P02-1038,0,0.12026,"ted corpus to the already available human-translated corpora. This paper is organized as follows. In the next section we first describe the baseline SMT system trained on human-provided translations only. We then proceed by explaining our parallel sentence selection scheme and the post-processing. Section 4 summarizes our experimental results and the paper concludes with a discussion and perspectives of this work. human translations Fr = arg max Pr(f |e) Pr(e) e up to 275M words e automatic translations En Figure 2: Using an SMT system used to translate large amounts of monolingual data. set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted using the default settings of the Moses SMT toolkit. The 4-gram back-off target LM is trained on the English part of the bitexts and the Gigaword"
E09-1003,J03-1002,0,0.0120107,"the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on (1) (2) e∗ = arg max P r(e|f ) X 4−gram LM SMT baseline system Fr where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This approach is usually referred to as the noisy sourcechannel approach in SMT (Brown et al., 1993). Bilingual corpora are needed to train the translation model and monolingual texts to train the target language model. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) instead of the original word-based approach. A phrase is defined as a group of source words f˜ that should be translated together into a group of target words e˜. The translation model in phrase-based systems includes the phrase translation probabilities in both directions, i.e. P (˜ e|f˜) and P (f˜|˜ e). The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : = arg max{exp( 3.3G words phrase table The goal of SMT is to produce a target sentence e from a source sentence f . Among all possible target language sentences"
E09-1003,J03-3002,0,0.603464,"target language side of the comparable corpus to attain the same goal, thus the comparable corpus itself helps to better extract possible parallel sentences. The Gigaword comparable corpora were used in this paper, but the same approach can be extended to ex5 Conclusion and discussion Sentence aligned parallel corpora are essential for any SMT system. The amount of in-domain parallel corpus available accounts for the quality of the 22 tract parallel sentences from huge amounts of corpora available on the web by identifying comparable articles using techniques such as (Yang and Li, 2003) and (Resnik and Y, 2003). This technique is particularly useful for language pairs for which very little parallel corpora exist. Other probable sources of comparable corpora to be exploited include multilingual encyclopedias like Wikipedia, encyclopedia Encarta etc. There also exist domain specific comparable corpora (which are probably potentially parallel), like the documentations that are done in the national/regional language as well as English, or the translations of many English research papers in French or some other language used for academic proposes. We are currently working on several extensions of the pro"
E09-1003,2006.amta-papers.25,0,0.0195775,"). WER measures the number of operations required to transform one sentence into the other (insertions, deletions and substitutions). A zero WER would mean the two sentences are identical, subsequently lower WER sentence pairs would be sharing most of the common words. However two correct translations may differ in the order in which the words appear, something that WER is incapable of taking into account as it works on word to word basis. This shortcoming is addressed by TER which allows block movements of words and thus takes into account the reorderings of words and phrases in translation (Snover et al., 2006). We used both WER and TER to choose the most suitable sentence pairs. pus (search space for IR). These day-specific files are then used for information retrieval using a robust information retrieval system. The Lemur IR toolkit (Ogilvie and Callan, 2001) was used for sentence extraction. The top 5 scoring sentences are returned by the IR process. We found no evidence that retrieving more than 5 top scoring sentences helped get better sentences. At the end of this step, we have for each query sentence 5 potentially matching sentences as per the IR score. The information retrieval step is the m"
E09-1003,P03-1010,0,0.0606219,"tin to put an end to human rights violations in the northern Caucasus. Result: Nicola Duckworth, head of Amnesty International’s Europe and Central Asia department, said the non-governmental organisations (NGOs) would call on Putin to put an end to human rights abuses in the North Caucasus, including the war-torn province of Chechnya. Figure 1: Some examples of a French source sentence, the SMT translation used as query and the potential parallel sentence as determined by information retrieval. Bold parts are the extra tails at the end of the sentences which we automatically removed. include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. They identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on a sentence pair similarity score and use DP to find the least-cost alignment over the document pair. Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. They work on “very non-parallel corpora”. They then generate all possible sentence pairs and select the best ones based"
E09-1003,J93-2003,0,\N,Missing
E09-1003,N03-1017,0,\N,Missing
E12-2003,P07-2045,0,0.00303578,"Table 1 presents statistics of these in-domain data. The data extracted from HAL were used to adapt a generic system to the scientific literature domain. The generic system was mostly trained on data provided for the shared task of Sixth Workshop on Statistical Machine Translation6 (WMT 2011), described in Table 2. Table 3 presents results showing, in the English–French direction, the impact on the statistical engine of introducing the resources extracted from HAL, as well as the impact of domain adaptation techniques. The baseline statistical engine is a standard PBSMT system based on Moses (Koehn et al., 2007) and the SRILM tookit (Stolcke, 2002). Is was trained and tuned only on WMT11 data (out-of-domain). Incorporating the HAL data into the language model and tuning the system on the HAL development set, Domain Lg Monolingual data Train cs En 2.5 M Fr 761 k phys En 2.1 M Fr 662 k 54 M 19 M 50 M 17 M 457 k 274 k 646 k 292 k Table 1: Statistics for the parallel training, development, and test data sets extracted from thesis abstracts contained in HAL, as well as monolingual data extracted from all documents in HAL, in computer science (cs) and physics (phys). The following statistics are given for"
E12-2003,2008.iwslt-papers.6,1,0.861276,"f the corpus: the number of sentences, the number of running words (after tokenisation) and the number of words in the vocabulary (M and k stand for millions and thousands, respectively). yielded a gain of more than 7 BLEU points, in both domains (computer science and physics). Including the theses abstracts in the parallel training corpus, a further gain of 2.3 BLEU points is observed for computer science, and 3.1 points for physics. The last experiment performed aims at increasing the amount of in-domain parallel texts by translating automatically in-domain monolingual data, as suggested by Schwenk (2008). The synthesised bitext does not bring new words into the system, but increases the probability of indomain bilingual phrases. By adding a synthetic bitext of 12 million words to the parallel training data, we observed a gain of 0.5 BLEU point for computer science, and 0.7 points for physics. Although not shown here, similar results were obtained in the French–English direction. The French–English system is actually slightly better than the English–French one as it is an easier translation direction. 13 Translation Model Language Model Tuning Domain wmt11 wmt11+hal wmt11+hal wmt11+hal wmt11 h"
E17-1104,C14-1008,0,0.0841201,"ngth of the sentence and the position of this layer in the network. (Zhang et al., 2015) were the first to perform sentiment analysis entirely at the character level. Their systems use up to six convolutional layers, followed by three fully connected classification layers. Convolutional kernels of size 3 and 7 are used, as well as simple max-pooling layers. Another interesting aspect of this paper is the introduction of several large-scale data sets for text classification. We use the same experimental setting (see section 4.1). The use of character level information was also proposed by (Dos Santos and Gatti, 2014): all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information in a shallow architecture. In parallel to our work, (Yang et al., 2016) proposed a based hierarchical attention network for document classification that perform an attention first on the sentences in the document, and on the words in the sentence. Their architecture performs very well on datasets whose samples contain multiple sentences. In the computer vision community, the combination of recurrent and convolutional networks in one architecture has also"
E17-1104,P14-1062,0,0.410003,"h are shared for all nodes (Socher et al., 2011). The state of the top node is fed to the classifier. A recurrent neural net1108 work (RNN) could be considered as a special case of a recursive NN: the combination is performed sequentially, usually from left to right. The last state of the RNN is used as fixed-sized representation of the sentence, or eventually a combination of all the hidden states. First works using convolutional neural networks for NLP appeared in (Collobert and Weston, 2008; Collobert et al., 2011). They have been subsequently applied to sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015). We will discuss these techniques in more detail below. If not otherwise stated, all approaches operate on words which are projected into a high-dimensional space. A rather shallow neural net was proposed in (Kim, 2014): one convolutional layer (using multiple widths and filters) followed by a max pooling layer over time. The final classifier uses one fully connected layer with drop-out. Results are reported on six data sets, in particular Stanford Sentiment Treebank (SST). A similar system was proposed in (Kalchbrenner et al., 2014), but using five convolutional layers."
E17-1104,D14-1181,0,0.0559411,"eights which are shared for all nodes (Socher et al., 2011). The state of the top node is fed to the classifier. A recurrent neural net1108 work (RNN) could be considered as a special case of a recursive NN: the combination is performed sequentially, usually from left to right. The last state of the RNN is used as fixed-sized representation of the sentence, or eventually a combination of all the hidden states. First works using convolutional neural networks for NLP appeared in (Collobert and Weston, 2008; Collobert et al., 2011). They have been subsequently applied to sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015). We will discuss these techniques in more detail below. If not otherwise stated, all approaches operate on words which are projected into a high-dimensional space. A rather shallow neural net was proposed in (Kim, 2014): one convolutional layer (using multiple widths and filters) followed by a max pooling layer over time. The final classifier uses one fully connected layer with drop-out. Results are reported on six data sets, in particular Stanford Sentiment Treebank (SST). A similar system was proposed in (Kalchbrenner et al., 2014), but using"
E17-1104,P16-1162,0,0.0127549,"ve been applied to text processing. 1 Introduction The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently. We may want to associate categories to parts of the text (e.g. POS tagging or sentiment analysis), structure text differently (e.g. parsing), or convert it to some other form which preserves all or part of the content (e.g. machine translation, summarization). The level of granularity of this processing can range from individual characters to subword units (Sennrich et al., 2016) or words up to whole sentences or even paragraphs. After a couple of pioneer works (Bengio et al. (2001), Collobert and Weston (2008), Collobert et al. (2011) among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks. However, while the use of (deep) neural networks in NLP has shown very good results for many tasks, it seems that they have not yet reached the level to outperform the state-of-the-art by a large margin, as it was observed in computer vision and speech recognition. Con"
E17-1104,D11-1014,0,0.0202546,"een projected into a low-dimensional space, and these embeddings are combined to obtain a fixed size representation of the input sentence, which then serves as input for the classifier. The simplest combination is the element-wise mean. This usually performs badly since all notion of token order is disregarded. Another class of approaches are recursive neural networks. The main idea is to use an external tool, namely a parser, which specifies the order in which the word embeddings are combined. At each node, the left and right context are combined using weights which are shared for all nodes (Socher et al., 2011). The state of the top node is fed to the classifier. A recurrent neural net1108 work (RNN) could be considered as a special case of a recursive NN: the combination is performed sequentially, usually from left to right. The last state of the RNN is used as fixed-sized representation of the sentence, or eventually a combination of all the hidden states. First works using convolutional neural networks for NLP appeared in (Collobert and Weston, 2008; Collobert et al., 2011). They have been subsequently applied to sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015)."
E17-1104,N16-1174,0,0.221121,"ollowed by three fully connected classification layers. Convolutional kernels of size 3 and 7 are used, as well as simple max-pooling layers. Another interesting aspect of this paper is the introduction of several large-scale data sets for text classification. We use the same experimental setting (see section 4.1). The use of character level information was also proposed by (Dos Santos and Gatti, 2014): all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information in a shallow architecture. In parallel to our work, (Yang et al., 2016) proposed a based hierarchical attention network for document classification that perform an attention first on the sentences in the document, and on the words in the sentence. Their architecture performs very well on datasets whose samples contain multiple sentences. In the computer vision community, the combination of recurrent and convolutional networks in one architecture has also been investigated, with the goal to “get the best of both worlds”, e.g. (Pinheiro and Collobert, 2014). The same idea was recently applied to sentence classification (Xiao and Cho, 2016). A convolutional network"
F12-2039,2010.jeptalnrecital-long.28,0,0.0713715,"Missing"
F12-2039,C04-1151,0,0.0788859,"Missing"
F12-2039,W11-1209,0,0.0222291,"Missing"
F12-2039,P07-2045,0,0.00730689,"Missing"
F12-2039,J05-4003,0,0.0937007,"Missing"
F12-2039,P02-1040,0,0.0932168,"Missing"
F12-2039,J03-3002,0,0.0988077,"Missing"
F12-2039,2011.iwslt-evaluation.10,1,0.896882,"Missing"
F12-2039,P03-1010,0,0.0426264,"Missing"
H05-1026,J92-4003,0,0.0174698,"Missing"
H05-1026,W04-3242,0,\N,Missing
I08-2089,D07-1090,0,0.0600348,"creasingly large LMs for MT has been recognized in recent years. The effect of doubling LM size has been powerfully demonstrated by Google’s submissions to the NIST evaluation campaigns. The use of billions of words of LM training data has become standard in large-scale SMT systems, and even trillion word LMs have been demonstrated. Since lookup of LM scores is one of the fundamental functions in SMT decoding, efficient storage and access of the model becomes increasingly difficult. A recent trend is to store the LM in a distributed cluster of machines, which are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). T"
I08-2089,W07-0718,1,0.802514,"ssible translations, we may miss the translation that we would have found with a LM integrated into the decoding process. 6 Experiments In our experiments we are looking for answers to the open questions on the use of LMs for SMT: Do perplexity and B LEU score performance correlate when interpolating LMs? Should LMs be combined by interpolation or be used as separate feature functions in the log-linear machine translation model? Is the use of LMs in re-ranking sufficient to increase machine translation performance? 6.1 29 Dev Test Interpolation In the WMT 2007 shared task evaluation campaign (Callison-Burch et al., 2007) domain adaptation was a special challenge. Two training corpora were provided: a small in-domain corpus (News Commentary) and the about 30 times bigger out-of-domain Europarl corpus (see Table 1). One method for domain adaptation is to bias the LM towards the indomain data. We train two LMs and interpolate them to optimize performance on in-domain data. In our experiments, the translation model is first trained on the combined corpus without weighting. We use the Moses decoder (Koehn et al., 2007) with default settings. The 5-gram LM was trained using the SRILM toolkit. We only run minimum er"
I08-2089,2007.mtsummit-papers.18,1,0.852093,"Missing"
I08-2089,W06-3113,0,0.0419437,"torage and access of the model becomes increasingly difficult. A recent trend is to store the LM in a distributed cluster of machines, which are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Models LM training data may be any text in the output language. Typically, however, we are interested in building a MT system for a particular domain. If text resources come from a diversity of domains, some may be more suitable than others. A common stratNeural Network input output layer probability estimation wj−n+1 projection laye"
I08-2089,W06-3114,1,0.787443,"Missing"
I08-2089,P07-2045,1,0.0103622,"Missing"
I08-2089,2005.mtsummit-papers.11,1,0.0176338,"Missing"
I08-2089,2006.iwslt-papers.2,1,0.886116,"Missing"
I08-2089,P07-1065,0,0.0131391,"ch are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Models LM training data may be any text in the output language. Typically, however, we are interested in building a MT system for a particular domain. If text resources come from a diversity of domains, some may be more suitable than others. A common stratNeural Network input output layer probability estimation wj−n+1 projection layer p1 = P (wj =1|hj ) hidden layer M cl wj−n+2 oi dj pi = P (wj =i|hj ) V shared projections H wj−1 P N discrete continuous representat"
I08-2089,D07-1049,0,0.0136637,"ch are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Models LM training data may be any text in the output language. Typically, however, we are interested in building a MT system for a particular domain. If text resources come from a diversity of domains, some may be more suitable than others. A common stratNeural Network input output layer probability estimation wj−n+1 projection layer p1 = P (wj =1|hj ) hidden layer M cl wj−n+2 oi dj pi = P (wj =i|hj ) V shared projections H wj−1 P N discrete continuous representat"
I08-2089,N07-1062,0,0.0194608,"Missing"
I08-2089,W06-1626,0,0.0109876,"een powerfully demonstrated by Google’s submissions to the NIST evaluation campaigns. The use of billions of words of LM training data has become standard in large-scale SMT systems, and even trillion word LMs have been demonstrated. Since lookup of LM scores is one of the fundamental functions in SMT decoding, efficient storage and access of the model becomes increasingly difficult. A recent trend is to store the LM in a distributed cluster of machines, which are queried via network requests (Brants et al., 2007; Emami et al., 2007). It is easier, however, to use such large LMs in reranking (Zhang et al., 2006). Since the use of clusters of machines is not always practical (or affordable) for SMT applications, an alternative strategy is to find more efficient ways to store the LM in the working memory of a single machine, for instance by using efficient prefix trees and fewer bits to store the LM probability (Federico and Bertoldi, 2006). Also the use of lossy data structures based on Bloom filters has been demonstrated to be effective for LMs (Talbot and Osborne, 2007a; Talbot and Osborne, 2007b). This allows the use of much larger LMs, but increases the risk of errors. 3 Combination of Language Mo"
I11-1148,D10-1044,0,0.0511251,"data often comes from different sources, e.g. Europarl, UN, in-domain data in limited amounts, data crawled from the Internet or even bitexts automatically extracted from comparable corpora. It seems obvious that the appropriateness and the usefulness of this parallel data for There have been several attempts in the literature to address some of these problems. Matsoukas et al. (2009) proposed to weight each sentence in the training bitexts by optimizing a discriminative function on a tuning set. Sentencelevel features are extracted to estimate the weights that are relevant to the given task. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instance-weighting model into a linear combination of feature functions. The technique presented in this paper is related to these previous works as it concerns the weighting of corpora or sentences. However, it does not 1323 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1323–1331, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Alignments obtained from time-stamped training parallel text"
I11-1148,2010.amta-papers.21,0,0.597558,"to extract and score the phrase pairs by simple relative frequency. Doing this, the parallel data is (wrongly) considered as one homogeneous pool of knowledge. We argue that the parallel data is quite inhomogeneous in many practical applications with respect to several factors: During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further verified it on the o"
I11-1148,P07-2045,0,0.00393663,"be used to achieve best weights. Only one weight is given to each time span, consequently the span size will have an impact on the alignment selection process. Using smaller spans results in a more fine grained weighting scheme. We have tested different settings with different time spans to see whether the impact of weighting changes with the { { 500k Dev=2.5k Test=2.5k Our first experiments are based on the FrenchEnglish portion of the freely available timestamped Europarl data (Koehn, 2005) from April 1996 to December 2010. We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007), though our approach is equally applicable to any other approach based on alignments and could be used for any language pairs. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The coefficients of these feature functions are optimized by minimum error training. In the first experiments, the whole Europarl corpus was split into train, development and test as shown in Figure 3. The most r"
I11-1148,2005.mtsummit-papers.11,0,0.0293589,"iament. As an example Figure 4 shows the histogram of the data per year. One can ask which time granularity should be used to achieve best weights. Only one weight is given to each time span, consequently the span size will have an impact on the alignment selection process. Using smaller spans results in a more fine grained weighting scheme. We have tested different settings with different time spans to see whether the impact of weighting changes with the { { 500k Dev=2.5k Test=2.5k Our first experiments are based on the FrenchEnglish portion of the freely available timestamped Europarl data (Koehn, 2005) from April 1996 to December 2010. We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007), though our approach is equally applicable to any other approach based on alignments and could be used for any language pairs. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The coefficients of these feature functions are optimized by minimum error training. In th"
I11-1148,N10-1062,0,0.476804,"phrase pairs by simple relative frequency. Doing this, the parallel data is (wrongly) considered as one homogeneous pool of knowledge. We argue that the parallel data is quite inhomogeneous in many practical applications with respect to several factors: During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further verified it on the official WMT’11 task, tran"
I11-1148,D09-1074,0,0.0584299,"ntroduction Statistical machine translation (SMT) systems are based on two types of resources: monolingual data to build a language model (LM) and bilingual data – also called bitexts – to train the translation model (TM). The parallel data often comes from different sources, e.g. Europarl, UN, in-domain data in limited amounts, data crawled from the Internet or even bitexts automatically extracted from comparable corpora. It seems obvious that the appropriateness and the usefulness of this parallel data for There have been several attempts in the literature to address some of these problems. Matsoukas et al. (2009) proposed to weight each sentence in the training bitexts by optimizing a discriminative function on a tuning set. Sentencelevel features are extracted to estimate the weights that are relevant to the given task. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instance-weighting model into a linear combination of feature functions. The technique presented in this paper is related to these previous works as it concerns the weighting of corpora or sentences. However, it doe"
I11-1148,P00-1056,0,0.242428,"Missing"
I11-1148,W11-2158,1,0.838473,"e extracted from different parallel sentences coming from different time spans. Furthermore, weighting the alignments with their scores has shown improvements in the BLEU score as presented in Table 3, but considering the alignment score at the phrase level is not straight forward. 6 Experiments on the WMT task To further verify whether our results are robust beyond the narrow experimental conditions, we considered a task where the development and test data do not come from the same source than the bitexts. We took the official test sets of the 2011 WMT translation tasks as dev and test sets (Schwenk et al., 2011) i.e news-test09 and news-test10 respectively. We built English-French systems by using the Europarl and News-Commentary (NC) corpora, both contain news data over a long time period. For this set-up, there are three coefficients to optimize: the decay factor for Europarl λ1 , the decay factor for the news-commentary texts λ2 and a coefficient for the alignments α. The Europarl corpus was divided into time span according to years and NC corpus was assumed to be sorted over time since time-stamp information was not available for the NC corpus. Remaining settings are kept same as mentioned in pre"
I11-1148,W10-1759,1,0.764199,"45 0.4 !t Normalized weights 0.05 0.09 .................................. 0.17 0.20 Resampling with replacement 1996 1997 .......................... 2009 2010 Resampled alignments used to build Phrase Table Figure 1: Overview of the weighting scheme. The alignments are weighted by an exponential decay function, parameterized by λ. Resampling with replacement is used to create a new corpus (parts with higher weight will appear more often). The phrase table is built from this corpus using the standard procedure. require the calculation of additional sentence-level features. In our previous work Shah et al. (2010) we proposed a technique to weight heterogeneous data by weighted resampling of the alignments. The weights were numerically optimized on development data. Hardt and Elming (2010) has shown recency effect in terms of file-context and concluded that the data within the same file is of greater importance than the rest. Levenberg et al. (2010) proposed an incremental training procedure to deal with a continuous stream of parallel text. Word alignment was performed by the stepwise online EM algorithm and the phrase table was represented with suffix arrays. The authors showed that it is better to u"
I13-1033,C10-1073,0,0.0933475,"arallel data than the sentence extraction method for each TER threshold, and this difference of quantities improves results of MT system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4."
I13-1033,C04-1151,0,0.263326,"nk Universit´e du Maine, Avenue Olivier Messiaen F-72085 - LE MANS, France FirstName.LastName@lium.univ-lemans.fr Abstract beneficial in order to overcome the lack of parallel data. The ability to detect these parallel data enables the automatic creation of large parallel corpora. Most of existing studies dealing with comparable corpora look for parallel data at the sentence level (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; AbdulRauf and Schwenk, 2011). However, the degree of parallelism can vary considerably, from noisy parallel texts, to quasi parallel texts (Fung and Cheung, 2004). Corpora from the last category contain none or few good parallel sentence pairs. However, there could have parallel phrases in comparable sentences that can prove to be helpful for SMT (Munteanu and Marcu, 2006). As an example, consider Figure 1, which presents two news articles with their video from the English and French editions of the Euronews website1 . The articles report on the same event with different sentences that contain some parallel translations at the phrase level. These two documents contain in particular no exact sentence pairs, so techniques for extracting parallel sentence"
I13-1033,J05-4003,0,0.105594,"ignificant improvements over a state-ofthe-art baseline. 1 Introduction The development of a statistical machine translation (SMT) system requires one or more parallel corpora called bitexts for training the translation model and monolingual data to build the target language model. Unfortunately, parallel texts are a limited resource and they are often not available for some specific domains and language pairs. That is why, recently, there has been a huge interest in the automatic creation of parallel data. Since comparable corpora exist in large quantities and are much more easily available (Munteanu and Marcu, 2005), the ability to exploit them is highly 1 www.euronews.com/ 286 International Joint Conference on Natural Language Processing, pages 286–292, Nagoya, Japan, 14-18 October 2013. Multimodal Comparable Corpora Audio L1 ASR Comparable audio Sentences L1 Split Manual Phrases L1 Transcription Texts L2 Manual Transcription SMT Comparable texts Parallel Data Split Translations L2 IR Phrases L2 Figure 1: Example of multimodal comparable corpora from the Euronews website. Phrases L2 Filter Figure 3: Principle of the parallel phrase extraction system from multimodal comparable corpora. 2.2 System Archite"
I13-1033,P98-1069,0,0.15657,"Table 4. The benefit of our method is that it can generates more quantities of parallel data than the sentence extraction method for each TER threshold, and this difference of quantities improves results of MT system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k"
I13-1033,P06-1011,0,0.117389,"parallel data enables the automatic creation of large parallel corpora. Most of existing studies dealing with comparable corpora look for parallel data at the sentence level (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; AbdulRauf and Schwenk, 2011). However, the degree of parallelism can vary considerably, from noisy parallel texts, to quasi parallel texts (Fung and Cheung, 2004). Corpora from the last category contain none or few good parallel sentence pairs. However, there could have parallel phrases in comparable sentences that can prove to be helpful for SMT (Munteanu and Marcu, 2006). As an example, consider Figure 1, which presents two news articles with their video from the English and French editions of the Euronews website1 . The articles report on the same event with different sentences that contain some parallel translations at the phrase level. These two documents contain in particular no exact sentence pairs, so techniques for extracting parallel sentences will not give good results. We need a method to extract parallel phrases which exist at the sub-sentential level. For some languages, text comparable corpora may not cover all topics in some specific domains and"
I13-1033,W08-0509,0,0.0370378,"Missing"
I13-1033,P03-1021,0,0.00575989,"(Koehn et al., 2007). The standard fourteen feature functions are used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model. It is constructed as follows. First, word alignments in both directions are calculated. We used the multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of our system were tuned on a development corpus, using the MERT tool (Och, 2003). We use the Lemur IR toolkit (Ogilvie and Callan, 2001) for the phrases extraction procedure. We first index all the French text (after splitting it into segments) into a database using Indri Index. This feature enable us to index our text documents in such a way we can use the translated phrases as queries to run information retrieval in the database, with the specialized Indri Query Language. By these means we can retrieve the best matching phrases from the French side of the comparable corpus. For each candidate phrases pair, we need to decide whether the two phrases are mutual translation"
I13-1033,P04-1067,0,0.105618,"Missing"
I13-1033,P02-1040,0,0.088425,"Missing"
I13-1033,2007.mtsummit-papers.50,0,0.0186078,"n automatic speech recognition system, a statistical machine translation system and information retrieval system. We showed by experiments conducted on English-French data, that parallel phrases extracted with this method improves significantly SMT performance. Our approach can be improved in several aspects. The automatic splitting is very simple; more advanced phrases generation might work better, and eliminate redundancy. Trying other method on filtering can also improve the precision of the method. The second type of approach in extracting parallel phrases is the alignment-based approach (Quirk et al., 2007; Riesa and Marcu, 2012). These methods are promising, but since the proposed method in (Quirk et al., 2007) do not improve significantly MT performance and model in (Riesa and Marcu, 2012) is designed for parallel data, it’s hard to say that this approach is actually effective for comparable data. 6 Acknowledgments This work has been partially funded by the French Government under the project DEPART. References This work is similar to the work by (Afli et al., 2012) where the extraction is done at the phrase level instead of the sentence level. Our methodology is the first effort aimed at det"
I13-1033,W11-1209,0,0.0213336,"threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4.3M 7.02M 11.4M 13.8M 18.04M 25.3M 35.9M 45.3M - Table 4: Number of tokens extracted and BLEU scores on DevTED obtained with PhrExtract and SentExtract method"
I13-1033,N12-1061,0,0.0996885,"wever, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4.3M 7.02M 11.4M 13.8M 18.04M 25.3M 35.9M 45.3M - Table 4: Number of tokens extracted and BLEU scores on DevTED obtained with PhrExtract and SentExtract methods for each TER threshold"
I13-1033,2011.iwslt-evaluation.10,1,0.897826,"Missing"
I13-1033,N03-1017,0,0.0129603,"Missing"
I13-1033,2006.amta-papers.25,0,0.016165,"f multimodal comparable data coming from the TED website 2 . We have an audio source of a talk in English and its text translation in French. We think that we can extract parallel data from this corpora, at the sentence and the sub-sentential level. In this work we seek to adapt and to improve machine translation systems that suffer from resource deficiency by automatically extracting parallel data in specific domains. 2 Our technique is similar to that of (Munteanu and Marcu, 2006), but we bypass the need of the Log-Likelihood-Ratio lexicon by using a baseline SMT system and the TER measure (Snover et al., 2006) for filtering. We also report an extension of the work of (Afli et al., 2012) by splitting transcribed sentences and the text parts of the multimodal corpus into phrases with length between two to ten tokens. We extract from each sentence on the corpus all combinations of two to ten sequential words. http://www.ted.com/ 287 2.3 Schwenk, 2011),4 i.e. between automatic translation, and the phrases selected by IR. Baseline systems Our ASR system is a five-pass system based on the open-source CMU Sphinx toolkit 3 (version 3 and 4), similar to the LIUM’08 French ASR system described in (Del´eglise"
I13-1033,P07-2045,0,0.0128362,"Missing"
I13-1033,W12-0102,0,0.0147084,"system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60.1M # tokens (fr) PhrExtract 1.06M 1.4M 2.5M 4.3M 7.02M 11.4M 13.8M 18.04M 25.3M 35.9M 45.3M - Table 4: Number of tokens extracted and BLEU scores on DevTED obtained with Ph"
I13-1033,P03-1010,0,0.304405,"Missing"
I13-1033,E03-1050,0,0.0417892,"t can generates more quantities of parallel data than the sentence extraction method for each TER threshold, and this difference of quantities improves results of MT system until the TER threshold of 80 is reached. However, we can see in Table 4 that the quality of only 39.35k (TER 80) extracted by SentExtract can have exactly the same impact of 25.3M extracted by our new technique. That is why we intend to investigate in the filtering module of our system. 4 Related Work Research on exploiting comparable corpora goes back to more than 15 years ago (Fung and Yee, 1998; Koehn and Knight, 2000; Vogel, 2003; Gaussier et al., 2004; Li and Gaussier, 2010). A lot of studies on data acquisition from comparable corpora for machine translation have been reported (Su and Babych, 2012; Hewavitharana and Vogel, 2011; Riesa and Marcu, 2012). To the best of our knowledge (Munteanu and 289 TER 0 10 20 30 40 50 60 70 80 90 100 Baseline BLEU score SentExtract 22.86 22.97 23.06 22.95 22.92 23.26 23.10 23.29 23.40 23.39 23.34 22.93 BLEU score PhrExtract 23.39 23.35 23.53 23.39 23.45 23.54 23.70 23.41 23.40 23.18 23.26 - # tokens (fr) SentExtract 55 313 1.7k 6.9k 23.5k 62.4k 13.82k 25.15k 39.35k 57.54k 83.60k 60"
I13-1033,C98-1066,0,\N,Missing
L18-1560,D14-1181,0,0.00549203,"een proposed to learn multilingual word embeddings, which are then combined to perform cross-lingual document classifications. These word embeddings are trained on either word alignments or sentencealigned parallel corpora. To provide reproducible benchmark results, we use MultiCCA word embeddings published by (Ammar et al., 2016). There are multiple ways to combine these word embeddings for classification. We train a simple one-layer convolutional neural network (CNN) on top of the word embeddings, which has shown to perform well on text classification tasks regardless of training data size (Kim, 2014). Specifically, convolutional filters are applied to windows of word embeddings, with a max-over-time pooling on top of them. We freeze the multilingual word embeddings while only training the classifier. Hyper-parameters such as convolutional output dimension, window sizes are done by grid search over the Dev set of the same language as the train set. 3.2. Multilingual sentence representations A second direction of research is to directly learn multilingual sentence representations. In this paper, we evaluate a recently proposed technique to learn joint multilingual sentence representations ("
L18-1560,C12-1089,0,0.596662,"ining data for other languages, but this would be costly and time consuming. An interesting alternative is “crosslingual document classification”. The underlying idea is to use a representation of the words or whole documents which is independent of the language. By these means, a classifier trained on one language can be transferred to a different one, without the need of resources in that transfer language. Ideally, the performance obtained by crosslingual transfer should be as close as possible to training the entire system on language specific resources. Such a task was first proposed by (Klementiev et al., 2012) using the Reuters Corpus Volume 2. The aim was to first train a classifier on English and then to transfer it to German, and vice versa. An extension to the transfer between English and French and Spanish respectively was proposed by (Mogadala and Rettinger, 2016). However, only few comparative results are available for these transfer directions. The contributions of this work are as follows. We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross-lingual document classification tasks for eight very different languages, namely English, French, Spanish, Ita"
L18-1560,2005.mtsummit-papers.11,0,0.0572926,"Missing"
L18-1560,N16-1083,0,0.0288886,"ge. By these means, a classifier trained on one language can be transferred to a different one, without the need of resources in that transfer language. Ideally, the performance obtained by crosslingual transfer should be as close as possible to training the entire system on language specific resources. Such a task was first proposed by (Klementiev et al., 2012) using the Reuters Corpus Volume 2. The aim was to first train a classifier on English and then to transfer it to German, and vice versa. An extension to the transfer between English and French and Spanish respectively was proposed by (Mogadala and Rettinger, 2016). However, only few comparative results are available for these transfer directions. The contributions of this work are as follows. We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross-lingual document classification tasks for eight very different languages, namely English, French, Spanish, Italian, German, Russian, Chinese and Japanese. For each language, we define a train, development and test corpus. We also provide strong reference results for all transfer directions between the eight languages, e.g. not limited to the transfer between a foreign lang"
L18-1560,W17-2619,1,0.806485,". Specifically, convolutional filters are applied to windows of word embeddings, with a max-over-time pooling on top of them. We freeze the multilingual word embeddings while only training the classifier. Hyper-parameters such as convolutional output dimension, window sizes are done by grid search over the Dev set of the same language as the train set. 3.2. Multilingual sentence representations A second direction of research is to directly learn multilingual sentence representations. In this paper, we evaluate a recently proposed technique to learn joint multilingual sentence representations (Schwenk and Douze, 2017). The underlying idea is to use multiple sequence encoders and decoders and to train them with aligned corpora from 3550 Accuracy on test languages DE EN ES FR IT Joint sentence embeddings (Europarl) DE (92.03) 76.48 76.95 76.72 66.27 EN 81.17 (88.40) 70.75 77.80 62.35 ES 77.38 67.58 (88.28) 67.92 64.07 FR 82.78 76.72 76.97 (89.75) 64.07 IT 77.10 72.70 72.60 76.97 (82.88) Train Train Accuracy on test languages Size DE EN ES FR IT MultiCCA word embeddings 1k 91.23 79.08 86.95 81.70 77.58 Joint sentence embeddings (Europarl) 1k 88.02 82.42 80.12 84.55 75.08 Avg 77.69 76.09 73.05 78.06 76.45 Tabl"
L18-1560,L16-1561,0,0.0549416,"Missing"
lambert-etal-2012-automatic,W10-1716,1,\N,Missing
lambert-etal-2012-automatic,J93-2003,0,\N,Missing
lambert-etal-2012-automatic,2008.iwslt-papers.6,1,\N,Missing
lambert-etal-2012-automatic,P07-2045,0,\N,Missing
lambert-etal-2012-automatic,W07-0733,0,\N,Missing
lambert-etal-2012-automatic,P03-1021,0,\N,Missing
lambert-etal-2012-automatic,W09-0439,0,\N,Missing
N15-1103,D11-1033,0,0.0329288,"ng the translators to post-edit translations of a Legal document from English into French (about 15k words) over five days (i.e. about 3k words/day). An in-domain adapted (DA) system was used as baseline system for the first day, before project adapted (PA) systems have taken over. 4.1 Domain adapted system Before the human translator starts working, our DA system is trained using an extracted subset of bilingual training data that is mostly relevant to our specific domain. The extraction process, widely known as data selection, is applied using cross-entropy difference algorithm proposed by (Axelrod et al., 2011)2 . In order to augment the amount of training data3 (about 22M words) we also select a bilingual subset from Europarl, JRC-Acquis, news commentary, software manuals of the OPUS corpus, translation memories and the United Nations corpus. About 700M additional newspaper monolingual data selected from WMT evaluation campaign are also used for language modeling. 4.2 Project adapted system Our project-adaptation scenario, which is repeated iteratively during the lifetime of the translation 2 3 We used the XenC tool for data selection DGT+ECB corpora (see http://opus.lingfil.uu.se) project, is achi"
N15-1103,P07-2045,0,0.00512288,"of data is translated every day by each translator. The systems are then adapted, individually for each translator, using previous days of work, and used by the translators during the next day, and so on. 3 4 Evaluation Protocol We defined an adaptation protocol with the goal to assess the same task with and without adaptation procedure. Like in (Guerberof, 2009; Plitt and Masselot, 2010), three professional translators were involved in a two parts experiment: during the first part, translators receive MT suggestions from a state-of-the-art domain-adapted engine built with the Moses toolkit (Koehn et al., 2007), without being adapted with the data generated during the translation of the project.For the second part, the MT suggestions are provided by a MT system which was previously adapted to the current project using the human translations of prior working days. Since we asked the same translators to post-edit the same document twice (i.e. with and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the po"
N15-1103,P02-1040,0,0.0934212,"th and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the post-editing effort measured with TER (Snover et al., 2006) which corresponds to the number of edits made individually by each translator, (ii) the time-to-edit rate expressed in number of translated words per hour. In addition to these two key indicators, we evaluated the translation quality using an automatic measure, namely BLEU score (Papineni et al., 2002). This measure is used to make sure that no regression in the translation quality is observed after several days 1002 Experimental framework We ran contrastive experiments by asking the translators to post-edit translations of a Legal document from English into French (about 15k words) over five days (i.e. about 3k words/day). An in-domain adapted (DA) system was used as baseline system for the first day, before project adapted (PA) systems have taken over. 4.1 Domain adapted system Before the human translator starts working, our DA system is trained using an extracted subset of bilingual trai"
N15-1103,2006.amta-papers.25,0,0.060854,"generated during the translation of the project.For the second part, the MT suggestions are provided by a MT system which was previously adapted to the current project using the human translations of prior working days. Since we asked the same translators to post-edit the same document twice (i.e. with and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the post-editing effort measured with TER (Snover et al., 2006) which corresponds to the number of edits made individually by each translator, (ii) the time-to-edit rate expressed in number of translated words per hour. In addition to these two key indicators, we evaluated the translation quality using an automatic measure, namely BLEU score (Papineni et al., 2002). This measure is used to make sure that no regression in the translation quality is observed after several days 1002 Experimental framework We ran contrastive experiments by asking the translators to post-edit translations of a Legal document from English into French (about 15k words) over five"
N15-1103,W09-0441,0,0.0199861,"mber of edits performed by the translator in order to obtain a suitable translation. The first column indicates the day of the experiment. The second column represents three SMT systems, namely: the baseline system adapted to the domain (DA), the same system with a CSLM (DA+CSLM) and the project adapted system (all models were updated, including the CSLM) noted “PA+CSLM-adapt”. The third, fourth and fifth columns represent respectively the TER scores for the three translators. The first score is calculated using the reference produced by the translator himself. It could be considered as HTER (Snover et al., 2009). The second score (in parenthesis) is calculated using the three references produced by the translators. The third score (in brackets) is calculated according to an official “generic” reference provided by the European Commission. By these additional results, we aim to assess whether their is a tendency of the systems to adapt strongly to the particular style of one translator, or whether they still perform well with respect to independent references. On day 1, only the DA and DA+CSLM systems are presented since the project adaptation can only start after the first working day. First of all,"
N15-1103,2009.mtsummit-btm.7,0,\N,Missing
N15-1103,W15-4006,1,\N,Missing
P06-2093,2003.mtsummit-papers.6,0,0.106106,"Missing"
P06-2093,W06-2606,0,0.0166728,"ontexts. This approach was successfully used in large vocabulary speech recognition (Schwenk and Gauvain, 2005), and we are interested here if similar ideas can be applied to statistical machine translation. This paper is organized as follows. In the next section we first describe the baseline statistical machine translation system. Section 3 presents the architecture of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions. 2003), and reranking of translation hypothesis using structural properties in (Hasan et al., 2006). An interesting experiment was reported at the NIST 2005 MT evaluation workshop (Och, 2005): starting with a 5-gram LM trained on 75 million words of Broadcast News data, a gain of about 0.5 point BLEU was observed each time when the amount of LM training data was doubled, using at the end 237 billion words of texts. Most of this additional data was collected by Google on the Internet. We believe that this kind of approach is difficult to apply to other tasks than Broadcast News and other target languages than English. There are many areas where automatic machine translation could be deployed"
P06-2093,W05-0821,0,0.0400682,"cular several ones that aim at improving the modeling of the target language. In most SMT systems the use of a 4-gram back-off language model usually achieves improvements in the BLEU score in comparison to the 3-gram LM used during decoding. It seems however difficult to improve upon the 4-gram LM. Many different feature functions were explored in (Och et al., 2004). In that work, the incorporation of part-of-speech (POS) information gave only a small improvement compared to a 3-gram backoff LM. In another study, a factored LM using POS information achieved the same results as the 4-gram LM (Kirchhoff and Yang, 2005). Syntaxbased LMs were investigated in (Charniak et al., We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks. 1 Introduction The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Among all possible target sentences the one with maximal probability is chosen. The classical Bayes relation is used to introduce a target language model (Brown et al., 1993): ˆ = arg max Pr(e|f ) = arg max Pr(f |e) Pr(e) e e (1) e where Pr(f |e) is the translation model and P"
P06-2093,P02-1038,0,0.0513644,"many improvements have been made, but it seems that research is mainly focused on better translation and alignment models or phrase extraction algorithms as demonstrated by numerous publications on these topics. On the other hand, we are aware of only a small amount of papers investigating new approaches to language modeling for statistical machine translation. Traditionally, statistical machine translation systems use a simple 3-gram back-off language model (LM) during decoding to generate n-best lists. These n-best lists are then rescored using a log-linear combination of feature functions (Och and Ney, 2002): Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part"
P06-2093,N04-1021,0,0.0283574,"e development and test data. ˆ ≈ arg max Pr(e)λ1 Pr(f |e)λ2 e e where the coefficients λi are optimized on a development set, usually maximizing the BLEU score. In addition to the standard feature functions, many others have been proposed, in particular several ones that aim at improving the modeling of the target language. In most SMT systems the use of a 4-gram back-off language model usually achieves improvements in the BLEU score in comparison to the 3-gram LM used during decoding. It seems however difficult to improve upon the 4-gram LM. Many different feature functions were explored in (Och et al., 2004). In that work, the incorporation of part-of-speech (POS) information gave only a small improvement compared to a 3-gram backoff LM. In another study, a factored LM using POS information achieved the same results as the 4-gram LM (Kirchhoff and Yang, 2005). Syntaxbased LMs were investigated in (Charniak et al., We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks. 1 Introduction The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Among all possible ta"
P06-2093,H05-1026,1,0.867922,"e sentence f . Among all possible target sentences the one with maximal probability is chosen. The classical Bayes relation is used to introduce a target language model (Brown et al., 1993): ˆ = arg max Pr(e|f ) = arg max Pr(f |e) Pr(e) e e (1) e where Pr(f |e) is the translation model and Pr(e) 723 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723–730, c Sydney, July 2006. 2006 Association for Computational Linguistics sible context of length n-1 instead of backing-off to shorter contexts. This approach was successfully used in large vocabulary speech recognition (Schwenk and Gauvain, 2005), and we are interested here if similar ideas can be applied to statistical machine translation. This paper is organized as follows. In the next section we first describe the baseline statistical machine translation system. Section 3 presents the architecture of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions. 2003), and reranking of translation hypothesis using structural properties in (Hasan et al., 2006). An interesting experiment was reported at the NIST 2005 MT evaluation workshop (Och, 2005)"
P06-2093,J96-1002,0,0.0316264,"ave been forgotten .” Reference 2: “it is good to remember this , because maybe we forgot it .” explored search space. Figure 1 shows an example of such a search space, here heavily pruned for the sake of clarity. 2.1 Sentence Splitting The execution complexity of our SMT decoder increases non-linearly with the length of the sentence to be translated. Therefore, the source text is split into smaller chunks, each one being translated separately. The chunks are then concatenated together. Several algorithms have been proposed in the literature that try to find the best splits, see for instance (Berger et al., 1996). In this work, we first split long sentences at punctuation marks, the remaining segments that still exceed the allowed length being split linearly. In a second pass, adjoining very short chunks are merged together. During decoding, target LM probabilities of the type Pr(w1 |<s>) and Pr(</s>|wn−1 wn ) will be requested at the beginning and at the end of the hypothesized target sentence respectively.1 This is correct when a whole sentence is translated, but leads to wrong LM probabilities when processing smaller chunks. Therefore, we define a sentence break symbol, <b>, that is used at the beg"
P06-2093,2005.mtsummit-papers.36,0,\N,Missing
P06-2093,W04-3242,0,\N,Missing
P06-2093,J93-2003,0,\N,Missing
P06-2093,P07-1065,0,\N,Missing
P06-2093,D07-1090,0,\N,Missing
P06-2093,D07-1045,1,\N,Missing
P06-2093,P07-2045,0,\N,Missing
P06-2093,2006.iwslt-papers.2,1,\N,Missing
P06-2093,W07-0712,0,\N,Missing
P13-1082,2012.amta-papers.4,0,0.0499181,"ur clustering experiments, the development set is the concatenation of the LEGAL and IT development sets. However, we always use the gold segmentation between LEGAL and IT for MERT and testing. This allows for a detailed analysis of the effect of development data clustering for the purpose of model adaptation. In an unlabelled setting, one would have to run MERT either on the full development set (as we will do for the CZ–EN task) or separately on each cluster, or use an alternative approach to optimize log-linear weights in a multidomain setting, such as feature augmentation as described by (Clark et al., 2012). For both data sets, language models are trained on the target side of the bitexts. In all experiments, we keep the number of component models constant: 12 for EN–DE, 7 for CZ–EN. We vary the number of clusters k from 1, which corresponds to adapting the models to the full development set, to 16. The baseline is the concatenation of all train837 system baseline 1 cluster (no split) 2 clusters 4 clusters 8 clusters 16 clusters gold clusters TM adaptation 34.4 34.5 34.6 34.7* 34.7* 34.7* 35.0* LM adaptation 34.4 33.7 34.0 34.3 34.5 34.7* 35.0* TM+LM adaptation 34.4 34.1 34.4 34.6 34.9* 35.0* 35"
P13-1082,W07-0717,0,0.36588,"e present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 B LEU over unadapted systems and single-domain adaptation. 1 Introduction The effectiveness of domain adaptation approaches such as mixture-modeling (Foster and Kuhn, 2007) has been established, and has led to research on a wide array of adaptation techniques in SMT, for instance (Matsoukas et al., 2009; Shah et al., 2012). In all these approaches, adaptation is performed during model training, with respect to a representative development corpus, and the models are kept unchanged when then system is deployed. Therefore, when working with multiple and/or unlabelled domains, domain adaptation is often impractical for a number of reasons. Firstly, maintaining multiple systems for each language pair, each adapted to a different domain, is costly 2 Related Work (Orti"
P13-1082,N03-1017,0,0.0071177,"on of translation model features to the decoding phase, and allow for multiple knowledge sources (e.g. bitexts or user-provided data) to contribute to their calculation. Our immediate purpose for this paper is domain adaptation in a multi-domain environment, but the delay of the feature computation has other potential applications, e.g. in interactive MT. We are concerned with calculating four features during decoding, henceforth just referred to as the translation model features: p(s|t), lex(s|t), p(t|s) and lex(t|s). s and t denote the source and target phrase. We follow the definitions in (Koehn et al., 2003). Traditionally, the phrase translation probabilities p(s|t) and p(t|s) are estimated through unsmoothed maximum likelihood estimation (MLE). weight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea,"
P13-1082,P07-2045,0,0.00955353,"ta set eu fiction navajo news paraweb subtitles techdoc total (train) dev test sentences 1 270 000 830 000 30 000 110 000 370 000 2 840 000 970 000 6 420 000 3500 3500 words (en) 25 600 000 13 700 000 490 000 2 550 000 3 930 000 21 200 000 7 270 000 74 700 000 50 700 49 600 Table 3: Parallel data sets Czech–English. pass decoding, with an unadapted language model in the first phase, and rescoring with a language model adapted online, could perform adequately, and keep the complexity independent of the number of clusters. 5 5.1 Evaluation Data and Methods We conduct all experiments with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). Log-linear weights are optimized using MERT (Och and Ney, 2003). We keep the word alignment and lexical reordering models constant through the experiments to minimize the number of confounding factors. We report translation quality using B LEU (Papineni et 5 If the development set is labelled, one can also use a gold segmentation of development sets instead of k-means clustering. At decoding time, cluster assignment can be performed by automatically assigning each sentence to the closest centroid, or again through gold labels, if availab"
P13-1082,D09-1074,0,0.52833,"mixture-modeling techniques at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 B LEU over unadapted systems and single-domain adaptation. 1 Introduction The effectiveness of domain adaptation approaches such as mixture-modeling (Foster and Kuhn, 2007) has been established, and has led to research on a wide array of adaptation techniques in SMT, for instance (Matsoukas et al., 2009; Shah et al., 2012). In all these approaches, adaptation is performed during model training, with respect to a representative development corpus, and the models are kept unchanged when then system is deployed. Therefore, when working with multiple and/or unlabelled domains, domain adaptation is often impractical for a number of reasons. Firstly, maintaining multiple systems for each language pair, each adapted to a different domain, is costly 2 Related Work (Ortiz-Mart´ınez et al., 2010) delay the computation of translation model features for the purpose of interactive machine translation wit"
P13-1082,2010.amta-papers.16,0,0.386013,"Missing"
P13-1082,J03-1002,0,0.00272636,"total (train) dev test sentences 1 270 000 830 000 30 000 110 000 370 000 2 840 000 970 000 6 420 000 3500 3500 words (en) 25 600 000 13 700 000 490 000 2 550 000 3 930 000 21 200 000 7 270 000 74 700 000 50 700 49 600 Table 3: Parallel data sets Czech–English. pass decoding, with an unadapted language model in the first phase, and rescoring with a language model adapted online, could perform adequately, and keep the complexity independent of the number of clusters. 5 5.1 Evaluation Data and Methods We conduct all experiments with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). Log-linear weights are optimized using MERT (Och and Ney, 2003). We keep the word alignment and lexical reordering models constant through the experiments to minimize the number of confounding factors. We report translation quality using B LEU (Papineni et 5 If the development set is labelled, one can also use a gold segmentation of development sets instead of k-means clustering. At decoding time, cluster assignment can be performed by automatically assigning each sentence to the closest centroid, or again through gold labels, if available. 836 system baseline 1 cluster (no split) 2 clusters"
P13-1082,N10-1079,0,0.0175553,"Missing"
P13-1082,W12-3102,0,0.0477569,"Missing"
P13-1082,P02-1040,0,0.11158,"Missing"
P13-1082,P11-2031,0,0.0162457,"ers gold clusters TM adaptation IT LEGAL 21.1 49.9 21.3* 49.9 21.6* 49.9 21.7* 49.9 22.1* 49.9 21.1 49.9 21.8* 50.1* LM adaptation IT LEGAL 21.1 49.9 21.8* 49.7 22.2* 50.4* 23.1* 50.2* 23.1* 50.1* 22.6* 50.3* 22.4* 50.1* TM+LM adaptation IT LEGAL 21.1 49.9 21.8* 49.8 22.8* 50.2* 22.6* 50.2* 22.7* 50.3* 21.9* 50.1* 23.2* 49.9 Table 4: Translation experiments EN–DE. B LEU scores reported. Data set kde kdedoc kdegb oo oo3 php tm acquis dgt ecb ep7 nc7 al., 2002). We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al., 2011). Systems significantly better than the baseline with p < 0.01 are marked with (*). We conduct experiments on two data sets. The first is an English–German translation task with two domains, texts related to information technology (IT) and legal documents (LEGAL). We use data sets from both domains, plus out-of-domain corpora, as shown in table 2. 7 data sets come from the domain IT: 6 from OPUS (Tiedemann, 2009) and a translation memory (tm) provided by our industry partner. 3 data sets are from the legal domain: the ECB corpus from OPUS, plus the JRCAcquis (Steinberger et al., 2006) and DGT-"
P13-1082,P12-1099,0,0.0641363,"eight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea, with the main difference that the actual combination is delayed until decoding, to support adaptation to multiple domains in a single system. (Razmara et al., 2012) describe an ensemble decoding framework which combines several translation models in the decoding step. Our work is similar to theirs in that the combination is done at runtime, but we also delay the computation of translation model probabilities, and thus have access to richer sufficient statistics. In principle, our architecture can support all mixture operations that (Razmara et al., 2012) describe, plus additional ones such as forms of instance weighting, which are not possible after the translation probabilities have been computed. (Banerjee et al., 2010) focus on the problem of domain i"
P13-1082,2012.eamt-1.43,1,0.814142,"ures: p(s|t), lex(s|t), p(t|s) and lex(t|s). s and t denote the source and target phrase. We follow the definitions in (Koehn et al., 2003). Traditionally, the phrase translation probabilities p(s|t) and p(t|s) are estimated through unsmoothed maximum likelihood estimation (MLE). weight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea, with the main difference that the actual combination is delayed until decoding, to support adaptation to multiple domains in a single system. (Razmara et al., 2012) describe an ensemble decoding framework which combines several translation models in the decoding step. Our work is similar to theirs in that the combination is done at runtime, but we also delay the computation of translation model probabilities, and thus have access to richer sufficient statistics. In princ"
P13-1082,E12-1055,1,0.846285,"ures: p(s|t), lex(s|t), p(t|s) and lex(t|s). s and t denote the source and target phrase. We follow the definitions in (Koehn et al., 2003). Traditionally, the phrase translation probabilities p(s|t) and p(t|s) are estimated through unsmoothed maximum likelihood estimation (MLE). weight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea, with the main difference that the actual combination is delayed until decoding, to support adaptation to multiple domains in a single system. (Razmara et al., 2012) describe an ensemble decoding framework which combines several translation models in the decoding step. Our work is similar to theirs in that the combination is done at runtime, but we also delay the computation of translation model probabilities, and thus have access to richer sufficient statistics. In princ"
P13-1082,2012.amta-papers.21,1,0.867317,"ues at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 B LEU over unadapted systems and single-domain adaptation. 1 Introduction The effectiveness of domain adaptation approaches such as mixture-modeling (Foster and Kuhn, 2007) has been established, and has led to research on a wide array of adaptation techniques in SMT, for instance (Matsoukas et al., 2009; Shah et al., 2012). In all these approaches, adaptation is performed during model training, with respect to a representative development corpus, and the models are kept unchanged when then system is deployed. Therefore, when working with multiple and/or unlabelled domains, domain adaptation is often impractical for a number of reasons. Firstly, maintaining multiple systems for each language pair, each adapted to a different domain, is costly 2 Related Work (Ortiz-Mart´ınez et al., 2010) delay the computation of translation model features for the purpose of interactive machine translation with online training. T"
P13-1082,steinberger-etal-2006-jrc,0,0.0294506,"with MultEval (Clark et al., 2011). Systems significantly better than the baseline with p < 0.01 are marked with (*). We conduct experiments on two data sets. The first is an English–German translation task with two domains, texts related to information technology (IT) and legal documents (LEGAL). We use data sets from both domains, plus out-of-domain corpora, as shown in table 2. 7 data sets come from the domain IT: 6 from OPUS (Tiedemann, 2009) and a translation memory (tm) provided by our industry partner. 3 data sets are from the legal domain: the ECB corpus from OPUS, plus the JRCAcquis (Steinberger et al., 2006) and DGT-TM (Steinberger et al., 2012). 2 data sets are out-ofdomain, made available by the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The development sets are random samples from the respective in-domain bitexts (heldout from training). The test sets have been provided by Translated, our industry partner in the M ATE C AT project. λIT 1.0 0.64 1.6 0.76 1.8 0.79 1.3 0.024 0.053 0.071 0.037 0.1 λLEGAL 1.0 12.0 2.3 1.6 4.7 6.3 1.3 3.5 4.5 2.3 0.53 1.1 λcluster 1 1.0 86.0 1.7 0.73 2.4 0.69 1.5 0.018 0.033 0.039 0.024 0.063 λcluster 2 1.0 6.4 2.7 1.7 2.7 3.5 1."
P13-1082,steinberger-etal-2012-dgt,0,0.00728276,"Missing"
P13-1082,D07-1054,0,0.0206694,"n model probabilities, and thus have access to richer sufficient statistics. In principle, our architecture can support all mixture operations that (Razmara et al., 2012) describe, plus additional ones such as forms of instance weighting, which are not possible after the translation probabilities have been computed. (Banerjee et al., 2010) focus on the problem of domain identification in a multi-domain setting. They use separate translation systems for each domain, and a supervised setting, whereas we aim for a system that integrates support for multiple domains, with or without supervision. (Yamamoto and Sumita, 2007) propose unsupervised clustering at both training and decoding time. The training text is divided into a number of clusters, a model is trained on each, and during decoding, each sentence is assigned to the closest cluster-specific model. Our approach bears resemblance to this clustering, but is different in that Yamamoto and Sumita assign each sentence to the closest model, and use this model for decoding, whereas in our approach, each cluster is associated with a mixture of models that is optimized to the cluster, and the number of clusters need not be equal to the number of component models"
P18-2037,D11-1033,0,0.151389,"Missing"
P18-2037,Q17-1024,0,0.127103,"Missing"
P18-2037,D15-1166,0,0.0351777,"Missing"
P18-2037,N16-1083,0,0.0204288,"pecial token to indicate the target language. After training, the sum of the encoder output states is used to obtain a fixed size sentence representation. 3 4 Multilingual sentence embeddings We are aiming at an embedding of entire sentences in different languages into one joint space, with the goal that the distance in that space reflects their semantic difference, independently of the language. There are several works on learning multilingual sentence embeddings which could be used for that purpose, i.e. (Hermann and Blunsom, 2014; Pham et al., 2015; Zhou et al., 2016; Chandar et al., 2013; Mogadala and Rettinger, 2016). In this paper, we extend our initial approach (Schwenk and Douze, 2017). The underlying idea is to use multiple sequence encoders and decoders and to train them with N -way aligned corpora from the MT community. Instead of using one encoder for each language as in the original paper, we use a shared encoder which handles all the input languages. Joint encoders (and decoders) have already been used in NMT (Johnson et al., 2016). In contrast to that work, we do not use a special input token to indicate the target language. Our joint encoder has no information at all on the encoded language, or"
P18-2037,J05-4003,0,0.472311,"r NMT (van der Wees et al., 2017). It should be stressed that domain adaptation is different from filtering noisy training data. Data selection extracts the most relevant bitexts for the test set domain, but does not necessarily remove wrong translations, e.g. source and target sentences are both in-domain and well formed, but they are not mutual translations. There is a huge body of research on mining bitexts, e.g. by analyzing the name of WEB pages or links (Resnik and Smith, 2003). Another direction of research is to use cross-lingual information retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Rauf and Schwenk, 2009). There are some works which use joint embeddings in the process of filtering or mining bitexts. For instance, Gr´egoire and Langlais (2017) first embed sentences into two separate spaces. Then, a classifier is learned on labeled data to decide whether sentences are parallel or not. Our approach clearly outperforms this technique on the BUCC corpus (cf. section 4). Bouamor and Sajjad (2018) use averaged multilingual word embeddings to calculate a joint embedding of all senThe approach is generic, it can be applied to many language pairs and it is independent of the arc"
P18-2037,P16-1160,0,0.0144764,"called fairseq-py. It implements a convolutional model which achieves very competitive results (Gehring et al., 2017). We use this system to show the improvements obtained by filtering the standard training data and by integrating additional mined data. We will freely share this data so that it can be used to train different NMT architectures. In this work, we focus on translating from English into German using the WMT’14 data. This task was selected for two reasons: • it is the de-facto standard to evaluate NMT systems and many comparable results are available, e.g. (Sennrich et al., 2016b; Chunga et al., 2016; Wu et al., 2016; Gehring et al., 2017; Ashish Vaswani et al., 2017); • only a limited amount of parallel training data is available (4.5M sentences). 2.1M are high quality human translations and 2.4M are crawled and aligned sentences (Common Crawl corpus). As in other works, we use newstest-2014 as test set. However, in order to follow the standard WMT evaluation setting, we use mteval-v14.pl on untokenized hypothesis to calculate case-sensitive BLEU scores. Note that in some papers, BLEU is calculated with multi-bleu.perl on tokenized hypothesis. All our results are for one single system on"
P18-2037,W15-1512,0,0.0753927,"Missing"
P18-2037,E09-1003,1,0.887766,", 2017). It should be stressed that domain adaptation is different from filtering noisy training data. Data selection extracts the most relevant bitexts for the test set domain, but does not necessarily remove wrong translations, e.g. source and target sentences are both in-domain and well formed, but they are not mutual translations. There is a huge body of research on mining bitexts, e.g. by analyzing the name of WEB pages or links (Resnik and Smith, 2003). Another direction of research is to use cross-lingual information retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Rauf and Schwenk, 2009). There are some works which use joint embeddings in the process of filtering or mining bitexts. For instance, Gr´egoire and Langlais (2017) first embed sentences into two separate spaces. Then, a classifier is learned on labeled data to decide whether sentences are parallel or not. Our approach clearly outperforms this technique on the BUCC corpus (cf. section 4). Bouamor and Sajjad (2018) use averaged multilingual word embeddings to calculate a joint embedding of all senThe approach is generic, it can be applied to many language pairs and it is independent of the architecture of the machine"
P18-2037,J03-3002,0,0.574826,"Santamar´ıa and Axelrod, 2017). It was successfully used in many phrase-based MT systems, but it was reported to be less successful for NMT (van der Wees et al., 2017). It should be stressed that domain adaptation is different from filtering noisy training data. Data selection extracts the most relevant bitexts for the test set domain, but does not necessarily remove wrong translations, e.g. source and target sentences are both in-domain and well formed, but they are not mutual translations. There is a huge body of research on mining bitexts, e.g. by analyzing the name of WEB pages or links (Resnik and Smith, 2003). Another direction of research is to use cross-lingual information retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Rauf and Schwenk, 2009). There are some works which use joint embeddings in the process of filtering or mining bitexts. For instance, Gr´egoire and Langlais (2017) first embed sentences into two separate spaces. Then, a classifier is learned on labeled data to decide whether sentences are parallel or not. Our approach clearly outperforms this technique on the BUCC corpus (cf. section 4). Bouamor and Sajjad (2018) use averaged multilingual word embeddings to"
P18-2037,P16-1189,0,0.207503,"for the joint encoder. Once the system is learned, all the BLSTM 1 2 Experimental evaluation: BUCC shared task on mining bitexts Since 2017, the workshop on Building and Using Comparable Corpora (BUCC) is organizing a shared task to evaluate the performance of approaches to mine for parallel sentences in comparable corpora (Zweigenbaum et al., 2018). Table 1 summarizes the available data, and Table 2 the official results. Roughly a 40th of the sentences are aligned. The best performing system “VIC” is based on the so-called STACC method which was shown to achieve state-of-the-art performance (Etchegoyhen and Azpeitia, 2016). It combines probabilistic dictionaries, search for similar sentences in both directions and a decision module which explores various features (common word prefixes, numbers, capitalized true-case tokens, etc). This STACC system was improved and adapted to the BUCC tasks with a word weighting scheme which is optimized on the monolingual corpora, and a named entity penalty. This task adaption substantially improved the generic STACC approach (Azpeitia et al., 2018). The systems RALI (Gr´egoire and Langlais, 2017) and H2 (Bouamor and Sajjad, 2018) have been already described in section 2. NLP2C"
P18-2037,W17-2619,1,0.922457,"ncoder output states is used to obtain a fixed size sentence representation. 3 4 Multilingual sentence embeddings We are aiming at an embedding of entire sentences in different languages into one joint space, with the goal that the distance in that space reflects their semantic difference, independently of the language. There are several works on learning multilingual sentence embeddings which could be used for that purpose, i.e. (Hermann and Blunsom, 2014; Pham et al., 2015; Zhou et al., 2016; Chandar et al., 2013; Mogadala and Rettinger, 2016). In this paper, we extend our initial approach (Schwenk and Douze, 2017). The underlying idea is to use multiple sequence encoders and decoders and to train them with N -way aligned corpora from the MT community. Instead of using one encoder for each language as in the original paper, we use a shared encoder which handles all the input languages. Joint encoders (and decoders) have already been used in NMT (Johnson et al., 2016). In contrast to that work, we do not use a special input token to indicate the target language. Our joint encoder has no information at all on the encoded language, or what will be done with sentence representation. We trained this architec"
P18-2037,P16-1009,0,0.306376,"with N -way aligned corpora from the MT community. Instead of using one encoder for each language as in the original paper, we use a shared encoder which handles all the input languages. Joint encoders (and decoders) have already been used in NMT (Johnson et al., 2016). In contrast to that work, we do not use a special input token to indicate the target language. Our joint encoder has no information at all on the encoded language, or what will be done with sentence representation. We trained this architecture on nine languages1 of the Europarl corpus with about 2M sentences each. We use BPE (Sennrich et al., 2016b) to learn one 20k joint vocabulary for all the nine languages.2 The joint encoder is a 3-layer BLSTM. The word embeddings are of size 384 and the hidden layer of the BLSTM is 512-dimensional. The 1024 dimensional sentence embedding is obtained by max-pooling over the BLSTM outputs. Dropout is set to 0.1. These settings are identical to those reported in (Schwenk and Douze, 2017), with the difference that we observe slight improvement by using a deeper network for the joint encoder. Once the system is learned, all the BLSTM 1 2 Experimental evaluation: BUCC shared task on mining bitexts Since"
P18-2037,P03-1010,0,0.350635,"ed to be less successful for NMT (van der Wees et al., 2017). It should be stressed that domain adaptation is different from filtering noisy training data. Data selection extracts the most relevant bitexts for the test set domain, but does not necessarily remove wrong translations, e.g. source and target sentences are both in-domain and well formed, but they are not mutual translations. There is a huge body of research on mining bitexts, e.g. by analyzing the name of WEB pages or links (Resnik and Smith, 2003). Another direction of research is to use cross-lingual information retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Rauf and Schwenk, 2009). There are some works which use joint embeddings in the process of filtering or mining bitexts. For instance, Gr´egoire and Langlais (2017) first embed sentences into two separate spaces. Then, a classifier is learned on labeled data to decide whether sentences are parallel or not. Our approach clearly outperforms this technique on the BUCC corpus (cf. section 4). Bouamor and Sajjad (2018) use averaged multilingual word embeddings to calculate a joint embedding of all senThe approach is generic, it can be applied to many language pairs and it"
P18-2037,P14-1006,0,0.0214297,"is trained on several languages pairs, similar to (Johnson et al., 2016), including a special token to indicate the target language. After training, the sum of the encoder output states is used to obtain a fixed size sentence representation. 3 4 Multilingual sentence embeddings We are aiming at an embedding of entire sentences in different languages into one joint space, with the goal that the distance in that space reflects their semantic difference, independently of the language. There are several works on learning multilingual sentence embeddings which could be used for that purpose, i.e. (Hermann and Blunsom, 2014; Pham et al., 2015; Zhou et al., 2016; Chandar et al., 2013; Mogadala and Rettinger, 2016). In this paper, we extend our initial approach (Schwenk and Douze, 2017). The underlying idea is to use multiple sequence encoders and decoders and to train them with N -way aligned corpora from the MT community. Instead of using one encoder for each language as in the original paper, we use a shared encoder which handles all the input languages. Joint encoders (and decoders) have already been used in NMT (Johnson et al., 2016). In contrast to that work, we do not use a special input token to indicate t"
P18-2037,D17-1147,0,0.0547424,"Missing"
P18-2037,P16-1162,0,0.211715,"with N -way aligned corpora from the MT community. Instead of using one encoder for each language as in the original paper, we use a shared encoder which handles all the input languages. Joint encoders (and decoders) have already been used in NMT (Johnson et al., 2016). In contrast to that work, we do not use a special input token to indicate the target language. Our joint encoder has no information at all on the encoded language, or what will be done with sentence representation. We trained this architecture on nine languages1 of the Europarl corpus with about 2M sentences each. We use BPE (Sennrich et al., 2016b) to learn one 20k joint vocabulary for all the nine languages.2 The joint encoder is a 3-layer BLSTM. The word embeddings are of size 384 and the hidden layer of the BLSTM is 512-dimensional. The 1024 dimensional sentence embedding is obtained by max-pooling over the BLSTM outputs. Dropout is set to 0.1. These settings are identical to those reported in (Schwenk and Douze, 2017), with the difference that we observe slight improvement by using a deeper network for the joint encoder. Once the system is learned, all the BLSTM 1 2 Experimental evaluation: BUCC shared task on mining bitexts Since"
P18-2037,P16-1133,0,0.0242465,"to (Johnson et al., 2016), including a special token to indicate the target language. After training, the sum of the encoder output states is used to obtain a fixed size sentence representation. 3 4 Multilingual sentence embeddings We are aiming at an embedding of entire sentences in different languages into one joint space, with the goal that the distance in that space reflects their semantic difference, independently of the language. There are several works on learning multilingual sentence embeddings which could be used for that purpose, i.e. (Hermann and Blunsom, 2014; Pham et al., 2015; Zhou et al., 2016; Chandar et al., 2013; Mogadala and Rettinger, 2016). In this paper, we extend our initial approach (Schwenk and Douze, 2017). The underlying idea is to use multiple sequence encoders and decoders and to train them with N -way aligned corpora from the MT community. Instead of using one encoder for each language as in the original paper, we use a shared encoder which handles all the input languages. Joint encoders (and decoders) have already been used in NMT (Johnson et al., 2016). In contrast to that work, we do not use a special input token to indicate the target language. Our joint encoder"
P19-1309,E09-1003,1,0.806099,"d during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1 (Schwenk, 2018; Guo et al., 2018). These methods use an NMT inspired encoder-decoder to train sentence embeddings on existing parallel data, which are then directly applied to retrieve and filter new parallel sentences using nearest neighbor retrieval over cosine similarity with a hard threshold (Espa˜na-Bonet et al., 2017; Hassan et al., 2018; Schwenk, 2018). In this paper, we argue that this retrieval me"
P19-1309,W17-2508,0,0.0277249,"snik, 1999; Shi et al., 2006). More recent methods focus on the textual content instead. For instance, Zipporah learns a classifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1 (Schwenk, 2018; Guo et al., 2018). These methods use an NMT inspired encoder-decoder to train sentence embeddings on existing parallel data, which are then directly applied to retrieve and filter new parallel sentences using nearest neighbor ret"
P19-1309,D18-1045,0,0.0795003,"Missing"
P19-1309,P16-1189,0,0.0333387,"information from web crawls (Resnik, 1999; Shi et al., 2006). More recent methods focus on the textual content instead. For instance, Zipporah learns a classifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1 (Schwenk, 2018; Guo et al., 2018). These methods use an NMT inspired encoder-decoder to train sentence embeddings on existing parallel data, which are then directly applied to retrieve and filter new parallel sentences usi"
P19-1309,W17-2509,0,0.0340842,"Missing"
P19-1309,W18-6317,0,0.348533,"lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1 (Schwenk, 2018; Guo et al., 2018). These methods use an NMT inspired encoder-decoder to train sentence embeddings on existing parallel data, which are then directly applied to retrieve and filter new parallel sentences using nearest neighbor retrieval over cosine similarity with a hard threshold (Espa˜na-Bonet et al., 2017; Hassan et al., 2018; Schwenk, 2018). In this paper, we argue that this retrieval method suffers from the scale of cosine similarity not being globally consistent. As illustrated by the example in Table 1, some sentences without any correct translation have overall high cosine scores, making them rank highe"
P19-1309,2005.mtsummit-papers.11,0,0.130667,"score: Combination of forward and backward candidates that, instead of discarding all inconsistent alignments, it selects those with the highest score. These candidates are then sorted according to their margin scores, and a threshold is applied. This can be either optimized on the development data, or adjusted to obtain the desired corpus size. 4 Experiments and results We next present our results on the BUCC mining task, UN corpus reconstruction, and machine translation over filtered ParaCrawl. All experiments use an English/French/Spanish/German multilingual encoder trained on Europarl v7 (Koehn, 2005) for 10 epochs. To cover all languages in BUCC, we use a separate English/French/Russian/Chinese model trained on the UN corpus (Ziemski et al., 2016) for 4 epochs. 4.1 BUCC mining task The shared task of the workshop on Building and Using Comparable Corpora (BUCC) is a wellestablished evaluation framework for bitext mining (Zweigenbaum et al., 2017, 2018). The task is 7 For efficiency, only the k nearest neighbors over cosine similarity are considered, where the neighborhood size k is the same as that used for the margin-based scoring. 3199 EN-DE Func. Retrieval en-de en-fr en-ru en-zh EN-FR"
P19-1309,W17-3204,0,0.0181334,"riments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the EnglishGerman ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version. 1 Introduction While Neural Machine Translation (NTM) has obtained breakthrough improvements in standard benchmarks, it is known to be particularly sensitive to the size and quality of the training data (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). In this context, effective approaches to mine and filter parallel corpora are crucial to apply NMT in practical settings. Traditional parallel corpus mining has relied on heavily engineered systems. Early approaches were mostly based on metadata information from web crawls (Resnik, 1999; Shi et al., 2006). More recent methods focus on the textual content instead. For instance, Zipporah learns a classifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distingu"
P19-1309,J05-4003,0,0.146822,"ssifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1 (Schwenk, 2018; Guo et al., 2018). These methods use an NMT inspired encoder-decoder to train sentence embeddings on existing parallel data, which are then directly applied to retrieve and filter new parallel sentences using nearest neighbor retrieval over cosine similarity with a hard threshold (Espa˜na-Bonet et al., 2017; Hassan et al., 2018; Schwenk, 2018). In this p"
P19-1309,P06-1011,0,0.159554,"Missing"
P19-1309,W18-6301,0,0.0969008,"orpus. This task does not require any filtering, so we use forward retrieval with the ratio margin function. As shown in Table 4, our system outperforms that of Guo et al. (2018) by a large margin despite using only a fraction of the training data (2M sentences from Europarl in contrast with over 400M sentences from Google’s internal data). 4.3 Filtering ParaCrawl for NMT Finally, we filter the English-German ParaCrawl corpus and evaluate NMT models trained on them. Our NMT models use fairseq’s implementation of the big transformer model (Vaswani et al., 2017), using the same configuration as Ott et al. (2018) and training for 100 epochs. Following common practice, we use newstest2013 and newstest2014 as our development and test sets, respectively, and report both tokenized and detokenized BLEU scores as computed by multi-bleu.perl and sacreBLEU. We decode with a beam size of 5 using an ensemble of the last 10 epochs. One single model is only slightly worse. Given the large size of ParaCrawl, we first preprocess it to remove all duplicated sentence pairs, 3200 28.0 ● DATA ● BLEU (detok) ● ● 27.5 ● ● BLEU tok detok ● 27.0 ● 26.5 26.0 ● 25.5 0 10 20 30 Sentences (millions) Figure 2: English-German De"
P19-1309,P99-1068,0,0.379918,"nt of more than one point over the best official filtered version. 1 Introduction While Neural Machine Translation (NTM) has obtained breakthrough improvements in standard benchmarks, it is known to be particularly sensitive to the size and quality of the training data (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). In this context, effective approaches to mine and filter parallel corpora are crucial to apply NMT in practical settings. Traditional parallel corpus mining has relied on heavily engineered systems. Early approaches were mostly based on metadata information from web crawls (Resnik, 1999; Shi et al., 2006). More recent methods focus on the textual content instead. For instance, Zipporah learns a classifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia e"
P19-1309,P18-2037,1,0.923169,"STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1 (Schwenk, 2018; Guo et al., 2018). These methods use an NMT inspired encoder-decoder to train sentence embeddings on existing parallel data, which are then directly applied to retrieve and filter new parallel sentences using nearest neighbor retrieval over cosine similarity with a hard threshold (Espa˜na-Bonet et al., 2017; Hassan et al., 2018; Schwenk, 2018). In this paper, we argue that this retrieval method suffers from the scale of cosine similarity not being globally consistent. As illustrated by the example in Table 1, some sentences without any correct translation have overall high cosine scores, mak"
P19-1309,N18-2074,0,0.0701821,"Missing"
P19-1309,P06-1062,0,0.116528,"n one point over the best official filtered version. 1 Introduction While Neural Machine Translation (NTM) has obtained breakthrough improvements in standard benchmarks, it is known to be particularly sensitive to the size and quality of the training data (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). In this context, effective approaches to mine and filter parallel corpora are crucial to apply NMT in practical settings. Traditional parallel corpus mining has relied on heavily engineered systems. Early approaches were mostly based on metadata information from web crawls (Resnik, 1999; Shi et al., 2006). More recent methods focus on the textual content instead. For instance, Zipporah learns a classifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018)."
P19-1309,P03-1010,0,0.109236,"ance, Zipporah learns a classifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1 (Schwenk, 2018; Guo et al., 2018). These methods use an NMT inspired encoder-decoder to train sentence embeddings on existing parallel data, which are then directly applied to retrieve and filter new parallel sentences using nearest neighbor retrieval over cosine similarity with a hard threshold (Espa˜na-Bonet et al., 2017; Hassan et al., 2018;"
P19-1309,D17-1319,0,0.0407953,"ches to mine and filter parallel corpora are crucial to apply NMT in practical settings. Traditional parallel corpus mining has relied on heavily engineered systems. Early approaches were mostly based on metadata information from web crawls (Resnik, 1999; Shi et al., 2006). More recent methods focus on the textual content instead. For instance, Zipporah learns a classifier ∗ This work was performed during an internship at Facebook AI Research. Holger Schwenk Facebook AI Research schwenk@fb.com over bag-of-word features to distinguish between ground truth translations and synthetic noisy ones (Xu and Koehn, 2017). STACC uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Many of these approaches rely on cross-lingual document retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005, 2006; Abdul-Rauf and Schwenk, 2009) or machine translation (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018). More recently, a new research line has shown promising results using multilingual sentence embeddings alone1"
P19-1309,L16-1561,0,0.0366688,"est score. These candidates are then sorted according to their margin scores, and a threshold is applied. This can be either optimized on the development data, or adjusted to obtain the desired corpus size. 4 Experiments and results We next present our results on the BUCC mining task, UN corpus reconstruction, and machine translation over filtered ParaCrawl. All experiments use an English/French/Spanish/German multilingual encoder trained on Europarl v7 (Koehn, 2005) for 10 epochs. To cover all languages in BUCC, we use a separate English/French/Russian/Chinese model trained on the UN corpus (Ziemski et al., 2016) for 4 epochs. 4.1 BUCC mining task The shared task of the workshop on Building and Using Comparable Corpora (BUCC) is a wellestablished evaluation framework for bitext mining (Zweigenbaum et al., 2017, 2018). The task is 7 For efficiency, only the k nearest neighbors over cosine similarity are considered, where the neighborhood size k is the same as that used for the margin-based scoring. 3199 EN-DE Func. Retrieval en-de en-fr en-ru en-zh EN-FR P R F1 P R F1 Forward Abs. Backward (cos) Intersection Max. score 78.9 79.0 84.9 83.1 75.1 73.1 80.8 77.2 77.0 75.9 82.8 80.1 82.1 77.2 83.6 80.9 74.2"
Q19-1038,D15-1075,0,0.0292828,". Note, moreover, that the multilingual sentence embeddings are fixed and not fine-tuned on the task or the language. We report our results in Table 2, along with several baselines from Conneau et al. (2018b) and the multilingual BERT model (Devlin et al., 2019).9 Our proposed method obtains the best results in zero-shot cross-lingual transfer for all languages but Spanish. Moreover, our transfer results are strong and homogeneous across all languages: For 11 of them, the zero-short performance 4.1 XNLI: Cross-lingual NLI NLI has become a widely used task to evaluate sentence representations (Bowman et al., 2015; Williams et al., 2018). Given two sentences, a premise and a hypothesis, the task consists in deciding whether there is an entailment, contradiction, or neutral relationship between them. XNLI is a recent effort to create a data set similar to the English MultiNLI for several languages (Conneau et al., 2018b). It consists of 2,500 development and 5,000 test instances 9 Note that the multilingual variant of BERT is not discussed in its paper (Devlin et al., 2019). Instead, the reported results were extracted from the README of the official GitHub project at https://github.com/googleresearch/b"
Q19-1038,D18-2029,0,0.0432684,"Missing"
Q19-1038,P18-1073,1,0.918557,", and machine translation (Cer et al., 2018; Subramanian et al., 2018). While the previous methods consider a single language at a time, multilingual representations have recently attracted a large attention. Most of this research focuses on cross-lingual word embeddings (Ruder et al., 2017), which are commonly learned jointly from parallel corpora (Gouws et al., 2015; Luong et al., 2015). An alternative approach that is becoming increasingly popular is to separately train word embeddings for each language, and map them to a shared space based on a bilingual dictionary (Mikolov et al., 2013a; Artetxe et al., 2018a) or even in a fully unsupervised manner (Conneau et al., 2018a; Artetxe et al., 2018b). Cross-lingual word embeddings are often used to build bag-of-word representations of longer linguistic units by taking their respective (IDF-weighted) average (Klementiev et al., 2012; Dufter et al., 2018). Although this approach has the advantage of requiring weak or no cross-lingual signal, it has been shown that the resulting sentence embeddings work poorly in practical cross-lingual transfer settings (Conneau et al., 2018b). Related Work Following the success of word embeddings (Mikolov et al., 2013b;"
Q19-1038,D17-1070,1,0.85713,"Missing"
Q19-1038,D18-1399,1,0.903589,", and machine translation (Cer et al., 2018; Subramanian et al., 2018). While the previous methods consider a single language at a time, multilingual representations have recently attracted a large attention. Most of this research focuses on cross-lingual word embeddings (Ruder et al., 2017), which are commonly learned jointly from parallel corpora (Gouws et al., 2015; Luong et al., 2015). An alternative approach that is becoming increasingly popular is to separately train word embeddings for each language, and map them to a shared space based on a bilingual dictionary (Mikolov et al., 2013a; Artetxe et al., 2018a) or even in a fully unsupervised manner (Conneau et al., 2018a; Artetxe et al., 2018b). Cross-lingual word embeddings are often used to build bag-of-word representations of longer linguistic units by taking their respective (IDF-weighted) average (Klementiev et al., 2012; Dufter et al., 2018). Although this approach has the advantage of requiring weak or no cross-lingual signal, it has been shown that the resulting sentence embeddings work poorly in practical cross-lingual transfer settings (Conneau et al., 2018b). Related Work Following the success of word embeddings (Mikolov et al., 2013b;"
Q19-1038,W17-2508,0,0.0610446,"Missing"
Q19-1038,D18-1269,1,0.918465,", 2018). While the previous methods consider a single language at a time, multilingual representations have recently attracted a large attention. Most of this research focuses on cross-lingual word embeddings (Ruder et al., 2017), which are commonly learned jointly from parallel corpora (Gouws et al., 2015; Luong et al., 2015). An alternative approach that is becoming increasingly popular is to separately train word embeddings for each language, and map them to a shared space based on a bilingual dictionary (Mikolov et al., 2013a; Artetxe et al., 2018a) or even in a fully unsupervised manner (Conneau et al., 2018a; Artetxe et al., 2018b). Cross-lingual word embeddings are often used to build bag-of-word representations of longer linguistic units by taking their respective (IDF-weighted) average (Klementiev et al., 2012; Dufter et al., 2018). Although this approach has the advantage of requiring weak or no cross-lingual signal, it has been shown that the resulting sentence embeddings work poorly in practical cross-lingual transfer settings (Conneau et al., 2018b). Related Work Following the success of word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interes"
Q19-1038,P18-1141,0,0.0478716,"Missing"
Q19-1038,P18-1031,0,0.0254501,"er of languages is limited to word embeddings (Ammar et al., 2016; Dufter et al., 2018) specific applications like typology prediction (Malaviya et al., 2017) or machine translation (Neubig and Hu, 2018)—ours being the first paper exploring general purpose massively multilingual sentence representations. All the previous approaches learn a fixedlength representation for each sentence. A recent research line has obtained very strong results using variable-length representations instead, consisting of contextualized embeddings of the words in the sentence (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). For that purpose, these methods train either an RNN or self-attentional encoder over unnanotated corpora using some form of language modeling. A classifier can then be learned on top of the resulting encoder, which is commonly further fine-tuned during this supervised training. Concurrent to our work, Lample and Conneau (2019) propose a cross-lingual extension of these models, and report strong results in cross-lingual NLI, machine translation, and language modeling. In contrast, our focus is on scaling to a large number of languages, for which we argue that fixed-lengt"
Q19-1038,D18-1045,0,0.018187,"ffectiveness of our approach. We also introduce a new test set of multilingual similarity search in 112 languages, and show that our approach is competitive even for low-resource languages. To the best of our knowledge, this is the first successful exploration of general purpose massively multilingual sentence representations. In the future, we would like to explore alternative encoder architectures like self-attention (Vaswani et al., 2017). We would also like to explore strategies to exploit monolingual data, such as using pre-trained word embeddings, backtranslation (Sennrich et al., 2016; Edunov et al., 2018), or other ideas from unsupervised MT (Artetxe et al., 2018c; Lample et al., 2018). Finally, we would like to replace our language-dependant preprocessing with a language-agnostic approach like SentencePiece.12 Our implementation, the pre-trained encoder, and the multilingual test set are freely available at https://github.com/facebookresearch/ LASER. 5.1 Encoder Depth Table 5 reports the performance on the different tasks for encoders with 1, 3, or 5 layers. We were not able to achieve good convergence with deeper models. It can be seen that all tasks benefit from deeper models, in particular"
Q19-1038,Q17-1024,0,0.0608482,"Missing"
Q19-1038,C12-1089,0,0.263095,"s (Ruder et al., 2017), which are commonly learned jointly from parallel corpora (Gouws et al., 2015; Luong et al., 2015). An alternative approach that is becoming increasingly popular is to separately train word embeddings for each language, and map them to a shared space based on a bilingual dictionary (Mikolov et al., 2013a; Artetxe et al., 2018a) or even in a fully unsupervised manner (Conneau et al., 2018a; Artetxe et al., 2018b). Cross-lingual word embeddings are often used to build bag-of-word representations of longer linguistic units by taking their respective (IDF-weighted) average (Klementiev et al., 2012; Dufter et al., 2018). Although this approach has the advantage of requiring weak or no cross-lingual signal, it has been shown that the resulting sentence embeddings work poorly in practical cross-lingual transfer settings (Conneau et al., 2018b). Related Work Following the success of word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning continuous vector representations of longer linguistic units like sentences (Le and Mikolov, 2014; Kiros et al., 2015). These sentence embeddings are commonly obtained using a recurrent neural net"
Q19-1038,N18-1202,0,0.0717113,"ions for a large number of languages is limited to word embeddings (Ammar et al., 2016; Dufter et al., 2018) specific applications like typology prediction (Malaviya et al., 2017) or machine translation (Neubig and Hu, 2018)—ours being the first paper exploring general purpose massively multilingual sentence representations. All the previous approaches learn a fixedlength representation for each sentence. A recent research line has obtained very strong results using variable-length representations instead, consisting of contextualized embeddings of the words in the sentence (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). For that purpose, these methods train either an RNN or self-attentional encoder over unnanotated corpora using some form of language modeling. A classifier can then be learned on top of the resulting encoder, which is commonly further fine-tuned during this supervised training. Concurrent to our work, Lample and Conneau (2019) propose a cross-lingual extension of these models, and report strong results in cross-lingual NLI, machine translation, and language modeling. In contrast, our focus is on scaling to a large number of languages, for which w"
Q19-1038,D14-1162,0,\N,Missing
Q19-1038,W15-1521,0,\N,Missing
Q19-1038,W17-2509,0,\N,Missing
Q19-1038,W17-2510,0,\N,Missing
Q19-1038,W17-2512,0,\N,Missing
Q19-1038,P18-2037,1,\N,Missing
Q19-1038,D18-1103,0,\N,Missing
Q19-1038,P16-1009,0,\N,Missing
W07-0409,J93-2003,0,0.00881613,"Missing"
W07-0409,2003.mtsummit-papers.6,0,0.029494,"mising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat65 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65–71, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics egories. The same idea has already been applied in (Hwang et al., 2007) to the Basic Travel Expression Corpus (BTEC). To our knowledge, th"
W07-0409,W06-2606,0,0.132853,"ge model) discards linguistic properties such as long term word dependency and word-order or phrase-order syntactic constraints. Therefore, explicit introduction of structure in the language models becomes a major and promising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat65 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65–"
W07-0409,W05-0821,0,0.314467,"stic properties such as long term word dependency and word-order or phrase-order syntactic constraints. Therefore, explicit introduction of structure in the language models becomes a major and promising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphosyntactic cat65 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65–71, c Rochester, New York, Apri"
W07-0409,N03-1017,0,0.0336336,"Missing"
W07-0409,P00-1056,0,0.147669,"., 2002) on a development set (Och and Ney, 2002). For this purpose, the public numerical optimization tool Condor (Berghen and Bersini, 2005) is integrated in the following iterative algorithm: 0. Using good general purpose weights, the Moses decoder is used to generate 1000-best lists. 1. The 1000-best lists are reranked using the current set of weights. 2. The current hypothesis is extracted and scored. Moses1 is an open-source, state-of-the-art phrasebased decoder. It implements an efficient beamsearch algorithm. Scripts are also provided to train a phrase-based model. The popular Giza++ (Och and Ney, 2000b) tool is used to align the parallel corpora. The baseline system uses 8 feature functions hi , namely phrase translation probabilities in both directions, lexical translation probabilities in both directions, a distortion feature, a word and a phrase 1 penalty and a trigram target language model. Additional features can be added, as described in the following sections. The weights λi are typically optimized so as to maximize a scoring function on a development set (Och and Ney, 2002). The moses decoder can output n-best lists, producing either distinct target sentences or not (as different s"
W07-0409,P02-1038,0,0.0171446,"ntences the one with the highest probability is chosen. The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process: e∗ = arg max Pr(e|f ) X = arg max{exp( λi hi (e, f ))} e (1) i where the feature functions hi are the system models characterizing the translation process, and the coefficients λi act as weights. 2.1 Moses decoder 2.2 Weight optimization A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al., 2002) on a development set (Och and Ney, 2002). For this purpose, the public numerical optimization tool Condor (Berghen and Bersini, 2005) is integrated in the following iterative algorithm: 0. Using good general purpose weights, the Moses decoder is used to generate 1000-best lists. 1. The 1000-best lists are reranked using the current set of weights. 2. The current hypothesis is extracted and scored. Moses1 is an open-source, state-of-the-art phrasebased decoder. It implements an efficient beamsearch algorithm. Scripts are also provided to train a phrase-based model. The popular Giza++ (Och and Ney, 2000b) tool is used to align the par"
W07-0409,N04-1021,0,0.0424268,"ering words to recover its syntactic structure. Modeling language generation as a word-based Markovian source (an ngram language model) discards linguistic properties such as long term word dependency and word-order or phrase-order syntactic constraints. Therefore, explicit introduction of structure in the language models becomes a major and promising focus of attention. However, as of today, it seems difficult to outperform a 4-gram word language model. Several studies have attempted to use morphosyntactic information (also known as part-of-speech or POS information) to improve translation. (Och et al., 2004) have explored many different feature functions. Reranking n-best lists using POS has also been explored by (Hasan et al., 2006). In (Kirchhoff and Yang, 2005), a factored language model using POS information showed similar performance to a 4-gram word language model. Syntax-based language models have also been investigated in (Charniak et al., 2003). All these studies use word phrases as translation units and POS information in just a post-processing step. This paper explores the integration of morphosyntactic information into the translation model itself by enriching words with their morphos"
W07-0409,P02-1040,0,0.0806419,"nce f . Among all possible target language sentences the one with the highest probability is chosen. The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process: e∗ = arg max Pr(e|f ) X = arg max{exp( λi hi (e, f ))} e (1) i where the feature functions hi are the system models characterizing the translation process, and the coefficients λi act as weights. 2.1 Moses decoder 2.2 Weight optimization A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al., 2002) on a development set (Och and Ney, 2002). For this purpose, the public numerical optimization tool Condor (Berghen and Bersini, 2005) is integrated in the following iterative algorithm: 0. Using good general purpose weights, the Moses decoder is used to generate 1000-best lists. 1. The 1000-best lists are reranked using the current set of weights. 2. The current hypothesis is extracted and scored. Moses1 is an open-source, state-of-the-art phrasebased decoder. It implements an efficient beamsearch algorithm. Scripts are also provided to train a phrase-based model. The popular Giza++ (Och and"
W07-0725,2003.mtsummit-tttt.3,0,0.0632032,"the 2007 WMT shared tasks. Several different translation strategies were explored. We also use a statistical language model that is based on a continuous representation of the words in the vocabulary. By these means we expect to take better advantage of the limited amount of training data. Finally, we have investigated the usefulness of a second reference translation of the development data. 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) e λi hi (e, f ))} (1) i 1 Introduction This paper describes the development of a statistical machine translation system based on the Moses decoder (Koehn et al., 2007) for the 2007 WMT shared tasks. Due to time constraints, we only considered the translation between French and English. A system with a similar architecture was successfully applied to the translation between Spanish and English in the framework of the 2007 T C -S TAR evaluation.1 For the 2"
W07-0725,P07-2045,0,0.0147479,"investigated the usefulness of a second reference translation of the development data. 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) e λi hi (e, f ))} (1) i 1 Introduction This paper describes the development of a statistical machine translation system based on the Moses decoder (Koehn et al., 2007) for the 2007 WMT shared tasks. Due to time constraints, we only considered the translation between French and English. A system with a similar architecture was successfully applied to the translation between Spanish and English in the framework of the 2007 T C -S TAR evaluation.1 For the 2007 WMT shared task a recipe is provided to build a baseline translation system using the Moses decoder. Our system differs in several aspects from this base-line: 1) the training data is not lower-cased; 2) Giza alignments are calculated on sentences of up to 90 words; 3) a two pass-decoding was used; and 4"
W07-0725,P02-1038,0,0.0348052,"baseline translation system using the Moses decoder. Our system differs in several aspects from this base-line: 1) the training data is not lower-cased; 2) Giza alignments are calculated on sentences of up to 90 words; 3) a two pass-decoding was used; and 4) a so called continuous space language model is used in order to take better advantage of the limited amount of training data. 1 X = arg max{exp( A paper on this work is submitted to MT Sumit 2007. The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted using the default settings of the Moses SMT toolkit. A target LM is then constructed as detailed in section 2.1. The translation itself is performed in two passes: first, Moses in run and a 1000bes"
W07-0725,2006.iwslt-papers.2,1,0.872924,"Missing"
W07-0725,N03-1017,0,\N,Missing
W07-0725,J03-1002,0,\N,Missing
W08-0313,2007.mtsummit-papers.18,1,0.861805,"Missing"
W08-0313,2003.mtsummit-tttt.3,0,0.312688,"ese systems seem to indicate that they have reached a level of performance allowing a human being to understand the automatic translations and to answer complicated questions on its content (Jones, 2008). In a joint project between the University of Le Mans and the company SYSTRAN, we try to build similar general purpose SMT systems for European languages. In the final version, these systems 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is"
W08-0313,P07-2045,0,0.015397,"Missing"
W08-0313,P02-1038,0,0.182453,"ean languages. In the final version, these systems 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. 119 Proceedings of the Third Workshop on Statistical Machine Translation, pages 119–122, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexic"
W08-0313,J03-1002,0,0.00432248,"indicate that they have reached a level of performance allowing a human being to understand the automatic translations and to answer complicated questions on its content (Jones, 2008). In a joint project between the University of Le Mans and the company SYSTRAN, we try to build similar general purpose SMT systems for European languages. In the final version, these systems 2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses S"
W08-0313,2006.iwslt-papers.2,1,0.884061,"Missing"
W08-0313,W07-0725,1,0.408966,"s: first, Moses is run and a 1000-best list is generated for each sentence. The parameters of Moses are tuned on devtest2006 for the Europarl task and nc-devtest2007 for the news task, using the cmert tool. These 1000-best lists are then rescored with a continuous space 5-gram LM and the weights of the feature functions are optimized again using the numerical optimization toolkit Condor (Berghen and Bersini, 2005). Note that this step operates only on the 1000-best lists, no re-decoding is performed. This basic architecture of the system is identical to the one used in the 2007 WMT evaluation(Schwenk, 2007a). 2.1 Translation model In the frame work of the 2008 WMT shared task, two parallel corpora were provided: the Europarl corpus (about 33M words) and the newscommentary corpus (about 1.2M words). It is known that the minutes of the debates of the European parliament use a particular jargon and these texts alone do not seem to be the appropriate to build a French/English SMT system for other texts. The more general news-commentary corpus is unfortunately rather small in size. Therefore, with the goal to build a general purpose system, we investigated whether more bilingual resources are availa"
W08-0313,2003.mtsummit-papers.53,0,0.070136,"Missing"
W08-0313,N03-1017,0,\N,Missing
W09-0423,D07-1090,0,0.0203199,"adding this data improves the overall system and they were not used in the final system, in order to keep the phrase-table small. We also performed experiments with the provided so-called bilingual French/English Gigaword corpus (575M English words in release 3). Again, we were not able to achieve any improvement by adding this data to the training material of the translation model. These findings are somehow surprising since it was eventually believed by the community that adding large amounts of bitexts should improve the translation model, as it is usually observed for the language model (Brants et al., 2007). In addition to these human generated bitexts, we also integrated a high quality bilingual dictionary from SYSTRAN. The entries of the dictionary were directly added to the bitexts. This technique has the potential advantage that the dictionary words could improve the alignments of these words when they also appear in the other bitexts. However, it is not guaranteed that multi-word expressions will be correctly aligned by GIZA++ and that only meaningful translations will actually appear in the phrase-table. A typical example is fire engine – camion de pompiers, for which the individual consti"
W09-0423,H93-1039,0,0.895104,"Missing"
W09-0423,W07-0732,1,0.833621,"rg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system SYSTRAN Eparl+NC Eparl+NC+AFP # En words Dev09a Dev09b Test09 41.6M 44.0M 51.7M 21.89 22.28 22.21 21.78 22.35# 21.43 23.80 24.13 23.88 44.2M 53.3M 18.68 23.03 22.95 18.84 23.15 23.15∗ 20.29 24.36 24.62 Table 3: Case sensitive NIST BLEU scores for the English-French systems. “NC” denotes the newscommentary bitexts, “dict” denotes SYSTRAN’s bilingual dictionary and “AFP” the automatically aligned news texts (∗ =primary,"
W09-0423,W08-0509,0,0.0200328,"ngual dictionary and “AFP” the automatically aligned news texts (∗ =primary, # =contrastive system) are given in Table 2. Adding the new news-train08 monolingual data had an important impact on the quality of the LM, even when the Gigaword data is already included. Data Vocabulary size Eparl+news + LDC Gigaword + Hansard and UN news-train08 alone all French 407k 248.8 142.2 137.5 165.0 120.6 The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).2 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. This previously caused problems when adding the entries of the bilingual dictionary to the bitexts. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses are tuned on news-dev2009a, using the cmert tool. The basic architecture of the system is identical to the one used in the 2008 WMT evaluation (Schwenk et al., 2008), but we did not use two pass decoding and n-best list rescoring with a continuous space language model. The results of t"
W09-0423,2003.mtsummit-tttt.3,0,0.0883033,"tems that include the additional AFP texts exhibit a bad generalisation behavior. We provide also the performance of the different systems on the official test set, calculated after the evaluation. In most of the cases, the observed improvements carry over on the test set. English 299k 416.7 194.9 187.5 245.9 174.8 Table 2: Perplexities on the development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture"
W09-0423,P07-2045,0,0.0198073,"Missing"
W09-0423,P02-1038,0,0.0848608,"e development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system"
W09-0423,J03-1002,0,0.0119318,"e additional AFP texts exhibit a bad generalisation behavior. We provide also the performance of the different systems on the official test set, calculated after the evaluation. In most of the cases, the observed improvements carry over on the test set. English 299k 416.7 194.9 187.5 245.9 174.8 Table 2: Perplexities on the development data of various language models. 4 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system D"
W09-0423,E09-1003,1,0.863217,"ines 580934–581316 and 599839–600662. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 130–134, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 130 French Gigaword English translations used as queries per day articles parallel sentences with extra words at ends candidate sentence pairs parallel sentences SMT FR EN length comparison tail removal + number / table removing 174M words WER 133M words 10.3M words 9.3M words +−5 day articles from English Gigaword Figure 1: Architecture of the parallel sentence extraction system (Rauf and Schwenk, 2009). (Brown et al., 1993). In comparison to our previous work (Schwenk et al., 2008), we also included all verbs in the French subjonctif and pass´e simple tense. In fact, those tenses seem to be frequently used in news material. In total about 10,000 verbs, 1,500 adjectives/adverbs and more than 100,000 noun forms were added. 2.2 word error rate is described in detail in (Rauf and Schwenk, 2009). 2.3 Monolingual data The French and English target language models were trained on all provided monolingual data. We realized that the news-train08 corpora contained some foreign texts, in particular in"
W09-0423,W08-0313,1,0.883458,"m based on the Moses decoder and a statistical post-editing system using SYSTRAN’s rule-based system. We also investigated techniques to automatically extract additional bilingual texts from comparable corpora. 1 Introduction This paper describes the machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2009 WMT shared task evaluation. This work was performed in cooperation with the company SYSTRAN. We only consider the translation between French and English (in both directions). The main differences to the previous year’s system (Schwenk et al., 2008) are as follows: better usage of SYSTRAN’s bilingual dictionary in the statistical system, less bilingual training data, additional language model training data (news-train08 as distributed by the organizers), usage of comparable corpora to improve the translation model, and development of a statistical post-editing system (SPE). These different components are described in the following. 2 Used Resources In the frame work of the 2009 WMT shared translation task many resources were made available. The following sections describe how they were used to train the translation and language models of"
W09-0423,2008.iwslt-papers.6,1,0.811009,"human evaluation. With respect to the SMT system, we were not able to improve the translation model by adding large amounts of bitexts, although different 133 sources were available (Canadian Hansard, UN or WEB data). Eventually these corpora are too noisy or out-of-domain. On the other hand, the integration of a high quality bilingual dictionary was helpful, as well as the automatic alignment of news texts from comparable corpora. Future work will concentrate on the integration of previously successful techniques, in particular continuous space language models and lightlysupervised training (Schwenk, 2008). We also believe that the tokenization could be improved, in particular for the French sources texts. Numbers, dates and other numerical expressions could be translated by a rule-based system. System combination has recently shown to provide important improvements of translation quality. We are currently working on a combination of the SMT and SPE system. It may be also interesting to add a third (hierarchical) MT system. 7 Acknowledgments This work has been partially funded by the French Government under the project I NSTAR (ANR JCJC06 143038) and the by the Higher Education Commission, Paki"
W09-0423,W07-0728,0,0.0556224,"arg max p(e|f ) X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 5 Architecture of the SPE system During the last years statistical post-editing systems have shown to achieve very competitive performance (Simard et al., 2007; Dugast et al., 2007). The main idea of this techniques is to use 2 The source is available at http://www.cs.cmu. edu/˜qing/ 132 Corpus SMT system Eparl+NC Eparl+NC+dict Eparl+NC+dict+AFP SPE system SYSTRAN Eparl+NC Eparl+NC+AFP # En words Dev09a Dev09b Test09 41.6M 44.0M 51.7M 21.89 22.28 22.21 21.78 22.35# 21.43 23.80 24.13 23.88 44.2M 53.3M 18.68 23.03 22.95 18.84 23.15 23.15∗ 20.29 24.36 24.62 Table 3: Case sensitive NIST BLEU scores for the English-French systems. “NC” denotes the newscommentary bitexts, “dict” denotes SYSTRAN’s bilingual dictionary and “AFP” the automatically aligned ne"
W09-0423,N03-1017,0,\N,Missing
W09-3109,2006.amta-papers.25,0,0.0302596,"w Translation Edit Rate plus (TERp). WER measures the number of operations required to transform one sentence into the other (insertions, deletions and substitutions). A zero WER would mean the two sentences are identical, subsequently lower WER sentence pairs would be sharing most of the common words. However two correct translations may differ in the order in which the words appear, something that WER is incapable of taking into account. This shortcoming is addressed by TER which allows block movements of words and thus takes into account the reorderings of words and phrases in translation (Snover et al., 2006). TERp is an extension of Translation Edit Rate and was one of the top performing metrics at the NIST Metric MATR workshop 4 . It had the highest absolute correlation, as measured by the Pearson correlation coefficient, with human judgments in 9 of the 45 test conditions. TERp tries to address the weaknesses of TER through the use of paraphrases, morphological stemming, and synonyms, as well as edit costs that are optimized to correlate better with various types of human judgments (Snover et al., 2009). The TER filter allows shifts if the two strings (the word sequence in the translated and th"
W09-3109,W04-3208,0,0.0447908,"es. Resnik and Smith (2003) propose their STRAND web-mining based system and show that their approach is able to find large numbers of similar document pairs. Works aimed at discovering parallel sentences include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. They identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on a sentence pair similarity score and use DP to find the least-cost alignment over the document pair. Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. They work on “very non-parallel corpora”. They then generate all possible sentence pairs and select the best ones based on a threshold on cosine similarity scores. Using the extracted sentences they learn a dictionary and iterate over with more sentence pairs. Recent work by Munteanu and Marcu (2005) uses a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using information retrieval (IR) t"
W09-3109,J93-1004,0,0.273133,"ding articles from the English Gigaword corpus (search space for IR). These day-specific files are then used for information retrieval using a robust information retrieval system. The Lemur IR toolkit (Ogilvie and Callan, 2001) was used for sentence extraction. 4.2 Candidate Sentence Pair Selection The information retrieval process gives us the potential parallel sentences per query sentence, the decision of their being parallel or not needs to be made about them. At this stage we choose the best scoring sentence as determined by the toolkit and pass the sentence pair through further filters. Gale and Church (1993) based their align program on the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. We initially used the same logic in our selection of the candidate sentence pairs. However our observation was that the filters that we use, WER, TER and TERp implicitly place a penalty when the length differThe information retrieval step is the most time consuming task in the whole system. The time taken depends upon various factors like size of the index to search in, length of"
W09-3109,D08-1090,0,0.0403792,"Missing"
W09-3109,N03-1015,0,0.0178847,"r experimental results and the paper concludes with a discussion and perspectives of this work. each other. Reliable identification of these pairs would enable the automatic creation of large and diverse parallel corpora. The ease of availability of these comparable corpora and the potential for parallel corpus as well as dictionary creation has sparked an interest in trying to make maximum use of these comparable resources, some of these works include dictionary learning and identifying word translations (Rapp, 1995), named entity recognition (Sproat et al., 2006), word sense disambiguation (Kaji, 2003), improving SMT performance using extracted parallel sentences (Munteanu and Marcu, 2005), (Rauf and Schwenk, 2009). There has been considerable amount of work on bilingual comparable corpora to learn word translations as well as discovering parallel sentences. Yang and Lee (2003) use an approach based on dynamic programming to identify potential parallel sentences in title pairs. Longest common sub sequence, edit operations and match-based score functions are subsequently used to determine confidence scores. Resnik and Smith (2003) propose their STRAND web-mining based system and show that th"
W09-3109,W09-0441,0,0.0146203,"ts of words and thus takes into account the reorderings of words and phrases in translation (Snover et al., 2006). TERp is an extension of Translation Edit Rate and was one of the top performing metrics at the NIST Metric MATR workshop 4 . It had the highest absolute correlation, as measured by the Pearson correlation coefficient, with human judgments in 9 of the 45 test conditions. TERp tries to address the weaknesses of TER through the use of paraphrases, morphological stemming, and synonyms, as well as edit costs that are optimized to correlate better with various types of human judgments (Snover et al., 2009). The TER filter allows shifts if the two strings (the word sequence in the translated and the IR retrieved sentence) match exactly, however TERp allows shifts if the words being shifted are exactly the same, are synonyms, stems or paraphrases of each other, or any such combination. This allows better sentence comparison by incorporation of sort of linguistic information about words. 5 Bitexts Baseline +WER-10 +WER-40 +WER-60 +WER-70 +TER-30 +TER-50 +TER-60 +TER-75 +TERp-10 +TERp-40 +TERp-60 +TERp-80 #words Arabic 5.8M 5.8M 7.2M 14.5M 20.4M 6.5M 12.5M 17.3M 24.1M 5.8M 10.2M 20.8M 27.7M BLEU Ev"
W09-3109,2003.mtsummit-tttt.3,0,0.0177559,"es; about 65.M Arabic words for XIN ). For our experiments on effect on SMT quality we use only the XIN corpus. We use the combination of AFP and XIN for comparison of sentences extracted by our approach with that of (Munteanu and Marcu, 2005). These translations are then treated as queries for the IR process. The design of our sentence extraction process is based on the heuristic that considering the corpus at hand, we The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) 1 LDC2003T07, 2004E72, T17, T18, 2005E46 and 2006E25. 2 LDC2005E83, 2006E24, E34, E85 and E92. 48 Arabic comparable corpus English translations used as queries per day articles parallel sentences with extra words at ends candidate sentence pairs parallel sentences SMT AR EN length comparison + number / table removing tail removal WER/TER/TERp +−5 day articles from English Gigaword Figure 1: Architecture of the parallel sentence extraction system. “the”,"
W09-3109,P06-1010,0,0.0280977,"and the post-processing. Section 5 summarizes our experimental results and the paper concludes with a discussion and perspectives of this work. each other. Reliable identification of these pairs would enable the automatic creation of large and diverse parallel corpora. The ease of availability of these comparable corpora and the potential for parallel corpus as well as dictionary creation has sparked an interest in trying to make maximum use of these comparable resources, some of these works include dictionary learning and identifying word translations (Rapp, 1995), named entity recognition (Sproat et al., 2006), word sense disambiguation (Kaji, 2003), improving SMT performance using extracted parallel sentences (Munteanu and Marcu, 2005), (Rauf and Schwenk, 2009). There has been considerable amount of work on bilingual comparable corpora to learn word translations as well as discovering parallel sentences. Yang and Lee (2003) use an approach based on dynamic programming to identify potential parallel sentences in title pairs. Longest common sub sequence, edit operations and match-based score functions are subsequently used to determine confidence scores. Resnik and Smith (2003) propose their STRAND"
W09-3109,P07-2045,0,0.00654924,"Missing"
W09-3109,J05-4003,0,0.125386,"multilingual encyclopedias like Wikipedia, Encarta etc. Such comparable corpora are widely available from LDC, in particular the Gigaword corpora, or over the WEB for many languages and domains, e.g. Wikipedia. They often contain many sentences that are reasonable translations of In this paper we present an extension of a successful simple and effective method for extracting parallel sentences from comparable corpora and we apply it to an Arabic/English NIST system. We experiment with a new TERp filter, along with WER and TER filters. We also report a comparison of our approach with that of (Munteanu and Marcu, 2005) using exactly the same corpora and show performance gain by using much lesser data. Our approach employs an SMT system built from small amounts of parallel texts to translate the source side of the nonparallel corpus. The target side texts are used, along with other corpora, in the language model of this SMT system. We then use information retrieval techniques and simple filters to create parallel data from a comparable news corpora. We evaluate the quality of the extracted data by showing that it significantly improves the performance of an SMT systems. 1 Introduction Parallel corpora, a req"
W09-3109,P03-1010,0,0.179218,"s been considerable amount of work on bilingual comparable corpora to learn word translations as well as discovering parallel sentences. Yang and Lee (2003) use an approach based on dynamic programming to identify potential parallel sentences in title pairs. Longest common sub sequence, edit operations and match-based score functions are subsequently used to determine confidence scores. Resnik and Smith (2003) propose their STRAND web-mining based system and show that their approach is able to find large numbers of similar document pairs. Works aimed at discovering parallel sentences include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. They identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on a sentence pair similarity score and use DP to find the least-cost alignment over the document pair. Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. They work on “very non-parallel corpora”. They then generate all possible sentence pairs and select the best ones based"
W09-3109,P02-1038,0,0.0552643,"e used the XIN corpus for all of our reported results and the collection of the AFP and XIN for comparison with ISI. Table 1 summarizes the characteristics of the corpora used. Note that the English part is much larger than the Arabic one (we found the same to be the case for French-English AFP comparable corpora that we used in our previous study). The number of words are given after tokenization. Source AFP XIN Arabic 138M 51M e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted using the default settings of the Moses SMT toolkit. The target 4-gram back-off language model is trained on the English part of all bitexts"
W09-3109,J03-1002,0,0.00382597,"c words for XIN ). For our experiments on effect on SMT quality we use only the XIN corpus. We use the combination of AFP and XIN for comparison of sentences extracted by our approach with that of (Munteanu and Marcu, 2005). These translations are then treated as queries for the IR process. The design of our sentence extraction process is based on the heuristic that considering the corpus at hand, we The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) 1 LDC2003T07, 2004E72, T17, T18, 2005E46 and 2006E25. 2 LDC2005E83, 2006E24, E34, E85 and E92. 48 Arabic comparable corpus English translations used as queries per day articles parallel sentences with extra words at ends candidate sentence pairs parallel sentences SMT AR EN length comparison + number / table removing tail removal WER/TER/TERp +−5 day articles from English Gigaword Figure 1: Architecture of the parallel sentence extraction system. “the”, are normally not ind"
W09-3109,P95-1050,0,0.172868,"our parallel sentence selection scheme and the post-processing. Section 5 summarizes our experimental results and the paper concludes with a discussion and perspectives of this work. each other. Reliable identification of these pairs would enable the automatic creation of large and diverse parallel corpora. The ease of availability of these comparable corpora and the potential for parallel corpus as well as dictionary creation has sparked an interest in trying to make maximum use of these comparable resources, some of these works include dictionary learning and identifying word translations (Rapp, 1995), named entity recognition (Sproat et al., 2006), word sense disambiguation (Kaji, 2003), improving SMT performance using extracted parallel sentences (Munteanu and Marcu, 2005), (Rauf and Schwenk, 2009). There has been considerable amount of work on bilingual comparable corpora to learn word translations as well as discovering parallel sentences. Yang and Lee (2003) use an approach based on dynamic programming to identify potential parallel sentences in title pairs. Longest common sub sequence, edit operations and match-based score functions are subsequently used to determine confidence score"
W09-3109,E09-1003,1,0.272979,", 6 August 2009. 2009 ACL and AFNLP formed by a maximum entropy classifier trained on parallel sentences. Bootstrapping is used and the size of the learned bilingual dictionary is increased over iterations to get better results. Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation edit rate (TER) to decide whether sentences are parallel or not. We also report an extension of our work (Rauf and Schwenk, 2009) by experimenting with an additional filter TERp, and building a named entity noun dictionary using the unknown words from the SMT (section 5.2). TERp has been tried encouraged by the outperformance of TER in our previous study on French-English. We have applied our technique on a different language pair Arabic-English, versus French-English that we reported the technique earlier on. Our use of full SMT sentences, gives us an added advantage of being able to detect one of the major errors of these approaches, also identified by (Munteanu and Marcu, 2005), i.e, the cases where the initial sente"
W09-3109,J03-3002,0,0.182612,"l corpus as an initial resource, our system exploits the target language side of the comparable corpus to attain the same goal, thus the comparable corpus itself helps to better extract possible parallel sentences. We have also presented a comparison with their approach and found our bitexts to achieve nice improvements using much less words. The LDC comparable corpora were used in this paper, but the same approach can be extended to extract parallel sentences from huge amounts of corpora available on the web by identifying comparable articles using techniques such as (Yang and Li, 2003) and (Resnik and Y, 2003).We have successfully applied our approach to French-English and ArabicEnglish language pairs. As this study strongly hinted towards language pair dependancy on the choice of the filter to use to select better sentences, we intend to investigate this trend in detail. Our ISI BLEU score on nist06 44 43.5 43 42.5 42 5 10 15 20 25 Arabic words for training [M] 42.5 30 Our ISI BLEU score on nist08 42 41.5 41 40.5 40 39.5 5 10 15 20 25 Arabic words for training [M] 30 Figure 5: BLEU scores on the NIST06 and NIST08 data using the ISI parallel corpus and our comparative extracted bitexts in function"
W09-3109,N03-1017,0,\N,Missing
W10-1716,E09-1003,1,0.842246,"sulting bitext has no new words in the English side, since all words of the translation output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retrieval techniques. The Lemur toolkit (Ogilvie and Callan, 2.4 Monolingual data Development data All development was done on news-test2008, and newstest2009 was used as internal test set. For all corpora except the French side of the bitexts used to train the French–English system (see above), the default Moses tokenization was used. However, we added abbreviations"
W10-1716,J93-2003,0,0.0276462,"Missing"
W10-1716,W09-0401,0,0.103917,"Missing"
W10-1716,2003.mtsummit-tttt.3,0,0.211376,"reviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the multi-bleu.perl tool and are case sensitive. The BLEU score was one of metrics with the best correlation with human ratings in last year evaluation (CallisonBurch et al., 2009) for the French–English and English–French directions. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: numbers in parentheses are the standard deviation of these three values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. Note that most of the improvements shown in the tables are small and not significant. However many of the gains are cumulative an"
W10-1716,2008.iwslt-papers.6,1,0.866721,"the UN corpus seem to be out-of domain for this task. We used two types of automatically extracted resources to adapt our system to the task domain. First, we generated automatic translations of the French News corpus provided (231M words), and selected the sentences with a normalised translation cost (returned by the decoder) inferior to a threshold. The resulting bitext has no new words in the English side, since all words of the translation output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using inform"
W10-1716,P07-2045,0,0.0135341,"Missing"
W10-1716,P02-1038,0,0.114464,"erence between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. Note that most of the improvements shown in the tables are small and not significant. However many of the gains are cumulative and the sum of several small gains makes a significant difference. e∗ = arg max p(e|f ) e X = arg max{exp( e λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are ex"
W10-1716,J03-1002,0,0.00523272,"rench tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the multi-bleu.perl tool and are case sensitive. The BLEU score was one of metrics with the best correlation with human ratings in last year evaluation (CallisonBurch et al., 2009) for the French–English and English–French directions. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: numbers in parentheses are the standard deviation of these three values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. Note that most of the improvements shown in the tables are small and not significant. However many of the gains are cumulative and the sum of several"
W10-1716,W09-0423,1,0.605151,"have been discarded because they contain many unknown or unfrequent n-grams. The lexical filter was based on the IBM model 1 cost (Brown et al., 1993) of each side of a sentence pair given the other side, normalised with respect to both sentence lengths. This filter Introduction This paper describes the machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2010 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2009) are as follows: restriction to the data recommended for the workshop, usage of the (filtered) French–English gigaword bitext, pruning of the phrase table, and usage of automatic translations of the monolingual news corpus to improve the translation model. We also used a larger amount of bilingual data extracted from comparable corpora than was done in 2009. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. 121 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and Metri"
W10-1716,D07-1103,0,\N,Missing
W10-1716,N03-1017,0,\N,Missing
W10-1716,W08-0509,0,\N,Missing
W10-1759,W07-0733,0,0.107222,"e used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two decoding paths were used to translate the text. Comparable corpora are exploited to find additional parallel texts. Information retrieval techniques are used to identify candidate sentences (Hildebrand et al., 2005). (Snover et al., 2008) used cross-lingual information retrieval to find texts in the target language that are related to the domain of the source texts. A self-enhancing approach was applied by (Ueffing, 2006) to filter the translations of the test set with the help"
W10-1759,D09-1074,0,0.091401,"table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. Then certain parts of the training bitexts were downweighted to optimize an objective function on the development data. This can lead to parameter over-fitting if the function that maps sentence features to weights is complex. The technique proposed in this paper is somehow related to the above approach of weighting the texts. Our method does not require an explicit specificatio"
W10-1759,P08-2040,0,0.0861323,"ate texts of all kinds. Therefore, it is the domain of the training resources that influences the translations that are selected among several choices. While monolingual 392 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracte"
W10-1759,W07-0722,0,0.104862,"organized as follows. The next section describes related work on weighting the corpora and model adaptation. Section 3 describes the architecture allowing to resample and to weight the bitexts. Experimental results are presented in section 4 and the paper concludes with a discussion. 2 Related Work Adaptation of SMT systems is a topic of increasing interest since few years. In previous work, adaptation is done by using mixture models, by exploiting comparable corpora and by selfenhancement of translation models. Mixture models were used to optimize the coefficients to the adaptation domain. (Civera and Juan, 2007) proposed a model that can be used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriat"
W10-1759,2009.mtsummit-posters.17,1,0.610293,"chine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. Then certain parts of the training bitexts were downweighted to optimize an objective function on the development data. This can lead to parameter over-fitting if the function"
W10-1759,W07-0717,0,0.0686785,"increasing interest since few years. In previous work, adaptation is done by using mixture models, by exploiting comparable corpora and by selfenhancement of translation models. Mixture models were used to optimize the coefficients to the adaptation domain. (Civera and Juan, 2007) proposed a model that can be used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two decoding paths were used to translate the text. Comparable corpora are exploited to find additional parallel texts. Information retrieval techniques are used to identify"
W10-1759,2008.iwslt-papers.6,1,0.88566,"op on Statistical Machine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. Then certain parts of the training bitexts were downweighted to optimize an objective function on the development data. This can lead to param"
W10-1759,2006.iwslt-papers.3,0,0.207015,"eighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two decoding paths were used to translate the text. Comparable corpora are exploited to find additional parallel texts. Information retrieval techniques are used to identify candidate sentences (Hildebrand et al., 2005). (Snover et al., 2008) used cross-lingual information retrieval to find texts in the target language that are related to the domain of the source texts. A self-enhancing approach was applied by (Ueffing, 2006) to filter the translations of the test set with the help of a confidence score and to use reliable alignments to train an additional 3 Description of the algorithm The architecture of the algorithm is summarized in figure 1. The starting point is an (arbitrary) number of parallel corpora. We first concatenate these bitexts and perform word alignments in both directions using GIZA++. This is done on the concatenated bitexts since GIZA++ may perform badly if some of the individual bitexts are rather small. Next, the alignments are separated in parts corre393 be used by the standard phrase extra"
W10-1759,P07-1004,0,0.100745,"ls depends of course on the quality and quantity of the available resources. Today, most SMT systems are generic, i.e. the same system is used to translate texts of all kinds. Therefore, it is the domain of the training resources that influences the translations that are selected among several choices. While monolingual 392 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392–399, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009)"
W10-1759,C04-1059,0,0.054191,"weight the bitexts. Experimental results are presented in section 4 and the paper concludes with a discussion. 2 Related Work Adaptation of SMT systems is a topic of increasing interest since few years. In previous work, adaptation is done by using mixture models, by exploiting comparable corpora and by selfenhancement of translation models. Mixture models were used to optimize the coefficients to the adaptation domain. (Civera and Juan, 2007) proposed a model that can be used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two"
W10-1759,D08-1090,0,\N,Missing
W10-1759,P07-2045,0,\N,Missing
W10-1759,2005.eamt-1.19,0,\N,Missing
W11-2132,2008.iwslt-papers.1,0,0.0203009,"nstance, Wuebker et al. (2010) proposed to translate the training data, using forced alignment and a leave-one-out technique, and to use the induced alignments to extract phrases. They have observed improvements with respect to word alignment obtained by GIZA++. On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses. They reported a very small drop in performance with respect to training word alignments with GIZA++. Similar ideas were also used in pivot translation. Bertoldi et al. (2008) translated from the pivot language to the source language to create parallel training data for the direct translation. 2.3 Treatment of unknown words Statistical machine translation systems have some trouble dealing with morphologically rich languages. It can happen, in function of the available training data, that translations of words are only Source language French finies effac´es hawaienne ... Source language stemmed form fini effac´e hawaien ... Target language English finished erased Hawaiian ... Table 1: Example of translations from French to English which are automatically extracted f"
W11-2132,P08-2040,0,0.346638,"te to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic"
W11-2132,W07-0722,0,0.0325184,"distribution of the existing phrases without necessarily modifying the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter"
W11-2132,W07-0717,0,0.0237288,"ions that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly u"
W11-2132,W08-0509,0,0.0164172,"Table 4: Case sensitive BLEU scores as a function of the amount of parallel training data. (Eparl=Europarl, nc=News Commentary, crawled1/2=sub-sampled crawled bitexts, un=sub-sampled United Nations bitexts). Corpus Bitexts: Europarl News Commentary United Nations Crawled (109 bitexts) Development data: newstest2009 newstest2010 Monolingual data: LDC Gigaword Crawled news English French 50.5M 2.9M 344M 667M 54.4M 3.3M 393M 794M 65k 62k 73k 71k 4.1G 2.6G 920M 612M structed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the MERT tool. For most of the runs, we performed three optimizations using different starting points and report average results. English and French texts were tokenised using a modified version of the tools of the Moses suite. Punctuation and case were preserved. Table 3: Available training data for the translation between French and English for the translation evaluation at WMT’11 (number of words a"
W11-2132,P08-2015,0,0.0150668,"ons of words are only Source language French finies effac´es hawaienne ... Source language stemmed form fini effac´e hawaien ... Target language English finished erased Hawaiian ... Table 1: Example of translations from French to English which are automatically extracted from the phrase-table with the stemmed form. known in some forms and not in others. For instance, for a user of MT technology it is quite difficult to understand why the system can translate the French word “je pense”1 , but not “tu penses”2 . There have been attempts in the literature to address this problem, for instance by Habash (2008) to deal with the Arabic language. It is actually possible to automatically infer possible translations when translating from a morphologically rich language, to a simpler language. In our case we use this approach to translate from French to English. Several of the unknown words are actually adjectives, nouns or verbs in a particular form that itself is not known, but the phrase table would contain the translation of a different form. As an example we can mention the French adjective finies which is in the female plural form. After stemming we may be able to find the translation in a dictiona"
W11-2132,D07-1103,0,0.0263124,"es of the phrase-table are automatically extracted from sentence aligned parallel data and they are usually quite noisy. It is not uncommon to encounter several hundreds, or even thousands of possible translations of frequent source phrases. Many of these automatically extracted translations are probably wrong and are never used since their probabilities are (fortunately) small in comparison to better translations. Therefore, several approaches were proposed to filter these phrase-tables, reducing considerably their size without any loss of the quality, or even achieving improved performance (Johnson et al., 2007). Given these observations, adaptation of the translation model of PBSMT systems could be performed by modifying the probability distribution of the existing phrases without necessarily modifying the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficie"
W11-2132,W07-0733,0,0.0618445,"the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most r"
W11-2132,P07-2045,0,0.00738019,"rench and English for the translation evaluation at WMT’11 (number of words after tokenisation). in July 2011. Preliminary results of this evaluation are available on the Internet.4 Table 3 summarizes the available training and development data. We optimized our systems on newstest2009 and used newstest2010 as internal test set. For both corpora, only one reference translations is available. Scoring was performed with NIST’s implementation of the BLEU score (‘mt-eval’ version 13). 3.1 Baseline system The baseline system is a standard phrase-based SMT system based on the the Moses SMT toolkit (Koehn et al., 2007). It uses fourteen features functions for translation, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. It is con4 http://matrix.statmt.org 288 The language models were trained on all the available data, i.e. the target side of the bitexts, the whole Gigaword corpus and the crawled monolingual data. We build 4-gram back-off LMs with the SRI LM toolkit using Modified Kneser-Ney and no cut-off on all the n-grams. Past experience has shown that keeping all n-grams"
W11-2132,W10-1716,1,0.860867,"perience has shown that keeping all n-grams slightly improves the performance although this produces quite huge models (10G and 30G of disk space for French and English respectively). Table 4 gives the baseline results using various amounts of bitexts. Starting with the Europarl and the News Commentary corpora, various amounts of human translated data were added. The organizers of the evaluation provide the so called 109 FrenchEnglish parallel corpus which contains almost 800 million words of data crawled from Canadian and European Internet pages. Following works from the 2010 WMT evaluation (Lambert et al., 2010), we filtered this data using IBM-1 probabilities and language model scores to keep only the most reliable translations. Two subsets were built with 115M and 232M English words respectively (using two differalignment giza reused giza reused moses Dev Test BLEU BLEU TER 27.34 (0.01) 27.40 (0.05) 27.42 (0.02) 29.80 (0.06) 29.82 (0.10) 29.77 (0.06) 55.34 (0.06) 55.30 (0.02) 55.27 (0.03) Table 5: Results for systems trained via different word alignment configurations. The values are the average over 3 MERT runs performed with different seeds. The numbers in parentheses are the standard deviation o"
W11-2132,J03-1002,0,0.0141338,"g as proposed by Ueffing (2006), lightly-supervised training does not adapt itself to the test data, but large amounts of monolingual training data are translated and a completely new model is built. This model can be applied to any test data, including a WEB service. In this paper we propose to extend this approach in several ways. First, we argue that the automatic translations should not be performed from the source to the target language, but in the opposite direction. Second, we propose to use the segmentation obtained during translation instead of performing word alignments with GIZA++ (Och and Ney, 2003) of the automatic translations. Finally, we propose to enrich the vocabulary of the adapted system by detecting untranslated words and automatically inferring possible translations from the stemmed form and the existing translations in the phrase table. This paper is organized as follows. In the next section we first describe our approach in detail. Section 3 describes the considered task, the available resources and the baseline PBSMT system. Results are summarized in section 4 and the paper concludes with a discussion and perspectives of this work. 2 Architecture of the approach In this pape"
W11-2132,2009.mtsummit-posters.17,1,0.691222,"a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing et al., 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. This approach was reported to obtain interesting improvements in the translations quality (Schwenk and Senellart, 2009; Bertoldi and Federico, 2009). In comparison to self enhancing as proposed by Ueffing (2006), lightly-supervised training does not adapt itself to the test data, but large amounts of monolingual training data are translated and a completely new model is built. This model can be applied to any test data, including a WEB service. In this paper we propose to extend this approach in several ways. First, we argue that the automatic translations should not be performed from the source to the target language, but in the opposite direction. Second, we propose to use the segmentation obtained during t"
W11-2132,2008.iwslt-papers.6,1,0.819379,"only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing et al., 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. This approach was reported to obtain interesting improvements in the translations quality (Schwenk and Senellart, 2009; Bertoldi and Federico, 2009). In comparison to self enhancing as proposed by Ueffing (2006), lightly-supervised training does not adapt itself to the test data, but large amounts of monolingual training data are translated and a completely new model is built. This model can be applied to any test data, including a WEB ser"
W11-2132,P07-1004,0,0.0831173,"that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such 285 an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing et al., 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008). A somehow related approach was named lightlysupervised training (Schwenk, 2008). In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. This approach was reported to obtain interesting improvements in the translations quality (Schwenk and Senellart, 2009; Bertoldi and Federico, 2009). In comparison to self enhancing as proposed by Ueffing (2006), lightly-supervised traini"
W11-2132,2006.iwslt-papers.3,0,0.283523,"l authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such 285 an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems t"
W11-2132,P10-1049,0,0.0221497,"f millions of words is a very time consuming step. Therefore we propose to use the segmentation into phrases and words obtained implicitly during the translation of the monolingual data with the moses toolkit. These alignments are simply added to the previously calculated alignments of the human translated bitexts and a new phrase table is built. This new procedure does not only speed-up the overall processing, but there are also investigations that these alignments obtained by decoding are more suitable to extract phrases than the symmetrized word alignments produced by GIZA++. For instance, Wuebker et al. (2010) proposed to translate the training data, using forced alignment and a leave-one-out technique, and to use the induced alignments to extract phrases. They have observed improvements with respect to word alignment obtained by GIZA++. On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses. They reported a very small drop in performance with respect to training word alignments with GIZA++. Similar ideas were also used in pivot translation. Bertoldi et al. (2008) transla"
W11-2132,C04-1059,0,0.024749,"cessarily modifying the entries. The idea is of course to increase the probabilities of translations that are appropriate to the task and to decrease the probabilities of the other ones. Ideally, we should also add new translations or source phrase, but this seems to be more challenging without any additional parallel data. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007), for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008). This mixture approach has the advantage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Another direction of research is self-enhancing of the translation model. This was first proposed by Ueffing (2006). The idea is to translate the test data, to filter the translations with help of a confidence"
W11-2132,W09-0432,0,\N,Missing
W11-2158,E09-1003,1,0.897701,"Missing"
W11-2158,J93-2003,0,0.0301933,"Missing"
W11-2158,J07-2003,0,0.0504628,"010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The hierarchical system"
W11-2158,W08-0509,0,0.0820888,"Missing"
W11-2158,P08-2015,0,0.0489218,"Missing"
W11-2158,D07-1103,0,0.0170723,"nal bitext added to Eparl+NC+1092 , we observe no improvement in French to English, and a very small improvement in English to French. However, added to the base468 line system (Eparl+NC+1092 ) adapted with the News data, the IR additional bitexts yield a small (0.2 BLEU) improvement in both translation directions. Final System In both translation directions our best system was the one trained on Eparl+NC+1092 +News+IR. We further achieved small improvements by pruning the phrase-table and by increasing the beam size. To prune the phrase-table, we used the ‘sigtest-filter’ available in Moses (Johnson et al., 2007), more precisely the α −  filter3 . We also build hierarchical systems on the various human translated corpora, using up to 323M words (corpora Eparl+NC+1092 ). The systems yielded similar results than the phrase-based approach, but required much more computational resources, in particular large amounts of main memory to perform the translations. Running the decoder was actually only possible with binarized rule-tables. Therefore, the hierarchical system was not used in the evaluation system. 3 The p-value of two-by-two contingency tables (describing the degree of association between a source"
W11-2158,2003.mtsummit-tttt.3,0,0.0945186,"oved from the Gigaword collections. 2.4 Development data All development was done on newstest2009, and newstest2010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicali"
W11-2158,P07-2045,0,0.0249647,"Missing"
W11-2158,W10-1716,1,0.404823,"o performed initial experiments with hierarchical systems. Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words. 1 Resources Used 2.1 Introduction This paper describes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2011 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Lambert et al., 2010) are as follows: use of more training data as provided by the organizers, improved translation model adaptation by unsupervised training, a continuous space language model for the translation into French, some attempts to automatically induce translations of unknown words and first experiments with hierarchical systems. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. Bilingual data Our system was developed in two stages. First, a baseline system was built to generate automatic translations"
W11-2158,W11-2132,1,0.92181,"contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). This year, we improved this method in the following way. In the original approach, the automatic translations are added to the human translated bitexts and a complete new system is build, including time consuming word alignment with GIZA++. For WMT’11, we directly used the word-to-word alignments produced by the decoder at the output instead of GIZA’s alignments. This speeds-up the procedure and yields the same results in our experiments. A detailed comparison is given in (Lambert et al., 2011). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. We used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retr"
W11-2158,P02-1038,0,0.26782,"Missing"
W11-2158,J03-1002,0,0.0108264,"rd collections. 2.4 Development data All development was done on newstest2009, and newstest2010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model"
W11-2158,2008.iwslt-papers.6,1,0.92273,"Missing"
W11-2158,N03-1017,0,\N,Missing
W12-2702,D07-1090,0,0.0277934,"ic level seems to be more important for language modeling in SMT than LVCSR. In both applications, so called back-off n-gram language models are the de facto standard since several decades. They were first introduced in the eighties, followed by intensive research on smoothing methods. An extensive comparison can be found in (Chen and Goodman, 1999). ModifiedKneser Ney smoothing seems to be the best performing method and it is this approach that is almost exclusively used today. Some years ago, there was a clear tendency in SMT to use huge LMs trained on hundreds on billions (101 1) of words (Brants et al., 2007). The authors report continuous improvement of the translation quality with increasing size of the LM training data, but these models require a large cluster to train and to perform inference using distributed storage. Therefore, several approaches were proposed to limit the storage size of large LMs, for instance (Federico and Cettolo, 2007; Talbot and Osborne, 2007; Heafield, 2011). 1.1 Continuous space language models The main drawback of back-off n-gram language models is the fact that the probabilities are estimated in a discrete space. This prevents any kind of interpolation in order to"
W12-2702,W07-0712,0,0.0178122,"Goodman, 1999). ModifiedKneser Ney smoothing seems to be the best performing method and it is this approach that is almost exclusively used today. Some years ago, there was a clear tendency in SMT to use huge LMs trained on hundreds on billions (101 1) of words (Brants et al., 2007). The authors report continuous improvement of the translation quality with increasing size of the LM training data, but these models require a large cluster to train and to perform inference using distributed storage. Therefore, several approaches were proposed to limit the storage size of large LMs, for instance (Federico and Cettolo, 2007; Talbot and Osborne, 2007; Heafield, 2011). 1.1 Continuous space language models The main drawback of back-off n-gram language models is the fact that the probabilities are estimated in a discrete space. This prevents any kind of interpolation in order to estimate the LM probability of an n-gram which was not observed in the training data. In order to attack this problem, it was proposed to project the words into a continuous space and to perform the estimation task in this space. The projection as well as the estimation can be jointly performed by a multi-layer neural network (Bengio and Duc"
W12-2702,W11-2123,0,0.0354967,"be the best performing method and it is this approach that is almost exclusively used today. Some years ago, there was a clear tendency in SMT to use huge LMs trained on hundreds on billions (101 1) of words (Brants et al., 2007). The authors report continuous improvement of the translation quality with increasing size of the LM training data, but these models require a large cluster to train and to perform inference using distributed storage. Therefore, several approaches were proposed to limit the storage size of large LMs, for instance (Federico and Cettolo, 2007; Talbot and Osborne, 2007; Heafield, 2011). 1.1 Continuous space language models The main drawback of back-off n-gram language models is the fact that the probabilities are estimated in a discrete space. This prevents any kind of interpolation in order to estimate the LM probability of an n-gram which was not observed in the training data. In order to attack this problem, it was proposed to project the words into a continuous space and to perform the estimation task in this space. The projection as well as the estimation can be jointly performed by a multi-layer neural network (Bengio and Ducharme, 2001; Bengio et al., 2003). The basi"
W12-2702,P07-2045,0,0.00531697,"Missing"
W12-2702,D10-1076,0,0.177724,"onsidered to be unfeasible to train back-off LMs on billions of words for orders larger than 5. The CSLM was very successfully applied to large vocabulary speech recognition. It is usually used to rescore lattices and improvements of the word error rate of about one point were consistently observed for many languages and domains, for instance (Schwenk and Gauvain, 2002; Schwenk, 2007; Park et al., 2010; Liu et al., 2011; Lamel et al., 2011). More recently, the CSLM was also successfully applied to statistical machine translation (Schwenk et al., 2006; Schwenk and Est`eve, 2008; Schwenk, 2010; Le et al., 2010) During the last years, several extensions were proposed in the literature, for instance: • Mikolov and his colleagues are working on the use of recurrent neural networks instead of multi-layer feed-forward architecture (Mikolov et al., 2010; Mikolov et al., 2011). • A simplified calculation of the short-list probability mass and the addition of an adaptation layer (Park et al., 2010; Liu et al., 2011) • the so-called SOUL architecture which allows to cover all the words at the output layer instead 13 of using a short-list (Le et al., 2011a; Le et al., 2011b), based on work by (Morin and Bengi"
W12-2702,P10-2041,0,0.0396083,"e sampling in large corpora (Xu et al., 2011) Despite significant and consistent gains in LVCSR and SMT, CSLMs are not yet in widespread use. Possible reasons for this could be the large computational complexity which requires flexible and carefully tuned software so that the models can be build and used in an efficient manner. In this paper we provide a detailed comparison of the current most promising language modeling techniques for SMT: huge back-off LMs that integrate all available data, LMs trained on data selected with respect to its relevance to the task by a recently proposed method (Moore and Lewis, 2010), and a new very efficient implementation of the CSLM which integrates data selection. 2 Continuous space LM toolkit Free software to train and use CSLM was proposed in (Schwenk, 2010). The first version of this toolkit provided no support for short lists or other means to train CSLMs with large output vocabularies. Therefore, it was not possible to use it for LVCSR and large SMT tasks. We extended our tool with full support for short lists during training and inference. Short lists are implemented as proposed in (Park et al., 2010), i.e. we add one extra output neuron for all words that are n"
W12-2702,P06-2093,1,0.830638,"Missing"
W12-2702,D07-1049,0,0.0114446,"er Ney smoothing seems to be the best performing method and it is this approach that is almost exclusively used today. Some years ago, there was a clear tendency in SMT to use huge LMs trained on hundreds on billions (101 1) of words (Brants et al., 2007). The authors report continuous improvement of the translation quality with increasing size of the LM training data, but these models require a large cluster to train and to perform inference using distributed storage. Therefore, several approaches were proposed to limit the storage size of large LMs, for instance (Federico and Cettolo, 2007; Talbot and Osborne, 2007; Heafield, 2011). 1.1 Continuous space language models The main drawback of back-off n-gram language models is the fact that the probabilities are estimated in a discrete space. This prevents any kind of interpolation in order to estimate the LM probability of an n-gram which was not observed in the training data. In order to attack this problem, it was proposed to project the words into a continuous space and to perform the estimation task in this space. The projection as well as the estimation can be jointly performed by a multi-layer neural network (Bengio and Ducharme, 2001; Bengio et al."
W12-2702,D11-1104,0,0.0130029,", for instance: • Mikolov and his colleagues are working on the use of recurrent neural networks instead of multi-layer feed-forward architecture (Mikolov et al., 2010; Mikolov et al., 2011). • A simplified calculation of the short-list probability mass and the addition of an adaptation layer (Park et al., 2010; Liu et al., 2011) • the so-called SOUL architecture which allows to cover all the words at the output layer instead 13 of using a short-list (Le et al., 2011a; Le et al., 2011b), based on work by (Morin and Bengio, 2005; Mnih and Hinton, 2008). • alternative sampling in large corpora (Xu et al., 2011) Despite significant and consistent gains in LVCSR and SMT, CSLMs are not yet in widespread use. Possible reasons for this could be the large computational complexity which requires flexible and carefully tuned software so that the models can be build and used in an efficient manner. In this paper we provide a detailed comparison of the current most promising language modeling techniques for SMT: huge back-off LMs that integrate all available data, LMs trained on data selected with respect to its relevance to the task by a recently proposed method (Moore and Lewis, 2010), and a new very effici"
W12-3147,E09-1003,1,0.887484,"Missing"
W12-3147,2011.mtsummit-papers.1,0,0.0352814,"were no differences when more training data is available. Due to time constraints, this procedure was not used in the submitted system. 4 Results and Discussion The results of our SMT systems are summarized in Table 2. The MT metric scores for the development set are the average of three optimisations performed with different seeds (see Section 3). For the test set, they are the average of four values: the three values corresponding to these different optimisations, plus a fourth value obtained by taking as weight for each model, the average of the weights obtained in the three optimisations (Cettolo et al., 2011). The numbers in parentheses are the standard deviation of these three or four values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. The results of Table 2 show that adding several adapted corpora (the filtered 109 corpus, the synBitext Translation : En→Fr Eparl+NC Eparl+NC+ntsXX Eparl+NC+ntsXX+109f Eparl+NC+ntsXX+109f +IR Eparl+NC+ntsXX+109f +news+IR Translation : Fr→En"
W12-3147,W08-0509,0,0.0287729,"Missing"
W12-3147,2003.mtsummit-tttt.3,0,0.0303475,". We had time to apply the domain-based data selection only for French. Thus all data were used for English. We used this method to filter the French–English 109 parallel corpus as well, based on the difference between in-domain cross-entropy and out-ofdomain cross-entropy calculated for each sentence of the English side of the corpus. We kept 49 million words (in the English side) to train our models, called 109f . 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . We have build phrase-based systems (Koehn et al., 2003; Och and Ney, 2003), using the standard log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X i λi hi (e, f ))} (1) The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och, 2003). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The"
W12-3147,P07-2045,0,0.0187376,"Missing"
W12-3147,W11-2132,1,0.887796,"Missing"
W12-3147,P10-2041,0,0.0502836,"cribes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2012 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2011) are as follows: (i) use of more training data as provided by the organizers and (ii) better selection of the monolingual and parallel data according to the domain, using the cross-entropy difference with respect to in-domain and out-of-domain language models (Moore and Lewis, 2010). We kept some previous features: the improvement of the translation model adaptation by unsupervised training, a parallel corpus retrieved by Information Retrieval (IR) techniques and finally, the rescoring with a continuous space target language model for the translation into French. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. Bilingual data The latest version of the News-Commentary (NC) corpus and of the Europarl (Eparl) corpus (version 7) were used. We also took as training data a s"
W12-3147,J03-1002,0,0.0037201,"ly the domain-based data selection only for French. Thus all data were used for English. We used this method to filter the French–English 109 parallel corpus as well, based on the difference between in-domain cross-entropy and out-ofdomain cross-entropy calculated for each sentence of the English side of the corpus. We kept 49 million words (in the English side) to train our models, called 109f . 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . We have build phrase-based systems (Koehn et al., 2003; Och and Ney, 2003), using the standard log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) = arg max{exp( e X i λi hi (e, f ))} (1) The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och, 2003). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on"
W12-3147,P03-1021,0,0.0190644,"Missing"
W12-3147,W11-2158,1,0.664778,"ase-based systems based on the Moses decoder, trained on the provided data only. Additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection. 1 Resources Used 2.1 Introduction This paper describes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2012 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2011) are as follows: (i) use of more training data as provided by the organizers and (ii) better selection of the monolingual and parallel data according to the domain, using the cross-entropy difference with respect to in-domain and out-of-domain language models (Moore and Lewis, 2010). We kept some previous features: the improvement of the translation model adaptation by unsupervised training, a parallel corpus retrieved by Information Retrieval (IR) techniques and finally, the rescoring with a continuous space target language model for the translation into French. These different points are des"
W12-3147,2008.iwslt-papers.6,1,0.898559,"Missing"
W12-3147,N03-1017,0,\N,Missing
W15-4006,D11-1033,0,0.0884987,"Missing"
W15-4006,N09-2038,0,0.025546,"one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many settings. Adaptation of the LM of an SMT models in an CAT environment was also investigated in several studies, e.g. (Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014). Adaptation to new data was also investigated in the neural network community, usually by some type of incremental training on a (subset) of the 2 3 3 Statistical Machine Translation In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is, applying Bayes rule: t∗ = arg max P (t|s) = arg max P (s|t)P (t) t t (1) The translation model P (s|t) is"
W15-4006,W12-3155,0,0.0124636,"tion coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many settings. Adaptation of the LM of an SMT models in an CAT environment was also investigated in several studies, e.g. (Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014). Adaptation to new data was also investigated in the neural network community, usually by some type of incremental training on a (subset) of the 2 3 3 Statistical Machine Translation In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is, applying Bayes rule: t∗ = arg max P (t|s) = arg max P (s|t)P (t) t t (1) The translation model P (s|t) is estimated from bitexts,"
W15-4006,P03-1021,0,0.00901973,"his idea was previously proposed in framework of a speech recognition system (Park et al., 2010). We build on this work and explore different variants of this technique. An interesting alternative is to keep the original architecture of the NN and to only modify one layer, e.g. the weights between two tanh layers in Figure 1. This variant will be explored in future work. Ney, 2003). The translation probabilities of these phrase pairs are usually estimated by simple relative frequency. The LM is normally a 4-gram back-off model. The log-linear approach is commonly used to consider more models (Och, 2003), instead of just a translation and language model: t∗ = arg max t M X λm hm (s, t), (2) m=1 where hm (s, t) are so-called feature functions. The weights λm are optimized during the tuning stage. In the Moses system, fourteen feature functions are usually used. Automatic evaluation of an SMT system remains an open question and many metrics have been proposed. In this study we use the BLEU score which measures the n-gram precision between the translation and a human reference translation (Papineni et al., 2002). Higher values mean better translation quality. 4 Adaptation schemes Continuous Spac"
W15-4006,P02-1040,0,0.0944133,"mally a 4-gram back-off model. The log-linear approach is commonly used to consider more models (Och, 2003), instead of just a translation and language model: t∗ = arg max t M X λm hm (s, t), (2) m=1 where hm (s, t) are so-called feature functions. The weights λm are optimized during the tuning stage. In the Moses system, fourteen feature functions are usually used. Automatic evaluation of an SMT system remains an open question and many metrics have been proposed. In this study we use the BLEU score which measures the n-gram precision between the translation and a human reference translation (Papineni et al., 2002). Higher values mean better translation quality. 4 Adaptation schemes Continuous Space Language Model The basic architecture of an CSLM is shown in Figure 1. The words are first projected onto a continuous representation, the remaining part of the network estimates the probabilities. Usually one tanh hidden and a softmax output layer are used, but recent studies have shown that deeper architecture perform better (Schwenk et al., 2014). We will use three tanh hidden and a softmax output layer as depicted in Figure 1. This type of architecture is now well known and the reader is referred to the"
W15-4006,P13-2119,0,0.0199103,"tion of a phoneme classifier based on TRAPS (Trmal et al., 2010). There are also a few publications which investigate adaptation of neural network language models, most of them very recent. The insertion of an additional adaption layer to perform speaker adaptation was proposed by Park et al. (Park et al., 2010). Earlier this idea was explored in (Yao et al., 2012) for speech recognition through an affine transform of the output layer. Adaptation through data selection was studied in (Jalalvand, 2013) (selection of sentences in out-of-domain corpora based on similarity between sentences) and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Related work Popular approaches to adapt the LM in an SMT system are mixture models, e.g. (Foster and Kuhn, 2007;"
W15-4006,W07-0717,0,0.0256608,"and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Related work Popular approaches to adapt the LM in an SMT system are mixture models, e.g. (Foster and Kuhn, 2007; Koehn and Schroeder, 2007) and data selection. In the former case, separate LMs are trained on the available corpora and are then merged into one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the mo"
W15-4006,R13-2013,0,0.021625,"to transfer features in convolutional networks (Yosinski et al., 2014), or research to perform speaker adaptation of a phoneme classifier based on TRAPS (Trmal et al., 2010). There are also a few publications which investigate adaptation of neural network language models, most of them very recent. The insertion of an additional adaption layer to perform speaker adaptation was proposed by Park et al. (Park et al., 2010). Earlier this idea was explored in (Yao et al., 2012) for speech recognition through an affine transform of the output layer. Adaptation through data selection was studied in (Jalalvand, 2013) (selection of sentences in out-of-domain corpora based on similarity between sentences) and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Re"
W15-4006,W07-0733,0,0.0342231,"(training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain). Several variants of curriculum learning are explored by Shi et al. to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014). Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used to rescore the n-best list, speed-up the rescoring process, adapt an LM and estimate the influence of history. Related work Popular approaches to adapt the LM in an SMT system are mixture models, e.g. (Foster and Kuhn, 2007; Koehn and Schroeder, 2007) and data selection. In the former case, separate LMs are trained on the available corpora and are then merged into one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many set"
W15-4006,C12-2104,1,0.88246,"Missing"
W15-4006,N03-1017,0,0.0157558,"tion In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s to a sentence in the target language t. Then, the fundamental equation of SMT is, applying Bayes rule: t∗ = arg max P (t|s) = arg max P (s|t)P (t) t t (1) The translation model P (s|t) is estimated from bitexts, bilingual sentence aligned data, and the language model P (t) from monolingual data in the target language. A popular approach are phrasebased models which translate short sequences of words together (Koehn et al., 2003; Och and https://www.matecat.com/ http://www.statmt.org/moses/ 49 All our experiments were performed with the open-source CSLM toolkit4 (Schwenk, 2013), which was extended for our purposes. A major challenge for neural network LMs is how to handle the words at the output layer since a the softmax normalization would be very costly for large vocabularies. Various solutions have been proposed: short-lists (Schwenk, 2007), a class decomposition (Mikolov et al., 2011) or an hierarchical decomposition (Le et al., 2011). In this work, we use short-lists, but our adaptation scheme could be equally a"
W15-4006,P07-2045,0,0.0463206,"Missing"
W15-4006,N12-1005,0,0.0311623,"Missing"
W15-4006,P10-2041,0,0.161083,"em are mixture models, e.g. (Foster and Kuhn, 2007; Koehn and Schroeder, 2007) and data selection. In the former case, separate LMs are trained on the available corpora and are then merged into one, the interpolation coefficients being estimated to minimize perplexity on an in-domain development corpus. This is known as linear mixture models. We can also integrate the various corpusspecific LMs as separate feature functions in the usual log-linear model of an SMT system. Data selection aims at extracting the most relevant subset of all the available LM training data. The approach proposed in (Moore and Lewis, 2010) has turned out to be the most effective one in many settings. Adaptation of the LM of an SMT models in an CAT environment was also investigated in several studies, e.g. (Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014). Adaptation to new data was also investigated in the neural network community, usually by some type of incremental training on a (subset) of the 2 3 3 Statistical Machine Translation In the statistical approach to machine translation, all models are automatically estimated from examples. Let us assume that we want to translate a sentence in the source language s"
W15-4006,J03-1002,0,\N,Missing
W15-4006,D14-1179,1,\N,Missing
W17-2619,S16-1081,0,0.0317607,"ctive is to move frequently aligned words closer in the joint embeddings space. DWA (Distributed Word Alignment) (Kocisk´y et al., 2014) learns word alignments and bilingual word embeddings simultaneously using translation probability as objecTwo main approaches have been used in the literature to evaluate multilingual sentence embeddings: 1) cross-lingual document classification based on the Reuters corpus, first described in (Klementiev et al., 2012); and 2) cross-lingual evaluation of semantic textual similarity (in short STS). This task was first introduced in the 2016 edition of SemEval (Agirre et al., 2016). Both tasks focus on the evaluation of joint sentence representations of two languages only. In the Reuters task, a document classifier is trained on English 160 sentence representations and then applied to texts in another language, and in the opposite direction respectively. STS seeks to measure the degree of semantic equivalence between two sentences (or small paragraphs). Semantic similarity is expressed by a score between 0 (the two sentences are completely dissimilar) and 5 (the two sentences are completely equivalent). In 2016, a cross lingual task was introduced (Es/En) and extended t"
W17-2619,P14-1006,0,0.0571582,"lar LSTMs, eg. (Cho et al., 2014), convolutional NNs, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg. (Zhao et al., 2015). In all these works, the sentence representations are learned for one language only. It is important to note that our multiple encoder/decoder architecture and the different training paths make no assumption on the type of encoder and decoder used. In principle, all these sentence representations methods could be used. This is left for future research. 3 There are several works on learning multilingual representations at document level (Hermann and Blunsom, 2014; Zhou et al., 2016b; Pham et al., 2015). (Hermann and Blunsom, 2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. (Pham et al., 2015) directly learn a vector representations for sentences in the absence of compositional property. (Zhou et al., 2016b) learn bilingual document representation by minimizing Euclidean distance between document representations and their translation. Evaluation protocol An important question is how to evaluate multilingual joint sentence embeddings. Let us first define some d"
W17-2619,Q17-1024,0,0.0701714,"Missing"
W17-2619,2012.eamt-1.60,0,0.00629218,"re language pairs in 2017 (Ar/En and Tr/En). In this work, we propose an additional evaluation framework for multilingual joint representations, based on similarity search. Our metric can be automatically calculated without the need of new human-labeled data and scaled to many languages and large corpora. We only need collections of S sentences, and their translations in L different languages, ie. spi , i = 1 . . . S, p = 1 . . . L. Such L-way parallel corpora are freely available, for instance Europarl2 (20 languages), the UN corpus, 6 languages (Ziemski et al., 2016), or TED, 23 languages, (Cettolo et al., 2012). reference one. This is difficult to handle automatically at large scale and counted as error by our algorithm. Similarity search mainly evaluates the multilingual closeness property and can be easily scaled to many languages. We will report results how the similarity error rate is influenced by the number of language pairs and the size of the corpus. We have compared three distance metrics: L2, inner product and cosine. In general, cosine performed best. Note that all metrics are equivalent if the vectors are normalized. 4 We have performed all our experiments with the freely available UN co"
W17-2619,D13-1176,0,0.0673096,"Missing"
W17-2619,C12-1089,0,0.0372799,"of a task, eg. classification, multilingual NMT or semantic relatedness. This requires that enough information is preserved in the representations to perform the task; • scalability to many languages: it is desirable that the metric can be extended to many languages without important computational cost or need for human labeling of data. Finally, many papers address the problem of learning bi- or multilingual word representations which are used to perform cross-lingual document classification. They are trained either on word alignments or sentence-aligned parallel corpora, or both. I-Matrix (Klementiev et al., 2012) uses word alignments to do multi-task learning, where each word is a single task and the objective is to move frequently aligned words closer in the joint embeddings space. DWA (Distributed Word Alignment) (Kocisk´y et al., 2014) learns word alignments and bilingual word embeddings simultaneously using translation probability as objecTwo main approaches have been used in the literature to evaluate multilingual sentence embeddings: 1) cross-lingual document classification based on the Reuters corpus, first described in (Klementiev et al., 2012); and 2) cross-lingual evaluation of semantic text"
W17-2619,P14-2037,0,0.0196002,"Missing"
W17-2619,D17-1070,1,0.0305726,"Missing"
W17-2619,D15-1131,0,0.0565283,"the input. This corresponds to training an auto-encoder which is easier than a translation model and may have an negative impact. 159 tive. Without using word alignments, BilBOWA (Gouews et al., 2014) optimizes both monolingual and bilingual objectives, uses Skip-gram as monolingual loss, while formulating the bilingual loss as Euclidean distance between bag-ofwords representations of aligned sentences. UnsupAlign (Luong et al., 2015b) learns bilingual word embeddings by extending the monolingual Skip-gram model with bilingual contexts based on word alignments within the sentence. TransGram (Coulmance et al., 2015) is similar to (Pham et al., 2015) but treats all words in the parallel sentence as context words, thus eliminating the need for word alignments. representations. Common approaches include: simple techniques like bag-of-words or some type of pooling, eg (Arora et al., 2017), recursive NNs, eg. (Socher et al., 2011), recurrent NNs, in particular LSTMs, eg. (Cho et al., 2014), convolutional NNs, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg. (Zhao et al., 2015). In all these works, the sentence representations are learned for one language only. It is importa"
W17-2619,W15-1521,0,0.435232,"e sequence length since it is difficult to encode long sequences into a single, fixed-size representation. A plausible solution is the so-called attention mechanism (Bahdanau et al., 2015): where the generation of each target word is conditioned on a weighted subset of source words, instead of the full sentence. NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015a; Firat et al., 2016a). In this work, we aim at learning multilingual sentence representations, i.e. which are independent of the language. Since we have to compare these representations among each other, for the same or between multiple languages, we only consider representations of fixed size. There are many motivations to learn such a multilingual sentence representation, in particular: In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the la"
W17-2619,P15-1166,0,0.0827081,"2seq models substantially degrades with the sequence length since it is difficult to encode long sequences into a single, fixed-size representation. A plausible solution is the so-called attention mechanism (Bahdanau et al., 2015): where the generation of each target word is conditioned on a weighted subset of source words, instead of the full sentence. NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015a; Firat et al., 2016a). In this work, we aim at learning multilingual sentence representations, i.e. which are independent of the language. Since we have to compare these representations among each other, for the same or between multiple languages, we only consider representations of fixed size. There are many motivations to learn such a multilingual sentence representation, in particular: In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a re"
W17-2619,N13-1090,0,0.0126695,"p to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages. 1 Introduction It is today common practice to use distributed representations of words, often called word embeddings, in almost all NLP applications. It has been shown that syntactic and semantic relations can be captured in this embedding space, see for instance (Mikolov et al., 2013). To process sequences of words, ie. sentences or small paragraphs, these word embeddings need to be “combined” into a representation of the whole sequence. Common approaches include: simple techniques like bag-of-words or some type of pooling, eg. (Arora et al., 2017), recursive neural networks, eg. (Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg. (Cho et al., 2014), convolutional neural networks, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg. (Zhao et al., 2015). In some NLP applications, both the input and output are sentences."
W17-2619,N16-1101,0,0.351281,"ce it is difficult to encode long sequences into a single, fixed-size representation. A plausible solution is the so-called attention mechanism (Bahdanau et al., 2015): where the generation of each target word is conditioned on a weighted subset of source words, instead of the full sentence. NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015a; Firat et al., 2016a). In this work, we aim at learning multilingual sentence representations, i.e. which are independent of the language. Since we have to compare these representations among each other, for the same or between multiple languages, we only consider representations of fixed size. There are many motivations to learn such a multilingual sentence representation, in particular: In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to"
W17-2619,N16-1083,0,0.019116,"ed properties of such embeddings: • multilingual closeness: the representations of the same sentence for different languages should be as similar as possible; • semantic closeness: similar sentences should be also close in the embeddings space, ie. sentences conveying the same meaning, but not necessarily the syntactic structure and word choice; Other multilingual sentence representation learning techniques include BAE (Chandar et al., 2013) which trains bilingual autoencoders with the objective of minimizing reconstruction error between two languages, and BRAVE (Bilingual paRAgraph VEctors) (Mogadala and Rettinger, 2016) which learns both bilingual word embeddings and sentence embeddings from either sentence-aligned parallel corpora (BRAVE-S), or label-aligned non-parallel corpora (BRAVE-D). • preservation of content: sentence representations are usually used in the context of a task, eg. classification, multilingual NMT or semantic relatedness. This requires that enough information is preserved in the representations to perform the task; • scalability to many languages: it is desirable that the metric can be extended to many languages without important computational cost or need for human labeling of data. F"
W17-2619,D16-1026,0,0.414506,"ce it is difficult to encode long sequences into a single, fixed-size representation. A plausible solution is the so-called attention mechanism (Bahdanau et al., 2015): where the generation of each target word is conditioned on a weighted subset of source words, instead of the full sentence. NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015a; Firat et al., 2016a). In this work, we aim at learning multilingual sentence representations, i.e. which are independent of the language. Since we have to compare these representations among each other, for the same or between multiple languages, we only consider representations of fixed size. There are many motivations to learn such a multilingual sentence representation, in particular: In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to"
W17-2619,W15-1512,0,0.205836,"Missing"
W17-2619,C16-1011,0,0.0144424,"shot NMT. To best of our knowledge, all these works focus on the improvement and extensions of seq2seq modeling, and fixed-sized vector representations have not analyzed in depth in a multilingual context. Several publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016). The usual approach is to optimize a distance or correlation between the two representations or predictive auto-encoders (Chandar et al., 2013). The same approach was applied to transliteration and captioning (Saha et al., 2016). There is a large body of research on sentence 1 One could also use the common output language at the input. This corresponds to training an auto-encoder which is easier than a translation model and may have an negative impact. 159 tive. Without using word alignments, BilBOWA (Gouews et al., 2014) optimizes both monolingual and bilingual objectives, uses Skip-gram as monolingual loss, while formulating the bilingual loss as Euclidean distance between bag-ofwords representations of aligned sentences. UnsupAlign (Luong et al., 2015b) learns bilingual word embeddings by extending the monolingual"
W17-2619,Q16-1027,0,0.0400958,"2014), convolutional NNs, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg. (Zhao et al., 2015). In all these works, the sentence representations are learned for one language only. It is important to note that our multiple encoder/decoder architecture and the different training paths make no assumption on the type of encoder and decoder used. In principle, all these sentence representations methods could be used. This is left for future research. 3 There are several works on learning multilingual representations at document level (Hermann and Blunsom, 2014; Zhou et al., 2016b; Pham et al., 2015). (Hermann and Blunsom, 2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. (Pham et al., 2015) directly learn a vector representations for sentences in the absence of compositional property. (Zhou et al., 2016b) learn bilingual document representation by minimizing Euclidean distance between document representations and their translation. Evaluation protocol An important question is how to evaluate multilingual joint sentence embeddings. Let us first define some desired properties o"
W17-2619,P16-1162,0,0.020346,"Missing"
W17-2619,D11-1014,0,0.387224,"languages. 1 Introduction It is today common practice to use distributed representations of words, often called word embeddings, in almost all NLP applications. It has been shown that syntactic and semantic relations can be captured in this embedding space, see for instance (Mikolov et al., 2013). To process sequences of words, ie. sentences or small paragraphs, these word embeddings need to be “combined” into a representation of the whole sequence. Common approaches include: simple techniques like bag-of-words or some type of pooling, eg. (Arora et al., 2017), recursive neural networks, eg. (Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg. (Cho et al., 2014), convolutional neural networks, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg. (Zhao et al., 2015). In some NLP applications, both the input and output are sentences. A very popular approach to handle such tasks is the so-called “encoder• it is likely to capture the underlying semantics of the sentence (since the meaning is the only common characteristic of a sentence formulated in several languages); • it has the potential to transfer many sentence 157 Proceedings of the 2nd Workshop"
W17-2619,P16-1133,0,0.0224723,"2014), convolutional NNs, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg. (Zhao et al., 2015). In all these works, the sentence representations are learned for one language only. It is important to note that our multiple encoder/decoder architecture and the different training paths make no assumption on the type of encoder and decoder used. In principle, all these sentence representations methods could be used. This is left for future research. 3 There are several works on learning multilingual representations at document level (Hermann and Blunsom, 2014; Zhou et al., 2016b; Pham et al., 2015). (Hermann and Blunsom, 2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. (Pham et al., 2015) directly learn a vector representations for sentences in the absence of compositional property. (Zhou et al., 2016b) learn bilingual document representation by minimizing Euclidean distance between document representations and their translation. Evaluation protocol An important question is how to evaluate multilingual joint sentence embeddings. Let us first define some desired properties o"
W17-2619,L16-1561,0,0.0105386,"Missing"
W19-5435,D18-1549,0,0.141799,"e trained using the same setting as the public LASER encoder which involves normalizing texts and tokenization with Moses tools (falling back to the English mode). We first learn a joint 50k BPE vocabulary on the concatenated training data using fastBPE11 . The encoder sees Sinhala, Nepali, Hindi and English sentences at the input, without having any information about the current language. This input is always translated into English.12 We experimented with various techniques to add noise to the English input sentences, similar to what is used in unsupervised neural machine translation, e.g. (Artetxe et al., 2018; Lample et al., 2018), but this did not improve the results. The encoder is a five-layer BLSTM with 512 dimensional layers. The LSTM decoder has one hidden layer of size 2048, trained with the Adam optimizer. For development, we calculate similarity error on the concatenation of the flores dev sets for Sinhala–English and Nepali–English. Our models were trained for seven epochs for about 2.5 hours on 8 Nvidia GPUs. We experimented with various methods using a setup that closely mirrors the official scoring of the shared task. All methods are trained on the provided clean parallel data (see Ta"
W19-5435,D19-1632,1,0.878038,"Missing"
W19-5435,N19-4009,0,0.0476785,", we are provided with a very noisy 40.6 million-word (English token count) Nepali– English corpus and a 59.6 million-word Sinhala– English corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demonstrate that the proposed approach outperforms other existing approaches"
W19-5435,P13-2121,1,0.88624,"Missing"
W19-5435,W18-6319,0,0.0232643,"glish corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demonstrate that the proposed approach outperforms other existing approaches. Moreover we make use of an ensemble of multiple scoring functions to further boost the filtering performance. In this paper, w"
W19-5435,W18-6478,0,0.23575,"orks better than the global one. In that setting, LASER is on average 0.71 BLEU above the best non-LASER system. These gaps are higher for the 1M condition (0.94 BLEU). (iii) The best ensemble configuration provides small improvements over the best LASER configuration. For Sinhala–English the best configuration includes every other scoring method (ALL). For Nepali–English the best configuration is an ensemble of LASER scores. (iv) Dual cross entropy shows mixed results. For Sinhala–English, it only works once the language id filtering is enabled which is consistent with previous observations (Junczys-Dowmunt, 2018). For Nepali– English, it provides scores well below the rest of the scoring methods. Note that we did not perform an architecture exploration. LASER Encoder Training For our experiments and the official submission, we trained a multilingual sentence encoder using the permitted resources in Table 1. We trained a single encoder using all the parallel data for Sinhala–English, Nepali–English and HindiEnglish. Since Hindi and Nepali share the same script, we concatenated their corpora into a single parallel corpus. To account for the difference in size of the parallel training data, we over-sampl"
W19-5435,W18-6488,0,0.112392,"Missing"
W19-5435,W18-2709,1,0.554005,"ods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali–English and Sinhala–English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios. 1 Introduction The availability of high-quality parallel training data is critical for obtaining good translation performance, as neural machine translation (NMT) systems are less robust against noisy parallel data than statistical machine translation (SMT) systems (Khayrallah and Koehn, 2018). Recently, there is an increased interest in the filtering of noisy parallel corpora (such as Paracrawl1 ) to increase the amount of data that can be used to train translation systems (Koehn et al., 2018). While the state-of-the-art methods that use NMT models have proven effective in mining 1 2 http://www.paracrawl.eu/ https://github.com/facebookresearch/LASER 261 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 261–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Methodology 2.1 LASER Multi"
W19-5435,P18-2037,1,0.903256,"tion systems (Koehn et al., 2018). While the state-of-the-art methods that use NMT models have proven effective in mining 1 2 http://www.paracrawl.eu/ https://github.com/facebookresearch/LASER 261 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 261–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Methodology 2.1 LASER Multilingual Representations The underlying idea is to use the distances between two multilingual representations as a notion of parallelism between the two embedded sentences (Schwenk, 2018). To do this, we first train an encoder that learns to produce a multilingual, fixed-size sentence representation; and then compute a distance between two sentences in the learned embedding space. In addition, we use a margin criterion, which uses a k nearest neighbors approach to normalize the similarity scores given that cosine similarity is not globally consistent (Artetxe and Schwenk, 2018a). The WMT 2018 shared task for parallel corpus filtering (Koehn et al., 2018)3 introduced several methods to tackle a high-resource GermanEnglish data condition.While many of these methods were successf"
W19-5435,W18-6479,1,0.802465,"econd a local method, in which we only scored the noisy data using the noisy neighborhood, or the clean data using the clean neighborhood.5 4 We explored the absolute, distance and ratio margin criteria, but the latter worked best 5 this last part was only done for training an ensemble 3 http://statmt.org/wmt18/ parallel-corpus-filtering.html 262 2.2 Other Similarity Methods Forward and backward cross entropy scores, HF (y|x) and HB (x|y) respectively, are then averaged with an additional penalty on a large difference between the two scores |HF (y|x) − HB (x|y)|. Zipporah (Xu and Koehn, 2017; Khayrallah et al., 2018), which is often used as a baseline comparison, uses language model and word translation scores, with weights optimized to separate clean and synthetic noise data. In our setup, we trained Zipporah models for both language pairs Sinhala–English and Nepali–English. We used the open source release6 of the Zipporah tool without modifications. All components of the Zipporah model (probabilistic translation dictionaries and language models) were trained on the provided clean data (excluding the dictionaries). Language models were trained using KenLM (Heafield et al., 2013) over the clean parallel d"
W19-5435,D17-1319,1,0.834528,"th the clean data. Second a local method, in which we only scored the noisy data using the noisy neighborhood, or the clean data using the clean neighborhood.5 4 We explored the absolute, distance and ratio margin criteria, but the latter worked best 5 this last part was only done for training an ensemble 3 http://statmt.org/wmt18/ parallel-corpus-filtering.html 262 2.2 Other Similarity Methods Forward and backward cross entropy scores, HF (y|x) and HB (x|y) respectively, are then averaged with an additional penalty on a large difference between the two scores |HF (y|x) − HB (x|y)|. Zipporah (Xu and Koehn, 2017; Khayrallah et al., 2018), which is often used as a baseline comparison, uses language model and word translation scores, with weights optimized to separate clean and synthetic noise data. In our setup, we trained Zipporah models for both language pairs Sinhala–English and Nepali–English. We used the open source release6 of the Zipporah tool without modifications. All components of the Zipporah model (probabilistic translation dictionaries and language models) were trained on the provided clean data (excluding the dictionaries). Language models were trained using KenLM (Heafield et al., 2013)"
W19-5435,W19-5404,1,0.727968,"Missing"
W19-5435,P07-2045,1,0.0160857,"t known yet. For the task of low-resource filtering (Koehn et al., 2019), we are provided with a very noisy 40.6 million-word (English token count) Nepali– English corpus and a 59.6 million-word Sinhala– English corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demons"
W19-5435,W18-6453,1,\N,Missing
