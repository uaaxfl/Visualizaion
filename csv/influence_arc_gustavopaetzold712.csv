2020.semeval-1.140,C18-1157,0,0.017195,"ndencies (O’dea et al., 2015), just to name a few. It is easy to see how a reliable approach to such tasks could help both businesses and institutions in more reliably and cost-effectively gauging public opinion, monitoring user behavior, driving the creation of new policies, products, services, and so on. Similarly, gauging how humorous a certain piece of content is could also help the entertainment industry, for example, to both evaluate and maybe even automatically create quality content for its users. Automatic humor recognition has been addressed in many insightful contributions already. Cattle and Ma (2018), for instance, explore the use of many different kinds of word embeddings and neural models in judging whether or not a certain piece of text is humorous. Similarly, Barbieri and Saggion (2014) combine an extensive set of features with tree-based machine learning algorithms to identify humor and irony in text. What these contributions have in common is the fact that, because identifying humor is something inherently subjective and complex, they combine numerous, sometimes hard to obtain resources to perform well. In this contribution, we attempt to take a different approach and try to address"
2020.semeval-1.140,D17-1050,0,0.0170916,"e is not sufficient to gauge funniness for humorous content that targets a diverse target audience. 1 Introduction It is no secret that the Natural Language Processing community has been developing a growing interest in increasingly challenging text classification and regression tasks in recent years. Nowadays the community is coming together to find effective solutions to tasks that delve ever deeper into human subjectivity, such as identifying offensive language and racist remarks (Zampieri et al., 2019), categorizing complex and nuanced emotions (Klinger et al., 2018), identifying sarcasm (Ghosh and Veale, 2017), quantifying suicidal tendencies (O’dea et al., 2015), just to name a few. It is easy to see how a reliable approach to such tasks could help both businesses and institutions in more reliably and cost-effectively gauging public opinion, monitoring user behavior, driving the creation of new policies, products, services, and so on. Similarly, gauging how humorous a certain piece of content is could also help the entertainment industry, for example, to both evaluate and maybe even automatically create quality content for its users. Automatic humor recognition has been addressed in many insightfu"
2020.semeval-1.140,N19-1012,0,0.111056,". The task was divided into two sub-tasks: 1. Assigning a funniness score to each headline in the range of 0.0 (not funny) to 3.0 (funny); and 2. Given two different edited versions of the same headline, judging which one was funniest. We chose to participate in Task 2, exclusively. The training, dev, and test set for Task 2 were composed of 9381, 2355, and 2960 instances, respectively. Each instance was composed of a news headline with a highlighted word, two replacements for the highlighted word, and a score indicating which one of the replacements made the headline funnier. As discussed by Hossain et al. (2019), the annotation process of the dataset was quite extensive, involving both academics and Amazon turkers. Both the replacements and the funniness scores were produced by humans subject to monetary compensation. Several measures were taken to prevent annotator’s fatigue and ensure annotation quality. 3 The UTFPR System As was mentioned in Section 1, our system is a very minimalist unsupervised co-occurrence-based model that gauges headlines’ funniness by trying to capture how unexpected they are. Given a headline h and a funny replacement wf to the headline’s target word, our system calculates"
2020.semeval-1.140,2020.semeval-1.98,0,0.035099,"owcases our approach and preliminary results obtained on the dev set, Section 4 reveals our performance on the official shared task, and Section 5 summarizes our contributions and conclusions. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1066 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1066–1070 Barcelona, Spain (Online), December 12, 2020. 2 Task Summary The UTFPR systems were submitted to the SemEval-2020’s Task 7: Assessing Humor in Edited News Headlines (Hossain et al., 2020). In this task, participants were challenged to conceive systems that quantify and/or compare the funniness of news headlines that had been modified for humorous effect. The task was divided into two sub-tasks: 1. Assigning a funniness score to each headline in the range of 0.0 (not funny) to 3.0 (funny); and 2. Given two different edited versions of the same headline, judging which one was funniest. We chose to participate in Task 2, exclusively. The training, dev, and test set for Task 2 were composed of 9381, 2355, and 2960 instances, respectively. Each instance was composed of a news headl"
2020.semeval-1.140,W18-6206,0,0.0637642,"Missing"
2020.semeval-1.140,S17-2004,0,0.121527,"combine an extensive set of features with tree-based machine learning algorithms to identify humor and irony in text. What these contributions have in common is the fact that, because identifying humor is something inherently subjective and complex, they combine numerous, sometimes hard to obtain resources to perform well. In this contribution, we attempt to take a different approach and try to address this task as minimalistically as possible to establish a foundation for more elaborate strategies. As it has been discussed in many academic contributions (Goatly, 2017; Borgianni et al., 2017; Potash et al., 2017), one of the most important characteristics of quality humorous content is its unexpectedness, regardless of the means through which it is delivered (text, audio, or video). Because unexpectedness is something that could potentially be captured through Natural Language Processing techniques, we decided to create a system that tries to gauge funniness by using unexpectedness alone. Because of our interest in potentially adapting our approach to under-resourced languages, we also set out to make ours an unsupervised system that uses as few resources as possible. In this paper, we describe the UT"
2020.semeval-1.140,S19-2010,0,0.0628631,"Missing"
2020.semeval-1.297,2020.lrec-1.758,0,0.0365846,"20) and consists of developing a classification system for each of the sub-tasks described below: • Task A: Categorization of a tweet as offensive or not. • Task B: Classification of an offensive tweet as targeted or not to someone. • Task C: Identification of the target of the offensive tweet (individual, group, or other). To help the participants create a solution for the shared task, datasets in five different languages were provided (Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), English (Rosenthal et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020)). However, only the English dataset ran all three sub-tasks, the other languages ran only sub-task A. Our team focused on the English dataset, so only this dataset will be described ahead. For sub-task A, the organizers provided a dataset containing 9,087,118 instances; for sub-task B, a dataset containing 188,973 instances; and for sub-task C, a dataset containing 188,973 instances. Our team focused on developing a system based on the English dataset but only for solving the sub-task A. For this sub-task, a total of 9,087,118 instances were released to be used as training and development set"
2020.semeval-1.297,D14-1181,0,0.00317328,"from and each sentence is further vectorized in an array where each column represents how many times that word appeared in the sentence. CNN: The Convolutional Neural Network (CNN) was first presented as an approach to object recognition in 1999 (LeCun et al., 1999). Since then, this kind of neural network has been applied as a solution to many other problems, even in Natural Language Processing tasks, and one of them is the Text Classification (Bhandare et al., 2016). A CNN model can detect variations of patterns in the input data, which promotes good feature extraction even from dirty data (Kim, 2014). Looking for a robust and computationally efficient model, we choose this approach to address this task. This choice was backed too by the good results obtained by other teams that used CNN models at OffensEval 2019 (Zampieri et al., 2019). spaCy TextCategorizer: The TextCategorizer1 is a spaCy2 model that, as its name suggests, is used for text classification and it is the model used for the system herein described. Inside of TextCategorizer, there are four parameters: vocab, which is a Vocab object that is the vocabulary that will be used in the model; model, the language model used which,"
2020.semeval-1.297,2020.lrec-1.629,0,0.0189302,"emEval 2020 workshop (Zampieri et al., 2020) and consists of developing a classification system for each of the sub-tasks described below: • Task A: Categorization of a tweet as offensive or not. • Task B: Classification of an offensive tweet as targeted or not to someone. • Task C: Identification of the target of the offensive tweet (individual, group, or other). To help the participants create a solution for the shared task, datasets in five different languages were provided (Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), English (Rosenthal et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020)). However, only the English dataset ran all three sub-tasks, the other languages ran only sub-task A. Our team focused on the English dataset, so only this dataset will be described ahead. For sub-task A, the organizers provided a dataset containing 9,087,118 instances; for sub-task B, a dataset containing 188,973 instances; and for sub-task C, a dataset containing 188,973 instances. Our team focused on developing a system based on the English dataset but only for solving the sub-task A. For this sub-task, a total of 9,087,118 instances were released to be use"
2020.semeval-1.297,2020.lrec-1.430,0,0.0198166,"0. Licence details: 2 Task Summary The OffensEval shared task is part of the SemEval 2020 workshop (Zampieri et al., 2020) and consists of developing a classification system for each of the sub-tasks described below: • Task A: Categorization of a tweet as offensive or not. • Task B: Classification of an offensive tweet as targeted or not to someone. • Task C: Identification of the target of the offensive tweet (individual, group, or other). To help the participants create a solution for the shared task, datasets in five different languages were provided (Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), English (Rosenthal et al., 2020), Greek (Pitenis et al., 2020), and Turkish (C¸o¨ ltekin, 2020)). However, only the English dataset ran all three sub-tasks, the other languages ran only sub-task A. Our team focused on the English dataset, so only this dataset will be described ahead. For sub-task A, the organizers provided a dataset containing 9,087,118 instances; for sub-task B, a dataset containing 188,973 instances; and for sub-task C, a dataset containing 188,973 instances. Our team focused on developing a system based on the English dataset but only for solving the sub-task A. For this"
2020.semeval-1.297,S19-2010,0,0.0602481,"Missing"
2021.semeval-1.1,P19-1267,0,0.017165,"rank systems based on their score on Pearson’s correlation, giving a final ranking over all systems, it should be noted that there is very little variation in score between the top systems and all other systems. For Task 1 there are 0.0182 points of Pearson’s Correlation separating the systems at ranks 1 and 10. For Task 2 a similar difference of 0.021 points of Pearson’s Correlation separates the systems at ranks 1 and 10. These are small differences and it may be the case that had we selected a different random split in our dataset this would have led to a different ordering in our results (Gorman and Bedrick, 2019; Søgaard et al., 2020). This is not unique to our task and is something for the SemEval community to ruminate on as the focus of NLP tasks continues to move towards better evaluation rather than better systems. References Ahmed AbuRa’ed and Horacio Saggion. 2018. LaSTUS/TALN at Complex Word Identification (CWI) 2018 Shared Task. In Proceedings of the 13th Workshop on Innovative Use of NLP for Building Educational Applications, New Orleans, United States. Association for Computational Linguistics. David Alfter and Ildik´o Pil´an. 2018. SB@GU at the Complex Word Identification 2018 Shared Task."
2021.semeval-1.1,W18-0540,0,0.136791,"Missing"
2021.semeval-1.78,N19-1423,0,0.195547,"majority of the most successful systems submitted to these shared tasks combined ensemble methods, such as Random Forests (Ho, 1995) and AdaBoost (Freund and Schapire, 1997) with numerous linguistic features, including word frequencies, n-gram frequencies, word length, number of senses, number of syllables, psycholinguistic metrics, and word embeddings (Konkol, 2016; Malmasi et al., 2016; Paetzold and Specia, 2016d; Gooding and Kochmar, 2018; Hartmann and Dos Santos, 2018). However, because these tasks were held prior to the ascension of transformer-based masked language models, such as BERT (Devlin et al., 2019) and ROBERTA (Liu et al., 2019), we could not find any systems that exploited the power of the features produced by them. In this paper, we describe the UTFPR systems for the Lexical Complexity Prediction shared task of SemEval 2021 (LCP 2021), which combine classic complexity prediction features with contextual word and phrase representations extracted from transformer-based models. In our experiments, we explore the efficacy of a number of different machine learning models, feature combinations, and corpora sources for our features. In what follows, we present the task being addressed (Secti"
2021.semeval-1.78,W18-0540,0,0.0522985,"Missing"
2021.semeval-1.78,W11-2123,0,0.0497816,"We created 12 SubIMDB splits for this experiment: Children movies (Chi-M), children series (Chi-S), children movies and series (ChiMS), family movies (Fam-M), family series (FamS), family movies and series (Fam-MS), comedy movies (Com-M), comedy series (Com-S), comedy movies and series (Com-MS), all movies (Movies), all series (Series), and the entire corpus (All). We calculate the Pearson correlation between the trial set complexity scores and n-gram frequencies for all n-gram configurations described in Section 3.1. To do so, we trained 5-gram language models over these splits using KenLM (Heafield, 2011). The results illustrated in Table 1 are absolute correlation scores for the trial set of the single words sub-track (original values were negative, given that complexity inversely correlates with word frequency). We chose absolute scores to make the table more compact. It can be observed that the (0, 0) configuration (no context) yields the best correlations in every scenario. It can also be noted that, while the family movies split (Fam-M) is best for (0, 0), the remaining configurations tend to benefit from larger splits. Based on that observation, in the experiments that follow, we use fam"
2021.semeval-1.78,S16-1162,0,0.0243286,"complexity of words in context. While the CWI 2016 task used a simple binary complex/not complex classification setup for English only, the CWI 2018 task explored both a binary classification and a regression setup and multiple languages. The majority of the most successful systems submitted to these shared tasks combined ensemble methods, such as Random Forests (Ho, 1995) and AdaBoost (Freund and Schapire, 1997) with numerous linguistic features, including word frequencies, n-gram frequencies, word length, number of senses, number of syllables, psycholinguistic metrics, and word embeddings (Konkol, 2016; Malmasi et al., 2016; Paetzold and Specia, 2016d; Gooding and Kochmar, 2018; Hartmann and Dos Santos, 2018). However, because these tasks were held prior to the ascension of transformer-based masked language models, such as BERT (Devlin et al., 2019) and ROBERTA (Liu et al., 2019), we could not find any systems that exploited the power of the features produced by them. In this paper, we describe the UTFPR systems for the Lexical Complexity Prediction shared task of SemEval 2021 (LCP 2021), which combine classic complexity prediction features with contextual word and phrase representations ex"
2021.semeval-1.78,S16-1154,0,0.0260553,"words in context. While the CWI 2016 task used a simple binary complex/not complex classification setup for English only, the CWI 2018 task explored both a binary classification and a regression setup and multiple languages. The majority of the most successful systems submitted to these shared tasks combined ensemble methods, such as Random Forests (Ho, 1995) and AdaBoost (Freund and Schapire, 1997) with numerous linguistic features, including word frequencies, n-gram frequencies, word length, number of senses, number of syllables, psycholinguistic metrics, and word embeddings (Konkol, 2016; Malmasi et al., 2016; Paetzold and Specia, 2016d; Gooding and Kochmar, 2018; Hartmann and Dos Santos, 2018). However, because these tasks were held prior to the ascension of transformer-based masked language models, such as BERT (Devlin et al., 2019) and ROBERTA (Liu et al., 2019), we could not find any systems that exploited the power of the features produced by them. In this paper, we describe the UTFPR systems for the Lexical Complexity Prediction shared task of SemEval 2021 (LCP 2021), which combine classic complexity prediction features with contextual word and phrase representations extracted from transform"
2021.semeval-1.78,C16-2017,1,0.926488,"hand, are a great complement to classic features. We also find that employing the principle of compositionality can potentially help in phrase complexity prediction. Our systems place 45th out of 55 for single words and 29th out of 38 for phrases. 1 Introduction Accurately measuring the complexity of words can be useful in many ways. It facilitates the creation of text simplification technologies that could, for example, help in identifying and adapting challenging excerpts of literary pieces targeting specific groups, such as children (De Belder and Moens, 2010) and second language learners (Paetzold and Specia, 2016e), and make news articles and official documents more accessible to the general population (Paetzold and Specia, 2016a). This task has received a considerable amount of attention in the past few years, especially due to the popularity of the Complex Word Identification (CWI) shared tasks of 2016 (Paetzold and Specia, 2016c) and 2018 (Yimam et al., 2018), where dozens of teams were challenged to judge the complexity of words in context. While the CWI 2016 task used a simple binary complex/not complex classification setup for English only, the CWI 2018 task explored both a binary classification"
2021.semeval-1.78,C16-1157,1,0.934813,"hand, are a great complement to classic features. We also find that employing the principle of compositionality can potentially help in phrase complexity prediction. Our systems place 45th out of 55 for single words and 29th out of 38 for phrases. 1 Introduction Accurately measuring the complexity of words can be useful in many ways. It facilitates the creation of text simplification technologies that could, for example, help in identifying and adapting challenging excerpts of literary pieces targeting specific groups, such as children (De Belder and Moens, 2010) and second language learners (Paetzold and Specia, 2016e), and make news articles and official documents more accessible to the general population (Paetzold and Specia, 2016a). This task has received a considerable amount of attention in the past few years, especially due to the popularity of the Complex Word Identification (CWI) shared tasks of 2016 (Paetzold and Specia, 2016c) and 2018 (Yimam et al., 2018), where dozens of teams were challenged to judge the complexity of words in context. While the CWI 2016 task used a simple binary complex/not complex classification setup for English only, the CWI 2018 task explored both a binary classification"
2021.semeval-1.78,S16-1149,1,0.890245,"Missing"
2021.semeval-1.78,W16-4912,0,0.0538538,"hand, are a great complement to classic features. We also find that employing the principle of compositionality can potentially help in phrase complexity prediction. Our systems place 45th out of 55 for single words and 29th out of 38 for phrases. 1 Introduction Accurately measuring the complexity of words can be useful in many ways. It facilitates the creation of text simplification technologies that could, for example, help in identifying and adapting challenging excerpts of literary pieces targeting specific groups, such as children (De Belder and Moens, 2010) and second language learners (Paetzold and Specia, 2016e), and make news articles and official documents more accessible to the general population (Paetzold and Specia, 2016a). This task has received a considerable amount of attention in the past few years, especially due to the popularity of the Complex Word Identification (CWI) shared tasks of 2016 (Paetzold and Specia, 2016c) and 2018 (Yimam et al., 2018), where dozens of teams were challenged to judge the complexity of words in context. While the CWI 2016 task used a simple binary complex/not complex classification setup for English only, the CWI 2018 task explored both a binary classification"
2021.semeval-1.78,2020.readi-1.9,0,0.0214477,": We use the word/phrase’s number of senses catalogued in the WordNet database (Miller et al., 1990). In line with our setup for word length, for phrases, we use the average number of senses of all individual words. a sentence, a target word or phrase within the sentence, and a complexity score calculated based on judgments made by 20 English speakers from the USA, UK and Australia. The source identifier describes from where the sentence came from, the possibilities being the Bible, biomedical documents and the Europarl corpus. The task’s dataset is an extended version of the CompLex dataset (Shardlow et al., 2020). The training, trial, and test sets for single words have 7662, 421, and 917 instances, respectively. The training, trial and test sets for phrases have 1517, 99, and 184 instances, respectively. Participants were allowed and encouraged to use any external resources they saw fit. 3 • BERT vector: We use the numerical representation of 768 dimensions produced by the pre-trained BERT model (Devlin et al., 2019). For phrases and out-of-vocabulary words that were fragmented during tokenization, we average the representations produced for all fragments. More specifically, we used the bertbase-unca"
2021.semeval-1.78,2021.semeval-1.1,1,0.762986,"ared task of SemEval 2021 (LCP 2021), which combine classic complexity prediction features with contextual word and phrase representations extracted from transformer-based models. In our experiments, we explore the efficacy of a number of different machine learning models, feature combinations, and corpora sources for our features. In what follows, we present the task being addressed (Section 2), our approach (Section 3), some preliminary experiments (Section 4), our final shared task results (Section 5), and our conclusions (Section 6). 2 Task Description We address the LCP 2021 shared task (Shardlow et al., 2021), held at SemEval 2021. The shared task is split into two sub-tasks: predicting the incontext lexical complexity of single words and phrases for the English language. Participants could choose to submit systems to either or both sub-tasks. The organizers provided training, trial and test sets for both sub-tasks. Each instance of these datasets is composed of an ID, a source identifier, 617 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 617–622 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics • Number of s"
2021.semeval-1.78,2020.emnlp-demos.6,0,0.0428183,"Missing"
2021.semeval-1.78,W18-0507,1,0.856814,"n of text simplification technologies that could, for example, help in identifying and adapting challenging excerpts of literary pieces targeting specific groups, such as children (De Belder and Moens, 2010) and second language learners (Paetzold and Specia, 2016e), and make news articles and official documents more accessible to the general population (Paetzold and Specia, 2016a). This task has received a considerable amount of attention in the past few years, especially due to the popularity of the Complex Word Identification (CWI) shared tasks of 2016 (Paetzold and Specia, 2016c) and 2018 (Yimam et al., 2018), where dozens of teams were challenged to judge the complexity of words in context. While the CWI 2016 task used a simple binary complex/not complex classification setup for English only, the CWI 2018 task explored both a binary classification and a regression setup and multiple languages. The majority of the most successful systems submitted to these shared tasks combined ensemble methods, such as Random Forests (Ho, 1995) and AdaBoost (Freund and Schapire, 1997) with numerous linguistic features, including word frequencies, n-gram frequencies, word length, number of senses, number of syllab"
C16-1069,W10-1607,0,0.295739,"Missing"
C16-1069,P11-2087,0,0.158825,"Missing"
C16-1069,J96-2004,0,0.496536,"Missing"
C16-1069,E99-1042,0,0.174289,"Missing"
C16-1069,C96-2183,0,0.738103,"ses feature valuable new insight on the relationship between the non-natives’ notion of complexity and various morphological, semantic and lexical word properties. Some of our findings contradict long-standing misconceptions about word simplicity. The data produced in our studies consists of 211,564 annotations made by 1,100 volunteers, which we hope will guide forthcoming research on Text Simplification for non-native speakers of English. 1 Introduction Text Simplification is a useful application both to improve other Natural Language Processing tasks and to assist language-impaired readers (Chandrasekar et al., 1996). When a simplifier aims to help people, understanding their needs becomes very important. In Lexical Simplification – the task of replacing complex words and expressions with simpler alternatives – this has been shown to be the case (Rello et al., 2013b; Rello et al., 2013a; Rello et al., 2013c). They describe several user studies conducted with readers suffering from Dyslexia and outline the most recurring challenges faced by them, as well as the most effective ways to overcome these challenges. Given the widespread availability of content in English, non-native speakers of English become an"
C16-1069,P15-2011,0,0.113051,"Missing"
C16-1069,P14-2075,0,0.0543055,"Missing"
C16-1069,O13-1007,0,0.110547,"Missing"
C16-1069,P13-1151,0,0.0955934,"Missing"
C16-1069,P03-1054,0,0.0194155,"Missing"
C16-1069,S07-1009,0,0.0360823,"Missing"
C16-1069,S10-1002,0,0.0607105,"Missing"
C16-1069,W13-4813,1,0.936196,"Missing"
C16-1069,P15-4015,1,0.84938,"Missing"
C16-1069,W16-4912,0,0.09625,"Missing"
C16-1069,N15-2002,1,0.884135,"Missing"
C16-1069,P13-3015,0,0.169869,"Missing"
C16-1069,W13-2908,0,0.0352526,"Missing"
C16-1069,shardlow-2014-open,0,0.257223,"Missing"
C16-1069,S12-1046,1,0.927886,"Missing"
C16-1157,N15-1156,0,0.0255822,"Missing"
C16-1157,W10-1505,0,0.0137854,"a sentence, a target word, and candidate substitutions ranked by simplicity. This dataset has been widely used and hence allows the comparison of SubIMDB against state-of-the-art solutions for the task. For evaluation, we use Spearman (r) and Pearson (ρ) correlation, as well as the TRank metric proposed by Specia et al. (2012), which measures the rate with which a candidate substitution with the highest gold rank i.e. the simplest, was ranked first by the system. We compare the performance of all frequency norms described in Section 3 to Google 1T, a corpus composed of over 1 trillion words (Evert, 2010), and the winner system in the SemEval 2012 task, which 1675 Norm KF HAL Wiki SimpleWiki SUBTLEX Open2016 SubIMDB SubMOV SubSER SubFAM r 0.619 0.630 0.575 0.626 0.649 0.650 0.654 0.660 0.648 0.649 ρ 0.626 0.633 0.583 0.632 0.649 0.647 0.652 0.658 0.647 0.650 TRank 0.589 0.598 0.516 0.570 0.619 0.619 0.622 0.623 0.619 0.615 F-test ••• ••• ••• ••• ••• ••• ••• ••• ••• Norm SubCOM SubCHI SubFAM-M SubFAM-S SubCOM-M SubCOM-S SubCHI-M SubCHI-S Google 1T Best SemEval r 0.655 0.643 0.653 0.647 0.660 0.647 0.650 0.640 N/A N/A ρ 0.653 0.645 0.653 0.650 0.658 0.648 0.654 0.644 N/A N/A TRank 0.623 0.611 0."
C16-1157,P14-2075,0,0.046101,".1 (•), p &lt; 0.01 (••) or p &lt; 0.001 (• • •) (F-test). employs a Support Vector Machine ranker that uses a wide array of features (Jauhar and Specia, 2012). The results in Table 94 reveal that SubIMDB outperforms all baselines, including Google 1T and the former state-of-the-art for the task in TRank. Nonetheless, some SubIMDB subcorpora are even more effective than using our corpus in its entirety, despite being much smaller. Work in Text Simplification has, however, explored more than single-word frequency norms, considering for example raw n-gram frequencies and language model probabilities (Horn et al., 2014; BaezaYates et al., 2015; Paetzold and Specia, 2016b). Table 10 shows TRank scores obtained on the SemEval 2012 task when using 3-gram and 5-gram raw frequencies and language model probabilities extracted from various corpora. The 3-grams and 5-grams consist in a candidate substitution surrounded by one and two tokens, respectively. For probabilities, we trained 5-gram language models using SRILM (Stolcke, 2002). For the Kucera-Francis (KF) norms we use the Brown corpus (Francis and Kucera, 1979). The HAL corpus is not available for download and hence it could not be tested here. Table 10 sho"
C16-1157,S12-1066,1,0.883235,"FAM-M SubFAM-S SubCOM-M SubCOM-S SubCHI-M SubCHI-S Google 1T Best SemEval r 0.655 0.643 0.653 0.647 0.660 0.647 0.650 0.640 N/A N/A ρ 0.653 0.645 0.653 0.650 0.658 0.648 0.654 0.644 N/A N/A TRank 0.623 0.611 0.618 0.620 0.623 0.618 0.600 0.608 0.585 0.602 F-test • ••• ••• ••• ••• ••• ••• ••• - Table 9: Correlation and TRank scores for frequency norms with respect to simplicity. The fifth column indicates a statistically significant difference with SubIMDB given p &lt; 0.1 (•), p &lt; 0.01 (••) or p &lt; 0.001 (• • •) (F-test). employs a Support Vector Machine ranker that uses a wide array of features (Jauhar and Specia, 2012). The results in Table 94 reveal that SubIMDB outperforms all baselines, including Google 1T and the former state-of-the-art for the task in TRank. Nonetheless, some SubIMDB subcorpora are even more effective than using our corpus in its entirety, despite being much smaller. Work in Text Simplification has, however, explored more than single-word frequency norms, considering for example raw n-gram frequencies and language model probabilities (Horn et al., 2014; BaezaYates et al., 2015; Paetzold and Specia, 2016b). Table 10 shows TRank scores obtained on the SemEval 2012 task when using 3-gram"
C16-1157,P13-1151,0,0.055492,"s (SubFAM-S) Comedy movies (SubCOM-M) Comedy series (SubCOM-S) Children movies (SubCHI-M) Children series (SubCHI-S) Table 4: Subcorpora from SubIMDB used to predict lexical decision times We compare ours to six frequency norms: • KF: Oldest and most widely used frequency norm, calculated over the Brown corpus (Rudell, 1993; Francis and Kucera, 1979). • HAL: Hyperspace Analogue to Language word frequency norm, calculated over the HAL corpus, which contains over 131 million words from Usenet newsgroups (Burgess and Livesay, 1998). • Wiki: Word frequencies from Wikipedia, with 97 million words (Kauchak, 2013). • SimpleWiki: Word frequencies from Simple Wikipedia, with 9 million words (Kauchak, 2013). • SUBTLEX: Word frequencies from SUBTLEXus , with 51 million words (Brysbaert and New, 2009). • Open2016: Word frequencies from OpenSubtitles2016, with 2 billion words (Lison and Tiedemann, 2016). We regularise all norms using Equation 1, in which f is the frequency norm value of a word w. This transformation has shown to best represent the relationship between word frequencies and lexical decision times (Balota et al., 2004). norm(f (w)) = log10 (f (w) + 1) (1) We use the same lexical decision datase"
C16-1157,L16-1147,0,0.165703,"extracted is one of its most important defining traits. For example, the experiments of Brysbaert and New (2009) and Shardlow (2013) reveal that frequencies from spoken text have a much stronger correlation with psycholinguistic word properties than those from other sources. Their findings greatly highlight the potential of spoken language text, but there are very few examples of resources of this kind available for English. SUBTLEXus is a notable exception: it contains texts extracted from 8,388 subtitles of American movies, and is freely available for download. The OpenSubtitles2016 corpus (Lison and Tiedemann, 2016) is another example, featuring sentences extracted from numerous subtitle files aligned at sentence level across 60 languages. This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1669 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1669–1679, Osaka, Japan, December 11-17 2016. However, since the subtitles in these corpora are not restricted with respect to genre or domain, their proficiency in capturing everyday language can also be"
C16-1157,N16-1050,1,0.892455,"e a poor performance for the Kucera-Francis coefficient. Despite its use in numerous previous contributions (Burgess and Livesay, 1998; Zevin and Seidenberg, 2002; Brysbaert and New, 2009), more modern resources proved more effective. We believe this is caused by the fact that these coefficients are calculated from a corpus that is very small when compared to the other resources presented in this paper. 4 Predicting Psycholinguistic Properties In addition to lexical decision times, other psycholinguistic properties of words have been studied in terms of their correlation with frequency norms (Paetzold and Specia, 2016a). In this experiment, we evaluate how well the norms described in Section 3 correlate with four psycholinguistic properties extracted from the MRC psycholinguistic Database: • Familiarity: Available for 9,392 words – frequency with which a word is seen, heard or used daily. • Age of Acquisition: Available for 3,503 words – age at which a word is learned. • Concreteness: Available for 8,228 words – how “palpable” the object the word refers to is. • Imagery: Available for 9,240 words – intensity with which a word arouses images. The results in Table 6 reveal that SubFAM-M (family movies) perfo"
C16-1157,W16-4912,0,0.0733417,"e a poor performance for the Kucera-Francis coefficient. Despite its use in numerous previous contributions (Burgess and Livesay, 1998; Zevin and Seidenberg, 2002; Brysbaert and New, 2009), more modern resources proved more effective. We believe this is caused by the fact that these coefficients are calculated from a corpus that is very small when compared to the other resources presented in this paper. 4 Predicting Psycholinguistic Properties In addition to lexical decision times, other psycholinguistic properties of words have been studied in terms of their correlation with frequency norms (Paetzold and Specia, 2016a). In this experiment, we evaluate how well the norms described in Section 3 correlate with four psycholinguistic properties extracted from the MRC psycholinguistic Database: • Familiarity: Available for 9,392 words – frequency with which a word is seen, heard or used daily. • Age of Acquisition: Available for 3,503 words – age at which a word is learned. • Concreteness: Available for 8,228 words – how “palpable” the object the word refers to is. • Imagery: Available for 9,240 words – intensity with which a word arouses images. The results in Table 6 reveal that SubFAM-M (family movies) perfo"
C16-1157,pak-paroubek-2010-twitter,0,0.0110531,"word frequency analysis is that the type of resource used as a corpus is often built for a specific communication purpose, such as news (Burgess and Livesay, 1998). This is however not representative of everyday language usage, particularly from a psycholinguistic perspective. The other extreme of the spectrum features resources compiled from user-generated content, such as micro-blogs. However, these resources often suffer from grammar errors and misspellings, excessive use of acronyms and shortenings, partly due to the constrains of the publication means (e.g. limited number of characters) (Pak and Paroubek, 2010). This is particularly concerning given that previous research has shown that the source from which a corpus was extracted is one of its most important defining traits. For example, the experiments of Brysbaert and New (2009) and Shardlow (2013) reveal that frequencies from spoken text have a much stronger correlation with psycholinguistic word properties than those from other sources. Their findings greatly highlight the potential of spoken language text, but there are very few examples of resources of this kind available for English. SUBTLEXus is a notable exception: it contains texts extrac"
C16-1157,P13-3015,0,0.0445307,"olinguistic perspective. The other extreme of the spectrum features resources compiled from user-generated content, such as micro-blogs. However, these resources often suffer from grammar errors and misspellings, excessive use of acronyms and shortenings, partly due to the constrains of the publication means (e.g. limited number of characters) (Pak and Paroubek, 2010). This is particularly concerning given that previous research has shown that the source from which a corpus was extracted is one of its most important defining traits. For example, the experiments of Brysbaert and New (2009) and Shardlow (2013) reveal that frequencies from spoken text have a much stronger correlation with psycholinguistic word properties than those from other sources. Their findings greatly highlight the potential of spoken language text, but there are very few examples of resources of this kind available for English. SUBTLEXus is a notable exception: it contains texts extracted from 8,388 subtitles of American movies, and is freely available for download. The OpenSubtitles2016 corpus (Lison and Tiedemann, 2016) is another example, featuring sentences extracted from numerous subtitle files aligned at sentence level"
C16-1157,S12-1046,1,0.945539,"cting word simplicity. In this experiment, we evaluate how well SubIMDB fairs against other corpora when employed as a solution to Lexical Simplification. As our test set, we use the one from the English Lexical Simplification task of SemEval 2012, which contains 1,710 instances composed of a sentence, a target word, and candidate substitutions ranked by simplicity. This dataset has been widely used and hence allows the comparison of SubIMDB against state-of-the-art solutions for the task. For evaluation, we use Spearman (r) and Pearson (ρ) correlation, as well as the TRank metric proposed by Specia et al. (2012), which measures the rate with which a candidate substitution with the highest gold rank i.e. the simplest, was ranked first by the system. We compare the performance of all frequency norms described in Section 3 to Google 1T, a corpus composed of over 1 trillion words (Evert, 2010), and the winner system in the SemEval 2012 task, which 1675 Norm KF HAL Wiki SimpleWiki SUBTLEX Open2016 SubIMDB SubMOV SubSER SubFAM r 0.619 0.630 0.575 0.626 0.649 0.650 0.654 0.660 0.648 0.649 ρ 0.626 0.633 0.583 0.632 0.649 0.647 0.652 0.658 0.647 0.650 TRank 0.589 0.598 0.516 0.570 0.619 0.619 0.622 0.623 0.61"
C16-2017,W10-1607,0,0.0172874,"texts according to the needs of individual users, and its enhancement module allows the user to search for a word’s definitions, synonyms, translations, and visual cues through related images. These utilities are brought together in an easy-to-use interface of a freely available web browser extension. 1 Introduction Readers who suffer from reading impairments find it difficult to understand certain types of texts which, to an average reader, would not pose any challenge. Low literacy readers and second language learners, for example, often have very limited vocabulary (Watanabe et al., 2009; Aluisio and Gasperin, 2010), while those with Dyslexia may have problems understanding the meaning of rare and/or long words (Ellis, 1993; Rello et al., 2013b). Other notable examples of such conditions are Aphasia and some forms of Autism, which can also hinder the patient’s capability of comprehending sentences made up of a large amount of words and/or complex syntactic constructs (Devlin and Tait, 1998; Barbu et al., 2015). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches"
C16-2017,N15-3024,0,0.124057,"15). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches handle vocabulary limitations by replacing complex words with simpler alternatives (Devlin and Tait, 1998; Paetzold and Specia, 2016a), Syntactic Simplification approaches address the problem of long, complex syntactic constructs by re-structuring them (Siddharthan, 2006; Paetzold and Specia, 2013). Text Enhancement approaches can also help: Devlin and Unthank (2006), Watanabe et al. (2009) and Azab et al. (2015) adorn the words of a text with definitions, images and synonyms in order to facilitate their comprehension. Rello et al. (2013a) reveal that while simplification tends to increase a document’s readability, enhancement tends to improve its comprehensibility. One important limitation of the state of the art Text Adaptation systems is that they are not available for download and/or use. Online demos are provided for some, but they only allow the processing of small snippets of text through online interfaces. Another limitation is that the adaptations made by these systems are not personalised i."
C16-2017,W13-4813,1,0.846484,"ing sentences made up of a large amount of words and/or complex syntactic constructs (Devlin and Tait, 1998; Barbu et al., 2015). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches handle vocabulary limitations by replacing complex words with simpler alternatives (Devlin and Tait, 1998; Paetzold and Specia, 2016a), Syntactic Simplification approaches address the problem of long, complex syntactic constructs by re-structuring them (Siddharthan, 2006; Paetzold and Specia, 2013). Text Enhancement approaches can also help: Devlin and Unthank (2006), Watanabe et al. (2009) and Azab et al. (2015) adorn the words of a text with definitions, images and synonyms in order to facilitate their comprehension. Rello et al. (2013a) reveal that while simplification tends to increase a document’s readability, enhancement tends to improve its comprehensibility. One important limitation of the state of the art Text Adaptation systems is that they are not available for download and/or use. Online demos are provided for some, but they only allow the processing of small snippets of tex"
C16-2017,P15-4015,1,0.807253,"user must select a word they do not understand. The reading assistance wizard depicted in Figure 1b will then pop-up. Anita currently offers two types of adaptation: Simplification and Enhancement. 3 Simplification Module Anita’s simplification module attempts to replace the selected word with a simpler alternative. To do so, Anita first finds the sentence containing the selected word and then sends this information to the remote server where Anita’s Lexical Simplification engine is running. The engine runs a state-of-the-art Lexical Simplification system powered by the LEXenstein framework (Paetzold and Specia, 2015). The strategy used here has been shown to outperform all other simplifiers from previous work (Paetzold and Specia, 2016a). Upon receiving a simplification request for a word, Anita’s simplifier performs the following steps: 1. Generation: A context-aware word embeddings model trained over 7 billion words which accounts for grammatical information (Paetzold and Specia, 2016b) is used to produce candidate substitutions for the word. 2. Selection: The Unsupervised Boundary Ranking approach (Paetzold and Specia, 2016b) is used to select the candidates that best fit the context of the complex wor"
C16-2017,W16-4912,0,0.117325,"93; Rello et al., 2013b). Other notable examples of such conditions are Aphasia and some forms of Autism, which can also hinder the patient’s capability of comprehending sentences made up of a large amount of words and/or complex syntactic constructs (Devlin and Tait, 1998; Barbu et al., 2015). Previous work has proposed a wide array of approaches that aim to adapt texts for these audiences. Text Simplification strategies are good examples of that. While Lexical Simplification approaches handle vocabulary limitations by replacing complex words with simpler alternatives (Devlin and Tait, 1998; Paetzold and Specia, 2016a), Syntactic Simplification approaches address the problem of long, complex syntactic constructs by re-structuring them (Siddharthan, 2006; Paetzold and Specia, 2013). Text Enhancement approaches can also help: Devlin and Unthank (2006), Watanabe et al. (2009) and Azab et al. (2015) adorn the words of a text with definitions, images and synonyms in order to facilitate their comprehension. Rello et al. (2013a) reveal that while simplification tends to increase a document’s readability, enhancement tends to improve its comprehensibility. One important limitation of the state of the art Text Ada"
C16-3004,P12-3024,0,0.0270183,"Missing"
C16-3004,P10-1064,0,0.0826527,"Missing"
C16-3004,2015.iwslt-papers.4,1,0.838642,"Missing"
C16-3004,L16-1582,1,0.769073,"n entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence, word and document-leve"
C16-3004,2014.eamt-1.21,1,0.770324,"word-level QE over the years. An application that can benefit from word-level QE is spotting errors (incorrect words) in a post-editing/revision scenario. A recent variant of this task is quality prediction at the level of phrases (Logacheva and L.Specia, 2015; Blain et al., 2016), where a phrase can be defined in different ways, e.g. using the segmentation from a statistical MT decoder in WMT16 (Bojar et al., 2016). Document-level QE has received much less attention than the other levels. This task consists in predicting a single quality label for an entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et a"
C16-3004,W15-4916,1,0.786531,"Missing"
C16-3004,2015.iwslt-papers.11,0,0.0579597,"e quality label for an entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence"
C16-3004,2014.eamt-1.22,1,0.76135,"arly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction level has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012 (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016). Wh"
C16-3004,P10-1063,0,0.198553,"these instances and quality predictions are then produced by the model. Figure 2: QE model prediction. QE is a reasonably new field, but over the last decade has become particularly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction level has been covered in shared tasks organised by the Workshop o"
C16-3004,P13-4014,1,0.892777,"Missing"
C16-3004,P15-4020,1,0.730678,"an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence, word and document-level and includes the ext"
C16-3004,2011.eamt-1.12,1,0.848372,"re used to train the QE model are extracted from these instances and quality predictions are then produced by the model. Figure 2: QE model prediction. QE is a reasonably new field, but over the last decade has become particularly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction leve"
C16-3004,2015.eamt-1.17,1,\N,Missing
C18-1021,W18-0503,1,0.839312,"ch that amendments to and maintenance of the latter need only be implemented on the server side. Lexi is currently limited to performing lexical simplification. Note, however, that this is merely a limitation of the backend system, which only implements a lexical simplification system for now. From the frontend perspective, however, there are no limitations as to the nature and length of the simplified items in a text, and extending Lexi to support higher-level modes of simplification simply amounts to 2 An alternative to traditional, one-size-fits-all approaches has recently been proposed by Bingel et al. (2018), who use eye-tracking measures to induce personalized models to predict misreadings in children with reading difficulties. 248 Figure 1: Lexical simplification pipeline as identified by Paetzold and Specia (2015). The simplification workflow consists of identifying simplification targets, i.e. words that pose a challenge to the reader. In the generation step, possible alternatives for each target are retrieved, which are then filtered in the selection step, eliminating words that do not fit the context. In the ranking step, the system finally orders the candidates by simplicity. implementing"
C18-1021,P11-2087,0,0.0259131,"arget group has its own simplification needs, and there is considerable variation as to how well the specifics of what makes a text difficult is defined for each group and simplification strategy. While difficult items in a text may be identified more easily and generally for problems such as resolving pronoun reference, questions such as what makes a French word difficult for a native speaker of Japanese, or what dyslexic children consider a difficult character combination or an overly long sentence, are much harder to answer. Nevertheless, there is a vast body of work (Yatskar et al., 2010; Biran et al., 2011; Horn et al., 2014) that ventures to build very general-purpose simplification models from simplification corpora such as the Simple English Wikipedia corpus (Coster and Kauchak, 2011), which has been edited by amateurs without explicit regard to a specific audience, and with rather vague guidelines as to what constitutes difficult or simple language. Other work in simplification attempts to answer the above questions by inducing models from specifically compiled datasets, which for instance may have been collected by surveying specific target groups and asking them to indicate difficult mate"
C18-1021,P11-2117,0,0.424489,"users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users. 1 Introduction Many a research paper on text simplification starts out by sketching the problem of text simplification as rewriting a text such that it becomes easier to read, changing or removing as little of its informational content as possible (Zhu et al., 2010; Coster and Kauchak, 2011; De Belder and Moens, 2010; Paetzold and Specia, 2015; Bingel and Søgaard, 2016). Such a statement may describe the essence of simplification as a research task, but it hides the fact that it is not always easy to decide what is easy for a particular user. This paper discusses why we need custom-tailored simplifications for individual users, and argues that previous research on non-adaptive text simplification has been too generic to unfold the full potential of text simplification. Even when limiting ourselves to lexical substitution, i.e. the task of reducing the complexity of a document by"
C18-1021,W14-1215,0,0.345672,"pursued, often focusing on lexical (Tweissi, 1998) but also sentence-level simplification (Liu and Matsumoto, 2016). Other notable groups that have been specifically targeted in text simplification research include dyslexics (Rello et al., 2013), and the aphasic (Carroll et al., 1998), for whom particularly long words and sentences, but also certain surface forms such as specific character combinations, may pose difficulties. People on the autism spectrum have also been addressed, with the focus lying on reducing the amount of figurative expressions in a text or reducing syntactic complexity (Evans et al., 2014). Reading beginners (both children and adults) are another group with very particular needs, and text simplification This paper is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 245 Proceedings of the 27th International Conference on Computational Linguistics, pages 245–258 Santa Fe, New Mexico, USA, August 20-26, 2018. research has tried to provide this group with methods to reduce the amount of high-register language and non-frequent words (De Belder and Moens, 2010). Evidently, each target group has its"
C18-1021,P15-2011,0,0.154431,"asks), such that data points sampled from users can quickly have an impact on a generic base model.2 3.1.2 Adaptive Substitution Ranking Substitution Ranking has received relatively little attention in the community compared to CWI. Most lexical simplifiers rank candidates using unsupervised approaches. The earliest example is the approach of Carroll et al. (1998), who rank candidates according to their Kucera-Francis coefficients, which are calculated based on frequencies extracted from the Brown corpus (Rudell, 1993). Other unsupervised approaches, such as those of Ligozat et al. (2012) and Glavaš and Štajner (2015), go a step further and use metrics that incorporate multiple aspects of word complexity, including context-aware features such as n-gram frequencies and language model probabilities. But even though unsupervised rankers perform well in the task, they are incapable of learning from data, which makes them unsuitable for adaptive SR. Our approach to adaptive SR is similar to our approach to adaptive CWI, namely to train an initial model over manually produced simplicity rankings, then continuously update them with new knowledge as Lexi users provide feedback on the simplifications they receive."
C18-1021,P14-2075,0,0.0420329,"own simplification needs, and there is considerable variation as to how well the specifics of what makes a text difficult is defined for each group and simplification strategy. While difficult items in a text may be identified more easily and generally for problems such as resolving pronoun reference, questions such as what makes a French word difficult for a native speaker of Japanese, or what dyslexic children consider a difficult character combination or an overly long sentence, are much harder to answer. Nevertheless, there is a vast body of work (Yatskar et al., 2010; Biran et al., 2011; Horn et al., 2014) that ventures to build very general-purpose simplification models from simplification corpora such as the Simple English Wikipedia corpus (Coster and Kauchak, 2011), which has been edited by amateurs without explicit regard to a specific audience, and with rather vague guidelines as to what constitutes difficult or simple language. Other work in simplification attempts to answer the above questions by inducing models from specifically compiled datasets, which for instance may have been collected by surveying specific target groups and asking them to indicate difficult material in a text. Yet"
C18-1021,S12-1068,0,0.173203,"in the mentioned shared tasks), such that data points sampled from users can quickly have an impact on a generic base model.2 3.1.2 Adaptive Substitution Ranking Substitution Ranking has received relatively little attention in the community compared to CWI. Most lexical simplifiers rank candidates using unsupervised approaches. The earliest example is the approach of Carroll et al. (1998), who rank candidates according to their Kucera-Francis coefficients, which are calculated based on frequencies extracted from the Brown corpus (Rudell, 1993). Other unsupervised approaches, such as those of Ligozat et al. (2012) and Glavaš and Štajner (2015), go a step further and use metrics that incorporate multiple aspects of word complexity, including context-aware features such as n-gram frequencies and language model probabilities. But even though unsupervised rankers perform well in the task, they are incapable of learning from data, which makes them unsuitable for adaptive SR. Our approach to adaptive SR is similar to our approach to adaptive CWI, namely to train an initial model over manually produced simplicity rankings, then continuously update them with new knowledge as Lexi users provide feedback on the"
C18-1021,W16-4901,0,0.023613,"f text simplification, including resources across languages, see Siddharthan (2014), Shardlow (2014b) and Collins-Thompson (2014). 1.1 There is no one-size-fits-all solution to text simplification Text simplification is a diverse task, or perhaps rather a family of tasks, with a number of different target audiences that different papers and research projects have focused on. Among the most prominent target audiences are foreign language learners, for whom various approaches to simplifying text have been pursued, often focusing on lexical (Tweissi, 1998) but also sentence-level simplification (Liu and Matsumoto, 2016). Other notable groups that have been specifically targeted in text simplification research include dyslexics (Rello et al., 2013), and the aphasic (Carroll et al., 1998), for whom particularly long words and sentences, but also certain surface forms such as specific character combinations, may pose difficulties. People on the autism spectrum have also been addressed, with the focus lying on reducing the amount of figurative expressions in a text or reducing syntactic complexity (Evans et al., 2014). Reading beginners (both children and adults) are another group with very particular needs, and"
C18-1021,P15-4015,1,0.947247,"ation, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users. 1 Introduction Many a research paper on text simplification starts out by sketching the problem of text simplification as rewriting a text such that it becomes easier to read, changing or removing as little of its informational content as possible (Zhu et al., 2010; Coster and Kauchak, 2011; De Belder and Moens, 2010; Paetzold and Specia, 2015; Bingel and Søgaard, 2016). Such a statement may describe the essence of simplification as a research task, but it hides the fact that it is not always easy to decide what is easy for a particular user. This paper discusses why we need custom-tailored simplifications for individual users, and argues that previous research on non-adaptive text simplification has been too generic to unfold the full potential of text simplification. Even when limiting ourselves to lexical substitution, i.e. the task of reducing the complexity of a document by replacing difficult words with easier-to-read synonym"
C18-1021,C16-2017,1,0.879594,"k (2006), who present HAPPI, a web platform that allows users to request simplified versions of words, as well as other “memory jogging” pieces of information, such as related images. Another example is the work of Azab et al. (2015), who present a web platform that allows users to select words they do not comprehend, then presents them with synonyms in order to facilitate comprehension. Notice that their approach does not simplify the selected complex words directly, it simply shows semantically equivalent alternatives that could be within the vocabulary known by the user. The recent work of Paetzold and Specia (2016a) describes Anita, yet another web platform of this kind. It allows users to select complex words and then request a simplified version, related images, synonyms, definitions and translations. Paetzold and Specia (2016a) claim that their approach outputs customized simplifications depending on the user’s profile, and evolves as users provide feedback on the output produced. However, they provide no details of the approach they use to do so, nor do they present any results showcasing its effectiveness. Therefore not counting Paetzold and Specia (2016a) as work in personalized simplification, w"
C18-1021,W16-4912,0,0.0339028,"k (2006), who present HAPPI, a web platform that allows users to request simplified versions of words, as well as other “memory jogging” pieces of information, such as related images. Another example is the work of Azab et al. (2015), who present a web platform that allows users to select words they do not comprehend, then presents them with synonyms in order to facilitate comprehension. Notice that their approach does not simplify the selected complex words directly, it simply shows semantically equivalent alternatives that could be within the vocabulary known by the user. The recent work of Paetzold and Specia (2016a) describes Anita, yet another web platform of this kind. It allows users to select complex words and then request a simplified version, related images, synonyms, definitions and translations. Paetzold and Specia (2016a) claim that their approach outputs customized simplifications depending on the user’s profile, and evolves as users provide feedback on the output produced. However, they provide no details of the approach they use to do so, nor do they present any results showcasing its effectiveness. Therefore not counting Paetzold and Specia (2016a) as work in personalized simplification, w"
C18-1021,E17-2006,1,0.831883,"to train an initial model over manually produced simplicity rankings, then continuously update them with new knowledge as Lexi users provide feedback on the simplifications they receive. The feedback in this scenario is composed of a complex word in context, a simplification produced by Lexi, and a binary rank provided by the user determining which word (complex or simplification) makes the sentence easier to understand. For that purpose, we need a supervised model that (i) supports online learning so that it can be efficiently updated after each session, and (ii) can learn from binary ranks. Paetzold and Specia (2017) offer some intuition on how this can be done. They exploit the fact that one can decompose a sequence of elements {e1 , e2 , ..., en } with ranks {r1 , r2 , ..., rn } into a matrix m ∈ Rn×n , such that m(i, j) = f (ri , rj ), and function f (ri , rj ) estimates a value that describes the relationship between the ranks of elements ei and ej . For example, f could be described as:   1 if ri < rj f (ri , rj ) −1 if ri > rj  0 otherwise (2) The ranker of Paetzold and Specia (2017) uses a deep multi-layer perceptron that predicts each value of m individually. It takes as input feature represent"
C18-1021,shardlow-2014-open,0,0.480113,"acing difficult words with easier-to-read synonyms, we see plenty of evidence that, for instance, dyslexics are highly individual in what material is deemed easy and complex (Ziegler et al., 2008). Lexi, which we introduce in this paper, is a free, open-source and easily extensible tool for adaptively learning what items specific users find difficult, using this information to provide better (lexical) simplification. Our system initially serves Danish, but is easily extended to further languages. For surveys of text simplification, including resources across languages, see Siddharthan (2014), Shardlow (2014b) and Collins-Thompson (2014). 1.1 There is no one-size-fits-all solution to text simplification Text simplification is a diverse task, or perhaps rather a family of tasks, with a number of different target audiences that different papers and research projects have focused on. Among the most prominent target audiences are foreign language learners, for whom various approaches to simplifying text have been pursued, often focusing on lexical (Tweissi, 1998) but also sentence-level simplification (Liu and Matsumoto, 2016). Other notable groups that have been specifically targeted in text simplif"
C18-1021,N10-1056,0,0.0358474,"10). Evidently, each target group has its own simplification needs, and there is considerable variation as to how well the specifics of what makes a text difficult is defined for each group and simplification strategy. While difficult items in a text may be identified more easily and generally for problems such as resolving pronoun reference, questions such as what makes a French word difficult for a native speaker of Japanese, or what dyslexic children consider a difficult character combination or an overly long sentence, are much harder to answer. Nevertheless, there is a vast body of work (Yatskar et al., 2010; Biran et al., 2011; Horn et al., 2014) that ventures to build very general-purpose simplification models from simplification corpora such as the Simple English Wikipedia corpus (Coster and Kauchak, 2011), which has been edited by amateurs without explicit regard to a specific audience, and with rather vague guidelines as to what constitutes difficult or simple language. Other work in simplification attempts to answer the above questions by inducing models from specifically compiled datasets, which for instance may have been collected by surveying specific target groups and asking them to ind"
C18-1021,W18-0507,1,0.863339,"tivity. 3.1 Adaptivity in the lexical simplification pipeline Lexical simplification, i.e. replacing single words with simpler synonyms, classically employs a pipeline approach illustrated in Figure 1 (Shardlow, 2014a; Paetzold and Specia, 2015). This pipeline consists of a four-step process, the first step of which is to identify simplification targets, i.e. words that the model believes will pose a difficulty for the user. This step is called Complex Word Identification (CWI) and has received a great deal of attention in the community, including two shared tasks (Paetzold and Specia, 2016b; Yimam et al., 2018). In a second step, known as Substitution Generation, synonyms are retrieved as candidate replacements for the target These are then filtered to match the context, resolving word sense ambiguities or stylistic mismatches, in Substitution Selection. Finally, those filtered candidate are ranked in order of simplicity in what is known as Substitution Ranking (SR). Out of these four steps, we consider CWI and SR as the most natural ones to make adaptive, whereas generation and selecting candidates can be regarded as relatively independent from a specific user. In order to implement adaptivity, we"
C18-1021,C10-1152,0,0.0475755,"needs of specific users. As a first step towards personalized simplification, we propose a framework for adaptive lexical simplification and introduce Lexi, a free open-source and easily extensible tool for adaptive, personalized text simplification. Lexi is easily installed as a browser extension, enabling easy access to the service for its users. 1 Introduction Many a research paper on text simplification starts out by sketching the problem of text simplification as rewriting a text such that it becomes easier to read, changing or removing as little of its informational content as possible (Zhu et al., 2010; Coster and Kauchak, 2011; De Belder and Moens, 2010; Paetzold and Specia, 2015; Bingel and Søgaard, 2016). Such a statement may describe the essence of simplification as a research task, but it hides the fact that it is not always easy to decide what is easy for a particular user. This paper discusses why we need custom-tailored simplifications for individual users, and argues that previous research on non-adaptive text simplification has been too generic to unfold the full potential of text simplification. Even when limiting ourselves to lexical substitution, i.e. the task of reducing the c"
E17-2006,P15-2011,0,0.513357,"Missing"
E17-2006,P14-2075,0,0.340551,"but most of them still adhere to the following pipeline: Complex Word Identification (CWI) to select words to simplify; Substitution Generation (SG) to produce candidate substitutions for each complex word; Substitution Selection (SS) to filter candidates that do not fit the context of the complex word; and Substitution Ranking (SR) to rank them according to their simplicity. The most effective LS approaches exploit Machine Learning techniques. In CWI, ensembles that use large corpora and thesauri dominate the top 10 systems in the CWI task of SemEval 2016 (Paetzold and Specia, 2016d). In SG, Horn et al. (2014) extract candidates from a parallel Wikipedia and Simple Wikipedia corpus, yielding major improvements over previous approaches 2 Hybrid Substitution Generation Our approach combines candidate substitutions from two sources: the Newsela corpus and retrofitted context-aware word embedding models. 34 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 34–40, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2.1 SG via Parallel Data their synonyms, then use the algorithm describ"
E17-2006,O13-1007,0,0.10872,"simplicity. Let M (ci , cj ) be the value estimated by our model for a pair of candidates ci and cj of a generated set C. During the ordering, we calculate the final score R(ci ) of all candidates ci (Eq. 1). X M (ci , cj ) Substitution Generation Evaluation Here we assess the performance of our SG approach in isolation (NNLS/SG), and when paired with our SS strategy (NNLS/SG+SS), as described in Sections 2 and 3. We compare them to the generators of all approaches featured in the benchmarks of Paetzold and Specia (2016a): Devlin (Devlin and Tait, 1998), Biran (Biran et al., 2011), Yamamoto (Kajiwara et al., 2013), Horn (Horn ˇ et al., 2014), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paetzold and Specia, 2015; Paetzold and Specia, 2016f). These SG strategies extract candidates from WordNet, Wikipedia and Simple Wikipedia articles, Merriam dictionary, sentencealigned Wikipedia and Simple Wikipedia articles, typical word embeddings and context-aware word embeddings, respectively. They are all available in the LEXenstein framework (Paetzold and Specia, 2015). We use two common evaluation datasets for LS: BenchLS (Paetzold and Specia, 2016a), which contains 929 instances and is annotated by English"
E17-2006,P13-1151,0,0.106186,"an Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 34–40, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2.1 SG via Parallel Data their synonyms, then use the algorithm described in (Faruqui et al., 2015). We train a bag-of-words (CBOW) model (Mikolov et al., 2013b) of 1,300 dimensions with word2vec (Mikolov et al., 2013a) using a corpus of over 7 billion words that includes the SubIMDB corpus (Paetzold and Specia, 2016b), UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We retrofit the model over WordNet’s synonym relations only. We choose this model training configuration because it has been shown to perform best for LS in a recent extensive benchmarking (Paetzold, 2016). For each target word in the Newsela vocabulary we then generate as complementary candidate substitutions the three words in the model with the lowest cosine distances from the target word that have the same POS tag and are not a morphological variant. As demonstrated by Paetzold and Specia (2016a), in SG parallel corpora tend to yield higher Precision, but noticeably lower Recall than emb"
E17-2006,W11-2107,0,0.0487807,"To employ the Newsela corpus in SG, we first produce sentence alignments for all pairs of versions of a given article. To do so, we use paragraph and sentence alignment algorithms from (Paetzold and Specia, 2016g). They align paragraphs with sentences that have high TF-IDF similarity, concatenate aligned paragraphs, and finally align concatenated paragraphs at sentence-level using the TF-IDF similarity between them. Using this algorithm, we produce 550,644 sentence alignments. We then tag sentences using the Stanford Tagger (Toutanvoa and Manning, 2000), produce word alignments using Meteor (Denkowski and Lavie, 2011), and extract candidates using a strategy similar to that of Horn et al. (2014). First we consider all aligned complex-to-simple word pairs as candidates. Then we filter them by discarding pairs which: do not share the same POS tag, have at least one non-content word, have at least one proper noun, or share the same stem. After filtering, we inflect all nouns, verbs, adjectives and adverbs to all possible variants. We then complement the candidate substitutions from the Newsela corpus using the following word embeddings model. 2.2 3 We pair our generator with the Unsupervised Boundary Ranking"
E17-2006,P15-4015,1,0.903509,"Missing"
E17-2006,L16-1491,1,0.908722,"Missing"
E17-2006,N15-1184,0,0.0523727,"Missing"
E17-2006,C16-1157,1,0.896984,"Missing"
E17-2006,W00-1308,0,0.108991,"Missing"
E17-2006,Q15-1021,0,0.111984,"Computer Science University of Sheffield, UK {g.h.paetzold,l.specia}@sheffield.ac.uk Abstract (Devlin, 1999; Biran et al., 2011). Glavaˇs and ˇ Stajner (2015) and Paetzold and Specia (2016f) employ word embedding models to generate candidates, leading to even better results. In SR, the state-of-the-art performance is achieved by employing supervised approaches: SVMRank (Horn et al., 2014) and Boundary Ranking (Paetzold and Specia, 2015). Supervised approaches have the caveat of requiring annotated data, but as a consequence they can adapt to the needs of a specific target audience. Recently, (Xu et al., 2015) introduced the Newsela corpus, a new resource composed of thousands of news articles simplified by professionals. Their analysis reveals the potential use of this corpus in simplification, but thus far no simplifiers exist that exploit this resource. The scale of this corpus and the fact that it was created by professionals opens new avenues for research, including using Neural Network approaches, which have proved promising for many related problems. Neural Networks for supervised ranking have performed well in Information Retrieval (Burges et al., 2005), Medical Risk Evaluation (Caruana et"
E17-2006,S16-1149,1,0.892293,"Missing"
E17-2006,W16-4912,0,0.554948,"simplifiers are more sophisticated, but most of them still adhere to the following pipeline: Complex Word Identification (CWI) to select words to simplify; Substitution Generation (SG) to produce candidate substitutions for each complex word; Substitution Selection (SS) to filter candidates that do not fit the context of the complex word; and Substitution Ranking (SR) to rank them according to their simplicity. The most effective LS approaches exploit Machine Learning techniques. In CWI, ensembles that use large corpora and thesauri dominate the top 10 systems in the CWI task of SemEval 2016 (Paetzold and Specia, 2016d). In SG, Horn et al. (2014) extract candidates from a parallel Wikipedia and Simple Wikipedia corpus, yielding major improvements over previous approaches 2 Hybrid Substitution Generation Our approach combines candidate substitutions from two sources: the Newsela corpus and retrofitted context-aware word embedding models. 34 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 34–40, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2.1 SG via Parallel Data their synonyms, t"
E17-2006,C16-1069,1,0.859164,"their synonyms, then use the algorithm described in (Faruqui et al., 2015). We train a bag-of-words (CBOW) model (Mikolov et al., 2013b) of 1,300 dimensions with word2vec (Mikolov et al., 2013a) using a corpus of over 7 billion words that includes the SubIMDB corpus (Paetzold and Specia, 2016b), UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We retrofit the model over WordNet’s synonym relations only. We choose this model training configuration because it has been shown to perform best for LS in a recent extensive benchmarking (Paetzold, 2016). For each target word in the Newsela vocabulary we then generate as complementary candidate substitutions the three words in the model with the lowest cosine distances from the target word that have the same POS tag and are not a morphological variant. As demonstrated by Paetzold and Specia (2016a), in SG parallel corpora tend to yield higher Precision, but noticeably lower Recall than embedding models. We add only three candidates in order increase Recall without compromising the high Precision from the Newsela corpus. corpus1 The Newsela (version 2016-01-29.1) contains 1,911 news articles i"
E17-2006,shardlow-2014-open,0,0.350489,"Missing"
E17-2006,S12-1046,1,0.69357,"titution Ranking Evaluation We also compare our Neural Ranking SR approach (NNLS/SR) to the rankers of all aforementioned lexical simplifiers. The Devlin, Biran, Yamamoto, Horn, Glavas and Paetzold rankers exploit Kucera-Francis coefficients (Rudell, 1993), hand-crafted complexity metrics, a supervised SVM ranker, rank averaging and Boundary Ranking, respectively. In this experiment we disregard the step of Confidence Check, since we aim to analyse the performance of our ranking strategy alone. The datasets used are those introduced for the English Lexical Simplification task of SemEval 2012 (Specia et al., 2012), to which dozens of systems were submitted. The training and test sets are composed of 300 and 1,710 instances, respectively. Each instance is composed of a sentence, a target complex word, and a series of candidate substitutions ranked by simplicity. We use TRank, the official metric of the SemEval 2012 task, which measures the proportion of instances for which the candidate with the highest goldrank was ranked first, as well Pearson (p) correlation. While TRank best captures the reliability of • Precision: The proportion of instances in which the target word was either replaced by a gold ca"
I17-1030,E17-2006,1,0.875786,"Missing"
I17-1030,P02-1040,0,0.100406,"thermore, we use a more reliable (professionally created) corpus and our approach is more flexible as we do not rely on syntactic parse trees at test time. recent work that relies on the professionally edited Newsela corpus (Xu et al., 2015). Simple English Wikipedia Zhu et al. (2010) propose a syntax-based translation model for TS that learns operations over the parse trees of the complex sentences. They outperform several baselines in terms of Flesch index. Coster and Kauchak (2011b) train a phrase-based machine translation (PBMT) system and obtain significant improvements in terms of BLEU (Papineni et al., 2002) over a baseline. Coster and Kauchak (2011a) extend a PBMT model to include phrase deletion and outperform Coster and Kauchak (2011b). Wubben et al. (2012) also train a PBMT system for TS with a dissimilarity-based re-ranking heuristic, outperforming Zhu et al. (2010) in terms of BLEU. Narayan and Gardent (2014) built TS systems by combining discourse representation structures with a PBMT model, which outperforms previous approaches. Xu et al. (2016) modify a syntax-based MT system in order to use a new metric – SARI – for optimization and to include special rules for paraphrasing. Although th"
I17-1030,W03-1004,0,0.14258,"el we implement as a baseline in this paper is equivalent to that in Zhang et al. (2017) without the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative form"
I17-1030,E17-3017,0,0.038702,"Missing"
I17-1030,P16-2055,1,0.841416,"s are learned from a more informed labeled FA and JB contributed equally to this paper. 295 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 295–305, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP dataset of natural simplifications, and can then be applied in a controlled way, e.g., in adaptive simplification scenarios that prioritize different ways of simplifying (e.g. compression or sentence splitting) depending on a particular user’s needs. The only previous work on TS via explicitly predicting simplification operations is that by Bingel and Søgaard (2016), who create training data from comparable text to label entire syntactic units and train a sequence labeling model to predict deletions and phrase substitutions in a complex sentence. Our approach is different in that it captures a larger variety of operations in a more global fashion, by using sentence-wide word alignments rather than surface heuristics. Furthermore, we use a more reliable (professionally created) corpus and our approach is more flexible as we do not rely on syntactic parse trees at test time. recent work that relies on the professionally edited Newsela corpus (Xu et al., 20"
I17-1030,W11-1603,0,0.0232335,"hout the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative formulations of the original text. With a few exceptions (by the neural model), they tend to"
I17-1030,W16-2323,0,0.0609456,"Missing"
I17-1030,W11-1601,0,0.796852,"Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS s"
I17-1030,N10-1063,0,0.0542761,"nt to that in Zhang et al. (2017) without the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative formulations of the original text. With a few excep"
I17-1030,P11-2117,0,0.827854,"Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS s"
I17-1030,2006.amta-papers.25,0,0.237758,"Missing"
I17-1030,W13-2901,0,0.0320694,"Missing"
I17-1030,Q14-1018,0,0.0483654,"Missing"
I17-1030,P07-2045,0,0.00635879,"t audience rather than research) resource to date. • Identical: The alignment is one-to-one and the sentences are exactly the same (96,909 pairs across all adjacent levels). • 1-to-1: The alignment is one-to-one and the original-simplified sentences are different (130,790 pairs across all adjacent levels). • Split: The alignment is 1-to-N (42,545 pairs across all adjacent levels). • Join: The alignment is N-to-1 (7,962 pairs across all adjacent levels). Translation Models. We built two types of models using state-of-the-art MT-based approaches: a phrase-based statistical MT model using Moses (Koehn et al., 2007),3 and a neural MT model using Nematus (Sennrich et al., 2017).4 The Neural Text Simplification tool (NTS) made available by Nisioi et al. (2017) was also used for comparison.5 For our translation-based experiments, we consider two combinations of sentence alignments, using (i) only one-to-one alignments (1-to-1) (130,970 sentence pairs), and (ii) all alignments (all), i.e., the entire sentence-aligned corpus with identical, 1-to-1, split and join alignments (278,206 sentence pairs). The first type of data (1to-1) is the focus of this paper (see §4). The latter variant is included in the exper"
I17-1030,D11-1038,0,0.271991,"Missing"
I17-1030,P12-1107,0,0.636948,"Missing"
I17-1030,P14-1041,0,0.764692,"resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotat"
I17-1030,Q15-1021,0,0.54195,"f.alva,g.h.paetzold,c.scarton,l.specia}@sheffield.ac.uk bingel@di.ku.dk (Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extens"
I17-1030,P17-2014,0,0.294853,"m given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degr"
I17-1030,Q16-1029,0,0.385239,"m favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These compl"
I17-1030,D17-1062,0,0.328819,"earning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sente"
I17-1030,C10-1152,0,0.835255,"bingel@di.ku.dk (Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural"
I17-3001,W03-1004,0,0.25572,"translators, simplifiers and summarizers that automate the process of adapting content. In order to do so, machine learning algorithms benefit from texts aligned at lower levels, such as paragraph, sentence, or even word levels. These alignments are however challenging to obtain since documents often do not even have the same number of sentences, i.e. they are comparable but not parallel. For monolingual texts, which are the focus of this paper, previous work has proposed different ways for obtaining sentence alignments: Xu et al. (2015) extract alignments based on a similarity metric, while Barzilay and Elhadad (2003) employ a more complex data-driven model, and Paetzold and Specia (2016) employ a vicinitydriven search method. However, we were not able to find any available and easy-to-use tool that allows one to align comparable documents at different levels of granularity. To solve that problem, we introduce MASSAlign: a user friendly tool that allows one to align monolingual comparable documents at both paragraph and sentence level, annotate words in aligned sentences with transformation labels, and also visualize the output produced. We introduce MASSAlign: a Python library for the alignment and annota"
I17-3001,W11-1603,0,0.0537678,"Missing"
I17-3001,N13-1073,0,0.0303595,"l. Some examples of operations include deletions, where words and/or phrases are discarded; and lexical simplifications, where words and/or phrases are replaced with more familiar alternatives. MASSAlign’s annotation module provides novel algorithms that automatically identify deletions, substitutions, re-orderings, and additions of words and phrases. The annotation module requires a pair of aligned sentences, their constituency parse trees, and the word alignments between them. To obtain word alignments, many consolidated tools can be employed, such as Giza++ (Och and Ney, 2003), fast align (Dyer et al., 2013), and the monolingual word aligner (Sultan et al., 2014). Our annotation algorithms only require that the word alignments be in 1-index Pharaoh format, which can be obtained from any of the previously mentioned tools. Our module first annotates word-level substitutions, deletions and additions: if two words are aligned and are not an exact match, the word in the original sentence receives a REPLACE tag; if a word in the original sentence is not aligned, it is annotated as a DELETE; and if a word in the modified sentence is not aligned, it is annotated as an ADD . There may be some cases of sub"
I17-3001,C16-1109,0,0.0844432,"Missing"
I17-3001,J03-1002,0,0.00979647,"performed at phrase or word-level. Some examples of operations include deletions, where words and/or phrases are discarded; and lexical simplifications, where words and/or phrases are replaced with more familiar alternatives. MASSAlign’s annotation module provides novel algorithms that automatically identify deletions, substitutions, re-orderings, and additions of words and phrases. The annotation module requires a pair of aligned sentences, their constituency parse trees, and the word alignments between them. To obtain word alignments, many consolidated tools can be employed, such as Giza++ (Och and Ney, 2003), fast align (Dyer et al., 2013), and the monolingual word aligner (Sultan et al., 2014). Our annotation algorithms only require that the word alignments be in 1-index Pharaoh format, which can be obtained from any of the previously mentioned tools. Our module first annotates word-level substitutions, deletions and additions: if two words are aligned and are not an exact match, the word in the original sentence receives a REPLACE tag; if a word in the original sentence is not aligned, it is annotated as a DELETE; and if a word in the modified sentence is not aligned, it is annotated as an ADD"
I17-3001,E17-2006,1,0.616628,"chical alignment approach, i.e. it exploits information from higher-level alignments to support and improve the quality of lower-level alignments. Moreover, the method can be used in documents that are not organized as a set of paragraphs: one can simply take each comparable document as a large paragraph and then apply the sentence-level alignment algorithm. The method is also entirely unsupervised and one can easily customize the alignment process by changing the similarity metric, the threshold α, or the sets of vicinities considered. Finally, this method has already been shown effective in Paetzold and Specia (2017), where it is used in the extraction of complex-to-simple word 2 We then proceed to labeling re-orderings (MOVE) by determining if the relative index of a word (considering preceding or following DELETE s and ADD s) in the original sentence changes in the modified one. Words that are kept, replaced or rewritten may be subject to re-orderings, such that a token may have more than one label (e.g. REPLACE and MOVE). For that, we extend the set of operations by the compound operations REPLACE + MOVE (RM) and REWRITE + MOVE (RWM). In order to capture operations that span across syntactic units, suc"
I17-3001,Q14-1018,0,0.0460055,"e words and/or phrases are discarded; and lexical simplifications, where words and/or phrases are replaced with more familiar alternatives. MASSAlign’s annotation module provides novel algorithms that automatically identify deletions, substitutions, re-orderings, and additions of words and phrases. The annotation module requires a pair of aligned sentences, their constituency parse trees, and the word alignments between them. To obtain word alignments, many consolidated tools can be employed, such as Giza++ (Och and Ney, 2003), fast align (Dyer et al., 2013), and the monolingual word aligner (Sultan et al., 2014). Our annotation algorithms only require that the word alignments be in 1-index Pharaoh format, which can be obtained from any of the previously mentioned tools. Our module first annotates word-level substitutions, deletions and additions: if two words are aligned and are not an exact match, the word in the original sentence receives a REPLACE tag; if a word in the original sentence is not aligned, it is annotated as a DELETE; and if a word in the modified sentence is not aligned, it is annotated as an ADD . There may be some cases of substitutions where two synonymous are not aligned. In orde"
I17-3001,Q15-1021,0,0.314955,"l Language Processing (NLP): it can be used in the training of automatic translators, simplifiers and summarizers that automate the process of adapting content. In order to do so, machine learning algorithms benefit from texts aligned at lower levels, such as paragraph, sentence, or even word levels. These alignments are however challenging to obtain since documents often do not even have the same number of sentences, i.e. they are comparable but not parallel. For monolingual texts, which are the focus of this paper, previous work has proposed different ways for obtaining sentence alignments: Xu et al. (2015) extract alignments based on a similarity metric, while Barzilay and Elhadad (2003) employ a more complex data-driven model, and Paetzold and Specia (2016) employ a vicinitydriven search method. However, we were not able to find any available and easy-to-use tool that allows one to align comparable documents at different levels of granularity. To solve that problem, we introduce MASSAlign: a user friendly tool that allows one to align monolingual comparable documents at both paragraph and sentence level, annotate words in aligned sentences with transformation labels, and also visualize the out"
L16-1491,P11-2087,0,0.181277,"us used for the Glavas generator, parsed with the Stanford Parser. Word vectors are trained using the Bag-ofWords (CBOW) architecture and 1300 vector dimensions. new resources have been used, such as aligned complex-tosimple parallel corpora (Paetzold, 2013; Paetzold and Specia, 2013; Horn et al., 2014) and word embedding models ˇ (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). 3.1. Systems We re-implemented the following SG systems for evaluation: • Devlin (Devlin and Tait, 1998): Extracts synonyms of complex words from WordNet 3.0 (Fellbaum, 1998). 3.2. Datasets and Metrics • Biran (Biran et al., 2011): Creates the Cartesian product between Wikipedia and Simple Wikipedia by simply pairing every word that appears in Wikipedia with every word in Simple Wikipedia. It then discards any pairs in which: As a gold-standard, we use the candidate substitutions in BenchLS. The evaluation metrics are: 1. At least one of the words is a stop-word, numeral or punctuation. 2. The words share the same lemma. 3. The words are not registered as synonyms or hypernyms in WordNet. • Precision: Proportion of generated substitutions that are in the gold-standard. • Yamamoto (Kajiwara et al., 2013): Given a comple"
L16-1491,W11-1603,0,0.0487713,"nd F1. Although the Precision obtained by the Belder selector is the highest, it comes at noticeable losses in Potential and Recall. 5. Substitution Ranking Substitution Ranking (SR) is the task of ranking candidates by their simplicity. The goal is to replace the complex word by its simplest candidate substitute. The most widely used SR strategy in the literature is metricbased ranking, in which candidates are ranked according to a manually crafted combination of features such as word frequency and length (Devlin and Tait, 1998; Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011; Bott and Saggion, 2011). Recently, however, more sophisticated supervised approaches have been explored, such as SVM rankers (Horn et al., 2014) and Boundary Ranking (Paetzold and Specia, 2015). 5.1. • Paetzold (Paetzold and Specia, 2015): Uses a supervised Boundary Ranking approach. It learns a ranking model from data using a binary classification setup inferred from the ranking examples. This strategy is the same one used by the Paetzold selector, but instead of learning the ranking model from training data obtained in unsupervised fashion, it learns the model from manually annotated data. 10 morphological, semant"
L16-1491,C12-1023,0,0.132126,"Missing"
L16-1491,J92-4003,0,0.0440002,"get complex word at least once in News Crawl. 3. The conditional probability of a candidate given the POS tag of the target word. To calculate this feature, we learn the probability distribution P (c|pt ), described in Equation 1, of all words in the News Crawl corpus. C(c, pt ) , p∈P C(c, p) • Belder (De Belder and Moens, 2010): Intersects the candidates generated for a target word with the words in a cluster in which the target word is included, as determined by a latent-variable language model. To replicate their approach, we learn 2, 000 word clusters using the Brown clustering algorithm (Brown et al., 1992). • Biran (Biran et al., 2011): Selects candidates using a word co-occurrence model. It first discards any candidates for which the cosine similarity between its co-occurrence vector and the co-occurrence vector of the sentence in which the target was found is smaller than a threshold value t1 . In order to avoid incoherent replacements, it then discards any candidates for which the cosine similarity between its common co-occurrence vector with the target word and the co-occurrence vector of the sentence is larger than a threshold value t2 . We train the co-occurrence model over the same corpu"
L16-1491,E99-1042,0,0.778639,"roduce BenchLS, a new dataset for the task. In the following Sections, we describe our dataset and present our experiments. 3. Substitution Generation Substitution Generation (SG) aims to produce candidate substitutions for complex words, which can be later ranked or filtered according to different criteria. This step does not take into account the ambiguity of words, i.e. it generates candidate substitutions for a word in all or any of its possible meanings. The most frequently used SG solution consists in extracting synonyms from linguistic databases, such as WordNet (Devlin and Tait, 1998; Carroll et al., 1999) or the UMLS database for medical content (Ong et al., 2007; Leroy et al., 2013). Recently, however, 3074 1 2 http://norvig.com/spell-correct.html http://www.statmt.org/wmt11/translation-task.html • Paetzold (Paetzold and Specia, 2016): Produces candidates using a context-aware word embeddings model. 10 candidates for each target word are retrieved with a model trained using the word2vec toolkit (Mikolov et al., 2013) over the same corpus used for the Glavas generator, parsed with the Stanford Parser. Word vectors are trained using the Bag-ofWords (CBOW) architecture and 1300 vector dimensions"
L16-1491,W10-1505,0,0.0123736,"systems require pre-generated candidates, in order to avoid any biases toward a given SG approach, we use candidates produced by generators altogether. 3076 Selector Lesk Aluisio Belder Biran Paetzold First Sense No Selection Pot. 0.337 0.916 0.297 0.478 0.851 0.207 0.940 Prec. 0.053 0.098 0.188 0.068 0.166 0.052 0.062 Rec. 0.075 0.398 0.057 0.185 0.284 0.036 0.438 F1 0.062 0.157 0.088 0.099 0.209 0.042 0.109 • Horn (Horn et al., 2014): Uses Support Vector Machines (Joachims, 2002) to learn a ranking model from data with several word and n-gram frequency features extracted from the Google 1T (Evert, 2010), Wikipedia and Simple Wikipedia corpora. ˇ • Glavas (Glavaˇs and Stajner, 2015): Ranks candidates according to several features, such as n-gram frequencies and word vector similarity with the target word, and then re-ranks them according to their average rankings. The word embeddings model used is the same one used by the Glavas generator, and n-gram frequencies were extracted from the Google 1T corˇ pus (Glavaˇs and Stajner, 2015). Table 3: SS benchmarking results 4.3. Results As illustrated in Table 3, only the Aluisio and Paetzold selectors have managed to obtain higher F1 scores than not"
L16-1491,P15-2011,0,0.376254,"Missing"
L16-1491,P14-2075,0,0.0677173,"is to replace the complex words in a text with simpler alternatives, without compromising its meaning or grammaticality. The LS task is often addressed as the series of steps in Figure 1, as introduced by (Devlin and Tait, 1998). Their work has inspired others to conceive new LS solutions for the aphasic (Carroll et al., 1998), dyslexic (Bott et al., 2012), illiterate (Watanabe et al., 2009), non-native English speakers (Paetzold, 2013; Paetzold and Specia, 2013), children (Kajiwara et al., 2013) and others. BenchLS: A New Dataset To create our dataset we combined two resources: the LexMTurk (Horn et al., 2014) and LSeval (De Belder and Moens, 2012) datasets. The instances in both datasets, 929 in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any misspelled candidates using Norvig’s algorithm1 . We trained our spelling model over the News Crawl2 corpus. 2. Inflection Correction: We inflected all cand"
L16-1491,O13-1007,0,0.383694,"plification, Text Simplification, Evaluation Dataset 1. Introduction 2. The goal of a Lexical Simplification (LS) system is to replace the complex words in a text with simpler alternatives, without compromising its meaning or grammaticality. The LS task is often addressed as the series of steps in Figure 1, as introduced by (Devlin and Tait, 1998). Their work has inspired others to conceive new LS solutions for the aphasic (Carroll et al., 1998), dyslexic (Bott et al., 2012), illiterate (Watanabe et al., 2009), non-native English speakers (Paetzold, 2013; Paetzold and Specia, 2013), children (Kajiwara et al., 2013) and others. BenchLS: A New Dataset To create our dataset we combined two resources: the LexMTurk (Horn et al., 2014) and LSeval (De Belder and Moens, 2012) datasets. The instances in both datasets, 929 in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any misspelled candidates using Norvig’s al"
L16-1491,P13-1151,0,0.0568814,"d the Stanford Parser. ˇ • Glavas (Glavaˇs and Stajner, 2015): Produces candidates using a word embeddings model. They retrieve the 10 words for which the embeddings vector has the highest cosine similarity with that of the target complex word, except for its morphological variants. Their model uses 200 vector dimensions and is trained with the GloVe toolkit (Pennington et al., 2014). We train their model over a corpus of 7 billion words which combines combines the SubIMDB corpus (Paetzold, 2015), UMBC webbase4 , News Crawl5 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). As illustrated in Table 1, the Paetzold generator outperforms all others, including the supervised Horn generator, by a considerable margin in almost all metrics used, revealing the potential of context-aware embedding models for SG. In order to further highlight the importance of using context-aware as opposed to traditional embedding models, we have trained a modified version of the Glavas generator. ˇ Instead of using the model specified in (Glavaˇs and Stajner, 2015), it uses a model trained with the same settings specified for the context-aware model of the Paetzold generator: the Bag-o"
L16-1491,P03-1054,0,0.0163441,"ion metrics are: 1. At least one of the words is a stop-word, numeral or punctuation. 2. The words share the same lemma. 3. The words are not registered as synonyms or hypernyms in WordNet. • Precision: Proportion of generated substitutions that are in the gold-standard. • Yamamoto (Kajiwara et al., 2013): Given a complex word, it retrieves its definition from a dictionary, annotates it using a POS tagger, and then extracts as candidates any words that have the same POS tag as the complex word itself. This system queries the Merriam Dictionary3 , and tags definitions with the Stanford Parser (Klein and Manning, 2003). • Potential: Proportion of instances in which at least one of the candidates generated is in the gold-standard. • Recall: The proportion of gold-standard substitutions that are among the generated substitutions. • F1: Harmonic mean between Precision & Recall. 3.3. Results Generator Devlin Biran Yamamoto Horn Glavas Paetzold • Horn (Horn et al., 2014): Produces alignments for complex-to-simple parallel corpora, then extracts any hcomplex → simplei pairs of aligned words in which: 1. The complex word is not a stop-word. 2. The POS tag of both words are the same. 3. Neither word is a proper nou"
L16-1491,J03-1002,0,0.0139895,"ween Precision & Recall. 3.3. Results Generator Devlin Biran Yamamoto Horn Glavas Paetzold • Horn (Horn et al., 2014): Produces alignments for complex-to-simple parallel corpora, then extracts any hcomplex → simplei pairs of aligned words in which: 1. The complex word is not a stop-word. 2. The POS tag of both words are the same. 3. Neither word is a proper noun. Pot. 0.647 0.610 0.360 0.569 0.724 0.856 Prec. 0.133 0.130 0.032 0.235 0.142 0.180 Rec. 0.153 0.144 0.087 0.131 0.191 0.252 F1 0.143 0.136 0.047 0.168 0.163 0.210 Table 1: SG benchmarking results To produce alignments, we use GIZA++ (Och and Ney, 2003). The words are then inflected to all their morphological forms using Morph Adorner (Burns, 2013). For this system, we use the parallel Wikipedia and Simple Wikipedia corpus (Horn et al., 2014) and the Stanford Parser. ˇ • Glavas (Glavaˇs and Stajner, 2015): Produces candidates using a word embeddings model. They retrieve the 10 words for which the embeddings vector has the highest cosine similarity with that of the target complex word, except for its morphological variants. Their model uses 200 vector dimensions and is trained with the GloVe toolkit (Pennington et al., 2014). We train their m"
L16-1491,W13-4813,1,0.948707,"stems evaluated. Keywords: Lexical Simplification, Text Simplification, Evaluation Dataset 1. Introduction 2. The goal of a Lexical Simplification (LS) system is to replace the complex words in a text with simpler alternatives, without compromising its meaning or grammaticality. The LS task is often addressed as the series of steps in Figure 1, as introduced by (Devlin and Tait, 1998). Their work has inspired others to conceive new LS solutions for the aphasic (Carroll et al., 1998), dyslexic (Bott et al., 2012), illiterate (Watanabe et al., 2009), non-native English speakers (Paetzold, 2013; Paetzold and Specia, 2013), children (Kajiwara et al., 2013) and others. BenchLS: A New Dataset To create our dataset we combined two resources: the LexMTurk (Horn et al., 2014) and LSeval (De Belder and Moens, 2012) datasets. The instances in both datasets, 929 in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any missp"
L16-1491,P15-4015,1,0.909843,"in total, contain a sentence, a target complex word, and several candidate substitutions ranked according to their simplicity. The candidates in both datasets were suggested and ranked by English speakers from the U.S. To increase its reliability, we applied the following corrections over each instance of our dataset: 1. Spelling Filtering: We discard any misspelled candidates using Norvig’s algorithm1 . We trained our spelling model over the News Crawl2 corpus. 2. Inflection Correction: We inflected all candidates to the tense of the target word using the Text Adorning module of LEXenstein (Paetzold and Specia, 2015; Burns, 2013). The resulting dataset – BenchLS – contains 929 instances, with an average of 7.37 candidate substitutions per complex word. We use it in all our experiments, as described in what follows. Figure 1: Lexical Simplification Pipeline Although various LS systems can be found in the literature, very little effort has been made to compare their performance. Apart from the work of (Shardlow, 2013) and (Specia et al., 2012), which provide brief benchmarkings of Complex Word Identification and Substitution Ranking, respectively, no other comparisons have been reported. To address this li"
L16-1491,W16-4912,0,0.328204,"x words, which can be later ranked or filtered according to different criteria. This step does not take into account the ambiguity of words, i.e. it generates candidate substitutions for a word in all or any of its possible meanings. The most frequently used SG solution consists in extracting synonyms from linguistic databases, such as WordNet (Devlin and Tait, 1998; Carroll et al., 1999) or the UMLS database for medical content (Ong et al., 2007; Leroy et al., 2013). Recently, however, 3074 1 2 http://norvig.com/spell-correct.html http://www.statmt.org/wmt11/translation-task.html • Paetzold (Paetzold and Specia, 2016): Produces candidates using a context-aware word embeddings model. 10 candidates for each target word are retrieved with a model trained using the word2vec toolkit (Mikolov et al., 2013) over the same corpus used for the Glavas generator, parsed with the Stanford Parser. Word vectors are trained using the Bag-ofWords (CBOW) architecture and 1300 vector dimensions. new resources have been used, such as aligned complex-tosimple parallel corpora (Paetzold, 2013; Paetzold and Specia, 2013; Horn et al., 2014) and word embedding models ˇ (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). 3.1. S"
L16-1491,N15-2002,1,0.819422,"r (Burns, 2013). For this system, we use the parallel Wikipedia and Simple Wikipedia corpus (Horn et al., 2014) and the Stanford Parser. ˇ • Glavas (Glavaˇs and Stajner, 2015): Produces candidates using a word embeddings model. They retrieve the 10 words for which the embeddings vector has the highest cosine similarity with that of the target complex word, except for its morphological variants. Their model uses 200 vector dimensions and is trained with the GloVe toolkit (Pennington et al., 2014). We train their model over a corpus of 7 billion words which combines combines the SubIMDB corpus (Paetzold, 2015), UMBC webbase4 , News Crawl5 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). As illustrated in Table 1, the Paetzold generator outperforms all others, including the supervised Horn generator, by a considerable margin in almost all metrics used, revealing the potential of context-aware embedding models for SG. In order to further highlight the importance of using context-aware as opposed to traditional embedding models, we have trained a modified version of the Glavas generator. ˇ Instead of using the model specified in (Glavaˇs and Stajner, 2015), it uses"
L16-1491,D14-1162,0,0.0790735,"Missing"
L16-1491,W04-2013,0,0.0154432,"Missing"
L16-1491,P13-3015,0,0.0386882,"d our spelling model over the News Crawl2 corpus. 2. Inflection Correction: We inflected all candidates to the tense of the target word using the Text Adorning module of LEXenstein (Paetzold and Specia, 2015; Burns, 2013). The resulting dataset – BenchLS – contains 929 instances, with an average of 7.37 candidate substitutions per complex word. We use it in all our experiments, as described in what follows. Figure 1: Lexical Simplification Pipeline Although various LS systems can be found in the literature, very little effort has been made to compare their performance. Apart from the work of (Shardlow, 2013) and (Specia et al., 2012), which provide brief benchmarkings of Complex Word Identification and Substitution Ranking, respectively, no other comparisons have been reported. To address this limitation, we present a systematic benchmarking of LS systems. We innovate by comparing the performance of not only systems in their entirety, but also of system components individually, such as Substitution Generation, Selection and Ranking approaches. In addition, we introduce BenchLS, a new dataset for the task. In the following Sections, we describe our dataset and present our experiments. 3. Substitut"
L16-1491,S12-1046,1,0.846175,"Missing"
L18-1553,W03-1004,0,0.239373,"a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all"
L18-1553,P11-2087,0,0.024947,"Missing"
L18-1553,W11-1603,0,0.01484,"s a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments (Stajner et al., 2017), while still offeri"
L18-1553,P11-2117,0,0.247664,"work. Keywords: text Simplification, simplification corpora, Newsela 1. Introduction Text Simplification (TS) consists in making texts more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS"
L18-1553,W11-2107,0,0.0303121,"old 0.352 0.341 0.238 0.231 0.378 0.400 Table 4: Accuracy in the full pipeline evaluation Here we assess the potential of our corpus in LS. LS is commonly addressed as a pipeline of steps: candidates for a target complex word are produced via a Substitution Generation (SG) method, filtered with respect to the context of the complex word via a Substitution Selection (SS) method, and finally ordered for simplicity by a Substitution Ranking (SR) method. We use our aligned corpus for SG following the state of the art approach in (Horn et al., 2014). First, we produce word alignments using Meteor (Denkowski and Lavie, 2011) and extract complex-to-simple word correspondences. Then we filter word pairs with different POS tags, where the complex word is a stop word, or either word is a proper noun. Finally, we generate all possible inflections for nouns and verbs (Burns, 2013). We compare this approach to six other generators from a recent benchmark (Paetzold and Specia, 2016a): the Horn generator (Horn et al., 2014), which employs the approach described above over Wikipedia-Simple Wikipedia data, the Devlin (Devlin and Tait, 1998), Biran (Biran et ˇ al., 2011), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paet"
L18-1553,W13-2901,0,0.129862,"e. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 20"
L18-1553,P15-2011,0,0.0613146,"Missing"
L18-1553,P14-2075,0,0.0841375,"implification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplification operations, which are applied in ad hoc ways by volunteer editors. Xu et al. (2015) introduce a new resou"
L18-1553,P07-2045,0,0.00703543,"Missing"
L18-1553,W13-4813,1,0.897302,"s more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence"
L18-1553,P15-4015,1,0.92596,"verbs (Burns, 2013). We compare this approach to six other generators from a recent benchmark (Paetzold and Specia, 2016a): the Horn generator (Horn et al., 2014), which employs the approach described above over Wikipedia-Simple Wikipedia data, the Devlin (Devlin and Tait, 1998), Biran (Biran et ˇ al., 2011), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paetzold and Specia, 2016c) generators, which exploit WordNet, comparable complex-to-simple documents, typical word embeddings and context-aware word embeddings, respectively. All generators were implemented with the LEXenstein framework (Paetzold and Specia, 2015). We use the BenchLS dataset as our gold-standard dataset (Paetzold and Specia, 2016a). It is the largest dataset of its kind, with 929 instances, each composed by a sentence, a target complex word, and a set of gold substitutions given by humans. To compare the generators, we use standard metrics: Potential – the proportion of instances in which at least one of the candidates generated is in the gold-standard, Precision – the proportion of generated substitutions that are in the gold-standard, Recall – the proportion of goldstandard substitutions that are among the generated substitutions, an"
L18-1553,L16-1491,1,0.858958,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,N16-1050,1,0.859101,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,W16-4912,0,0.0361996,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,N10-1063,0,0.0372608,"ng level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments"
L18-1553,P17-2016,0,0.0934999,"Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments (Stajner et al., 2017), while still offering comparable alignment accuracy. The result of the alignment is a corpus with 19, 198 pairs of articles aligned at both paragraph (300, 475 pairs) and sentence (550, 644 pairs) levels. This is over three times larger than the Wikipedia–Simple Wikipedia corpus (Coster and Kauchak, 2011), making it the largest corpus of its kind. Columns 2 to 4 in Table 1 illustrate the number of paragraph and sentence alignments for all version pairs in the corpus. We categorise the sentence alignments according to four types of simplification: • None: Complex and simple sentences are ident"
L18-1553,D11-1038,0,0.0732401,"ch complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar mo"
L18-1553,Q15-1021,0,0.457792,"hu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplification operations, which are applied in ad hoc ways by volunteer editors. Xu et al. (2015) introduce a new resource that allegedly addresses these limitations: the Newsela corpus (Newsela, 2016). Unlike Simple Wikipedia, the Newsela corpus was created by professional editors and targets a specific audience (students), which should make it a more reliable resource for TS. However,"
L18-1553,Q16-1029,0,0.0482634,"0.330 0.238 0.272 0.330 0.240 FLESCH-S 66.93 71.12 67.29 75.04 87.49 70.34 75.10 87.52 70.41 FLESCH-O 66.21 69.85 65.34 74.89 87.33 70.19 74.89 87.33 70.19 FLESCH-R 74.32 69.85 79.98 80.62 87.33 78.91 80.62 87.33 78.91 Table 5: Results for SMT-based simplifiers not a reliable metric when original, reference and simplified sentences are the same. For all cases where TER = 0, the SARI value was 0.330, which can be seem as a low value if the systems are producing an output equal to the reference. Since this metric was designed for cases where sentences should also be simplified (as explained in Xu et al. (2016)), the use of SARI for cases where the original sentences are already simple is not reliable. 6. Conclusions Upon studying the sentence-aligned Newsela corpus we found that: (i) it follows an expected TER distribution, with the lowest TER being between adjacent levels; (ii) the simplified sentences score as more readable than their original counterparts according to traditional readability metrics, and (iii) the corpus proved a more reliable source of complex-simple correspondences for LS and MT-based simplification than the Wikipedia-Simple Wikipedia corpus. We achieve some the highest perfor"
L18-1553,N10-1056,0,0.0550178,"ation (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplificat"
L18-1553,D17-1062,0,0.0627507,"Missing"
L18-1553,C10-1152,0,0.235267,"ification (TS) consists in making texts more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015)"
L18-1685,P11-2087,0,0.00887031,"ification (TS) is the task of reducing lexical and/or structural complexity of texts (Siddharthan, 2004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan an"
L18-1685,bott-etal-2012-text,0,0.147002,"focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplificat"
L18-1685,W14-1206,0,0.0470791,"Missing"
L18-1685,W09-2105,1,0.911308,"Missing"
L18-1685,E99-1042,0,0.257767,"ntroduction Text simplification (TS) is the task of reducing lexical and/or structural complexity of texts (Siddharthan, 2004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et"
L18-1685,W11-1601,0,0.136429,"l et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specifi"
L18-1685,P15-2011,0,0.0783328,"Missing"
L18-1685,P14-2075,0,0.0215017,"implification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations joint"
L18-1685,P14-1041,0,0.258899,"al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS e"
L18-1685,P17-2014,0,0.261149,"Missing"
L18-1685,W13-4813,1,0.897804,"in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain"
L18-1685,L16-1491,1,0.915491,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,N16-1050,1,0.921091,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,W16-4912,0,0.0353036,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,E17-2006,1,0.871376,"and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main pa"
L18-1685,I17-3007,1,0.734194,"ng passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish a"
L18-1685,E14-1076,0,0.018032,"de simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not b"
L18-1685,W11-2802,0,0.14289,"ement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela"
L18-1685,D11-1038,0,0.0952986,"S has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover"
L18-1685,P12-1107,0,0.437046,"Missing"
L18-1685,Q16-1029,0,0.334396,"proaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS evaluation, only a"
L18-1685,D17-1062,0,0.0586889,"word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS evaluation, only a few exist, mostly for E"
L18-1685,C10-1152,0,0.727031,"approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to mo"
N15-2002,P11-2087,0,0.36042,"ord. The words’ POS tags are used in the works of (Aluisio and Gasperin, 2010), (Yamamoto, 2013) and (Paetzold and Specia, 2013). While using POS tags may help with words of more than one grammatical type, it does not solve the problem of highly ambiguous words. Semantic Similarity Estimate the semantic similarity between words and verify if they are replaceable. In (Keskis¨arkk¨a, 2012) is employed a simple approach: if a pair of words has a synonymy coefficient higher than a threshold, they are replaceable. This approach, however, requires for a database of synonymy levels. The approach of (Biran et al., 2011) solves that by representing the semantic context of words with word vectors estimated over large corpora, then using the cosine distance between vectors as its semantic dissimilarity. We did not find mentions of Machine Learning methods being applied to SS. Such methods have been used to produce state-of-the-art results in many classification tasks, and hence modelling SS as a classification problem can be a promising strategy. 2.4 Substitution Ranking Consists in deciding which substitution is the simplest of the ones available. The LS task of SemEval 11 2012 brought a lot of visibility to t"
N15-2002,C12-1023,0,0.199591,"part of a lexicon L of complex/simple words, then it does/does not need to be simplified. While (Watanabe and Junior, 2009) and (Aluisio and Gasperin, 2010) use as lexicons books for children, (Elhadad and Sutaria, 2007), (Del´eger and Zweigenbaum, 2009) and (Elhadad, 2006) use a database of complex medical terms. Acquiring lexicons can be easy, but they must correlate with the needs of the target audience in question. Threshold-Based Explore the hypothesis that a threshold t over a word metric M (w) can separate complex from simple words. The most frequently used metrics are word frequency (Bott et al., 2012), (Leroy et al., 2013) and word length (Keskis¨arkk¨a, 2012). However, the corpus evaluation of (Bott et al., 2012) shows that determining such threshold t is impractical. User-Driven Such approaches allow the users themselves to select which words are complex, and simplify them on demand. Although the results obtained by (Devlin and Unthank, 2006) and (Rello et al., 2013) show that this is a very effective strategy, it might be difficult for it to be used in smaller devices, such as phones. Classification Methods Train classifiers which discriminate between complex and simple words. For Engli"
N15-2002,E99-1042,0,0.77292,"Missing"
N15-2002,C12-1034,0,0.0367967,"Missing"
N15-2002,W09-3102,0,0.129377,"Missing"
N15-2002,W07-1007,0,0.0861351,"nt Research Workshop (SRW), pages 9–16, c Denver, Colorado, June 1, 2015. 2015 Association for Computational Linguistics 1999) choose to not address this task, but as shown in (Paetzold and Specia, 2013), this can lead to the production of incoherent and/or ungrammatical sentences. Several categories of CWI strategies can be found in literature: Lexicon-Based Explore the hypothesis that, if a word w is part of a lexicon L of complex/simple words, then it does/does not need to be simplified. While (Watanabe and Junior, 2009) and (Aluisio and Gasperin, 2010) use as lexicons books for children, (Elhadad and Sutaria, 2007), (Del´eger and Zweigenbaum, 2009) and (Elhadad, 2006) use a database of complex medical terms. Acquiring lexicons can be easy, but they must correlate with the needs of the target audience in question. Threshold-Based Explore the hypothesis that a threshold t over a word metric M (w) can separate complex from simple words. The most frequently used metrics are word frequency (Bott et al., 2012), (Leroy et al., 2013) and word length (Keskis¨arkk¨a, 2012). However, the corpus evaluation of (Bott et al., 2012) shows that determining such threshold t is impractical. User-Driven Such approaches all"
N15-2002,W13-2901,0,0.0601502,"Missing"
N15-2002,P13-2121,0,0.0201537,"Missing"
N15-2002,P14-2075,0,0.492457,"oach of (Carroll et al., 1998) are caused by WordNet not having simpler synonyms for complex words. Using such resources also limits the cross-lingual capabilities of the approach, since most of those resources are restricted to one or very few languages. Automatic Generation Consists in automatically generating pairs of related words and paraphrases. The works of (Elhadad and Sutaria, 2007), (Kauchak and Barzilay, 2006) and (Del´eger and Zweigenbaum, 2009) focus on extracting paraphrases from comparable documents. The methods of (Paetzold and Specia, 2013), (Feblowitz and Kauchak, 2013), and(Horn et al., 2014) extract pairs of similar expressions from a aligned sentences from Wikipedia and Simple Wikipedia. But although such approaches do not need linguistic databases, they require for other resources, such as parallel corpora, which are also scarce. They can also suffer for extracting too many meaningless substitutions, such as observed in (Paetzold and Specia, 2013). In order to solve the cross-lingual problem, an SG approach would have to be able to find substitutions by exploiting only resources which are either abundant in most languages or easy to produce. In Section 3 we discuss how we attem"
N15-2002,S12-1066,0,0.0346741,"ity Elaborate metrics to represent the simplicity of a word. The metric of (Sinha, 2012) considers the word’s length, number of senses and frequency, and have tied in 2nd place in SemEval 2012 with the Google 1T baseline. The other examples in literature, (Biran et al., 2011) and (Bott et al., 2012), were published before SemEval 2012, and hence have not yet been compared to other approaches. Linear Scoring Functions Rank candidates based on a linear scoring function over various metrics, such as frequency and word length. This strategy is used by the approach that placed 1st in SemEval 2012 (Jauhar and Specia, 2012). In (Shardlow, 2014) it is shown that word frequencies from spoken text corpora have great potential in SR. In Section 3.4 we describe an experiment which reveals the potential of such resources. 3 Planning and Preliminary Results In the following Sections, we discuss which challenges we aim to address in the near future, and briefly describe the solutions we intend explore. 3.1 User Studies and Datasets As pointed out in Section 2, the scarcity of user studies about audiences that may benefit from LS compel authors to treat simplification as a generalised process, forcing them to use dataset"
N15-2002,P13-1151,0,0.456194,"kes made by the approach of (Carroll et al., 1998) are caused by WordNet not having simpler synonyms for complex words. Using such resources also limits the cross-lingual capabilities of the approach, since most of those resources are restricted to one or very few languages. Automatic Generation Consists in automatically generating pairs of related words and paraphrases. The works of (Elhadad and Sutaria, 2007), (Kauchak and Barzilay, 2006) and (Del´eger and Zweigenbaum, 2009) focus on extracting paraphrases from comparable documents. The methods of (Paetzold and Specia, 2013), (Feblowitz and Kauchak, 2013), and(Horn et al., 2014) extract pairs of similar expressions from a aligned sentences from Wikipedia and Simple Wikipedia. But although such approaches do not need linguistic databases, they require for other resources, such as parallel corpora, which are also scarce. They can also suffer for extracting too many meaningless substitutions, such as observed in (Paetzold and Specia, 2013). In order to solve the cross-lingual problem, an SG approach would have to be able to find substitutions by exploiting only resources which are either abundant in most languages or easy to produce. In Section 3"
N15-2002,N06-1058,0,0.141715,"Missing"
N15-2002,W13-4813,1,0.903078,"y. For this reason, we choose instead to discuss the merits and limitations of strategies used by authors to address each step of the LS pipeline. 2.1 Complex Word Identification The goal of Complex Word Identification (CWI) is to identify which words in a given sentence need to be simplified. Some authors, such as (Devlin and Tait, 1998), (Carroll et al., 1998) and (Carroll et al., 9 Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 9–16, c Denver, Colorado, June 1, 2015. 2015 Association for Computational Linguistics 1999) choose to not address this task, but as shown in (Paetzold and Specia, 2013), this can lead to the production of incoherent and/or ungrammatical sentences. Several categories of CWI strategies can be found in literature: Lexicon-Based Explore the hypothesis that, if a word w is part of a lexicon L of complex/simple words, then it does/does not need to be simplified. While (Watanabe and Junior, 2009) and (Aluisio and Gasperin, 2010) use as lexicons books for children, (Elhadad and Sutaria, 2007), (Del´eger and Zweigenbaum, 2009) and (Elhadad, 2006) use a database of complex medical terms. Acquiring lexicons can be easy, but they must correlate with the needs of the tar"
N15-2002,W04-2013,0,0.0813841,"ve to be able to find substitutions by exploiting only resources which are either abundant in most languages or easy to produce. In Section 3 we discuss how we attempt to address this problem. 2.3 Substitution Selection Substitution Selection (SS) is the task of determining which substitutions fit the context in which a complex word appears, and hence ensuring meaning preservation. SS have been addressed by authors in three ways: Word Sense Disambiguation Determine the sense of a complex word in a target sentence, and then filter substitutions which do not share such sense. The approaches of (Sedding and Kazakov, 2004) and (Nunes et al., 2013) have proven to be successful in SS alone, but have not been evaluated in practice. The main limitation of this strategy is that it relies on manually constructed sense databases, which are scarce. Adapted Disambiguation Use surrogate classes to discriminate between the meanings of an ambiguous word. The words’ POS tags are used in the works of (Aluisio and Gasperin, 2010), (Yamamoto, 2013) and (Paetzold and Specia, 2013). While using POS tags may help with words of more than one grammatical type, it does not solve the problem of highly ambiguous words. Semantic Simila"
N15-2002,P13-3015,0,0.485137,"d word length (Keskis¨arkk¨a, 2012). However, the corpus evaluation of (Bott et al., 2012) shows that determining such threshold t is impractical. User-Driven Such approaches allow the users themselves to select which words are complex, and simplify them on demand. Although the results obtained by (Devlin and Unthank, 2006) and (Rello et al., 2013) show that this is a very effective strategy, it might be difficult for it to be used in smaller devices, such as phones. Classification Methods Train classifiers which discriminate between complex and simple words. For English, the SVM approach of (Shardlow, 2013a) is the only example in literature. Although their study shows that their SVM is not able to outperform neither a threshold-based approach or a “simplify everything” method, we believe the results obtained are controversial. In another study conducted by the same author (Shardlow, 2014) it was found that replacing words which do not need simplification is one of the most frequent mistakes made by naive LS approaches, and hence we believe the results obtained by (Shardlow, 2013a) do not reveal the potential of classification methods in CWI. Also, the dataset used the 10 experiments of (Shardl"
N15-2002,S12-1069,0,0.0211193,"the simpler it is. Most authors use raw frequencies from large corpora (Keskis¨arkk¨a, 2012), (Leroy et al., 2013), (Aluisio and Gasperin, 2010), (Nunes et al., 2013) or the Kucera-Francis coefficient (Rudell, 1993), (Devlin and Tait, 1998), (Carroll et al., 1998). Although (Brysbaert and New, 2009) points out several issues with the Kucera-Francis coefficient, the results of SemEval 2012 (Specia et al., 2012) show that raw frequencies from the Google 1T corpus outperform almost all other approaches. Measuring Simplicity Elaborate metrics to represent the simplicity of a word. The metric of (Sinha, 2012) considers the word’s length, number of senses and frequency, and have tied in 2nd place in SemEval 2012 with the Google 1T baseline. The other examples in literature, (Biran et al., 2011) and (Bott et al., 2012), were published before SemEval 2012, and hence have not yet been compared to other approaches. Linear Scoring Functions Rank candidates based on a linear scoring function over various metrics, such as frequency and word length. This strategy is used by the approach that placed 1st in SemEval 2012 (Jauhar and Specia, 2012). In (Shardlow, 2014) it is shown that word frequencies from spo"
N15-2002,S12-1046,0,0.322858,"Missing"
N15-2002,O13-1007,0,0.134923,"ays: Word Sense Disambiguation Determine the sense of a complex word in a target sentence, and then filter substitutions which do not share such sense. The approaches of (Sedding and Kazakov, 2004) and (Nunes et al., 2013) have proven to be successful in SS alone, but have not been evaluated in practice. The main limitation of this strategy is that it relies on manually constructed sense databases, which are scarce. Adapted Disambiguation Use surrogate classes to discriminate between the meanings of an ambiguous word. The words’ POS tags are used in the works of (Aluisio and Gasperin, 2010), (Yamamoto, 2013) and (Paetzold and Specia, 2013). While using POS tags may help with words of more than one grammatical type, it does not solve the problem of highly ambiguous words. Semantic Similarity Estimate the semantic similarity between words and verify if they are replaceable. In (Keskis¨arkk¨a, 2012) is employed a simple approach: if a pair of words has a synonymy coefficient higher than a threshold, they are replaceable. This approach, however, requires for a database of synonymy levels. The approach of (Biran et al., 2011) solves that by representing the semantic context of words with word vectors"
N15-2002,W10-1600,0,\N,Missing
N16-1050,P15-2011,0,0.145597,"Missing"
N16-1050,P14-2118,0,0.0590501,"ound that humans When quantified, these aspects can be used as features for various Natural Language Processing (NLP) tasks. The Lexical Simplification approach in (Jauhar and Specia, 2012) is an example. By combining various collocational features and psycholinguistic measures extracted from the MRC Psycholinguistic Database (Coltheart, 1981), they trained a ranker (Joachims, 2002) that reached first place in the English Lexical Simplification task at SemEval 2012. Semantic Classification tasks have also benefited from the use of such features: by combining Concreteness with other features, (Hill and Korhonen, 2014) reached the state-of-theart performance in Semantic Composition (denotative/connotative) and Semantic Modification (intersective/subsective) prediction. Despite the evident usefulness of psycholinguistic properties of words, resources describing such properties are rare. The most extensively developed resource for English is the MRC Psycholinguistic Database (Section 2). However, it is far from complete, most likely due to the inherent cost of manually entering such properties. In this paper we propose a method to automatically infer these missing properties. We train regressors by performing"
N16-1050,P14-2075,0,0.0519683,"vised ranking algorithms capture word simplicity. Using the parameters described in Section 4, we train bootstrappers for these two properties using all instances in the MRC Database as seeds. We then train three rankers with (W) and without (W/O) psycholinguistic features: • Age of Acquisition: 700 word vector dimensions with a CBOW model, and ζ = 0.7. • Concreteness: 1,100 word vector dimensions with a Skip-Gram model, and ζ = 0.7. • Imagery: 1,100 word vector dimensions with a Skip-Gram model, and ζ = 0.7. 438 5 Psycholinguistic Features for LS 4 http://ghpaetzold.github.io/subimdb • Horn (Horn et al., 2014): Uses an SVM ranker trained on various n-gram probability features. ˇ • Glavas (Glavaˇs and Stajner, 2015): Ranks candidates using various collocational and semantic metrics, and then re-ranks them according to their average rankings. • Paetzold (Paetzold and Specia, 2015): Ranks words according to their distance to a decision boundary learned from a classification setup inferred from ranking examples. Uses n-gram frequencies as features. We use data from the English Lexical Simplification task of SemEval 2012 to assess systems’ performance. The goal of the task is to rank words in different"
N16-1050,S12-1066,1,0.939075,"life. Other examples of psycholinguistic properties, such as Familiarity and Concreteness, influence one’s proficiency in word recognition and text comprehension. The experiments in (Connine et al., 1990; Morrel-Samuels and Krauss, 1992) show that words with high Familiarity yield lower reaction times in both visual and auditory lexical decision, and require less hand gesticulation in order to be described. Begg and Paivio (1969) found that humans When quantified, these aspects can be used as features for various Natural Language Processing (NLP) tasks. The Lexical Simplification approach in (Jauhar and Specia, 2012) is an example. By combining various collocational features and psycholinguistic measures extracted from the MRC Psycholinguistic Database (Coltheart, 1981), they trained a ranker (Joachims, 2002) that reached first place in the English Lexical Simplification task at SemEval 2012. Semantic Classification tasks have also benefited from the use of such features: by combining Concreteness with other features, (Hill and Korhonen, 2014) reached the state-of-theart performance in Semantic Composition (denotative/connotative) and Semantic Modification (intersective/subsective) prediction. Despite the"
N16-1050,P13-1151,0,0.0512709,"revious work for this task, in these experiments, we compare the performance of our bootstrapping strategy to various baselines. For training, we use the Ridge regression algorithm (Tikhonov, 1963). As features, our regressor uses the word’s raw embedding values, along with the following 15 lexical features: • Word’s length and number of syllables, as determined by the Morph Adorner module of LEXenstein (Paetzold and Specia, 2015). • Word’s frequency in the Brown (Francis and Kucera, 1979), SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016), Wikipedia and Simple Wikipedia (Kauchak, 2013) corpora. • Number of senses, synonyms, hypernyms and hyponyms for word in WordNet (Fellbaum, 1998). • Minimum, maximum and average distance between the word’s senses in WordNet and the thesaurus’ root sense. • Number of images found for word in the Getty Images database1 . We train our embedding models using word2vec (Mikolov et al., 2013a) over a corpus of 7 billion words composed by the SubIMDB corpus, UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We use 5-fold cross-validation to optimise parameters: ζ, embeddings model arc"
N16-1050,N13-1090,0,0.245663,"one instance was added to S, go to step 2, otherwise, return the resulting classifier. One critical difference between this approach and ours is that our task requires regression algorithms instead of classifiers. In classification, the prediction confidence c is often calculated as the maximum signed distance between an instance and the estimated hyperplanes. There is, however, no analogous confidence estimation technique for regression problems. We address this problem by using word embedding models. Embedding models have been proved effective in capturing linguistic regularities of words (Mikolov et al., 2013b). In order to exploit these regularities, we assume that the quality of a regressor’s prediction on an instance is directly proportional to how similar the instance is to the ones in the labelled set. Since the input for the regressors are words, we compute the similarity between a test word and the words in the labelled dataset as the maximum cosine similarity between the test word’s vector and the vectors in the labelled set. Let M be an embeddings model trained over vocabulary V , S a set of training seeds, ζ a minimum confidence threshold, sim(w, S, M ) the maximum cosine similarity betw"
N16-1050,P15-4015,1,0.939037,"resources, we were only able to predict the Familiarity, Age of Acquisition, Concreteness and Imagery values of the remaining 85,942 words in MRC. 4 Evaluation Since we were not able to find previous work for this task, in these experiments, we compare the performance of our bootstrapping strategy to various baselines. For training, we use the Ridge regression algorithm (Tikhonov, 1963). As features, our regressor uses the word’s raw embedding values, along with the following 15 lexical features: • Word’s length and number of syllables, as determined by the Morph Adorner module of LEXenstein (Paetzold and Specia, 2015). • Word’s frequency in the Brown (Francis and Kucera, 1979), SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016), Wikipedia and Simple Wikipedia (Kauchak, 2013) corpora. • Number of senses, synonyms, hypernyms and hyponyms for word in WordNet (Fellbaum, 1998). • Minimum, maximum and average distance between the word’s senses in WordNet and the thesaurus’ root sense. • Number of images found for word in the Getty Images database1 . We train our embedding models using word2vec (Mikolov et al., 2013a) over a corpus of 7 billion words composed by the SubIMDB corpus, UMBC webbas"
N16-1050,W16-4912,0,0.102431,"words in MRC. 4 Evaluation Since we were not able to find previous work for this task, in these experiments, we compare the performance of our bootstrapping strategy to various baselines. For training, we use the Ridge regression algorithm (Tikhonov, 1963). As features, our regressor uses the word’s raw embedding values, along with the following 15 lexical features: • Word’s length and number of syllables, as determined by the Morph Adorner module of LEXenstein (Paetzold and Specia, 2015). • Word’s frequency in the Brown (Francis and Kucera, 1979), SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016), Wikipedia and Simple Wikipedia (Kauchak, 2013) corpora. • Number of senses, synonyms, hypernyms and hyponyms for word in WordNet (Fellbaum, 1998). • Minimum, maximum and average distance between the word’s senses in WordNet and the thesaurus’ root sense. • Number of images found for word in the Getty Images database1 . We train our embedding models using word2vec (Mikolov et al., 2013a) over a corpus of 7 billion words composed by the SubIMDB corpus, UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We use 5-fold cross-validation"
N16-1050,N15-2002,1,0.839505,"Paetzold (Paetzold and Specia, 2015): Ranks words according to their distance to a decision boundary learned from a classification setup inferred from ranking examples. Uses n-gram frequencies as features. We use data from the English Lexical Simplification task of SemEval 2012 to assess systems’ performance. The goal of the task is to rank words in different contexts according to their simplicity. The training and test sets contain 300 and 1,710 instances, respectively. The official metric from the task – TRank (Specia et al., 2012) – is used to measure systems’ performance. As discussed in (Paetzold, 2015), this metric best represents LS performance in practice. The results in Table 2 show that the addition of our features lead to performance increases with all rankers. Performing F-tests over the rankings estimated for the simplest candidate in each instance, we have found these differences to be statistically significant (p < 0.05). Using our features, the Paetzold ranker reaches the best published results for the dataset, significantly superior to the best system in SemEval (Jauhar and Specia, 2012). Ranker Best SemEval Horn Glavas Paetzold TRank W/O W 0.602 0.625 0.635 0.623 0.636 0.653 0.6"
N16-1050,S12-1046,1,0.875178,"semantic metrics, and then re-ranks them according to their average rankings. • Paetzold (Paetzold and Specia, 2015): Ranks words according to their distance to a decision boundary learned from a classification setup inferred from ranking examples. Uses n-gram frequencies as features. We use data from the English Lexical Simplification task of SemEval 2012 to assess systems’ performance. The goal of the task is to rank words in different contexts according to their simplicity. The training and test sets contain 300 and 1,710 instances, respectively. The official metric from the task – TRank (Specia et al., 2012) – is used to measure systems’ performance. As discussed in (Paetzold, 2015), this metric best represents LS performance in practice. The results in Table 2 show that the addition of our features lead to performance increases with all rankers. Performing F-tests over the rankings estimated for the simplest candidate in each instance, we have found these differences to be statistically significant (p < 0.05). Using our features, the Paetzold ranker reaches the best published results for the dataset, significantly superior to the best system in SemEval (Jauhar and Specia, 2012). Ranker Best SemE"
N16-1050,P95-1026,0,0.660134,"te-of-theart performance in Semantic Composition (denotative/connotative) and Semantic Modification (intersective/subsective) prediction. Despite the evident usefulness of psycholinguistic properties of words, resources describing such properties are rare. The most extensively developed resource for English is the MRC Psycholinguistic Database (Section 2). However, it is far from complete, most likely due to the inherent cost of manually entering such properties. In this paper we propose a method to automatically infer these missing properties. We train regressors by performing bootstrapping (Yarowsky, 1995) over the existing features in the MRC database, exploiting word embedding models and other linguistic resources for that (Section 3). This approach outperform various strong baselines (Section 4) and the resulting properties lead to significant improvements when used in Lexical Simplification models (Section 5). 435 Proceedings of NAACL-HLT 2016, pages 435–440, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 The MRC Psycholinguistic Database 3. Predict values for a set of unlabelled instances U . Introduced by Coltheart (1981), the MRC (Machine Read"
P15-4015,P14-2075,0,0.17849,"ed in WordNet. We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et"
P15-4015,O13-1007,0,0.29547,"module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et al., 2013), or by discarding substitutions which do not share the same POS tag of the target complex word (Kajiwara et al., 2013; Paetzold and Specia, 2013). LEXenstein’s SS module provides access to three approaches. All approaches require as input a dictionary of substitutions generated by a given approach and a dataset in the VICTOR format (as in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex word in the hi th position of Si , cji a substitution candidate and rij its simplicity ranking. Each bracketed compone"
P15-4015,P13-1151,0,0.194569,"rmat, lists of stop and basic words, as well as language models and lexica built over Wikipedia and Simple Wikipedia. 3 Experiments In this Section, we discuss the results obtained in four benchmarking experiments. SelectorEvaluator: Provides evaluation metrics for SS methods. It requires a gold-standard in the VICTOR format and a set of selected substitutions. It returns the Potential, Precision and Fmeasure of the SS approach, as defined above. 3.1 Substitution Generation In this experiment we evaluate all SG approaches in LEXenstein. For the KauchakGenerator, we use the corpus provided by (Kauchak, 2013), composed of 150, 569 complex-to-simple parallel sentences, parsed by the Stanford Parser (Klein and Manning, 1965). From the the same corpus, we build the required vocabularies and language models for the BiranGenerator. We used the LexMturk dataset as the gold-standard (Horn et al., 2014), which is composed by 500 sentences, each with a single target complex word and 50 substitutions suggested by turkers. The results are presented in Table 1. The results in Table 1 show that the method of (Horn et al., 2014) yields the best F-Measure results, although combining the output of all generation"
P15-4015,P00-1056,0,0.144076,"set. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset, wi a target complex word in the hi th position of Si , cji a substitution candidate and rij its simplicity ranking. Each bracketed component is separated by a tabulation marker.   hS1 i hw1 i hh1 i r11 :c11 · · ·hrn1 :cn1 i   ..   (1) . 1 1 hSm i hwm i hhm i rm :cm · · ·hrnm :cnm i KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). BiranGenerator (Biran et al., 2011) Filters substitutions based on the Cartesian product between vocabularies of complex and simple words. It requires vocabularies of complex and simple words, as well as two language models trained over complex and simple corpora. It produces a dictionary linking words to a set of synonyms and hypernyms filtered by the criteria described in (Biran et al., 2011). LEXenstein includes two resources for training/testing in the VICTOR format: the"
P15-4015,W13-4813,1,0.844781,"to their synonyms, as listed in WordNet. We define Substitution Generation (SG) as the task of producing candidate substitutions for complex words, which is normally done regardless of the context of the complex word. Previous work commonly addresses this task by querying general domain thesauri such as WordNet (Fellbaum, 1998), or domain specific ones such as UMLS (Bodenreider, 2004). Examples of work resorting to this strategy are (Devlin and Tait, 1998) and (Carroll et al., 1999). Recent work focuses on learning substitutions from sentence-aligned parallel corpora of complex-simple texts (Paetzold and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kaza"
P15-4015,W04-2013,0,0.0585151,"and Specia, 2013; Horn et al., 2014). LEXenstein’s SG module offers support for five approaches. All approaches use LEXenstein’s Text Adorning module to create substitutions for all possible inflections of verbs and nouns. Each approach is represented by one of the following Python classes: 2.2 Substitution Selection (SS) is the task of selecting which substitutions – from a given list – can replace a complex word in a given sentence without altering its meaning. Most work addresses this task referring to the context of the complex word by employing Word Sense Disambiguation (WSD) approaches (Sedding and Kazakov, 2004; Nunes et al., 2013), or by discarding substitutions which do not share the same POS tag of the target complex word (Kajiwara et al., 2013; Paetzold and Specia, 2013). LEXenstein’s SS module provides access to three approaches. All approaches require as input a dictionary of substitutions generated by a given approach and a dataset in the VICTOR format (as in Victor Frankenstein (Shelley, 2007)). As output, they produce a set of selected substitutions for each entry in the VICTOR dataset. The VICTOR format is structured as illustrated in Example 1, where Si is the ith sentence in the dataset,"
P15-4015,P11-2087,0,0.569157,"on of Si , cji a substitution candidate and rij its simplicity ranking. Each bracketed component is separated by a tabulation marker.   hS1 i hw1 i hh1 i r11 :c11 · · ·hrn1 :cn1 i   ..   (1) . 1 1 hSm i hwm i hhm i rm :cm · · ·hrnm :cnm i KauchakGenerator (Horn et al., 2014) Automatically extracts substitutions from parallel corpora. It requires a set of tagged parallel sentences and the word alignments between them in Pharaoh format (Och and Ney, 2000). It produces a dictionary of complex-to-simple substitutions filtered by the criteria described in (Horn et al., 2014). BiranGenerator (Biran et al., 2011) Filters substitutions based on the Cartesian product between vocabularies of complex and simple words. It requires vocabularies of complex and simple words, as well as two language models trained over complex and simple corpora. It produces a dictionary linking words to a set of synonyms and hypernyms filtered by the criteria described in (Biran et al., 2011). LEXenstein includes two resources for training/testing in the VICTOR format: the LexMTurk (Horn et al., 2014) and the SemEval corpus (Specia et al., 2012). Each approach in the SS module is represented by one of the following Python cla"
P15-4015,S12-1046,1,0.906795,"Missing"
P15-4015,E99-1042,0,0.781784,"the most effective approach to Lexical Simplification. 1 Introduction The goal of a Lexical Simplification (LS) approach is to replace complex words and expressions in a given text, often a sentence, with simpler alternatives of equivalent meaning in context. Although very intuitive, this is a challenging task since the substitutions must preserve both the original meaning and the grammaticality of the sentence being simplified. The LS task has been gaining significant attention since the late 1990’s, thanks to the positive influence of the early work presented by (Devlin and Tait, 1998) and (Carroll et al., 1999). More recently, the LS task at SemEval-2012 (Specia et al., 2012) has given LS wider visibility. Participants had the opportunity to compare their approaches in the task of ranking candidate substitutions, all of which were already known to fit the context, according to their “simplicity”. Despite its growth in popularity, the inexistence of tools to support the process and help researchers to build upon has been hampering progress in the area. We were only able to find one tool for LS: a Figure 1: Lexical Simplification Pipeline LEXenstein was devised to facilitate performance comparisons am"
P15-4015,P03-1054,0,\N,Missing
P15-4015,P94-1019,0,\N,Missing
P15-4015,S12-1066,1,\N,Missing
P15-4020,W14-3340,0,0.0963817,"Missing"
P15-4020,2014.eamt-1.21,1,0.93638,"al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metr"
P15-4020,W15-4916,1,0.841275,"Missing"
P15-4020,2014.eamt-1.22,1,0.821768,"hine learning algorithms to build and test quality estimation models. Results on recent datasets show that Q U E ST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source f"
P15-4020,P10-1063,0,0.153973,"on. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metrics have to be used as approximation (Scarton et al., 2015). Some applications require fine-g"
P15-4020,P13-4014,1,0.872058,"Missing"
P15-4020,2011.eamt-1.12,1,0.122579,"a higher level (e.g. sentences). Q U E ST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that Q U E ST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard"
P15-4020,W14-3339,0,0.0335233,"esources and preprocessing steps so that extractors for new features can be easily added. The basic functioning of the feature extraction module requires raw text files with the source and translation texts, and a few resources (where available) such as the MT source training corpus and source and target language models (LMs). Configuration files are used to indicate paths for resources and the features that should be extracted. For its main resources (e.g. LMs), if a resource is missing, Q U E ST++ can generate it automatically. 3.1 Word level We explore a range of features from recent work (Bicici and Way, 2014; Camargo de Souza et al., 2014; Luong et al., 2014; Wisniewski et al., 2014), totalling 40 features of seven types: Figure 1 depicts the architecture of Q U E ST++ . Document and Paragraph classes are used for document-level feature extraction. A Document is a group of Paragraphs, which in turn is a group of Sentences. Sentence is used for both word- and sentence-level feature extraction. A Feature Processing Module was created for each level. Each processing level is independent and can deal with the peculiarities of its type of feature. Target context These are features that explore the con"
P15-4020,C04-1046,0,0.539275,"uality Prediction with Q U E ST++ Lucia Specia, Gustavo Henrique Paetzold and Carolina Scarton Department of Computer Science University of Sheffield, UK {l.specia,ghpaetzold1,c.scarton}@sheffield.ac.uk extraction toolkits are available for that: A SIYA1 and Q U E ST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While ce"
P15-4020,D12-1097,0,0.0149439,"at always predicts “Unintelligible” for Multi-Class, “Fluency” for Level 1, and “Bad” for the Binary setup. Document level Results The F-1 scores for the WMT14 datasets are given in Tables 1–4, for Q U E ST++ and systems that oficially participated in the task. The results show that Q U E ST++ was able to outperform all participating systems in WMT14 except for the English-Spanish baseline in the Binary and Level 1 tasks. The results in Table 5 also highlight the importance of selecting an adequate learning algorithm in CRF models. Our document-level features follow from those in the work of (Wong and Kit, 2012) on MT evaluation and (Scarton and Specia, 2014) for documentlevel QE. Nine features are extracted, in addition to aggregated values of sentence-level features for the entire document: • content words/lemmas/nouns repetition in S/T , • ratio of content words/lemmas/nouns in S/T , 4 System Q U E ST++ Baseline LIG/BL LIG/FS FBK-1 FBK-2 LIMSI RTM-1 RTM-2 Experiments In what follows, we evaluate Q U E ST++’s performance for the three prediction levels and various datasets. 4.1 Word-level QE Binary 0.502 0.525 0.441 0.444 0.487 0.426 0.473 0.350 0.328 Level 1 0.392 0.404 0.317 0.317 0.372 0.385 − 0"
P15-4020,P11-1022,0,\N,Missing
P15-4020,W14-3344,0,\N,Missing
P15-4020,2015.eamt-1.17,1,\N,Missing
S16-1085,E99-1042,0,0.0319809,"dictor of word complexity. 1 Introduction Complex Word Identification (CWI) is the task of deciding which words should be simplified in a given text. It is commonly connected with the task of Lexical Simplification (LS), which has as goal to replace complex words and expressions with simpler alternatives. In the usual LS pipeline, which was first introduced by (Shardlow, 2014), CWI is the first step. An effective CWI strategy can prevent LS approaches from replacing simple words, and hence prevent them from making grammatical and/or semantic errors. Early LS approaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dat"
S16-1085,P13-1151,0,0.035208,"h word in a given sentence. In the following we provide more details on the sentences used and the annotation process. 3.1 Data Sources We selected 9,200 sentences to be annotated, after filtering out cases with spurious characters, HTML 561 CW Corpus (Shardlow, 2013b): composed of 731 sentences from the Simple English Wikipedia in which exactly one word had been simplified by Wikipedia editors from the standard English Wikipedia. Commonly used for the training and evaluation of Complex Word Identification systems. 231 sentences that conformed to our criteria were extracted. Simple Wikipedia (Kauchak, 2013): composed of 167,689 sentences from the Simple English Wikipedia, each aligned to an equivalent sentence in the standard English Wikipedia. We selected a set of 8,700 sentences from the Simple Wikipedia version that conformed to our criteria and were aligned to an identical sentence in Wikipedia. The goal was to evaluate the ability of the Wikipedia (human) editors in identifying complex words for readers of the Simple Wikipedia. 3.2 Annotation Process 400 non-native speakers of English participated in the experiment, mostly university students or staff. Volunteers provided anonymous informat"
S16-1085,padro-stanilovsky-2012-freeling,0,0.0331487,"evaluate the ability of the Wikipedia (human) editors in identifying complex words for readers of the Simple Wikipedia. 3.2 Annotation Process 400 non-native speakers of English participated in the experiment, mostly university students or staff. Volunteers provided anonymous information about their native language, age, education level and English proficiency level according to CEFR (Common European Framework of Reference for Languages). They were asked to judge whether or not they could understand the meaning of each content word (nouns, verbs, adjectives and adverbs, as tagged by Freeling (Padr and Stanilovsky, 2012)) in a set of sentences, each of which was judged independently. Volunteers were instructed to annotate all words that they could not understand individually, even if they could comprehend the meaning of the sentence as a whole. A subset of 200 sentences was split into 20 subsets of 10 sentences, and each subset was annotated by a total of 20 volunteers. The remaining 9,000 sentences were split into 300 subsets of 30 sentences, each of which was annotated by a single volunteer. 4 Analysis A total of 35,958 distinct words were annotated (232,481 in total). Out of these, 3,854 distinct words (6,"
S16-1085,W13-4813,1,0.927707,"Identification (CWI) is the task of deciding which words should be simplified in a given text. It is commonly connected with the task of Lexical Simplification (LS), which has as goal to replace complex words and expressions with simpler alternatives. In the usual LS pipeline, which was first introduced by (Shardlow, 2014), CWI is the first step. An effective CWI strategy can prevent LS approaches from replacing simple words, and hence prevent them from making grammatical and/or semantic errors. Early LS approaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dataset for CWI. Although a relevant contribution, this datase"
S16-1085,P13-3015,0,0.280809,"proaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dataset for CWI. Although a relevant contribution, this dataset contains only 731 instances extracted automatically from the Simple English Wikipedia edits, which raises concerns about its reliability and applicability. The results obtained by Shardlow (2013a) highlight some of the issues of the dataset. They use the CW corpus to compare the performance of three solutions to CWI: a Threshold-Based approach, a Support Vector Machine (SVM), and a “Simplify Everything” approach. In their experiments, the “Simplify Everything” approach achieves higher Accuracy,"
S16-1085,W13-2908,0,0.0792963,"proaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS model can find a candidate substitution which is simpler. Their results, however, show that the approach is unable to find simplifications for one third of the complex words in the dataset. (Shardlow, 2013b) presents the CW corpus: the first dataset for CWI. Although a relevant contribution, this dataset contains only 731 instances extracted automatically from the Simple English Wikipedia edits, which raises concerns about its reliability and applicability. The results obtained by Shardlow (2013a) highlight some of the issues of the dataset. They use the CW corpus to compare the performance of three solutions to CWI: a Threshold-Based approach, a Support Vector Machine (SVM), and a “Simplify Everything” approach. In their experiments, the “Simplify Everything” approach achieves higher Accuracy,"
S16-1085,shardlow-2014-open,0,0.101234,"re submitted from 21 distinct teams, and nine baselines were provided. The results highlight the effectiveness of Decision Trees and Ensemble methods for the task, but ultimately reveal that word frequencies remain the most reliable predictor of word complexity. 1 Introduction Complex Word Identification (CWI) is the task of deciding which words should be simplified in a given text. It is commonly connected with the task of Lexical Simplification (LS), which has as goal to replace complex words and expressions with simpler alternatives. In the usual LS pipeline, which was first introduced by (Shardlow, 2014), CWI is the first step. An effective CWI strategy can prevent LS approaches from replacing simple words, and hence prevent them from making grammatical and/or semantic errors. Early LS approaches (Devlin and Tait, 1998; Carroll et al., 1999) do not include CWI. As shown in (Paetzold and Specia, 2013; Shardlow, 2014), ignoring this step can considerably decrease the quality of the output produced by a simplifier. CWI has been gaining popularity in recent research. The LS approach in (Horn et al., 2014) employs an implicit CWI strategy in which a target word is only deemed complex if the LS mod"
S16-1149,W07-1007,0,0.0237372,"ge the members of a given target audience. It is part of the usual Lexical Simplification pipeline, which is illustrated in Figure 1. As shown by the results obtained by (Paetzold and Specia, 2013) and (Shardlow, 2014), ignoring the step of Complex Word Identification in Lexical Simplification can lead simplifiers to neglect challenging words, as well as to replace simple words with inappropriate alternatives. Various strategies have been devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is complex. The complexity identifier for the lexical simplifier in (Keskis¨arkk¨a, 2012), for Swedish, uses a threshold over word frequencies to distinguish complex from simple words. Recently, however, more sophisticated approaches have been used. (Shardlow, 2013) presents a CWI benchmarking that compares the performance of a Threshold-Based strategy, a Support Vector Machine (SVM) model trained over various features, and a “simp"
S16-1149,N06-1058,0,0.00985943,"series of sub-systems, to avoid confusion, we henceforth refer to these subsystems as “voters”. 3.1 Features Our voters use a total of 69 features. They can be divided in four categories: • Binary: If a target word is part of a certain vocabulary, then it receives label 1, otherwise, 0. We extract vocabularies from Simple Wikipedia (Kauchak, 2013), Ogden’s Basic English (Ogden, 1968) and SubIMDB (Paetzold, 2015). 971 • Lexical: Includes word length, number of syllables, number of senses, synonyms, hypernyms and hyponyms in WordNet (Fellbaum, 1998), and language model probability in Wikipedia (Kauchak and Barzilay, 2006), Simple Wikipedia and SubIMDB. • Collocational: Language model probabilities of all n-gram combinations with windows w < 3 to the left and right of the target complex word in Wikipedia, SUBTLEX (Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS tag n-gram combinations with windows w < 3 to the left and right, and the word’s language model backoff behavior (Uhrik and Ward, 1997) according to a 5-gram language model trained over Simple Wikipedia with SRILM (Stolcke and others, 2002). In order for language model probabilit"
S16-1149,P13-1151,0,0.032053,"classes. In what follows, we described the features and settings used in the creation of our two CWI systems: SV000gg-Hard and SV000gg-Soft. While SV000gg-Hard uses basic Hard Voting, SV000ggSoft uses Performance-Oriented Soft Voting. Since both of them combine a series of sub-systems, to avoid confusion, we henceforth refer to these subsystems as “voters”. 3.1 Features Our voters use a total of 69 features. They can be divided in four categories: • Binary: If a target word is part of a certain vocabulary, then it receives label 1, otherwise, 0. We extract vocabularies from Simple Wikipedia (Kauchak, 2013), Ogden’s Basic English (Ogden, 1968) and SubIMDB (Paetzold, 2015). 971 • Lexical: Includes word length, number of syllables, number of senses, synonyms, hypernyms and hyponyms in WordNet (Fellbaum, 1998), and language model probability in Wikipedia (Kauchak and Barzilay, 2006), Simple Wikipedia and SubIMDB. • Collocational: Language model probabilities of all n-gram combinations with windows w < 3 to the left and right of the target complex word in Wikipedia, SUBTLEX (Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS ta"
S16-1149,W13-4813,1,0.846746,"its prediction confidence, allowing for completely heterogeneous systems to be combined. Our performance comparison shows that our voting techniques outperform traditional Soft Voting, as well as other systems submitted to the shared task, ranking first and second overall. 1 Figure 1: Lexical Simplification pipeline Introduction In Complex Word Identification (CWI), the goal is to find which words in a given text may challenge the members of a given target audience. It is part of the usual Lexical Simplification pipeline, which is illustrated in Figure 1. As shown by the results obtained by (Paetzold and Specia, 2013) and (Shardlow, 2014), ignoring the step of Complex Word Identification in Lexical Simplification can lead simplifiers to neglect challenging words, as well as to replace simple words with inappropriate alternatives. Various strategies have been devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is com"
S16-1149,P15-4015,1,0.841177,"(Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS tag n-gram combinations with windows w < 3 to the left and right, and the word’s language model backoff behavior (Uhrik and Ward, 1997) according to a 5-gram language model trained over Simple Wikipedia with SRILM (Stolcke and others, 2002). In order for language model probabilities to be calculated, we train a 5-gram language model for each of the aforementioned corpora using SRILM (Stolcke and others, 2002). Nominal features were obtained with the help of LEXenstein (Paetzold and Specia, 2015). 3.2 Voters We train a total of 21 voters which we have grouped in three categories: • Lexicon-Based (LB): If a word is present in a given vocabulary of simple words, then it is simple, otherwise, it is complex. We train one Lexicon-Based voter for each binary feature described in the previous Section. • Threshold-Based (TB): Given a certain feature, learns the threshold t which best separates complex and simple words. In order to learn t, it first calculates the feature value for all instances in the training data and obtains its minimum and maximum. It then divides the interval into 10, 000"
S16-1149,N15-2002,1,0.843404,"used in the creation of our two CWI systems: SV000gg-Hard and SV000gg-Soft. While SV000gg-Hard uses basic Hard Voting, SV000ggSoft uses Performance-Oriented Soft Voting. Since both of them combine a series of sub-systems, to avoid confusion, we henceforth refer to these subsystems as “voters”. 3.1 Features Our voters use a total of 69 features. They can be divided in four categories: • Binary: If a target word is part of a certain vocabulary, then it receives label 1, otherwise, 0. We extract vocabularies from Simple Wikipedia (Kauchak, 2013), Ogden’s Basic English (Ogden, 1968) and SubIMDB (Paetzold, 2015). 971 • Lexical: Includes word length, number of syllables, number of senses, synonyms, hypernyms and hyponyms in WordNet (Fellbaum, 1998), and language model probability in Wikipedia (Kauchak and Barzilay, 2006), Simple Wikipedia and SubIMDB. • Collocational: Language model probabilities of all n-gram combinations with windows w < 3 to the left and right of the target complex word in Wikipedia, SUBTLEX (Brysbaert and New, 2009), Simple Wikipedia and SubIMDB. • Nominal: Includes the word itself, its POS tag, both word and POS tag n-gram combinations with windows w < 3 to the left and right, an"
S16-1149,P13-3015,0,0.121076,"devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is complex. The complexity identifier for the lexical simplifier in (Keskis¨arkk¨a, 2012), for Swedish, uses a threshold over word frequencies to distinguish complex from simple words. Recently, however, more sophisticated approaches have been used. (Shardlow, 2013) presents a CWI benchmarking that compares the performance of a Threshold-Based strategy, a Support Vector Machine (SVM) model trained over various features, and a “simplify everything” baseline. (Shardlow, 2013)’s SVM model has shown promising results, but CWI approaches do not tend to explore Machine Learning techniques and, in particular, their combination. As an effort to fill this gap, in this paper we describe our contributions to the Complex Word Identification task of SemEval 2016. We introduce two systems, SV000gg-Hard and SV000gg-Soft, both of which use straightforward Ensemble Metho"
S16-1149,shardlow-2014-open,0,0.415258,"wing for completely heterogeneous systems to be combined. Our performance comparison shows that our voting techniques outperform traditional Soft Voting, as well as other systems submitted to the shared task, ranking first and second overall. 1 Figure 1: Lexical Simplification pipeline Introduction In Complex Word Identification (CWI), the goal is to find which words in a given text may challenge the members of a given target audience. It is part of the usual Lexical Simplification pipeline, which is illustrated in Figure 1. As shown by the results obtained by (Paetzold and Specia, 2013) and (Shardlow, 2014), ignoring the step of Complex Word Identification in Lexical Simplification can lead simplifiers to neglect challenging words, as well as to replace simple words with inappropriate alternatives. Various strategies have been devised to address CWI and most of them are very simple in nature. For example, to identify complex words, the lexical simplifier for the medical domain in (Elhadad and Sutaria, 2007) uses a Lexicon-Based approach that exploits the UMLS (Bodenreider, 2004) database: if a medical expression is among the technical terms registered in UMLS, then it is complex. The complexity"
S19-2093,S19-2007,0,0.0280948,"ns, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each language, we created two variants of UTFPR: one trained exclusively over the training data provided by the organizers (UTFPR/O), and another that uses a pre-trained set of character-toword RNN layers extracted from the models introduced by Paetzold (2018) (UTFPR/W). The pretrained model was trained for the English multiclass classification Emotion Analysis shared task Task Description HatEval (Basile et al., 2019) provides participants with annotated datasets to create systems capable of properly identifying hate speech in tweets writ1 https://competitions.codalab.org/ competitions/20011 520 Figure 1: Architecture of the UTFPR models. missions for English, and 31st out of 35 valid submissions for Spanish. of WASSA 2018, which featured a training set of 153, 383 instances composed of a tweet and an emotion label. This pre-trained model for English was used for the UTFPR/W variant of both languages, since we wanted to test the hypothesis that pre-training a character-to-word RNN on a large dataset for En"
S19-2093,W18-6224,1,0.837176,"ectly predicting all three of the aforementioned labels. In this paper, we focus on Task A exclusively, for both English and Spanish. We participated in the competition using the team name UTFPR. 4 The UTFPR Models The UTFPR models are minimalistic Recurrent Neural Networks (RNNs) that learn compositional numerical representations of words based on the sequence of characters that compose them, then use them to learn a final representation for the sentence being analyzed. These models, of which the architecture is illustrated in Figure 1, are somewhat similar to those of Ling et al. (2015) and Paetzold (2018), who use RNNs to create compositional neural models for different tasks. As illustrated, the UTFPR models take as input a sentence, split it into words, then split the words into a sequence of characters in order to pass them through a character embedding layer. The character embeddings are passed onto a set of bidirectional RNN layers that produces word representations, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each language, we created two v"
S19-2093,W17-1101,0,0.0327898,"vesting in ways to cope with such content using human moderation of posts, triage of content, deletion of offensive posts, and banning abusive users. One of the most common and effective strategies to tackle the problem of offensive language online is to train systems capable of recognizing such content. Several studies have been published in the last few years on identifying abusive language (Nobata et al., 2016), cyber aggression (Kumar et al., 2018), cyber bullying (Dadvar et al., 2013), and hate speech (Burnap and Williams, 2015; Davidson et al., 2017). As evidenced in two recent surveys (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018) and in a number of other 2 Related Work As evidenced in the introduction of this paper, there have been a number of studies on automatic hate speech identification published in the last few years. One of the most influential recent papers on hate speech identification is the one by Davidson et al. (2017). In this paper, the authors presented the Hate Speech Detection dataset which contains posts retrieved from social media labeled with three categories: OK (posts not containing profanity or hate speech), Offensive (posts containing swear words and general profanity),"
S19-2093,W17-3012,0,0.0138337,"ation for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important role in the identification and the definition of hate speech when compared to other forms of abusive content. The two SemEval-2019 shared tasks, HatEval and OffensEval, both include a sub-task"
S19-2093,N19-1144,1,0.884481,"in swear words. It has been ar519 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 519–523 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important ro"
S19-2093,W17-3013,0,0.0670864,"Missing"
S19-2093,S19-2010,1,0.83211,"in swear words. It has been ar519 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 519–523 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important ro"
S19-2093,W18-4401,1,0.891356,"Missing"
S19-2093,D15-1176,0,0.0398909,"ful. • Sub-task B: Correctly predicting all three of the aforementioned labels. In this paper, we focus on Task A exclusively, for both English and Spanish. We participated in the competition using the team name UTFPR. 4 The UTFPR Models The UTFPR models are minimalistic Recurrent Neural Networks (RNNs) that learn compositional numerical representations of words based on the sequence of characters that compose them, then use them to learn a final representation for the sentence being analyzed. These models, of which the architecture is illustrated in Figure 1, are somewhat similar to those of Ling et al. (2015) and Paetzold (2018), who use RNNs to create compositional neural models for different tasks. As illustrated, the UTFPR models take as input a sentence, split it into words, then split the words into a sequence of characters in order to pass them through a character embedding layer. The character embeddings are passed onto a set of bidirectional RNN layers that produces word representations, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each langua"
S19-2093,malmasi-zampieri-2017-detecting,1,\N,Missing
S19-2140,S19-2010,0,0.0853494,"y prejudice against a certain group or individual, offensive speech is often more challenging to identify because it encompasses a more broad spectrum of language, featuring expressions that do not necessarily convey prejudice (Malmasi and Zampieri, 2018). Technologies that identify these types of patterns could, for instance, help a social media platform on profiling users and take appropriate action whenever someone breaks user agreements and/or terms of use. 2 Task Summary The UTFPR systems described herein are a contribution to the OffensEval shared task held at the SemEval 2019 workshop (Zampieri et al., 2019b). In this shared task, participants were tasked with creating innovative classifiers capable of identify801 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 801–805 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics ing and categorizing offensive tweets. This shared tasks has 3 sub-tasks: Because of that, we decided to train two different model variants: • Task A: Binary classification task that consists in judging whether a tweet is offensive or not. • UTFPR-Scratch: The model depicted in Figure 1 trained f"
S19-2140,W18-6208,0,0.0169935,"referred methods for handling the orthographic irregularity of social media content are using word embeddings trained over tweets (Rozental et al., 2018) or regularizing unusual spellings (Bertaglia and das Grac¸as Volpe Nunes, 2017), but neither of them ensure that every possible orthographically irregular word will be understood by the NLP model in question. Recently, however, there have been a lot of contributions that present compositional neural models that learn numerical representations of words based on the sequence of characters that compose them (Kim et al., 2016; Ling et al., 2015; Balazs et al., 2018; Paetzold, 2018). These models have been demonstrated to be both effective in text classification, and robust when faced with orthographic irregularity. In this paper, we present the UTFPR system submitted to the OffensEval shared task of SemEval 2019, which employs compositional neural models to identify offensive language in tweets. In the following sections we describe the task (section 2), our model (section 3) and experiments (sections 4 to 5). We present the UTFPR system for the OffensEval shared task of SemEval 2019: A character-to-word-to-sentence compositional RNN model trained exclu"
S19-2140,W18-6206,0,0.0284589,"Missing"
S19-2140,D15-1176,0,0.247327,"way. Some of the preferred methods for handling the orthographic irregularity of social media content are using word embeddings trained over tweets (Rozental et al., 2018) or regularizing unusual spellings (Bertaglia and das Grac¸as Volpe Nunes, 2017), but neither of them ensure that every possible orthographically irregular word will be understood by the NLP model in question. Recently, however, there have been a lot of contributions that present compositional neural models that learn numerical representations of words based on the sequence of characters that compose them (Kim et al., 2016; Ling et al., 2015; Balazs et al., 2018; Paetzold, 2018). These models have been demonstrated to be both effective in text classification, and robust when faced with orthographic irregularity. In this paper, we present the UTFPR system submitted to the OffensEval shared task of SemEval 2019, which employs compositional neural models to identify offensive language in tweets. In the following sections we describe the task (section 2), our model (section 3) and experiments (sections 4 to 5). We present the UTFPR system for the OffensEval shared task of SemEval 2019: A character-to-word-to-sentence compositional RN"
S19-2140,W18-6224,1,0.433656,"handling the orthographic irregularity of social media content are using word embeddings trained over tweets (Rozental et al., 2018) or regularizing unusual spellings (Bertaglia and das Grac¸as Volpe Nunes, 2017), but neither of them ensure that every possible orthographically irregular word will be understood by the NLP model in question. Recently, however, there have been a lot of contributions that present compositional neural models that learn numerical representations of words based on the sequence of characters that compose them (Kim et al., 2016; Ling et al., 2015; Balazs et al., 2018; Paetzold, 2018). These models have been demonstrated to be both effective in text classification, and robust when faced with orthographic irregularity. In this paper, we present the UTFPR system submitted to the OffensEval shared task of SemEval 2019, which employs compositional neural models to identify offensive language in tweets. In the following sections we describe the task (section 2), our model (section 3) and experiments (sections 4 to 5). We present the UTFPR system for the OffensEval shared task of SemEval 2019: A character-to-word-to-sentence compositional RNN model trained exclusively over the t"
S19-2140,W18-6207,0,0.0155064,"ghpaetzold@utfpr.edu.br Abstract Identifying offensive language within the content of social media platforms is particularly challenging, since this type of content is usually littered with irregular orthography, meta-characters, slang and others. Since a lot of the effort from the research community focuses on identifying offensive language in social media platforms, an NLP approach for such a task must be able to overcome those hurdles in some way. Some of the preferred methods for handling the orthographic irregularity of social media content are using word embeddings trained over tweets (Rozental et al., 2018) or regularizing unusual spellings (Bertaglia and das Grac¸as Volpe Nunes, 2017), but neither of them ensure that every possible orthographically irregular word will be understood by the NLP model in question. Recently, however, there have been a lot of contributions that present compositional neural models that learn numerical representations of words based on the sequence of characters that compose them (Kim et al., 2016; Ling et al., 2015; Balazs et al., 2018; Paetzold, 2018). These models have been demonstrated to be both effective in text classification, and robust when faced with orthogr"
S19-2140,N19-1144,0,0.100033,"y prejudice against a certain group or individual, offensive speech is often more challenging to identify because it encompasses a more broad spectrum of language, featuring expressions that do not necessarily convey prejudice (Malmasi and Zampieri, 2018). Technologies that identify these types of patterns could, for instance, help a social media platform on profiling users and take appropriate action whenever someone breaks user agreements and/or terms of use. 2 Task Summary The UTFPR systems described herein are a contribution to the OffensEval shared task held at the SemEval 2019 workshop (Zampieri et al., 2019b). In this shared task, participants were tasked with creating innovative classifiers capable of identify801 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 801–805 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics ing and categorizing offensive tweets. This shared tasks has 3 sub-tasks: Because of that, we decided to train two different model variants: • Task A: Binary classification task that consists in judging whether a tweet is offensive or not. • UTFPR-Scratch: The model depicted in Figure 1 trained f"
W13-4813,I11-1053,0,0.0392347,"Missing"
W13-4813,J96-2004,0,0.0479818,"Missing"
W13-4813,P96-1041,0,0.168474,"of the tree transformations, we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus [Coster and Kauchak 2011] parsed by the Stanford constituency parser [Klein and Manning 2003]. Language Model For this experiment a 3-gram language model is used to rank candidate simplifications. The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple sentences. We use the SRILM toolkit [Stolcke et al. 2011] to build the 3-gram language model. The motivation to use a 3-gram rather than a 4 or 5-gram language model is based on the studies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either noisy or not large enough. Test Corpus The corpus to be simplified in our tests contains 130 original sentences randomly selected from a held-out portion of the parallel Simple Wikipedia corpus. All sentences have a simpler version in the corpus which is different from the original version, and vary between 50 and 240 characters in length. Evaluation Metrics We evaluate the simplification both automatically by the string matching metric BLEU [Papineni et al. 2002], and manually using"
W13-4813,P11-2117,0,0.0516376,"2.3. Ranking Module This is a simple module that scores all candidate simplifications for a given complex sentence produced by the lexical or syntactic simplification modules. The scores are then used to rank the candidates such that the top scoring candidate can be selected. At this stage, we score candidates using their predicted perplexity based on a language model of simplified sentences, as described in the next section. 3. Experimental Settings Training Corpus For the extraction of the tree transformations, we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus [Coster and Kauchak 2011] parsed by the Stanford constituency parser [Klein and Manning 2003]. Language Model For this experiment a 3-gram language model is used to rank candidate simplifications. The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple sentences. We use the SRILM toolkit [Stolcke et al. 2011] to build the 3-gram language model. The motivation to use a 3-gram rather than a 4 or 5-gram language model is based on the studies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either no"
W13-4813,W11-2107,0,0.0301312,"lification approach uses as a basis possible transformations learned with the Tree Transducer Toolkit (T3) [Cohn and Lapata 2009]. The approach is composed by three main components: a training module, a simplification module, and a ranking module. 2.1. Training Module This module is responsible for generating a large number of candidate transformation rules, some of which will be purely syntactic or purely lexical, while others will be lexico-syntactic. It takes as input a parsed version of a parallel corpus of complex and simple sentences aligned at the word level (produced using Meteor 1.4 [Denkowski and Lavie 2011]). It uses the harvest function in T3 to extract a synchronous tree-substitution grammar which describes tree transformations. The harvest function extracts the maximally general (smallest) pairs of tree fragments that are consistent with the word-alignment. In Table 1 we show examples of transformations produced by this function using the Simple Wikipedia corpus, where # indicates indexes of place holders for any type of syntactic subtree. By inspecting Table 1, one will notice that the tree transformations can take very distinct forms. The first example shows a transformation which removes"
W13-4813,P03-1054,0,0.0240031,"simplifications for a given complex sentence produced by the lexical or syntactic simplification modules. The scores are then used to rank the candidates such that the top scoring candidate can be selected. At this stage, we score candidates using their predicted perplexity based on a language model of simplified sentences, as described in the next section. 3. Experimental Settings Training Corpus For the extraction of the tree transformations, we use 133K pairs of original-simple sentences from the Simple Wikipedia corpus [Coster and Kauchak 2011] parsed by the Stanford constituency parser [Klein and Manning 2003]. Language Model For this experiment a 3-gram language model is used to rank candidate simplifications. The model is trained on a superset of the Simple Wikipedia corpus containing 710K simple sentences. We use the SRILM toolkit [Stolcke et al. 2011] to build the 3-gram language model. The motivation to use a 3-gram rather than a 4 or 5-gram language model is based on the studies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either noisy or not large enough. Test Corpus The corpus to be simplified in"
W13-4813,P02-1040,0,0.0856712,"tudies presented at [Chen and Goodman 1996], which suggests that language models of higher order tend to lead to data sparsity when the corpus is either noisy or not large enough. Test Corpus The corpus to be simplified in our tests contains 130 original sentences randomly selected from a held-out portion of the parallel Simple Wikipedia corpus. All sentences have a simpler version in the corpus which is different from the original version, and vary between 50 and 240 characters in length. Evaluation Metrics We evaluate the simplification both automatically by the string matching metric BLEU [Papineni et al. 2002], and manually using five human subjects. Our system produces two simplified versions for each sentence, resulting in 260 simplified sentences to be evaluated. As part of the manual evaluation, each evaluator is assigned 100 sentences: the syntactically and lexically simplified versions of 50 originally complex sentences. 30 of these sentences are common to all evaluators so that inter-annotator agreement can be computed. Evaluators are asked to answer three questions for each syntactically or lexically simplified sentence: • Is it simpler than the original sentence? • Is it grammatical? • Do"
W13-4813,S12-1046,1,0.817339,"h is proposed in [Woodsend and Lapata 2011] using a quasi-synchronous grammar and integer programming to generate syntactic simplification rules. These are generally precise, but at the cost of low coverage. Most work on lexical simplification is based on synonym substitution using frequency-related or readability statistics, and also context models based mostly on bagof-words to identify whether a candidate substitution fits the context [Keskisarkka 2012, Kandula et al. 2010, Carroll et al. 1998]. For a comprehensive overview, we refer the reader to the SemEval-2012 shared task on the topic [Specia et al. 2012]. Overall, lexical simplification work disregards explicit syntactic cues. To overcome these limitations we investigate the learning of tree transduction rules to produce larger volumes of both lexical and lexicalized syntactic simplification rules. This approach has the potential to produce rule sets with high coverage and variability, and at the same time make them more specific by lexicalising some of the rule components of syntactic simplification rules. 2. Simplification Approach Our simplification approach uses as a basis possible transformations learned with the Tree Transducer Toolkit"
W13-4813,D11-1038,0,0.0605627,"c Fortaleza, CE, Brazil, October 21–23, 2013. 2013 Sociedade Brasileira de Computa¸c˜ ao Efforts on using statistical and machine learning techniques to address syntactic simplification include several approaches inspired by phrase- or tree-based statistical models for machine translation [Specia 2010, Zhu et al. 2010, Wubben et al. 2012], which learn limited transformations such as short-distance reordering. [Bach et al. 2011] design templates for subject-verb-object simplification, which limit the application of rules to cases matching the templates. A more general approach is proposed in [Woodsend and Lapata 2011] using a quasi-synchronous grammar and integer programming to generate syntactic simplification rules. These are generally precise, but at the cost of low coverage. Most work on lexical simplification is based on synonym substitution using frequency-related or readability statistics, and also context models based mostly on bagof-words to identify whether a candidate substitution fits the context [Keskisarkka 2012, Kandula et al. 2010, Carroll et al. 1998]. For a comprehensive overview, we refer the reader to the SemEval-2012 shared task on the topic [Specia et al. 2012]. Overall, lexical simp"
W13-4813,P12-1107,0,0.112594,"Missing"
W13-4813,C10-1152,0,0.170855,"Missing"
W15-1835,P02-1034,0,0.481059,"rate that PST’s grow in log-linear fashion, scaling well to large tree datasets. Introduction 2 A tree kernel is a type of convolution kernel that represents as features the substructures that compose a tree. They can be interpreted as a function K (T1 , T2 ), of which the value is a similarity measure between tree structures T1 and T2 . Recently, tree kernels have become popular, and shown to be an efficient solution in tasks such as Question Classification (Moschitti, 2006), Relation Extraction (Zelenko et al., 2003), Named Entity Recognition (Culotta and Sorensen, 2004), Syntactic Parsing (Collins and Duffy, 2002), Semantic Role Labeling (Moschitti, 2006), Semantic Parsing (Moschitti, 2004), Glycan Classification (Yamanishi et al., 2007) and Plagiarism Detection (Son et al., 2006). However efficient, tree kernels can be very difficult to calculate in a reasonable amount of time. Calculating K (T1 , T2 ) usually requires many verifications between node labels and can easily achieve quadratic complexity. Although algorithms of much lower complexity have been proposed (Moschitti, 2006), their performance can still be unsatisfactory in solving problems which involve large datasets. Positional Suffix Trees"
W15-1835,P04-1054,0,0.0594935,"gorithm is over 22 times faster. We also demonstrate that PST’s grow in log-linear fashion, scaling well to large tree datasets. Introduction 2 A tree kernel is a type of convolution kernel that represents as features the substructures that compose a tree. They can be interpreted as a function K (T1 , T2 ), of which the value is a similarity measure between tree structures T1 and T2 . Recently, tree kernels have become popular, and shown to be an efficient solution in tasks such as Question Classification (Moschitti, 2006), Relation Extraction (Zelenko et al., 2003), Named Entity Recognition (Culotta and Sorensen, 2004), Syntactic Parsing (Collins and Duffy, 2002), Semantic Role Labeling (Moschitti, 2006), Semantic Parsing (Moschitti, 2004), Glycan Classification (Yamanishi et al., 2007) and Plagiarism Detection (Son et al., 2006). However efficient, tree kernels can be very difficult to calculate in a reasonable amount of time. Calculating K (T1 , T2 ) usually requires many verifications between node labels and can easily achieve quadratic complexity. Although algorithms of much lower complexity have been proposed (Moschitti, 2006), their performance can still be unsatisfactory in solving problems which inv"
W15-1835,P03-1054,0,0.0108013,"l in Cpst .Children[ j].Keys: 8. then EFTK(Ct , Cpst , j, Ss ) 9. else Ss = {} 10. Result = new Hash() 11. for ID in Ss: 12. Result [ID] = 1 13. return Result 5 5.1 Experiments Performance Comparison In this experiment, we conduct a performance comparison between the MFTK and EFTK algorithms, the baseline QTK (Quadratic Tree Kernel) and the state-of-the-art FTK (Fast Tree Kernel), both of which were proposed by Moschitti (2006). We have chosen to estimate the processing time taken by the algorithms to calculate the kernel values between the constituency parses produced by the Stanford Parser (Klein and Manning, 2003) of 500 test and 5452 training questions. The datasets were devised for the task of Question Classification (Li and Roth, 2002). The algorithms were implemented in Python, and ran in a computer with R a quad-core Intel Core i7-4500U 1.8GHz and 8Gb of RAM running at 1600MHz. Since the time taken by both FTK and MFTK to calculate ST and SST kernels do not vary, we choose to present the performance results for ST kernel calculation only. Figure 3 illustrate the results obtained for increasing portions of the training set, and Figure 4 the average time taken to calculate K (Ti , Ti ) for Ti of dif"
W15-1835,C02-1150,0,0.0128098,"ID] = 1 13. return Result 5 5.1 Experiments Performance Comparison In this experiment, we conduct a performance comparison between the MFTK and EFTK algorithms, the baseline QTK (Quadratic Tree Kernel) and the state-of-the-art FTK (Fast Tree Kernel), both of which were proposed by Moschitti (2006). We have chosen to estimate the processing time taken by the algorithms to calculate the kernel values between the constituency parses produced by the Stanford Parser (Klein and Manning, 2003) of 500 test and 5452 training questions. The datasets were devised for the task of Question Classification (Li and Roth, 2002). The algorithms were implemented in Python, and ran in a computer with R a quad-core Intel Core i7-4500U 1.8GHz and 8Gb of RAM running at 1600MHz. Since the time taken by both FTK and MFTK to calculate ST and SST kernels do not vary, we choose to present the performance results for ST kernel calculation only. Figure 3 illustrate the results obtained for increasing portions of the training set, and Figure 4 the average time taken to calculate K (Ti , Ti ) for Ti of different node sizes. The MFTK algorithm is on average 3.54 times faster than FTK for different corpus sizes, while Proceedings of"
W15-1835,P04-1043,0,0.0755525,"duction 2 A tree kernel is a type of convolution kernel that represents as features the substructures that compose a tree. They can be interpreted as a function K (T1 , T2 ), of which the value is a similarity measure between tree structures T1 and T2 . Recently, tree kernels have become popular, and shown to be an efficient solution in tasks such as Question Classification (Moschitti, 2006), Relation Extraction (Zelenko et al., 2003), Named Entity Recognition (Culotta and Sorensen, 2004), Syntactic Parsing (Collins and Duffy, 2002), Semantic Role Labeling (Moschitti, 2006), Semantic Parsing (Moschitti, 2004), Glycan Classification (Yamanishi et al., 2007) and Plagiarism Detection (Son et al., 2006). However efficient, tree kernels can be very difficult to calculate in a reasonable amount of time. Calculating K (T1 , T2 ) usually requires many verifications between node labels and can easily achieve quadratic complexity. Although algorithms of much lower complexity have been proposed (Moschitti, 2006), their performance can still be unsatisfactory in solving problems which involve large datasets. Positional Suffix Trees In order for us to create faster algorithms for the calculation of tree kernel"
W15-3041,N13-1090,0,0.603059,"s which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). While neural network language models predict the next word given a preceding context, (Mikolov et al., 2013b) proposed a neural network framework to predict the word given the left and right contexts, or to predict the word’s left and right contexts in a given sentence. Recently, it has been shown that these distributed vector representations (or word embeddings) can be exploited across languages to predict translations (Mikolov et al., 2013a). The word representations are learned from large monolingual data independently for source and target languages. A small seed dictionary is used to learn mapping from the source into the target space. In this paper, we investigate the use of such resources in"
W15-3041,J03-1002,0,0.00477454,"dited by crowdsourced translators, and HTER labels were computed using the TER tool (settings: tokenised, case insensitive, exact matching only, with scores capped to 1). 4.2.2 Word alignment training To extract word embedding features, as explained in Section 3, we need word-to-word alignments between source and target data. As word-level alignments between the source and target corpora were not made available by WMT, we first aligned the QE datasets with a bilingual word-level alignment model trained on the same data used for the word2vec modelling step, with the help of the GIZA++ toolkit (Och and Ney, 2003). Working on target side, we refined the resulting n-m targetto-source word alignments to a set of 1-m alignments by filtering potential spurious source-side candidates out. To do so, the decision was based on the lexical probabilities extracted from the previous alignment training step. Hence, each target4.1.2 Feature set We extracted the following features: • AF: 80 black-box features using the QuEst framework (Specia et al., 2013; Shah et al., 2013a) as described in Shah et al. (2013b). • CSLM: A feature for each source and target sentence using CSLM as described in Section 2. • FS(AF): Top"
W15-3041,H05-1026,0,0.173581,"from segment pairs in isolation, ignoring contextual clues from other segments in the text. The focus of our contributions this year is to introduce a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). While neural network language models predict the next word given a preceding context, (Mikolov et al., 2013b) proposed a neural network framework to predict the word given the left and right contexts, or to predict the word’s left and right contexts in a given sentence. Recently, it has been shown that these distributed vector representations (or word embeddings) can be exploited across languages to predict translations (Mikolov et al., 2013a). The word representations are learned from large monolingual data independently for source and"
W15-3041,C12-2104,0,0.035367,"her segments in the text. The focus of our contributions this year is to introduce a new set of features which are language-independent, require minimal resources, and can be extracted in unsupervised ways with the use of neural networks. Word embeddings have shown their potential in modelling long distance dependencies in data, including syntactic and semantic information. For instance, neural network language models (Bengio et al., 2003) have been successfully explored in many problems including Automatic Speech Recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and Machine Translation (Schwenk, 2012). While neural network language models predict the next word given a preceding context, (Mikolov et al., 2013b) proposed a neural network framework to predict the word given the left and right contexts, or to predict the word’s left and right contexts in a given sentence. Recently, it has been shown that these distributed vector representations (or word embeddings) can be exploited across languages to predict translations (Mikolov et al., 2013a). The word representations are learned from large monolingual data independently for source and target languages. A small seed dictionary is used to le"
W15-3041,2013.mtsummit-papers.21,1,0.922831,"extracted word embeddings for all words in the Task 2 training, development and test sets from these models to be used as features. These distributed numerical representations of words as features aim at locating each word as a point in a 500-dimensional space. Inspired by the work of (Mikolov et al., 2013a), we extracted another feature by mapping the source space onto a target space using a seed dictionary (trained with Europarl + Newscommentary + News-crawl). A given word and 2 http://www-lium.univ-lemans.fr/cslm/ 343 https://code.google.com/p/word2vec/ (GPs) by the procedure described in (Shah et al., 2013b). its continuous vector representation a could be mapped to the other language space by computing z = M a, where M is a transformation matrix learned with stochastic gradient descent. The assumption is that the vector representations of similar words in different languages are related by a linear transformation because of similar geometric arrangements. The words whose representation are closest to a in the target language space, using cosine similarity, are considered as potential translations for a given word in the source language. Since the goal of QE is not to translate content, but to"
W15-3041,W13-2241,1,0.886632,"late content, but to measure the quality of translations, we take the source-to-target similarity scores as a feature itself. To calculate it, we first learn word alignments (see Section 4.2.2), and then compute the similarity scores between target word and the source word aligned to it. 4 4.1.3 We use the Support Vector Machines implementation in the scikit-learn toolkit (Pedregosa et al., 2011) to perform regression (SVR) on each feature set with either linear or RBF kernels and parameters optimised using grid search. We also apply GPs with similar settings to those in our WMT13 submission (Beck et al., 2013) using GPy toolkit 3 . For models with feature selection, we train a GP, select the top 20 features according to the produced feature ranking, and then retrain a SparseGP on the full training set using these 20 features and 50 inducing points. To evaluate the prediction models we use Mean Absolute Error (MAE), its squared version – Root Mean Squared Error (RMSE), and Spearman’s Correlation. Experiments We present experiments on the WMT15 QE Tasks 1 and 2, with CSLM features for Task 1, and word embedding features for Task 2. 4.1 Learning algorithms 4.2 Task 2 4.2.1 Dataset The data for this is"
W15-3041,P13-4014,1,0.89127,"Missing"
W15-3041,P15-4020,1,0.79936,"Missing"
W15-3041,P10-2041,0,\N,Missing
W16-2381,2009.eamt-1.5,1,0.8201,"f actions, which in this case are tag predictions. This setting allows us to incorporate structural information in the classifier by using features based on previous tag predictions. For instance, let us assume that we are trying to predict the tag ti for word wi . A simple classifier can use features derived from wi and also any other words in the sentence. By framing this as a sequence, it can also use features extracted from the previously predicted tags t{1:i−1} . Introduction Quality estimation (QE) models aim at predicting the quality of machine translated (MT) text (Blatz et al., 2004; Specia et al., 2009). This prediction can be at several levels, including word-, sentenceand document-level. In this paper we focus on our submission to the word-level QE WMT 2016 shared task, where the goal is to assign quality labels to each word of the output of an MT system. Word-level QE is traditionally treated as a structured prediction problem, similar to part-of-speech (POS) tagging. The baseline model used in the shared task employs a Conditional Random Field (CRF) (Lafferty et al., 2001) with a set of baseline features. Our system uses a linear classification model trained with imitation learning (Daum"
W16-2381,C04-1046,0,0.0316158,"perform a sequence of actions, which in this case are tag predictions. This setting allows us to incorporate structural information in the classifier by using features based on previous tag predictions. For instance, let us assume that we are trying to predict the tag ti for word wi . A simple classifier can use features derived from wi and also any other words in the sentence. By framing this as a sequence, it can also use features extracted from the previously predicted tags t{1:i−1} . Introduction Quality estimation (QE) models aim at predicting the quality of machine translated (MT) text (Blatz et al., 2004; Specia et al., 2009). This prediction can be at several levels, including word-, sentenceand document-level. In this paper we focus on our submission to the word-level QE WMT 2016 shared task, where the goal is to assign quality labels to each word of the output of an MT system. Word-level QE is traditionally treated as a structured prediction problem, similar to part-of-speech (POS) tagging. The baseline model used in the shared task employs a Conditional Random Field (CRF) (Lafferty et al., 2001) with a set of baseline features. Our system uses a linear classification model trained with im"
W16-2388,W15-3001,1,0.818079,"her a regression or a classification problem. Sentence-level QE has been covered by shared tasks organised by WMT since 2012, with subsequent years covering also word and documentlevel tasks. Recent advances in Distributional Semantics have been showing promising results in the context of QE strategies for different prediction levels. An example of that are modern word embedding architectures, such as the CBOW and Skip-Gram models introduced by (Mikolov et al., 2013b), which have been used as features in some of the best ranking systems in the sentence and word-level QE shared tasks of WMT15 (Bojar et al., 2015). Word embeddings are not only versatile, but also cheap to produce, making for both reliable and cost-effective QE solutions. Neural Networks have also been successfully employed in QE. The FBK-UPV-UEdin (Bojar et al., 2014) and HDCL (Bojar et al., 2015) systems are good examples of that. They achieved 1st and 2nd places in the word-level QE tasks of WMT14 and WMT15, respectively, outperforming strategies that resort to much more resource-heavy features. Another successful example are neural Language Models for sentence-level QE (Shah et al., 2015). We were not able to find, however, any exam"
W16-2388,P13-1151,0,0.0165544,"NT/SVM1 RTM/RTM-SVR BASELINE SHEF/SimpleNets-SRC SHEF/SimpleNets-TGT r 0.525 0.460 0.451 0.447 0.430 0.412 0.377 0.376 0.370 0.363 0.358 0.351 0.320 0.283 MAE 12.30 13.58 12.88 13.52 12.97 19.57 13.60 13.46 13.43 20.01 13.59 13.53 13.92 14.35 RMSE 16.41 18.60 17.03 18.38 17.33 24.11 17.64 17.81 18.15 24.63 18.06 18.39 18.23 18.22 Table 1: Sentence-level QE scores of systems submitted to the WMT16 task models over a corpus of around 7 billion words comprised by SubIMDB (Paetzold and Specia, 2016b), UMBC webbase2 , News Crawl3 , SUBTLEX (Brysbaert and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). For evaluation we use the task’s official metrics, which are Pearson correlation (r), Mean Average Error and Root Mean Squared Error. We compare our SimpleNets with the baseline provided by the task organisers, as well as all other systems submitted. The baseline uses SVM regression with an RBF kernel and grid search for parameter optimisation. 6 itself. Interestingly, the performance scores suggest that the model employed by SimpleNets is more proficient in learning how difficult it will be for the source sentence to be translated. The difference in performance between the SimpleNets varian"
W16-2388,W15-1521,0,0.042722,"Missing"
W16-2388,W16-4912,0,0.0250797,"Each of the M rows in matrix M×N represent a given word in the n-gram, and each of the N columns, its embedding values. If an n-gram is smaller than N , the matrix is padded with embedding values composed strictly of zeroes. The SimpleNets Approach SimpleNets aim to provide a resource-light and language agnostic approach for sentence-level QE. Our main goal in conceiving SimpleNets was to create a reliable enough solution that could be cheaply and easily adapted to other language pairs, moving away from the use of extensive feature engineering. The SimpleNets approach was first introduced by Paetzold and Specia (2016a) as a solution to the shared task on Quality Assessment for Text Simplification of QATS 20161 , in which participants were asked to create systems that predict discrete quality labels for a set of automatically produced text simplifications. Labels could take three values: “Good”, “Ok” and “Bad”. Text Simplification differs from Machine Translation in the sense 1 5. Learning: Training instances are then fed into a deep Long Short-Term Memory (LSTM) Recurrent Neural Network in minibatches so that a quality prediction model can be learned. Notice that this process yields a model that predicts"
W16-2388,2014.eamt-1.22,1,0.747524,"n more about a translation’s quality by focusing on the original sentence, rather than on the translation itself. 1 Introduction The task of Machine Translation Quality Estimation (QE) has gained noticeable popularity in the last few years. The goal of QE is to predict the quality of translations produced by a certain Machine Translation (MT) system in the absence of reference translations. Reliable solutions for QE can be useful in various tasks, such as improving post-editing efficiency (Specia, 2011), selecting high quality translations (Soricut and Echihabi, 2010), translation re-ranking (Shah and Specia, 2014), and visual assistance for manual translation revision (Bach et al., 2011). QE can be performed in various ways in order to suit different purposes. The most widely addressed form of this task is sentence-level QE. Most existing work addresses this task as a supervised learning problem, in which a set of training examples is used to learn a model that predicts the quality of unseen translations. As quality labels, previous work uses either real valued scores estimated by humans, which require for a given QE 812 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task"
W16-2388,D15-1125,1,0.835695,"ce and word-level QE shared tasks of WMT15 (Bojar et al., 2015). Word embeddings are not only versatile, but also cheap to produce, making for both reliable and cost-effective QE solutions. Neural Networks have also been successfully employed in QE. The FBK-UPV-UEdin (Bojar et al., 2014) and HDCL (Bojar et al., 2015) systems are good examples of that. They achieved 1st and 2nd places in the word-level QE tasks of WMT14 and WMT15, respectively, outperforming strategies that resort to much more resource-heavy features. Another successful example are neural Language Models for sentence-level QE (Shah et al., 2015). We were not able to find, however, any examples of sentence-level QE systems that combine word embedding models and Neural Networks. In this paper, we present our efforts in doing so. We introduce SimpleNets: the resource-light and language agnostic sentence-level QE systems submitted to WMT16 that exploit the principle of compositionality for QE. In the Sections that follow, we describe the sentence-level QE task of WMT16, introduce the approach used by the SimpleNets systems, and present the results obtained. We introduce SimpleNets: a resource-light solution to the sentence-level Quality"
W16-2388,2006.amta-papers.25,0,0.0777774,"nd meaning. For the Quality Assessment for Text Simplification task of QATS 2016, SimpleNets used the approach illustrated in Figure 1. For training, it performed the following five steps: SimpleNets are two systems submitted to the sentence-level QE task of WMT16. In this task, participants were challenged to predict real-valued quality scores in 0,100 of sentences translated from English into German. The translations were produced by an in-house phrase-based Statistical Machine Translation system, and were then postedited by professional translators. The real-valued quality scores are HTER (Snover et al., 2006) values that represent the post-editing effort spent on each given translation. The task organisers provided three datasets: 1. Decomposition: Given a simplification and maximum n-gram size M , it obtains the ngrams with size 1 ≤ n ≤ M of both original and simplified sentences. • Training: Contains 12,000 translation instances accompanied by their respective postedits and HTER values. 2. Union: It then creates a pool of n-grams by simply obtaining the union of n-grams from the original and simplified sentences. • Development: Contains 1,000 translation instances accompanied by their respective"
W16-2388,P10-1063,0,0.0316205,"eriments show that, surprisingly, our models can learn more about a translation’s quality by focusing on the original sentence, rather than on the translation itself. 1 Introduction The task of Machine Translation Quality Estimation (QE) has gained noticeable popularity in the last few years. The goal of QE is to predict the quality of translations produced by a certain Machine Translation (MT) system in the absence of reference translations. Reliable solutions for QE can be useful in various tasks, such as improving post-editing efficiency (Specia, 2011), selecting high quality translations (Soricut and Echihabi, 2010), translation re-ranking (Shah and Specia, 2014), and visual assistance for manual translation revision (Bach et al., 2011). QE can be performed in various ways in order to suit different purposes. The most widely addressed form of this task is sentence-level QE. Most existing work addresses this task as a supervised learning problem, in which a set of training examples is used to learn a model that predicts the quality of unseen translations. As quality labels, previous work uses either real valued scores estimated by humans, which require for a given QE 812 Proceedings of the First Conferenc"
W16-2388,P15-4020,1,0.865284,"Attribution: Exploiting an interpretation of the principle of compositionality, which states that the quality of a simplification can be determined by the quality of its n-grams, it assigns the quality label of the simplification instance itself to each and every n-gram in the pool. • Test: Contains 2,000 translation instances only, without their respective post-edits or HTER values. Each instance is composed by the original sentence in English along with its translation in German. HTER scores were capped to 100. The organisers also provided 17 baseline feature values extracted using QuEst++ (Specia et al., 2015) for each dataset. 3 4. Structuring: Using a trained word embeddings model, it transforms each n-gram into a training instance described by a matrix M×N , where M is the previously mentioned maximum n-gram size, and N the size of the word embeddings used. Each of the M rows in matrix M×N represent a given word in the n-gram, and each of the N columns, its embedding values. If an n-gram is smaller than N , the matrix is padded with embedding values composed strictly of zeroes. The SimpleNets Approach SimpleNets aim to provide a resource-light and language agnostic approach for sentence-level QE"
W16-2388,2011.eamt-1.12,1,0.885439,"mplification quality assessment in the past. Our experiments show that, surprisingly, our models can learn more about a translation’s quality by focusing on the original sentence, rather than on the translation itself. 1 Introduction The task of Machine Translation Quality Estimation (QE) has gained noticeable popularity in the last few years. The goal of QE is to predict the quality of translations produced by a certain Machine Translation (MT) system in the absence of reference translations. Reliable solutions for QE can be useful in various tasks, such as improving post-editing efficiency (Specia, 2011), selecting high quality translations (Soricut and Echihabi, 2010), translation re-ranking (Shah and Specia, 2014), and visual assistance for manual translation revision (Bach et al., 2011). QE can be performed in various ways in order to suit different purposes. The most widely addressed form of this task is sentence-level QE. Most existing work addresses this task as a supervised learning problem, in which a set of training examples is used to learn a model that predicts the quality of unseen translations. As quality labels, previous work uses either real valued scores estimated by humans, w"
W17-4765,W16-2388,1,0.490661,"d much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-layer perceptrons and multitask learning. We use these models to create the SHEF/CNN systems for the sentence-level Quality Estimation task of WMT 2017 and Emotion Intensity Analysis task of WASSA 2017. Our experiments reveal that combining character-level clues and engineer"
W17-4765,S16-1077,0,0.0469556,"Missing"
W17-4765,C12-1008,0,0.0669293,"Missing"
W17-4765,2006.amta-papers.25,0,0.0315253,"vramidis, 2012), or they can be used to help human translators decide which automatic translations are worth post-editing, and which should be re-translated from scratch (Turchi et al., 2015). Sentence-level QE is the most popular variant, mostly due the fact that most modern statistical and neural MT systems translate one sentence at a time. In this task, the input is the original-translated sentence pair and the output is some numeric label that represents quality. The most commonly used label is HTER, which measures the human post-editing effort required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 201"
W17-4765,W16-2382,0,0.0213286,"human post-editing effort required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep paralle"
W17-4765,D13-1170,0,0.00650731,"neered features and character-level information trained over HTER. The model used for the EIA task of WASSA 2017 applies only one convolution stack over the tweet being analysed, given that the task is not characterized by a sentence pair. The window range, convolution depth, as well as feature and final MLP depths are identical to the model used for the WMT 2017 task. We train one model for each emotion targeted in the shared task: anger, fear, joy and sadness. Since the organizers did not provide a set of baseline features, we produced our own features using the Stanford Sentiment Treebank (Socher et al., 2013), which is composed of 239,232 text segments annotated with respect to their positivity probability i.e. how likely they are to convey a positive emotion. The positivity values range from 0.0 (absolutely negative) to 1.0 (absolutely positive). Using this data, we extract nine features from each tweet: • SHEF/CNN-C+F+M: Uses the same architecture of SHEF/CNN-C+F, but the model is trained through multi-task learning over the values listed in Section 4. • The tweets’ emotion intensity; and Table 2 illustrates the Pearson, Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) scores for the"
W17-4765,P15-2087,0,0.013615,"ed Character-Level Convolutions for Text Regression Gustavo Henrique Paetzold and Lucia Specia Department of Computer Science University of Sheffield, UK {g.h.paetzold,l.specia}@sheffield.ac.uk Abstract sentence or even document level. Quality estimates can be incorporated in Machine Translation (MT) decoding or used for re-ranking of top candidates, for example, allowing for a more intelligently guided translation process (Avramidis, 2012), or they can be used to help human translators decide which automatic translations are worth post-editing, and which should be re-translated from scratch (Turchi et al., 2015). Sentence-level QE is the most popular variant, mostly due the fact that most modern statistical and neural MT systems translate one sentence at a time. In this task, the input is the original-translated sentence pair and the output is some numeric label that represents quality. The most commonly used label is HTER, which measures the human post-editing effort required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these"
W17-4765,W16-2384,0,0.246775,"employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-layer perceptrons and multitask learning. We use these models to create the SHEF/CNN systems for the sentence-level Quality Estimation task of WMT 2017 and Emotion Intensity Analysis task of WASSA 2017. Our experiments reveal that combining character-level clues and engineered features offers noticeable pe"
W17-4765,S16-1080,0,0.0670815,"Missing"
W17-4765,S16-1004,0,0.0220145,"uality at sentence-level. Though very interesting and distinct strategies, neither of them managed to outperform the best scoring SVM-based approach of WMT 2016. In the task of Emotion Intensity Analysis (EIA), Neural Networks have not yet been successfully employed. Unlike typical Sentiment Analysis tasks, which are set up as either binary or multiclass classification problems that require one to determine the opinion or sentiment in a given text, EIA aims at quantifying a certain emotion in a text, such as fear, anger, joy, sadness, etc. In the Emotion Intensity shared task of SemEval 2016 (Kiritchenko et al., 2016), which is the first of its kind, none of the five systems submitted employ neural regressors. We were also unable to find any other contributions outside the SemEval 2016 task that explore neural approaches to EIA. Given the volume of opportunities available when it comes to neural solutions for text regression, we introduce a new neural approach for the task. We innovate by using deep convolutional networks and multi-task learning to combine character-level information from the texts at hand with engineered features. Using this approach, we create the SHEF/CNN systems for the sentence-level"
W17-4765,W16-2385,0,0.281349,"lation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-layer perceptrons and multitask"
W17-4765,W15-3038,0,0.022573,"required to fix the translation in question (Snover et al., 2006). As shown in (Bojar et al., 2016), the performance of QE approaches submitted to the WMT shared tasks have steadily improved in recent years. However, the nature of these approaches have not changed much: most of the top ranking systems employ well-known regression methods and extensive feature engineering. Some of the most notable examples are the RTM systems of WMT 2014 and 15, which managed to reach the top of the ranks by employing Referential Translation Machines trained with SVMs for regression (Bicici, 2016). The LORIA (Langlois, 2015) and YSDA (Kozlova et al., 2016) systems of WMT 2015 and 2016, respectively, achieved similar performance by also pairing SVMs with many resource-heavy features. Neural Networks for sentence-level QE were introduced in WMT 2016 with the SimpleNets (Paetzold and Specia, 2016) and POSTECH (Kim and Lee, 2016) systems. While the SimpleNets system uses sequence-to-label LSTMs to predict the quality of a translation’s n-grams and then We present a new model for text regression that seamlessly combine engineered features and character-level information through deep parallel convolution stacks, multi-"
W17-4765,S17-1007,0,0.0315924,"ements over using only one of these sources of information in isolation. 1 Introduction Text regression consists in estimating a numeric label based on information available from the text. The label can represent any abstract property of said text: its appropriateness, sentiment, fluency, simplicity, quality, etc. Due to their wide applicability in both research and industry, some of these tasks have been gaining a lot of attention. These include Quality Estimation and Emotion Intensity Analysis, which are the subjects of shared tasks held at the WMT 2017 conference1 and WASSA 2017 workshop2 (Mohammad and Bravo-Marquez, 2017), respectively. In Quality Estimation (QE), one attempts to estimate the quality of a machine translated text based on the information that can be extracted from the original sentence and its translation. The task has many variants, given that the quality of a translation can be estimated at word, phrase, 1 2 http://www.statmt.org/wmt17 http://optima.jrc.it/wassa2017 575 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 575–581 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 23,000/25,000 and 1,000/1,000 i"
W17-4765,W16-2301,1,\N,Missing
W17-5910,S16-1150,0,0.136082,"Missing"
W17-5910,S16-1161,0,0.157498,"Missing"
W17-5910,L16-1284,1,0.850305,"Missing"
W17-5910,S16-1164,0,0.117013,"Missing"
W17-5910,S16-1157,0,0.169796,"Missing"
W17-5910,P13-3015,0,0.19796,"Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overv"
W17-5910,S16-1162,0,0.0588997,"Missing"
W17-5910,S16-1146,0,0.0839136,"Missing"
W17-5910,yimam-etal-2017-multilingual,0,0.0467335,"otators over a set of 200 sentences, while the test set is composed by the judgments made over 9,000 sentences by only one annotator. The 9,200 sentences were evenly distributed across the 400 annotators. In the training set, a word is considered to be complex if at least one of the 20 annotators judged them so, thus reproducing a scenario that captures one of the biggest challenges in lexical simplification: predicting the vocabulary limitations of individuals based on the overall limitations of a group. This dataset is one of the few datasets available for CWI, another example is the one by Yimam et al. (2017). We build ensemble classifiers taking the output of systems that participated in the SemEval CWI task as input. This approach is equivalent to training multiple classifiers and combining them using ensembles. Our first goal is to build highperformance classifiers using plurality voting. Our second goal is to estimate the theoretical upper bound performance given the output of the systems that participated in the SemEval CWI competition using the oracle classifier. Following Malmasi et al. (2015) and Goutte et al. (2016) we use two approaches: 2.2 3.1 Plurality Voting: This approach selects th"
W17-5910,W15-0620,1,0.877257,"nvestigate whether human annotation correlates to the systems’ performance by carefully analyzing the samples of multiple annotators. Although in the shared task complexity was modeled as a binary classification task, we pose that lexical complexity should actually be seen in a continuum spectrum. Intuitively, words that are labeled as complex more often should be easier to be predicted by CWI systems. This hypothesis is investigated in Section 3.3. To the best of our knowledge, no evaluation of this kind has been carried out for CWI. The most similar analyses to ours have been carried out by Malmasi et al. (2015) for native language identification and by Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI"
W17-5910,S16-1155,1,0.693934,"Missing"
W17-5910,S16-1152,0,0.123637,"Missing"
W17-5910,C16-1069,1,0.841348,"cal complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overview of the experiments we propose in this paper. The goal of the experiment"
W18-0507,W14-1206,0,0.0720755,"Missing"
W18-0507,W18-0519,0,0.153197,"Missing"
W18-0507,S16-1156,0,0.0850516,"Missing"
W18-0507,S16-1151,0,0.0753176,"Missing"
W18-0507,W18-0539,1,0.893727,"Missing"
W18-0507,W18-0538,0,0.100445,"lower log-probability for complex words. The systems submitted performed the best out of all systems for the cross-lingual task (the French dataset) both for the binary and probabilistic classification tasks, showing a promising direction in the creation of CWI dataset for new languages. 1 74 https://code.google.com/archive/p/word2vec/ tant features. Their best system shows an average performance compared to the other systems in the shared task for the monolingual English binary classification track. NLP-CIC present systems for the English and Spanish multilingual binary classification tasks (Aroyehun et al., 2018). The feature sets include morphological features such as frequency counts of target word on large corpora such as Wikipedia and Simple Wikipedia, syntactic and lexical features, psycholinguistic features from the MRC psycholinguistic database and entity features using the OpenNLP and CoreNLP tools, and word embedding distance as a feature which is computed between the target word and the sentence. Tree learners such as Random Forest, Gradient Boosted, and Tree Ensembles are used to train different classifiers. Furthermore, a deep learning approach based on 2D convolutional (CNN) and word embe"
W18-0507,S16-1148,0,0.0982594,"Missing"
W18-0507,W18-0518,0,0.065579,"on the ensemble techniques where AdaBoost classifier with 5000 estimators achieves the highest results, followed by the bootstrap aggregation classifier of Random Forest. All the features are used for the N EWS and W IKI N EWS datasets, but for the W IKIPEDIA dataset, MCR psycholinguistic features are excluded. For the probabilistic classification task, the same feature setups are used and the Linear Regression algorithm is used to estimate values of targets. CoastalCPH describe systems developed for multilingual and cross-lingual domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both"
W18-0507,W18-0520,0,0.193647,"l domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both for the binary and probabilistic classification tasks. They have used features that are based on the insights of the CWI shared task 2016 (Paetzold and Specia, 2016a) such as lexical features (word length, number of syllables, WordNet features such as the number of synsets), word n-gram and POS tags, and dependency parse relations. In addition, they have used features such as the number of words grammatically related to the target word, psycholinguistic features from the MRC database, CEFR (Common European Framework of Reference fo"
W18-0507,S16-1160,0,0.102164,"Missing"
W18-0507,W18-0540,0,0.0627627,"Missing"
W18-0507,W18-0521,0,0.24579,"layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using th"
W18-0507,S16-1164,0,0.0425763,"Missing"
W18-0507,S16-1149,1,0.538636,"elines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second edition focused on multilingualism providing datasets containing four languages: English, German, French, and Spanish. • English monolingual CWI; • German monolingual CWI; • Spanish monolingual CWI; and • Multilingual CWI with a French test set. For the first three tracks, participants were provided with training and testing data for the same language. For French, participants were provided only with a French test set and no French training data. In the CWI 2016, the task was cast as binary classification. To be able t"
W18-0507,S16-1162,0,0.138871,"Missing"
W18-0507,S16-1158,0,0.0719992,"Missing"
W18-0507,S16-1163,0,0.176167,"Missing"
W18-0507,D14-1162,0,0.0887949,"earning methods, 2) using the average embedding of target words as an input to a neural network, and 3) modeling the context of the target words using an LSTM. For the feature engineering-based systems, features such as linguistic, psycholinguistic, and language model features were used to train different binary and probabilistic classifiers. Lexical features include word length, number of syllables, and number of senses, hypernyms, and hyponyms in WordNet. For N-gram features, probabilities of the n-gram containing the target words were For embedding-based systems, a pre-trained GloVe model (Pennington et al., 2014) was used to get the vector representations of target words. For MWE, the average of the vectors is used. In the first approach, the resulting vector is passed on to a neural network with two ReLu layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For t"
W18-0507,S16-1154,1,0.847385,"Missing"
W18-0507,W18-0541,0,0.102296,"Missing"
W18-0507,S16-1153,1,0.879975,"Missing"
W18-0507,S16-1161,0,0.200613,"Missing"
W18-0507,S16-1147,0,0.032877,"Missing"
W18-0507,S16-1157,0,0.212408,"Missing"
W18-0507,I11-1017,0,0.0188214,"that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using the scikit-learn library. Feature ablation shows that both the length, frequency, and probability features (based on corpu"
W18-0507,P13-3015,0,0.247331,"ative speakers. To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity. One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation. More information about the data collection is presented in Section 3. Given the multilingual dataset provided, the CWI challenge was divided into four tracks: Introduction The most common first step in lexical simplification pipelines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second e"
W18-0507,S16-1152,0,0.262443,"Missing"
W18-0507,S16-1159,0,0.0576998,"Missing"
W18-0507,L16-1035,1,0.906663,"Missing"
W18-0507,W18-0522,0,0.230666,"n dependency relation, frequency features based on the BNC, Wikipedia, and Dale and Chall list corpora, number of synsets and senses in WordNet, and so on. The experiment is conducted using the Weka machine learning framework using the Support vector machine (with linear and radial basis function kernels), Na¨ıve Bayes, Logistic Regression, Random Tree, and Random Forest classification algorithms. The final experiments employ Support Vector Machines and Random Forest classifiers. CFILT IITB Developed ensemble-based classification systems for the English monolingual binary classification task (Wani et al., 2018). Lexical features based on WordNet for the target word are extracted as follows: 1) Degree of Polysemy: number of senses of the target word in WordNet, 2) Hyponym and Hypernym Tree Depth: the position of the word in WordNet’s hierarchical tree, and 3) Holonym and Meronym Counts: based on the relationship of the target word to its components (meronyms) or to the things it is contained in (Holonym’s). Additional feature classes include size-based features such as word count, word length, vowel counts, and syllable counts. They also use vocabulary-based features such as Ogden Basic (from Ogden’s"
W18-0507,S16-1146,0,0.112937,"Missing"
W18-0507,I17-2068,1,0.54282,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,yimam-etal-2017-multilingual,1,0.591517,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,W17-5910,1,0.857961,"Missing"
W18-0507,S16-1155,1,0.729801,"Missing"
W18-0507,S16-1150,0,\N,Missing
W18-6224,S18-1001,0,0.0664876,"Missing"
W18-6224,W16-4912,0,0.0277027,"o the details of how our preliminary experiment was structured. because the labels had not been made available at the time this experiment was conducted. Models: We tested three types of machine learning models; logistic regression, decision trees, and random forests. All these models were implemented with the help of scikit-learn1 . Input Features: We tried two types of features; TF-IDF weights from a bag-of-words model trained over our input training data (TF-IDF), and the average word embedding values of the words in the tweet (Embeddings). We used the 300dimension word embeddings model of Paetzold and Specia (2016), which was trained using the CBOW model (Mikolov et al., 2013) over a corpus of 7˜ billion words from assorted sources, such as news articles, subtitles, tweets, etc. Input Structure: We tested two types of inputs to the models; one in which we calculate and concatenate two separate feature representations of the words to the left and right of the target (Separate), and another in which we calculate only one feature representation of all words in the tweet aside from the target (Joint). Input Enhancement: We tested two variants of each model; one trained only on our training data (TR), and an"
W18-6483,W16-2347,0,0.21871,"corpus filtering task is a very simple one: given a large dataset containing many automatically harvested translations, rank them according to their quality i.e. how useful one can expect them to be to an MT system. The dataset provided contains around 1 billion words from English-to-German translations gath923 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 923–927 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64110 ered as part of the Paracrawl project (Buck and Koehn, 2016). The translations were of mixed domain, and among them are many spurious ones, such as misaligned translations, incomplete translations, translations with non-English and/or nonGerman sentences, etc. Participants were allowed to use the parallel corpora1 from the WMT 2018 MT shared task to train their systems, if they wished to do so. Participants were tasked with creating systems that assign a quality score to each translation in the dataset. To evaluate the systems, the organizers subsampled the dataset by choosing the N highest quality translations, training MT systems with them, then usin"
W18-6483,P18-4020,0,0.0492401,"Missing"
W18-6483,P07-2045,0,0.00557838,"eriment with three classification models: Logistic Regression (UTFPR-LR), Decision Trees (UTFPR-DT), and Random Forests (UTFPR-RF). We chose them because they use a varying array of learning methods, and can be trained efficiently even when presented with hundreds of millions of input instances. To evaluate our approach, the shared task organizers first created two sub-sampled sets of parallel translations containing the 10 million and 100 million highest scoring translations in the test set. They then used these sets to train both statistical (SMT) and neural MT (NMT) models using the Moses (Koehn et al., 2007) and Marian (Junczys-Dowmunt et al., 2018) toolkits, and evaluated the models according to BLEU-c (Koehn, 2011) over a combination of the newstest 20182 , iwslt 20173 , Acquis4 , EMEA5 , Global Voices6 , and KDE7 datasets. 2 http://statmt.org/wmt18/translation-task.html https://sites.google.com/site/iwsltevaluation2017 4 https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis 5 http://opus.nlpl.eu/EMEA.php 6 http://opus.nlpl.eu/GlobalVoices.php 7 http://opus.nlpl.eu/KDEdoc.php 3 5 Results We compare our approach to the 5 systems from the WMT 2018 parallel corpus filtering task with the hi"
W18-6483,2005.mtsummit-papers.11,0,0.213369,"abel 1). To create a set of filtered translations, we rank the translations according to 924 Figure 1: Architecture of the UTFPR systems their positive class probabilities and choose the ones with highest scores. We name our approach UTFPR in reference to the university sponsoring this contribution. 4 Experimental Setup As mentioned in Section 2, we submit our results to the parallel corpus filtering shared task of WMT 2018, of which the test set contains roughly one billion unfiltered parallel English-German translations. To train our supervised model, we use the Europarl v7 parallel corpus (Koehn, 2005), which contains 1, 920, 209 translations. For learning, we experiment with three classification models: Logistic Regression (UTFPR-LR), Decision Trees (UTFPR-DT), and Random Forests (UTFPR-RF). We chose them because they use a varying array of learning methods, and can be trained efficiently even when presented with hundreds of millions of input instances. To evaluate our approach, the shared task organizers first created two sub-sampled sets of parallel translations containing the 10 million and 100 million highest scoring translations in the test set. They then used these sets to train both"
W18-6483,L16-1147,0,0.0136328,"ed great success in this task. But regardless of how much MT approaches have evolved from a modelling standpoint, both c 2018 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. modern and legacy approaches learn from the same type of information: parallel data containing handcrafted translations. This data usually takes the form of millions (sometimes billions) of parallel original-to-translated sentences, and are often extracted from translated versions of documents, such as news articles (Bojar et al., 2017), and subtitles (Lison and Tiedemann, 2016). Despite being hand-crafted, sometimes these datasets contain a lot of spurious translation examples that would not necessarily teach anything useful to an MT model, potentially compromising its performance. Consequently, it is important to filter these datasets in order to maximise the model’s performance. Tiedemann (2012) and Lison et al. (2018) effectively filter large parallel corpora extracted from subtitles by using unsupervised metrics that combine features such as translation probabilities, language model probabilities, etc. In this contribution, we attempt to elaborate on the ideas o"
W18-6483,L18-1275,0,0.0129689,"lations. This data usually takes the form of millions (sometimes billions) of parallel original-to-translated sentences, and are often extracted from translated versions of documents, such as news articles (Bojar et al., 2017), and subtitles (Lison and Tiedemann, 2016). Despite being hand-crafted, sometimes these datasets contain a lot of spurious translation examples that would not necessarily teach anything useful to an MT model, potentially compromising its performance. Consequently, it is important to filter these datasets in order to maximise the model’s performance. Tiedemann (2012) and Lison et al. (2018) effectively filter large parallel corpora extracted from subtitles by using unsupervised metrics that combine features such as translation probabilities, language model probabilities, etc. In this contribution, we attempt to elaborate on the ideas of Tiedemann (2012) and Lison et al. (2018) by using such features as input to supervised machine learning models. In what follows, we present the UTFPR systems for the WMT 2018 parallel corpus filtering task: A minimalistic approach that aims at combining easy-to-harvest features with classic supervised binary classification models to create effici"
W18-6483,tiedemann-2012-parallel,0,0.0610185,"ing handcrafted translations. This data usually takes the form of millions (sometimes billions) of parallel original-to-translated sentences, and are often extracted from translated versions of documents, such as news articles (Bojar et al., 2017), and subtitles (Lison and Tiedemann, 2016). Despite being hand-crafted, sometimes these datasets contain a lot of spurious translation examples that would not necessarily teach anything useful to an MT model, potentially compromising its performance. Consequently, it is important to filter these datasets in order to maximise the model’s performance. Tiedemann (2012) and Lison et al. (2018) effectively filter large parallel corpora extracted from subtitles by using unsupervised metrics that combine features such as translation probabilities, language model probabilities, etc. In this contribution, we attempt to elaborate on the ideas of Tiedemann (2012) and Lison et al. (2018) by using such features as input to supervised machine learning models. In what follows, we present the UTFPR systems for the WMT 2018 parallel corpus filtering task: A minimalistic approach that aims at combining easy-to-harvest features with classic supervised binary classification"
W19-1423,P19-1068,0,0.0730096,"ip character bigrams and trigrams; • 2-skip character bigrams and trigrams; • 3-skip character bigrams and trigrams. Each feature class is used to train a single linear SVM classifier using LIBLINEAR (Fan et al., 1 210 http://oracc.museum.upenn.edu/ 2008). The outputs of these SVM classifiers on the training data are then used to train the metaclassifier. 4 As demonstrated by Ling et al. (2015), compositional recurrent neural networks can offer very reliable performance on a variety of NLP tasks. Previous language identification and dialect studies (Medvedeva et al., 2017; Kroon et al., 2018; Butnaru and Ionescu, 2019) and the results of the previous shared tasks organized at VarDial (Zampieri et al., 2017, 2018), however, showed that deep learning approaches do not outperform more linear n-gram-based methods so we were interested in comparing the performance of a neural model to the meta-classifier for this dataset. Results Table 2 showcases the results obtained by our team (PZ in bold) and the best submission by each of the eight teams which participating in the CLI shared task. Even though the competition allowed the use of other datasets (open submission), we have used only the dataset provided by the s"
W19-1423,L18-1387,0,0.0651532,"Missing"
W19-1423,L16-1522,0,0.0275791,"ialects and it was ranked fourth in the competition among eight teams. 1 Introduction As discussed in a recent survey (Jauhiainen et al., 2018), discriminating between similar languages, national language varieties, and dialects is an important challenge faced by state-of-the-art language identification systems. The topic has attracted more and more attention from the CL/NLP community in recent years with publications on similar languages of the Iberian peninsula (Zubiaga et al., 2016), and varieties and dialects of several languages such as Greek (Sababa and Stassopoulou, 2018) and Romanian (Ciobanu and Dinu, 2016) to name a few. As evidenced in Section 2, the focus of most of these studies is the identification of languages and dialects using contemporary data. A few exceptions include the work by Trieschnigg et al. (2012) who applied language identification methods to historical varieties of Dutch and the work by Jauhiainen et al. (2019) on languages written in cuneiform script: Sumerian and Akkadian. 2 Related Work Since its first edition in 2014, shared tasks on similar language and dialect identification have been organized together with the VarDial workshop co-located with international conference"
W19-1423,W19-1409,0,0.312218,"Missing"
W19-1423,W18-3928,0,0.045412,"of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the first writing systems in the world. We report the results obtained by the PZ team in the Cuneiform Language Identification (CLI) shared task organized within the scope of the VarDial Evaluation Campaign 2019. The task included two languages, Sumerian and Akkadian. The latter is divided into six diale"
W19-1423,W17-1201,1,0.915379,"Missing"
W19-1423,W18-3901,1,0.890509,"Missing"
W19-1423,W17-1222,1,0.931706,"uneiform is an ancient writing system invented by the Sumerians for more than three millennia. In this paper we describe computational approaches to language identification on texts written in cuneiform script. For this purpose we use the dataset made available by Jauhiainen et al. (2019) to participants of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the fi"
W19-1423,W17-1220,1,0.928976,"uneiform is an ancient writing system invented by the Sumerians for more than three millennia. In this paper we describe computational approaches to language identification on texts written in cuneiform script. For this purpose we use the dataset made available by Jauhiainen et al. (2019) to participants of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the fi"
W19-1423,W16-0314,1,0.927798,"l. (2012) who applied language identification methods to historical varieties of Dutch and the work by Jauhiainen et al. (2019) on languages written in cuneiform script: Sumerian and Akkadian. 2 Related Work Since its first edition in 2014, shared tasks on similar language and dialect identification have been organized together with the VarDial workshop co-located with international conferences such as COLING, EACL, and NAACL. The first and most well-attended of these competitions was the Discrminating between Similar Languages (DSL) shared task which has been organized between 2014 and 2017 (Malmasi et al., 2016b; Zampieri et al., 2014, 2015, 2017). The DSL provided the first benchmark for evaluation of language identification systems developed for similar languages and language varieties using the DSL Corpus Col209 Proceedings of VarDial, pages 209–213 c Minneapolis, MN, June 7, 2019 2019 Association for Computational Linguistics Language or Dialect Late Babylonian Middle Babylonian peripheral Neo-Assyrian Neo-Babylonian Old Babylonian Standard Babylonian Sumerian Total Code LTB MPB NE NEB OLB STB SUX Texts 671 365 3,570 1,212 527 1,661 5,000 13,006 Lines 31,893 11,015 65,932 19,414 7,605 35,633 107"
W19-1423,W14-5307,1,0.902016,"Missing"
W19-1423,W16-4801,1,0.88199,"Missing"
W19-1423,W15-5401,1,0.887227,"Missing"
W19-1423,W17-1219,0,0.0772777,"(2019) to participants of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the first writing systems in the world. We report the results obtained by the PZ team in the Cuneiform Language Identification (CLI) shared task organized within the scope of the VarDial Evaluation Campaign 2019. The task included two languages, Sumerian and Akkadian. The latter is d"
W19-1423,W18-6224,1,0.893538,"Missing"
W19-1423,S19-2140,1,0.86902,"Missing"
